__FILENAME__ = oqpath
# -*- coding: utf-8 -*-

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Helper code to set up system path values properly (for bin/ scripts).
"""

import os
import sys


def set_oq_path():
    """
    Adds the current directory to the system PATH so scripts can be run from
    root source dir.
    """
    if os.path.exists(os.path.join(os.path.dirname(os.path.dirname(__file__)),
                  'openquake')):
        sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

########NEW FILE########
__FILENAME__ = celeryconfig
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Config for all installed OpenQuake binaries and modules.
Should be installed by setup.py into /etc/openquake
eventually.
"""

import os
import sys

# just in the case that are you using oq-engine from sources
# with the rest of oq libraries installed into the system (or a
# virtual environment) you must set this environment variable
if os.environ.get("OQ_ENGINE_USE_SRCDIR"):
    sys.modules['openquake'].__dict__["__path__"].insert(
        0, os.path.join(os.path.dirname(__file__), "openquake"))

from openquake.engine.utils import config, get_core_modules
from openquake.engine.calculators import hazard, risk

config.abort_if_no_config_available()

sys.path.insert(0, os.path.dirname(__file__))

amqp = config.get_section("amqp")

BROKER_HOST = amqp.get("host")
BROKER_PORT = int(amqp.get("port"))
BROKER_USER = amqp.get("user")
BROKER_PASSWORD = amqp.get("password")
BROKER_VHOST = amqp.get("vhost")
# BROKER_POOL_LIMIT enables a connections pool so Celery can reuse
# a single connection to RabbitMQ. Value 10 is the default from
# Celery 2.5 where this feature is enabled by default.
# Actually disabled because it's not stable in production.
# See https://bugs.launchpad.net/oq-engine/+bug/1250402
BROKER_POOL_LIMIT = None

CELERY_RESULT_BACKEND = "amqp"

# CELERY_ACKS_LATE and CELERYD_PREFETCH_MULTIPLIER settings help evenly
# distribute tasks across the cluster. This configuration is intended
# make worker processes reserve only a single task at any given time.
# (The default settings for prefetching define that each worker process will
# reserve 4 tasks at once. For long running calculations with lots of long,
# heavy tasks, this greedy prefetching is not recommended and can result in
# performance issues with respect to cluster utilization.)
# CELERY_MAX_CACHED_RESULTS disable the cache on the results: this means
# that map_reduce will not leak memory by keeping the intermediate results
CELERY_ACKS_LATE = True
CELERYD_PREFETCH_MULTIPLIER = 1
CELERY_MAX_CACHED_RESULTS = 1

HAZARD_MODULES = get_core_modules(hazard)

RISK_MODULES = get_core_modules(risk)

CELERY_IMPORTS = HAZARD_MODULES + RISK_MODULES + [
    "openquake.engine.tests.utils.tasks"]

os.environ["DJANGO_SETTINGS_MODULE"] = "openquake.engine.settings"

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

# OpenQuake documentation build configuration file, created by
# sphinx-quickstart on Mon Aug  2 14:01:11 2010.
#
# This file is execfile()d with the current directory set to its containing
# dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
import sys

os.environ['DJANGO_SETTINGS_MODULE'] = 'openquake.engine.settings'

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('../..'))

# just in the case that are you using oq-engine from sources
# with the rest of oq libraries installed into the system (or a
# virtual environment) you must set this environment variable
if os.environ.get("OQ_ENGINE_USE_SRCDIR") != None:
    sys.modules['openquake'].__dict__["__path__"].insert(
            0, os.path.join(os.path.dirname(os.path.dirname(
                            os.path.dirname(__file__))), "openquake"))

import openquake.engine

# -- General configuration ----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = [
    'sphinx.ext.autodoc', 'sphinx.ext.todo', 'sphinx.ext.coverage',
    'sphinx.ext.pngmath', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'OpenQuake Engine'
copyright = u'2014, GEM Foundation'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = openquake.engine.__version__
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

autodoc_default_flags = ['members', 'undoc-members', 'show-inheritance']


# -- Options for HTML output --------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['.static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'OpenQuakedoc'


# -- Options for LaTeX output -------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass
# [howto/manual]).
latex_documents = [
  ('index', 'OpenQuake.tex', u'OpenQuake Documentation',
   u'GEM Foundation', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'openquake', u'OpenQuake Documentation',
     [u'GEM Foundation'], 1)]

########NEW FILE########
__FILENAME__ = oqscript
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

"""
OpenQuake: software for seismic hazard and risk assessment
"""

import argparse
import getpass
import os
import sys

from os.path import abspath
from os.path import dirname
from os.path import expanduser
from os.path import join

from django.core.exceptions import ObjectDoesNotExist

# just in the case that are you using oq-engine from sources
# with the rest of oq libraries installed into the system (or a
# virtual environment) you must set this environment variable
if os.environ.get("OQ_ENGINE_USE_SRCDIR") is not None:
    sys.modules['openquake'].__dict__["__path__"].insert(
        0, join(dirname(dirname(__file__)), "openquake")
    )

from openquake.engine.utils import config

config.abort_if_no_config_available()

try:
    import celeryconfig
except ImportError:
    sys.path.append('/usr/openquake/engine')

try:
    import oqpath
    oqpath.set_oq_path()
except ImportError:
    pass

from openquake.commonlib import source
import openquake.engine

from openquake.engine import __version__
from openquake.engine import engine
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from openquake.engine.export import risk as risk_export
from openquake.engine.tools.import_gmf_scenario import import_gmf_scenario
from openquake.engine.tools.import_hazard_curves import import_hazard_curves
from openquake.engine.tools import save_hazards, load_hazards

HAZARD_OUTPUT_ARG = "--hazard-output-id"
HAZARD_CALCULATION_ARG = "--hazard-calculation-id"
MISSING_HAZARD_MSG = ("Please specify the ID of the hazard output (or "
                      "calculation) to be used by using '%s (or %s) <id>'" %
                      (HAZARD_OUTPUT_ARG, HAZARD_CALCULATION_ARG))


def set_up_arg_parser():
    """Set up and return an :class:`argparse.ArgumentParser` with all of the
    OpenQuake command line options."""
    parser = argparse.ArgumentParser(
        description='OpenQuake Seismic Hazard and Risk Analysis Engine')

    general_grp = parser.add_argument_group('General')
    general_grp.add_argument(
        '--version', action='store_true', help='Display version information')
    general_grp.add_argument(
        '--log-file', '-L',
        help=('Location to store log messages; if not specified, log messages'
              ' will be printed to the console (to stderr)'),
        required=False, metavar='LOG_FILE')
    general_grp.add_argument(
        '--log-level', '-l',
        help='Defaults to "progress"', required=False,
        choices=['debug', 'info', 'progress', 'warn', 'error', 'critical'],
        default='progress')
    general_grp.add_argument(
        '--no-distribute', '--nd', action='store_true',
        help=('Disable calculation task distribution and run the computation'
              ' in a single process. This is intended for use in debugging '
              ' and profiling.')
    )
    general_grp.add_argument(
        '--list-inputs', '--li',
        help='List inputs of a specific input type',
        metavar="INPUT_TYPE")

    general_grp.add_argument(
        '--yes', '-y', action='store_true',
        help='Automatically answer "yes" when asked to confirm an action'
    )
    general_grp.add_argument(
        '--config-file',
        help='Custom openquake.cfg file, to override default configurations',
        metavar='CONFIG_FILE'
    )

    hazard_grp = parser.add_argument_group('Hazard')
    hazard_grp.add_argument(
        '--run-hazard',
        '--rh',
        help='Run a hazard job with the specified config file',
        metavar='CONFIG_FILE')
    hazard_grp.add_argument(
        '--list-hazard-calculations',
        '--lhc',
        help='List hazard calculation information',
        action='store_true')
    hazard_grp.add_argument(
        '--list-hazard-outputs',
        '--lho',
        help='List outputs for the specified hazard calculation',
        metavar='HAZARD_CALCULATION_ID')
    hazard_grp.add_argument(
        '--export-hazard',
        '--eh',
        help='Export the desired output to the specified directory',
        nargs=2, metavar=('OUTPUT_ID', 'TARGET_DIR'))
    hazard_grp.add_argument(
        '--export-hazard-outputs',
        '--eho',
        help='Export all the hazard outputs to the specified directory',
        nargs=2, metavar=('HAZARD_CALCULATION_ID', 'TARGET_DIR'))
    hazard_grp.add_argument(
        '--delete-hazard-calculation',
        '--dhc',
        help='Delete a hazard calculation and all associated outputs',
        metavar='HAZARD_CALCULATION_ID')

    hazard_grp.add_argument(
        '--delete-uncompleted-calculations',
        '--duc',
        action='store_true',
        help='Delete all the uncompleted calculations')

    risk_grp = parser.add_argument_group('Risk')
    risk_grp.add_argument(
        '--run-risk',
        '--rr',
        help='Run a risk job with the specified config file',
        metavar='CONFIG_FILE')
    risk_grp.add_argument(
        HAZARD_OUTPUT_ARG,
        '--ho',
        help='Use the desired hazard output as input for the risk job',
        metavar='HAZARD_OUTPUT')
    risk_grp.add_argument(
        HAZARD_CALCULATION_ARG,
        '--hc',
        help='Use the desired hazard calculation as input for the risk job',
        metavar='HAZARD_CALCULATION_ID')
    risk_grp.add_argument(
        '--list-risk-calculations',
        '--lrc',
        help='List risk calculation information',
        action='store_true')
    risk_grp.add_argument(
        '--list-risk-outputs',
        '--lro',
        help='List outputs for the specified risk calculation',
        metavar='RISK_CALCULATION_ID')
    risk_grp.add_argument(
        '--export-risk',
        '--er',
        help='Export the desired risk output to the specified directory',
        nargs=2,
        metavar=('OUTPUT_ID', 'TARGET_DIR'))
    risk_grp.add_argument(
        '--export-risk-outputs',
        '--ero',
        help='Export all the risk outputs to the specified directory',
        nargs=2, metavar=('RISK_CALCULATION_ID', 'TARGET_DIR'))
    risk_grp.add_argument(
        '--delete-risk-calculation',
        '--drc',
        help='Delete a risk calculation and all associated outputs',
        metavar='RISK_CALCULATION_ID')

    export_grp = parser.add_argument_group('Export')
    export_grp.add_argument(
        '--exports', choices=['xml'],  default=[], action="append",
        help=(
            'Use with --run-hazard and --run-risk to automatically export '
            'all calculation results to the specified format. Only "xml" is '
            'supported currently. This is optional. If not specified, '
            'nothing will be exported; results will only be stored in the '
            'database.'
        )
    )
    export_grp.add_argument(
        '--export-type', '--et',
        choices=['xml', 'geojson'],
        default='xml',
        action='store',
        help=('Use with --export-hazard or --export-risk, specify the '
              'desired output format. Defaults to "xml".')
    )

    save_load_grp = parser.add_argument_group('Save/Load')
    save_load_grp.add_argument(
        '--save-hazard-calculation', '--shc',
        help=('Save a hazard calculation to a new created directory.'),
        nargs=2, metavar=('HAZARD_CALCULATION_ID', 'DUMP_DIR'))
    save_load_grp.add_argument(
        '--load-hazard-calculation',
        help=("Load a hazard calculation from a saved import. "
              "Only SES outputs currently supported"),
        metavar=('DUMP_DIR'))

    import_grp = parser.add_argument_group('Import')
    import_grp.add_argument(
        '--load-gmf',
        help=('Load gmf from a file. Only single-source gmf are supported '
              'currently. The file can be xml or tab-separated.'),
        metavar='GMF_FILE',
    )
    import_grp.add_argument(
        '--load-curve',
        help=('Load hazard curves from an XML file.'),
        metavar='CURVE_FILE',
    )
    import_grp.add_argument(
        '--list-imported-outputs', action='store_true',
        help=('List outputs which were imported from a file, not calculated '
              'from a job'))

    return parser


def list_inputs(input_type):
    """
    Print a list of available input models
    """

    if input_type == "exposure":
        model = models.ExposureModel
    else:
        sys.exit("Wrong input type. Available input types: exposure")

    inputs = model.objects.all()

    if not inputs.count():
        print "No inputs found of type %s" % input_type
        return
    print ('model id | name')

    for inp in inputs:
        print "%9d|%s" % (inp.id, inp.name)


def list_calculations(calc_manager):
    """
    Print a summary of past calculations.

    :param calc_manager:

       a django manager (e.g.
       :class:`openquake.engine.db.models.RiskCalculation.objects`)
       which provides calculation instances
    """

    # FIXME(lp). As it might happen to have a calculation instance
    # without a OqJob instance (e.g. when the user imports outputs
    # directly from files) we filter out the calculation without the
    # corresponding job

    calcs = calc_manager.filter(
        oqjob__user_name=getpass.getuser(),
        oqjob__isnull=False).order_by('oqjob__last_update')

    if len(calcs) == 0:
        print 'None'
    else:
        print ('calc_id | status | last_update | '
               'description')
        for calc in calcs:
            latest_job = calc.oqjob
            if latest_job.is_running:
                status = 'pending'
            else:
                if latest_job.status == 'complete':
                    status = 'successful'
                else:
                    status = 'failed'
            last_update = latest_job.last_update.strftime(
                '%Y-%m-%d %H:%M:%S %Z'
            )

            print '%s | %s | %s | %s' % (
                calc.id, status, last_update, calc.description
            )


# TODO: the command-line switches are not tested, included this one
def list_imported_outputs():
    """
    List outputs which were imported from a file, not calculated from a job
    """
    outputs = models.Output.objects.filter(
        oq_job__hazard_calculation__description__contains=' importer, file ')
    engine.print_outputs_summary(outputs)


def export_hazard(haz_output_id, target_dir, export_type):
    export(hazard_export.export, haz_output_id, target_dir, export_type)


def export_hazard_outputs(hc_id, target_dir, export_type):
    for output in models.Output.objects.filter(
            oq_job__hazard_calculation=hc_id):
        print 'Exporting %s...' % output
        export(hazard_export.export, output.id, target_dir, export_type)

    
def export_risk(risk_output_id, target_dir, export_type):
    export(risk_export.export, risk_output_id, target_dir, export_type)


def export_risk_outputs(rc_id, target_dir, export_type):
    for output in models.Output.objects.filter(
            oq_job__risk_calculation=rc_id):
        print 'Exporting %s...' % output
        export(risk_export.export, output.id, target_dir, export_type)

    
def export(fn, output_id, target_dir, export_type):
    """
    Simple UI wrapper around
    :func:`openquake.engine.export.hazard.export` which prints a summary
    of files exported, if any.
    """
    queryset = models.Output.objects.filter(pk=output_id)
    if not queryset.exists():
        print 'No output found for OUTPUT_ID %s' % output_id
        return

    if queryset.all()[0].oq_job.status != "complete":
        print ("Exporting output produced by a job which did not run "
               "successfully. Results might be uncomplete")

    try:
        the_file = fn(output_id, target_dir, export_type)
        print 'File Exported:'
        print the_file
    except NotImplementedError, err:
        print err.message
        print 'This feature is probably not implemented yet'


def _touch_log_file(log_file):
    """If a log file destination is specified, attempt to open the file in
    'append' mode ('a'). If the specified file is not writable, an
    :exc:`IOError` will be raised."""
    open(abspath(log_file), 'a').close()


def delete_uncompleted_calculations():
    for rc in models.RiskCalculation.objects.filter(
            oqjob__user_name=getpass.getuser()).exclude(
            oqjob__status="successful"):
        del_risk_calc(rc.id, True)

    for hc in models.HazardCalculation.objects.filter(
            oqjob__user_name=getpass.getuser()).exclude(
            oqjob__status="successful"):
        del_haz_calc(hc.id, True)


def del_haz_calc(hc_id, confirmed=False):
    """
    Delete a hazard calculation and all associated outputs.
    """
    if confirmed or confirm(
            'Are you sure you want to delete this hazard calculation and all '
            'associated outputs?\nThis action cannot be undone. (y/n): '):
        try:
            engine.del_haz_calc(hc_id)
        except RuntimeError, err:
            print err.message


def del_risk_calc(rc_id, confirmed=False):
    """
    Delete a risk calculation and all associated outputs.
    """
    if confirmed or confirm(
            'Are you sure you want to delete this risk calculation and all '
            'associated outputs?\nThis action cannot be undone. (y/n): '):
        try:
            engine.del_risk_calc(rc_id)
        except RuntimeError, err:
            print err.message


def confirm(prompt):
    """
    Ask for confirmation, given a ``prompt`` and return a boolean value.
    """
    while True:
        try:
            answer = raw_input(prompt)
        except KeyboardInterrupt:
            # the user presses ctrl+c, just say 'no'
            return False

        answer = answer.strip().lower()

        if answer not in ('y', 'n'):
            print 'Please enter y or n'
            continue
        return answer == 'y'


def main():
    arg_parser = set_up_arg_parser()

    args = arg_parser.parse_args()

    if args.version:
        print __version__
        sys.exit(0)

    if args.config_file:
        os.environ[config.OQ_CONFIG_FILE_VAR] = \
            abspath(expanduser(args.config_file))
        config.refresh()

    if args.no_distribute:
        os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = '1'

    if args.list_inputs:
        list_inputs(args.list_inputs)
    # hazard
    elif args.list_hazard_calculations:
        list_calculations(models.HazardCalculation.objects)
    elif args.list_hazard_outputs is not None:
        engine.list_hazard_outputs(args.list_hazard_outputs)
    elif args.export_hazard is not None:
        output_id, target_dir = args.export_hazard
        output_id = int(output_id)
        export_hazard(output_id, expanduser(target_dir), args.export_type)
    elif args.export_hazard_outputs is not None:
        hc_id, target_dir = args.export_hazard_outputs
        export_hazard_outputs(int(hc_id), expanduser(target_dir),
                              args.export_type)
    elif args.run_hazard is not None:
        log_file = expanduser(args.log_file) \
            if args.log_file is not None else None
        engine.run_job(expanduser(args.run_hazard), args.log_level,
                       log_file, args.exports)
    elif args.delete_hazard_calculation is not None:
        del_haz_calc(args.delete_hazard_calculation, args.yes)
    # risk
    elif args.list_risk_calculations:
        list_calculations(models.RiskCalculation.objects)
    elif args.list_risk_outputs is not None:
        engine.list_risk_outputs(args.list_risk_outputs)
    elif args.export_risk is not None:
        output_id, target_dir = args.export_risk
        export_risk(output_id, expanduser(target_dir), args.export_type)
    elif args.export_risk_outputs is not None:
        rc_id, target_dir = args.export_risk_outputs
        export_risk_outputs(int(rc_id), expanduser(target_dir),
                              args.export_type)
    elif args.run_risk is not None:
        if (args.hazard_output_id is None
                and args.hazard_calculation_id is None):
            sys.exit(MISSING_HAZARD_MSG)
        log_file = expanduser(args.log_file) \
            if args.log_file is not None else None
        engine.run_job(expanduser(args.run_risk), args.log_level, log_file,
                       args.exports, hazard_output_id=args.hazard_output_id,
                       hazard_calculation_id=args.hazard_calculation_id)
    elif args.delete_risk_calculation is not None:
        del_risk_calc(args.delete_risk_calculation, args.yes)
    # import
    elif args.load_gmf is not None:
        with open(args.load_gmf) as f:
            out = import_gmf_scenario(f)
            print 'Added output id=%d of type %s; hazard_calculation_id=%d'\
                % (out.id, out.output_type, out.oq_job.hazard_calculation.id)
    elif args.load_curve is not None:
        with open(args.load_curve) as f:
            out = import_hazard_curves(f)
            print 'Added output id=%d of type %s; hazard_calculation_id=%d'\
                % (out.id, out.output_type, out.oq_job.hazard_calculation.id)
    elif args.list_imported_outputs:
        list_imported_outputs()
    elif args.delete_uncompleted_calculations:
        delete_uncompleted_calculations()
    elif args.save_hazard_calculation:
        save_hazards.main(*args.save_hazard_calculation)
    elif args.load_hazard_calculation:
        hc_ids = load_hazards.hazard_load(
            models.getcursor('admin').connection, args.load_hazard_calculation)
        print "Load hazard calculation with IDs: %s" % hc_ids
    else:
        arg_parser.print_usage()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = base
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""Base code for calculator classes."""

from openquake.engine import logs
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks

# Routing key format string for communication between tasks and the control
# node.
ROUTING_KEY_FMT = 'oq.job.%(job_id)s.tasks'


class Calculator(object):
    """
    Base class for all calculators.

    :param job: :class:`openquake.engine.db.models.OqJob` instance.
    """

    #: The core calculation Celery task function, which accepts the arguments
    #: generated by :func:`task_arg_gen`.
    core_calc_task = None

    def __init__(self, job):
        self.job = job
        self.num_tasks = None

    def monitor(self, operation):
        """
        Return a :class:`openquake.engine.performance.EnginePerformanceMonitor`
        instance, associated to the operation and with tracing and flushing
        enabled.

        :param str operation: the operation to monitor
        """
        return EnginePerformanceMonitor(
            operation, self.job.id, tracing=True, flush=True)

    def task_arg_gen(self):
        """
        Generator function for creating the arguments for each task.

        Subclasses must implement this.
        """
        raise NotImplementedError

    def concurrent_tasks(self):
        """
        Number of tasks to be in queue at any given time.

        Subclasses must implement this.
        """
        raise NotImplementedError

    def parallelize(self, task_func, task_arg_gen, task_completed):
        """
        Given a callable and a task arg generator, build an argument list and
        apply the callable to the arguments in parallel. The order is not
        preserved.

        Every time a task completes the method .task_completed() is called.

        :param task_func: a `celery` task callable
        :param task_args: an iterable over positional arguments

        NB: if the environment variable OQ_NO_DISTRIBUTE is set the
        tasks are run sequentially in the current process.
        """
        oqm = tasks.OqTaskManager(task_func, logs.LOG.progress)
        for args in task_arg_gen:
            oqm.submit(*args)
        oqm.aggregate_results(lambda acc, val: task_completed(val), None)

    def task_completed(self, task_result):
        """
        Method called when a task is completed. It can be overridden
        to aggregate the partial results of a computation.

        :param task_result: the result of the task
        """
        pass

    def pre_execute(self):
        """
        Override this method in subclasses to record pre-execution stats,
        initialize result records, perform detailed parsing of input data, etc.
        """

    @EnginePerformanceMonitor.monitor
    def execute(self):
        """
        Run the core_calc_task in parallel, by passing the arguments
        provided by the .task_arg_gen method. By default it uses the
        parallelize distribution, but it can be overridden is subclasses.
        """
        self.parallelize(self.core_calc_task,
                         self.task_arg_gen(),
                         self.task_completed)

    def post_execute(self):
        """
        Override this method in subclasses to any necessary post-execution
        actions, such as the consolidation of partial results.
        """

    def post_process(self):
        """
        Override this method in subclasses to perform post processing steps,
        such as computing mean results from a set of curves or plotting maps.
        """

    def _get_outputs_for_export(self):
        """
        Util function for getting :class:`openquake.engine.db.models.Output`
        objects to be exported.
        """
        raise NotImplementedError

    def _do_export(self, output_id, export_dir, export_type):
        """
        Perform a single export.
        """
        raise NotImplementedError()

    def export(self, *args, **kwargs):
        """
        If requested by the user, automatically export all result artifacts to
        the specified format. (NOTE: The only export format supported at the
        moment is NRML XML.

        :param exports:
            Keyword arg. List of export types.
        :returns:
            A list of the export filenames, including the absolute path to each
            file.
        """
        exported_files = []

        with logs.tracing('exports'):
            if 'exports' in kwargs:
                outputs = self._get_outputs_for_export()

                for export_type in kwargs['exports']:
                    for output in outputs:
                        with self.monitor('exporting %s to %s'
                                          % (output.output_type, export_type)):
                            fname = self._do_export(
                                output.id,
                                self.job.calculation.export_dir,
                                export_type
                            )
                            logs.LOG.info('exported %s', fname)
                            exported_files.append(fname)

        return exported_files

    def clean_up(self, *args, **kwargs):
        """Implement this method in subclasses to perform clean-up actions
           like garbage collection, etc."""

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core functionality for the classical PSHA hazard calculator.

The difficult part of this calculator is the management of the logic tree
realizations. Let me explain how it works in a real life case.

We want to perform a SHARE calculation with a single source
model with two tectonic region types (Stable Shallow Crust and
Active Shallow Crust) and a complex GMPEs logic tree with 7
branching points with the following structure:

Active_Shallow: 4 GMPEs, weights 0.35, 0.35, 0.20, 0.10
Stable_Shallow: 5 GMPEs, weights 0.2, 0.2, 0.2, 0.2, 0.2
Shield: 2 GMPEs, weights 0.5, 0.5
Subduction_Interface: 4 GMPEs, weights 0.2, 0.2, 0.2, 0.4
Subduction_InSlab: 4 GMPEs, weights 0.2, 0.2, 0.2, 0.4
Volcanic: 1 GMPE, weight 1
Deep: 2 GMPEs, weights 0.6, 0.4

The number of realizations generated by this logic tree is
4 * 5 * 2 * 4 * 4 * 1 * 2 = 1280 and at the end we will
generate 1280 hazard curve containers. However the independent
hazard curve containers are much less, there are actually only 9 of
them, 4 coming from the Active Shallow Crust tectonic region type
and 5 from the Stable Shallow Crust tectonic region type.
The dependent hazard curves (i.e. hazard curves by realization)
can be obtained from the independent hazard curves by a composition
rule implemented in :method:`openquake.engine.models.HazardCurve.build_data`

NB: notice that 1280 / 9 = 142.22, therefore storing all the hazard
curves takes 140+ times more disk space and resources than actually needed.
The plan for the future is store only the independent curves and to
compose them on-the-fly, when they are looked up for a given realization,
since the composition is pretty fast (there are just numpy multiplications),
faster than reading from the database all the redundant data. The
impact on hazard maps has to be determined.
"""
import time
import operator
import itertools

import numpy

from openquake.hazardlib.imt import from_string
from openquake.hazardlib.geo.utils import get_spherical_bounding_box
from openquake.hazardlib.geo.utils import get_longitudinal_extent
from openquake.hazardlib.geo.geodetic import npoints_between

from openquake.engine import logs, writer
from openquake.engine.calculators.hazard import general
from openquake.engine.calculators.hazard.classical import (
    post_processing as post_proc)
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine.performance import EnginePerformanceMonitor, LightMonitor


class BoundingBox(object):
    """
    A class to store the bounding box in distances, longitudes and magnitudes,
    given a source model and a site. This is used for disaggregation
    calculations. The goal is to determine the minimum and maximum
    distances of the ruptures generated from the model from the site;
    moreover the maximum and minimum longitudes and magnitudes are stored, by
    taking in account the international date line.
    """
    def __init__(self, lt_model_id, site_id):
        self.lt_model_id = lt_model_id
        self.site_id = site_id
        self.min_dist = self.max_dist = None
        self.east = self.west = self.south = self.north = None

    def update(self, dists, lons, lats):
        """
        Compare the current bounding box with the value in the arrays
        dists, lons, lats and enlarge it if needed.

        :param dists:
            a sequence of distances
        :param lons:
            a sequence of longitudes
        :param lats:
            a sequence of latitudes
        """
        if self.min_dist is not None:
            dists = [self.min_dist, self.max_dist] + dists
        if self.west is not None:
            lons = [self.west, self.east] + lons
        if self.south is not None:
            lats = [self.south, self.north] + lats
        self.min_dist, self.max_dist = min(dists), max(dists)
        self.west, self.east, self.north, self.south = \
            get_spherical_bounding_box(lons, lats)

    def update_bb(self, bb):
        """
        Compare the current bounding box with the given bounding box
        and enlarge it if needed.

        :param bb:
            an instance of :class:
            `openquake.engine.calculators.hazard.classical.core.BoundingBox`
        """
        if bb:  # the given bounding box must be non-empty
            self.update([bb.min_dist, bb.max_dist], [bb.west, bb.east],
                        [bb.south, bb.north])

    def bins_edges(self, dist_bin_width, coord_bin_width):
        """
        Define bin edges for disaggregation histograms, from the bin data
        collected from the ruptures.

        :param dists:
            array of distances from the ruptures
        :param lons:
            array of longitudes from the ruptures
        :param lats:
            array of latitudes from the ruptures
        :param dist_bin_width:
            distance_bin_width from job.ini
        :param coord_bin_width:
            coordinate_bin_width from job.ini
        """
        dist_edges = dist_bin_width * numpy.arange(
            int(self.min_dist / dist_bin_width),
            int(numpy.ceil(self.max_dist / dist_bin_width) + 1))

        west = numpy.floor(self.west / coord_bin_width) * coord_bin_width
        east = numpy.ceil(self.east / coord_bin_width) * coord_bin_width
        lon_extent = get_longitudinal_extent(west, east)

        lon_edges, _, _ = npoints_between(
            west, 0, 0, east, 0, 0,
            numpy.round(lon_extent / coord_bin_width) + 1)

        lat_edges = coord_bin_width * numpy.arange(
            int(numpy.floor(self.south / coord_bin_width)),
            int(numpy.ceil(self.north / coord_bin_width) + 1))

        return dist_edges, lon_edges, lat_edges

    def __nonzero__(self):
        """
        True if the bounding box is non empty.
        """
        return (self.min_dist is not None and self.west is not None
                and self.south is not None)


def _calc_pnes(gsim, r_sites, rupture, imts, imls, truncation_level,
               make_ctxt_mon, calc_poes_mon):
    # compute the probabilities of no exceedence for each IMT
    # for the given gsim and rupture; returns a list of pairs
    # [(imt, pnes), ...]
    with make_ctxt_mon:
        sctx, rctx, dctx = gsim.make_contexts(r_sites, rupture)
    with calc_poes_mon:
        for imt, levels in itertools.izip(imts, imls):
            poes = gsim.get_poes(
                sctx, rctx, dctx, imt, levels, truncation_level)
            pnes = rupture.get_probability_no_exceedance(poes)
            yield r_sites.expand(pnes, placeholder=1)


def all_equal(obj, value):
    """
    :param obj: a numpy array or something else
    :param value: a numeric value
    :returns: a boolean
    """
    if isinstance(obj, numpy.ndarray):
        return (obj == value).all()
    else:
        return obj == value


@tasks.oqtask
def compute_hazard_curves(
        job_id, sitecol, sources, trt_model_id, gsims, task_no):
    """
    This task computes R2 * I hazard curves (each one is a
    numpy array of S * L floats) from the given source_ruptures
    pairs.

    :param job_id:
        ID of the currently running job
    :param sitecol:
        a :class:`openquake.hazardlib.site.SiteCollection` instance
    :param sources:
        a block of source objects
    :param trt_model:
        a :class:`openquake.engine.db.TrtModel` instance
    :param gsims:
        a list of distint GSIM instances
    :param int task_no:
        the ordinal number of the current task
    """
    hc = models.HazardCalculation.objects.get(oqjob=job_id)
    total_sites = len(sitecol)
    sitemesh = sitecol.mesh
    sorted_imts = sorted(hc.intensity_measure_types_and_levels)
    sorted_imls = [hc.intensity_measure_types_and_levels[imt]
                   for imt in sorted_imts]
    sorted_imts = map(from_string, sorted_imts)
    curves = [[numpy.ones([total_sites, len(ls)]) for ls in sorted_imls]
              for gsim in gsims]
    if hc.poes_disagg:  # doing disaggregation
        lt_model_id = models.TrtModel.objects.get(pk=trt_model_id).lt_model.id
        bbs = [BoundingBox(lt_model_id, site_id) for site_id in sitecol.sids]
    else:
        bbs = []
    mon = LightMonitor(
        'getting ruptures', job_id, compute_hazard_curves)
    make_ctxt_mon = LightMonitor(
        'making contexts', job_id, compute_hazard_curves)
    calc_poes_mon = LightMonitor(
        'computing poes', job_id, compute_hazard_curves)

    # NB: rows are a namedtuples with fields (source, rupture, rupture_sites)
    for source, rows in itertools.groupby(
            hc.gen_ruptures(sources, mon, sitecol),
            key=operator.attrgetter('source')):
        t0 = time.time()
        num_ruptures = 0
        for _source, rupture, r_sites in rows:
            num_ruptures += 1
            if hc.poes_disagg:  # doing disaggregation
                jb_dists = rupture.surface.get_joyner_boore_distance(sitemesh)
                closest_points = rupture.surface.get_closest_points(sitemesh)
                for bb, dist, point in itertools.izip(
                        bbs, jb_dists, closest_points):
                    if dist < hc.maximum_distance:
                        # ruptures too far away are ignored
                        bb.update([dist], [point.longitude], [point.latitude])

            # compute probabilities for all realizations
            for gsim, curv in itertools.izip(gsims, curves):
                for i, pnes in enumerate(_calc_pnes(
                        gsim, r_sites, rupture, sorted_imts, sorted_imls,
                        hc.truncation_level, make_ctxt_mon, calc_poes_mon)):
                    curv[i] *= pnes

        logs.LOG.info('job=%d, src=%s:%s, num_ruptures=%d, calc_time=%fs',
                      job_id, source.source_id, source.__class__.__name__,
                      num_ruptures, time.time() - t0)

    make_ctxt_mon.flush()
    calc_poes_mon.flush()

    # the 0 here is a shortcut for filtered sources giving no contribution;
    # this is essential for performance, we want to avoid returning
    # big arrays of zeros (MS)
    cs = [[0 if all_equal(c, 1) else 1. - c for c in curv] for curv in curves]
    return zip(gsims, cs), trt_model_id, bbs


class ClassicalHazardCalculator(general.BaseHazardCalculator):
    """
    Classical PSHA hazard calculator. Computes hazard curves for a given set of
    points.

    For each realization of the calculation, we randomly sample source models
    and GMPEs (Ground Motion Prediction Equations) from logic trees.
    """

    core_calc_task = compute_hazard_curves

    def pre_execute(self):
        """
        Do pre-execution work. At the moment, this work entails:
        parsing and initializing sources, parsing and initializing the
        site model (if there is one), parsing vulnerability and
        exposure files and generating logic tree realizations. (The
        latter piece basically defines the work to be done in the
        `execute` phase.).
        """
        super(ClassicalHazardCalculator, self).pre_execute()
        self.imtls = self.hc.intensity_measure_types_and_levels
        realizations = self._get_realizations()
        n_rlz = len(realizations)
        n_levels = sum(len(lvls) for lvls in self.imtls.itervalues()
                       ) / float(len(self.imtls))
        n_sites = len(self.hc.site_collection)
        self.zero = numpy.array([numpy.zeros((n_sites, len(self.imtls[imt])))
                                 for imt in sorted(self.imtls)])
        total = n_rlz * len(self.imtls) * n_levels * n_sites
        logs.LOG.info('Considering %d realization(s), %d IMT(s), %d level(s) '
                      'and %d sites, total %d', n_rlz, len(self.imtls),
                      n_levels, n_sites, total)
        self.curves = {}  # {trt_model_id, gsim: curves_by_imt}

        # a dictionary with the bounding boxes for earch source
        # model and each site, defined only for disaggregation
        # calculations:
        if self.hc.poes_disagg:
            lt_models = models.LtSourceModel.objects.filter(
                hazard_calculation=self.hc)
            self.bb_dict = dict(
                ((lt_model.id, site.id), BoundingBox(lt_model.id, site.id))
                for site in self.hc.site_collection
                for lt_model in lt_models)

    @EnginePerformanceMonitor.monitor
    def task_completed(self, (result, trt_model_id, bbs)):
        """
        This is used to incrementally update hazard curve results by combining
        an initial value with some new results. (Each set of new results is
        computed over only a subset of seismic sources defined in the
        calculation model.)

        :param task_result:
            A dictionary rlz -> curves_by_imt where curves_by_imt is a
            list of 2-D numpy arrays representing the new results which need
            to be combined with the current value. These should be the same
            shape as self.curves[tr_model_id, gsim][j] where gsim is the
            GSIM name and j is the IMT ordinal.
        """
        for gsim_obj, probs in result:
            gsim = gsim_obj.__class__.__name__
            # probabilities of no exceedence per IMT
            pnes = numpy.array(
                [1 - (zero if all_equal(prob, 0) else prob)
                 for prob, zero in itertools.izip(probs, self.zero)])
            # TODO: add a test like Yufang computation testing the broadcast
            self.curves[trt_model_id, gsim] = 1 - (
                1 - self.curves.get((trt_model_id, gsim), self.zero)) * pnes

        if self.hc.poes_disagg:
            for bb in bbs:
                self.bb_dict[bb.lt_model_id, bb.site_id].update_bb(bb)

    # this could be parallelized in the future, however in all the cases
    # I have seen until now, the serialized approach is fast enough (MS)
    @EnginePerformanceMonitor.monitor
    def save_hazard_curves(self):
        """
        Post-execution actions. At the moment, all we do is finalize the hazard
        curve results.
        """
        imtls = self.hc.intensity_measure_types_and_levels
        points = self.hc.points_to_compute()

        for rlz in self._get_realizations():
            # create a new `HazardCurve` 'container' record for each
            # realization (virtual container for multiple imts)
            haz_curve_container = models.HazardCurve.objects.create(
                output=models.Output.objects.create_output(
                    self.job, "hc-multi-imt-rlz-%s" % rlz.id,
                    "hazard_curve_multi"),
                lt_realization=rlz,
                investigation_time=self.hc.investigation_time)

            with self.monitor('building curves per realization'):
                curves_by_imt = haz_curve_container.build_data(self.curves)

            # create a new `HazardCurve` 'container' record for each
            # realization for each intensity measure type
            for imt, curves in zip(sorted(imtls), curves_by_imt):
                hc_im_type, sa_period, sa_damping = from_string(imt)

                # save output
                hco = models.Output.objects.create(
                    oq_job=self.job,
                    display_name="Hazard Curve rlz-%s" % rlz.id,
                    output_type='hazard_curve',
                )

                # save hazard_curve
                haz_curve = models.HazardCurve.objects.create(
                    output=hco,
                    lt_realization=rlz,
                    investigation_time=self.hc.investigation_time,
                    imt=hc_im_type,
                    imls=imtls[imt],
                    sa_period=sa_period,
                    sa_damping=sa_damping,
                )

                # save hazard_curve_data
                logs.LOG.info('saving %d hazard curves for %s, imt=%s',
                              len(points), hco, imt)
                writer.CacheInserter.saveall([models.HazardCurveData(
                    hazard_curve=haz_curve,
                    poes=list(poes),
                    location='POINT(%s %s)' % (p.longitude, p.latitude),
                    weight=rlz.weight)
                    for p, poes in zip(points, curves)])

        self.curves = {}  # save memory for the post-processing phase

    def post_execute(self):
        """
        Generate the realizations and save the hazard curves
        """
        super(ClassicalHazardCalculator, self).post_execute()
        self.save_hazard_curves()

    def post_process(self):
        """
        Optionally generates aggregate curves, hazard maps and
        uniform_hazard_spectra.
        """
        logs.LOG.debug('> starting post processing')

        # means/quantiles:
        if self.hc.mean_hazard_curves or self.hc.quantile_hazard_curves:
            self.do_aggregate_post_proc()

        # hazard maps:
        # required for computing UHS
        # if `hazard_maps` is false but `uniform_hazard_spectra` is true,
        # just don't export the maps
        if self.hc.hazard_maps or self.hc.uniform_hazard_spectra:
            self.parallelize(
                post_proc.hazard_curves_to_hazard_map_task,
                post_proc.hazard_curves_to_hazard_map_task_arg_gen(self.job),
                lambda res: None)

        if self.hc.uniform_hazard_spectra:
            post_proc.do_uhs_post_proc(self.job)

        logs.LOG.debug('< done with post processing')

########NEW FILE########
__FILENAME__ = post_processing
# -*- coding: utf-8 -*-
# pylint: enable=W0511,W0142,I0011,E1101,E0611,F0401,E1103,R0801,W0232

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Post processing functionality for the classical PSHA hazard calculator.
E.g. mean and quantile curves.
"""

import numpy

from django.db import transaction
from itertools import izip

from openquake.engine import logs
from openquake.engine.calculators.hazard.general import CURVE_CACHE_SIZE
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine.writer import CacheInserter

# cutoff value for the poe
EPSILON = 1E-30

# Number of locations considered by each task
DEFAULT_LOCATIONS_PER_TASK = 1000


def compute_hazard_maps(curves, imls, poes):
    """
    Given a set of hazard curve poes, interpolate a hazard map at the specified
    ``poe``.

    :param curves:
        2D array of floats. Each row represents a curve, where the values
        in the row are the PoEs (Probabilities of Exceedance) corresponding to
        ``imls``. Each curve corresponds to a geographical location.
    :param imls:
        Intensity Measure Levels associated with these hazard ``curves``. Type
        should be an array-like of floats.
    :param float poes:
        Value(s) on which to interpolate a hazard map from the input
        ``curves``. Can be an array-like or scalar value (for a single PoE).

    :returns:
        A 2D numpy array of hazard map data. Each element/row in the resulting
        array represents the interpolated map for each ``poes`` value
        specified. If ``poes`` is just a single scalar value, the result array
        will have a length of 1.

        The results are structured this way so that it is easy to iterate over
        the hazard map results in a consistent way, no matter how many
        ``poes`` values are specified.
    """
    poes = numpy.array(poes)

    if len(poes.shape) == 0:
        # ``poes`` was passed in as a scalar;
        # convert it to 1D array of 1 element
        poes = poes.reshape(1)

    result = []
    imls = numpy.log(numpy.array(imls[::-1]))

    for curve in curves:
        # the hazard curve, having replaced the too small poes with EPSILON
        curve_cutoff = [max(poe, EPSILON) for poe in curve[::-1]]
        hmap_val = []
        for poe in poes:
            # special case when the interpolation poe is bigger than the
            # maximum, i.e the iml must be smaller than the minumum
            if poe > curve_cutoff[-1]:  # the greatest poes in the curve
                # extrapolate the iml to zero as per
                # https://bugs.launchpad.net/oq-engine/+bug/1292093
                # a consequence is that if all poes are zero any poe > 0
                # is big and the hmap goes automatically to zero
                hmap_val.append(0)
            else:
                # exp-log interpolation, to reduce numerical errors
                # see https://bugs.launchpad.net/oq-engine/+bug/1252770
                val = numpy.exp(
                    numpy.interp(
                        numpy.log(poe), numpy.log(curve_cutoff), imls))
                hmap_val.append(val)

        result.append(hmap_val)
    return numpy.array(result).transpose()


_HAZ_MAP_DISP_NAME_MEAN_FMT = 'Mean Hazard map(%(poe)s) %(imt)s'
_HAZ_MAP_DISP_NAME_QUANTILE_FMT = (
    '%(quantile)s Quantile Hazard Map(%(poe)s) %(imt)s')
# Hazard maps for a specific end branch
_HAZ_MAP_DISP_NAME_FMT = 'Hazard Map(%(poe)s) %(imt)s rlz-%(rlz)s'

_UHS_DISP_NAME_MEAN_FMT = 'Mean UHS (%(poe)s)'
_UHS_DISP_NAME_QUANTILE_FMT = '%(quantile)s Quantile UHS (%(poe)s)'
_UHS_DISP_NAME_FMT = 'UHS (%(poe)s) rlz-%(rlz)s'


# Silencing 'Too many local variables'
# pylint: disable=R0914
def hazard_curves_to_hazard_map(job_id, hazard_curve_id, poes):
    """
    Function to process a set of hazard curves into 1 hazard map for each PoE
    in ``poes``.

    Hazard map results are written directly to the database.

    :param int job_id:
        ID of the current :class:`openquake.engine.db.models.OqJob`.
    :param int hazard_curve_id:
        ID of a set of
        :class:`hazard curves <openquake.engine.db.models.HazardCurve>`.
    :param list poes:
        List of PoEs for which we want to iterpolate hazard maps.
    """
    job = models.OqJob.objects.get(id=job_id)
    hc = models.HazardCurve.objects.get(id=hazard_curve_id)

    hcd = models.HazardCurveData.objects.all_curves_simple(
        filter_args=dict(hazard_curve=hc.id), order_by='location'
    )
    hcd = list(hcd)

    imt = hc.imt
    if imt == 'SA':
        # if it's SA, include the period using the standard notation
        imt = 'SA(%s)' % hc.sa_period

    # Gather all of the curves and compute the maps, for all PoEs
    curves = (poes for _, _, poes in hcd)
    hazard_maps = compute_hazard_maps(curves, hc.imls, poes)

    # Prepare the maps to be saved to the DB
    for i, poe in enumerate(poes):
        map_values = hazard_maps[i]
        lons = numpy.empty(map_values.shape)
        lats = numpy.empty(map_values.shape)

        for loc_idx, _ in enumerate(map_values):
            lons[loc_idx] = hcd[loc_idx][0]
            lats[loc_idx] = hcd[loc_idx][1]

        # Create 'Output' records for the map for this PoE
        if hc.statistics == 'mean':
            disp_name = _HAZ_MAP_DISP_NAME_MEAN_FMT % dict(poe=poe, imt=imt)
        elif hc.statistics == 'quantile':
            disp_name = _HAZ_MAP_DISP_NAME_QUANTILE_FMT % dict(
                poe=poe, imt=imt, quantile=hc.quantile)
        else:
            disp_name = _HAZ_MAP_DISP_NAME_FMT % dict(
                poe=poe, imt=imt, rlz=hc.lt_realization.id)

        output = models.Output.objects.create_output(
            job, disp_name, 'hazard_map'
        )
        # Save the complete hazard map
        models.HazardMap.objects.create(
            output=output,
            lt_realization=hc.lt_realization,
            investigation_time=hc.investigation_time,
            imt=hc.imt,
            statistics=hc.statistics,
            quantile=hc.quantile,
            sa_period=hc.sa_period,
            sa_damping=hc.sa_damping,
            poe=poe,
            lons=lons.tolist(),
            lats=lats.tolist(),
            imls=map_values.tolist(),
        )

hazard_curves_to_hazard_map_task = tasks.oqtask(hazard_curves_to_hazard_map)


def hazard_curves_to_hazard_map_task_arg_gen(job):
    """
    Yield task arguments for processing hazard curves into hazard maps.

    :param job:
        A :class:`openquake.engine.db.models.OqJob` which has some hazard
        curves associated with it.
    """
    poes = job.hazard_calculation.poes

    hazard_curve_ids = models.HazardCurve.objects.filter(
        output__oq_job=job, imt__isnull=False).values_list('id', flat=True)
    logs.LOG.debug('num haz curves: %d', len(hazard_curve_ids))

    for hazard_curve_id in hazard_curve_ids:
        yield job.id, hazard_curve_id, poes


def do_uhs_post_proc(job):
    """
    Compute and save (to the DB) Uniform Hazard Spectra for all hazard maps for
    the given ``job``.

    :param job:
        Instance of :class:`openquake.engine.db.models.OqJob`.
    """
    hc = job.hazard_calculation

    rlzs = models.LtRealization.objects.filter(lt_model__hazard_calculation=hc)

    for poe in hc.poes:
        maps_for_poe = models.HazardMap.objects.filter(
            output__oq_job=job, poe=poe
        )

        # mean (if defined)
        mean_maps = maps_for_poe.filter(statistics='mean')
        if mean_maps.count() > 0:
            mean_uhs = make_uhs(mean_maps)
            _save_uhs(job, mean_uhs, poe, statistics='mean')

        # quantiles (if defined)
        for quantile in hc.quantile_hazard_curves:
            quantile_maps = maps_for_poe.filter(
                statistics='quantile', quantile=quantile
            )
            quantile_uhs = make_uhs(quantile_maps)
            _save_uhs(job, quantile_uhs, poe, statistics='quantile',
                      quantile=quantile)

        # for each logic tree branch:
        for rlz in rlzs:
            rlz_maps = maps_for_poe.filter(
                statistics=None, lt_realization=rlz
            )
            rlz_uhs = make_uhs(rlz_maps)
            _save_uhs(job, rlz_uhs, poe, rlz=rlz)


def make_uhs(maps):
    """
    Make Uniform Hazard Spectra curves for each location.

    It is assumed that the `lons` and `lats` for each of the ``maps`` are
    uniform.

    :param maps:
        A sequence of :class:`openquake.engine.db.models.HazardMap` objects, or
        equivalent objects with the same fields attributes.
    :returns:
        A `dict` with two values::
            * periods: a list of the SA periods from all of the ``maps``,
              sorted ascendingly
            * uh_spectra: a list of triples (lon, lat, imls), where `imls`
              is a `tuple` of the IMLs from all maps for each of the `periods`
    """
    result = dict()
    result['periods'] = []

    # filter out non-PGA -SA maps
    maps = [x for x in maps if x.imt in ('PGA', 'SA')]

    # give PGA maps an sa_period of 0.0
    # this is slightly hackish, but makes the sorting simple
    for each_map in maps:
        if each_map.imt == 'PGA':
            each_map.sa_period = 0.0

    # sort the maps by period:
    sorted_maps = sorted(maps, key=lambda m: m.sa_period)

    # start constructing the results:
    result['periods'] = [x.sa_period for x in sorted_maps]

    # assume the `lons` and `lats` are uniform for all maps
    lons = sorted_maps[0].lons
    lats = sorted_maps[0].lats

    result['uh_spectra'] = []
    imls_list = izip(*(x.imls for x in sorted_maps))
    for lon, lat, imls in izip(lons, lats, imls_list):
        result['uh_spectra'].append((lon, lat, imls))

    return result


def _save_uhs(job, uhs_results, poe, rlz=None, statistics=None, quantile=None):
    """
    Save computed UHS data to the DB.

    UHS results can be either for an end branch or for mean or quantile
    statistics.

    :param job:
        :class:`openquake.engine.db.models.OqJob` instance to be associated
        with the results.
    :param uhs_results:
        UHS computation results structured like the output of :func:`make_uhs`.
    :param float poe:
        Probability of exceedance of the hazard maps from which these UH
        Spectra were produced.
    :param rlz:
        :class:`openquake.engine.db.models.LtRealization`. Specify only if
        these results are for an end branch.
    :param statistics:
        'mean' or 'quantile'. Specify only if these are statistical results.
    :param float quantile:
        Specify only if ``statistics`` == 'quantile'.
    """
    output = models.Output(
        oq_job=job,
        output_type='uh_spectra'
    )
    uhs = models.UHS(
        poe=poe,
        investigation_time=job.hazard_calculation.investigation_time,
        periods=uhs_results['periods'],
    )
    if rlz is not None:
        uhs.lt_realization = rlz
        output.display_name = _UHS_DISP_NAME_FMT % dict(poe=poe, rlz=rlz.id)
    elif statistics is not None:
        uhs.statistics = statistics
        if statistics == 'quantile':
            uhs.quantile = quantile
            output.display_name = (_UHS_DISP_NAME_QUANTILE_FMT
                                   % dict(poe=poe, quantile=quantile))
        else:
            # mean
            output.display_name = _UHS_DISP_NAME_MEAN_FMT % dict(poe=poe)
    output.save()
    uhs.output = output
    # This should fail if neither `lt_realization` nor `statistics` is defined:
    uhs.save()

    with transaction.commit_on_success(using='job_init'):
        inserter = CacheInserter(models.UHSData, CURVE_CACHE_SIZE)
        for lon, lat, imls in uhs_results['uh_spectra']:
            inserter.add(
                models.UHSData(
                    uhs_id=uhs.id,
                    imls='{%s}' % ','.join(str(x) for x in imls),
                    location='POINT(%s %s)' % (lon, lat))
            )
        inserter.flush()

########NEW FILE########
__FILENAME__ = core
# -*- coding: utf-8 -*-
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Disaggregation calculator core functionality
"""

import sys
from collections import namedtuple
import numpy

from openquake.hazardlib.calc import disagg
from openquake.hazardlib.imt import from_string
from openquake.hazardlib.site import SiteCollection

from openquake.engine import logs
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine.performance import EnginePerformanceMonitor, LightMonitor
from openquake.engine.calculators.hazard.classical.core import \
    ClassicalHazardCalculator


# a 6-uple containing float 4 arrays mags, dists, lons, lats,
# 1 int array trts and a list of dictionaries pnes
BinData = namedtuple('BinData', 'mags, dists, lons, lats, trts, pnes')


def _collect_bins_data(mon, trt_num, source_ruptures, site, curves,
                       trt_model_id, gsims, imtls, poes, truncation_level,
                       n_epsilons):
    # returns a BinData instance
    sitecol = SiteCollection([site])
    mags = []
    dists = []
    lons = []
    lats = []
    trts = []
    pnes = []
    sitemesh = sitecol.mesh
    calc_dist = mon.copy('calc distances')
    make_ctxt = mon.copy('making contexts')
    disagg_poe = mon.copy('disaggregate_poe')
    trt_model = models.TrtModel.objects.get(pk=trt_model_id)
    rlzs = trt_model.get_rlzs_by_gsim()
    for source, ruptures in source_ruptures:
        try:
            tect_reg = trt_num[source.tectonic_region_type]
            for rupture in ruptures:
                # extract rupture parameters of interest
                mags.append(rupture.mag)
                with calc_dist:
                    [jb_dist] = rupture.surface.get_joyner_boore_distance(
                        sitemesh)
                    dists.append(jb_dist)
                    [closest_point] = rupture.surface.get_closest_points(
                        sitemesh)
                lons.append(closest_point.longitude)
                lats.append(closest_point.latitude)
                trts.append(tect_reg)

                pne_dict = {}
                # a dictionary rlz.id, poe, imt_str -> prob_no_exceed
                for gsim in gsims:
                    with make_ctxt:
                        sctx, rctx, dctx = gsim.make_contexts(sitecol, rupture)
                    for imt_str, imls in imtls.iteritems():
                        imt = from_string(imt_str)
                        imls = numpy.array(imls[::-1])
                        for rlz in rlzs[gsim.__class__.__name__]:
                            curve_poes = curves[rlz.id, imt_str].poes[::-1]
                            for poe in poes:
                                iml = numpy.interp(poe, curve_poes, imls)
                                # compute probability of exceeding iml given
                                # the current rupture and epsilon_bin, that is
                                # ``P(IMT >= iml | rup, epsilon_bin)``
                                # for each of the epsilon bins
                                with disagg_poe:
                                    [poes_given_rup_eps] = \
                                        gsim.disaggregate_poe(
                                            sctx, rctx, dctx, imt, iml,
                                            truncation_level, n_epsilons)
                                pne = rupture.get_probability_no_exceedance(
                                    poes_given_rup_eps)
                                pne_dict[rlz.id, poe, imt_str] = (iml, pne)

                pnes.append(pne_dict)
        except Exception as err:
            etype, err, tb = sys.exc_info()
            msg = 'An error occurred with source id=%s. Error: %s'
            msg %= (source.source_id, err.message)
            raise etype, msg, tb

    calc_dist.flush()
    make_ctxt.flush()
    disagg_poe.flush()

    return BinData(numpy.array(mags, float),
                   numpy.array(dists, float),
                   numpy.array(lons, float),
                   numpy.array(lats, float),
                   numpy.array(trts, int),
                   pnes)


_DISAGG_RES_NAME_FMT = 'disagg(%(poe)s)-rlz-%(rlz)s-%(imt)s-%(wkt)s'


def save_disagg_result(job_id, site_id, bin_edges, trt_names, matrix,
                       rlz_id, investigation_time, imt_str, iml, poe):
    """
    Save a computed disaggregation matrix to `hzrdr.disagg_result` (see
    :class:`~openquake.engine.db.models.DisaggResult`).

    :param int job_id:
        id of the current job.
    :param int site_id:
        id of the current site
    :param bin_edges:
        The 5-uple mag, dist, lon, lat, eps
    :param trt_names:
        The list of Tectonic Region Types
    :param matrix:
        A probability array
    :param rlz:
        :class:`openquake.engine.db.models.LtRealization` to which these
        results belong.
    :param float investigation_time:
        Investigation time (years) for the calculation.
    :param imt_str:
        Intensity measure type (PGA, SA, etc.)
    :param float iml:
        Intensity measure level interpolated (using ``poe``) from the hazard
        curve at the ``site``.
    :param float poe:
        Disaggregation probability of exceedance value for this result.
    """
    job = models.OqJob.objects.get(id=job_id)

    site_wkt = models.HazardSite.objects.get(pk=site_id).location.wkt

    disp_name = _DISAGG_RES_NAME_FMT % dict(
        poe=poe, rlz=rlz_id, imt=imt_str, wkt=site_wkt)

    output = models.Output.objects.create_output(
        job, disp_name, 'disagg_matrix')

    imt, sa_period, sa_damping = from_string(imt_str)
    mag, dist, lon, lat, eps = bin_edges
    models.DisaggResult.objects.create(
        output=output,
        lt_realization_id=rlz_id,
        investigation_time=investigation_time,
        imt=imt,
        sa_period=sa_period,
        sa_damping=sa_damping,
        iml=iml,
        poe=poe,
        mag_bin_edges=mag,
        dist_bin_edges=dist,
        lon_bin_edges=lon,
        lat_bin_edges=lat,
        eps_bin_edges=eps,
        trts=trt_names,
        location=site_wkt,
        matrix=matrix,
    )


@tasks.oqtask
def compute_disagg(job_id, sitecol, sources, trt_model_id, gsims,
                   trt_num, curves_dict, bin_edges):
    # see https://bugs.launchpad.net/oq-engine/+bug/1279247 for an explanation
    # of the algorithm used
    """
    :param int job_id:
        ID of the currently running :class:`openquake.engine.db.models.OqJob`
    :param sitecol:
        a :class:`openquake.hazardlib.site.SiteCollection` instance
    :param list sources:
        list of hazardlib source objects
    :param lt_model:
        an instance of :class:`openquake.engine.db.models.LtSourceModel`
    :param dict gsims:
        a list of distinct GSIM instances
    :param dict trt_num:
        a dictionary Tectonic Region Type -> incremental number
    :param curves_dict:
        a dictionary with the hazard curves for sites, realizations and IMTs
    :param bin_egdes:
        a dictionary (lt_model_id, site_id) -> edges
    :returns:
        a dictionary of probability arrays, with composite key
        (site.id, rlz.id, poe, imt, iml, trt_names).
    """
    mon = LightMonitor('disagg', job_id, compute_disagg)
    hc = models.OqJob.objects.get(id=job_id).hazard_calculation
    trt_model = models.TrtModel.objects.get(pk=trt_model_id)
    lt_model_id = trt_model.lt_model.id
    rlzs = trt_model.get_rlzs_by_gsim()
    trt_names = tuple(trt_model.lt_model.get_tectonic_region_types())
    result = {}  # site.id, rlz.id, poe, imt, iml, trt_names -> array

    for site in sitecol:
        # edges as wanted by disagg._arrange_data_in_bins
        try:
            edges = bin_edges[lt_model_id, site.id]
        except KeyError:
            # bin_edges for a given site are missing if the site is far away
            continue

        # generate source, rupture, sites once per site
        source_ruptures = list(hc.gen_ruptures_for_site(site, sources, mon))
        if not source_ruptures:
            continue
        logs.LOG.info('Collecting bins from %d ruptures close to %s',
                      sum(len(rupts) for src, rupts in source_ruptures),
                      site.location)

        with EnginePerformanceMonitor(
                'collecting bins', job_id, compute_disagg):
            bdata = _collect_bins_data(
                mon, trt_num, source_ruptures, site, curves_dict[site.id],
                trt_model_id, gsims, hc.intensity_measure_types_and_levels,
                hc.poes_disagg, hc.truncation_level,
                hc.num_epsilon_bins)

        if not bdata.pnes:  # no contributions for this site
            continue

        for poe in hc.poes_disagg:
            for imt in hc.intensity_measure_types_and_levels:
                for gsim in gsims:
                    for rlz in rlzs[gsim.__class__.__name__]:
                        # extract the probabilities of non-exceedance for the
                        # given realization, disaggregation PoE, and IMT
                        iml_pne_pairs = [pne[rlz.id, poe, imt]
                                         for pne in bdata.pnes]
                        iml = iml_pne_pairs[0][0]
                        probs = numpy.array(
                            [p for (i, p) in iml_pne_pairs], float)
                        # bins in a format handy for hazardlib
                        bins = [bdata.mags, bdata.dists,
                                bdata.lons, bdata.lats,
                                bdata.trts, None, probs]

                        # call disagg._arrange_data_in_bins
                        with EnginePerformanceMonitor(
                                'arranging bins', job_id, compute_disagg):
                            key = (site.id, rlz.id, poe, imt, iml, trt_names)
                            matrix = disagg._arrange_data_in_bins(
                                bins, edges + (trt_names,))
                            result[key] = numpy.array(
                                [fn(matrix) for fn in disagg.pmf_map.values()])

    return result


class DisaggHazardCalculator(ClassicalHazardCalculator):
    """
    A calculator which performs disaggregation calculations in a distributed /
    parallelized fashion.

    See :func:`openquake.hazardlib.calc.disagg.disaggregation` for more
    details about the nature of this type of calculation.
    """
    def get_curves(self, site):
        """
        Get all the relevant hazard curves for the given site.
        Returns a dictionary {(rlz_id, imt) -> curve}.
        """
        dic = {}
        wkt = site.location.wkt2d
        for rlz in self._get_realizations():
            for imt_str in self.hc.intensity_measure_types_and_levels:
                imt = from_string(imt_str)
                [curve] = models.HazardCurveData.objects.filter(
                    location=wkt,
                    hazard_curve__lt_realization=rlz,
                    hazard_curve__imt=imt[0],
                    hazard_curve__sa_period=imt[1],
                    hazard_curve__sa_damping=imt[2])
                if all(x == 0.0 for x in curve.poes):
                    logs.LOG.warn(
                        '* hazard curve %d contains all zero '
                        'probabilities; skipping SRID=4326;%s, rlz=%d, IMT=%s',
                        curve.id, wkt, rlz.id, imt_str)
                    continue
                dic[rlz.id, imt_str] = curve
        return dic

    @EnginePerformanceMonitor.monitor
    def full_disaggregation(self):
        """
        Run the disaggregation phase after hazard curve finalization.
        """
        hc = self.hc
        tl = self.hc.truncation_level
        mag_bin_width = self.hc.mag_bin_width
        eps_edges = numpy.linspace(-tl, tl, self.hc.num_epsilon_bins + 1)
        logs.LOG.info('%d epsilon bins from %s to %s', len(eps_edges) - 1,
                      min(eps_edges), max(eps_edges))

        self.bin_edges = {}
        curves_dict = dict((site.id, self.get_curves(site))
                           for site in self.hc.site_collection)

        oqm = tasks.OqTaskManager(compute_disagg, logs.LOG.progress)
        for job_id, sitecol, srcs, trt_model_id, gsims, task_no in \
                self.task_arg_gen():

            lt_model = models.TrtModel.objects.get(pk=trt_model_id).lt_model
            trt_num = dict((trt, i) for i, trt in enumerate(
                           lt_model.get_tectonic_region_types()))
            infos = list(models.TrtModel.objects.filter(
                         lt_model=lt_model))

            max_mag = max(i.max_mag for i in infos)
            min_mag = min(i.min_mag for i in infos)
            mag_edges = mag_bin_width * numpy.arange(
                int(numpy.floor(min_mag / mag_bin_width)),
                int(numpy.ceil(max_mag / mag_bin_width) + 1))
            logs.LOG.info('%d mag bins from %s to %s', len(mag_edges) - 1,
                          min_mag, max_mag)

            for site in self.hc.site_collection:
                curves = curves_dict[site.id]
                if not curves:
                    continue  # skip zero-valued hazard curves
                bb = self.bb_dict[lt_model.id, site.id]
                if not bb:
                    logs.LOG.info(
                        'location %s was too far, skipping disaggregation',
                        site.location)
                    continue

                dist_edges, lon_edges, lat_edges = bb.bins_edges(
                    hc.distance_bin_width, hc.coordinate_bin_width)
                logs.LOG.info(
                    '%d dist bins from %s to %s', len(dist_edges) - 1,
                    min(dist_edges), max(dist_edges))
                logs.LOG.info('%d lon bins from %s to %s', len(lon_edges) - 1,
                              bb.west, bb.east)
                logs.LOG.info('%d lat bins from %s to %s', len(lon_edges) - 1,
                              bb.south, bb.north)

                self.bin_edges[lt_model.id, site.id] = (
                    mag_edges, dist_edges, lon_edges, lat_edges, eps_edges)

            oqm.submit(self.job.id, sitecol, srcs, trt_model_id,
                       gsims, trt_num, curves_dict, self.bin_edges)

        res = oqm.aggregate_results(self.agg_result, {})
        self.save_disagg_results(res)  # dictionary key -> probability array

    def post_execute(self):
        super(DisaggHazardCalculator, self).post_execute()
        self.full_disaggregation()

    def agg_result(self, acc, result):
        """
        Collect the results coming from compute_disagg into self.results,
        a dictionary with key (site.id, rlz.id, poe, imt, iml, trt_names)
        and values which are probability arrays.

        :param acc: dictionary accumulating the results
        :param result: dictionary with the result coming from a task
        """
        for key, val in result.iteritems():
            acc[key] = 1. - (1. - acc.get(key, 0)) * (1. - val)
        return acc

    @EnginePerformanceMonitor.monitor
    def save_disagg_results(self, results):
        """
        Save all the results of the disaggregation. NB: the number of results
        to save is #sites * #rlzs * #disagg_poes * #IMTs.

        :param results:
            a dictionary of probability arrays
        """
        # since an extremely small subset of the full disaggregation matrix
        # is saved this method can be run sequentially on the controller node
        for key, probs in results.iteritems():
            site_id, rlz_id, poe, imt, iml, trt_names = key
            lt_model = models.LtRealization.objects.get(pk=rlz_id).lt_model
            edges = self.bin_edges[lt_model.id, site_id]
            save_disagg_result(
                self.job.id, site_id, edges, trt_names, probs,
                rlz_id, self.hc.investigation_time, imt, iml, poe)

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core calculator functionality for computing stochastic event sets and ground
motion fields using the 'event-based' method.

Stochastic events sets (which can be thought of as collections of ruptures) are
computed iven a set of seismic sources and investigation time span (in years).

For more information on computing stochastic event sets, see
:mod:`openquake.hazardlib.calc.stochastic`.

One can optionally compute a ground motion field (GMF) given a rupture, a site
collection (which is a collection of geographical points with associated soil
parameters), and a ground shaking intensity model (GSIM).

For more information on computing ground motion fields, see
:mod:`openquake.hazardlib.calc.gmf`.
"""

import time
import random
import collections

import numpy.random

from django.db import transaction
from openquake.hazardlib.calc import gmf, filters
from openquake.hazardlib.imt import from_string

from openquake.engine import logs, writer
from openquake.engine.calculators.hazard import general
from openquake.engine.calculators.hazard.classical import (
    post_processing as cls_post_proc)
from openquake.engine.calculators.hazard.event_based import post_processing
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine.performance import EnginePerformanceMonitor, LightMonitor

# NB: beware of large caches
inserter = writer.CacheInserter(models.GmfData, 1000)


@tasks.oqtask
def compute_ruptures(
        job_id, sitecol, src_seeds, trt_model_id, gsims, task_no):
    """
    Celery task for the stochastic event set calculator.

    Samples logic trees and calls the stochastic event set calculator.

    Once stochastic event sets are calculated, results will be saved to the
    database. See :class:`openquake.engine.db.models.SESCollection`.

    Optionally (specified in the job configuration using the
    `ground_motion_fields` parameter), GMFs can be computed from each rupture
    in each stochastic event set. GMFs are also saved to the database.

    :param int job_id:
        ID of the currently running job.
    :param sitecol:
        a :class:`openquake.hazardlib.site.SiteCollection` instance
    :param src_seeds:
        List of pairs (source, seed)
    :params gsims:
        list of distinct GSIM instances
    :param task_no:
        an ordinal so that GMV can be collected in a reproducible order
    """
    # NB: all realizations in gsims correspond to the same source model
    trt_model = models.TrtModel.objects.get(pk=trt_model_id)
    ses_coll = models.SESCollection.objects.get(lt_model=trt_model.lt_model)

    hc = models.HazardCalculation.objects.get(oqjob=job_id)
    all_ses = list(ses_coll)
    imts = map(from_string, hc.intensity_measure_types)
    params = dict(
        correl_model=general.get_correl_model(hc),
        truncation_level=hc.truncation_level,
        maximum_distance=hc.maximum_distance)

    rupturecollector = RuptureCollector(
        params, imts, gsims, trt_model.id, task_no)

    filter_sites_mon = LightMonitor(
        'filtering sites', job_id, compute_ruptures)
    generate_ruptures_mon = LightMonitor(
        'generating ruptures', job_id, compute_ruptures)
    filter_ruptures_mon = LightMonitor(
        'filtering ruptures', job_id, compute_ruptures)
    save_ruptures_mon = LightMonitor(
        'saving ruptures', job_id, compute_ruptures)

    # Compute and save stochastic event sets
    rnd = random.Random()
    num_distinct_ruptures = 0
    total_ruptures = 0

    for src, seed in src_seeds:
        t0 = time.time()
        rnd.seed(seed)

        with filter_sites_mon:  # filtering sources
            s_sites = src.filter_sites_by_distance_to_source(
                hc.maximum_distance, sitecol
            ) if hc.maximum_distance else sitecol
            if s_sites is None:
                continue

        # the dictionary `ses_num_occ` contains [(ses, num_occurrences)]
        # for each occurring rupture for each ses in the ses collection
        ses_num_occ = collections.defaultdict(list)
        with generate_ruptures_mon:  # generating ruptures for the given source
            for rup_no, rup in enumerate(src.iter_ruptures(), 1):
                rup.rup_no = rup_no
                for ses in all_ses:
                    numpy.random.seed(rnd.randint(0, models.MAX_SINT_32))
                    num_occurrences = rup.sample_number_of_occurrences()
                    if num_occurrences:
                        ses_num_occ[rup].append((ses, num_occurrences))
                        total_ruptures += num_occurrences

        # NB: the number of occurrences is very low, << 1, so it is
        # more efficient to filter only the ruptures that occur, i.e.
        # to call sample_number_of_occurrences() *before* the filtering
        for rup in ses_num_occ.keys():
            with filter_ruptures_mon:  # filtering ruptures
                r_sites = filters.filter_sites_by_distance_to_rupture(
                    rup, hc.maximum_distance, s_sites
                    ) if hc.maximum_distance else s_sites
                if r_sites is None:
                    # ignore ruptures which are far away
                    del ses_num_occ[rup]  # save memory
                    continue

            # saving ses_ruptures
            ses_ruptures = []
            with save_ruptures_mon:
                # using a django transaction make the saving faster
                with transaction.commit_on_success(using='job_init'):
                    indices = r_sites.indices if len(r_sites) < len(sitecol) \
                        else None  # None means that nothing was filtered
                    prob_rup = models.ProbabilisticRupture.create(
                        rup, ses_coll, indices)
                    for ses, num_occurrences in ses_num_occ[rup]:
                        for occ_no in range(1, num_occurrences + 1):
                            rup_seed = rnd.randint(0, models.MAX_SINT_32)
                            ses_rup = models.SESRupture.create(
                                prob_rup, ses, src.source_id,
                                rup.rup_no, occ_no, rup_seed)
                            ses_ruptures.append(ses_rup)

            # collecting ses_ruptures
            for ses_rup in ses_ruptures:
                rupturecollector.trts.add(src.tectonic_region_type)
                rupturecollector.rupture_data.append(
                    (r_sites, rup, ses_rup.id, ses_rup.seed))

        # log calc_time per distinct rupture
        if ses_num_occ:
            num_ruptures = len(ses_num_occ)
            tot_ruptures = sum(num for rup in ses_num_occ
                               for ses, num in ses_num_occ[rup])
            logs.LOG.info(
                'job=%d, src=%s:%s, num_ruptures=%d, tot_ruptures=%d, '
                'num_sites=%d, calc_time=%fs', job_id, src.source_id,
                src.__class__.__name__, num_ruptures, tot_ruptures,
                len(s_sites), time.time() - t0)
            num_distinct_ruptures += num_ruptures

    if num_distinct_ruptures:
        logs.LOG.info('job=%d, task %d generated %d/%d ruptures',
                      job_id, task_no, num_distinct_ruptures, total_ruptures)
    filter_sites_mon.flush()
    generate_ruptures_mon.flush()
    filter_ruptures_mon.flush()
    save_ruptures_mon.flush()

    return rupturecollector


@tasks.oqtask
def compute_and_save_gmfs(job_id, rupt_collector):
    """
    :param int job_id:
        ID of the currently running job
    :param rupt_collector:
        an instance of `openquake.engine.calculators.hazard.event_based.core.RuptureCollector`
    """
    with EnginePerformanceMonitor(
            'computing gmfs', job_id, compute_and_save_gmfs):
        for rupture_data in rupt_collector.rupture_data:
            rupt_collector.calc_gmf(*rupture_data)
    with EnginePerformanceMonitor(
            'saving gmfs', job_id, compute_and_save_gmfs):
        rupt_collector.save_gmfs()


class RuptureCollector(object):
    """
    A class to store ruptures and then compute and save ground motion fields.
    """
    def __init__(self, params, imts, gsims, trt_model_id, task_no):
        """
        :param params:
            a dictionary of parameters with keys
            correl_model, truncation_level, maximum_distance
        :param imts:
            a list of hazardlib intensity measure types
        :param gsims:
            a list of distinct GSIM instances
        :param int trt_model_id:
            the ID of a TRTModel instance
        """
        self.params = params
        self.imts = imts
        self.gsims = gsims
        self.trt_model_id = trt_model_id
        self.task_no = task_no
        # NB: I tried to use a single dictionary
        # {site_id: [(gmv, rupt_id),...]} but it took a lot more memory (MS)
        self.gmvs_per_site = collections.defaultdict(list)
        self.ruptures_per_site = collections.defaultdict(list)
        self.trts = set()
        self.rupture_data = []

    def calc_gmf(self, r_sites, rupture, rupture_id, rupture_seed):
        """
        Compute the GMF generated by the given rupture on the given
        sites and collect the values in the dictionaries
        .gmvs_per_site and .ruptures_per_site.

        :param r_sites:
            the collection of sites affected by the rupture
        :param rupture:
            an `openquake.hazardlib.source.rupture.
                ParametricProbabilisticRupture` instance
        :param id:
            the id of an `openquake.engine.db.models.SESRupture` instance
        :param seed:
            an integer to be used as stochastic seed
        """
        for gsim in self.gsims:
            gsim_name = gsim.__class__.__name__
            computer = gmf.GmfComputer(rupture, r_sites, self.imts, gsim,
                                       self.params['truncation_level'],
                                       self.params['correl_model'])
            gmf_dict = computer.compute(rupture_seed)
            for imt, gmvs in gmf_dict.iteritems():
                for site_id, gmv in zip(r_sites.sids, gmvs):
                    # convert a 1x1 matrix into a float
                    gmv = float(gmv)
                    if gmv:
                        self.gmvs_per_site[
                            gsim_name, imt, site_id].append(gmv)
                        self.ruptures_per_site[
                            gsim_name, imt, site_id].append(rupture_id)

    def save_gmfs(self):
        """
        Helper method to save the computed GMF data to the database.
        """
        rlzs = models.TrtModel.objects.get(
            pk=self.trt_model_id).get_rlzs_by_gsim()
        for gsim_name, imt, site_id in self.gmvs_per_site:
            if not rlzs[gsim_name]:
                logs.LOG.warn('No realizations for TrtModel=%d, GSIM=%s',
                              self.trt_model_id, gsim_name)
            for rlz in rlzs[gsim_name]:
                imt_name, sa_period, sa_damping = imt
                inserter.add(models.GmfData(
                    gmf=models.Gmf.objects.get(lt_realization=rlz),
                    task_no=self.task_no,
                    imt=imt_name,
                    sa_period=sa_period,
                    sa_damping=sa_damping,
                    site_id=site_id,
                    gmvs=self.gmvs_per_site[gsim_name, imt, site_id],
                    rupture_ids=self.ruptures_per_site[gsim_name, imt, site_id]
                ))
        inserter.flush()
        self.rupture_data[:] = []
        self.gmvs_per_site.clear()
        self.ruptures_per_site.clear()


class EventBasedHazardCalculator(general.BaseHazardCalculator):
    """
    Probabilistic Event-Based hazard calculator. Computes stochastic event sets
    and (optionally) ground motion fields.
    """
    core_calc_task = compute_ruptures

    def task_arg_gen(self, _block_size=None):
        """
        Loop through realizations and sources to generate a sequence of
        task arg tuples. Each tuple of args applies to a single task.
        Yielded results are tuples of the form job_id, sources, ses, seeds
        (seeds will be used to seed numpy for temporal occurence sampling).
        """
        hc = self.hc
        rnd = random.Random()
        rnd.seed(hc.random_seed)
        for job_id, sitecol, block, lt_model, gsims, task_no in \
                super(EventBasedHazardCalculator, self).task_arg_gen():
            ss = [(src, rnd.randint(0, models.MAX_SINT_32))
                  for src in block]  # source, seed pairs
            yield job_id, sitecol, ss, lt_model, gsims, task_no

        # now the source_blocks_per_ltpath dictionary can be cleared
        self.source_blocks_per_ltpath.clear()

    def task_completed(self, rupturecollector):
        """
        :param rupt_collector:
            an instance of `openquake.engine.calculators.hazard.event_based.core.RuptureCollector`

        If the parameter `ground_motion_fields` is set, compute and save
        the GMFs from the ruptures generated by the given task and stored
        in the `rupturecollector`.
        """
        if not self.hc.ground_motion_fields:
            return  # do nothing
        self.rupt_collectors.append(rupturecollector)
        self.num_ruptures[rupturecollector.trt_model_id] += \
            len(rupturecollector.rupture_data)

    @EnginePerformanceMonitor.monitor
    def post_execute(self):
        for trt_id, num_ruptures in self.num_ruptures.iteritems():
            trt = models.TrtModel.objects.get(pk=trt_id)
            trt.num_ruptures = num_ruptures
            trt.save()
        super(EventBasedHazardCalculator, self).post_execute()
        if not self.hc.ground_motion_fields:
            return  # do nothing

        # create a Gmf output for each realization
        for rlz in self._get_realizations():
            output = models.Output.objects.create(
                oq_job=self.job,
                display_name='GMF rlz-%s' % rlz.id,
                output_type='gmf')
            models.Gmf.objects.create(output=output, lt_realization=rlz)
        otm = tasks.OqTaskManager(compute_and_save_gmfs, logs.LOG.progress)
        for rupt_collector in self.rupt_collectors:
            otm.submit(self.job.id, rupt_collector)
        otm.aggregate_results(lambda acc, x: None, None)

    def initialize_ses_db_records(self, lt_model):
        """
        Create :class:`~openquake.engine.db.models.Output`,
        :class:`~openquake.engine.db.models.SESCollection` and
        :class:`~openquake.engine.db.models.SES` "container" records for
        a single realization.

        Stochastic event set ruptures computed for this realization will be
        associated to these containers.

        NOTE: Many tasks can contribute ruptures to the same SES.
        """
        output = models.Output.objects.create(
            oq_job=self.job,
            display_name='SES Collection smlt-%d' % lt_model.ordinal,
            output_type='ses')

        ses_coll = models.SESCollection.objects.create(
            output=output, lt_model=lt_model, ordinal=lt_model.ordinal)

        return ses_coll

    def pre_execute(self):
        """
        Do pre-execution work. At the moment, this work entails:
        parsing and initializing sources, parsing and initializing the
        site model (if there is one), parsing vulnerability and
        exposure files, and generating logic tree realizations. (The
        latter piece basically defines the work to be done in the
        `execute` phase.)
        """
        super(EventBasedHazardCalculator, self).pre_execute()
        for lt_model in models.LtSourceModel.objects.filter(
                hazard_calculation=self.hc):
            self.initialize_ses_db_records(lt_model)

    def post_process(self):
        """
        If requested, perform additional processing of GMFs to produce hazard
        curves.
        """
        if self.hc.hazard_curves_from_gmfs:
            with self.monitor('generating hazard curves'):
                self.parallelize(
                    post_processing.gmf_to_hazard_curve_task,
                    post_processing.gmf_to_hazard_curve_arg_gen(self.job),
                    lambda res: None)

            # If `mean_hazard_curves` is True and/or `quantile_hazard_curves`
            # has some value (not an empty list), do this additional
            # post-processing.
            if self.hc.mean_hazard_curves or self.hc.quantile_hazard_curves:
                with self.monitor('generating mean/quantile curves'):
                    self.do_aggregate_post_proc()

            if self.hc.hazard_maps:
                with self.monitor('generating hazard maps'):
                    self.parallelize(
                        cls_post_proc.hazard_curves_to_hazard_map_task,
                        cls_post_proc.hazard_curves_to_hazard_map_task_arg_gen(
                            self.job),
                        lambda res: None)

########NEW FILE########
__FILENAME__ = post_processing
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
GMFs to Hazard Curves

For each IMT, logic tree path, and point of interest, the number of GMF records
will be equal to the `ses_per_logic_tree_path`. The data contained in these
records can include ground motion values from many ruptures, stored in
variable length arrays; the quantity is random.

For post-processing, we will need to perform P * R * M queries to the database,
where P is the number of points in a given calculation, R is the total number
of tree paths, and M is the number of intensity measure levels defined for the
hazard curve processing. Each of these queries will give us all of the data we
need to compute a single hazard curve.

Typical values for P can go from 1 to a few 100,000s. *
Typical values for R can from 1 few 1,000s. *
Typical values for M are about 10.

* Considering maximum for both P and R is an extreme case.

P * R * M = 100,000 * 1,000 * 10 = 1 Billion queries, in the extreme case

This could be the target for future optimizations.
"""

import itertools
import numpy

from openquake.hazardlib.imt import from_string

from openquake.engine.db import models
from openquake.engine.utils import tasks


HAZ_CURVE_DISP_NAME_FMT = 'hazard-curve-rlz-%(rlz)s-%(imt)s'


def gmf_to_hazard_curve_arg_gen(job):
    """
    Generate a sequence of args for the GMF to hazard curve post-processing job
    for a given ``job``. These are task args.

    Yielded arguments are as follows:

    * job ID
    * point geometry
    * logic tree realization ID
    * IMT
    * IMLs
    * hazard curve "collection" ID
    * investigation time
    * duration
    * SA period
    * SA damping

    See :func:`gmf_to_hazard_curve_task` for more information about these
    arguments.

    As a side effect, :class:`openquake.engine.db.models.HazardCurve`
    records are
    created for each :class:`openquake.engine.db.models.LtRealization` and IMT.

    :param job:
        :class:`openquake.engine.db.models.OqJob` instance.
    """
    hc = job.hazard_calculation
    sites = models.HazardSite.objects.filter(hazard_calculation=hc)

    lt_realizations = models.LtRealization.objects.filter(
        lt_model__hazard_calculation=hc.id)

    invest_time = hc.investigation_time
    duration = hc.ses_per_logic_tree_path * invest_time

    for raw_imt, imls in hc.intensity_measure_types_and_levels.iteritems():
        imt, sa_period, sa_damping = from_string(raw_imt)

        for lt_rlz in lt_realizations:
            hc_output = models.Output.objects.create_output(
                job,
                HAZ_CURVE_DISP_NAME_FMT % dict(imt=raw_imt, rlz=lt_rlz.id),
                'hazard_curve')

            # Create the hazard curve "collection":
            hc_coll = models.HazardCurve.objects.create(
                output=hc_output,
                lt_realization=lt_rlz,
                investigation_time=invest_time,
                imt=imt,
                imls=imls,
                sa_period=sa_period,
                sa_damping=sa_damping)

            for site in sites:
                yield (job.id, site, lt_rlz.id, imt, imls, hc_coll.id,
                       invest_time, duration, sa_period, sa_damping)


# Disabling "Unused argument 'job_id'" (this parameter is required by @oqtask):
# pylint: disable=W0613
@tasks.oqtask
def gmf_to_hazard_curve_task(job_id, site, lt_rlz_id, imt, imls, hc_coll_id,
                             invest_time, duration, sa_period=None,
                             sa_damping=None):
    """
    For a given job, site, realization, and IMT, compute a hazard curve and
    save it to the database. The hazard curve will be computed from all
    available ground motion data for the specified site and realization.

    :param int job_id:
        ID of a currently running :class:`openquake.engine.db.models.OqJob`.
    :param site:
        A :class:`openquake.engine.db.models.HazardSite` instance.
    :param int lt_rlz_id:
        ID of a :class:`openquake.engine.db.models.LtRealization` for the
        current calculation.
    :param str imt:
        Intensity Measure Type (PGA, SA, PGV, etc.)
    :param imls:
        List of Intensity Measure Levels. These will serve as the abscissae for
        the computed hazard curve.
    :param int hc_coll_id:
        ID of a :class:`openquake.engine.db.models.HazardCurve`, which will be
        the 'container' for the computed hazard curve.
    :param float invest_time:
        Investigation time, in years. It is with this time span that we compute
        probabilities of exceedance.

        Another way to put it is the following. When computing a hazard curve,
        we want to answer the question: What is the probability of ground
        motion meeting or exceeding the specified levels (``imls``) in a given
        time span (``invest_time``).
    :param float duration:
        Time window during which GMFs occur. Another was to say it is, the
        period of time over which we simulate ground motion occurrences.

        NOTE: Duration is computed as the calculation investigation time
        multiplied by the number of stochastic event sets.
    :param float sa_period:
        Spectral Acceleration period. Used only with ``imt`` of 'SA'.
    :param float sa_damping:
        Spectral Acceleration damping. Used only with ``imt`` of 'SA'.
    """
    lt_rlz = models.LtRealization.objects.get(id=lt_rlz_id)
    gmfs = models.GmfData.objects.filter(
        gmf__lt_realization=lt_rlz_id,
        imt=imt,
        sa_period=sa_period,
        sa_damping=sa_damping,
        site=site)
    gmvs = list(itertools.chain(*(g.gmvs for g in gmfs)))

    # Compute the hazard curve PoEs:
    hc_poes = gmvs_to_haz_curve(gmvs, imls, invest_time, duration)
    # Save:
    models.HazardCurveData.objects.create(
        hazard_curve_id=hc_coll_id, poes=hc_poes, location=site.location,
        weight=lt_rlz.weight)


def gmvs_to_haz_curve(gmvs, imls, invest_time, duration):
    """
    Given a set of ground motion values (``gmvs``) and intensity measure levels
    (``imls``), compute hazard curve probabilities of exceedance.

    :param gmvs:
        A list of ground motion values, as floats.
    :param imls:
        A list of intensity measure levels, as floats.
    :param float invest_time:
        Investigation time, in years. It is with this time span that we compute
        probabilities of exceedance.

        Another way to put it is the following. When computing a hazard curve,
        we want to answer the question: What is the probability of ground
        motion meeting or exceeding the specified levels (``imls``) in a given
        time span (``invest_time``).
    :param float duration:
        Time window during which GMFs occur. Another was to say it is, the
        period of time over which we simulate ground motion occurrences.

        NOTE: Duration is computed as the calculation investigation time
        multiplied by the number of stochastic event sets.

    :returns:
        Numpy array of PoEs (probabilities of exceedence).
    """
    gmvs = numpy.array(gmvs)
    # convert to numpy arrary and redimension so that it can be broadcast with
    # the gmvs for computing PoE values
    imls = numpy.array(imls).reshape((len(imls), 1))

    num_exceeding = numpy.sum(gmvs >= imls, axis=1)

    poes = 1 - numpy.exp(- (invest_time / duration) * num_exceeding)

    return poes

########NEW FILE########
__FILENAME__ = general
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""Common code for the hazard calculators."""

import os
import collections

from openquake.hazardlib import correlation
from openquake.hazardlib.imt import from_string

# FIXME: one must import the engine before django to set DJANGO_SETTINGS_MODULE
from openquake.engine.db import models
from django.db import transaction

from openquake.nrmllib import parsers as nrml_parsers
from openquake.nrmllib.risk import parsers

from openquake.commonlib import logictree, source
from openquake.commonlib.general import block_splitter, ceil, distinct

from openquake.engine.input import exposure
from openquake.engine import logs
from openquake.engine import writer
from openquake.engine.calculators import base
from openquake.engine.calculators.post_processing import mean_curve
from openquake.engine.calculators.post_processing import quantile_curve
from openquake.engine.calculators.post_processing import (
    weighted_quantile_curve
)
from openquake.engine.export import core as export_core
from openquake.engine.export import hazard as hazard_export
from openquake.engine.utils import config
from openquake.engine.performance import EnginePerformanceMonitor

#: Maximum number of hazard curves to cache, for selects or inserts
CURVE_CACHE_SIZE = 100000

QUANTILE_PARAM_NAME = "QUANTILE_LEVELS"
POES_PARAM_NAME = "POES"
# Dilation in decimal degrees (http://en.wikipedia.org/wiki/Decimal_degrees)
# 1e-5 represents the approximate distance of one meter at the equator.
DILATION_ONE_METER = 1e-5


def make_gsim_lt(hc, trts):
    """
    Helper to instantiate a GsimLogicTree object from the logic tree file.

    :param hc: `openquake.engine.db.models.HazardCalculation` instance
    :param trts: list of tectonic region type strings
    """
    fname = os.path.join(hc.base_path, hc.inputs['gsim_logic_tree'])
    return logictree.GsimLogicTree(
        fname, 'applyToTectonicRegionType', trts,
        hc.number_of_logic_tree_samples, hc.random_seed)


def store_site_model(job, site_model_source):
    """Invoke site model parser and save the site-specified parameter data to
    the database.

    :param job:
        The job that is loading this site_model_source
    :param site_model_source:
        Filename or file-like object containing the site model XML data.
    :returns:
        `list` of ids of the newly-inserted `hzrdi.site_model` records.
    """
    parser = nrml_parsers.SiteModelParser(site_model_source)
    data = [models.SiteModel(vs30=node.vs30,
                             vs30_type=node.vs30_type,
                             z1pt0=node.z1pt0,
                             z2pt5=node.z2pt5,
                             location=node.wkt,
                             job_id=job.id)
            for node in parser.parse()]
    return writer.CacheInserter.saveall(data)


def get_correl_model(hc):
    """
    Helper function for constructing the appropriate correlation model.

    :param hc:
        A :class:`openquake.engine.db.models.HazardCalculation` instance.

    :returns:
        A correlation object. See :mod:`openquake.hazardlib.correlation` for
        more info.
    """
    correl_model_cls = getattr(
        correlation,
        '%sCorrelationModel' % hc.ground_motion_correlation_model,
        None)
    if correl_model_cls is None:
        # There's no correlation model for this calculation.
        return None

    return correl_model_cls(**hc.ground_motion_correlation_params)


class BaseHazardCalculator(base.Calculator):
    """
    Abstract base class for hazard calculators. Contains a bunch of common
    functionality, like initialization procedures.
    """

    def __init__(self, job):
        super(BaseHazardCalculator, self).__init__(job)
        # a dictionary (sm_lt_path, trt) -> source blocks
        self.source_blocks_per_ltpath = collections.defaultdict(list)
        self.rupt_collectors = []
        self.num_ruptures = collections.defaultdict(float)

    def clean_up(self, *args, **kwargs):
        """Clean up dictionaries at the end"""
        self.source_blocks_per_ltpath.clear()

    @property
    def hc(self):
        """
        A shorter and more convenient way of accessing the
        :class:`~openquake.engine.db.models.HazardCalculation`.
        """
        return self.job.hazard_calculation

    def concurrent_tasks(self):
        """
        For hazard calculators, the number of tasks to be in queue
        at any given time is specified in the configuration file.
        """
        return int(config.get('hazard', 'concurrent_tasks'))

    def task_arg_gen(self):
        """
        Loop through realizations and sources to generate a sequence of
        task arg tuples. Each tuple of args applies to a single task.

        For this default implementation, yielded results are quartets
        (job_id, sources, tom, gsim_by_rlz).

        Override this in subclasses as necessary.
        """
        sitecol = self.hc.site_collection
        trt_models = models.TrtModel.objects.filter(
            lt_model__hazard_calculation=self.hc)
        for task_no, trt_model in enumerate(trt_models):
            ltpath = tuple(trt_model.lt_model.sm_lt_path)
            trt = trt_model.tectonic_region_type
            gsims = [logictree.GSIM[gsim]() for gsim in trt_model.gsims]
            for block in self.source_blocks_per_ltpath[ltpath, trt]:
                yield (self.job.id, sitecol, block, trt_model.id,
                       gsims, task_no)

    def _get_realizations(self):
        """
        Get all of the logic tree realizations for this calculation.
        """
        return models.LtRealization.objects\
            .filter(lt_model__hazard_calculation=self.hc).order_by('id')

    def pre_execute(self):
        """
        Initialize risk models, site model and sources
        """
        self.parse_risk_models()
        self.initialize_site_model()
        lt_models = self.initialize_sources()
        js = models.JobStats.objects.get(oq_job=self.job)
        js.num_sources = [model.get_num_sources() for model in lt_models]
        js.save()

    def post_execute(self):
        """Inizialize realizations, except for the scenario calculator"""
        if self.hc.calculation_mode != 'scenario':
            self.initialize_realizations()

    @EnginePerformanceMonitor.monitor
    def initialize_sources(self):
        """
        Parse source models and validate source logic trees. It also
        filters the sources far away and apply uncertainties to the
        relevant ones. As a side effect it populates the instance dictionary
        `.source_blocks_per_ltpath`. Notice that sources are automatically
        split.

        :returns:
            a list with the number of sources for each source model
        """
        logs.LOG.progress("initializing sources")
        self.source_model_lt = logictree.SourceModelLogicTree.from_hc(self.hc)
        sm_paths = distinct(self.source_model_lt)
        nblocks = ceil(config.get('hazard', 'concurrent_tasks'), len(sm_paths))
        lt_models = []
        for i, (sm, weight, smpath) in enumerate(sm_paths):
            fname = os.path.join(self.hc.base_path, sm)
            source_collector = source.parse_source_model_smart(
                fname,
                self.hc.sites_affected_by,
                self.source_model_lt.make_apply_uncertainties(smpath),
                self.hc)
            if not source_collector.source_weights:
                raise RuntimeError(
                    'Could not find sources close to the sites in %s '
                    '(maximum_distance=%s km)' %
                    (fname, self.hc.maximum_distance))

            self.source_model_lt.tectonic_region_types.update(
                source_collector.source_weights)
            lt_model = models.LtSourceModel.objects.create(
                hazard_calculation=self.hc, sm_lt_path=smpath, ordinal=i,
                sm_name=sm, weight=weight)
            lt_models.append(lt_model)
            for trt, blocks in source_collector.split_blocks(nblocks):
                self.source_blocks_per_ltpath[smpath, trt] = blocks
                n = sum(len(block) for block in blocks)
                logs.LOG.info('Found %d relevant source(s) for %s %s, TRT=%s',
                              n, sm, smpath, trt)
                logs.LOG.info('Splitting in %d blocks', len(blocks))
                for i, block in enumerate(blocks, 1):
                    logs.LOG.debug('%s, block %d: %d source(s), weight %s',
                                   trt, i, len(block), block.weight)

            # save TrtModel objects for each tectonic region type
            trts = source_collector.sorted_trts()
            gsims_by_trt = make_gsim_lt(self.hc, trts).values
            for trt in trts:
                models.TrtModel.objects.create(
                    lt_model=lt_model,
                    tectonic_region_type=trt,
                    num_sources=len(source_collector.source_weights[trt]),
                    num_ruptures=source_collector.num_ruptures[trt],
                    min_mag=source_collector.min_mag[trt],
                    max_mag=source_collector.max_mag[trt],
                    gsims=gsims_by_trt[trt])
        return lt_models

    @EnginePerformanceMonitor.monitor
    def parse_risk_models(self):
        """
        If any risk model is given in the hazard calculation, the
        computation will be driven by risk data. In this case the
        locations will be extracted from the exposure file (if there
        is one) and the imt (and levels) will be extracted from the
        vulnerability model (if there is one)
        """
        hc = self.hc
        if hc.vulnerability_models:
            logs.LOG.progress("parsing risk models")

            hc.intensity_measure_types_and_levels = dict()
            hc.intensity_measure_types = list()

            for vf in hc.vulnerability_models:
                intensity_measure_types_and_levels = dict(
                    (record['IMT'], record['IML']) for record in
                    parsers.VulnerabilityModelParser(vf))

                for imt, levels in \
                        intensity_measure_types_and_levels.items():
                    if (imt in hc.intensity_measure_types_and_levels and
                        (set(hc.intensity_measure_types_and_levels[imt]) -
                         set(levels))):
                        logs.LOG.warning(
                            "The same IMT %s is associated with "
                            "different levels" % imt)
                    else:
                        hc.intensity_measure_types_and_levels[imt] = levels

                hc.intensity_measure_types.extend(
                    intensity_measure_types_and_levels)

            # remove possible duplicates
            if hc.intensity_measure_types is not None:
                hc.intensity_measure_types = list(set(
                    hc.intensity_measure_types))
            hc.save()
            logs.LOG.info("Got IMT and levels "
                          "from vulnerability models: %s - %s" % (
                              hc.intensity_measure_types_and_levels,
                              hc.intensity_measure_types))

        if 'fragility' in hc.inputs:
            hc.intensity_measure_types_and_levels = dict()
            hc.intensity_measure_types = list()

            parser = iter(parsers.FragilityModelParser(
                hc.inputs['fragility']))
            hc = self.hc

            fragility_format, _limit_states = parser.next()

            if (fragility_format == "continuous" and
                    hc.calculation_mode != "scenario"):
                raise NotImplementedError(
                    "Getting IMT and levels from "
                    "a continuous fragility model is not yet supported")

            hc.intensity_measure_types_and_levels = dict(
                (iml['IMT'], iml['imls'])
                for _taxonomy, iml, _params, _no_damage_limit in parser)
            hc.intensity_measure_types.extend(
                hc.intensity_measure_types_and_levels)
            hc.save()

        if 'exposure' in hc.inputs:
            with logs.tracing('storing exposure'):
                exposure.ExposureDBWriter(
                    self.job).serialize(
                    parsers.ExposureModelParser(hc.inputs['exposure']))

    @EnginePerformanceMonitor.monitor
    def initialize_site_model(self):
        """
        Populate the hazard site table.

        If a site model is specified in the calculation configuration,
        parse it and load it into the `hzrdi.site_model` table.
        """
        logs.LOG.progress("initializing sites")
        self.hc.points_to_compute(save_sites=True)

        site_model_inp = self.hc.site_model
        if site_model_inp:
            store_site_model(self.job, site_model_inp)

    def initialize_realizations(self):
        """
        Create records for the `hzrdr.lt_realization`.

        This function works either in random sampling mode (when lt_realization
        models get the random seed value) or in enumeration mode (when weight
        values are populated). In both cases we record the logic tree paths
        for both trees in the `lt_realization` record, as well as ordinal
        number of the realization (zero-based).
        """
        logs.LOG.progress("initializing realizations")
        if self.hc.number_of_logic_tree_samples:  # sampling
            gsim_lt = iter(make_gsim_lt(
                self.hc, self.source_model_lt.tectonic_region_types))
            # build 1 gsim realization for each source model realization

            def make_rlzs(lt_model):
                return [gsim_lt.next()]
        else:  # full enumeration
            def make_rlzs(lt_model):
                return list(
                    make_gsim_lt(
                        self.hc, lt_model.get_tectonic_region_types()))

        for idx, (sm, weight, sm_lt_path) in enumerate(self.source_model_lt):
            lt_model = models.LtSourceModel.objects.get(
                hazard_calculation=self.hc, sm_lt_path=sm_lt_path)
            self._initialize_realizations(idx, lt_model, make_rlzs(lt_model))

    @transaction.commit_on_success(using='job_init')
    def _initialize_realizations(self, idx, lt_model, realizations):
        # create the realizations for the given lt source model
        trt_models = lt_model.trtmodel_set.filter(num_ruptures__gt=0)
        if not trt_models:
            return
        rlz_ordinal = idx * len(realizations)
        for gsim_by_trt, weight, lt_path in realizations:
            if lt_model.weight is not None and weight is not None:
                weight = lt_model.weight * weight
            else:
                weight = None
            rlz = models.LtRealization.objects.create(
                lt_model=lt_model, gsim_lt_path=lt_path,
                weight=weight, ordinal=rlz_ordinal)
            rlz_ordinal += 1
            for trt_model in trt_models:
                # populate the association table rlz <-> trt_model
                models.AssocLtRlzTrtModel.objects.create(
                    rlz=rlz, trt_model=trt_model,
                    gsim=gsim_by_trt[trt_model.tectonic_region_type])

    def _get_outputs_for_export(self):
        """
        Util function for getting :class:`openquake.engine.db.models.Output`
        objects to be exported.

        Gathers all outputs for the job, but filters out `hazard_curve_multi`
        outputs if this option was turned off in the calculation profile.
        """
        outputs = export_core.get_outputs(self.job.id)
        if not self.hc.export_multi_curves:
            outputs = outputs.exclude(output_type='hazard_curve_multi')
        return outputs

    def _do_export(self, output_id, export_dir, export_type):
        """
        Hazard-specific implementation of
        :meth:`openquake.engine.calculators.base.Calculator._do_export`.

        Calls the hazard exporter.
        """
        return hazard_export.export(output_id, export_dir, export_type)

    @EnginePerformanceMonitor.monitor
    def do_aggregate_post_proc(self):
        """
        Grab hazard data for all realizations and sites from the database and
        compute mean and/or quantile aggregates (depending on which options are
        enabled in the calculation).

        Post-processing results will be stored directly into the database.
        """
        num_rlzs = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=self.hc).count()

        num_site_blocks_per_incr = int(CURVE_CACHE_SIZE) / int(num_rlzs)
        if num_site_blocks_per_incr == 0:
            # This means we have `num_rlzs` >= `CURVE_CACHE_SIZE`.
            # The minimum number of sites should be 1.
            num_site_blocks_per_incr = 1
        slice_incr = num_site_blocks_per_incr * num_rlzs  # unit: num records

        if self.hc.mean_hazard_curves:
            # create a new `HazardCurve` 'container' record for mean
            # curves (virtual container for multiple imts)
            models.HazardCurve.objects.create(
                output=models.Output.objects.create_output(
                    self.job, "mean-curves-multi-imt",
                    "hazard_curve_multi"),
                statistics="mean",
                imt=None,
                investigation_time=self.hc.investigation_time)

        if self.hc.quantile_hazard_curves:
            for quantile in self.hc.quantile_hazard_curves:
                # create a new `HazardCurve` 'container' record for quantile
                # curves (virtual container for multiple imts)
                models.HazardCurve.objects.create(
                    output=models.Output.objects.create_output(
                        self.job, 'quantile(%s)-curves' % quantile,
                        "hazard_curve_multi"),
                    statistics="quantile",
                    imt=None,
                    quantile=quantile,
                    investigation_time=self.hc.investigation_time)

        for imt, imls in self.hc.intensity_measure_types_and_levels.items():
            im_type, sa_period, sa_damping = from_string(imt)

            # prepare `output` and `hazard_curve` containers in the DB:
            container_ids = dict()
            if self.hc.mean_hazard_curves:
                mean_output = models.Output.objects.create_output(
                    job=self.job,
                    display_name='Mean Hazard Curves %s' % imt,
                    output_type='hazard_curve'
                )
                mean_hc = models.HazardCurve.objects.create(
                    output=mean_output,
                    investigation_time=self.hc.investigation_time,
                    imt=im_type,
                    imls=imls,
                    sa_period=sa_period,
                    sa_damping=sa_damping,
                    statistics='mean'
                )
                container_ids['mean'] = mean_hc.id

            if self.hc.quantile_hazard_curves:
                for quantile in self.hc.quantile_hazard_curves:
                    q_output = models.Output.objects.create_output(
                        job=self.job,
                        display_name=(
                            '%s quantile Hazard Curves %s' % (quantile, imt)
                        ),
                        output_type='hazard_curve'
                    )
                    q_hc = models.HazardCurve.objects.create(
                        output=q_output,
                        investigation_time=self.hc.investigation_time,
                        imt=im_type,
                        imls=imls,
                        sa_period=sa_period,
                        sa_damping=sa_damping,
                        statistics='quantile',
                        quantile=quantile
                    )
                    container_ids['q%s' % quantile] = q_hc.id

            all_curves_for_imt = models.order_by_location(
                models.HazardCurveData.objects.all_curves_for_imt(
                    self.job.id, im_type, sa_period, sa_damping))

            with transaction.commit_on_success(using='job_init'):
                inserter = writer.CacheInserter(
                    models.HazardCurveData, CURVE_CACHE_SIZE)

                for chunk in models.queryset_iter(all_curves_for_imt,
                                                  slice_incr):
                    # slice each chunk by `num_rlzs` into `site_chunk`
                    # and compute the aggregate
                    for site_chunk in block_splitter(chunk, num_rlzs):
                        site = site_chunk[0].location
                        curves_poes = [x.poes for x in site_chunk]
                        curves_weights = [x.weight for x in site_chunk]

                        # do means and quantiles
                        # quantiles first:
                        if self.hc.quantile_hazard_curves:
                            for quantile in self.hc.quantile_hazard_curves:
                                if self.hc.number_of_logic_tree_samples == 0:
                                    # explicitly weighted quantiles
                                    q_curve = weighted_quantile_curve(
                                        curves_poes, curves_weights, quantile
                                    )
                                else:
                                    # implicitly weighted quantiles
                                    q_curve = quantile_curve(
                                        curves_poes, quantile
                                    )
                                inserter.add(
                                    models.HazardCurveData(
                                        hazard_curve_id=(
                                            container_ids['q%s' % quantile]),
                                        poes=q_curve.tolist(),
                                        location=site.wkt)
                                )

                        # then means
                        if self.hc.mean_hazard_curves:
                            m_curve = mean_curve(
                                curves_poes, weights=curves_weights
                            )
                            inserter.add(
                                models.HazardCurveData(
                                    hazard_curve_id=container_ids['mean'],
                                    poes=m_curve.tolist(),
                                    location=site.wkt)
                            )
                inserter.flush()

########NEW FILE########
__FILENAME__ = core
# -*- coding: utf-8 -*-
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Scenario calculator core functionality
"""
import collections
import random
import numpy

from openquake.nrmllib.hazard.parsers import RuptureModelParser

# HAZARDLIB
from openquake.hazardlib.calc import filters
from openquake.hazardlib.calc.gmf import GmfComputer
from openquake.hazardlib.imt import from_string
import openquake.hazardlib.gsim

from openquake.commonlib.general import SequenceSplitter, distinct
from openquake.commonlib import source

from openquake.engine.calculators.hazard import general as haz_general
from openquake.engine.utils import tasks
from openquake.engine.db import models
from openquake.engine import writer
from openquake.engine.performance import EnginePerformanceMonitor

AVAILABLE_GSIMS = openquake.hazardlib.gsim.get_available_gsims()


@tasks.oqtask
def gmfs(job_id, ses_ruptures, sitecol, gmf_id, task_no):
    """
    :param int job_id: the current job ID
    :param ses_ruptures: a set of `SESRupture` instances
    :param sitecol: a `SiteCollection` instance
    :param int gmf_id: the ID of a `Gmf` instance
    :param int task_no: the task number
    """
    hc = models.HazardCalculation.objects.get(oqjob=job_id)
    # distinct is here to make sure that IMTs such as
    # SA(0.8) and SA(0.80) are considered the same
    imts = distinct(from_string(x) for x in hc.intensity_measure_types)
    gsim = AVAILABLE_GSIMS[hc.gsim]()  # instantiate the GSIM class
    realizations = 1  # one realization for each seed
    correlation_model = haz_general.get_correl_model(hc)

    cache = collections.defaultdict(list)  # {site_id, imt -> gmvs}
    inserter = writer.CacheInserter(models.GmfData, 1000)
    # insert GmfData in blocks of 1000 sites

    # NB: ses_ruptures a non-empty list produced by the block_splitter
    rupture = ses_ruptures[0].rupture  # ProbabilisticRupture instance
    with EnginePerformanceMonitor('computing gmfs', job_id, gmfs):
        gmf = GmfComputer(rupture, sitecol, imts, gsim, hc.truncation_level,
                          correlation_model)
        for ses_rup in ses_ruptures:
            gmf_dict = gmf.compute(ses_rup.seed)
            for imt in imts:
                for site_id, gmv in zip(sitecol.sids, gmf_dict[imt]):
                    # float may be needed below to convert 1x1 matrices
                    cache[site_id, imt].append((float(gmv), ses_rup.id))

    with EnginePerformanceMonitor('saving gmfs', job_id, gmfs):
        for (site_id, imt), data in cache.iteritems():
            gmvs, rup_ids = zip(*data)
            inserter.add(
                models.GmfData(
                    gmf_id=gmf_id,
                    task_no=task_no,
                    imt=imt[0],
                    sa_period=imt[1],
                    sa_damping=imt[2],
                    site_id=site_id,
                    rupture_ids=rup_ids,
                    gmvs=gmvs))
        inserter.flush()


class ScenarioHazardCalculator(haz_general.BaseHazardCalculator):
    """
    Scenario hazard calculator. Computes ground motion fields.
    """

    core_calc_task = gmfs
    output = None  # defined in pre_execute

    def __init__(self, *args, **kwargs):
        super(ScenarioHazardCalculator, self).__init__(*args, **kwargs)
        self.gmf = None
        self.rupture = None

    def initialize_sources(self):
        """
        Get the rupture_model file from the job.ini file, and set the
        attribute self.rupture.
        """
        nrml = RuptureModelParser(self.hc.inputs['rupture_model']).parse()
        self.rupture = source.NrmlHazardlibConverter(self.hc)(nrml)

    def pre_execute(self):
        """
        Do pre-execution work. At the moment, this work entails:
        parsing and initializing sources, parsing and initializing the
        site model (if there is one), parsing vulnerability and
        exposure files, and generating logic tree realizations. (The
        latter piece basically defines the work to be done in the
        `execute` phase.)
        """
        self.parse_risk_models()
        self.initialize_sources()
        self.initialize_site_model()
        self.create_ruptures()

    def create_ruptures(self):
        # check filtering
        hc = self.hc
        if hc.maximum_distance:
            self.sites = filters.filter_sites_by_distance_to_rupture(
                self.rupture, hc.maximum_distance, hc.site_collection)
            if self.sites is None:
                raise RuntimeError(
                    'All sites where filtered out! '
                    'maximum_distance=%s km' % hc.maximum_distance)

        # create ses output
        output = models.Output.objects.create(
            oq_job=self.job,
            display_name='SES Collection',
            output_type='ses')
        self.ses_coll = models.SESCollection.objects.create(
            output=output, lt_model=None, ordinal=0)

        # create gmf output
        output = models.Output.objects.create(
            oq_job=self.job,
            display_name="GMF",
            output_type="gmf_scenario")
        self.gmf = models.Gmf.objects.create(output=output)

        # creating seeds
        rnd = random.Random()
        rnd.seed(self.hc.random_seed)
        all_seeds = [
            rnd.randint(0, models.MAX_SINT_32)
            for _ in xrange(self.hc.number_of_ground_motion_fields)]

        with self.monitor('saving ruptures'):
            prob_rup = models.ProbabilisticRupture.create(
                self.rupture, self.ses_coll)
            inserter = writer.CacheInserter(models.SESRupture, 100000)
            for ses_idx, seed in enumerate(all_seeds):
                inserter.add(
                    models.SESRupture(
                        ses_id=1, rupture=prob_rup,
                        tag='scenario-%010d' % ses_idx,  seed=seed))
            inserter.flush()

    def task_arg_gen(self):
        """
        Yield a tuple of the form (job_id, sitecol, rupture_id, gmf_id,
        task_seed, num_realizations). `task_seed` will be used to seed
        numpy for temporal occurence sampling. Only a single task
        will be generated which is fine since the computation is fast
        anyway.
        """
        ses_ruptures = models.SESRupture.objects.filter(
            rupture__ses_collection=self.ses_coll.id)
        ss = SequenceSplitter(self.concurrent_tasks())
        for task_no, ruptures in enumerate(ss.split(ses_ruptures)):
            yield self.job.id, ruptures, self.sites, self.gmf.id, task_no

########NEW FILE########
__FILENAME__ = post_processing
# -*- coding: utf-8 -*-
# pylint: enable=W0511,W0142,I0011,E1101,E0611,F0401,E1103,R0801,W0232

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Post processing functionality for computing mean and quantile curves.
"""

import numpy


def mean_curve(curves, weights=None):
    """
    Compute the mean or weighted average of a set of curves.

    :param curves:
        2D array-like collection of hazard curve PoE values. Each element
        should be a sequence of PoE `float` values. Example::

            [[0.5, 0.4, 0.3], [0.6, 0.59, 0.1]]

        .. note::
            This data represents the curves for all realizations for a given
            site and IMT.

    :param weights:
        List or numpy array of weights, 1 weight value for each of the input
        ``curves``. This is only used for weighted averages.

    :returns:
        A curve representing the mean/average (or weighted average, in case
        ``weights`` are specified) of all the input ``curves``.
    """
    if weights is not None:
        # If all of the weights are None, don't compute a weighted average
        none_weights = [x is None for x in weights]
        if all(none_weights):
            weights = None
        elif any(none_weights):
            # some weights are defined, but some are none;
            # this is invalid input
            raise ValueError('`None` value found in weights: %s' % weights)

    return numpy.average(curves, weights=weights, axis=0)


def weighted_quantile_curve(curves, weights, quantile):
    """
    Compute the weighted quantile aggregate of a set of curves. This method is
    used in the case where hazard curves are computed using the logic tree
    end-branch enumeration approach. In this case, the weights are explicit.

    :param curves:
        2D array-like of curve PoEs. Each row represents the PoEs for a single
        curve
    :param weights:
        Array-like of weights, 1 for each input curve.
    :param quantile:
        Quantile value to calculate. Should in the range [0.0, 1.0].

    :returns:
        A numpy array representing the quantile aggregate of the input
        ``curves`` and ``quantile``, weighting each curve with the specified
        ``weights``.
    """
    # Each curve needs to be associated with a weight:
    assert len(weights) == len(curves)
    # NOTE(LB): Weights might be passed as a list of `decimal.Decimal`
    # types, and numpy.interp can't handle this (it throws TypeErrors).
    # So we explicitly cast to floats here before doing interpolation.
    weights = numpy.array(weights, dtype=numpy.float64)

    result_curve = []

    np_curves = numpy.array(curves)
    np_weights = numpy.array(weights)

    for poes in np_curves.transpose():
        sorted_poe_idxs = numpy.argsort(poes)
        sorted_weights = np_weights[sorted_poe_idxs]
        sorted_poes = poes[sorted_poe_idxs]

        # cumulative sum of weights:
        cum_weights = numpy.cumsum(sorted_weights)

        result_curve.append(numpy.interp(quantile, cum_weights, sorted_poes))

    return numpy.array(result_curve)


def quantile_curve(curves, quantile):
    """
    Compute the quantile aggregate of a set of curves. This method is used in
    the case where hazard curves are computed using the Monte-Carlo logic tree
    sampling approach. In this case, the weights are implicit.

    :param curves:
        2D array-like collection of hazard curve PoE values. Each element
        should be a sequence of PoE `float` values. Example::

            [[0.5, 0.4, 0.3], [0.6, 0.59, 0.1]]
    :param float quantile:
        The quantile value. We expected a value in the range [0.0, 1.0].

    :returns:
        A numpy array representing the quantile aggregate of the input
        ``curves`` and ``quantile``.
    """
    # this implementation is an alternative to:
    # return numpy.array(mstats.mquantiles(curves, prob=quantile, axis=0))[0]

    # more or less copied from the scipy mquantiles function, just special
    # cased for what we need (and a lot faster)

    arr = numpy.array(curves)

    p = numpy.array(quantile)
    m = 0.4 + p * 0.2

    n = len(arr)
    aleph = n * p + m
    k = numpy.floor(aleph.clip(1, n - 1)).astype(int)
    gamma = (aleph - k).clip(0, 1)

    data = numpy.sort(arr, axis=0).transpose()
    return (1.0 - gamma) * data[:, k - 1] + gamma * data[:, k]

########NEW FILE########
__FILENAME__ = base
# -*- coding: utf-8 -*-

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Base RiskCalculator class.
"""

import collections
import psutil

from openquake.engine import logs, export
from openquake.engine.db import models
from openquake.engine.calculators import base
from openquake.engine.calculators.risk import \
    writers, validation, loaders, hazard_getters
from openquake.engine.utils import config, tasks
from openquake.risklib.workflows import RiskModel
from openquake.engine.performance import EnginePerformanceMonitor

BLOCK_SIZE = 100  # number of assets per block

MEMORY_ERROR = '''Running the calculation will require approximately
%dM, i.e. more than the memory which is available right now (%dM).
Please increase the free memory or apply a stringent region
constraint to reduce the number of assets. Alternatively you can set
epsilons_management=fast in openquake.cfg. It the correlation is
nonzero, consider setting asset_correlation=0 to avoid building the
correlation matrix.'''


@tasks.oqtask
def distribute_by_assets(job_id, calc, taxonomy, counts, outputdict):
    """
    Spawn risk tasks and return an OqTaskManager instance.

    :param job_id:
        ID of the current risk job
    :param calc:
        :class:`openquake.engine.calculators.risk.base.RiskCalculator` instance
    :param str taxonomy:
        taxonomy of the current bunch of assets
    :param int counts:
        number of assets of the given taxonomy
    :param outputdict:
        :class:`openquake.engine.calculators.risk.writers.OutputDict` instance
    """
    logs.LOG.info('taxonomy=%s, assets=%d', taxonomy, counts)
    with calc.monitor("associating asset->site"):
        builder = hazard_getters.GetterBuilder(
            taxonomy, calc.rc, calc.eps_man)
    haz_outs = calc.rc.hazard_outputs()
    nbytes = builder.calc_nbytes(haz_outs)
    if nbytes:
        estimate_mb = nbytes / 1024 / 1024 * 3
        if calc.eps_man == 'fast' and calc.rc.asset_correlation == 0:
            pass  # using much less memory than the estimate, don't log
        else:
            logs.LOG.info('epsilons_management=%s: '
                          'you should need less than %dM (rough estimate)',
                          calc.eps_man, estimate_mb)
        phymem = psutil.phymem_usage()
        available_memory = (1 - phymem.percent / 100) * phymem.total
        available_mb = available_memory / 1024 / 1024
        if calc.eps_man == 'full' and nbytes * 3 > available_memory:
            raise MemoryError(MEMORY_ERROR % (estimate_mb, available_mb))

    task_no = 0
    name = calc.core_calc_task.__name__ + '[%s]' % taxonomy
    otm = tasks.OqTaskManager(calc.core_calc_task, logs.LOG.progress, name)
    with calc.monitor("building epsilons"):
        builder.init_epsilons(haz_outs)
    for offset in range(0, counts, BLOCK_SIZE):
        with calc.monitor("getting asset chunks"):
            assets = models.ExposureData.objects.get_asset_chunk(
                calc.rc, taxonomy, offset, BLOCK_SIZE)
        with calc.monitor("building getters"):
            try:
                getters = builder.make_getters(
                    calc.getter_class, haz_outs, assets)
            except hazard_getters.AssetSiteAssociationError as err:
                # TODO: add a test for this corner case
                # https://bugs.launchpad.net/oq-engine/+bug/1317796
                logs.LOG.warn('Taxonomy %s: %s', taxonomy, err)
                continue
        # submitting task
        task_no += 1
        logs.LOG.info('Built task #%d for taxonomy %s', task_no, taxonomy)
        risk_model = calc.risk_models[taxonomy].copy(getters=getters)
        otm.submit(calc.job.id, risk_model, outputdict,
                   calc.calculator_parameters)

    return otm


class RiskCalculator(base.Calculator):
    """
    Abstract base class for risk calculators. Contains a bunch of common
    functionality, including initialization procedures and the core
    distribution/execution logic.

    :attribute dict taxonomies_asset_count:
        A dictionary mapping each taxonomy with the number of assets the
        calculator will work on. Assets are extracted from the exposure input
        and filtered according to the `RiskCalculation.region_constraint`.

    :attribute dict risk_models:
        A nested dict taxonomy -> loss type -> instances of `RiskModel`.
    """

    # a list of :class:`openquake.engine.calculators.risk.validation` classes
    validators = [validation.HazardIMT, validation.EmptyExposure,
                  validation.OrphanTaxonomies, validation.ExposureLossTypes,
                  validation.NoRiskModels]

    bcr = False  # flag overridden in BCR calculators
    run_subtasks = distribute_by_assets

    def __init__(self, job):
        super(RiskCalculator, self).__init__(job)
        self.taxonomies_asset_count = None
        self.risk_models = None
        self.loss_types = set()
        self.acc = {}

    def agg_result(self, acc, res):
        """
        Aggregation method, to be overridden in subclasses
        """
        return acc

    def pre_execute(self):
        """
        In this phase, the general workflow is:
            1. Parse the exposure to get the taxonomies
            2. Parse the available risk models
            3. Validate exposure and risk models
        """
        with self.monitor('get exposure'):
            self.taxonomies_asset_count = (
                self.rc.preloaded_exposure_model or loaders.exposure(
                    self.job, self.rc.inputs['exposure'])
                ).taxonomies_in(self.rc.region_constraint)

        with self.monitor('parse risk models'):
            self.risk_models = self.get_risk_models()

            # consider only the taxonomies in the risk models if
            # taxonomies_from_model has been set to True in the
            # job.ini
            if self.rc.taxonomies_from_model:
                self.taxonomies_asset_count = dict(
                    (t, count)
                    for t, count in self.taxonomies_asset_count.items()
                    if t in self.risk_models)

        for validator_class in self.validators:
            validator = validator_class(self)
            error = validator.get_error()
            if error:
                raise ValueError("""Problems in calculator configuration:
                                 %s""" % error)

        num_assets = sum(self.taxonomies_asset_count.itervalues())
        num_taxonomies = len(self.taxonomies_asset_count)
        logs.LOG.info('Considering %d assets of %d distinct taxonomies',
                      num_assets, num_taxonomies)
        self.eps_man = config.get('risk', 'epsilons_management')

    @EnginePerformanceMonitor.monitor
    def execute(self):
        """
        Method responsible for the distribution strategy. It divides
        the considered exposure into chunks of homogeneous assets
        (i.e. having the same taxonomy).
        """
        def agg(acc, otm):
            return otm.aggregate_results(self.agg_result, acc)
        run = self.run_subtasks
        name = run.__name__ + '[%s]' % self.core_calc_task.__name__
        self.acc = tasks.map_reduce(
            run, self.task_arg_gen(), agg, self.acc, name)

    def task_arg_gen(self):
        """
        Yields the argument to be submitted to run_subtasks. Tasks with
        fewer assets are submitted first.
        """
        outputdict = writers.combine_builders(
            [ob(self) for ob in self.output_builders])
        ct = sorted((counts, taxonomy) for taxonomy, counts
                    in self.taxonomies_asset_count.iteritems())
        for counts, taxonomy in ct:
            yield self.job.id, self, taxonomy, counts, outputdict

    def _get_outputs_for_export(self):
        """
        Util function for getting :class:`openquake.engine.db.models.Output`
        objects to be exported.
        """
        return export.core.get_outputs(self.job.id)

    def _do_export(self, output_id, export_dir, export_type):
        """
        Risk-specific implementation of
        :meth:`openquake.engine.calculators.base.Calculator._do_export`.

        Calls the risk exporter.
        """
        return export.risk.export(output_id, export_dir, export_type)

    @property
    def rc(self):
        """
        A shorter and more convenient way of accessing the
        :class:`~openquake.engine.db.models.RiskCalculation`.
        """
        return self.job.risk_calculation

    @property
    def hc(self):
        """
        A shorter and more convenient way of accessing the
        :class:`~openquake.engine.db.models.HazardCalculation`.
        """
        return self.rc.get_hazard_calculation()

    @property
    def calculator_parameters(self):
        """
        The specific calculation parameters passed as args to the
        celery task function. A calculator must override this to
        provide custom arguments to its celery task
        """
        return []

    def get_risk_models(self):
        # regular risk models
        if self.bcr is False:
            return dict(
                (taxonomy, RiskModel(taxonomy, self.get_workflow(vfs)))
                for taxonomy, vfs in self._get_vfs())

        # BCR risk models
        risk_models = {}
        orig_data = self._get_vfs(retrofitted=False)
        retro_data = self._get_vfs(retrofitted=True)

        for orig, retro in zip(orig_data, retro_data):
            taxonomy, vfs = orig
            taxonomy_, vfs_ = retro
            assert taxonomy_ == taxonomy_  # same taxonomy
            risk_models[taxonomy] = RiskModel(
                taxonomy, self.get_workflow(vfs, vfs_))

        return risk_models

    def _get_vfs(self, retrofitted=False):
        """
        Parse vulnerability models for each loss type in
        `openquake.engine.db.models.LOSS_TYPES`,
        then set the `risk_models` attribute.

        :param bool retrofitted:
            True if retrofitted models should be retrieved
        :returns:
            A dictionary taxonomy -> instance of `RiskModel`.
        """
        data = collections.defaultdict(list)  # imt, loss_type, vf per taxonomy
        for v_input, loss_type in self.rc.vulnerability_inputs(retrofitted):
            self.loss_types.add(loss_type)
            for taxonomy, vf in loaders.vulnerability(v_input).items():
                data[taxonomy].append((loss_type, vf))
        for taxonomy in data:
            yield taxonomy, dict(data[taxonomy])

    def get_workflow(self, vulnerability_functions):
        """
        To be overridden in subclasses. Must return a workflow instance.
        """
        class Workflow():
            vulnerability_functions = {}
        return Workflow()

#: Calculator parameters are used to compute derived outputs like loss
#: maps, disaggregation plots, quantile/mean curves. See
#: :class:`openquake.engine.db.models.RiskCalculation` for a description

CalcParams = collections.namedtuple(
    'CalcParams', [
        'conditional_loss_poes',
        'poes_disagg',
        'sites_disagg',
        'insured_losses',
        'quantiles',
        'asset_life_expectancy',
        'interest_rate',
        'mag_bin_width',
        'distance_bin_width',
        'coordinate_bin_width',
        'damage_state_ids'
    ])


def make_calc_params(conditional_loss_poes=None,
                     poes_disagg=None,
                     sites_disagg=None,
                     insured_losses=None,
                     quantiles=None,
                     asset_life_expectancy=None,
                     interest_rate=None,
                     mag_bin_width=None,
                     distance_bin_width=None,
                     coordinate_bin_width=None,
                     damage_state_ids=None):
    """
    Constructor of CalculatorParameters
    """
    return CalcParams(conditional_loss_poes,
                      poes_disagg,
                      sites_disagg,
                      insured_losses,
                      quantiles,
                      asset_life_expectancy,
                      interest_rate,
                      mag_bin_width,
                      distance_bin_width,
                      coordinate_bin_width,
                      damage_state_ids)

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core functionality for the classical PSHA risk calculator.
"""

import itertools
from openquake.risklib import workflows

from django.db import transaction

from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.calculators import post_processing
from openquake.engine.calculators.risk import (
    base, hazard_getters, validation, writers)
from openquake.engine.utils import tasks


@tasks.oqtask
def classical(job_id, risk_model, outputdict, params):
    """
    Celery task for the classical risk calculator.

    :param int job_id:
      ID of the currently running job
    :param risk_model:
      A :class:`openquake.risklib.workflows.RiskModel` instance
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (e.g. a LossCurve)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    """
    monitor = EnginePerformanceMonitor(None, job_id, classical, tracing=True)

    # Do the job in other functions, such that they can be unit tested
    # without the celery machinery
    with transaction.commit_on_success(using='job_init'):
        do_classical(risk_model, outputdict, params, monitor)


def do_classical(risk_model, outputdict, params, monitor):
    """
    See `classical` for a description of the parameters.
    :param monitor:
      a context manager for logging/profiling purposes

    For each calculation unit we compute loss curves, loss maps and
    loss fractions. Then if the number of units are bigger than 1, we
    compute mean and quantile artifacts.
    """
    outputs_per_loss_type = risk_model.compute_outputs(
        monitor.copy('getting data'))
    stats_per_loss_type = risk_model.compute_stats(
        outputs_per_loss_type, params.quantiles, post_processing)
    for loss_type, outputs in outputs_per_loss_type.iteritems():
        stats = stats_per_loss_type[loss_type]
        with monitor.copy('saving risk'):
            for out in outputs:
                save_individual_outputs(
                    outputdict.with_args(
                        loss_type=loss_type, hazard_output_id=out.hid),
                    out.output, params)
            if stats is not None:
                save_statistical_output(
                    outputdict.with_args(
                        loss_type=loss_type, hazard_output_id=None),
                    stats, params)


def save_individual_outputs(outputdict, outs, params):
    """
    Save loss curves, loss maps and loss fractions associated with a
    calculation unit

    :param outputdict:
        a :class:`openquake.engine.calculators.risk.writers.OutputDict`
        instance holding the reference to the output container objects
    :param outs:
        a :class:`openquake.risklib.workflows.Classical.Output`
        holding the output data for a calculation unit
    :param params:
        a :class:`openquake.engine.calculators.risk.base.CalcParams`
        holding the parameters for this calculation
    """
    outputdict.write(
        outs.assets,
        (outs.loss_curves, outs.average_losses),
        output_type="loss_curve")

    if outs.insured_curves is not None:
        outputdict.write(
            outs.assets,
            (outs.insured_curves, outs.average_insured_losses),
            insured=True,
            output_type="loss_curve")

    outputdict.write_all(
        "poe", params.conditional_loss_poes,
        outs.loss_maps,
        outs.assets,
        output_type="loss_map")

    taxonomies = [a.taxonomy for a in outs.assets]
    outputdict.write_all(
        "poe", params.poes_disagg,
        outs.loss_fractions, outs.assets, taxonomies,
        output_type="loss_fraction", variable="taxonomy")


def save_statistical_output(outputdict, stats, params):
    """
    Save statistical outputs (mean and quantile loss curves, mean and
    quantile loss maps, mean and quantile loss fractions) for the
    calculation.

    :param outputdict:
        a :class:`openquake.engine.calculators.risk.writers.OutputDict`
        instance holding the reference to the output container objects
    :param outs:
        a :class:`openquake.risklib.workflows.Classical.StatisticalOutput`
        holding the statistical output data
    :param params:
        a :class:`openquake.engine.calculators.risk.base.CalcParams`
        holding the parameters for this calculation
    """

    # mean curves, maps and fractions
    outputdict.write(
        stats.assets, (stats.mean_curves, stats.mean_average_losses),
        output_type="loss_curve", statistics="mean")

    outputdict.write_all("poe", params.conditional_loss_poes,
                         stats.mean_maps, stats.assets,
                         output_type="loss_map",
                         statistics="mean")

    outputdict.write_all("poe", params.poes_disagg,
                         stats.mean_fractions,
                         stats.assets,
                         [a.taxonomy for a in stats.assets],
                         output_type="loss_fraction", statistics="mean",
                         variable="taxonomy")

    # quantile curves, maps and fractions
    outputdict.write_all(
        "quantile", params.quantiles,
        [(c, a) for c, a in itertools.izip(
            stats.quantile_curves, stats.quantile_average_losses)],
        stats.assets, output_type="loss_curve", statistics="quantile")

    for quantile, maps in zip(params.quantiles, stats.quantile_maps):
        outputdict.write_all("poe", params.conditional_loss_poes, maps,
                             stats.assets, output_type="loss_map",
                             statistics="quantile", quantile=quantile)

    for quantile, fractions in zip(params.quantiles, stats.quantile_fractions):
        outputdict.write_all("poe", params.poes_disagg, fractions,
                             stats.assets, [a.taxonomy for a in stats.assets],
                             output_type="loss_fraction",
                             statistics="quantile", quantile=quantile,
                             variable="taxonomy")

    # mean and quantile insured curves
    if stats.mean_insured_curves is not None:
        outputdict.write(
            stats.assets, (stats.mean_insured_curves,
                           stats.mean_average_insured_losses),
            output_type="loss_curve", statistics="mean", insured=True)

        outputdict.write_all(
            "quantile", params.quantiles,
            [(c, a) for c, a in itertools.izip(
                stats.quantile_insured_curves,
                stats.quantile_average_insured_losses)],
            stats.assets,
            output_type="loss_curve", statistics="quantile", insured=True)


class ClassicalRiskCalculator(base.RiskCalculator):
    """
    Classical PSHA risk calculator. Computes loss curves and loss maps
    for a given set of assets.
    """

    #: celery task
    core_calc_task = classical

    validators = base.RiskCalculator.validators + [
        validation.RequireClassicalHazard,
        validation.ExposureHasInsuranceBounds]

    output_builders = [writers.LossCurveMapBuilder,
                       writers.ConditionalLossFractionBuilder]

    getter_class = hazard_getters.HazardCurveGetter

    def get_workflow(self, vulnerability_functions):
        return workflows.Classical(
            vulnerability_functions,
            self.rc.lrem_steps_per_interval,
            self.rc.conditional_loss_poes,
            self.rc.poes_disagg,
            self.rc.insured_losses)

    @property
    def calculator_parameters(self):
        """
        Specific calculator parameters returned as list suitable to be
        passed in task_arg_gen
        """

        return base.make_calc_params(
            conditional_loss_poes=self.rc.conditional_loss_poes or [],
            quantiles=self.rc.quantile_loss_curves or [],
            poes_disagg=self.rc.poes_disagg or [])

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core functionality for the classical PSHA risk calculator.
"""

from django.db import transaction
from openquake.risklib import workflows

from openquake.engine.calculators.risk import (
    hazard_getters, writers, validation)
from openquake.engine.calculators.risk.classical import core as classical
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks


@tasks.oqtask
def classical_bcr(job_id, risk_model, outputdict, _params):
    """
    Celery task for the BCR risk calculator based on the classical
    calculator.

    Instantiates risklib calculators, computes BCR and stores the
    results to db in a single transaction.

    :param int job_id:
      ID of the currently running job
    :param risk_model:
      A :class:`openquake.risklib.workflows.RiskModel` instance
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (in this case only `BCRDistribution`)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    """
    monitor = EnginePerformanceMonitor(
        None, job_id, classical_bcr, tracing=True)

    # Do the job in other functions, such that it can be unit tested
    # without the celery machinery
    with transaction.commit_on_success(using='job_init'):
        do_classical_bcr(risk_model, outputdict, monitor)


def do_classical_bcr(risk_model, outputdict, monitor):
    out = risk_model.compute_outputs(monitor.copy('getting hazard'))
    for loss_type, outputs in out.iteritems():
        outputdict = outputdict.with_args(loss_type=loss_type)
        with monitor.copy('writing results'):
            for out in outputs:
                outputdict.write(
                    risk_model.workflow.assets,
                    out.output,
                    output_type="bcr_distribution",
                    hazard_output_id=out.hid)


class ClassicalBCRRiskCalculator(classical.ClassicalRiskCalculator):
    """
    Classical BCR risk calculator. Computes BCR distributions for a
    given set of assets.

    :attr dict vulnerability_functions_retrofitted:
        A dictionary mapping each taxonomy to a vulnerability functions for the
        retrofitted losses computation
    """
    core_calc_task = classical_bcr

    validators = classical.ClassicalRiskCalculator.validators + [
        validation.ExposureHasRetrofittedCosts]

    output_builders = [writers.BCRMapBuilder]

    getter_class = hazard_getters.HazardCurveGetter

    bcr = True

    def get_workflow(self, vf_orig, vf_retro):
        """
        Set the attributes .workflow and .getters
        """
        return workflows.ClassicalBCR(
            vf_orig, vf_retro,
            self.rc.lrem_steps_per_interval,
            self.rc.interest_rate,
            self.rc.asset_life_expectancy)

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Core functionality for the classical PSHA risk calculator.
"""
import collections
import itertools
import numpy

from django import db

from openquake.hazardlib.geo import mesh
from openquake.risklib import scientific, workflows

from openquake.engine.calculators import post_processing
from openquake.engine.calculators.risk import (
    base, hazard_getters, validation, writers)
from openquake.engine.db import models
from openquake.engine import writer
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks


@tasks.oqtask
def event_based(job_id, risk_model, outputdict, params):
    """
    Celery task for the event based risk calculator.

    :param job_id: the id of the current
        :class:`openquake.engine.db.models.OqJob`
    :param risk_model:
      A :class:`openquake.risklib.workflows.RiskModel` instance
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (e.g. a LossCurve)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    :returns:
      A dictionary {loss_type: event_loss_table}
    """
    monitor = EnginePerformanceMonitor(
        None, job_id, event_based, tracing=True)

    # Do the job in other functions, such that they can be unit tested
    # without the celery machinery
    with db.transaction.commit_on_success(using='job_init'):
        return do_event_based(risk_model, outputdict, params, monitor)


def do_event_based(risk_model, outputdict, params, monitor):
    """
    See `event_based` for a description of the params

    :returns: the event loss table generated by risk_model
    """
    outputs_per_loss_type = risk_model.compute_outputs(
        monitor.copy('getting data'))
    stats_per_loss_type = risk_model.compute_stats(
        outputs_per_loss_type, params.quantiles, post_processing)

    event_loss_table = {}

    # save outputs and stats and populate the event loss table
    for loss_type, outputs in outputs_per_loss_type.iteritems():
        stats = stats_per_loss_type[loss_type]
        for out in outputs:
            if params.sites_disagg:
                with monitor.copy('disaggregating results'):
                    rupture_ids = out.output.event_loss_table.keys()
                    disagg_outputs = disaggregate(
                        out.output, rupture_ids, params)
            else:
                disagg_outputs = None

            with monitor.copy('saving individual risk'):
                save_individual_outputs(
                    outputdict.with_args(hazard_output_id=out.hid,
                                         loss_type=loss_type),
                    out.output, disagg_outputs, params)

        if stats is not None:
            with monitor.copy('saving risk statistics'):
                save_statistical_output(
                    outputdict.with_args(
                        hazard_output_id=None, loss_type=loss_type),
                    stats, params)
            event_loss_table[loss_type] = stats.event_loss_table
        else:
            event_loss_table[loss_type] = outputs[0].output.event_loss_table

    return event_loss_table


def save_individual_outputs(outputdict, outputs, disagg_outputs, params):
    """
    Save loss curves, loss maps and loss fractions associated with a
    calculation unit

    :param outputdict:
        a :class:`openquake.engine.calculators.risk.writers.OutputDict`
        instance holding the reference to the output container objects
    :param outputs:
        a :class:`openquake.risklib.workflows.ProbabilisticEventBased.Output`
        holding the output data for a calculation unit
    :param disagg_outputs:
        a :class:`.DisaggregationOutputs` holding the disaggreation
        output data for a calculation unit
    :param params:
        a :class:`openquake.engine.calculators.risk.base.CalcParams`
        holding the parameters for this calculation
    """

    outputdict.write(
        outputs.assets,
        (outputs.loss_curves, outputs.average_losses, outputs.stddev_losses),
        output_type="event_loss_curve")

    outputdict.write_all(
        "poe", params.conditional_loss_poes,
        outputs.loss_maps,
        outputs.assets,
        output_type="loss_map")

    if disagg_outputs is not None:
        # FIXME. We should avoid synthetizing the generator
        assets = list(disagg_outputs.assets_disagg)
        outputdict.write(
            assets,
            disagg_outputs.magnitude_distance,
            disagg_outputs.fractions,
            output_type="loss_fraction",
            variable="magnitude_distance")
        outputdict.write(
            assets,
            disagg_outputs.coordinate, disagg_outputs.fractions,
            output_type="loss_fraction",
            variable="coordinate")

    if outputs.insured_curves is not None:
        outputdict.write(
            outputs.assets,
            (outputs.insured_curves, outputs.average_insured_losses,
             outputs.stddev_insured_losses),
            output_type="event_loss_curve", insured=True)


def save_statistical_output(outputdict, stats, params):
    """
    Save statistical outputs (mean and quantile loss curves, mean and
    quantile loss maps) for the calculation.

    :param outputdict:
        a :class:`openquake.engine.calculators.risk.writers.OutputDict`
        instance holding the reference to the output container objects
    :param stats:
        :class:
        `openquake.risklib.workflows.ProbabilisticEventBased.StatisticalOutput`
        holding the statistical output data
    :param params:
        a :class:`openquake.engine.calculators.risk.base.CalcParams`
        holding the parameters for this calculation
    """

    outputdict.write(
        stats.assets, (stats.mean_curves, stats.mean_average_losses),
        output_type="loss_curve", statistics="mean")

    outputdict.write_all(
        "poe", params.conditional_loss_poes, stats.mean_maps,
        stats.assets, output_type="loss_map", statistics="mean")

    # quantile curves and maps
    outputdict.write_all(
        "quantile", params.quantiles,
        [(c, a) for c, a in itertools.izip(stats.quantile_curves,
                                           stats.quantile_average_losses)],
        stats.assets, output_type="loss_curve", statistics="quantile")

    if params.quantiles:
        for quantile, maps in zip(params.quantiles, stats.quantile_maps):
            outputdict.write_all(
                "poe", params.conditional_loss_poes, maps,
                stats.assets, output_type="loss_map",
                statistics="quantile", quantile=quantile)

    # mean and quantile insured curves
    if stats.mean_insured_curves is not None:
        outputdict.write(
            stats.assets, (stats.mean_insured_curves,
                           stats.mean_average_insured_losses),
            output_type="loss_curve", statistics="mean", insured=True)

        outputdict.write_all(
            "quantile", params.quantiles,
            [(c, a) for c, a in itertools.izip(
                stats.quantile_insured_curves,
                stats.quantile_average_insured_losses)],
            stats.assets,
            output_type="loss_curve", statistics="quantile", insured=True)


class DisaggregationOutputs(object):
    def __init__(self, assets_disagg, magnitude_distance,
                 coordinate, fractions):
        self.assets_disagg = assets_disagg
        self.magnitude_distance = magnitude_distance
        self.coordinate = coordinate
        self.fractions = fractions


def disaggregate(outputs, rupture_ids, params):
    """
    Compute disaggregation outputs given the individual `outputs` and `params`

    :param outputs:
      an instance of
      :class:`openquake.risklib.workflows.ProbabilisticEventBased.Output`
    :param params:
      an instance of :class:`..base.CalcParams`
    :param list rupture_ids:
      a list of :class:`openquake.engine.db.models.SESRupture` IDs
    :returns:
      an instance of :class:`DisaggregationOutputs`
    """
    def disaggregate_site(site, loss_ratios):
        for fraction, rupture_id in zip(loss_ratios, rupture_ids):
            rupture = models.SESRupture.objects.get(pk=rupture_id).rupture
            s = rupture.surface
            m = mesh.Mesh(numpy.array([site.x]), numpy.array([site.y]), None)

            mag = numpy.floor(rupture.magnitude / params.mag_bin_width)
            dist = numpy.floor(
                s.get_joyner_boore_distance(m))[0] / params.distance_bin_width

            closest_point = iter(s.get_closest_points(m)).next()
            lon = closest_point.longitude / params.coordinate_bin_width
            lat = closest_point.latitude / params.coordinate_bin_width

            yield "%d,%d" % (mag, dist), "%d,%d" % (lon, lat), fraction

    assets_disagg = []
    disagg_matrix = []

    for asset, losses in zip(outputs.assets, outputs.loss_matrix):
        if asset.site in params.sites_disagg:
            disagg_matrix.extend(list(disaggregate_site(asset.site, losses)))

            # FIXME. the functions in
            # openquake.engine.calculators.risk.writers requires an
            # asset per each row in the disaggregation matrix. To this
            # aim, we repeat the assets that will be passed to such
            # functions
            assets_disagg = itertools.chain(
                assets_disagg,
                itertools.repeat(asset, len(rupture_ids)))

    if assets_disagg:
        magnitudes, coordinates, fractions = zip(*disagg_matrix)
    else:
        magnitudes, coordinates, fractions = [], [], []

    return DisaggregationOutputs(
        assets_disagg, magnitudes, coordinates, fractions)


class EventBasedRiskCalculator(base.RiskCalculator):
    """
    Probabilistic Event Based PSHA risk calculator. Computes loss
    curves, loss maps, aggregate losses and insured losses for a given
    set of assets.
    """

    #: The core calculation celery task function
    core_calc_task = event_based

    # FIXME(lp). Validate sites_disagg to ensure non-empty outputs
    validators = base.RiskCalculator.validators + [
        validation.RequireEventBasedHazard,
        validation.ExposureHasInsuranceBounds]

    output_builders = [writers.EventLossCurveMapBuilder,
                       writers.LossFractionBuilder]
    getter_class = hazard_getters.GroundMotionValuesGetter

    def __init__(self, job):
        super(EventBasedRiskCalculator, self).__init__(job)
        # accumulator for the event loss tables
        self.acc = collections.defaultdict(collections.Counter)

    def agg_result(self, acc, event_loss_tables):
        """
        Updates the event loss table
        """
        return dict((loss_type, acc[loss_type] + event_loss_tables[loss_type])
                    for loss_type in self.loss_types)

    def post_process(self):
        """
          Compute aggregate loss curves and event loss tables
        """
        with EnginePerformanceMonitor('post processing', self.job.id):

            time_span, tses = self.hazard_times()
            for loss_type, event_loss_table in self.acc.items():
                for hazard_output in self.rc.hazard_outputs():

                    event_loss = models.EventLoss.objects.create(
                        output=models.Output.objects.create_output(
                            self.job,
                            "Event Loss Table. type=%s, hazard=%s" % (
                                loss_type, hazard_output.id),
                            "event_loss"),
                        loss_type=loss_type,
                        hazard_output=hazard_output)
                    inserter = writer.CacheInserter(models.EventLossData, 9999)

                    ses_coll = models.SESCollection.objects.get(
                        lt_model=hazard_output.output_container.
                        lt_realization.lt_model)
                    rupture_ids = ses_coll.get_ruptures().values_list(
                        'id', flat=True)
                    for rupture_id in rupture_ids:
                        if rupture_id in event_loss_table:
                            inserter.add(
                                models.EventLossData(
                                    event_loss_id=event_loss.id,
                                    rupture_id=rupture_id,
                                    aggregate_loss=event_loss_table[
                                        rupture_id]))
                    inserter.flush()

                    aggregate_losses = [
                        event_loss_table[rupture_id]
                        for rupture_id in rupture_ids
                        if rupture_id in event_loss_table]

                    if aggregate_losses:
                        aggregate_loss_losses, aggregate_loss_poes = (
                            scientific.event_based(
                                aggregate_losses, tses=tses,
                                time_span=time_span,
                                curve_resolution=self.rc.loss_curve_resolution
                            ))

                        models.AggregateLossCurveData.objects.create(
                            loss_curve=models.LossCurve.objects.create(
                                aggregate=True, insured=False,
                                hazard_output=hazard_output,
                                loss_type=loss_type,
                                output=models.Output.objects.create_output(
                                    self.job,
                                    "aggregate loss curves. "
                                    "loss_type=%s hazard=%s" % (
                                        loss_type, hazard_output),
                                    "agg_loss_curve")),
                            losses=aggregate_loss_losses,
                            poes=aggregate_loss_poes,
                            average_loss=scientific.average_loss(
                                aggregate_loss_losses, aggregate_loss_poes),
                            stddev_loss=numpy.std(aggregate_losses))

    def get_workflow(self, vulnerability_functions):
        time_span, tses = self.hazard_times()
        return workflows.ProbabilisticEventBased(
            vulnerability_functions,
            time_span, tses,
            self.rc.loss_curve_resolution,
            self.rc.conditional_loss_poes,
            self.rc.insured_losses)

    def hazard_times(self):
        """
        Return the hazard investigation time related to the ground
        motion field and the so-called time representative of the
        stochastic event set
        """
        return (self.rc.investigation_time,
                self.hc.ses_per_logic_tree_path * self.hc.investigation_time)

    @property
    def calculator_parameters(self):
        """
        Calculator specific parameters
        """
        return base.make_calc_params(
            conditional_loss_poes=self.rc.conditional_loss_poes or [],
            quantiles=self.rc.quantile_loss_curves or [],
            insured_losses=self.rc.insured_losses,
            sites_disagg=self.rc.sites_disagg or [],
            mag_bin_width=self.rc.mag_bin_width,
            distance_bin_width=self.rc.distance_bin_width,
            coordinate_bin_width=self.rc.coordinate_bin_width)

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake. If not, see <http://www.gnu.org/licenses/>.

"""
Core functionality for the Event Based BCR Risk calculator.
"""
from django.db import transaction

from openquake.risklib import workflows

from openquake.engine.calculators.risk import (
    hazard_getters, writers, validation)
from openquake.engine.calculators.risk.event_based import core as event_based
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks


@tasks.oqtask
def event_based_bcr(job_id, risk_model, outputdict, _params):
    """
    Celery task for the BCR risk calculator based on the event based
    calculator.

    Instantiates risklib calculators, computes bcr
    and stores results to db in a single transaction.

    :param int job_id:
      ID of the currently running job
    :param risk_model:
      A :class:`openquake.risklib.workflows.RiskModel` instance
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (in this case only `BCRDistribution`)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    """
    monitor = EnginePerformanceMonitor(
        None, job_id, event_based_bcr, tracing=True)

    # Do the job in other functions, such that it can be unit tested
    # without the celery machinery
    with transaction.commit_on_success(using='job_init'):
            do_event_based_bcr(risk_model, outputdict, monitor)


def do_event_based_bcr(risk_model, outputdict, monitor):
    """
    See `event_based_bcr` for docstring
    """
    out = risk_model.compute_outputs(monitor.copy('getting hazard'))
    for loss_type, outputs in out.iteritems():
        outputdict = outputdict.with_args(loss_type=loss_type)
        with monitor.copy('writing results'):
            for out in outputs:
                outputdict.write(
                    risk_model.workflow.assets,
                    out.output,
                    output_type="bcr_distribution",
                    hazard_output_id=out.hid)


class EventBasedBCRRiskCalculator(event_based.EventBasedRiskCalculator):
    """
    Event based BCR risk calculator. Computes BCR distributions for a
    given set of assets.
    """
    core_calc_task = event_based_bcr

    validators = event_based.EventBasedRiskCalculator.validators + [
        validation.ExposureHasRetrofittedCosts]

    output_builders = [writers.BCRMapBuilder]

    getter_cls = hazard_getters.GroundMotionValuesGetter

    bcr = True

    def get_workflow(self, vf_orig, vf_retro):
        time_span, tses = self.hazard_times()
        return workflows.ProbabilisticEventBasedBCR(
            vf_orig, vf_retro,
            time_span, tses, self.rc.loss_curve_resolution,
            self.rc.interest_rate,
            self.rc.asset_life_expectancy)

    def post_process(self):
        """
        No need to compute the aggregate loss curve in the BCR calculator.
        """

    def agg_result(self, acc, event_loss_tables):
        """
        No need to update event loss tables in the BCR calculator
        """
        return acc

########NEW FILE########
__FILENAME__ = hazard_getters
# -*- coding: utf-8 -*-

# Copyright (c) 2012-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Hazard getters for Risk calculators.

A HazardGetter is responsible fo getting hazard outputs needed by a risk
calculation.
"""
import itertools
import operator

import numpy

from openquake.hazardlib.imt import from_string
from openquake.risklib import scientific

from openquake.engine import logs
from openquake.engine.db import models

BYTES_PER_FLOAT = numpy.zeros(1, dtype=float).nbytes


class AssetSiteAssociationError(Exception):
    pass


def make_epsilons(asset_count, num_ruptures, seed, correlation,
                  epsilons_management):
    """
    :param int asset_count: the number of assets
    :param int num_ruptures: the number of ruptures
    :param int seed: a random seed
    :param float correlation: the correlation coefficient
    :param str epsilons_management: specify how to compute the epsilon matrix

    If `epsilons_management` is 'full', generate the full epsilon matrix;
    if it is 'fast' generate a vector of epsilons.
    """
    if epsilons_management == 'full':
        # generate the full epsilon matrix
        zeros = numpy.zeros((asset_count, num_ruptures))
        return scientific.make_epsilons(zeros, seed, correlation)
    elif epsilons_management == 'fast':
        # use a single epsilon for all realizations
        zeros = numpy.zeros((asset_count, 1))
        return scientific.make_epsilons(zeros, seed, correlation).reshape(-1)
    else:
        raise RuntimeError('Wrong parameter epsilons_management '
                           'in openquake.cfg: %r' % epsilons_management)


class HazardGetter(object):
    """
    A Hazard Getter is used to query for the closest hazard data for
    each given asset. A Hazard Getter must be pickable such that it
    should be possible to use different strategies (e.g. distributed
    or not, using postgis or not).
    A Hazard Getter should be instantiated by a GetterBuilder and
    not directly.

    :attr hazard_output:
        A :class:`openquake.engine.db.models.Output` instance

    :attr assets:
        The assets for which we want to extract the hazard

   :attr site_ids:
        The ids of the sites associated to the hazards
    """
    def __init__(self, hazard_output, assets, site_ids):
        self.hazard_output = hazard_output
        self.assets = assets
        self.site_ids = site_ids
        self.epsilons = None

    def __repr__(self):
        shape = getattr(self.epsilons, 'shape', None)
        eps = ', %s epsilons' % str(shape) if shape else ''
        return "<%s %d assets%s>" % (
            self.__class__.__name__, len(self.assets), eps)

    def get_data(self):
        """
        Subclasses must implement this.
        """
        raise NotImplementedError

    @property
    def hid(self):
        """Return the id of the given hazard output"""
        return self.hazard_output.id

    @property
    def weight(self):
        """Return the weight of the realization of the hazard output"""
        h = self.hazard_output.output_container
        if hasattr(h, 'lt_realization') and h.lt_realization:
            return h.lt_realization.weight


class HazardCurveGetter(HazardGetter):
    """
    Simple HazardCurve Getter that performs a spatial query for each
    asset.
    """ + HazardGetter.__doc__

    def get_data(self, imt):
        """
        Extracts the hazard curves for the given `imt` from the hazard output.

        :param str imt: Intensity Measure Type
        :returns: a list of N curves, each one being a list of pairs (iml, poe)
        """
        imt_type, sa_period, sa_damping = from_string(imt)

        oc = self.hazard_output.output_container
        if oc.output.output_type == 'hazard_curve':
            imls = oc.imls
        elif oc.output.output_type == 'hazard_curve_multi':
            oc = models.HazardCurve.objects.get(
                output__oq_job=oc.output.oq_job,
                output__output_type='hazard_curve',
                statistics=oc.statistics,
                lt_realization=oc.lt_realization,
                imt=imt_type,
                sa_period=sa_period,
                sa_damping=sa_damping)
            imls = oc.imls

        cursor = models.getcursor('job_init')
        query = """\
        SELECT hzrdr.hazard_curve_data.poes
        FROM hzrdr.hazard_curve_data
        WHERE hazard_curve_id = %s AND location = %s
        """
        all_curves = []
        for site_id in self.site_ids:
            location = models.HazardSite.objects.get(pk=site_id).location
            cursor.execute(query, (oc.id, 'SRID=4326; ' + location.wkt))
            poes = cursor.fetchall()[0][0]
            all_curves.append(zip(imls, poes))
        return all_curves


class GroundMotionValuesGetter(HazardGetter):
    """
    Hazard getter for loading ground motion values.
    """ + HazardGetter.__doc__
    rupture_ids = None  # set by the GetterBuilder
    epsilons = None  # set by the GetterBuilder

    def _get_gmv_dict(self, imt_type, sa_period, sa_damping):
        """
        :returns: a dictionary {rupture_id: gmv} for the given site and IMT
        """
        gmf_id = self.hazard_output.output_container.id
        if sa_period:
            imt_query = 'imt=%s and sa_period=%s and sa_damping=%s'
        else:
            imt_query = 'imt=%s and sa_period is %s and sa_damping is %s'
        gmv_dict = {}
        cursor = models.getcursor('job_init')
        cursor.execute('select site_id, rupture_ids, gmvs from '
                       'hzrdr.gmf_data where gmf_id=%s and site_id in %s '
                       'and {} order by site_id, task_no'.format(imt_query),
                       (gmf_id, tuple(self.site_ids),
                        imt_type, sa_period, sa_damping))
        for sid, group in itertools.groupby(cursor, operator.itemgetter(0)):
            gmvs = []
            ruptures = []
            for site_id, rupture_ids, gmvs in group:
                gmvs.extend(gmvs)
                ruptures.extend(rupture_ids)
            gmv_dict[sid] = dict(itertools.izip(ruptures, gmvs))
        return gmv_dict

    def get_data(self, imt):
        """
        Extracts the GMFs for the given `imt` from the hazard output.

        :param str imt: Intensity Measure Type
        :returns: a list of N arrays with R elements each.
        """
        imt_type, sa_period, sa_damping = from_string(imt)
        gmv_dict = self._get_gmv_dict(imt_type, sa_period, sa_damping)
        all_gmvs = []
        for site_id in self.site_ids:
            gmv = gmv_dict.get(site_id, {})
            if not gmv:
                logs.LOG.info('No data for site_id=%d, imt=%s', site_id, imt)
            array = numpy.array([gmv.get(r, 0.) for r in self.rupture_ids])
            all_gmvs.append(array)
        return all_gmvs


class ScenarioGetter(HazardGetter):
    """
    Hazard getter for loading ground motion values.
    """ + HazardGetter.__doc__

    rupture_ids = []  # there are no ruptures on the db
    epsilons = None  # set by the GetterBuilder
    num_samples = None  # set by the GetterBuilder

    def get_data(self, imt):
        """
        Extracts the GMFs for the given `imt` from the hazard output.

        :param str imt: Intensity Measure Type
        :returns: a list of N arrays with R elements each.
        """
        imt_type, sa_period, sa_damping = from_string(imt)
        gmf_id = self.hazard_output.output_container.id
        if sa_period:
            imt_query = 'imt=%s and sa_period=%s and sa_damping=%s'
        else:
            imt_query = 'imt=%s and sa_period is %s and sa_damping is %s'
        cursor = models.getcursor('job_init')
        templ = ('select site_id, gmvs from '
                 'hzrdr.gmf_data where gmf_id=%s and site_id in %s '
                 'and {} order by site_id, task_no'.format(imt_query))
        args = (gmf_id, tuple(set(self.site_ids)),
                imt_type, sa_period, sa_damping)
        cursor.execute(templ, args)

        gmvs_dict = {}  # site_id -> gmvs array
        for sid, group in itertools.groupby(cursor, operator.itemgetter(0)):
            gmvs = []
            for site_id, gmvs_ in group:
                gmvs.extend(gmvs_)
            gmvs_dict[sid] = numpy.array(gmvs, dtype=float)
        # TODO: add the test for the case where there is no data
        # for the given site and vector of zeros is returned
        return [gmvs_dict[sid] if sid in gmvs_dict
                else numpy.zeros(self.num_samples, dtype=float)
                for sid in self.site_ids]


class GetterBuilder(object):
    """
    A facility to build hazard getters. When instantiated, populates
    the lists .asset_ids and .site_ids with the associations between
    the assets in the current exposure model and the sites in the
    previous hazard calculation.

    :param str taxonomy: the taxonomy we are interested in
    :param rc: a :class:`openquake.engine.db.models.RiskCalculation` instance

    Warning: instantiating a GetterBuilder performs a potentially
    expensive geospatial query.
    """
    def __init__(self, taxonomy, rc, epsilons_management='full'):
        self.taxonomy = taxonomy
        self.rc = rc
        self.epsilons_management = epsilons_management
        self.hc = rc.get_hazard_calculation()
        max_dist = rc.best_maximum_distance * 1000  # km to meters
        cursor = models.getcursor('job_init')
        cursor.execute("""
SELECT DISTINCT ON (exp.id) exp.id AS asset_id, hsite.id AS site_id
FROM riski.exposure_data AS exp
JOIN hzrdi.hazard_site AS hsite
ON ST_DWithin(exp.site, hsite.location, %s)
WHERE hsite.hazard_calculation_id = %s
AND exposure_model_id = %s AND taxonomy=%s
AND ST_COVERS(ST_GeographyFromText(%s), exp.site)
ORDER BY exp.id, ST_Distance(exp.site, hsite.location, false)
""", (max_dist, self.hc.id, rc.exposure_model.id, taxonomy,
            rc.region_constraint.wkt))
        assets_sites = cursor.fetchall()
        if not assets_sites:
            raise AssetSiteAssociationError(
                'Could not associated any asset of taxonomy %s to '
                'hazard sites within the distance of %s km'
                % (taxonomy, self.rc.best_maximum_distance))

        self.asset_ids, self.site_ids = zip(*assets_sites)
        self.rupture_ids = {}
        self.epsilons = {}
        self.epsilons_shape = {}

    def calc_nbytes(self, hazard_outputs):
        """
        :param hazard_outputs: the outputs of a hazard calculation
        :returns: the number of bytes to be allocated (can be
                  much less if there is no correlation and the 'fast'
                  epsilons management is enabled).

        If the hazard_outputs come from an event based or scenario computation,
        populate the .epsilons_shape dictionary.
        """
        num_assets = len(self.asset_ids)
        if self.hc.calculation_mode == 'event_based':
            lt_model_ids = set(ho.output_container.lt_realization.lt_model.id
                               for ho in hazard_outputs)
            for lt_model_id in lt_model_ids:
                ses_coll = models.SESCollection.objects.get(
                    lt_model=lt_model_id)
                if self.epsilons_management == 'full':
                    samples = ses_coll.get_ruptures().count()
                else:
                    samples = 1
                self.epsilons_shape[ses_coll.id] = (num_assets, samples)
        elif self.hc.calculation_mode == 'scenario':
                if self.epsilons_management == 'full':
                    samples = self.hc.number_of_ground_motion_fields
                else:
                    samples = 1
                self.epsilons_shape[0] = (num_assets, samples)
        if self.epsilons_management == 'fast' and self.epsilons_shape:
            # size of the correlation matrix
            return num_assets * num_assets * BYTES_PER_FLOAT
        nbytes = 0
        for (n, r) in self.epsilons_shape.values():
            # the max(n, r) is taken because if n > r then the limiting
            # factor is the size of the correlation matrix, i.e. n
            nbytes += max(n, r) * n * BYTES_PER_FLOAT
        return nbytes

    def init_epsilons(self, hazard_outputs):
        """
        :param hazard_outputs: the outputs of a hazard calculation

        If the hazard_outputs come from an event based or scenario computation,
        populate the .epsilons and the .rupture_ids dictionaries.
        """
        if not self.epsilons_shape:
            self.calc_nbytes(hazard_outputs)
        if self.hc.calculation_mode == 'event_based':
            lt_model_ids = set(ho.output_container.lt_realization.lt_model.id
                               for ho in hazard_outputs)
            for lt_model_id in lt_model_ids:
                ses_coll = models.SESCollection.objects.get(
                    lt_model=lt_model_id)
                scid = ses_coll.id
                self.rupture_ids[scid] = ses_coll.get_ruptures(
                    ).values_list('id', flat=True)
                self.epsilons[scid] = make_epsilons(
                    len(self.asset_ids), len(self.rupture_ids[scid]),
                    self.rc.master_seed, self.rc.asset_correlation,
                    self.epsilons_management)
        elif self.hc.calculation_mode == 'scenario':
            self.rupture_ids[0] = []
            self.epsilons[0] = make_epsilons(
                len(self.asset_ids), self.hc.number_of_ground_motion_fields,
                self.rc.master_seed, self.rc.asset_correlation,
                self.epsilons_management)

    def _indices_asset_site(self, asset_block):
        """
        Filter the given assets by the asset_ids known to the builder
        and determine their indices.

        :param asset_block: a block of assets of the right taxonomy
        :returns: three lists of the same lenght indices, assets, site_ids
        """
        indices = []
        assets = []
        site_ids = []
        for asset in asset_block:
            assert asset.taxonomy == self.taxonomy, (
                asset.taxonomy, self.taxonomy)
            try:
                idx = self.asset_ids.index(asset.id)
            except ValueError:  # asset.id not in list
                logs.LOG.info(
                    "No hazard has been found for "
                    "the asset %s within %s km", asset,
                    self.rc.best_maximum_distance)
            else:
                site_ids.append(self.site_ids[idx])
                assets.append(asset)
                indices.append(idx)
        return indices, assets, site_ids

    def make_getters(self, gettercls, hazard_outputs, asset_block):
        """
        Build the appropriate hazard getters from the given hazard
        outputs. The assets which have no corresponding hazard site
        within the maximum distance are discarded. A RuntimeError is
        raised if all assets are discarded. From outputs coming from
        an event based or a scenario calculation the right epsilons
        corresponding to the assets are stored in the getters.

        :param gettercls: the HazardGetter subclass to use
        :param hazard_outputs: the outputs of a hazard calculation
        :param asset_block: a block of assets

        :returns: a list of HazardGetter instances
        """
        indices, assets, site_ids = self._indices_asset_site(asset_block)
        if not indices:
            raise AssetSiteAssociationError(
                'Could not associated any asset in %s to '
                'hazard sites within the distance of %s km'
                % (asset_block, self.rc.best_maximum_distance))
        if not self.epsilons:
            self.init_epsilons(hazard_outputs)
        getters = []
        for ho in hazard_outputs:
            getter = gettercls(ho, assets, site_ids)
            if self.hc.calculation_mode == 'event_based':
                ses_coll_id = models.SESCollection.objects.get(
                    lt_model=ho.output_container.lt_realization.lt_model).id
                getter.rupture_ids = self.rupture_ids[ses_coll_id]
                getter.epsilons = self.epsilons[ses_coll_id][indices]
            elif self.hc.calculation_mode == 'scenario':
                getter.num_samples = self.epsilons_shape[0][1]
                getter.epsilons = self.epsilons[0][indices]
            getters.append(getter)
        return getters

########NEW FILE########
__FILENAME__ = loaders
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
I/O handling for risk calculators
"""

import collections
from openquake.risklib import scientific

from openquake.nrmllib.risk import parsers
from openquake.engine.input.exposure import ExposureDBWriter
from openquake.engine.db.models import DmgState


def exposure(job, exposure_model_input):
    """
    Load exposure assets and write them to database.

    :param exposure_model_input:
        the pathname to an exposure file
    """
    return ExposureDBWriter(job).serialize(
        parsers.ExposureModelParser(exposure_model_input))


def vulnerability(vulnerability_file):
    """
    :param vulnerability_file:
        the pathname to a vulnerability file
    :returns:
        a dictionary {taxonomy: (imt, vulnerability_function)}
    :raises:
        * `ValueError` if validation of any vulnerability function fails
    """
    vfs = {}
    for record in parsers.VulnerabilityModelParser(vulnerability_file):
        taxonomy = record['ID']
        imt = record['IMT']
        loss_ratios = record['lossRatio']
        covs = record['coefficientsVariation']
        distribution = record['probabilisticDistribution']

        if taxonomy in vfs:
            raise ValueError("Error creating vulnerability function for "
                             "taxonomy %s. A taxonomy can not "
                             "be associated with "
                             "different vulnerability functions" % taxonomy)
        try:
            vfs[taxonomy] = scientific.VulnerabilityFunction(
                imt, record['IML'], loss_ratios, covs, distribution)
        except ValueError, err:
            msg = "Invalid vulnerability function with ID '%s': %s" % (
                taxonomy, err.message)
            raise ValueError(msg)

    return vfs


def fragility(risk_calculation, fragility_input):
    damage_states, data = _parse_fragility(fragility_input)

    for lsi, dstate in enumerate(damage_states):
        DmgState.objects.get_or_create(
            risk_calculation=risk_calculation, dmg_state=dstate, lsi=lsi)
    damage_state_ids = [d.id for d in DmgState.objects.filter(
        risk_calculation=risk_calculation).order_by('lsi')]

    return data, damage_state_ids


class List(list):
    """
    Class to store lists of objects with common attributes
    """
    def __init__(self, elements, **attrs):
        list.__init__(self, elements)
        vars(self).update(attrs)


def _parse_fragility(content):
    """
    Parse the fragility XML file and return fragility_model,
    fragility_functions, and damage_states for usage in get_risk_models.
    """
    iterparse = iter(parsers.FragilityModelParser(content))
    fmt, limit_states = iterparse.next()

    damage_states = ['no_damage'] + limit_states
    fragility_functions = collections.defaultdict(dict)

    tax_imt = dict()
    for taxonomy, iml, params, no_damage_limit in iterparse:
        tax_imt[taxonomy] = iml['IMT']

        if fmt == "discrete":
            if no_damage_limit is None:
                fragility_functions[taxonomy] = [
                    scientific.FragilityFunctionDiscrete(
                        iml['imls'], poes, iml['imls'][0])
                    for poes in params]
            else:
                fragility_functions[taxonomy] = [
                    scientific.FragilityFunctionDiscrete(
                        [no_damage_limit] + iml['imls'], [0.0] + poes,
                        no_damage_limit)
                    for poes in params]
        else:
            fragility_functions[taxonomy] = [
                scientific.FragilityFunctionContinuous(*mean_stddev)
                for mean_stddev in params]

    data = [(tax, dict(damage=List(ffs, imt=tax_imt[tax])))
            for tax, ffs in fragility_functions.items()]
    return damage_states, data

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Core functionality for the scenario risk calculator.
"""
import itertools
import numpy
from django import db

from openquake.risklib import workflows
from openquake.engine.calculators.risk import (
    base, hazard_getters, validation, writers)
from openquake.engine.db import models
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks


@tasks.oqtask
def scenario(job_id, risk_model, outputdict, _params):
    """
    Celery task for the scenario risk calculator.

    :param int job_id:
      ID of the currently running job
    :param list risk_models:
      A list of :class:`openquake.risklib.workflows.CalculationUnit` instances
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (in this case only `LossMap`)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
    """
    monitor = EnginePerformanceMonitor(None, job_id, scenario, tracing=True)
    with db.transaction.commit_on_success(using='job_init'):
        return do_scenario(risk_model, outputdict, monitor)


def do_scenario(risk_model, outputdict, monitor):
    """
    See `scenario` for a description of the input parameters
    """
    out = risk_model.compute_outputs(monitor.copy('getting data'))
    agg, ins = {}, {}
    for loss_type, [output] in out.iteritems():
        outputdict = outputdict.with_args(
            loss_type=loss_type, output_type="loss_map")

        (assets, loss_ratio_matrix, aggregate_losses,
         insured_loss_matrix, insured_losses) = output.output
        agg[loss_type] = aggregate_losses
        ins[loss_type] = insured_losses

        with monitor.copy('saving risk outputs'):
            outputdict.write(
                assets,
                loss_ratio_matrix.mean(axis=1),
                loss_ratio_matrix.std(ddof=1, axis=1),
                hazard_output_id=risk_model.getters[0].hid,
                insured=False)

            if insured_loss_matrix is not None:
                outputdict.write(
                    assets,
                    insured_loss_matrix.mean(axis=1),
                    insured_loss_matrix.std(ddof=1, axis=1),
                    itertools.cycle([True]),
                    hazard_output_id=risk_model.getters[0].hid,
                    insured=True)

    return agg, ins


class ScenarioRiskCalculator(base.RiskCalculator):
    """
    Scenario Risk Calculator. Computes a Loss Map,
    for a given set of assets.
    """

    core_calc_task = scenario

    validators = base.RiskCalculator.validators + [
        validation.RequireScenarioHazard,
        validation.ExposureHasInsuranceBounds,
        validation.ExposureHasTimeEvent]

    output_builders = [writers.LossMapBuilder]

    getter_class = hazard_getters.ScenarioGetter

    def __init__(self, job):
        super(ScenarioRiskCalculator, self).__init__(job)
        self.acc = ({}, {})  # aggregate_losses and insured_losses accumulators

    def agg_result(self, acc, task_result):
        aggregate_losses_acc, insured_losses_acc = acc[0].copy(), acc[1].copy()
        aggregate_losses_dict, insured_losses_dict = task_result

        for loss_type in self.loss_types:
            aggregate_losses = aggregate_losses_dict.get(loss_type)

            if aggregate_losses is not None:
                if aggregate_losses_acc.get(loss_type) is None:
                    aggregate_losses_acc[loss_type] = (
                        numpy.zeros(aggregate_losses.shape))
                aggregate_losses_acc[loss_type] += aggregate_losses

        if self.rc.insured_losses:
            for loss_type in self.loss_types:
                insured_losses = insured_losses_dict.get(
                    loss_type)
                if insured_losses is not None:
                    if insured_losses_acc.get(loss_type) is None:
                        insured_losses_acc[loss_type] = numpy.zeros(
                            insured_losses.shape)
                    insured_losses_acc[loss_type] += insured_losses
        return aggregate_losses_acc, insured_losses_acc

    def post_process(self):
        aggregate_losses_acc, insured_losses_acc = self.acc
        for loss_type, aggregate_losses in aggregate_losses_acc.items():
            with db.transaction.commit_on_success(using='job_init'):
                models.AggregateLoss.objects.create(
                    output=models.Output.objects.create_output(
                        self.job,
                        "aggregate loss. type=%s" % loss_type,
                        "aggregate_loss"),
                    loss_type=loss_type,
                    mean=numpy.mean(aggregate_losses),
                    std_dev=numpy.std(aggregate_losses, ddof=1))

                if self.rc.insured_losses:
                    insured_losses = insured_losses_acc[loss_type]
                    models.AggregateLoss.objects.create(
                        output=models.Output.objects.create_output(
                            self.job,
                            "insured aggregate loss. type=%s" % loss_type,
                            "aggregate_loss"),
                        insured=True,
                        loss_type=loss_type,
                        mean=numpy.mean(insured_losses),
                        std_dev=numpy.std(insured_losses, ddof=1))

    def get_workflow(self, vulnerability_functions):
        return workflows.Scenario(
            vulnerability_functions, self.rc.insured_losses)

########NEW FILE########
__FILENAME__ = core
#  -*- coding: utf-8 -*-
#  vim: tabstop=4 shiftwidth=4 softtabstop=4

#  Copyright (c) 2010-2014, GEM Foundation

#  OpenQuake is free software: you can redistribute it and/or modify it
#  under the terms of the GNU Affero General Public License as published
#  by the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.

#  OpenQuake is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.

#  You should have received a copy of the GNU Affero General Public License
#  along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core functionality for the scenario_damage risk calculator.
"""

import numpy

from django import db

from openquake.risklib import workflows
from openquake.risklib.workflows import RiskModel

from openquake.engine.calculators.risk import (
    base, hazard_getters, writers, validation, loaders)
from openquake.engine.performance import EnginePerformanceMonitor
from openquake.engine.utils import tasks
from openquake.engine.db import models


@tasks.oqtask
def scenario_damage(job_id, risk_model, outputdict, params):
    """
    Celery task for the scenario damage risk calculator.

    :param int job_id:
      ID of the currently running job
    :param list units:
      A list of :class:`openquake.risklib.workflows.CalculationUnit` instances
    :param outputdict:
      An instance of :class:`..writers.OutputDict` containing
      output container instances (in this case only `LossMap`)
    :param params:
      An instance of :class:`..base.CalcParams` used to compute
      derived outputs
   :returns:
      A matrix of fractions and a taxonomy string
    """
    monitor = EnginePerformanceMonitor(
        None, job_id, scenario_damage, tracing=True)
    # in scenario damage calculation the only loss_type is 'damage'
    [getter] = risk_model.getters
    [ffs] = risk_model.vulnerability_functions

    # and NO containes
    assert len(outputdict) == 0
    with db.transaction.commit_on_success(using='job_init'):

        with monitor.copy('getting hazard'):
            ground_motion_values = getter.get_data(ffs.imt)

        with monitor.copy('computing risk'):
            fractions = risk_model.workflow(ground_motion_values)
            aggfractions = sum(fractions[i] * asset.number_of_units
                               for i, asset in enumerate(getter.assets))

        with monitor.copy('saving damage per assets'):
            writers.damage_distribution(
                getter.assets, fractions, params.damage_state_ids)

        return aggfractions, risk_model.taxonomy


class ScenarioDamageRiskCalculator(base.RiskCalculator):
    """
    Scenario Damage Risk Calculator. Computes four kinds of damage
    distributions: per asset, per taxonomy, total and collapse map.

    :attr dict fragility_functions:
        A dictionary of dictionary mapping taxonomy ->
        (limit state -> fragility function) where a fragility function is an
        instance of
        :class:`openquake.risklib.scientific.FragilityFunctionContinuous` or
        :class:`openquake.risklib.scientific.FragilityFunctionDiscrete`.
    """

    #: The core calculation celery task function
    core_calc_task = scenario_damage
    validators = [validation.HazardIMT, validation.EmptyExposure,
                  validation.OrphanTaxonomies,
                  validation.NoRiskModels, validation.RequireScenarioHazard]

    # FIXME. scenario damage calculator does not use output builders
    output_builders = []
    getter_class = hazard_getters.ScenarioGetter

    def __init__(self, job):
        super(ScenarioDamageRiskCalculator, self).__init__(job)
        # let's define a dictionary taxonomy -> fractions
        # updated in task_completed method when the fractions per taxonomy
        # becomes available, as computed by the workers
        self.acc = {}
        self.damage_state_ids = None

    def get_workflow(self, fragility_functions):
        return workflows.Damage(fragility_functions)

    def agg_result(self, acc, task_result):
        """
        Update the dictionary acc, i.e. aggregate the damage distribution
        by taxonomy; called every time a block of assets is computed for each
        taxonomy. Fractions and taxonomy are extracted from task_result

        :param task_result:
            A pair (fractions, taxonomy)
        """
        acc = acc.copy()
        fractions, taxonomy = task_result
        if fractions is not None:
            if taxonomy not in acc:
                acc[taxonomy] = numpy.zeros(fractions.shape)
            acc[taxonomy] += fractions
        return acc

    def post_process(self):
        """
        Save the damage distributions by taxonomy and total on the db.
        """

        models.Output.objects.create_output(
            self.job, "Damage Distribution per Asset",
            "dmg_dist_per_asset")

        models.Output.objects.create_output(
            self.job, "Collapse Map per Asset",
            "collapse_map")

        if self.acc:
            models.Output.objects.create_output(
                self.job, "Damage Distribution per Taxonomy",
                "dmg_dist_per_taxonomy")

        tot = None
        for taxonomy, fractions in self.acc.iteritems():
            writers.damage_distribution_per_taxonomy(
                fractions, self.damage_state_ids, taxonomy)
            if tot is None:  # only the first time
                tot = numpy.zeros(fractions.shape)
            tot += fractions

        if tot is not None:
            models.Output.objects.create_output(
                self.job, "Damage Distribution Total",
                "dmg_dist_total")
            writers.total_damage_distribution(tot, self.damage_state_ids)

    def get_risk_models(self):
        """
        Load fragility model and store damage states
        """
        data, damage_state_ids = loaders.fragility(
            self.rc, self.rc.inputs['fragility'])
        self.damage_state_ids = damage_state_ids
        self.loss_types.add('damage')  # single loss_type
        risk_models = {}
        for taxonomy, ffs in data:
            risk_models[taxonomy] = RiskModel(taxonomy, self.get_workflow(ffs))

        return risk_models

    @property
    def calculator_parameters(self):
        """
        Provides calculator specific params coming from
        :class:`openquake.engine.db.RiskCalculation`
        """

        return base.make_calc_params(damage_state_ids=self.damage_state_ids)

########NEW FILE########
__FILENAME__ = validation
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Custom validation module for risk calculators
"""

from openquake.engine.db import models


class Validator(object):
    """
    abstract class describing a risk calculator validator
    """
    def __init__(self, risk_calculator):
        self.calc = risk_calculator

    def get_error(self):
        raise NotImplementedError


class HazardIMT(Validator):
    """
    Check that a proper hazard output exists in any of the
    intensity measure types given in the risk models
    """
    def get_error(self):
        model_imts = set()
        for rm in self.calc.risk_models.values():
            model_imts.update(vf.imt for vf in rm.vulnerability_functions)
        imts = self.calc.hc.get_imts()

        # check that the hazard data have all the imts needed by the
        # risk calculation
        missing = set(model_imts) - set(imts)

        if missing:
            return ("There is no hazard output for: %s. "
                    "The available IMTs are: %s." % (", ".join(missing),
                                                     ", ".join(imts)))


class EmptyExposure(Validator):
    """
    Checks that at least one asset is in the exposure
    """
    def get_error(self):
        if not sum(self.calc.taxonomies_asset_count.values()):
            return ('Region of interest is not covered by the exposure input. '
                    'This configuration is invalid. '
                    'Change the region constraint input or use a proper '
                    'exposure')


class OrphanTaxonomies(Validator):
    """
    Checks that the taxonomies in the risk models match with the ones
    in the exposure.
    """
    def get_error(self):
        taxonomies = self.calc.taxonomies_asset_count
        orphans = set(taxonomies) - set(self.calc.risk_models)
        if orphans and not self.calc.rc.taxonomies_from_model:
            return ('The following taxonomies are in the exposure model '
                    'but not in the risk model: %s' % orphans)


class ExposureLossTypes(Validator):
    """
    Check that the exposure has all the cost informations for the loss
    types given in the risk models
    """
    def get_error(self):
        for loss_type in self.calc.loss_types:
            if not self.calc.rc.exposure_model.supports_loss_type(loss_type):
                return ("Invalid exposure "
                        "for computing loss type %s. " % loss_type)


class NoRiskModels(Validator):
    def get_error(self):
        if not self.calc.risk_models:
            return 'At least one risk model of type %s must be defined' % (
                models.LOSS_TYPES)


class RequireClassicalHazard(Validator):
    """
    Checks that the given hazard has hazard curves
    """
    def get_error(self):
        rc = self.calc.rc

        if rc.hazard_calculation:
            if rc.hazard_calculation.calculation_mode != 'classical':
                return ("The provided hazard calculation ID "
                        "is not a classical calculation")
        elif not rc.hazard_output.is_hazard_curve():
            return "The provided hazard output is not an hazard curve"


class RequireScenarioHazard(Validator):
    """
    Checks that the given hazard has ground motion fields got from a
    scenario hazard calculation
    """
    def get_error(self):
        rc = self.calc.rc

        if rc.hazard_calculation:
            if rc.hazard_calculation.calculation_mode != "scenario":
                return ("The provided hazard calculation ID "
                        "is not a scenario calculation")
        elif not rc.hazard_output.output_type == "gmf_scenario":
            return "The provided hazard is not a gmf scenario collection"


class RequireEventBasedHazard(Validator):
    """
    Checks that the given hazard has ground motion fields (or
    stochastic event set) got from a event based hazard calculation
    """
    def get_error(self):
        rc = self.calc.rc

        if rc.hazard_calculation:
            if rc.hazard_calculation.calculation_mode != "event_based":
                return ("The provided hazard calculation ID "
                        "is not a event based calculation")
        elif not rc.hazard_output.output_type in ["gmf", "ses"]:
            return "The provided hazard is not a gmf or ses collection"

        if rc.hazard_outputs()[0].output_type == "ses":
            if 'source_model_logic_tree' not in rc.inputs:
                return ("source_model_logic_tree_file and "
                        "gsim_logic_tree_file "
                        "is mandatory "
                        "when the hazard output is a ses collection")


class ExposureHasInsuranceBounds(Validator):
    """
    If insured losses are required we check for the presence of
    the deductible and insurance limit
    """

    def get_error(self):
        if (self.calc.rc.insured_losses and
                not self.calc.rc.exposure_model.has_insurance_bounds()):
            return "Deductible or insured limit missing in exposure"


class ExposureHasRetrofittedCosts(Validator):
    """
    Check that the retrofitted value is present in the exposure
    """
    def get_error(self):
        if not self.calc.rc.exposure_model.has_retrofitted_costs():
            return "Some assets do not have retrofitted costs"


class ExposureHasTimeEvent(Validator):
    """
    If fatalities are considered check that the exposure has the
    proper time_event
    """

    def get_error(self):
        if (self.calc.rc.vulnerability_input("occupants") is not None and
            not self.calc.rc.exposure_model.has_time_event(
                self.calc.rc.time_event)):
            return ("Some assets are missing an "
                    "occupancy with period=%s" % self.calc.rc.time_event)

########NEW FILE########
__FILENAME__ = writers
# -*- coding: utf-8 -*-

# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""DB writing functionality for Risk calculators."""

import collections
import itertools
from openquake.risklib import scientific
from openquake.engine.db import models


def loss_map(
        loss_type, loss_map_id, assets, losses, std_devs=None, absolute=False):
    """
    Create :class:`openquake.engine.db.models.LossMapData`

    :param int loss_map_id:
        The ID of the output container.
    :param list assets:
        A list of instances of :class:`openquake.engine.db.models.ExposureData`
    :param loss:
        Loss values to be written.
    :param float std_devs:
        Standard devations on each loss.
    :param absolute:
        False if the provided losses are loss ratios
    """

    for i, asset in enumerate(assets):
        loss = losses[i]
        if std_devs is not None:
            std_dev = std_devs[i]
        else:
            std_dev = None

        if not absolute:
            loss *= asset.value(loss_type)
            if std_devs is not None:
                std_dev *= asset.value(loss_type)

        models.LossMapData.objects.create(
            loss_map_id=loss_map_id,
            asset_ref=asset.asset_ref,
            value=loss,
            std_dev=std_dev,
            location=asset.site)


def bcr_distribution(loss_type, bcr_distribution_id, assets, bcr_data):
    """
    Create a new :class:`openquake.engine.db.models.BCRDistributionData` from
    `asset_output` and links it to the output container identified by
    `bcr_distribution_id`.

    :param int bcr_distribution_id:
        The ID of :class:`openquake.engine.db.models.BCRDistribution` instance
        that holds the BCR map.

    :param assets:
        A list of instance of :class:`openquake.engine.db.models.ExposureData`

    :param tuple bcr_data: a 3-tuple with
      1) eal_original: expected annual loss in the original model
      2) eal_retrofitted: expected annual loss in the retrofitted model
      3) bcr: Benefit Cost Ratio parameter.
    """
    for asset, (eal_original, eal_retrofitted, bcr) in zip(assets, bcr_data):
        models.BCRDistributionData.objects.create(
            bcr_distribution_id=bcr_distribution_id,
            asset_ref=asset.asset_ref,
            average_annual_loss_original=eal_original * asset.value(loss_type),
            average_annual_loss_retrofitted=(eal_retrofitted *
                                             asset.value(loss_type)),
            bcr=bcr,
            location=asset.site)


def loss_curve(loss_type, loss_curve_id, assets, curve_data):
    """
    Store :class:`openquake.engine.db.models.LossCurveData`
    where the
    :class:`openquake.engine.db.models.LossCurve` output container is
    identified by `loss_curve_id` and has type `loss_curve` (produced by
    a classical calculation).

    :param str loss_type:
        The loss type of the curve
    :param int loss_curve_id:
        The ID of the output container.
    :param assets:
        A list of N :class:`openquake.engine.db.models.ExposureData` instances
    :param tuple curve_data:
        A tuple of the form (curves, averages) holding a numpy array with N
        loss curve data and N average loss value associated with the curve
    """

    curves, averages = curve_data
    for asset, (losses, poes), average in itertools.izip(
            assets, curves, averages):
        models.LossCurveData.objects.create(
            loss_curve_id=loss_curve_id,
            asset_ref=asset.asset_ref,
            location=asset.site,
            poes=poes,
            loss_ratios=losses,
            asset_value=asset.value(loss_type),
            average_loss_ratio=average,
            stddev_loss_ratio=None)


def event_loss_curve(loss_type, loss_curve_id, assets, curve_data):
    """
    Store :class:`openquake.engine.db.models.LossCurveData`
    where the
    :class:`openquake.engine.db.models.LossCurve` output container is
    identified by `loss_curve_id` and the output type is `event_loss_curve`
    .

    :param str loss_type:
        The loss type of the curve
    :param int loss_curve_id:
        The ID of the output container.
    :param assets:
        A list of N :class:`openquake.engine.db.models.ExposureData` instances
    :param tuple curve_data:
        A tuple of the form (curves, averages, stddevs) holding a numpy array
        loss curve data and N average loss value associated with the curve
    """

    curves, averages, stddevs = curve_data
    for asset, (losses, poes), average, stddev in itertools.izip(
            assets, curves, averages, stddevs):
        models.LossCurveData.objects.create(
            loss_curve_id=loss_curve_id,
            asset_ref=asset.asset_ref,
            location=asset.site,
            poes=poes,
            loss_ratios=losses,
            asset_value=asset.value(loss_type),
            average_loss_ratio=average,
            stddev_loss_ratio=stddev)


def loss_fraction(loss_type, loss_fraction_id, assets, values, fractions):
    """
    Create, save and return an instance of
    :class:`openquake.engine.db.models.LossFractionData` associated
    with `loss_fraction_id`, `value`, `location` and `absolute_loss`
    :param int loss_fraction_id:
       an ID to an output container instance
       of type :class:`openquake.engine.db.models.LossFraction
    :param list values:
       A list of value representing the fraction. In case of
       disaggregation by taxonomy it is a taxonomy string.
    :param assets: the assets, the fractions refer to
    :param absolute_losses:
       the absolute loss contributions of `values` in `assets`
    """
    for asset, value, fraction in itertools.izip(assets, values, fractions):
        models.LossFractionData.objects.create(
            loss_fraction_id=loss_fraction_id,
            value=value,
            location=asset.site,
            absolute_loss=fraction * asset.value(loss_type))


###
### Damage Distributions
###

def damage_distribution(assets, fraction_matrix, dmg_state_ids):
    """
    Save the damage distribution for a given asset.
    :param assets:
       a list of ExposureData instances
    :param fraction_matrix:
       numpy array with the damage fractions for each asset
    :param dmg_state_ids:
       a list of  IDs of instances of
       :class:`openquake.engine.db.models.DmgState` ordered by `lsi`
    """
    for fractions, asset in zip(fraction_matrix, assets):
        fractions *= asset.number_of_units
        means, stds = scientific.mean_std(fractions)

        for mean, std, dmg_state_id in zip(means, stds, dmg_state_ids):
            models.DmgDistPerAsset.objects.create(
                dmg_state_id=dmg_state_id,
                mean=mean, stddev=std, exposure_data=asset)


def damage_distribution_per_taxonomy(fractions, dmg_state_ids, taxonomy):
    """
    Save the damage distribution for a given taxonomy, by summing over
    all assets.

    :param fractions: numpy array with the damage fractions
    :param dmg_state_ids:
       a list of  IDs of instances of
       :class:`openquake.engine.db.models.DmgState` ordered by `lsi`
    :param str: the taxonomy string
    """
    means, stddevs = scientific.mean_std(fractions)
    for dmg_state_id, mean, stddev in zip(dmg_state_ids, means, stddevs):
        models.DmgDistPerTaxonomy.objects.create(
            dmg_state_id=dmg_state_id,
            mean=mean, stddev=stddev, taxonomy=taxonomy)


def total_damage_distribution(fractions, dmg_state_ids):
    """
    Save the total distribution, by summing over all assets and taxonomies.

    :param fractions: numpy array with the damage fractions
    :param dmg_state_ids:
       a list of  IDs of instances of
       :class:`openquake.engine.db.models.DmgState` ordered by `lsi`
    """
    means, stds = scientific.mean_std(fractions)
    for mean, std, dmg_state in zip(means, stds, dmg_state_ids):
        models.DmgDistTotal.objects.create(
            dmg_state_id=dmg_state, mean=mean, stddev=std)


# A namedtuple that identifies an Output object in a risk calculation
# E.g. A Quantile LossCurve associated with a specific hazard output is
# OutputKey(output_type="loss_curve",
#           loss_type="structural",
#           hazard_output_id=foo,
#           poe=None,
#           quantile=bar,
#           statistics="quantile",
#           variable=None,
#           insured=False)

OutputKey = collections.namedtuple('OutputKey', [
    'output_type',  # as in :class:`openquake.engine.db.models.Output`
    'loss_type',  # as in risk output outputdict
    'hazard_output_id',  # as in risk output outputdict
    'poe',  # for loss map and classical loss fractions
    'quantile',  # for quantile outputs
    'statistics',  # as in risk output outputdict
    'variable',  # for disaggregation outputs
    'insured',  # as in :class:`openquake.engine.db.models.LossCurve`
])


class OutputDict(dict):
    """
    A dict keying OutputKey instances to database ID, with convenience
    setter and getter methods to manage Output outputdict.

    It also automatically links an Output type with its specific
    writer.

    Risk Calculators create OutputDict instances with Output IDs keyed
    by OutputKey instances.

    Worker tasks compute results than get the proper writer and use it
    to actually write the results
    """

    def get(self,
            output_type=None, loss_type=None, hazard_output_id=None, poe=None,
            quantile=None, statistics=None, variable=None, insured=False):
        """
        Get the ID associated with the `OutputKey` instance built with the
        given kwargs.
        """
        return self[OutputKey(output_type, loss_type, hazard_output_id, poe,
                              quantile, statistics, variable, insured)]

    def with_args(self, **kwargs):
        clone = self.__class__(self)
        clone.kwargs = self.kwargs
        clone.kwargs.update(kwargs)
        return clone

    def __init__(self, *args, **kwargs):
        super(OutputDict, self).__init__(*args, **kwargs)
        self.kwargs = dict()

    def write(self, *args, **kwargs):
        """
        1) Get the ID associated with the `OutputKey` instance built with
        the given kwargs.
        2) Get a writer function from the `writers` module with
        function name given by the `output_type` argument.
        3) Call such function with the given positional arguments.
        """

        kwargs.update(self.kwargs)
        output_id = self.get(**kwargs)
        globals().get(kwargs['output_type'])(
            kwargs.pop('loss_type'), output_id, *args)

    def write_all(self, arg, values, items,
                  *initial_args, **initial_kwargs):
        """
        Call iteratively `write`.

        In each call, the keyword arguments are built by merging
        `initial_kwargs` with a dict storing the association between
        `arg` and the value taken iteratively from `values`. The
        positional arguments are built by chaining `initial_args` with
        a value taken iteratively from `items`.

        :param str arg: a keyword argument to be passed to `write`

        :param list values: a list of keyword argument values to be
        passed to `write`

        :param list items: a list of positional arguments to be passed
        to `write`
        """
        if not len(values) or not len(items):
            return
        for value, item in itertools.izip(values, items):
            kwargs = {arg: value}
            kwargs.update(initial_kwargs)
            args = list(initial_args) + [item]
            self.write(*args, **kwargs)

    def set(self, container):
        """Store an ID (got from `container`) keyed by a new
        `OutputKey` built with the attributes guessed on `container`

        :param container: a django model instance of an output
        container (e.g. a LossCurve)
        """
        hazard_output_id = getattr(container, "hazard_output_id")
        loss_type = getattr(container, "loss_type")
        poe = getattr(container, "poe", None)
        quantile = getattr(container, "quantile", None)
        statistics = getattr(container, "statistics", None)
        variable = getattr(container, "variable", None)
        insured = getattr(container, "insured", False)

        key = OutputKey(
            output_type=container.output.output_type,
            loss_type=loss_type,
            hazard_output_id=hazard_output_id,
            poe=poe,
            quantile=quantile,
            statistics=statistics,
            variable=variable,
            insured=insured)

        assert super(OutputDict, self).get(
            key, None) is None, "OutputDict can not be updated"

        self[key] = container.id

    def extend(self, output_list):
        for o in output_list:
            self.set(o)
        return self


class OutputBuilder(object):
    def __init__(self, calculator):
        self.calc = calculator

    def statistical_outputs(self, _loss_type):
        return OutputDict()

    def individual_outputs(self, _loss_type, _hazard_output):
        return OutputDict()


def combine_builders(builders):
    outputs = OutputDict()

    if not builders:
        return outputs

    a_builder = builders[0]

    hazard_outputs = a_builder.calc.rc.hazard_outputs()

    for builder in builders:
        for loss_type in a_builder.calc.loss_types:

            if len(hazard_outputs) > 1:
                outputs.extend(builder.statistical_outputs(loss_type))

            for hazard in hazard_outputs:
                outputs.extend(builder.individual_outputs(loss_type, hazard))

    return outputs


class LossCurveMapBuilder(OutputBuilder):
    """
    Create output outputdict for Loss Curves, Insured Loss Curves and
    Loss Maps
    """
    LOSS_CURVE_TYPE = "loss_curve"

    def individual_outputs(self, loss_type, hazard_output):
        lc = [models.LossCurve.objects.create(
            hazard_output_id=hazard_output.id,
            loss_type=loss_type,
            output=models.Output.objects.create_output(
                self.calc.job,
                "loss curves. type=%s, hazard=%s" % (
                    loss_type, hazard_output.id), self.LOSS_CURVE_TYPE))]

        maps = [models.LossMap.objects.create(
            hazard_output_id=hazard_output.id,
            loss_type=loss_type,
            output=models.Output.objects.create_output(
                self.calc.job,
                "loss maps. type=%s poe=%s, hazard=%s" % (
                    loss_type, poe, hazard_output.id),
                "loss_map"), poe=poe)
                for poe in self.calc.rc.conditional_loss_poes or []]

        if loss_type != "fatalities" and self.calc.rc.insured_losses:
            ins = [
                models.LossCurve.objects.create(
                    insured=True,
                    loss_type=loss_type,
                    hazard_output=hazard_output,
                    output=models.Output.objects.create_output(
                        self.calc.job,
                        "insured loss curves. type=%s hazard %s" % (
                            loss_type, hazard_output),
                        self.LOSS_CURVE_TYPE))]
        else:
            ins = []

        return lc + maps + ins

    def statistical_outputs(self, loss_type):
        mean_loss_curve = [models.LossCurve.objects.create(
            output=models.Output.objects.create_output(
                job=self.calc.job,
                display_name='Mean Loss Curves. type=%s' % loss_type,
                output_type='loss_curve'),
            statistics='mean', loss_type=loss_type)]

        if loss_type != "fatalities" and self.calc.rc.insured_losses:
            mean_insured_loss_curve = [models.LossCurve.objects.create(
                output=models.Output.objects.create_output(
                    job=self.calc.job,
                    display_name='Mean Insured Curves. type=%s' % loss_type,
                    output_type='loss_curve'),
                statistics='mean', insured=True, loss_type=loss_type)]
        else:
            mean_insured_loss_curve = []

        quantile_loss_curves = []
        quantile_insured_loss_curves = []
        for quantile in self.calc.rc.quantile_loss_curves or []:
            quantile_loss_curves.append(models.LossCurve.objects.create(
                output=models.Output.objects.create_output(
                    job=self.calc.job,
                    display_name='%s Quantile Loss Curves. type=%s' % (
                        quantile, loss_type),
                    output_type='loss_curve'),
                statistics='quantile',
                quantile=quantile,
                loss_type=loss_type))
            if loss_type != "fatalities" and self.calc.rc.insured_losses:
                quantile_insured_loss_curves.append(
                    models.LossCurve.objects.create(
                        output=models.Output.objects.create_output(
                            job=self.calc.job,
                            display_name=(
                                '%s Quantile Insured Loss Curves. type=%s' % (
                                    quantile, loss_type)),
                            output_type='loss_curve'),
                        statistics='quantile',
                        insured=True,
                        quantile=quantile,
                        loss_type=loss_type))

        mean_loss_maps = []
        for poe in self.calc.rc.conditional_loss_poes or []:
            mean_loss_maps.append(models.LossMap.objects.create(
                output=models.Output.objects.create_output(
                    job=self.calc.job,
                    display_name="Mean Loss Map type=%s poe=%.4f" % (
                        loss_type, poe),
                    output_type="loss_map"),
                statistics="mean",
                loss_type=loss_type,
                poe=poe))

        quantile_loss_maps = []
        for quantile in self.calc.rc.quantile_loss_curves or []:
            for poe in self.calc.rc.conditional_loss_poes or []:
                name = "%.4f Quantile Loss Map type=%s poe=%.4f" % (
                    quantile, loss_type, poe)
                quantile_loss_maps.append(models.LossMap.objects.create(
                    output=models.Output.objects.create_output(
                        job=self.calc.job,
                        display_name=name,
                        output_type="loss_map"),
                    statistics="quantile",
                    quantile=quantile,
                    loss_type=loss_type,
                    poe=poe))

        return (mean_loss_curve + quantile_loss_curves +
                mean_loss_maps + quantile_loss_maps +
                mean_insured_loss_curve + quantile_insured_loss_curves)


class EventLossCurveMapBuilder(LossCurveMapBuilder):
    LOSS_CURVE_TYPE = "event_loss_curve"


class LossMapBuilder(OutputBuilder):
    def individual_outputs(self, loss_type, hazard_output):
        loss_maps = [models.LossMap.objects.create(
            output=models.Output.objects.create_output(
                self.calc.job, "Loss Map", "loss_map"),
            hazard_output=hazard_output,
            loss_type=loss_type)]

        if self.calc.rc.insured_losses:
            loss_maps.append(models.LossMap.objects.create(
                output=models.Output.objects.create_output(
                    self.calc.job, "Insured Loss Map", "loss_map"),
                hazard_output=hazard_output,
                loss_type=loss_type,
                insured=True))

        return loss_maps


class BCRMapBuilder(OutputBuilder):
    def individual_outputs(self, loss_type, hazard_output):
        return [models.BCRDistribution.objects.create(
                hazard_output=hazard_output,
                loss_type=loss_type,
                output=models.Output.objects.create_output(
                    self.calc.job,
                    "BCR Map. type=%s hazard=%s" % (loss_type, hazard_output),
                    "bcr_distribution"))]


class LossFractionBuilder(OutputBuilder):
    def individual_outputs(self, loss_type, hazard_output):
        variables = ["magnitude_distance", "coordinate"]

        loss_fractions = []
        if self.calc.rc.sites_disagg:
            for variable in variables:
                name = ("loss fractions. type=%s variable=%s "
                        "hazard=%s" % (loss_type, hazard_output, variable))
                loss_fractions.append(
                    models.LossFraction.objects.create(
                        output=models.Output.objects.create_output(
                            self.calc.job, name, "loss_fraction"),
                        hazard_output=hazard_output,
                        loss_type=loss_type,
                        variable=variable))

        return loss_fractions


class ConditionalLossFractionBuilder(OutputBuilder):
    def statistical_outputs(self, loss_type):
        loss_fractions = []
        for poe in self.calc.rc.poes_disagg or []:
            loss_fractions.append(models.LossFraction.objects.create(
                variable="taxonomy",
                poe=poe,
                loss_type=loss_type,
                output=models.Output.objects.create_output(
                    job=self.calc.job,
                    display_name="Mean Loss Fractions. type=%s poe=%.4f" % (
                        loss_type, poe),
                    output_type="loss_fraction"),
                statistics="mean"))

        for quantile in self.calc.rc.quantile_loss_curves or []:
            for poe in self.calc.rc.poes_disagg or []:
                loss_fractions.append(models.LossFraction.objects.create(
                    variable="taxonomy",
                    poe=poe,
                    loss_type=loss_type,
                    output=models.Output.objects.create_output(
                        job=self.calc.job,
                        display_name=("%.4f Quantile Loss Fractions "
                                      "loss_type=%s poe=%.4f" % (
                                          quantile, loss_type, poe)),
                        output_type="loss_fraction"),
                    statistics="quantile",
                    quantile=quantile))

        return loss_fractions

    def individual_outputs(self, loss_type, hazard_output):
        loss_fractions = []

        for poe in self.calc.rc.poes_disagg or []:
            loss_fractions.append(models.LossFraction.objects.create(
                hazard_output_id=hazard_output.id,
                variable="taxonomy",
                loss_type=loss_type,
                output=models.Output.objects.create_output(
                    self.calc.job,
                    "loss fractions. type=%s poe=%s hazard=%s" % (
                        loss_type, poe, hazard_output.id),
                    "loss_fraction"),
                poe=poe))

        return loss_fractions

########NEW FILE########
__FILENAME__ = celery_node_monitor
#  -*- coding: utf-8 -*-
#  vim: tabstop=4 shiftwidth=4 softtabstop=4

#  Copyright (c) 2014, GEM Foundation

#  OpenQuake is free software: you can redistribute it and/or modify it
#  under the terms of the GNU Affero General Public License as published
#  by the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.

#  OpenQuake is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.

#  You should have received a copy of the GNU Affero General Public License
#  along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import sys
import time
import signal
import threading

import celery.task.control

from openquake.engine import logs
from openquake.engine.utils import config
from openquake.commonlib.general import str2bool


class MasterKilled(KeyboardInterrupt):
    """
    Exception raised when a job is killed manually or aborted
    by the `openquake.engine.engine.CeleryNodeMonitor`.
    """
    registered_handlers = False  # set when the signal handlers are registered

    @classmethod
    def handle_signal(cls, signum, _stack):
        """
        When a SIGTERM or a SIGABRT is received, raise the MasterKilled
        exception with an appropriate error message.

        :param int signum: the number of the received signal
        :param _stack: the current frame object, ignored
        """
        if signum == signal.SIGTERM:
            msg = 'The openquake master process was killed manually'
        elif signum == signal.SIGABRT:
            msg = ('The openquake master process was killed by the '
                   'CeleryNodeMonitor because some node failed')
        else:
            msg = 'This should never happen'
        raise cls(msg)

    @classmethod
    def register_handlers(cls):
        """
        Register the signal handlers for SIGTERM and SIGABRT
        if they were not registered before.
        """
        if not cls.registered_handlers:  # called only once
            signal.signal(signal.SIGTERM, cls.handle_signal)
            signal.signal(signal.SIGABRT, cls.handle_signal)
            cls.registered_handlers = True


class CeleryNodeMonitor(object):
    """
    Context manager wrapping a block of code with a monitor thread
    checking that the celery nodes are accessible. The check is
    performed periodically by pinging the nodes. If some node fail,
    for instance due to an out of memory error, a SIGABRT signal
    is sent to the master process.

    :param float interval:
        polling interval in seconds
    :param bool no_distribute:
        if True, the CeleryNodeMonitor will do nothing at all
    """
    def __init__(self, no_distribute, interval):
        self.interval = interval
        self.no_distribute = no_distribute
        self.job_running = True
        self.live_nodes = None  # set of live worker nodes
        self.th = None
        MasterKilled.register_handlers()

    def __enter__(self):
        if self.no_distribute:
            return self  # do nothing
        self.live_nodes = self.ping(timeout=1)
        if not self.live_nodes:
            print >> sys.stderr, "No live compute nodes, aborting calculation"
            sys.exit(2)
        self.th = threading.Thread(None, self.check_nodes)
        self.th.start()
        return self

    def __exit__(self, etype, exc, tb):
        self.job_running = False
        if self.th:
            self.th.join()

    def ping(self, timeout):
        """
        Ping the celery nodes by using .interval as timeout parameter
        """
        celery_inspect = celery.task.control.inspect(timeout=timeout)
        return set(celery_inspect.ping() or {})

    def check_nodes(self):
        """
        Check that the expected celery nodes are all up. The loop
        continues until the main thread keeps running.
        """
        while self.job_is_running(sleep=self.interval):
            live_nodes = self.ping(timeout=self.interval)
            if live_nodes < self.live_nodes:
                dead_nodes = list(self.live_nodes - live_nodes)
                logs.LOG.critical(
                    'Cluster nodes not accessible: %s', dead_nodes)
                terminate = str2bool(
                    config.get('celery', 'terminate_job_when_celery_is_down'))
                if terminate:
                    os.kill(os.getpid(), signal.SIGABRT)  # commit suicide

    def job_is_running(self, sleep):
        """
        Check for 10 times during the sleep interval if the flag
        self.job_running becomes false and then exit.
        """
        for _ in range(10):
            if not self.job_running:
                break
            time.sleep(sleep / 10.)
        return self.job_running

########NEW FILE########
__FILENAME__ = fields
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""Custom Django field and formfield types (for models and forms."""

import numpy
import re
import zlib
import json
import cPickle as pickle

from django.utils.encoding import smart_unicode
from django.contrib.gis import forms
from django.contrib.gis.db import models as djm

#: regex for splitting string lists on whitespace and/or commas
ARRAY_RE = re.compile(r'[\s,]+')

# Disable pylint for 'Too many public methods'
# pylint: disable=R0904


class StringArrayFormField(forms.Field):
    """
    Base class containing general functionality for handling list-like
    parameters.
    """
    # No change; customize this in subclasses if a cast is needed (to `float`,
    # for example)
    cast = str
    value_type = 'str'

    # Disable pylint for 'Method could be a function'
    # pylint: disable=R0201
    def clean(self, value):
        """Try to coerce either a string list of values (separated by
        whitespace and/or commas or a list/tuple of values to a list of
        floats. If unsuccessful, raise a
        :exc:`django.forms.ValidationError`
        """
        if value is None:
            return None

        if isinstance(value, (tuple, list)):
            try:
                value = [self.cast(x) for x in value]
            except (TypeError, ValueError):
                raise forms.ValidationError(
                    'Could not coerce sequence values to `float` values'
                )
        elif isinstance(value, str):
            # it could be a string list, like this: "1, 2,3 , 4 5"
            # try to convert it to a an actual list and cast the values to the
            # chosen type
            if len(value) == 0:
                # It's an empty string list
                value = []
            else:
                try:
                    value = [self.cast(x) for x in ARRAY_RE.split(value)]
                except ValueError:
                    raise forms.ValidationError(
                        'Could not coerce `str` to a list of `%s` values'
                        % self.value_type
                    )
        else:
            raise forms.ValidationError(
                'Could not convert value to `list` of `%s` values: %s'
                % (self.value_type, value)
            )
        return value


class FloatArrayFormField(StringArrayFormField):
    """Form field for properly handling float arrays/lists."""

    cast = float
    value_type = 'float'


class PickleFormField(forms.Field):
    """Form field for Python objects which are pickle and saved to the
    database."""

    def clean(self, value):
        """We assume that the Python value specified for this field is exactly
        what we want to pickle and save to the database.

        The value will not modified.
        """
        return value


class FloatArrayField(djm.Field):
    """This field models a postgres `float` array."""

    def db_type(self, connection):
        return 'float[]'

    def get_prep_value(self, value):
        if value is None:
            return None

        # Normally, the value passed in here will be a list.
        # It could also be a string list, each value separated by
        # comma/whitespace.
        if isinstance(value, str):
            if len(value) == 0:
                # It's an empty string list
                value = []
            else:
                # try to coerce the string to a list of floats
                value = [float(x) for x in ARRAY_RE.split(value)]
                # If there's an exception here, just let it be raised.
        return "{" + ', '.join(str(v) for v in value) + "}"

    def formfield(self, **kwargs):
        """Specify a custom form field type so forms know how to handle fields
        of this type.
        """
        defaults = {'form_class': FloatArrayFormField}
        defaults.update(kwargs)
        return super(FloatArrayField, self).formfield(**defaults)


class IntArrayField(djm.Field):
    """This field models a postgresql `int` array"""

    # TODO(lp) implement the corresponding form field
    def db_type(self, connection):
        return 'int[]'

    def get_prep_value(self, value):
        if value is None:
            return
        return "{%s}" % ','.join(str(v) for v in value)


class CharArrayField(djm.Field):
    """This field models a postgres `varchar` array."""

    def db_type(self, _connection=None):
        return 'varchar[]'

    def to_python(self, value):
        """
        Split strings on whitespace or commas and return the list.

        If the input ``value`` is not a string, just return the value (for
        example, if ``value`` is already a list).
        """
        if isinstance(value, basestring):
            return list(ARRAY_RE.split(value))

        return value

    def get_prep_value(self, value):
        """Return data in a format that has been prepared for use as a
        parameter in a query.

        :param value: sequence of string values to be saved in a varchar[]
            field
        :type value: list or tuple

        >>> caf = CharArrayField()
        >>> caf.get_prep_value(['foo', 'bar', 'baz123'])
        '{"foo", "bar", "baz123"}'
        """
        if value is None:
            return None

        # Normally, the value passed in here will be a list.
        # It could also be a string list, each value separated by
        # comma/whitespace.
        if isinstance(value, str):
            if len(value) == 0:
                # It's an empty string list
                value = []
            else:
                # try to coerce the string to a list of strings
                value = list(ARRAY_RE.split(value))
        return '{' + ', '.join('"%s"' % str(v) for v in value) + '}'

    def formfield(self, **kwargs):
        """
        Specify a custom form field type so forms know how to handle fields of
        this type.
        """
        defaults = {'form_class': StringArrayFormField}
        defaults.update(kwargs)
        return super(CharArrayField, self).formfield(**defaults)


class PickleField(djm.Field):
    """Field for transparent pickling and unpickling of python objects."""

    __metaclass__ = djm.SubfieldBase

    SUPPORTED_BACKENDS = set((
        'django.contrib.gis.db.backends.postgis',
        'django.db.backends.postgresql_psycopg2'
    ))

    def db_type(self, connection):
        """Return "bytea" as postgres' column type."""
        assert connection.settings_dict['ENGINE'] in self.SUPPORTED_BACKENDS
        return 'bytea'

    def to_python(self, value):
        """Unpickle the value."""
        if isinstance(value, (buffer, str, bytearray)) and value:
            return pickle.loads(str(value))
        else:
            return value

    def get_prep_value(self, value):
        """Pickle the value."""
        return bytearray(pickle.dumps(value, pickle.HIGHEST_PROTOCOL))

    def formfield(self, **kwargs):
        """Specify a custom form field type so forms don't treat this as a
        default type (such as a string). Any Python object is valid for this
        field type.
        """
        defaults = {'form_class': PickleFormField}
        defaults.update(kwargs)
        return super(PickleField, self).formfield(**defaults)


class GzippedField(djm.Field):
    """
    Automatically stores gzipped text as a bytearray
    """
    __metaclass__ = djm.SubfieldBase

    def db_type(self, _connection):
        return 'bytea'

    def get_prep_value(self, value):
        """Compress the value"""
        return bytearray(zlib.compress(value))

    def to_python(self, value):
        """Decompress the value"""
        return zlib.decompress(value)


class DictField(PickleField):
    """Field for storing Python `dict` objects (or a JSON text representation.
    """

    def to_python(self, value):
        """The value of a DictField can obviously be a `dict`. The value can
        also be specified as a JSON string. If it is, convert it to a `dict`.
        """
        if isinstance(value, str):
            try:
                value = json.loads(value)
            except ValueError:
                # This string is not JSON.
                value = super(DictField, self).to_python(value)
        else:
            value = super(DictField, self).to_python(value)

        return value


class NumpyListField(PickleField):
    """
    Field for storing numpy arrays as pickled blobs. The actual blob stored in
    the database is simply a pickled `list`. When the field is instantiated,
    the value is converted back to a numpy array.
    """

    def to_python(self, value):
        """
        Try to reconstruct a `numpy.ndarray` from the given ``value``.

        :param value:
            The pickled representation of an object which can be reconstituted
            using `pickle.loads`.
        """
        if value is None:
            return None

        # first, unpickle:
        value = super(NumpyListField, self).to_python(value)
        if isinstance(value, (list, tuple, numpy.ndarray)):
            return numpy.array(value)

        # NOTE: If the value is not a list or tuple, raise an exception.
        # The reason we do this is because this field type is intended to be
        # used only for storing list-like data. (Technically any object can be
        # wrapped in a numpy array, like `numpy.array(object())`, but this is
        # not our use case.
        raise ValueError(
            "Unexpected value of type '%s'. Expected 'list' or 'tuple'."
            % type(value)
        )

    def get_prep_value(self, value):
        """
        Convert the ``value`` to the pickled representation of a `list`. If
        ``value`` is a `numpy.ndarray`, it will be converted to a list of the
        same size and shape before being pickled.

        :param value:
            A `list`, `tuple`, or `numpy.ndarray`.
        """
        # convert to list first before pickling, if it's a numpy array
        if isinstance(value, numpy.ndarray):
            return super(NumpyListField, self).get_prep_value(value.tolist())
        else:
            if not isinstance(value, (list, tuple)):
                raise ValueError(
                    "Unexpected value of type '%s'. Expected 'list', 'tuple', "
                    "or 'numpy.ndarray'"
                    % type(value)
                )


class OqNullBooleanField(djm.NullBooleanField):
    """
    A `NullBooleanField` that can convert meaningful strings to boolean
    values (in the case of config file parameters).
    """

    def to_python(self, value):
        """
        If ``value`` is a `str`, try to extract some boolean value from it.
        """
        if isinstance(value, str):
            if value.lower() in ('t', 'true', 'y', 'yes'):
                value = True
            elif value.lower() in ('f', 'false', 'n', 'no'):
                value = False
        # otherwise, it will just get cast to a bool, which could produce some
        # strange results
        value = super(OqNullBooleanField, self).to_python(value)
        return value


class NullFloatField(djm.FloatField):
    """
    A nullable float field that handles blank input values properly.
    """

    def get_prep_value(self, value):
        if isinstance(value, basestring):
            if value.strip():
                return super(NullFloatField, self).get_prep_value(value)
            else:
                return None
        else:
            return value


class NullTextField(djm.TextField):
    def __init__(self, **kwargs):
        kwargs.update(dict(blank=True, null=True))
        super(NullTextField, self).__init__(**kwargs)

    def formfield(self, **kwargs):
        """
        Specify a custom form field type so forms know how to handle fields of
        this type.
        """
        defaults = {'form_class': NullCharField}
        defaults.update(kwargs)
        return super(NullTextField, self).formfield(**defaults)


class NullCharField(forms.CharField):
    def to_python(self, value):
        "Returns a Unicode object."
        if value is None:
            return None
        else:
            return smart_unicode(value)

########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

# Disable:
# - 'Maximum number of public methods for a class'
# - 'Missing docstring' (because of all of the model Meta)
# pylint: disable=R0904,C0111

'''
Model representations of the OpenQuake DB tables.
'''

import collections
import operator
import itertools
from datetime import datetime


import numpy
from scipy import interpolate

from django.db import transaction, connections
from django.core.exceptions import ObjectDoesNotExist

from django.contrib.gis.db import models as djm
from shapely import wkt

from openquake.hazardlib.imt import from_string
from openquake.hazardlib import source, geo
from openquake.hazardlib.calc import filters
from openquake.hazardlib.site import Site, SiteCollection

from openquake.commonlib.general import distinct
from openquake.engine.db import fields
from openquake.engine import writer

# source prefiltering is enabled for #sites <= FILTERING_THRESHOLD
FILTERING_THRESHOLD = 10000

#: Kind of supported curve statistics
STAT_CHOICES = (
    (u'mean', u'Mean'),
    (u'quantile', u'Quantile'))


#: System Reference ID used for geometry objects
DEFAULT_SRID = 4326


VS30_TYPE_CHOICES = (
    (u"measured", u"Value obtained from on-site measurements"),
    (u"inferred", u"Estimated value"),
)

IMT_CHOICES = (
    (u'PGA', u'Peak Ground Acceleration'),
    (u'PGV', u'Peak Ground Velocity'),
    (u'PGD', u'Peak Ground Displacement'),
    (u'SA', u'Spectral Acceleration'),
    (u'IA', u'Arias Intensity'),
    (u'RSD', u'Relative Significant Duration'),
    (u'MMI', u'Modified Mercalli Intensity'),
)

#: Default Loss Curve Resolution used for probabilistic risk calculators
DEFAULT_LOSS_CURVE_RESOLUTION = 50


#: Minimum value for a seed number
MIN_SINT_32 = -(2 ** 31)
#: Maximum value for a seed number
MAX_SINT_32 = (2 ** 31) - 1


#: Kind of supported type of loss outputs
LOSS_TYPES = ["structural", "nonstructural", "fatalities", "contents"]


#: relative tolerance to consider two risk outputs (almost) equal
RISK_RTOL = 0.05


#: absolute tolerance to consider two risk outputs (almost) equal
RISK_ATOL = 0.01

# TODO: these want to be dictionaries
INPUT_TYPE_CHOICES = (
    (u'unknown', u'Unknown'),
    (u'source', u'Source Model'),
    (u'source_model_logic_tree', u'Source Model Logic Tree'),
    (u'gsim_logic_tree', u'Ground Shaking Intensity Model Logic Tree'),
    (u'exposure', u'Exposure'),
    (u'fragility', u'Fragility'),
    (u'site_model', u'Site Model'),
    (u'rupture_model', u'Rupture Model'),

    # vulnerability models
    (u'structural_vulnerability', u'Structural Vulnerability'),
    (u'nonstructural_vulnerability', u'Non Structural Vulnerability'),
    (u'contents_vulnerability', u'Contents Vulnerability'),
    (u'business_interruption_vulnerability',
     u'Business Interruption Vulnerability'),
    (u'occupants_vulnerability', u'Occupants Vulnerability'),
    (u'structural_vulnerability_retrofitted',
     u'Structural Vulnerability Retrofitted'))

VULNERABILITY_TYPE_CHOICES = [choice[0]
                              for choice in INPUT_TYPE_CHOICES
                              if choice[0].endswith('vulnerability')]


#: The output of HazardCalculation.gen_ruptures
SourceRuptureSites = collections.namedtuple(
    'SourceRuptureSites',
    'source rupture sites')


############## Fix FloatField underflow error ##################
# http://stackoverflow.com/questions/9556586/floating-point-numbers-of-python-float-and-postgresql-double-precision

def _get_prep_value(self, value):
    if value is None:
        return None
    val = float(value)
    if val < 1E-300:
        return 0.
    return val

djm.FloatField.get_prep_value = _get_prep_value


def cost_type(loss_type):
    if loss_type == "fatalities":
        return "occupants"
    else:
        return loss_type


def risk_almost_equal(o1, o2, key=lambda x: x, rtol=RISK_RTOL, atol=RISK_ATOL):
    return numpy.testing.assert_allclose(
        numpy.array(key(o1)), numpy.array(key(o2)), rtol=rtol, atol=atol)


def loss_curve_almost_equal(curve, expected_curve):
    if getattr(curve, 'asset_value', None) == 0.0 and getattr(
            expected_curve, 'asset_value', None) == 0.0:
        return risk_almost_equal(curve.loss_ratios, expected_curve.loss_ratios)
    elif curve.losses[curve.losses > 0].any():
        poes = interpolate.interp1d(
            curve.losses, curve.poes,
            bounds_error=False, fill_value=0)(expected_curve.losses)
    else:
        poes = numpy.zeros(len(expected_curve.poes))

    return risk_almost_equal(poes, expected_curve.poes)


def getcursor(route):
    """Return a cursor from a Django route"""
    return connections[route].cursor()


def order_by_location(queryset):
    """
    Utility function to order a queryset by location. This works even if
    the location is of Geography object (a simple order_by('location') only
    works for Geometry objects).
    """
    return queryset.extra(
        select={'x': 'ST_X(geometry(location))',
                'y': 'ST_Y(geometry(location))'},
        order_by=["x", "y"])


def queryset_iter(queryset, chunk_size):
    """
    Given a QuerySet, split it into smaller queries and yield the result of
    each.

    :param queryset:
        A :class:`django.db.models.query.QuerySet` to iterate over, in chunks
        of ``chunk_size``.
    :param int chunksize:
        Chunk size for iteration over query results. For an unexecuted
        QuerySet, this will result in splitting a (potentially large) query
        into smaller queries.
    """
    offset = 0
    while True:
        chunk = list(queryset[offset:offset + chunk_size].iterator())
        if len(chunk) == 0:
            raise StopIteration
        else:
            yield chunk
            offset += chunk_size


## Tables in the 'admin' schema.


class RevisionInfo(djm.Model):
    '''
    Revision information
    '''
    artefact = djm.TextField(unique=True)
    revision = djm.TextField()
    step = djm.IntegerField(default=0)
    last_update = djm.DateTimeField(editable=False, default=datetime.utcnow)

    class Meta:
        db_table = 'admin\".\"revision_info'


## Tables in the 'hzrdi' (Hazard Input) schema.

class SiteModel(djm.Model):
    '''
     A model for site-specific parameters.

    Used in Hazard calculations.
    '''

    job = djm.ForeignKey('OqJob')
    # Average shear wave velocity for top 30 m. Units m/s.
    vs30 = djm.FloatField()
    # 'measured' or 'inferred'. Identifies if vs30 value has been measured or
    # inferred.
    vs30_type = djm.TextField(choices=VS30_TYPE_CHOICES)
    # Depth to shear wave velocity of 1.0 km/s. Units m.
    z1pt0 = djm.FloatField()
    # Depth to shear wave velocity of 2.5 km/s. Units km.
    z2pt5 = djm.FloatField()
    location = djm.PointField(srid=DEFAULT_SRID)

    def __repr__(self):
        return (
            'SiteModel(location="%s", vs30=%s, vs30_type=%s, z1pt0=%s, '
            'z2pt5=%s)'
            % (self.location.wkt, self.vs30, self.vs30_type, self.z1pt0,
               self.z2pt5))

    class Meta:
        db_table = 'hzrdi\".\"site_model'


## Tables in the 'uiapi' schema.

class OqJob(djm.Model):
    '''
    An OpenQuake engine run started by the user
    '''
    user_name = djm.TextField()
    hazard_calculation = djm.OneToOneField('HazardCalculation', null=True)
    risk_calculation = djm.OneToOneField('RiskCalculation', null=True)
    LOG_LEVEL_CHOICES = (
        (u'debug', u'Debug'),
        (u'info', u'Info'),
        (u'progress', u'Progress'),
        (u'warn', u'Warn'),
        (u'error', u'Error'),
        (u'critical', u'Critical'),
    )
    log_level = djm.TextField(choices=LOG_LEVEL_CHOICES, default='progress')
    STATUS_CHOICES = (
        (u'pre_executing', u'Pre-Executing'),
        (u'executing', u'Executing'),
        (u'post_executing', u'Post-Executing'),
        (u'post_processing', u'Post-Processing'),
        (u'export', u'Exporting results'),
        (u'clean_up', u'Cleaning up'),
        (u'complete', u'Complete'),
    )
    status = djm.TextField(choices=STATUS_CHOICES, default='pre_executing')
    oq_version = djm.TextField(null=True, blank=True)
    hazardlib_version = djm.TextField(null=True, blank=True)
    nrml_version = djm.TextField(null=True, blank=True)
    risklib_version = djm.TextField(null=True, blank=True)
    is_running = djm.BooleanField(default=False)
    duration = djm.IntegerField(default=0)
    job_pid = djm.IntegerField(default=0)
    supervisor_pid = djm.IntegerField(default=0)
    last_update = djm.DateTimeField(editable=False, default=datetime.utcnow)

    class Meta:
        db_table = 'uiapi\".\"oq_job'

    @property
    def calculation(self):
        """
        :returns: a calculation object (hazard or risk) depending on
        the type of calculation. Useful in situations (e.g. core
        engine, stats, kvs, progress) where you do not have enough
        context about which kind of calculation is but still you want
        to access the common feature of a Calculation object.
        """
        return self.hazard_calculation or self.risk_calculation


class Performance(djm.Model):
    '''
    Contains performance information about the operations performed by a task
    launched by a job.
    '''
    oq_job = djm.ForeignKey('OqJob')
    task_id = djm.TextField(null=True)
    task = djm.TextField(null=True)
    operation = djm.TextField(null=False)
    start_time = djm.DateTimeField(editable=False)
    duration = djm.FloatField(null=True)
    pymemory = djm.IntegerField(null=True)
    pgmemory = djm.IntegerField(null=True)

    class Meta:
        db_table = 'uiapi\".\"performance'


class JobStats(djm.Model):
    '''
    Capture various statistics about a job.
    '''
    oq_job = djm.ForeignKey('OqJob')
    start_time = djm.DateTimeField(editable=False, default=datetime.utcnow)
    stop_time = djm.DateTimeField(editable=False)
    # The number of total sites in job
    num_sites = djm.IntegerField(null=True)
    # The disk space occupation in bytes
    disk_space = djm.IntegerField(null=True)
    num_sources = fields.IntArrayField(null=True)

    class Meta:
        db_table = 'uiapi\".\"job_stats'


class HazardCalculation(djm.Model):
    '''
    Parameters needed to run a Hazard job.
    '''
    _site_collection = ()  # see the corresponding instance variable

    @classmethod
    def create(cls, **kw):
        _prep_geometry(kw)
        return cls(**kw)

    # Contains the absolute path to the directory containing the job config
    # file.
    base_path = djm.TextField()
    export_dir = djm.TextField(null=True, blank=True)

    #####################
    # General parameters:
    #####################

    # A description for this config profile which is meaningful to a user.
    description = djm.TextField(default='', blank=True)

    CALC_MODE_CHOICES = (
        (u'classical', u'Classical PSHA'),
        (u'event_based', u'Probabilistic Event-Based'),
        (u'disaggregation', u'Disaggregation'),
        (u'scenario', u'Scenario'),
    )
    calculation_mode = djm.TextField(choices=CALC_MODE_CHOICES)
    inputs = fields.PickleField(blank=True)

    # For the calculation geometry, choose either `region` (with
    # `region_grid_spacing`) or `sites`.
    region = djm.PolygonField(srid=DEFAULT_SRID, null=True, blank=True)
    # Discretization parameter for a `region`. Units in degrees.
    region_grid_spacing = djm.FloatField(null=True, blank=True)
    # The points of interest for a calculation.
    sites = djm.MultiPointField(srid=DEFAULT_SRID, null=True, blank=True)

    ########################
    # Logic Tree parameters:
    ########################
    random_seed = djm.IntegerField(null=True, blank=True)
    number_of_logic_tree_samples = djm.IntegerField(null=True, blank=True)

    ###############################################
    # ERF (Earthquake Rupture Forecast) parameters:
    ###############################################
    rupture_mesh_spacing = djm.FloatField(
        help_text=('Rupture mesh spacing (in kilometers) for simple/complex '
                   'fault sources rupture discretization'),
        null=True,
        blank=True,

    )
    width_of_mfd_bin = djm.FloatField(
        help_text=('Truncated Gutenberg-Richter MFD (Magnitude Frequency'
                   'Distribution) bin width'),
        null=True,
        blank=True,
    )
    area_source_discretization = djm.FloatField(
        help_text='Area Source Disretization, in kilometers',
        null=True,
        blank=True,
    )

    ##################
    # Site parameters:
    ##################
    # If there is no `site_model`, these 4 parameters must be specified:
    reference_vs30_value = djm.FloatField(
        help_text='Shear wave velocity in the uppermost 30 m. In m/s.',
        null=True,
        blank=True,
    )
    VS30_TYPE_CHOICES = (
        (u'measured', u'Measured'),
        (u'inferred', u'Inferred'),
    )
    reference_vs30_type = djm.TextField(
        choices=VS30_TYPE_CHOICES,
        null=True,
        blank=True,
    )
    reference_depth_to_2pt5km_per_sec = djm.FloatField(
        help_text='Depth to where shear-wave velocity = 2.5 km/sec. In km.',
        null=True,
        blank=True,
    )
    reference_depth_to_1pt0km_per_sec = djm.FloatField(
        help_text='Depth to where shear-wave velocity = 1.0 km/sec. In m.',
        null=True,
        blank=True,
    )

    #########################
    # Calculation parameters:
    #########################
    investigation_time = djm.FloatField(
        help_text=('Time span (in years) for probability of exceedance '
                   'calculation'),
        null=True,
        blank=True,
    )
    intensity_measure_types_and_levels = fields.DictField(
        help_text=(
            'Dictionary containing for each intensity measure type ("PGA", '
            '"PGV", "PGD", "SA", "IA", "RSD", "MMI"), the list of intensity '
            'measure levels for calculating probability of exceedence'),
        null=True,
        blank=True,
    )
    truncation_level = fields.NullFloatField(
        help_text='Level for ground motion distribution truncation',
        null=True,
        blank=True,
    )
    maximum_distance = djm.FloatField(
        help_text=('Maximum distance (in km) of sources to be considered in '
                   'the probability of exceedance calculation. Sources more '
                   'than this distance away (from the sites of interest) are '
                   'ignored.'),
    )

    ################################
    # Event-Based Calculator params:
    ################################
    intensity_measure_types = fields.CharArrayField(
        help_text=(
            'List of intensity measure types (input for GMF calculation)'),
        null=True,
        blank=True,
    )
    ses_per_logic_tree_path = djm.IntegerField(
        help_text=('Number of Stochastic Event Sets to compute per logic tree'
                   ' branch (enumerated or randomly sampled'),
        null=True,
        blank=True,
    )
    GROUND_MOTION_CORRELATION_MODELS = (
        (u'JB2009', u'Jayaram-Baker 2009'),
    )
    ground_motion_correlation_model = djm.TextField(
        help_text=('Name of the ground correlation model to use in the'
                   ' calculation'),
        null=True,
        blank=True,
        choices=GROUND_MOTION_CORRELATION_MODELS,
    )
    ground_motion_correlation_params = fields.DictField(
        help_text=('Parameters specific to the chosen ground motion'
                   ' correlation model'),
        null=True,
        blank=True,
    )

    ###################################
    # Disaggregation Calculator params:
    ###################################
    mag_bin_width = djm.FloatField(
        help_text=('Width of magnitude bins, which ultimately defines the size'
                   ' of the magnitude dimension of a disaggregation matrix'),
        null=True,
        blank=True,
    )
    distance_bin_width = djm.FloatField(
        help_text=('Width of distance bins, which ultimately defines the size'
                   ' of the distance dimension of a disaggregation matrix'),
        null=True,
        blank=True,
    )
    coordinate_bin_width = djm.FloatField(
        help_text=('Width of coordinate bins, which ultimately defines the'
                   ' size of the longitude and latitude dimensions of a'
                   ' disaggregation matrix'),
        null=True,
        blank=True,
    )
    num_epsilon_bins = djm.IntegerField(
        help_text=('Number of epsilon bins, which defines the size of the'
                   ' epsilon dimension of a disaggregation matrix'),
        null=True,
        blank=True,
    )
    ################################
    # Scenario Calculator params:
    ################################
    gsim = djm.TextField(
        help_text=('Name of the ground shaking intensity model to use in the '
                   'calculation'),
        null=True,
        blank=True,
    )
    number_of_ground_motion_fields = djm.IntegerField(
        null=True,
        blank=True,
    )
    poes_disagg = fields.FloatArrayField(
        help_text=('The probabilities of exceedance for which we interpolate'
                   ' grond motion values from hazard curves. This GMV is used'
                   ' as input for computing disaggregation histograms'),
        null=True,
        blank=True,
    )

    ################################
    # Output/post-processing params:
    ################################
    # Classical params:
    ###################
    mean_hazard_curves = fields.OqNullBooleanField(
        help_text='Compute mean hazard curves',
        null=True,
        blank=True,
    )
    quantile_hazard_curves = fields.FloatArrayField(
        help_text='Compute quantile hazard curves',
        null=True,
        blank=True,
    )
    poes = fields.FloatArrayField(
        help_text=('PoEs (probabilities of exceedence) to be used for '
                   'computing hazard maps and uniform hazard spectra'),
        null=True,
        blank=True,
    )
    hazard_maps = fields.OqNullBooleanField(
        help_text='Compute hazard maps',
        null=True,
        blank=True,
    )
    uniform_hazard_spectra = fields.OqNullBooleanField(
        help_text=('Compute uniform hazard spectra; if true, hazard maps will'
                   ' be computed as well'),
        null=True,
        blank=True,
    )
    export_multi_curves = fields.OqNullBooleanField(
        help_text=('If true hazard curve outputs that groups multiple curves '
                   'in multiple imt will be exported when asked in export '
                   'phase.'))
    # Event-Based params:
    #####################
    ground_motion_fields = fields.OqNullBooleanField(
        help_text=('If true, ground motion fields will be computed (in '
                   'addition to stochastic event sets)'),
        null=True,
        blank=True,
    )
    hazard_curves_from_gmfs = fields.OqNullBooleanField(
        help_text=('If true, ground motion fields will be post-processed into '
                   'hazard curves.'),
        null=True,
        blank=True,
    )

    class Meta:
        db_table = 'uiapi\".\"hazard_calculation'

    # class attributes used as defaults; I am avoiding `__init__`
    # to avoid issues with Django caching mechanism (MS)
    _points_to_compute = None

    @property
    def prefiltered(self):
        """
        Prefiltering is enabled when there are few sites (up to %d)
        """ % FILTERING_THRESHOLD
        return self.maximum_distance and \
            len(self.site_collection) <= FILTERING_THRESHOLD

    @property
    def vulnerability_models(self):
        return [self.inputs[vf_type]
                for vf_type in VULNERABILITY_TYPE_CHOICES
                if vf_type in self.inputs]

    @property
    def site_model(self):
        """
        Get the site model filename for this calculation
        """
        return self.inputs.get('site_model')

    ## TODO: this could be implemented with a view, now that there is
    ## a site table
    def get_closest_site_model_data(self, point):
        """Get the closest available site model data from the database
        for a given site model and :class:`openquake.hazardlib.geo.point.Point`

        :param site:
            :class:`openquake.hazardlib.geo.point.Point` instance.

        :returns:
            The closest :class:`openquake.engine.db.models.SiteModel`
            for the given ``point`` of interest.

            This function uses the PostGIS `ST_Distance_Sphere
            <http://postgis.refractions.net/docs/ST_Distance_Sphere.html>`_
            function to calculate distance.

            If there is no site model data, return `None`.
        """
        query = """
        SELECT
            hzrdi.site_model.*,
            min(ST_Distance_Sphere(location, %s))
                AS min_distance
        FROM hzrdi.site_model
        WHERE job_id = %s
        GROUP BY id
        ORDER BY min_distance
        LIMIT 1;"""

        raw_query_set = SiteModel.objects.raw(
            query, ['SRID=4326; %s' % point.wkt2d, self.oqjob.id]
        )

        site_model_data = list(raw_query_set)

        assert len(site_model_data) <= 1, (
            "This query should return at most 1 record.")

        if len(site_model_data) == 1:
            return site_model_data[0]

    def points_to_compute(self, save_sites=True):
        """
        Generate a :class:`~openquake.hazardlib.geo.mesh.Mesh` of points.
        These points indicate the locations of interest in a hazard
        calculation.

        The mesh can be calculated given a `region` polygon and
        `region_grid_spacing` (the discretization parameter), or from a list of
        `sites`.

        .. note::
            This mesh is cached for efficiency when dealing with large numbers
            of calculation points. If you need to clear the cache and
            recompute, set `_points_to_compute` to `None` and call this method
            again.
        """
        if self._points_to_compute is None:
            if self.pk and 'exposure' in self.inputs:
                assets = self.oqjob.exposuremodel.exposuredata_set.all(
                    ).order_by('asset_ref')

                # the points here must be sorted
                lons, lats = zip(*sorted(set((asset.site.x, asset.site.y)
                                             for asset in assets)))
                # Cache the mesh:
                self._points_to_compute = geo.Mesh(
                    numpy.array(lons), numpy.array(lats), depths=None
                )
            elif self.region and self.region_grid_spacing:
                # assume that the polygon is a single linear ring
                coords = self.region.coords[0]
                points = [geo.Point(*x) for x in coords]
                poly = geo.Polygon(points)
                # Cache the mesh:
                self._points_to_compute = poly.discretize(
                    self.region_grid_spacing
                )
            elif self.sites is not None:
                lons, lats = zip(*self.sites.coords)
                # Cache the mesh:
                self._points_to_compute = geo.Mesh(
                    numpy.array(lons), numpy.array(lats), depths=None
                )
            # store the sites
            if save_sites and self._points_to_compute:
                self.save_sites([(pt.longitude, pt.latitude)
                                 for pt in self._points_to_compute])

        return self._points_to_compute

    @property
    def site_collection(self):
        """
        Create a SiteCollection from a HazardCalculation object.
        First, take all of the points/locations of interest defined by the
        calculation geometry. For each point, do distance queries on the site
        model and get the site parameters which are closest to the point of
        interest. This aggregation of points to the closest site parameters
        is what we store in the `site_collection` field.
        If the computation does not specify a site model the same 4 reference
        site parameters are used for all sites. The sites are ordered by id,
        to ensure reproducibility in tests.
        """
        if len(self._site_collection):
            return self._site_collection

        hsites = HazardSite.objects.filter(
            hazard_calculation=self).order_by('id')
        if not hsites:
            raise RuntimeError('No sites were imported!')
        # NB: the sites MUST be ordered. The issue is that the disaggregation
        # calculator has a for loop of kind
        # for site in sites:
        #     bin_edge, disagg_matrix = disaggregation(site, ...)
        # the generated ruptures are random if the order of the sites
        # is random, even if the seed is fixed; in particular for some
        # ordering no ruptures are generated and the test
        # qa_tests/hazard/disagg/case_1/test.py fails with a bad
        # error message
        if self.site_model:
            sites = []
            for hsite in hsites:
                pt = geo.point.Point(hsite.location.x, hsite.location.y)
                smd = self.get_closest_site_model_data(pt)
                measured = smd.vs30_type == 'measured'
                vs30 = smd.vs30
                z1pt0 = smd.z1pt0
                z2pt5 = smd.z2pt5
                sites.append(Site(pt, vs30, measured, z1pt0, z2pt5, hsite.id))
            sc = SiteCollection(sites)
        else:
            lons = [hsite.location.x for hsite in hsites]
            lats = [hsite.location.y for hsite in hsites]
            site_ids = [hsite.id for hsite in hsites]
            sc = SiteCollection.from_points(lons, lats, site_ids, self)
        self._site_collection = sc
        js = JobStats.objects.get(oq_job=self.oqjob)
        js.num_sites = len(sc)
        js.save()
        return sc

    def get_imts(self):
        """
        Returns intensity mesure types or
        intensity mesure types with levels.
        """

        return (self.intensity_measure_types or
                self.intensity_measure_types_and_levels.keys())

    def save_sites(self, coordinates):
        """
        Save all the gives sites on the hzrdi.hazard_site table.
        :param coordinates: a sequence of (lon, lat) pairs
        :returns: the ids of the inserted HazardSite instances
        """
        sites = [HazardSite(hazard_calculation=self,
                            location='POINT(%s %s)' % coord)
                 for coord in coordinates]
        return writer.CacheInserter.saveall(sites)

    def total_investigation_time(self):
        """
        Helper method to compute the total investigation time for a
        complete set of stochastic event sets for all realizations.
        """
        if self.number_of_logic_tree_samples > 0:
            # The calculation is set to do Monte-Carlo sampling of logic trees
            # The number of logic tree realizations is specified explicitly in
            # job configuration.
            n_lt_realizations = self.number_of_logic_tree_samples
        else:
            # The calculation is set do end-branch enumeration of all logic
            # tree paths
            # We can get the number of logic tree realizations by counting
            # initialized lt_realization records.
            n_lt_realizations = LtRealization.objects.filter(
                lt_model__hazard_calculation=self).count()

        investigation_time = (self.investigation_time
                              * self.ses_per_logic_tree_path
                              * n_lt_realizations)

        return investigation_time

    def sites_affected_by(self, src):
        """
        If the maximum_distance is set and the prefiltering is on,
        i.e. if the computation involves only few (<=%d) sites,
        return the filtered subset of the site collection, otherwise
        return the whole connection. NB: this method returns `None`
        if the filtering does not find any site close to the source.

        :param src: the source object used for the filtering
        """ % FILTERING_THRESHOLD
        if self.prefiltered:
            return src.filter_sites_by_distance_to_source(
                self.maximum_distance, self.site_collection)
        return self.site_collection

    def gen_ruptures(self, sources, monitor, site_coll):
        """
        Yield (source, rupture, affected_sites) for each rupture
        generated by the given sources.

        :param sources: a sequence of sources
        :param monitor: a Monitor object
        """
        filtsources_mon = monitor.copy('filtering sources')
        genruptures_mon = monitor.copy('generating ruptures')
        filtruptures_mon = monitor.copy('filtering ruptures')
        for src in sources:
            with filtsources_mon:
                s_sites = src.filter_sites_by_distance_to_source(
                    self.maximum_distance, site_coll
                ) if self.maximum_distance else site_coll
                if s_sites is None:
                    continue

            with genruptures_mon:
                ruptures = list(src.iter_ruptures())
            if not ruptures:
                continue

            for rupture in ruptures:
                with filtruptures_mon:
                    r_sites = filters.filter_sites_by_distance_to_rupture(
                        rupture, self.maximum_distance, s_sites
                        ) if self.maximum_distance else s_sites
                    if r_sites is None:
                        continue
                yield SourceRuptureSites(src, rupture, r_sites)
        filtsources_mon.flush()
        genruptures_mon.flush()
        filtruptures_mon.flush()

    def gen_ruptures_for_site(self, site, sources, monitor):
        """
        Yield source, <ruptures close to site>

        :param site: a Site object
        :param sources: a sequence of sources
        :param monitor: a Monitor object
        """
        source_rupture_sites = self.gen_ruptures(
            sources, monitor, SiteCollection([site]))
        for src, rows in itertools.groupby(
                source_rupture_sites, key=operator.attrgetter('source')):
            yield src, [row.rupture for row in rows]


class RiskCalculation(djm.Model):
    '''
    Parameters needed to run a Risk job.
    '''
    @classmethod
    def create(cls, **kw):
        _prep_geometry(kw)
        return cls(**kw)

    #: Default maximum asset-hazard distance in km
    DEFAULT_MAXIMUM_DISTANCE = 5

    # Contains the absolute path to the directory containing the job config
    # file.
    base_path = djm.TextField()
    export_dir = djm.TextField(null=True, blank=True)

    #####################
    # General parameters:
    #####################

    # A description for this config profile which is meaningful to a user.
    description = djm.TextField(default='', blank=True)

    CALC_MODE_CHOICES = (
        (u'classical', u'Classical PSHA'),
        (u'classical_bcr', u'Classical BCR'),
        (u'event_based', u'Probabilistic Event-Based'),
        (u'scenario', u'Scenario'),
        (u'scenario_damage', u'Scenario Damage'),
        (u'event_based_bcr', u'Probabilistic Event-Based BCR'),
    )
    calculation_mode = djm.TextField(choices=CALC_MODE_CHOICES)
    inputs = fields.PickleField(blank=True)
    region_constraint = djm.PolygonField(
        srid=DEFAULT_SRID, null=True, blank=True)

    preloaded_exposure_model = djm.ForeignKey(
        'ExposureModel', null=True, blank=True)

    # the maximum distance for an hazard value with the corresponding
    # asset. Expressed in kilometers
    maximum_distance = djm.FloatField(
        null=True, blank=True, default=DEFAULT_MAXIMUM_DISTANCE)
    # the hazard output (it can point to an HazardCurvem, to a
    # Gmf or to a SES collection) used by the risk calculation
    hazard_output = djm.ForeignKey("Output", null=True, blank=True)

    # the HazardCalculation object used by the risk calculation when
    # each individual Output (i.e. each hazard logic tree realization)
    # is considered
    hazard_calculation = djm.ForeignKey("HazardCalculation",
                                        null=True, blank=True)

    risk_investigation_time = djm.FloatField(
        help_text=('Override the time span (in years) with which the '
                   'hazard has been computed.'),
        null=True,
        blank=True,
    )

    # A seed used to generate random values to be applied to
    # vulnerability functions
    master_seed = djm.IntegerField(null=True, blank=True)

    ####################################################
    # For calculators that output (conditional) loss map
    ####################################################
    conditional_loss_poes = fields.FloatArrayField(null=True, blank=True)

    ####################################################
    # For calculators that output statistical results
    ####################################################
    quantile_loss_curves = fields.FloatArrayField(
        help_text='List of quantiles for computing quantile outputs',
        null=True,
        blank=True)

    taxonomies_from_model = fields.OqNullBooleanField(
        help_text='if true calculation only consider the taxonomies in '
        'the fragility model', null=True, blank=True)

    ##################################
    # Probabilistic shared parameters
    ##################################
    # 0 == uncorrelated, 1 == perfect correlation by taxonomy
    asset_correlation = djm.FloatField(null=True, blank=True, default=0)

    #######################
    # Classical parameters:
    #######################
    lrem_steps_per_interval = djm.IntegerField(null=True, blank=True)

    poes_disagg = fields.FloatArrayField(
        null=True, blank=True,
        help_text='The probability of exceedance used to interpolate '
                  'loss curves for disaggregation purposes')

    #########################
    # Event-Based parameters:
    #########################
    loss_curve_resolution = djm.IntegerField(
        null=False, blank=True, default=DEFAULT_LOSS_CURVE_RESOLUTION)
    insured_losses = djm.NullBooleanField(null=True, blank=True, default=False)

    # The points of interest for disaggregation
    sites_disagg = djm.MultiPointField(
        srid=DEFAULT_SRID, null=True, blank=True)

    mag_bin_width = djm.FloatField(
        help_text=('Width of magnitude bins'),
        null=True,
        blank=True,
    )
    distance_bin_width = djm.FloatField(
        help_text=('Width of distance bins'),
        null=True,
        blank=True,
    )
    coordinate_bin_width = djm.FloatField(
        help_text=('Width of coordinate bins'),
        null=True,
        blank=True,
    )

    ######################################
    # BCR (Benefit-Cost Ratio) parameters:
    ######################################
    interest_rate = djm.FloatField(null=True, blank=True)
    asset_life_expectancy = djm.FloatField(null=True, blank=True)

    ######################################
    # Scenario parameters:
    ######################################
    time_event = fields.NullTextField()

    class Meta:
        db_table = 'uiapi\".\"risk_calculation'

    def get_hazard_calculation(self):
        """
        Get the hazard calculation associated with the hazard output used as an
        input to this risk calculation.

        :returns:
            :class:`HazardCalculation` instance.
        """
        try:
            hcalc = (self.hazard_calculation or
                     self.hazard_output.oq_job.hazard_calculation)
        except ObjectDoesNotExist:
            raise RuntimeError("The provided hazard does not exist")
        return hcalc

    def hazard_outputs(self):
        """
        Returns the list of hazard outputs to be considered. Apply
        `filters` to the default queryset
        """

        if self.hazard_output:
            return [self.hazard_output]
        elif self.hazard_calculation:
            if self.calculation_mode in ["classical", "classical_bcr"]:
                filters = dict(output_type='hazard_curve_multi',
                               hazard_curve__lt_realization__isnull=False)
            elif self.calculation_mode in ["event_based", "event_based_bcr"]:
                filters = dict(
                    output_type='gmf', gmf__lt_realization__isnull=False)
            elif self.calculation_mode in ['scenario', 'scenario_damage']:
                filters = dict(output_type='gmf_scenario')
            else:
                raise NotImplementedError

            return self.hazard_calculation.oqjob.output_set.filter(
                **filters).order_by('id')
        else:
            raise RuntimeError("Neither hazard calculation "
                               "neither a hazard output has been provided")

    @property
    def best_maximum_distance(self):
        """
        Get the asset-hazard maximum distance (in km) to be used in
        hazard getters.

        :returns:
            The minimum between the maximum distance provided by the user (if
            not given, `DEFAULT_MAXIMUM_DISTANCE` is used as default) and the
            step (if exists) used by the hazard calculation.
        """
        dist = self.maximum_distance

        if dist is None:
            dist = self.DEFAULT_MAXIMUM_DISTANCE

        hc = self.get_hazard_calculation()
        if hc.sites is None and hc.region_grid_spacing is not None:
            dist = min(dist, hc.region_grid_spacing * numpy.sqrt(2) / 2)

        # if we are computing hazard at exact location we set the
        # maximum_distance to a very small number in order to help the
        # query to find the results.
        if 'exposure' in hc.inputs:
            dist = 0.001
        return dist

    @property
    def is_bcr(self):
        return self.calculation_mode in ['classical_bcr', 'event_based_bcr']

    @property
    def exposure_model(self):
        return self.preloaded_exposure_model or self.oqjob.exposuremodel

    def vulnerability_inputs(self, retrofitted):
        for loss_type in LOSS_TYPES:
            ctype = cost_type(loss_type)

            vulnerability_input = self.vulnerability_input(ctype, retrofitted)
            if vulnerability_input is not None:
                yield vulnerability_input, loss_type

    def vulnerability_input(self, ctype, retrofitted=False):
        if retrofitted:
            input_type = "%s_vulnerability_retrofitted" % ctype
        else:
            input_type = "%s_vulnerability" % ctype

        return self.inputs.get(input_type)

    @property
    def investigation_time(self):
        return (self.risk_investigation_time or
                self.get_hazard_calculation().investigation_time)


def _prep_geometry(kwargs):
    """
    Helper function to convert geometry specified in a job config file to WKT,
    so that it can save to the database in a geometry field.

    :param dict kwargs:
        `dict` representing some keyword arguments, which may contain geometry
        definitions in some sort of string or list form

    :returns:
        The modified ``kwargs``, with WKT to replace the input geometry
        definitions.
    """
    # If geometries were specified as string lists of coords,
    # convert them to WKT before doing anything else.
    for field, wkt_fmt in (('sites', 'MULTIPOINT(%s)'),
                           ('sites_disagg', 'MULTIPOINT(%s)'),
                           ('region', 'POLYGON((%s))'),
                           ('region_constraint', 'POLYGON((%s))')):
        if field in kwargs:
            geom = kwargs[field]
            if geom is None:
                continue
            try:
                wkt.loads(geom)
                # if this succeeds, we know the wkt is at least valid
                # we don't know the geometry type though; we'll leave that
                # to subsequent validation
            except wkt.ReadingError:
                try:
                    coords = [
                        float(x) for x in fields.ARRAY_RE.split(geom)
                    ]
                except ValueError:
                    raise ValueError(
                        'Could not coerce `str` to a list of `float`s'
                    )
                else:
                    if not len(coords) % 2 == 0:
                        raise ValueError(
                            'Got an odd number of coordinate values'
                        )
                    else:
                        # Construct WKT from the coords
                        # NOTE: ordering is expected to be lon,lat
                        points = ['%s %s' % (coords[i], coords[i + 1])
                                  for i in xrange(0, len(coords), 2)]
                        # if this is the region, close the linear polygon
                        # ring by appending the first coord to the end
                        if field in ('region', 'region_constraint'):
                            points.append(points[0])
                        # update the field
                        kwargs[field] = wkt_fmt % ', '.join(points)

    # return the (possbily) modified kwargs
    return kwargs


class OutputManager(djm.Manager):
    """
    Manager class to filter and create Output objects
    """
    def create_output(self, job, display_name, output_type):
        """
        Create an output for the given `job`, `display_name` and
        `output_type` (default to hazard_curve)
        """
        return self.create(oq_job=job,
                           display_name=display_name,
                           output_type=output_type)


class Output(djm.Model):
    '''
    A single artifact which is a result of an OpenQuake job.
    The data may reside in a file or in the database.
    '''

    #: Metadata of hazard outputs used by risk calculation. See
    #: `hazard_metadata` property for more details
    HazardMetadata = collections.namedtuple(
        'hazard_metadata',
        'investigation_time statistics quantile sm_path gsim_path')

    #: Hold the full paths in the model trees of ground shaking
    #: intensity models and of source models, respectively.
    LogicTreePath = collections.namedtuple(
        'logic_tree_path',
        'gsim_path sm_path')

    #: Hold the statistical params (statistics, quantile).
    StatisticalParams = collections.namedtuple(
        'statistical_params',
        'statistics quantile')

    oq_job = djm.ForeignKey('OqJob', null=False)

    display_name = djm.TextField()

    HAZARD_OUTPUT_TYPE_CHOICES = (
        (u'disagg_matrix', u'Disaggregation Matrix'),
        (u'gmf', u'Ground Motion Field'),
        (u'gmf_scenario', u'Ground Motion Field'),
        (u'hazard_curve', u'Hazard Curve'),
        (u'hazard_curve_multi', u'Hazard Curve (multiple imts)'),
        (u'hazard_map', u'Hazard Map'),
        (u'ses', u'Stochastic Event Set'),
        (u'uh_spectra', u'Uniform Hazard Spectra'),
    )

    RISK_OUTPUT_TYPE_CHOICES = (
        (u'agg_loss_curve', u'Aggregate Loss Curve'),
        (u'aggregate_loss', u'Aggregate Losses'),
        (u'bcr_distribution', u'Benefit-cost ratio distribution'),
        (u'collapse_map', u'Collapse Map Distribution'),
        (u'dmg_dist_per_asset', u'Damage Distribution Per Asset'),
        (u'dmg_dist_per_taxonomy', u'Damage Distribution Per Taxonomy'),
        (u'dmg_dist_total', u'Total Damage Distribution'),
        (u'event_loss', u'Event Loss Table'),
        (u'loss_curve', u'Loss Curve'),
        (u'event_loss_curve', u'Loss Curve'),
        (u'loss_fraction', u'Loss fractions'),
        (u'loss_map', u'Loss Map'),
    )

    output_type = djm.TextField(
        choices=HAZARD_OUTPUT_TYPE_CHOICES + RISK_OUTPUT_TYPE_CHOICES)
    last_update = djm.DateTimeField(editable=False, default=datetime.utcnow)

    objects = OutputManager()

    def __str__(self):
        return "%d||%s||%s" % (self.id, self.output_type, self.display_name)

    class Meta:
        db_table = 'uiapi\".\"output'

    def is_hazard_curve(self):
        return self.output_type in ['hazard_curve', 'hazard_curve_multi']

    @property
    def output_container(self):
        """
        :returns: the output container associated with this output
        """

        # FIXME(lp). Remove the following outstanding exceptions
        if self.output_type in ['agg_loss_curve', 'event_loss_curve']:
            return self.loss_curve
        elif self.output_type == 'hazard_curve_multi':
            return self.hazard_curve
        elif self.output_type == 'gmf_scenario':
            return self.gmf
        return getattr(self, self.output_type)

    @property
    def lt_realization_paths(self):
        """
        :returns: an instance of `LogicTreePath` the output is
        associated with. When the output is not associated with any
        logic tree branch then it returns a LogicTreePath namedtuple
        with a couple of None.
        """
        hazard_output_types = [el[0] for el in self.HAZARD_OUTPUT_TYPE_CHOICES]
        risk_output_types = [el[0] for el in self.RISK_OUTPUT_TYPE_CHOICES]
        container = self.output_container

        if self.output_type in hazard_output_types:
            rlz = getattr(container, 'lt_realization_id', None)
            if rlz is not None:
                return self.LogicTreePath(
                    tuple(container.lt_realization.gsim_lt_path),
                    tuple(container.lt_realization.sm_lt_path))
            else:
                return self.LogicTreePath(None, None)
        elif self.output_type in risk_output_types:
            if getattr(container, 'hazard_output_id', None):
                return container.hazard_output.lt_realization_paths
            else:
                return self.LogicTreePath(None, None)

        raise RuntimeError("unexpected output type %s" % self.output_type)

    @property
    def statistical_params(self):
        """
        :returns: an instance of `StatisticalParams` the output is
        associated with
        """
        if getattr(self.output_container, 'statistics', None) is not None:
            return self.StatisticalParams(self.output_container.statistics,
                                          self.output_container.quantile)
        elif getattr(
                self.output_container, 'hazard_output_id', None) is not None:
            return self.output_container.hazard_output.statistical_params
        else:
            return self.StatisticalParams(None, None)

    @property
    def hazard_metadata(self):
        """
        Given an Output produced by a risk calculation it returns the
        corresponding hazard metadata.

        :returns:
            A `namedtuple` with the following attributes::

                * investigation_time: the hazard investigation time (float)
                * statistics: the kind of hazard statistics (None, "mean" or
                  "quantile")
                * quantile: quantile value (when `statistics` is "quantile")
                * sm_path: a list representing the source model path
                * gsim_path: a list representing the gsim logic tree path

        """
        investigation_time = self.oq_job\
                                 .risk_calculation\
                                 .get_hazard_calculation()\
                                 .investigation_time

        statistics, quantile = self.statistical_params
        gsim_lt_path, sm_lt_path = self.lt_realization_paths

        return self.HazardMetadata(investigation_time,
                                   statistics, quantile,
                                   sm_lt_path, gsim_lt_path)


## Tables in the 'hzrdr' schema.


class HazardMap(djm.Model):
    '''
    Hazard Map header (information which pertains to entire map)
    '''
    output = djm.OneToOneField('Output', related_name="hazard_map")
    # FK only required for non-statistical results (i.e., mean or quantile
    # curves).
    lt_realization = djm.ForeignKey('LtRealization', null=True)
    investigation_time = djm.FloatField()
    imt = djm.TextField(choices=IMT_CHOICES)
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)
    sa_period = djm.FloatField(null=True)
    sa_damping = djm.FloatField(null=True)
    poe = djm.FloatField()
    # lons, lats, and imls are stored as numpy arrays with a uniform size and
    # shape
    lons = fields.FloatArrayField()
    lats = fields.FloatArrayField()
    imls = fields.FloatArrayField()

    class Meta:
        db_table = 'hzrdr\".\"hazard_map'

    def __str__(self):
        return (
            'HazardMap(poe=%(poe)s, imt=%(imt)s, sa_period=%(sa_period)s, '
            'statistics=%(statistics)s, quantile=%(quantile)s)'
        ) % self.__dict__

    def __repr__(self):
        return self.__str__()


class HazardCurve(djm.Model):
    '''
    Hazard Curve header information
    '''
    output = djm.OneToOneField(
        'Output', null=True, related_name="hazard_curve")
    # FK only required for non-statistical results (i.e., mean or quantile
    # curves).
    lt_realization = djm.ForeignKey('LtRealization', null=True)
    investigation_time = djm.FloatField()
    imt = djm.TextField(choices=IMT_CHOICES, default=None, blank=True)
    imls = fields.FloatArrayField()
    STAT_CHOICES = (
        (u'mean', u'Mean'),
        (u'quantile', u'Quantile'),
    )
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)
    sa_period = djm.FloatField(null=True)
    sa_damping = djm.FloatField(null=True)

    class Meta:
        db_table = 'hzrdr\".\"hazard_curve'

    def build_data(self, curves_by_trt_model_gsim):
        """
        Build on the fly the hazard curves for the current realization
        by looking at the associations stored in the database table
        `hzrdr.assoc_lt_rlz_trt_model`.
        """
        if self.imt:
            # build_data cannot be called from real curves
            raise TypeError('%r is not a multicurve', self)

        # fixed a realization, there are T associations where T is the
        # number of TrtModels
        curves = 0
        for art in AssocLtRlzTrtModel.objects.filter(rlz=self.lt_realization):
            pnes = 1. - curves_by_trt_model_gsim[art.trt_model_id, art.gsim]
            curves = 1. - (1. - curves) * pnes
        return curves

    @property
    def imt_long(self):
        """
        :returns: a string representing the imt associated with the
        curve (if any) in the long form, e.g. SA(0.01)
        """
        if self.imt:
            if self.imt == "SA":
                return "%s(%s)" % (self.imt, self.sa_damping)
            else:
                return self.imt

    def __iter__(self):
        assert self.output.output_type == 'hazard_curve_multi'

        siblings = self.__class__.objects.filter(
            output__oq_job=self.output.oq_job,
            output__output_type='hazard_curve')

        if not self.statistics:
            return iter(siblings.filter(lt_realization__isnull=False))
        elif self.quantile:
            return iter(
                siblings.filter(statistics="quantile", quantile=self.quantile))
        else:
            return iter(siblings.filter(statistics="mean"))


class HazardCurveDataManager(djm.GeoManager):
    """
    Manager class to filter and create HazardCurveData objects
    """

    def all_curves_for_imt(self, job, imt, sa_period, sa_damping):
        """
        Helper function for creating a :class:`django.db.models.query.QuerySet`
        for selecting all curves from all realizations for a given ``job_id``
        and ``imt``.

        :param job:
            An :class:`openquake.engine.db.models.OqJob` instance.
        :param str imt:
            Intensity measure type.
        :param sa_period:
            Spectral Acceleration period value. Only relevant if the ``imt`` is
            "SA".
        :param sa_damping:
            Spectrail Acceleration damping value. Only relevant if the ``imt``
            is "SA".
        """
        return self.filter(hazard_curve__output__oq_job=job,
                           hazard_curve__imt=imt,
                           hazard_curve__sa_period=sa_period,
                           hazard_curve__sa_damping=sa_damping,
                           # We only want curves associated with a logic tree
                           # realization (and not statistical aggregates):
                           hazard_curve__lt_realization__isnull=False)

    def all_curves_simple(self, filter_args=None, order_by='id'):
        """
        Get all :class:`HazardCurveData` records matching `filter_args` and
        return the results in a simple, lean format: a sequence of (x, y, poes)
        triples, where x and y are longitude and latitude of the `location`.

        For querying large sets of hazard curve data, this is a rather lean
        and efficient method for getting the results.

        :param dict filter_args:
            Optional. Dictionary of filter arguments to apply to the query.
        :param str order_by:
            Defaults to the primary key ('id'). Field by which to order
            results. Currently, only one `ORDER BY` field is supported.
        """
        if filter_args is None:
            filter_args = dict()

        return self\
            .filter(**filter_args)\
            .order_by(order_by)\
            .extra(select={'x': 'ST_X(location)', 'y': 'ST_Y(location)'})\
            .values_list('x', 'y', 'poes')\
            .iterator()


class HazardCurveData(djm.Model):
    '''
    Hazard Curve data

    Contains an list of PoE (Probability of Exceedance)
    values and the geographical point associated with the curve
    '''
    hazard_curve = djm.ForeignKey('HazardCurve')
    poes = fields.FloatArrayField()
    location = djm.PointField(srid=DEFAULT_SRID)
    # weight can be null/None if the weight is implicit:
    weight = djm.DecimalField(decimal_places=100, max_digits=101, null=True)

    objects = HazardCurveDataManager()

    class Meta:
        db_table = 'hzrdr\".\"hazard_curve_data'


class SESCollection(djm.Model):
    """
    Stochastic Event Set Collection: A container for 1 or more Stochastic Event
    Sets for a given logic tree realization.

    See also :class:`SES` and :class:`SESRupture`.
    """
    output = djm.OneToOneField('Output', related_name="ses")
    lt_model = djm.OneToOneField(
        'LtSourceModel', related_name='ses_collection', null=True)
    ordinal = djm.IntegerField(null=False)

    class Meta:
        db_table = 'hzrdr\".\"ses_collection'
        ordering = ['ordinal']

    def __iter__(self):
        """
        Iterator for walking through all child :class:`SES` objects.
        """
        hc = self.output.oq_job.hazard_calculation
        n = hc.ses_per_logic_tree_path or 1  # scenario
        for ordinal in xrange(1, n + 1):
            yield SES(self, ordinal)

    def __len__(self):
        """
        Return the ses_per_logic_tree_path parameter
        """
        return self.output.oq_job.hazard_calculation.ses_per_logic_tree_path

    def get_ruptures(self):
        """Return the SESRuptures associated to self"""
        return SESRupture.objects.filter(rupture__ses_collection=self)

    @property
    def sm_lt_path(self):
        """
        The source model logic tree path corresponding to the collection
        """
        if self.lt_model is None:  # scenario
            return ()
        return tuple(self.lt_model.sm_lt_path)


class SES(object):
    """
    Stochastic Event Set: A container for 1 or more ruptures associated with a
    specific investigation time span.
    """
    # the ordinal must be > 0: the reason is that it appears in the
    # exported XML file and the schema constraints the number to be
    # nonzero
    def __init__(self, ses_collection, ordinal=1):
        self.ses_collection = ses_collection
        self.ordinal = ordinal

    @property
    def investigation_time(self):
        hc = self.ses_collection.output.oq_job.hazard_calculation
        return hc.investigation_time

    def __cmp__(self, other):
        return cmp(self.ordinal, other.ordinal)

    def __iter__(self):
        """
        Iterator for walking through all child :class:`SESRupture` objects.
        """
        return SESRupture.objects.filter(
            rupture__ses_collection=self.ses_collection.id,
            ses_id=self.ordinal).order_by('tag').iterator()


def is_from_fault_source(rupture):
    """
    If True, this rupture was generated from a simple/complex fault
    source. If False, this rupture was generated from a point/area source.

    :param rupture: an instance of :class:
    `openquake.hazardlib.source.rupture.BaseProbabilisticRupture`
    """
    typology = rupture.source_typology
    is_char = typology is source.CharacteristicFaultSource
    is_complex_or_simple = typology in (
        source.ComplexFaultSource,
        source.SimpleFaultSource)
    is_complex_or_simple_surface = isinstance(
        rupture.surface, (geo.ComplexFaultSurface,
                          geo.SimpleFaultSurface))
    return is_complex_or_simple or (
        is_char and is_complex_or_simple_surface)


def is_multi_surface(rupture):
    """
    :param rupture: an instance of :class:
    `openquake.hazardlib.source.rupture.BaseProbabilisticRupture`

    :returns: a boolean
    """
    typology = rupture.source_typology
    is_char = typology is source.CharacteristicFaultSource
    is_multi_sur = isinstance(rupture.surface, geo.MultiSurface)
    return is_char and is_multi_sur


def get_geom(surface, is_from_fault_source, is_multi_surface):
    """
    The following fields can be interpreted different ways,
    depending on the value of `is_from_fault_source`. If
    `is_from_fault_source` is True, each of these fields should
    contain a 2D numpy array (all of the same shape). Each triple
    of (lon, lat, depth) for a given index represents the node of
    a rectangular mesh. If `is_from_fault_source` is False, each
    of these fields should contain a sequence (tuple, list, or
    numpy array, for example) of 4 values. In order, the triples
    of (lon, lat, depth) represent top left, top right, bottom
    left, and bottom right corners of the the rupture's planar
    surface. Update: There is now a third case. If the rupture
    originated from a characteristic fault source with a
    multi-planar-surface geometry, `lons`, `lats`, and `depths`
    will contain one or more sets of 4 points, similar to how
    planar surface geometry is stored (see above).

    :param rupture: an instance of :class:
    `openquake.hazardlib.source.rupture.BaseProbabilisticRupture`

    :param is_from_fault_source: a boolean
    :param is_multi_surface: a boolean
    """
    if is_from_fault_source:
        # for simple and complex fault sources,
        # rupture surface geometry is represented by a mesh
        surf_mesh = surface.get_mesh()
        lons = surf_mesh.lons
        lats = surf_mesh.lats
        depths = surf_mesh.depths
    else:
        if is_multi_surface:
            # `list` of
            # openquake.hazardlib.geo.surface.planar.PlanarSurface
            # objects:
            surfaces = surface.surfaces

            # lons, lats, and depths are arrays with len == 4*N,
            # where N is the number of surfaces in the
            # multisurface for each `corner_*`, the ordering is:
            #   - top left
            #   - top right
            #   - bottom left
            #   - bottom right
            lons = numpy.concatenate([x.corner_lons for x in surfaces])
            lats = numpy.concatenate([x.corner_lats for x in surfaces])
            depths = numpy.concatenate([x.corner_depths for x in surfaces])
        else:
            # For area or point source,
            # rupture geometry is represented by a planar surface,
            # defined by 3D corner points
            lons = numpy.zeros((4))
            lats = numpy.zeros((4))
            depths = numpy.zeros((4))

            # NOTE: It is important to maintain the order of these
            # corner points. TODO: check the ordering
            for i, corner in enumerate((surface.top_left,
                                        surface.top_right,
                                        surface.bottom_left,
                                        surface.bottom_right)):
                lons[i] = corner.longitude
                lats[i] = corner.latitude
                depths[i] = corner.depth
    return lons, lats, depths


class ProbabilisticRupture(djm.Model):
    """
    A rupture as part of a Stochastic Event Set Collection.
    """
    ses_collection = djm.ForeignKey('SESCollection')
    magnitude = djm.FloatField(null=False)
    hypocenter = djm.PointField(srid=DEFAULT_SRID)
    rake = djm.FloatField(null=False)
    tectonic_region_type = djm.TextField(null=False)
    is_from_fault_source = djm.NullBooleanField(null=False)
    is_multi_surface = djm.NullBooleanField(null=False)
    surface = fields.PickleField(null=False)
    site_indices = fields.IntArrayField(null=True)

    class Meta:
        db_table = 'hzrdr\".\"probabilistic_rupture'

    @classmethod
    def create(cls, rupture, ses_collection, site_indices=None):
        """
        Create a ProbabilisticRupture row on the database.

        :param rupture:
            a hazardlib rupture
        :param ses_collection:
            a Stochastic Event Set Collection object
        :param site_indices:
            an array of indices for the site_collection
        """
        iffs = is_from_fault_source(rupture)
        ims = is_multi_surface(rupture)
        lons, lats, depths = get_geom(rupture.surface, iffs, ims)
        return cls.objects.create(
            ses_collection=ses_collection,
            magnitude=rupture.mag,
            rake=rupture.rake,
            tectonic_region_type=rupture.tectonic_region_type or 'NA',
            is_from_fault_source=iffs,
            is_multi_surface=ims,
            surface=rupture.surface,
            hypocenter=rupture.hypocenter.wkt2d,
            site_indices=site_indices)

    _geom = None

    @property
    def geom(self):
        """
        Extract the triple (lons, lats, depths) from the surface geometry
        (cached).
        """
        if self._geom is not None:
            return self._geom
        self._geom = get_geom(self.surface, self.is_from_fault_source,
                              self.is_multi_surface)
        return self._geom

    @property
    def lons(self):
        return self.geom[0]

    @property
    def lats(self):
        return self.geom[1]

    @property
    def depths(self):
        return self.geom[2]

    @property
    def strike(self):
        return self.surface.get_strike()

    @property
    def dip(self):
        return self.surface.get_dip()

    @property
    def mag(self):
        return self.magnitude

    def _validate_planar_surface(self):
        """
        A rupture's planar surface (existing only in the case of ruptures from
        area/point sources) may only consist of 4 points (top left, top right,
        bottom right, and bottom left corners, in that order).

        If the surface is not valid, a :exc:`ValueError` is raised.

        This should only be used if `is_from_fault_source` is `False`.
        """
        if not (4 == len(self.lons) == len(self.lats) == len(self.depths)):
            raise ValueError(
                "Incorrect number of points; there should be exactly 4")

    @property
    def top_left_corner(self):
        if not (self.is_from_fault_source or self.is_multi_surface):
            self._validate_planar_surface()
            return self.lons[0], self.lats[0], self.depths[0]
        return None

    @property
    def top_right_corner(self):
        if not (self.is_from_fault_source or self.is_multi_surface):
            self._validate_planar_surface()
            return self.lons[1], self.lats[1], self.depths[1]
        return None

    @property
    def bottom_left_corner(self):
        if not (self.is_from_fault_source or self.is_multi_surface):
            self._validate_planar_surface()
            return self.lons[2], self.lats[2], self.depths[2]
        return None

    @property
    def bottom_right_corner(self):
        if not (self.is_from_fault_source or self.is_multi_surface):
            self._validate_planar_surface()
            return self.lons[3], self.lats[3], self.depths[3]
        return None


class SESRupture(djm.Model):
    """
    A rupture as part of a Stochastic Event Set.
    """
    rupture = djm.ForeignKey('ProbabilisticRupture')
    ses_id = djm.IntegerField(null=False)
    tag = djm.TextField(null=False)
    seed = djm.IntegerField(null=False)

    class Meta:
        db_table = 'hzrdr\".\"ses_rupture'
        ordering = ['tag']

    @classmethod
    def create(cls, prob_rupture, ses, source_id, rupt_no, rupt_occ, seed):
        """
        Create a SESRupture row in the database.

        :param prob_rupture:
            :class:`openquake.engine.db.models.ProbabilisticRupture` instance
        :param ses:
            :class:`openquake.engine.db.models.SES` instance
        :param str source_id:
            id of the source that generated the rupture
        :param rupt_no:
            the rupture number (an ordinal from source.iter_ruptures())
        :param rupt_occ:
            the occurrence number of the rupture in the given ses
        :param int seed:
            a seed that will be used when computing the GMF from the rupture
        """
        tag = 'smlt=%02d|ses=%04d|src=%s|rup=%03d-%02d' % (
            ses.ses_collection.ordinal, ses.ordinal, source_id, rupt_no,
            rupt_occ)
        return cls.objects.create(
            rupture=prob_rupture, ses_id=ses.ordinal, tag=tag, seed=seed)

    @property
    def surface(self):
        """The surface of the underlying rupture"""
        return self.rupture.surface

    @property
    def hypocenter(self):
        """The hypocenter of the underlying rupture"""
        return self.rupture.hypocenter


class _Point(object):
    def __init__(self, x, y):
        self.x = x
        self.y = y


class Gmf(djm.Model):
    """
    A collection of ground motion field (GMF) sets for a given logic tree
    realization.
    """
    output = djm.OneToOneField('Output', related_name="gmf")
    lt_realization = djm.ForeignKey('LtRealization', null=False)

    class Meta:
        db_table = 'hzrdr\".\"gmf'

    # this part is tested in models_test:GmfsPerSesTestCase
    def __iter__(self):
        """
        Get the ground motion fields per SES ("GMF set") for
        the XML export. Each "GMF set" should:

            * have an `investigation_time` attribute
            * have an `stochastic_event_set_id` attribute
            * be iterable, yielding a sequence of "GMF" objects

            Each "GMF" object should:

            * have an `imt` attribute
            * have an `sa_period` attribute (only if `imt` is 'SA')
            * have an `sa_damping` attribute (only if `imt` is 'SA')
            * have a `rupture_id` attribute (to indicate which rupture
              contributed to this gmf)
            * be iterable, yielding a sequence of "GMF node" objects

            Each "GMF node" object should have:

            * a `gmv` attribute (to indicate the ground motion value
            * `lon` and `lat` attributes (to indicate the geographical location
              of the ground motion field)

        If a SES does not generate any GMF, it is ignored.
        """
        hc = self.output.oq_job.hazard_calculation
        for ses_coll in SESCollection.objects.filter(
                output__oq_job=self.output.oq_job):
            for ses in ses_coll:
                query = """\
        SELECT imt, sa_period, sa_damping, tag,
               array_agg(gmv) AS gmvs,
               array_agg(ST_X(location::geometry)) AS xs,
               array_agg(ST_Y(location::geometry)) AS ys
        FROM (SELECT imt, sa_period, sa_damping,
             unnest(rupture_ids) as rupture_id, location, unnest(gmvs) AS gmv
           FROM hzrdr.gmf_data, hzrdi.hazard_site
            WHERE site_id = hzrdi.hazard_site.id AND hazard_calculation_id=%s
           AND gmf_id=%d) AS x, hzrdr.ses_rupture AS y,
           hzrdr.probabilistic_rupture AS z
        WHERE x.rupture_id = y.id AND y.rupture_id=z.id
        AND y.ses_id=%d AND z.ses_collection_id=%d
        GROUP BY imt, sa_period, sa_damping, tag
        ORDER BY imt, sa_period, sa_damping, tag;
        """ % (hc.id, self.id, ses.ordinal, ses_coll.id)
                with transaction.commit_on_success(using='job_init'):
                    curs = getcursor('job_init')
                    curs.execute(query)
                # a set of GMFs generate by the same SES, one per rupture
                gmfset = []
                for (imt, sa_period, sa_damping, rupture_tag, gmvs,
                     xs, ys) in curs:
                    # using a generator here saves a lot of memory
                    nodes = (_GroundMotionFieldNode(gmv, _Point(x, y))
                             for gmv, x, y in zip(gmvs, xs, ys))
                    gmfset.append(
                        _GroundMotionField(
                            imt, sa_period, sa_damping, rupture_tag,
                            nodes))
                if gmfset:
                    yield GmfSet(ses, gmfset)


class GmfSet(object):
    """
    Small wrapper around the list of Gmf objects associated to the given SES.
    """
    def __init__(self, ses, gmfset):
        self.ses = ses
        self.gmfset = gmfset
        self.investigation_time = ses.investigation_time
        self.stochastic_event_set_id = ses.ordinal

    def __iter__(self):
        return iter(self.gmfset)

    def __str__(self):
        return (
            'GMFsPerSES(investigation_time=%f, '
            'stochastic_event_set_id=%s,\n%s)' % (
                self.ses.investigation_time,
                self.ses.ordinal, '\n'.join(str(g) for g in self.gmfset)))


class _GroundMotionField(object):
    """
    The Ground Motion Field generated by the given rupture
    """
    def __init__(self, imt, sa_period, sa_damping, rupture_id, gmf_nodes):
        self.imt = imt
        self.sa_period = sa_period
        self.sa_damping = sa_damping
        self.rupture_id = rupture_id
        self.gmf_nodes = gmf_nodes

    def __iter__(self):
        return iter(self.gmf_nodes)

    def __getitem__(self, key):
        return self.gmf_nodes[key]

    def __str__(self):
        """
        String representation of a _GroundMotionField object showing the
        content of the nodes (lon, lat an gmv). This is useful for debugging
        and testing.
        """
        mdata = ('imt=%(imt)s sa_period=%(sa_period)s '
                 'sa_damping=%(sa_damping)s rupture_id=%(rupture_id)s' %
                 vars(self))
        nodes = sorted(map(str, self.gmf_nodes))
        return 'GMF(%s\n%s)' % (mdata, '\n'.join(nodes))


class _GroundMotionFieldNode(object):

    # the signature is not (gmv, x, y) because the XML writer expect a location
    # object
    def __init__(self, gmv, loc):
        self.gmv = gmv
        self.location = loc

    def __str__(self):
        "Return lon, lat and gmv of the node in a compact string form"
        return '<X=%9.5f, Y=%9.5f, GMV=%9.7f>' % (
            self.location.x, self.location.y, self.gmv)


class GmfData(djm.Model):
    """
    Ground Motion Field: A collection of ground motion values and their
    respective geographical locations.
    """
    gmf = djm.ForeignKey('Gmf')
    task_no = djm.IntegerField(null=False)
    imt = djm.TextField(choices=IMT_CHOICES)
    sa_period = djm.FloatField(null=True)
    sa_damping = djm.FloatField(null=True)
    gmvs = fields.FloatArrayField()
    rupture_ids = fields.IntArrayField(null=True)
    site = djm.ForeignKey('HazardSite')
    objects = djm.GeoManager()

    class Meta:
        db_table = 'hzrdr\".\"gmf_data'
        ordering = ['gmf', 'task_no']


def _get_gmf(curs, gmf_id, imtype, sa_period, sa_damping):
    # returns site_id, gmvs for the given gmf_id and imt
    query = '''\
    SELECT site_id, array_concat(gmvs ORDER BY task_no)
    FROM hzrdr.gmf_data WHERE gmf_id=%s AND imt=%s {}
    GROUP BY site_id ORDER BY site_id'''
    if imtype == 'SA':
        curs.execute(query.format('AND sa_period=%s AND sa_damping=%s'),
                     (gmf_id, imtype, sa_period, sa_damping))
    else:
        curs.execute(query.format(''), (gmf_id, imtype))
    return curs.fetchall()


def get_gmvs_per_site(output, imt, sort=sorted):
    """
    Iterator for walking through all :class:`GmfData` objects associated
    to a given output. Notice that values for the same site are
    displayed together and then ordered according to the iml, so that
    it is possible to get reproducible outputs in the test cases.

    :param output: instance of :class:`openquake.engine.db.models.Output`

    :param string imt: a string with the IMT to extract

    :param sort: callable used for sorting the list of ground motion values.

    :returns: a list of ground motion values per each site
    """
    curs = getcursor('job_init')
    for site_id, gmvs in _get_gmf(curs, output.gmf.id, *from_string(imt)):
        yield sort(gmvs)


def get_gmfs_scenario(output, imt=None):
    """
    Iterator for walking through all :class:`GmfData` objects associated
    to a given output. Notice that the fields are ordered according to the
    location, so it is possible to get reproducible outputs in the test cases.

    :param output: instance of :class:`openquake.engine.db.models.Output`

    :param string imt: a string with the IMT to extract; the default
                       is None, all the IMT in the job.ini file are extracted

    :returns: an iterator over
              :class:`openquake.engine.db.models._GroundMotionField` instances
    """
    hc = output.oq_job.hazard_calculation
    if imt is None:
        imts = distinct(from_string(x) for x in hc.intensity_measure_types)
    else:
        imts = [from_string(imt)]
    curs = getcursor('job_init')
    for imt, sa_period, sa_damping in imts:
        nodes = collections.defaultdict(list)  # realization -> gmf_nodes
        for site_id, gmvs in _get_gmf(
                curs, output.gmf.id, imt, sa_period, sa_damping):
            for i, gmv in enumerate(gmvs):  # i is the realization index
                site = HazardSite.objects.get(pk=site_id)
                nodes[i].append(_GroundMotionFieldNode(gmv, site.location))
        for gmf_nodes in nodes.itervalues():
            yield _GroundMotionField(
                imt=imt,
                sa_period=sa_period,
                sa_damping=sa_damping,
                rupture_id=None,
                gmf_nodes=sorted(gmf_nodes, key=operator.attrgetter('gmv')))


class DisaggResult(djm.Model):
    """
    Storage for disaggregation historgrams. Each histogram is stored in
    `matrix` as a 6-dimensional numpy array (pickled). The dimensions of the
    matrix are as follows, in order:

    * magnitude
    * distance
    * longitude
    * latitude
    * epsilon
    * tectonic region type

    Bin edges are defined for all of these dimensions (except tectonic region
    type) as:

    * `mag_bin_edges`
    * `dist_bin_edges`
    * `lat_bin_edges`
    * `lon_bin_edges`
    * `eps_bin_edges`

    The size of the tectonic region type (TRT) dimension is simply determined
    by the length of `trts`.

    Additional metadata for the disaggregation histogram is stored, including
    location (POINT geometry), disaggregation PoE (Probability of Exceedance)
    and the corresponding IML (Intensity Measure Level) extracted from the
    hazard curve, logic tree path information, and investigation time.
    """

    output = djm.OneToOneField('Output', related_name="disagg_matrix")
    lt_realization = djm.ForeignKey('LtRealization')
    investigation_time = djm.FloatField()
    imt = djm.TextField(choices=IMT_CHOICES)
    iml = djm.FloatField()
    poe = djm.FloatField()
    sa_period = djm.FloatField(null=True)
    sa_damping = djm.FloatField(null=True)
    mag_bin_edges = fields.FloatArrayField()
    dist_bin_edges = fields.FloatArrayField()
    lon_bin_edges = fields.FloatArrayField()
    lat_bin_edges = fields.FloatArrayField()
    eps_bin_edges = fields.FloatArrayField()
    trts = fields.CharArrayField()
    location = djm.PointField(srid=DEFAULT_SRID)
    matrix = fields.PickleField()

    class Meta:
        db_table = 'hzrdr\".\"disagg_result'


class UHS(djm.Model):
    """
    UHS/Uniform Hazard Spectra:
    * "Uniform" meaning "the same PoE"
    * "Spectrum" because it covers a range/band of periods/frequencies

    Records in this table contain metadata for a collection of UHS data.
    """
    output = djm.OneToOneField('Output', null=True, related_name="uh_spectra")
    # FK only required for non-statistical results (i.e., mean or quantile
    # curves).
    lt_realization = djm.ForeignKey('LtRealization', null=True)
    investigation_time = djm.FloatField()
    poe = djm.FloatField()
    periods = fields.FloatArrayField()
    STAT_CHOICES = (
        (u'mean', u'Mean'),
        (u'quantile', u'Quantile'),
    )
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)

    class Meta:
        db_table = 'hzrdr\".\"uhs'

    def __iter__(self):
        """
        Iterate over the :class:`UHSData` which belong this object.
        """
        return self.uhsdata_set.iterator()


class UHSData(djm.Model):
    """
    UHS curve for a given location.
    """
    uhs = djm.ForeignKey('UHS')
    imls = fields.FloatArrayField()
    location = djm.PointField(srid=DEFAULT_SRID)

    class Meta:
        db_table = 'hzrdr\".\"uhs_data'


class LtSourceModel(djm.Model):
    """
    Identify a logic tree source model.
    """
    hazard_calculation = djm.ForeignKey('HazardCalculation')
    ordinal = djm.IntegerField()
    sm_lt_path = fields.CharArrayField()
    sm_name = djm.TextField(null=False)
    weight = djm.DecimalField(decimal_places=100, max_digits=101, null=True)

    def get_num_sources(self):
        """
        Return the number of sources in the model.
        """
        return sum(info.num_sources for info in self.trtmodel_set.all())

    def get_tectonic_region_types(self):
        """
        Return the tectonic region types in the model,
        ordered by number of sources.
        """
        return self.trtmodel_set.filter(
            lt_model=self, num_ruptures__gt=0).values_list(
            'tectonic_region_type', flat=True)

    class Meta:
        db_table = 'hzrdr\".\"lt_source_model'
        ordering = ['ordinal']

    def __iter__(self):
        """
        Yield the realizations corresponding to the given model
        """
        return self.ltrealization_set.all()


class TrtModel(djm.Model):
    """
    Source submodel containing sources of the same tectonic region type.
    """
    lt_model = djm.ForeignKey('LtSourceModel')
    tectonic_region_type = djm.TextField(null=False)
    num_sources = djm.IntegerField(null=False)
    num_ruptures = djm.IntegerField(null=False)
    min_mag = djm.FloatField(null=False)
    max_mag = djm.FloatField(null=False)
    gsims = fields.CharArrayField(null=True)

    def get_realizations(self, gsim_name):
        """
        Return the realizations associated to the current TrtModel and
        the given GSIM.

        :param str gsim_name: name of a GSIM class
        """
        assert gsim_name in self.gsims, gsim_name
        for art in AssocLtRlzTrtModel.objects.filter(
                trt_model=self.id, gsim=gsim_name):
            yield art.rlz

    def get_rlzs_by_gsim(self):
        """
        Return the realizations associated to the current TrtModel
        as a dictionary {gsim_name: [rlz, ...]}
        """
        dic = collections.defaultdict(list)
        for art in AssocLtRlzTrtModel.objects.filter(trt_model=self.id):
            dic[art.gsim].append(art.rlz)
        return dic

    class Meta:
        db_table = 'hzrdr\".\"trt_model'
        ordering = ['tectonic_region_type', 'num_sources']


class AssocLtRlzTrtModel(djm.Model):
    """
    Associations between logic tree realizations and TrtModels
    """
    rlz = djm.ForeignKey('LtRealization')
    trt_model = djm.ForeignKey('TrtModel')
    gsim = djm.TextField(null=False)

    class Meta:
        db_table = 'hzrdr\".\"assoc_lt_rlz_trt_model'
        ordering = ['id']


class LtRealization(djm.Model):
    """
    Identify a logic tree branch.
    """

    lt_model = djm.ForeignKey('LtSourceModel')
    ordinal = djm.IntegerField()
    weight = djm.DecimalField(decimal_places=100, max_digits=101)
    gsim_lt_path = fields.CharArrayField()

    @property
    def sm_lt_path(self):
        """
        The source model logic tree path extracted from the underlying
        source model
        """
        return self.lt_model.sm_lt_path

    class Meta:
        db_table = 'hzrdr\".\"lt_realization'
        ordering = ['ordinal']


## Tables in the 'riskr' schema.

class LossFraction(djm.Model):
    """
    Holds metadata for loss fraction data
    """
    output = djm.OneToOneField("Output", related_name="loss_fraction")
    variable = djm.TextField(choices=(("taxonomy", "taxonomy"),
                                      ("magnitude_distance",
                                       "Magnitude Distance"),
                                      ("coordinate", "Coordinate")))
    hazard_output = djm.ForeignKey(
        "Output", related_name="risk_loss_fractions")
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)
    poe = djm.FloatField(null=True)
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"loss_fraction'

    def __iter__(self):
        return iter(self.lossfractiondata_set.all())

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.output.hazard_metadata,
                self.statistics, self.quantile,
                self.variable, self.poe, self.loss_type)

    def display_value(self, value, rc):
        """
        Converts `value` in a form that is best suited to be
        displayed.

        :param rc:
           A `RiskCalculation` object used to get the bin width

        :returns: `value` if the attribute `variable` is equal to
           taxonomy. if the attribute `variable` is equal to
           `magnitude-distance`, then it extracts two integers (comma
           separated) from `value` and convert them into ranges
           encoded back as csv.
        """

        if self.variable == "taxonomy":
            return value
        elif self.variable == "magnitude_distance":
            magnitude, distance = map(float, value.split(","))
            return "%.4f,%.4f|%.4f,%.4f" % (
                magnitude * rc.mag_bin_width,
                (magnitude + 1) * rc.mag_bin_width,
                distance * rc.distance_bin_width,
                (distance + 1) * rc.distance_bin_width)
        elif self.variable == "coordinate":
            lon, lat = map(float, value.split(","))
            return "%.4f,%.4f|%.4f,%.4f" % (
                lon * rc.coordinate_bin_width,
                (lon + 1) * rc.coordinate_bin_width,
                lat * rc.coordinate_bin_width,
                (lat + 1) * rc.coordinate_bin_width)
        else:
            raise RuntimeError(
                "disaggregation of type %s not supported" % self.variable)

    def total_fractions(self):
        """
        :returns: a dictionary mapping values of `variable` (e.g. a
        taxonomy) to tuples yielding the associated absolute losses
        (e.g. the absolute losses for assets of a taxonomy) and the
        percentage (expressed in decimal format) over the total losses
        """
        cursor = connections['job_init'].cursor()

        total = self.lossfractiondata_set.aggregate(
            djm.Sum('absolute_loss')).values()[0]

        if not total:
            return {}

        query = """
        SELECT value, sum(absolute_loss)
        FROM riskr.loss_fraction_data
        WHERE loss_fraction_id = %s
        GROUP BY value
        """
        cursor.execute(query, (self.id,))

        rc = self.output.oq_job.risk_calculation

        loss_fraction = collections.namedtuple('loss_fraction', 'bin loss')

        return collections.OrderedDict(
            sorted(
                [loss_fraction(
                    self.display_value(value, rc),
                    (loss, loss / total))
                 for value, loss in cursor],
                key=operator.attrgetter('loss'),
                reverse=True))

    def iteritems(self):
        """
        Yields tuples with two elements. The first one is a location
        (described by a lon/lat tuple), the second one is a dictionary
        modeling the disaggregation of the losses on such location. In
        this dictionary, each key is a value of `variable`, and each
        corresponding value is a tuple holding the absolute losses and
        the fraction of losses occurring in that location.
        """
        rc = self.output.oq_job.risk_calculation
        cursor = connections['job_init'].cursor()

        # Partition by lon,lat because partitioning on geometry types
        # seems not supported in postgis 1.5
        query = """
        SELECT lon, lat, value,
               fraction_loss,
               SUM(fraction_loss) OVER w,
               COUNT(*) OVER w
        FROM (SELECT ST_X(location) as lon,
                     ST_Y(location) as lat,
              value, sum(absolute_loss) as fraction_loss
              FROM riskr.loss_fraction_data
              WHERE loss_fraction_id = %s
              GROUP BY location, value) g
        WINDOW w AS (PARTITION BY lon, lat)
        """

        cursor.execute(query, (self.id, ))

        def display_value_and_fractions(value, absolute_loss, total_loss):
            display_value = self.display_value(value, rc)

            if total_loss > 0:
                fraction = absolute_loss / total_loss
            else:
                # When a rupture is associated with a positive ground
                # shaking (gmv > 0) but with a loss = 0, we still
                # store this information. In that case, total_loss =
                # absolute_loss = 0
                fraction = 0
            return display_value, fraction

        # We iterate on loss fraction data by location in two steps.
        # First we fetch a loss fraction for a single location and a
        # single value. In the same query we get the number `count` of
        # bins stored for such location. Then, we fetch `count` - 1
        # fractions to finalize the fractions on the current location.

        while 1:
            data = cursor.fetchone()
            if data is None:
                raise StopIteration
            lon, lat, value, absolute_loss, total_loss, count = data

            display_value, fraction = display_value_and_fractions(
                value, absolute_loss, total_loss)
            node = [(lon, lat),
                    {display_value: (absolute_loss, fraction)}]

            data = cursor.fetchmany(count - 1)

            for lon, lat, value, absolute_loss, total_loss, count in data:
                display_value, fraction = display_value_and_fractions(
                    value, absolute_loss, total_loss)
                node[1][display_value] = (absolute_loss, fraction)

            node[1] = collections.OrderedDict(
                sorted([(k, v) for k, v in node[1].items()],
                       key=lambda kv: kv[0]))
            yield node

    def to_array(self):
        """
        :returns: the loss fractions as numpy array

        :NOTE:  (not memory efficient)
        """
        def to_tuple():
            for (lon, lat), data in self.iteritems():
                for taxonomy, (absolute_loss, fraction) in data.items():
                    yield lon, lat, taxonomy, absolute_loss, fraction

        return numpy.array(list(to_tuple()), dtype='f4, f4, S3, f4, f4')


class LossFractionData(djm.Model):
    loss_fraction = djm.ForeignKey(LossFraction)
    location = djm.PointField(srid=DEFAULT_SRID)
    value = djm.TextField()
    absolute_loss = djm.TextField()

    class Meta:
        db_table = 'riskr\".\"loss_fraction_data'

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return (self.loss_fraction.output_hash +
                ("%.5f" % self.location.x, "%.5f" % self.location.y,
                 self.value))

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('absolute_loss'))


class LossMap(djm.Model):
    '''
    Holds metadata for loss maps
    '''

    output = djm.OneToOneField("Output", related_name="loss_map")
    hazard_output = djm.ForeignKey("Output", related_name="risk_loss_maps")
    insured = djm.BooleanField(default=False)
    poe = djm.FloatField(null=True)
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"loss_map'

    def __iter__(self):
        return iter(self.lossmapdata_set.all())

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.output.hazard_metadata,
                self.statistics, self.quantile,
                self.insured, self.poe, self.loss_type)


class LossMapData(djm.Model):
    '''
    Holds an asset, its position and a value plus (for
    non-scenario maps) the standard deviation for its loss
    '''

    loss_map = djm.ForeignKey("LossMap")
    asset_ref = djm.TextField()
    value = djm.FloatField()
    std_dev = djm.FloatField(default=0.0, null=True)
    location = djm.PointField(srid=DEFAULT_SRID)

    class Meta:
        db_table = 'riskr\".\"loss_map_data'

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.loss_map.output_hash + (self.asset_ref,)

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('value', 'stddev'))


class AggregateLoss(djm.Model):
    output = djm.OneToOneField("Output", related_name="aggregate_loss")
    insured = djm.BooleanField(default=False)
    mean = djm.FloatField()
    std_dev = djm.FloatField()
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"aggregate_loss'

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.output.hazard_metadata,
                self.insured,
                self.mean, self.std_dev,
                self.loss_type)

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.output_hash

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, lambda x: operator.attrgetter('mean', 'std_dev'))

    def to_csv_str(self):
        """
        Convert AggregateLoss into a CSV string
        """
        return '\n'.join(data.to_csv_str('row-%d' % i)
                         for i, data in enumerate(self, 1))


class LossCurve(djm.Model):
    '''
    Holds the parameters common to a set of loss curves
    '''

    output = djm.OneToOneField("Output", related_name="loss_curve")
    hazard_output = djm.ForeignKey("Output", related_name="risk_loss_curves")
    aggregate = djm.BooleanField(default=False)
    insured = djm.BooleanField(default=False)

    # If the curve is a result of an aggregation over different
    # hazard_output the following fields must be set
    statistics = djm.TextField(null=True, choices=STAT_CHOICES)
    quantile = djm.FloatField(null=True)
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"loss_curve'

    def __iter__(self):
        if self.aggregate:
            return iter([self.aggregatelosscurvedata])
        else:
            return iter(self.losscurvedata_set.all())

    def to_csv_str(self):
        """
        Convert LossCurve into a CSV string
        """
        return '\n'.join(data.to_csv_str('row-%d' % i)
                         for i, data in enumerate(self, 1))

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.output.hazard_metadata,
                self.statistics, self.quantile,
                self.aggregate, self.insured, self.loss_type)


class LossCurveData(djm.Model):
    '''
    Holds the probabilities of exceedance for a given loss curve
    '''

    loss_curve = djm.ForeignKey("LossCurve")
    asset_ref = djm.TextField()
    asset_value = djm.FloatField()
    loss_ratios = fields.FloatArrayField()
    poes = fields.FloatArrayField()
    location = djm.PointField(srid=DEFAULT_SRID)
    average_loss_ratio = djm.FloatField()
    stddev_loss_ratio = djm.FloatField(blank=True, null=True)

    class Meta:
        db_table = 'riskr\".\"loss_curve_data'

    @property
    def losses(self):
        return numpy.array(self.loss_ratios) * self.asset_value

    @property
    def average_loss(self):
        return self.average_loss_ratio * self.asset_value

    @property
    def stddev_loss(self):
        if self.stddev_loss_ratio is not None:
            return self.stddev_loss_ratio * self.asset_value

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.loss_curve.output_hash + (self.asset_ref,)

    def assertAlmostEqual(self, data):
        return loss_curve_almost_equal(self, data)

    def to_csv_str(self, label):
        """
        Convert LossCurveData into a CSV string.

        :param str label:
            an identifier for the curve (for instance the asset_ref)
        """
        ratios = [label, 'Ratios'] + map(str, self.loss_ratios)
        data = ','.join(ratios) + '\n'
        data += ','.join(map(str, [self.asset_value, 'PoE'] + list(self.poes)))
        return data


class AggregateLossCurveData(djm.Model):
    '''
    Holds the probabilities of exceedance for the whole exposure model
    '''

    loss_curve = djm.OneToOneField("LossCurve")
    losses = fields.FloatArrayField()
    poes = fields.FloatArrayField()
    average_loss = djm.FloatField()
    stddev_loss = djm.FloatField(blank=True, null=True)

    class Meta:
        db_table = 'riskr\".\"aggregate_loss_curve_data'

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.loss_curve.output_hash

    def assertAlmostEqual(self, data):
        return loss_curve_almost_equal(self, data)

    def to_csv_str(self, label):
        """
        Convert AggregateLossCurveData into a CSV string.

        :param str label:
            an identifier for the curve (for instance the cost type)
        """
        data = ','.join(map(str, [label, 'Losses'] + list(self.losses))) + '\n'
        data += ','.join(map(str, ['', 'PoE'] + list(self.poes)))
        return data


class EventLoss(djm.Model):
    """
    Holds the aggregate loss we have for each rupture
    """

    #: Foreign key to an :class:`openquake.engine.db.models.Output`
    #: object with output_type == event_loss
    output = djm.OneToOneField('Output', related_name="event_loss")
    hazard_output = djm.ForeignKey(
        "Output", related_name="risk_event_loss_tables")
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"event_loss'

    def __iter__(self):
        return iter(self.eventlossdata_set.all().order_by('-aggregate_loss'))

    def to_csv_str(self):
        """
        Convert EventLoss into a CSV with fields rupture_tag, aggregate_loss
        """
        return '\n'.join('%s,%s' % (self.rupture.tag, self.aggregate_loss)
                         for data in self)

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        # FIXME(lp) this is not db-sequence independent
        return (self.output.output_type, self.output.hazard_metadata,
                self.loss_type)


class EventLossData(djm.Model):
    event_loss = djm.ForeignKey(EventLoss)
    rupture = djm.ForeignKey('SESRupture')
    aggregate_loss = djm.FloatField()

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.event_loss.output_hash + (self.rupture_id,)

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('aggregate_loss'))

    class Meta:
        db_table = 'riskr\".\"event_loss_data'

    def to_csv_str(self):
        """
        Convert EventLossData into a CSV string
        """
        return '%s,%s' % (self.rupture.id, self.aggregate_loss)


class BCRDistribution(djm.Model):
    '''
    Holds metadata for the benefit-cost ratio distribution
    '''

    output = djm.OneToOneField("Output", related_name="bcr_distribution")
    hazard_output = djm.ForeignKey(
        "Output", related_name="risk_bcr_distribution")
    loss_type = djm.TextField(choices=zip(LOSS_TYPES, LOSS_TYPES))

    class Meta:
        db_table = 'riskr\".\"bcr_distribution'

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.output.hazard_metadata,
                self.loss_type)


class BCRDistributionData(djm.Model):
    '''
    Holds the actual data for the benefit-cost ratio distribution
    '''

    bcr_distribution = djm.ForeignKey("BCRDistribution")
    asset_ref = djm.TextField()
    average_annual_loss_original = djm.FloatField()
    average_annual_loss_retrofitted = djm.FloatField()
    bcr = djm.FloatField()
    location = djm.PointField(srid=DEFAULT_SRID)

    class Meta:
        db_table = 'riskr\".\"bcr_distribution_data'

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.bcr_distribution.output_hash + (self.asset_ref,)

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data,
            operator.attrgetter('average_annual_loss_original',
                                'average_loss_retrofitted',
                                'bcr'))


class DmgState(djm.Model):
    """Holds the damage_states associated to a given output"""
    # they actually come from the fragility model xml input
    risk_calculation = djm.ForeignKey("RiskCalculation")
    dmg_state = djm.TextField(
        help_text="The name of the damage state")
    lsi = djm.PositiveSmallIntegerField(
        help_text="limit state index, to order the limit states")

    class Meta:
        db_table = 'riskr\".\"dmg_state'


class DmgDistPerAsset(djm.Model):
    """Holds the actual data for damage distributions per asset."""

    dmg_state = djm.ForeignKey("DmgState")
    exposure_data = djm.ForeignKey("ExposureData")
    mean = djm.FloatField()
    stddev = djm.FloatField()

    class Meta:
        db_table = 'riskr\".\"dmg_dist_per_asset'

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.dmg_state.dmg_state, self.exposure_data.asset_ref)

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.output_hash

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('mean', 'stddev'))

    @property
    def output(self):
        return self.dmg_state.rc_calculation.oqjob.output_set.get(
            output_type="dmg_dist_per_asset")


class DmgDistPerTaxonomy(djm.Model):
    """Holds the actual data for damage distributions per taxonomy."""

    dmg_state = djm.ForeignKey("DmgState")
    taxonomy = djm.TextField()
    mean = djm.FloatField()
    stddev = djm.FloatField()

    class Meta:
        db_table = 'riskr\".\"dmg_dist_per_taxonomy'

    @property
    def output(self):
        return self.dmg_state.rc_calculation.oqjob.output_set.get(
            output_type="dmg_dist_per_taxonomy")

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type,
                self.dmg_state.dmg_state, self.taxonomy)

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.output_hash

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('mean', 'stddev'))


class DmgDistTotal(djm.Model):
    """Holds the actual 'total damage distribution' values for for an entire
    calculation. There should be  one record per calculation per damage state.
    """

    dmg_state = djm.ForeignKey("DmgState")
    mean = djm.FloatField()
    stddev = djm.FloatField()

    class Meta:
        db_table = 'riskr\".\"dmg_dist_total'

    @property
    def output(self):
        return self.dmg_state.rc_calculation.oqjob.output_set.get(
            output_type="dmg_dist_total")

    @property
    def output_hash(self):
        """
        :returns:
            a (db-sequence independent) tuple that identifies this output among
            which the ones created in the same calculation
        """
        return (self.output.output_type, self.dmg_state.dmg_state, )

    @property
    def data_hash(self):
        """
        A db-sequence independent tuple that identifies this output
        """
        return self.output_hash

    def assertAlmostEqual(self, data):
        return risk_almost_equal(
            self, data, operator.attrgetter('mean', 'stddev'))


## Tables in the 'riski' schema.


class ExposureModel(djm.Model):
    '''
    A risk exposure model
    '''

    job = djm.OneToOneField("OqJob")
    name = djm.TextField()
    description = djm.TextField(null=True)
    category = djm.TextField()
    taxonomy_source = djm.TextField(
        null=True, help_text="the taxonomy system used to classify the assets")
    AREA_CHOICES = (
        (u'aggregated', u'Aggregated area value'),
        (u'per_asset', u'Per asset area value'),
    )
    area_type = djm.TextField(null=True, choices=AREA_CHOICES)
    area_unit = djm.TextField(null=True)
    deductible_absolute = djm.BooleanField(default=True)
    insurance_limit_absolute = djm.BooleanField(default=True)

    class Meta:
        db_table = 'riski\".\"exposure_model'

    def taxonomies_in(self, region_constraint):
        """
        :param str region_constraint:
            polygon in wkt format the assets must be contained into
        :returns:
            A dictionary mapping each taxonomy with the number of assets
            contained in `region_constraint`
        """

        return ExposureData.objects.taxonomies_contained_in(
            self.id, region_constraint)

    def unit(self, loss_type):
        if loss_type == "fatalities":
            return "people"
        else:
            return self.costtype_set.get(name=loss_type).unit

    def has_insurance_bounds(self):
        return not self.exposuredata_set.filter(
            (djm.Q(cost__deductible_absolute__isnull=True) |
             djm.Q(cost__insurance_limit_absolute__isnull=True))).exists()

    def has_retrofitted_costs(self):
        return not (
            self.exposuredata_set.filter(
                cost__converted_retrofitted_cost__isnull=True)).exists()

    def has_time_event(self, time_event):
        return (
            self.exposuredata_set.filter(occupancy__period=time_event).count()
            ==
            self.exposuredata_set.count())

    def supports_loss_type(self, loss_type):
        """
        :returns:
            True if the exposure contains the asset data needed
            for computing losses for `loss_type`
        """
        if loss_type != "fatalities":
            ct = cost_type(loss_type)
            return self.exposuredata_set.filter(
                cost__cost_type__name=ct).exists()
        else:
            if self.category == "population":
                return not self.exposuredata_set.filter(
                    number_of_units__isnull=True).exists()
            else:
                return not self.exposuredata_set.filter(
                    occupancy__isnull=True).exists()


class CostType(djm.Model):
    COST_TYPE_CHOICES = (
        ("structuralCost", "structuralCost"),
        ("retrofittedStructuralCost", "retrofittedStructuralCost"),
        ("nonStructuralCost", "nonStructuralCost"),
        ("contentsCost", "contentsCost"),
        ("businessInterruptionCost", "businessInterruptionCost"))
    CONVERSION_CHOICES = (
        (u'aggregated', u'Aggregated economic value'),
        (u'per_area', u'Per area economic value'),
        (u'per_asset', u'Per asset economic value'),
    )

    exposure_model = djm.ForeignKey(ExposureModel)
    name = djm.TextField(choices=COST_TYPE_CHOICES)
    conversion = djm.TextField(choices=CONVERSION_CHOICES)
    unit = djm.TextField(null=True)
    retrofitted_conversion = djm.TextField(
        null=True, choices=CONVERSION_CHOICES)
    retrofitted_unit = djm.TextField(null=True)

    class Meta:
        db_table = 'riski\".\"cost_type'


class Occupancy(djm.Model):
    '''
    Asset occupancy data
    '''

    exposure_data = djm.ForeignKey("ExposureData")
    period = djm.TextField()
    occupants = djm.FloatField()

    class Meta:
        db_table = 'riski\".\"occupancy'


class AssetManager(djm.GeoManager):
    """
    Asset manager
    """

    def get_asset_chunk(self, rc, taxonomy, offset, size):
        """
        :returns:

           a list of instances of
           :class:`openquake.engine.db.models.ExposureData` (ordered
           by location) contained in `region_constraint`(embedded in
           the risk calculation `rc`) of `taxonomy` associated with
           the `openquake.engine.db.models.ExposureModel` associated
           with `rc`.

           It also add an annotation to each ExposureData object to provide the
           occupants value for the risk calculation given in input and the cost
           for each cost type considered in `rc`
        """

        query, args = self._get_asset_chunk_query_args(
            rc, taxonomy, offset, size)
        return list(self.raw(query, args))

    def _get_asset_chunk_query_args(self, rc, taxonomy, offset, size):
        """
        Build a parametric query string and the corresponding args for
        #get_asset_chunk
        """
        args = (rc.exposure_model.id, taxonomy,
                "SRID=4326; %s" % rc.region_constraint.wkt)

        people_field, occupants_cond, occupancy_join, occupants_args = (
            self._get_people_query_helper(
                rc.exposure_model.category, rc.time_event))

        args += occupants_args + (size, offset)

        cost_type_fields, cost_type_joins = self._get_cost_types_query_helper(
            rc.exposure_model.costtype_set.all())

        query = """
            SELECT riski.exposure_data.*,
                   {people_field} AS people,
                   {costs}
            FROM riski.exposure_data
            {occupancy_join}
            ON riski.exposure_data.id = riski.occupancy.exposure_data_id
            {costs_join}
            WHERE exposure_model_id = %s AND
                  taxonomy = %s AND
                  ST_COVERS(ST_GeographyFromText(%s), site) AND
                  {occupants_cond}
            GROUP BY riski.exposure_data.id
            ORDER BY ST_X(geometry(site)), ST_Y(geometry(site))
            LIMIT %s OFFSET %s
            """.format(people_field=people_field,
                       occupants_cond=occupants_cond,
                       costs=cost_type_fields,
                       costs_join=cost_type_joins,
                       occupancy_join=occupancy_join)

        return query, args

    def _get_people_query_helper(self, category, time_event):
        """
        Support function for _get_asset_chunk_query_args
        """
        args = ()
        # if the exposure model is of type "population" we extract the
        # data from the `number_of_units` field
        if category == "population":
            occupants_field = "number_of_units"
            occupants_cond = "1 = 1"
            occupancy_join = ""
        else:
            # otherwise we will "left join" the occupancy table
            occupancy_join = "LEFT JOIN riski.occupancy"
            occupants_field = "AVG(riski.occupancy.occupants)"

            # and the time_event is not specified we compute the
            # number of occupants by averaging the occupancy data for
            # each asset, otherwise we get the unique proper occupants
            # value.
            if time_event is None:
                occupants_cond = "1 = 1"
            else:
                args += (time_event,)
                occupants_cond = "riski.occupancy.period = %s"
        return occupants_field, occupants_cond, occupancy_join, args

    def _get_cost_types_query_helper(self, cost_types):
        """
        Support function for _get_asset_chunk_query_args
        """
        # For each cost type associated with the exposure model we
        # join the `cost` table to the current queryset in order to
        # lookup for a cost value for each asset.

        # Actually we extract 4 values: the cost, the retrofitted
        # cost, the deductible and the insurance limit

        costs = []
        costs_join = ""

        for cost_type in cost_types:
            # here the max value is irrelevant as we are sureto join
            # against one row
            costs.append("max(%s.converted_cost) AS %s" % (cost_type.name,
                                                           cost_type.name))
            costs.append(
                "max(%s.converted_retrofitted_cost) AS retrofitted_%s" % (
                    cost_type.name, cost_type.name))
            costs.append(
                "max(%s.deductible_absolute) AS deductible_%s" % (
                    cost_type.name, cost_type.name))
            costs.append(
                "max(%s.insurance_limit_absolute) AS insurance_limit_%s" % (
                    cost_type.name, cost_type.name))

            costs_join += """
            LEFT JOIN riski.cost AS %(name)s
            ON %(name)s.cost_type_id = '%(id)s' AND
            %(name)s.exposure_data_id = riski.exposure_data.id""" % dict(
                name=cost_type.name, id=cost_type.id)

        return ", ".join(costs), costs_join

    def taxonomies_contained_in(self, exposure_model_id, region_constraint):
        """

        :returns:
            A dictionary which map each taxonomy associated with
            `exposure_model` and contained in `region_constraint` with the
            number of assets.
        """
        cursor = connections['job_init'].cursor()

        cursor.execute("""
        SELECT riski.exposure_data.taxonomy, COUNT(*)
        FROM riski.exposure_data WHERE
        exposure_model_id = %s AND ST_COVERS(ST_GeographyFromText(%s), site)
        group by riski.exposure_data.taxonomy
        """, [exposure_model_id, "SRID=4326; %s" % region_constraint.wkt])

        return dict(cursor)


class ExposureData(djm.Model):
    '''
    Per-asset risk exposure data
    '''

    NO_RETROFITTING_COST = "no retrofitting cost"

    exposure_model = djm.ForeignKey("ExposureModel")
    asset_ref = djm.TextField()
    taxonomy = djm.TextField()
    site = djm.PointField(geography=True)

    number_of_units = djm.FloatField(
        null=True, help_text="number of assets, people, etc.")
    area = djm.FloatField(null=True)

    objects = AssetManager()

    class Meta:
        db_table = 'riski\".\"exposure_data'

    def __str__(self):
        return "%s (%s-%s @ %s)" % (
            self.id, self.exposure_model_id, self.asset_ref, self.site)

    @staticmethod
    def per_asset_value(
            cost, cost_type, area, area_type, number_of_units, category):
        """
        Return per-asset value for the given exposure data set.

        Calculate per asset value by considering the given exposure
        data as follows:

            case 1: cost type: aggregated:
                cost = economic value
            case 2: cost type: per asset:
                cost * number (of assets) = economic value
            case 3: cost type: per area and area type: aggregated:
                cost * area = economic value
            case 4: cost type: per area and area type: per asset:
                cost * area * number = economic value

        The same "formula" applies to contenst/retrofitting cost analogously.

        :returns:
            The per-asset value as a `float`.
        :raises:
            `ValueError` in case of a malformed (risk exposure data) input.
        """
        if category is not None and category == "population":
            return number_of_units
        if cost_type == "aggregated":
            return cost
        elif cost_type == "per_asset":
            return cost * number_of_units
        elif cost_type == "per_area":
            if area_type == "aggregated":
                return cost * area
            elif area_type == "per_asset":
                return cost * area * number_of_units
        raise ValueError("Invalid input")

    # we expect several annotations depending on "loss_type" to be
    # present. See `get_asset_chunk` for details.

    def value(self, loss_type):
        """
        Extract the value of the asset for the given `loss_type`.
        Although the Django Model definition does not have a value for
        each loss type, we rely on the fact that an annotation on the
        asset named `loss_type` is present.
        """
        if loss_type == "fatalities":
            return getattr(self, "people")

        return getattr(self, loss_type)

    def retrofitted(self, loss_type):
        """
        Extract the retrofitted cost of the asset for the given
        `loss_type`. See the method `value` for details.
        """
        return getattr(self, "retrofitted_%s" % loss_type)

    def deductible(self, loss_type):
        """
        Extract the deductible limit of the asset for the given
        `loss_type`. See the method `value` for details.
        """
        return (getattr(self, "deductible_%s" % loss_type) /
                getattr(self, loss_type))

    def insurance_limit(self, loss_type):
        """
        Extract the insurance limit of the asset for the given
        `loss_type`. See the method `value` for details.
        """
        return (getattr(self, "insurance_limit_%s" % loss_type) /
                getattr(self, loss_type))


def make_absolute(limit, value, is_absolute=None):
    """
    :returns:
        `limit` if `is_absolute` is True or `limit` is None,
        else `limit` * `value`
    """
    if limit is not None:
        if not is_absolute:
            return value * limit
        else:
            return limit


class Cost(djm.Model):
    exposure_data = djm.ForeignKey(ExposureData)
    cost_type = djm.ForeignKey(CostType)
    converted_cost = djm.FloatField()
    converted_retrofitted_cost = djm.FloatField(null=True)
    deductible_absolute = djm.FloatField(null=True, blank=True)
    insurance_limit_absolute = djm.FloatField(null=True, blank=True)

    class Meta:
        db_table = 'riski\".\"cost'

## Tables in the 'htemp' schema.


class HazardSite(djm.Model):
    """
    Contains pre-computed site parameter matrices. ``lons`` and ``lats``
    represent the calculation sites of interest. The associated site parameters
    are from the closest point in a site model in relation to each calculation
    point of interest.

    Used only if a calculation defines a site model (otherwise, reference
    parameters are use for all points of interest).
    """

    hazard_calculation = djm.ForeignKey('HazardCalculation')
    location = djm.PointField(srid=DEFAULT_SRID)

    class Meta:
        db_table = 'hzrdi\".\"hazard_site'

########NEW FILE########
__FILENAME__ = routers
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


'''
Database routers for the OpenQuake DB
'''

import re


class OQRouter(object):
    '''
    Helps determine which database credentials to use for read/write operations
    on a given model
    '''

    # Parses the schema name from model's _meta.db_table
    SCHEMA_TABLE_RE = re.compile(r'^(\w+)"\.\"(\w+)')

    @classmethod
    def _schema_table_from_model(cls, model):
        '''
        Get the db schema name from a given model.

        :param model: a Django model object

        :returns: schema name, or None if no schema is defined
        '''
        parts = model._meta.db_table.split('"."')  # pylint: disable=W0212
        if len(parts) == 2:
            return parts
        else:
            return None, parts[0]

    def db_for_read(self, model, **_hints):
        '''
        Get the name of the correct db configuration to use for read operations
        on the given model.
        '''
        schema = self._schema_table_from_model(model)[0]

        if schema in ("admin",):
            # The db name for these is the same as the schema
            return schema
        elif schema in ("hzrdr", "riskr", "htemp", "hzrdi", "riski", "uiapi"):
            return "job_init"
        else:
            return '%s_read' % schema

    def db_for_write(self, model, **_hints):
        '''
        Get the name of the correct db configuration to use for write
        operations on the given model.
        '''
        schema, table = self._schema_table_from_model(model)

        if schema in ('admin',):
            # The db name for these is the same as the schema
            return schema
        elif schema == "uiapi" and table == "output":
            return "job_init"
        elif schema in ("hzrdi", "riski", "uiapi", "hzrdr", "riskr", "htemp"):
            return "job_init"
        else:
            return '%s_write' % schema

    def allow_relation(self, _obj1, _obj2, **_hints):  # pylint: disable=R0201
        '''
        Determine if relations between two model objects should be allowed.

        For now, this always returns True.
        '''
        return True

########NEW FILE########
__FILENAME__ = engine
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""Engine: A collection of fundamental functions for initializing and running
calculations."""

import ConfigParser
import csv
import getpass
import os
import sys
import time
import logging
import warnings
from contextlib import contextmanager
from datetime import datetime

import celery.task.control

import openquake.engine

from django.core import exceptions
from django import db as django_db
from lxml import etree

from openquake.engine import logs
from openquake.engine.db import models
from openquake.engine.job.validation import validate
from openquake.engine.utils import config, get_calculator_class
from openquake.engine.celery_node_monitor import CeleryNodeMonitor
from openquake.engine.writer import CacheInserter
from openquake.engine.settings import DATABASES
from openquake.engine.db.models import Performance

from openquake import hazardlib
from openquake import risklib
from openquake import nrmllib

from openquake.commonlib.general import str2bool


INPUT_TYPES = dict(models.INPUT_TYPE_CHOICES)

UNABLE_TO_DEL_HC_FMT = 'Unable to delete hazard calculation: %s'
UNABLE_TO_DEL_RC_FMT = 'Unable to delete risk calculation: %s'

LOG_FORMAT = ('[%(asctime)s %(calc_domain)s #%(calc_id)s %(hostname)s '
              '%(levelname)s %(processName)s/%(process)s %(name)s] '
              '%(message)s')

TERMINATE = str2bool(config.get('celery', 'terminate_workers_on_revoke'))


def cleanup_after_job(job, terminate):
    """
    Release the resources used by an openquake job.
    In particular revoke the running tasks (if any).

    :param int job_id: the job id
    :param bool terminate: the celery revoke command terminate flag
    """
    # Using the celery API, terminate and revoke and terminate any running
    # tasks associated with the current job.
    task_ids = Performance.objects.filter(
        oq_job=job, operation='storing task id', task_id__isnull=False)\
        .values_list('task_id', flat=True)
    if task_ids:
        logs.LOG.warn('Revoking %d tasks', len(task_ids))
    else:  # this is normal when OQ_NO_DISTRIBUTE=1
        logs.LOG.debug('No task to revoke')
    for tid in task_ids:
        celery.task.control.revoke(tid, terminate=terminate)
        logs.LOG.debug('Revoked task %s', tid)


def _update_log_record(self, record):
    """
    Massage a log record before emitting it. Intended to be used by the
    custom log handlers defined in this module.
    """
    if not hasattr(record, 'hostname'):
        record.hostname = '-'
    if not hasattr(record, 'calc_domain'):
        record.calc_domain = self.calc_domain
    if not hasattr(record, 'calc_id'):
        record.calc_id = self.calc.id
    logger_name_prefix = 'oq.%s.%s' % (record.calc_domain, record.calc_id)
    if record.name.startswith(logger_name_prefix):
        record.name = record.name[len(logger_name_prefix):].lstrip('.')
        if not record.name:
            record.name = 'root'


class LogStreamHandler(logging.StreamHandler):
    """
    Log stream handler
    """
    def __init__(self, calc_domain, calc):
        super(LogStreamHandler, self).__init__()
        self.setFormatter(logging.Formatter(LOG_FORMAT))
        self.calc_domain = calc_domain
        self.calc = calc

    def emit(self, record):  # pylint: disable=E0202
        _update_log_record(self, record)
        super(LogStreamHandler, self).emit(record)


class LogFileHandler(logging.FileHandler):
    """
    Log file handler
    """
    def __init__(self, calc_domain, calc, log_file):
        super(LogFileHandler, self).__init__(log_file)
        self.setFormatter(logging.Formatter(LOG_FORMAT))
        self.calc_domain = calc_domain
        self.calc = calc
        self.log_file = log_file

    def emit(self, record):  # pylint: disable=E0202
        _update_log_record(self, record)
        super(LogFileHandler, self).emit(record)


@contextmanager
def job_stats(job):
    """
    A context manager saving information such as the number of sites
    and the disk space occupation in the job_stats table. The information
    is saved at the end of the job, even if the job fails.
    """
    dbname = DATABASES['default']['NAME']
    curs = models.getcursor('job_init')
    curs.execute("select pg_database_size(%s)", (dbname,))
    dbsize = curs.fetchall()[0][0]

    # create job stats, which implicitly records the start time for the job
    js = models.JobStats.objects.create(oq_job=job)
    job.is_running = True
    job.save()
    try:
        yield
    finally:
        job.is_running = False
        job.save()

        # save job stats
        curs.execute("select pg_database_size(%s)", (dbname,))
        new_dbsize = curs.fetchall()[0][0]
        js.disk_space = new_dbsize - dbsize
        js.stop_time = datetime.utcnow()
        js.save()

        cleanup_after_job(job, terminate=TERMINATE)


def prepare_job(user_name="openquake", log_level='progress'):
    """
    Create job for the given user, return it.

    :param str username:
        Username of the user who owns/started this job. If the username doesn't
        exist, a user record for this name will be created.
    :param str log_level:
        Defaults to 'progress'. Specify a logging level for this job. This
        level can be passed, for example, from the command line interface using
        the `--log-level` directive.
    :returns:
        :class:`openquake.engine.db.models.OqJob` instance.
    """
    return models.OqJob.objects.create(
        user_name=user_name,
        log_level=log_level,
        oq_version=openquake.engine.__version__,
        nrml_version=nrmllib.__version__,
        hazardlib_version=hazardlib.__version__,
        risklib_version=risklib.__version__,
    )


def parse_config(source):
    """Parse a dictionary of parameters from an INI-style config file.

    :param source:
        File-like object containing the config parameters.
    :returns:
        Two dictionaries (as a 2-tuple). The first contains all of the
        parameters/values parsed from the job.ini file. The second contains
        absolute paths to all of the files referenced in the job.ini, keyed by
        the parameter name.
    """
    cp = ConfigParser.ConfigParser()
    cp.readfp(source)

    base_path = os.path.dirname(
        os.path.join(os.path.abspath('.'), source.name))
    params = dict(base_path=base_path)
    params['inputs'] = dict()

    # Directory containing the config file we're parsing.
    base_path = os.path.dirname(os.path.abspath(source.name))

    for sect in cp.sections():
        for key, value in cp.items(sect):
            if key == 'sites_csv':
                # Parse site coordinates from the csv file,
                # return as MULTIPOINT WKT:
                path = value
                if not os.path.isabs(path):
                    # It's a relative path
                    path = os.path.join(base_path, path)
                params['sites'] = _parse_sites_csv(open(path, 'r'))
            elif key.endswith('_file'):
                input_type = key[:-5]
                if not input_type in INPUT_TYPES:
                    raise ValueError(
                        'The parameter %s in the .ini file does '
                        'not correspond to a valid input type' % key)
                path = value
                # The `path` may be a path relative to the config file, or it
                # could be an absolute path.
                if not os.path.isabs(path):
                    # It's a relative path.
                    path = os.path.join(base_path, path)

                params['inputs'][input_type] = path
            else:
                params[key] = value

    # convert back to dict as defaultdict is not pickleable
    params['inputs'] = dict(params['inputs'])

    # load source inputs (the paths are the source_model_logic_tree)
    smlt = params['inputs'].get('source_model_logic_tree')
    if smlt:
        params['inputs']['source'] = [
            os.path.join(base_path, src_path)
            for src_path in _collect_source_model_paths(smlt)]

    return params


def _parse_sites_csv(fh):
    """
    Get sites of interest from a csv file. The csv file (``fh``) is expected to
    have 2 columns: lon,lat.

    :param fh:
        File-like containing lon,lat coordinates in csv format.

    :returns:
        MULTIPOINT WKT representing all of the sites in the csv file.
    """
    reader = csv.reader(fh)
    coords = []
    for lon, lat in reader:
        coords.append('%s %s' % (lon, lat))
    return 'MULTIPOINT(%s)' % ', '.join(coords)


def create_calculation(model, params):
    """
    Given a params `dict` parsed from the config file, create a
    :class:`~openquake.engine.db.models.HazardCalculation`.

    :param model:
        a Calculation class object
    :param dict params:
        Dictionary of parameter names and values. Parameter names should match
        exactly the field names of
        :class:`openquake.engine.db.model.HazardCalculation`.
    :returns:
        an instance of a newly created `model`
    """
    if "export_dir" in params:
        params["export_dir"] = os.path.abspath(params["export_dir"])

    calc_fields = model._meta.get_all_field_names()

    for param in set(params) - set(calc_fields):
        # FIXME(lp). Django 1.3 does not allow using _id fields in model
        # __init__. We will check these fields in pre-execute phase
        if param not in [
                'preloaded_exposure_model_id', 'hazard_output_id',
                'hazard_calculation_id']:
            msg = "Unknown parameter '%s'. Ignoring."
            msg %= param
            warnings.warn(msg, RuntimeWarning)
            params.pop(param)

    calc = model.create(**params)
    calc.full_clean()
    calc.save()

    return calc


def _collect_source_model_paths(smlt):
    """
    Given a path to a source model logic tree or a file-like, collect all of
    the soft-linked path names to the source models it contains and return them
    as a uniquified list (no duplicates).
    """
    src_paths = []
    tree = etree.parse(smlt)
    for branch_set in tree.xpath('//nrml:logicTreeBranchSet',
                                 namespaces=nrmllib.PARSE_NS_MAP):

        if branch_set.get('uncertaintyType') == 'sourceModel':
            for branch in branch_set.xpath(
                    './nrml:logicTreeBranch/nrml:uncertaintyModel',
                    namespaces=nrmllib.PARSE_NS_MAP):
                src_paths.append(branch.text)
    return sorted(set(src_paths))


# used by bin/openquake and openquake.server.views
def run_calc(job, log_level, log_file, exports, job_type):
    """
    Run a calculation.

    :param job:
        :class:`openquake.engine.db.model.OqJob` instance which references a
        valid :class:`openquake.engine.db.models.RiskCalculation` or
        :class:`openquake.engine.db.models.HazardCalculation`.
    :param str log_level:
        The desired logging level. Valid choices are 'debug', 'info',
        'progress', 'warn', 'error', and 'critical'.
    :param str log_file:
        Complete path (including file name) to file where logs will be written.
        If `None`, logging will just be printed to standard output.
    :param list exports:
        A (potentially empty) list of export targets. Currently only "xml" is
        supported.
    :param str job_type:
        'hazard' or 'risk'
    """
    calc_mode = getattr(job, '%s_calculation' % job_type).calculation_mode
    calculator = get_calculator_class(job_type, calc_mode)(job)
    calc = job.calculation

    # initialize log handlers
    handler = (LogFileHandler(job_type, calc, log_file) if log_file
               else LogStreamHandler(job_type, calc))
    logging.root.addHandler(handler)
    try:
        with job_stats(job):  # run the job
            logs.set_level(log_level)
            _do_run_calc(calculator, exports, job_type)
    finally:
        logging.root.removeHandler(handler)
    return calculator


def _switch_to_job_phase(job, job_type, status):
    """
    Switch to a particular phase of execution.

    :param job:
        An :class:`~openquake.engine.db.models.OqJob` instance.
    :param str job_type:
        calculation type (hazard|risk)
    :param str status:
        one of the following: pre_executing, executing,
        post_executing, post_processing, export, clean_up, complete
    """
    job.status = status
    job.save()
    logs.LOG.progress("%s (%s)", status, job_type)


def _do_run_calc(calc, exports, job_type):
    """
    Step through all of the phases of a calculation, updating the job
    status at each phase.

    :param calc:
        An :class:`~openquake.engine.calculators.base.Calculator` instance.
    :param list exports:
        a (potentially empty) list of export targets, currently only "xml" is
        supported
    :param str job_type:
        calculation type (hazard|risk)
    """
    job = calc.job

    _switch_to_job_phase(job, job_type, "pre_executing")
    calc.pre_execute()

    _switch_to_job_phase(job, job_type, "executing")
    calc.execute()

    _switch_to_job_phase(job, job_type, "post_executing")
    calc.post_execute()

    _switch_to_job_phase(job, job_type, "post_processing")
    calc.post_process()

    _switch_to_job_phase(job, job_type, "export")
    calc.export(exports=exports)

    _switch_to_job_phase(job, job_type, "clean_up")
    calc.clean_up()

    CacheInserter.flushall()  # flush caches into the db

    _switch_to_job_phase(job, job_type, "complete")
    logs.LOG.debug("*> complete")


def del_haz_calc(hc_id):
    """
    Delete a hazard calculation and all associated outputs.

    :param hc_id:
        ID of a :class:`~openquake.engine.db.models.HazardCalculation`.
    """
    try:
        hc = models.HazardCalculation.objects.get(id=hc_id)
    except exceptions.ObjectDoesNotExist:
        raise RuntimeError('Unable to delete hazard calculation: '
                           'ID=%s does not exist' % hc_id)

    user = getpass.getuser()
    if hc.oqjob.user_name == user:
        # we are allowed to delete this

        # but first, check if any risk calculations are referencing any of our
        # outputs, or the hazard calculation itself
        msg = UNABLE_TO_DEL_HC_FMT % (
            'The following risk calculations are referencing this hazard'
            ' calculation: %s'
        )

        # check for a reference to hazard outputs
        assoc_outputs = models.RiskCalculation.objects.filter(
            hazard_output__oq_job__hazard_calculation=hc_id
        )
        if assoc_outputs.count() > 0:
            raise RuntimeError(msg % ', '.join([str(x.id)
                                                for x in assoc_outputs]))

        # check for a reference to the hazard calculation itself
        assoc_calcs = models.RiskCalculation.objects.filter(
            hazard_calculation=hc_id
        )
        if assoc_calcs.count() > 0:
            raise RuntimeError(msg % ', '.join([str(x.id)
                                                for x in assoc_calcs]))

        # No risk calculation are referencing what we want to delete.
        # Carry on with the deletion.
        hc.delete(using='admin')
    else:
        # this doesn't belong to the current user
        raise RuntimeError(UNABLE_TO_DEL_HC_FMT % 'Access denied')


def del_risk_calc(rc_id):
    """
    Delete a risk calculation and all associated outputs.

    :param hc_id:
        ID of a :class:`~openquake.engine.db.models.RiskCalculation`.
    """
    try:
        rc = models.RiskCalculation.objects.get(id=rc_id)
    except exceptions.ObjectDoesNotExist:
        raise RuntimeError('Unable to delete risk calculation: '
                           'ID=%s does not exist' % rc_id)

    if rc.oqjob.user_name == getpass.getuser():
        # we are allowed to delete this
        rc.delete(using='admin')
    else:
        # this doesn't belong to the current user
        raise RuntimeError('Unable to delete risk calculation: '
                           'Access denied')


def list_hazard_outputs(hc_id):
    """
    List the outputs for a given
    :class:`~openquake.engine.db.models.HazardCalculation`.

    :param hc_id:
        ID of a hazard calculation.
    """
    print_outputs_summary(get_outputs('hazard', hc_id))


def touch_log_file(log_file):
    """
    If a log file destination is specified, attempt to open the file in
    'append' mode ('a'). If the specified file is not writable, an
    :exc:`IOError` will be raised.
    """
    open(os.path.abspath(log_file), 'a').close()


def print_results(calc_id, duration, list_outputs):
    print 'Calculation %d completed in %d seconds. Results:' % (
        calc_id, duration)
    list_outputs(calc_id)


def print_outputs_summary(outputs):
    """
    List of :class:`openquake.engine.db.models.Output` objects.
    """
    if len(outputs) > 0:
        print 'id | output_type | name'
        for o in outputs.order_by('output_type'):
            print '%s | %s | %s' % (
                o.id, o.get_output_type_display(), o.display_name)


def run_job(cfg_file, log_level, log_file, exports, hazard_output_id=None,
            hazard_calculation_id=None):
    """
    Run a job using the specified config file and other options.

    :param str cfg_file:
        Path to calculation config (INI-style) file.
    :param str log_level:
        'debug', 'info', 'warn', 'error', or 'critical'
    :param str log_file:
        Path to log file.
    :param list exports:
        A list of export types requested by the user. Currently only 'xml'
        is supported.
    :param str hazard_ouput_id:
        The Hazard Output ID used by the risk calculation (can be None)
    :param str hazard_calculation_id:
        The Hazard Calculation ID used by the risk calculation (can be None)
    """
    with CeleryNodeMonitor(openquake.engine.no_distribute(), interval=3):
        hazard = hazard_output_id is None and hazard_calculation_id is None
        if log_file is not None:
            touch_log_file(log_file)

        job = job_from_file(
            cfg_file, getpass.getuser(), log_level, exports, hazard_output_id,
            hazard_calculation_id)

        # Instantiate the calculator and run the calculation.
        t0 = time.time()
        run_calc(
            job, log_level, log_file, exports, 'hazard' if hazard else 'risk')
        duration = time.time() - t0
        if hazard:
            if job.status == 'complete':
                print_results(job.hazard_calculation.id,
                              duration, list_hazard_outputs)
            else:
                sys.exit('Calculation %s failed' %
                         job.hazard_calculation.id)
        else:
            if job.status == 'complete':
                print_results(job.risk_calculation.id,
                              duration, list_risk_outputs)
            else:
                sys.exit('Calculation %s failed' %
                         job.risk_calculation.id)


@django_db.transaction.commit_on_success
def job_from_file(cfg_file_path, username, log_level, exports,
                  hazard_output_id=None, hazard_calculation_id=None):
    """
    Create a full job profile from a job config file.

    :param str cfg_file_path:
        Path to the job.ini.
    :param str username:
        The user who will own this job profile and all results.
    :param str log_level:
        Desired log level.
    :param exports:
        List of desired export types.
    :param int hazard_output_id:
        ID of a hazard output to use as input to this calculation. Specify
        this xor ``hazard_calculation_id``.
    :param int hazard_calculation_id:
        ID of a complete hazard calculation to use as input to this
        calculation. Specify this xor ``hazard_output_id``.

    :returns:
        :class:`openquake.engine.db.models.OqJob` object
    :raises:
        `RuntimeError` if the input job configuration is not valid
    """
    # create the job
    job = prepare_job(user_name=username, log_level=log_level)
    # read calculation params and create the calculation profile
    params = parse_config(open(cfg_file_path, 'r'))

    if hazard_output_id is None and hazard_calculation_id is None:
        # this is a hazard calculation, not a risk one
        job.hazard_calculation = create_calculation(
            models.HazardCalculation, params)
        job.save()

        # validate and raise an error if there are any problems
        error_message = validate(job, 'hazard', params, exports)
        if error_message:
            raise RuntimeError(error_message)
        return job

    # otherwise run a risk calculation
    params.update(dict(hazard_output_id=hazard_output_id,
                       hazard_calculation_id=hazard_calculation_id))

    calculation = create_calculation(models.RiskCalculation, params)
    job.risk_calculation = calculation
    job.save()

    # validate and raise an error if there are any problems
    error_message = validate(job, 'risk', params,  exports)
    if error_message:
        raise RuntimeError(error_message)
    return job


def list_risk_outputs(rc_id):
    """
    List the outputs for a given
    :class:`~openquake.engine.db.models.RiskCalculation`.

    :param rc_id:
        ID of a risk calculation.
    """
    print_outputs_summary(get_outputs('risk', rc_id))


def get_outputs(job_type, calc_id):
    """
    :param job_type:
        'hazard' or 'risk'
    :param calc_id:
        ID of a calculation.
    :returns:
        A sequence of :class:`openquake.engine.db.models.Output` objects
    """
    if job_type == 'risk':
        return models.Output.objects.filter(oq_job__risk_calculation=calc_id)
    else:
        return models.Output.objects.filter(oq_job__hazard_calculation=calc_id)

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""Functions for getting information about completed jobs and
calculation outputs, as well as exporting outputs from the database to various
file formats."""


import os

from openquake.engine.db import models


#: Used to separate node labels in a logic tree path
LT_PATH_JOIN_TOKEN = '_'


def _export_fn_not_implemented(output, _target_dir):
    """This gets called if an export is attempted on an unsupported output
    type. See :data:`_EXPORT_FN_MAP`."""
    raise NotImplementedError('Cannot export output of type: %s'
                              % output.output_type)


def makedirsdeco(fn):
    """Decorator for export functions. Creates intermediate directories (if
    necessary) to the target export directory.

    This is equivalent to `mkdir -p` and :func:`os.makedirs`.
    """

    def wrapped(output, target, **kwargs):
        """
        If the the ``target`` is a directory path (string), call
        :func:`os.makedirs` to create intermediate directories to
        the ``target``.

        Otherwise, the ``target`` could be a file-like object, in which case we
        don't do anything.
        """
        if isinstance(target, basestring):
            makedirs(target)
        return fn(output, target, **kwargs)

    # This fixes doc generation problems with decorators
    wrapped.__doc__ = fn.__doc__
    wrapped.__repr__ = fn.__repr__

    return wrapped


def makedirs(path):
    """
    Make all of the directories in the ``path`` using `os.makedirs`.
    """
    if os.path.exists(path):
        if not os.path.isdir(path):
            # If it's not a directory, we can't do anything.
            # This is a problem
            raise RuntimeError('%s already exists and is not a directory.'
                               % path)
    else:
        os.makedirs(path)


def get_outputs(job_id, output_type=None):
    """Get all :class:`openquake.engine.db.models.Output` objects associated
    with the specified job and output_type.

    :param int job_id:
        ID of a :class:`openquake.engine.db.models.OqJob`.
    :param str output_type:
        the output type; if None, return all outputs
    :returns:
        :class:`django.db.models.query.QuerySet` of
        :class:`openquake.engine.db.models.Output` objects.
    """
    queryset = models.Output.objects.filter(oq_job=job_id)
    if output_type:
        return queryset.filter(output_type=output_type)
    return queryset

########NEW FILE########
__FILENAME__ = hazard
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Functionality for exporting and serializing hazard curve calculation results.
"""


import os
import csv
import functools

from collections import namedtuple

from openquake.hazardlib.calc import disagg
from openquake.nrmllib.hazard import writers

from openquake.engine.db import models
from openquake.engine.export import core


# for each output_type there must be a function
# export_<output_type>(output, target)
def export(output_id, target, export_type='xml'):
    """
    Export the given hazard calculation output from the database to the
    specified directory.

    :param int output_id:
        ID of a :class:`openquake.engine.db.models.Output`.
    :param target:
        Output directory OR a file-like object to write results to.
    :param export_type:
        The desired export format. Defaults to 'xml'.
    :returns:
        Return the complete file path OR the file-like object itself where the
        results were written to, depending on what ``target`` is.
    """
    output = models.Output.objects.get(id=output_id)
    export_fn_name = 'export_%s_%s' % (output.output_type, export_type)
    try:
        export_fn = globals()[export_fn_name]
    except KeyError:
        raise NotImplementedError(
            'No "%(fmt)s" exporter is available for "%(output_type)s"'
            ' outputs' % dict(fmt=export_type, output_type=output.output_type)
        )

    # If the `target` is a string directory path, use expand user to handle
    # tokens like '~':
    if isinstance(target, (basestring, buffer)):
        target = os.path.expanduser(target)
    return export_fn(output, target)


def _get_result_export_dest(calc_id, target, result, file_ext='xml'):
    """
    Get the full absolute path (including file name) for a given ``result``.

    As a side effect, intermediate directories are created such that the file
    can be created and written to immediately.

    :param int calc_id:
        ID of the associated
        :class:`openquake.engine.db.models.HazardCalculation`.
    :param target:
        Destination directory location for exported files OR a file-like
        object. If file-like, we just simply return it.
    :param result:
        :mod:`openquake.engine.db.models` result object with a foreign key
        reference to :class:`~openquake.engine.db.models.Output`.
    :param file_ext:
        Desired file extension for the output file.
        Defaults to 'xml'.

    :returns:
        Full path (including filename) to the destination export file.
        If the ``target`` is a file-like, we don't do anything special
        and simply return it.
    """
    if not isinstance(target, (basestring, buffer)):
        # It's not a file path. In this case, we expect a file-like object.
        # Just return it.
        return target

    output = result.output
    output_type = output.output_type

    # Create the names for each subdirectory
    calc_dir = 'calc_%s' % calc_id
    type_dir = output_type

    imt_dir = ''  # if blank, we don't have an IMT dir
    if output_type in ('hazard_curve', 'hazard_map', 'disagg_matrix'):
        imt_dir = result.imt
        if result.imt == 'SA':
            imt_dir = 'SA-%s' % result.sa_period

    # construct the directory which will contain the result XML file:
    directory = os.path.join(target, calc_dir, type_dir, imt_dir)
    core.makedirs(directory)

    if output_type in ('hazard_curve', 'hazard_curve_multi', 'hazard_map',
                       'uh_spectra'):
        # include the poe in hazard map and uhs file names
        if output_type in ('hazard_map', 'uh_spectra'):
            output_type = '%s-poe_%s' % (output_type, result.poe)

        if result.statistics is not None:
            # we could have stats
            if result.statistics == 'quantile':
                # quantile
                filename = '%s-%s.%s' % (output_type,
                                         'quantile_%s' % result.quantile,
                                         file_ext)
            else:
                # mean
                filename = '%s-%s.%s' % (output_type, result.statistics,
                                         file_ext)
        else:
            # otherwise, we need to include logic tree branch info
            ltr = result.lt_realization
            sm_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.sm_lt_path)
            gsim_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.gsim_lt_path)
            if ltr.weight is None:
                # Monte-Carlo logic tree sampling
                filename = '%s-smltp_%s-gsimltp_%s-ltr_%s.%s' % (
                    output_type, sm_ltp, gsim_ltp, ltr.ordinal, file_ext
                )
            else:
                # End Branch Enumeration
                filename = '%s-smltp_%s-gsimltp_%s.%s' % (
                    output_type, sm_ltp, gsim_ltp, file_ext
                )
    elif output_type == 'gmf':
        # only logic trees, no stats
        ltr = result.lt_realization
        sm_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.sm_lt_path)
        gsim_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.gsim_lt_path)
        if ltr.weight is None:
            # Monte-Carlo logic tree sampling
            filename = '%s-smltp_%s-gsimltp_%s-ltr_%s.%s' % (
                output_type, sm_ltp, gsim_ltp, ltr.ordinal, file_ext
            )
        else:
            # End Branch Enumeration
            filename = '%s-smltp_%s-gsimltp_%s.%s' % (
                output_type, sm_ltp, gsim_ltp, file_ext
            )
    elif output_type == 'ses':
        sm_ltp = core.LT_PATH_JOIN_TOKEN.join(result.sm_lt_path)
        filename = '%s-smltp_%s.%s' % (
            output_type, sm_ltp, file_ext
        )
    elif output_type == 'disagg_matrix':
        # only logic trees, no stats

        out = '%s(%s)' % (output_type, result.poe)
        location = 'lon_%s-lat_%s' % (result.location.x, result.location.y)

        ltr = result.lt_realization
        sm_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.sm_lt_path)
        gsim_ltp = core.LT_PATH_JOIN_TOKEN.join(ltr.gsim_lt_path)
        if ltr.weight is None:
            # Monte-Carlo logic tree sampling
            filename = '%s-%s-smltp_%s-gsimltp_%s-ltr_%s.%s' % (
                out, location, sm_ltp, gsim_ltp, ltr.ordinal, file_ext
            )
        else:
            # End Branch Enumeration
            filename = '%s-%s-smltp_%s-gsimltp_%s.%s' % (
                out, location, sm_ltp, gsim_ltp, file_ext
            )
    else:
        filename = '%s.%s' % (output_type, file_ext)

    return os.path.abspath(os.path.join(directory, filename))


@core.makedirsdeco
def _export_hazard_curve(output, target, export_type):
    """
    Export the specified hazard curve ``output`` to the ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `hazard_curve`.
    :param target:
        The same ``target`` as :func:`export`.
    :param export_type:
        The export type, 'xml' or 'geojson'
    :returns:
        The same return value as defined by :func:`export`.
    """
    hc = models.HazardCurve.objects.get(output=output.id)

    hcd = _curve_data(hc)
    metadata = _curve_metadata(output, target)
    haz_calc = output.oq_job.hazard_calculation
    dest = _get_result_export_dest(
        haz_calc.id, target, hc, file_ext=export_type)
    writercls = writers.HazardCurveGeoJSONWriter \
        if export_type == 'geojson' else writers.HazardCurveXMLWriter
    writercls(dest, **metadata).serialize(hcd)

    return dest


export_hazard_curve_xml = functools.partial(
    _export_hazard_curve, export_type='xml')


export_hazard_curve_geojson = functools.partial(
    _export_hazard_curve, export_type='geojson')


@core.makedirsdeco
def export_hazard_curve_csv(output, target):
    """
    Save a hazard curve (of a given IMT) as a .csv file in the format
    (lon lat poe1 ... poeN), where the fields are space separated.
    """
    hc = models.HazardCurve.objects.get(output=output.id)
    haz_calc_id = output.oq_job.hazard_calculation.id
    dest = _get_result_export_dest(haz_calc_id, target, hc, file_ext='csv')
    x_y_poes = models.HazardCurveData.objects.all_curves_simple(
        filter_args=dict(hazard_curve=hc.id))
    with open(dest, 'wb') as f:
        writer = csv.writer(f, delimiter=' ')
        for x, y, poes in sorted(x_y_poes):
            writer.writerow([x, y] + poes)

    return dest


@core.makedirsdeco
def export_hazard_curve_multi_xml(output, target):
    hcs = output.hazard_curve

    data = [_curve_data(hc) for hc in hcs]

    metadata_set = []
    for hc in hcs:
        metadata = _curve_metadata(hc.output, target)
        metadata_set.append(metadata)

    haz_calc = output.oq_job.hazard_calculation
    dest = _get_result_export_dest(haz_calc.id, target, hcs)

    writer = writers.MultiHazardCurveXMLWriter(dest, metadata_set)
    writer.serialize(data)

    return dest


def _curve_metadata(output, target):
    hc = models.HazardCurve.objects.get(output=output.id)
    if hc.lt_realization is not None:
        # If the curves are for a specified logic tree realization,
        # get the tree paths
        lt_rlz = hc.lt_realization
        smlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.sm_lt_path)
        gsimlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.gsim_lt_path)
    else:
        # These curves must be statistical aggregates
        smlt_path = None
        gsimlt_path = None

    return {
        'quantile_value': hc.quantile,
        'statistics': hc.statistics,
        'smlt_path': smlt_path,
        'gsimlt_path': gsimlt_path,
        'sa_period': hc.sa_period,
        'sa_damping': hc.sa_damping,
        'investigation_time': hc.investigation_time,
        'imt': hc.imt,
        'imls': hc.imls,
    }


def _curve_data(hc):
    curves = models.HazardCurveData.objects.all_curves_simple(
        filter_args=dict(hazard_curve=hc.id)
    )
    # Simple object wrapper around the values, to match the interface of the
    # XML writer:
    Location = namedtuple('Location', 'x y')
    HazardCurveData = namedtuple('HazardCurveData', 'location poes')
    return (HazardCurveData(Location(x, y), poes) for x, y, poes in curves)


@core.makedirsdeco
def export_gmf_xml(output, target):
    """
    Export the GMF Collection specified by ``output`` to the ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `gmf`.
    :param target:
        The same ``target`` as :func:`export`.

    :returns:
        The same return value as defined by :func:`export`.
    """
    gmf_coll = models.Gmf.objects.get(output=output.id)
    lt_rlz = gmf_coll.lt_realization
    haz_calc = output.oq_job.hazard_calculation

    sm_lt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.sm_lt_path)
    gsim_lt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.gsim_lt_path)

    dest = _get_result_export_dest(haz_calc.id, target, output.gmf)

    writer = writers.EventBasedGMFXMLWriter(
        dest, sm_lt_path, gsim_lt_path)

    writer.serialize(gmf_coll)

    return dest


@core.makedirsdeco
def export_gmf_scenario_xml(output, target):
    """
    Export the GMFs specified by ``output`` to the ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output`
        with an `output_type` of `gmf_scenario`.
    :param target:
        The same ``target`` as :func:`export`.

    :returns:
        The same return value as defined by :func:`export`.
    """
    haz_calc = output.oq_job.hazard_calculation
    dest = _get_result_export_dest(haz_calc.id, target, output.gmf)
    gmfs = models.get_gmfs_scenario(output)
    writer = writers.ScenarioGMFXMLWriter(dest)
    writer.serialize(gmfs)
    return dest


@core.makedirsdeco
def export_ses_xml(output, target):
    """
    Export the Stochastic Event Set Collection specified by ``output`` to the
    ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `ses`.
    :param str target:
        Destination directory location for exported files.

    :returns:
        A list of exported file names (including the absolute path to each
        file).
    """
    ses_coll = models.SESCollection.objects.get(output=output.id)
    haz_calc = output.oq_job.hazard_calculation
    sm_lt_path = core.LT_PATH_JOIN_TOKEN.join(ses_coll.sm_lt_path)

    dest = _get_result_export_dest(haz_calc.id, target,
                                   output.ses)

    writer = writers.SESXMLWriter(dest, sm_lt_path)
    writer.serialize(ses_coll)

    return dest


def _export_hazard_map(output, target, writer_class, file_ext):
    """
    General hazard map export code.
    """
    hazard_map = models.HazardMap.objects.get(output=output)
    haz_calc = output.oq_job.hazard_calculation

    if hazard_map.lt_realization is not None:
        # If the maps are for a specified logic tree realization,
        # get the tree paths
        lt_rlz = hazard_map.lt_realization
        smlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.sm_lt_path)
        gsimlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.gsim_lt_path)
    else:
        # These maps must be constructed from mean or quantile curves
        smlt_path = None
        gsimlt_path = None

    dest = _get_result_export_dest(haz_calc.id, target, output.hazard_map,
                                   file_ext=file_ext)

    metadata = {
        'quantile_value': hazard_map.quantile,
        'statistics': hazard_map.statistics,
        'smlt_path': smlt_path,
        'gsimlt_path': gsimlt_path,
        'sa_period': hazard_map.sa_period,
        'sa_damping': hazard_map.sa_damping,
        'investigation_time': hazard_map.investigation_time,
        'imt': hazard_map.imt,
        'poe': hazard_map.poe,
    }

    writer = writer_class(dest, **metadata)
    writer.serialize(zip(hazard_map.lons, hazard_map.lats, hazard_map.imls))
    return dest


def export_hazard_map_xml(output, target):
    """
    Export the specified hazard map ``output`` to the ``target`` as
    NRML/XML.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `hazard_map`.
    :param target:
        Destination directory location for exported files.

    :returns:
        A list of exported file name (including the absolute path to each
        file).
    """
    return _export_hazard_map(output, target, writers.HazardMapXMLWriter,
                              'xml')


def export_hazard_map_geojson(output, target):
    """
    The same thing as :func:`export_hazard_map_xml`, except results are saved
    in GeoJSON format.
    """
    return _export_hazard_map(output, target,
                              writers.HazardMapGeoJSONWriter, 'geojson')


class _DisaggMatrix(object):
    """
    A simple data model into which disaggregation matrix information can be
    packed. The :class:`openquake.nrmllib.hazard.writers.DisaggXMLWriter`
    expects a sequence of objects which match this interface.

    :param matrix:
        A n-dimensional numpy array representing a probability mass function
        produced by the disaggregation calculator. The calculator produces a 6d
        matrix, but the final results which are saved to XML are "subsets" of
        matrix showing contributions to hazard from different combinations of
        magnitude, distance, longitude, latitude, epsilon, and tectonic region
        type.
    :param dim_labels:
        Expected values are (as tuples, lists, or similar) one of the
        following, depending on the result `matrix` type:

        * ['Mag']
        * ['Dist']
        * ['TRT']
        * ['Mag', 'Dist']
        * ['Mag', 'Dist', 'Eps']
        * ['Lon', 'Lat']
        * ['Mag', 'Lon', 'Lat']
        * ['Lon', 'Lat', 'TRT']
    :param float poe:
        Probability of exceedence (specified in the calculation config file).
    :param float iml:
        Interpolated intensity value, corresponding to the ``poe``, extracted
        from the aggregated hazard curve (which was used as input to compute
        the ``matrix``).
    """

    def __init__(self, matrix, dim_labels, poe, iml):
        self.matrix = matrix
        self.dim_labels = dim_labels
        self.poe = poe
        self.iml = iml


@core.makedirsdeco
def export_disagg_matrix_xml(output, target):
    """
    Export disaggregation histograms to the ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `disagg_matrix`.
    :param str target:
        Destination directory location for exported files.

    :returns:
        A list of exported file name (including the absolute path to each
        file).
    """
    # We expect 1 result per `Output`
    [disagg_result] = models.DisaggResult.objects.filter(output=output)
    lt_rlz = disagg_result.lt_realization
    haz_calc = output.oq_job.hazard_calculation

    dest = _get_result_export_dest(haz_calc.id, target,
                                   output.disagg_matrix)

    writer_kwargs = dict(
        investigation_time=disagg_result.investigation_time,
        imt=disagg_result.imt,
        lon=disagg_result.location.x,
        lat=disagg_result.location.y,
        sa_period=disagg_result.sa_period,
        sa_damping=disagg_result.sa_damping,
        mag_bin_edges=disagg_result.mag_bin_edges,
        dist_bin_edges=disagg_result.dist_bin_edges,
        lon_bin_edges=disagg_result.lon_bin_edges,
        lat_bin_edges=disagg_result.lat_bin_edges,
        eps_bin_edges=disagg_result.eps_bin_edges,
        tectonic_region_types=disagg_result.trts,
        smlt_path=core.LT_PATH_JOIN_TOKEN.join(lt_rlz.sm_lt_path),
        gsimlt_path=core.LT_PATH_JOIN_TOKEN.join(lt_rlz.gsim_lt_path),
    )

    writer = writers.DisaggXMLWriter(dest, **writer_kwargs)

    data = (_DisaggMatrix(disagg_result.matrix[i], dim_labels,
                          disagg_result.poe, disagg_result.iml)
            for i, dim_labels in enumerate(disagg.pmf_map))

    writer.serialize(data)

    return dest


@core.makedirsdeco
def export_uh_spectra_xml(output, target):
    """
    Export the specified UHS ``output`` to the ``target``.

    :param output:
        :class:`openquake.engine.db.models.Output` with an `output_type` of
        `uh_spectra`.
    :param str target:
        Destination directory location for exported files.

    :returns:
        A list containing the exported file name.
    """
    uhs = models.UHS.objects.get(output=output)
    haz_calc = output.oq_job.hazard_calculation

    dest = _get_result_export_dest(haz_calc.id, target, output.uh_spectra)

    if uhs.lt_realization is not None:
        lt_rlz = uhs.lt_realization
        smlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.sm_lt_path)
        gsimlt_path = core.LT_PATH_JOIN_TOKEN.join(lt_rlz.gsim_lt_path)
    else:
        smlt_path = None
        gsimlt_path = None

    metadata = {
        'quantile_value': uhs.quantile,
        'statistics': uhs.statistics,
        'smlt_path': smlt_path,
        'gsimlt_path': gsimlt_path,
        'poe': uhs.poe,
        'periods': uhs.periods,
        'investigation_time': uhs.investigation_time,
    }

    writer = writers.UHSXMLWriter(dest, **metadata)
    writer.serialize(uhs)

    return dest

########NEW FILE########
__FILENAME__ = risk
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Functions for exporting risk artifacts from the database.
"""


import os
import csv

from openquake.engine.db import models
from openquake.engine.export import core
from openquake.engine.utils import FileWrapper
from openquake.nrmllib.risk import writers


LOSS_CURVE_FILENAME_FMT = 'loss-curves-%(loss_curve_id)s.xml'
LOSS_MAP_FILENAME_FMT = 'loss-maps-%(loss_map_id)s.%(file_ext)s'
LOSS_FRACTION_FILENAME_FMT = 'loss-fractions-%(loss_fraction_id)s.xml'
AGGREGATE_LOSS_FILENAME_FMT = 'aggregate-loss-%s.csv'
BCR_FILENAME_FMT = 'bcr-distribution-%(bcr_distribution_id)s.xml'
EVENT_LOSS_FILENAME_FMT = 'event-loss-%s.csv'


# for each output_type there must be a function
# export_<output_type>_<export_type>(output, target)
def export(output_id, target, export_type='xml'):
    """
    Export the given risk calculation output from the database to the
    specified directory. See :func:`openquake.engine.export.hazard.export` for
    more details.
    """
    output = models.Output.objects.get(id=output_id)

    try:
        export_fn = EXPORTERS[export_type][output.output_type]
    except KeyError:
        raise NotImplementedError(
            'No "%(fmt)s" exporter is available for "%(output_type)s"'
            ' outputs' % dict(fmt=export_type, output_type=output.output_type)
        )

    if isinstance(target, basestring):
        target = os.path.expanduser(target)
    return export_fn(output, target)


def _get_result_export_dest(target, output, file_ext='xml'):
    """
    Get the full absolute path (including file name) for a given ``result``.

    As a side effect, intermediate directories are created such that the file
    can be created and written to immediately.

    :param target:
        Destination directory location for exported files OR a file-like
        object. If file-like, we just simply return it.
    :param output:
        :class:`~openquake.engine.db.models.Output`.
    :param file_ext:
        Desired file extension for the output file.
        Defaults to 'xml'.

    :returns:
        Full path (including filename) to the destination export file.
        If the ``target`` is a file-like, we don't do anything special
        and simply return it.
    """
    if not isinstance(target, basestring):
        # It's not a file path. In this case, we expect a file-like object.
        # Just return it.
        return target

    output_type = output.output_type

    filename = None

    if output_type in ('loss_curve', 'agg_loss_curve', 'event_loss_curve'):
        filename = LOSS_CURVE_FILENAME_FMT % dict(
            loss_curve_id=output.loss_curve.id,
        )
    elif output_type == 'loss_map':
        filename = LOSS_MAP_FILENAME_FMT % dict(
            loss_map_id=output.loss_map.id,
            file_ext=file_ext,
        )
    elif output_type == 'loss_fraction':
        filename = LOSS_FRACTION_FILENAME_FMT % dict(
            loss_fraction_id=output.loss_fraction.id,
        )
    elif output_type == 'bcr_distribution':
        filename = BCR_FILENAME_FMT % dict(
            bcr_distribution_id=output.bcr_distribution.id,
        )
    elif output_type == 'event_loss':
        filename = EVENT_LOSS_FILENAME_FMT % (output.id)
    elif output_type == 'aggregate_loss':
        filename = AGGREGATE_LOSS_FILENAME_FMT % (output.aggregate_loss.id)
    elif output_type in ('dmg_dist_per_asset', 'dmg_dist_per_taxonomy',
                         'dmg_dist_total', 'collapse_map'):
        filename = '%(output_type)s-%(job_id)s.%(file_ext)s' % dict(
            output_type=output_type,
            job_id=output.oq_job.id,
            file_ext=file_ext,
        )

    return os.path.abspath(os.path.join(target, filename))


def _export_common(output, loss_type):
    """
    Returns a dict containing the output metadata which are serialized
    by nrml writers before actually writing the `output` data.
    """
    metadata = output.hazard_metadata
    if metadata.sm_path is not None:
        source_model_tree_path = core.LT_PATH_JOIN_TOKEN.join(
            metadata.sm_path)
    else:
        source_model_tree_path = None
    if metadata.gsim_path is not None:
        gsim_tree_path = core.LT_PATH_JOIN_TOKEN.join(metadata.gsim_path)
    else:
        gsim_tree_path = None

    return dict(investigation_time=metadata.investigation_time,
                statistics=metadata.statistics,
                quantile_value=metadata.quantile,
                source_model_tree_path=source_model_tree_path,
                gsim_tree_path=gsim_tree_path,
                unit=output.oq_job.risk_calculation.exposure_model.unit(
                    loss_type),
                loss_type=loss_type)


@core.makedirsdeco
def export_agg_loss_curve_xml(output, target):
    """
    Export `output` to `target` by using a nrml loss curves
    serializer
    """
    args = _export_common(output, output.loss_curve.loss_type)
    dest = _get_result_export_dest(target, output)
    writers.AggregateLossCurveXMLWriter(dest, **args).serialize(
        output.loss_curve.aggregatelosscurvedata)
    return dest


@core.makedirsdeco
def export_loss_curve_xml(output, target):
    """
    Export `output` to `target` by using a nrml loss curves
    serializer
    """
    args = _export_common(output, output.loss_curve.loss_type)
    dest = _get_result_export_dest(target, output)
    args['insured'] = output.loss_curve.insured

    data = output.loss_curve.losscurvedata_set.all().order_by('asset_ref')

    writers.LossCurveXMLWriter(dest, **args).serialize(data)
    return dest


def _export_loss_map(output, target, writer_class, file_ext):
    """
    General loss map export code.
    """
    risk_calculation = output.oq_job.risk_calculation
    args = _export_common(output, output.loss_map.loss_type)

    dest = _get_result_export_dest(target, output, file_ext=file_ext)

    args.update(dict(
        poe=output.loss_map.poe,
        loss_category=risk_calculation.exposure_model.category))
    writer = writer_class(dest, **args)
    writer.serialize(
        models.order_by_location(
            output.loss_map.lossmapdata_set.all().order_by('asset_ref')))
    return dest


@core.makedirsdeco
def export_loss_map_xml(output, target):
    """
    Serialize a loss map to NRML/XML.
    """
    return _export_loss_map(output, target, writers.LossMapXMLWriter,
                            'xml')


@core.makedirsdeco
def export_loss_map_geojson(output, target):
    """
    Serialize a loss map to geojson.
    """
    return _export_loss_map(output, target, writers.LossMapGeoJSONWriter,
                            'geojson')


@core.makedirsdeco
def export_loss_fraction_xml(output, target):
    """
    Export `output` to `target` by using a nrml loss fractions
    serializer
    """
    risk_calculation = output.oq_job.risk_calculation
    args = _export_common(output, output.loss_fraction.loss_type)
    hazard_metadata = models.Output.HazardMetadata(
        investigation_time=args['investigation_time'],
        statistics=args['statistics'],
        quantile=args.get('quantile_value'),
        sm_path=args['source_model_tree_path'],
        gsim_path=args['gsim_tree_path'])
    dest = _get_result_export_dest(target, output)
    poe = output.loss_fraction.poe
    variable = output.loss_fraction.variable
    loss_category = risk_calculation.exposure_model.category
    writers.LossFractionsWriter(
        dest, variable, args['unit'], args['loss_type'],
        loss_category, hazard_metadata, poe).serialize(
        output.loss_fraction.total_fractions(),
        output.loss_fraction)
    return dest


@core.makedirsdeco
def export_bcr_distribution_xml(output, target):
    """
    Export `output` to `target` by using a nrml bcr distribution
    serializer
    """
    risk_calculation = output.oq_job.risk_calculation
    args = _export_common(output, output.bcr_distribution.loss_type)

    dest = _get_result_export_dest(target, output)
    args.update(
        dict(interest_rate=risk_calculation.interest_rate,
             asset_life_expectancy=risk_calculation.asset_life_expectancy))
    del args['investigation_time']

    writers.BCRMapXMLWriter(dest, **args).serialize(
        output.bcr_distribution.bcrdistributiondata_set.all().order_by(
            'asset_ref'))
    return dest


def make_dmg_dist_export(damagecls, writercls):
    # XXX: clearly this is not a good approach for large exposures
    @core.makedirsdeco
    def export_dmg_dist(output, target):
        """
        Export the damage distribution identified
        by the given output to the `target`.

        :param output:
            DB output record which identifies the distribution. A
            :class:`openquake.engine.db.models.Output` object.
        :param target:
            Destination directory of the exported file, or a file-like object
        """
        job = output.oq_job
        rc_id = job.risk_calculation.id

        dest = _get_result_export_dest(target, output)

        dmg_states = list(models.DmgState.objects.filter(
            risk_calculation__id=rc_id).order_by('lsi'))
        if writercls is writers.CollapseMapXMLWriter:  # special case
            writer = writercls(dest)
            data = damagecls.objects.filter(dmg_state=dmg_states[-1])
        else:
            writer = writercls(dest, [ds.dmg_state for ds in dmg_states])
            data = damagecls.objects.filter(
                dmg_state__risk_calculation__id=rc_id)
        writer.serialize(data.order_by('dmg_state__lsi'))
        return dest

    return export_dmg_dist


export_dmg_dist_per_asset_xml = make_dmg_dist_export(
    models.DmgDistPerAsset, writers.DmgDistPerAssetXMLWriter
)


export_dmg_dist_per_taxonomy_xml = make_dmg_dist_export(
    models.DmgDistPerTaxonomy, writers.DmgDistPerTaxonomyXMLWriter
)


export_dmg_dist_total_xml = make_dmg_dist_export(
    models.DmgDistTotal, writers.DmgDistTotalXMLWriter
)


export_collapse_map_xml = make_dmg_dist_export(
    models.DmgDistPerAsset, writers.CollapseMapXMLWriter
)


def export_aggregate_loss_csv(output, target):
    """
    Export aggregate losses in CSV
    """
    dest = _get_result_export_dest(target, output)

    with FileWrapper(dest, mode='wb') as csvfile:
        writer = csv.writer(csvfile, delimiter='|')
        writer.writerow(['Mean', 'Standard Deviation'])
        writer.writerow([output.aggregate_loss.mean,
                        output.aggregate_loss.std_dev])
    return dest

export_aggregate_loss = export_aggregate_loss_csv


def export_event_loss_csv(output, target):
    """
    Export Event Loss Table in CSV format
    """

    dest = _get_result_export_dest(target, output)

    with FileWrapper(dest, mode='wb') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Rupture', 'Magnitude', 'Aggregate Loss'])

        for event_loss in models.EventLossData.objects.filter(
                event_loss__output=output).select_related().order_by(
                '-aggregate_loss'):
            writer.writerow([event_loss.rupture.tag,
                             "%.07f" % event_loss.rupture.rupture.magnitude,
                             "%.07f" % event_loss.aggregate_loss])
    return dest


XML_EXPORTERS = {
    'agg_loss_curve': export_agg_loss_curve_xml,
    'bcr_distribution': export_bcr_distribution_xml,
    'collapse_map': export_collapse_map_xml,
    'dmg_dist_per_asset': export_dmg_dist_per_asset_xml,
    'dmg_dist_per_taxonomy': export_dmg_dist_per_taxonomy_xml,
    'dmg_dist_total': export_dmg_dist_total_xml,
    'loss_curve': export_loss_curve_xml,
    'event_loss_curve': export_loss_curve_xml,
    'loss_fraction': export_loss_fraction_xml,
    'loss_map': export_loss_map_xml,
    # TODO(LB):
    # There are two exceptions; aggregate_loss and event_loss can only be
    # exported in CSV format.
    # We should re-think the way we're handling the export cases.
    'aggregate_loss': export_aggregate_loss_csv,
    'event_loss': export_event_loss_csv,
}
GEOJSON_EXPORTERS = {
    'loss_map': export_loss_map_geojson,
}
EXPORTERS = {
    'xml': XML_EXPORTERS,
    'geojson': GEOJSON_EXPORTERS,
}

########NEW FILE########
__FILENAME__ = exposure
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Serializer and related functions to save exposure data to the database.
"""

from openquake.engine.db import models
from django.db import router
from django.db import transaction


class ExposureDBWriter(object):
    """
    Serialize the exposure model to database

    :attr job:
        an instance of :class:`openquake.engine.db.models.OqJob`
    """

    def __init__(self, job):
        """Create a new serializer"""
        self.job = job
        self.model = None
        self.cost_types = {}

    @transaction.commit_on_success(router.db_for_write(models.ExposureModel))
    def serialize(self, iterator):
        """
        Serialize a list of values produced by iterating over an instance of
        :class:`openquake.nrmllib.risk.parsers.ExposureParser`
        """
        for asset_data in iterator:
            if not self.model:
                self.model, self.cost_types = (
                    self.insert_model(asset_data.exposure_metadata))
            self.insert_datum(asset_data)
        return self.model

    def insert_model(self, model):
        """
        :returns:
            a 2-tuple holding a newly created instance of
            :class:`openquake.engine.db.models.ExposureModel` and
            a dictionary of (newly created)
            :class:`openquake.engine.db.models.ExposureModel` instances
            keyed by the cost type name

        :param model:
            an instance of
            :class:`openquake.nrmllib.risk.parsers.ExposureMetadata`
        """
        exposure_model = models.ExposureModel.objects.create(
            job=self.job,
            name=model.exposure_id,
            description=model.description,
            taxonomy_source=model.taxonomy_source,
            category=model.asset_category,
            area_type=model.conversions.area_type,
            area_unit=model.conversions.area_unit,
            deductible_absolute=model.conversions.deductible_is_absolute,
            insurance_limit_absolute=(
                model.conversions.insurance_limit_is_absolute))

        cost_types = {}
        for cost_type in model.conversions.cost_types:
            cost_types[cost_type.name] = models.CostType.objects.create(
                exposure_model=exposure_model,
                name=cost_type.name,
                conversion=cost_type.conversion_type,
                unit=cost_type.unit,
                retrofitted_conversion=cost_type.retrofitted_type,
                retrofitted_unit=cost_type.retrofitted_unit)

        return exposure_model, cost_types

    def insert_datum(self, asset_data):
        """
        Insert a single asset entry.

        :param asset_data:
            an instance of :class:`openquake.nrmllib.risk.parsers.AssetData`
        """
        asset = models.ExposureData.objects.create(
            exposure_model=self.model,
            asset_ref=asset_data.asset_ref,
            taxonomy=asset_data.taxonomy,
            area=asset_data.area,
            number_of_units=asset_data.number,
            site="POINT(%s %s)" % (asset_data.site.longitude,
                                   asset_data.site.latitude))

        for cost_type in self.cost_types:
            if not any([cost_type == cost.cost_type
                        for cost in asset_data.costs]):
                raise ValueError("Invalid Exposure. "
                                 "Missing cost %s for asset %s" % (
                                     cost_type, asset.asset_ref))

        model = asset_data.exposure_metadata
        deductible_is_absolute = model.conversions.deductible_is_absolute
        insurance_limit_is_absolute = (
            model.conversions.insurance_limit_is_absolute)

        for cost in asset_data.costs:
            cost_type = self.cost_types.get(cost.cost_type, None)

            if cost_type is None:
                raise ValueError("Invalid Exposure. Missing conversion "
                                 "for cost type %s" % cost.cost_type)

            if cost.retrofitted is not None:
                retrofitted = models.ExposureData.per_asset_value(
                    cost.retrofitted, cost_type.retrofitted_conversion,
                    asset_data.area,
                    model.conversions.area_type,
                    asset_data.number,
                    model.asset_category)
            else:
                retrofitted = None

            converted_cost = models.ExposureData.per_asset_value(
                cost.value, cost_type.conversion,
                asset_data.area,
                model.conversions.area_type,
                asset_data.number,
                model.asset_category)

            models.Cost.objects.create(
                exposure_data=asset,
                cost_type=cost_type,
                converted_cost=converted_cost,
                converted_retrofitted_cost=retrofitted,
                deductible_absolute=models.make_absolute(
                    cost.deductible,
                    converted_cost,
                    deductible_is_absolute),
                insurance_limit_absolute=models.make_absolute(
                    cost.limit,
                    converted_cost,
                    insurance_limit_is_absolute))

        for odata in asset_data.occupancy:
            models.Occupancy.objects.create(exposure_data=asset,
                                            occupants=odata.occupants,
                                            period=odata.period)

########NEW FILE########
__FILENAME__ = validation
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
This module contains functions and Django model forms for carrying out job
profile validation.
"""
import re
import warnings

from django.forms import ModelForm

import openquake.hazardlib
from openquake.engine.db import models
from openquake.engine.utils import get_calculator_class


AVAILABLE_GSIMS = openquake.hazardlib.gsim.get_available_gsims().keys()


# used in bin/openquake
def validate(job, job_type, params, exports):
    """
    Validate a job of type 'hazard' or 'risk' by instantiating its
    form class with the given files and exports.

    :param job:
        an instance of :class:`openquake.engine.db.models.OqJob`
    :param str job_type:
        "hazard" or "risk"
    :param dict params:
        The raw dictionary of parameters parsed from the config file.
    :param exports:
        a list of export types
    :returns:
        an error message if the form is invalid, None otherwise.
    """
    calculation = getattr(job, '%s_calculation' % job_type)
    calc_mode = calculation.calculation_mode
    calculator_cls = get_calculator_class(job_type, calc_mode)
    formname = calculator_cls.__name__.replace('Calculator', 'Form')
    try:
        form_class = globals()[formname]
    except KeyError:
        return 'Could not find form class for "%s"' % calc_mode

    files = set(params['inputs'])
    form = form_class(instance=calculation, files=files, exports=exports)

    # Check for superfluous params and raise warnings:
    params_copy = params.copy()
    # There are a couple of parameters we can ignore.
    # `calculation_mode` is supplied in every config file, but is validated in
    # a special way; therefore, we don't declare it on the forms.
    # The `base_path` is extracted from the directory containing the config
    # file; it's not a real param.
    # `hazard_output_id` and `hazard_calculation_id` are supplied via command
    # line args.
    for p in ('calculation_mode', 'base_path', 'hazard_output_id',
              'hazard_calculation_id'):
        if p in params_copy:
            params_copy.pop(p)

    for param in set(params_copy.keys()).difference(set(form._meta.fields)):
        msg = "Unknown parameter '%s' for calculation mode '%s'. Ignoring."
        msg %= (param, calc_mode)
        warnings.warn(msg, RuntimeWarning)

    if not form.is_valid():
        return 'Job configuration is not valid. Errors: %s' % dict(form.errors)


class BaseOQModelForm(ModelForm):
    """
    This class is based on :class:`django.forms.ModelForm`. Constructor
    arguments are the same.

    Since we're using forms (at the moment) purely for model validation, it's
    worth noting how we're using forms and what sort of inputs should be
    supplied.

    At the very least, an `instance` should be specified, which is expected to
    be a Django model object (perhaps one from
    :mod:`openquake.engine.db.models`).

    `data` can be specified to populate the form and model. If no `data` is
    specified, the form will take the current data from the `instance`.

    You can also specify `files`. In the Django web form context, this
    represents a `dict` of name-file_object pairs. The file object type can be,
    for example, one of the types in :mod:`django.core.files.uploadedfile`.

    In this case, however, we expect `files` to be a dict of
    filenames, keyed by config file parameter
    for the input. For example::

    {'site_model': 'site_model.xml'}
    """

    # These fields require more complex validation.
    # The rules for these fields depend on other parameters
    # and files.
    # At the moment, these are common to all hazard calculation modes.
    special_fields = (
        'export_dir',
        'inputs',
    )

    def __init__(self, *args, **kwargs):
        self.exports = kwargs.get('exports')
        if not 'data' in kwargs:
            # Because we're not using ModelForms in exactly the
            # originally-intended modus operandi, we need to pass all of the
            # field values from the instance model object as the `data` kwarg
            # (`data` needs to be a dict of fieldname-value pairs).
            # This serves to populate the form (as if a user had done so) and
            # immediately enables validation checking (through `is_valid()`,
            # for example).
            # This is, of course, only applicable if `instance` was supplied to
            # the form. For the purpose of just doing validation (which is why
            # these forms were created), we need to specify the `instance`.
            instance = kwargs.get('instance')
            if instance is not None:
                kwargs['data'] = instance.__dict__
        if "exports" in kwargs:
            del kwargs['exports']
        super(BaseOQModelForm, self).__init__(*args, **kwargs)

    def has_vulnerability(self):
        """
        :returns: True if a vulnerability file has been given
        """
        return [itype
                for itype, _desc in models.INPUT_TYPE_CHOICES
                if itype.endswith('vulnerability') and itype in self.files]

    def _add_error(self, field_name, error_msg):
        """
        Add an error to the `errors` dict.

        If errors for the given ``field_name`` already exist append the error
        to that list. Otherwise, a new entry will have to be created for the
        ``field_name`` to hold the ``error_msg``.

        ``error_msg`` can also be a list or tuple of error messages.
        """
        is_list = isinstance(error_msg, (list, tuple))
        if self.errors.get(field_name) is not None:
            if is_list:
                self.errors[field_name].extend(error_msg)
            else:
                self.errors[field_name].append(error_msg)
        else:
            # no errors for this field have been recorded yet
            if is_list:
                if len(error_msg) > 0:
                    self.errors[field_name] = error_msg
            else:
                self.errors[field_name] = [error_msg]

    def is_valid(self):
        """
        Overrides :meth:`django.forms.ModelForm.is_valid` to perform
        custom validation checks (in addition to superclass validation).

        :returns:
            If valid return `True`, else `False`.
        """

        # FIXME(lp). Django allows custom validation by overriding the
        # `clean` method and `clean_<field>` methods. We should go for
        # the standard approach

        super_valid = super(BaseOQModelForm, self).is_valid()
        all_valid = super_valid

        # Calculation
        calc = self.instance

        # First, check the calculation mode:
        valid, errs = calculation_mode_is_valid(calc, self.calc_mode)
        all_valid &= valid
        self._add_error('calculation_mode', errs)

        # Exclude special fields that require contextual validation.
        fields = self.__class__.Meta.fields

        for field in sorted(set(fields) - set(self.special_fields)):
            valid, errs = globals()['%s_is_valid' % field](calc)
            all_valid &= valid

            self._add_error(field, errs)

        if self.exports:
            # The user has requested that exports be performed after the
            # calculation i.e. an 'export_dir' parameter must be present.
            if not calc.export_dir:
                all_valid = False
                err = ('--exports specified on the command line but the '
                       '"export_dir" parameter is missing in the .ini file')
                self._add_error('export_dir', err)

        return all_valid


class BaseHazardModelForm(BaseOQModelForm):
    """
    Base ModelForm used to validate HazardCalculation objects
    """

    special_fields = (
        'region',
        'region_grid_spacing',
        'sites',
        'reference_vs30_value',
        'reference_vs30_type',
        'reference_depth_to_2pt5km_per_sec',
        'reference_depth_to_1pt0km_per_sec',
        'export_dir',
        'inputs',
    )

    def is_valid(self):
        super_valid = super(BaseHazardModelForm, self).is_valid()
        all_valid = super_valid

        # HazardCalculation
        hc = self.instance
        # Now do checks which require more context.

        # Cannot specify region AND sites
        if (hc.region is not None and hc.sites is not None):
            all_valid = False
            err = 'Cannot specify `region` and `sites`. Choose one.'
            self._add_error('region', err)
        # At least one must be specified (region OR sites)
        elif not (hc.region is not None or
                  hc.sites is not None or 'exposure' in self.files):
            all_valid = False
            err = 'Must specify either `region`, `sites` or `exposure_file`.'
            self._add_error('region', err)
            self._add_error('sites', err)
        # Only region is specified
        elif hc.region is not None:
            if hc.region_grid_spacing is not None:
                valid, errs = region_grid_spacing_is_valid(hc)
                all_valid &= valid

                self._add_error('region_grid_spacing', errs)
            else:
                all_valid = False
                err = '`region` requires `region_grid_spacing`'
                self._add_error('region', err)

            # validate the region
            valid, errs = region_is_valid(hc)
            all_valid &= valid
            self._add_error('region', errs)
        # Only sites was specified
        elif hc.sites:
            valid, errs = sites_is_valid(hc)
            all_valid &= valid
            self._add_error('sites', errs)

        if 'site_model' not in self.files:
            # make sure the reference parameters are defined and valid

            for field in (
                'reference_vs30_value',
                'reference_vs30_type',
                'reference_depth_to_2pt5km_per_sec',
                'reference_depth_to_1pt0km_per_sec',
            ):
                valid, errs = globals().get('%s_is_valid' % field)(hc)
                all_valid &= valid
                self._add_error(field, errs)

        return all_valid


class ClassicalHazardForm(BaseHazardModelForm):

    calc_mode = 'classical'

    class Meta:
        model = models.HazardCalculation
        fields = (
            'description',
            'region',
            'region_grid_spacing',
            'sites',
            'random_seed',
            'intensity_measure_types_and_levels',
            'number_of_logic_tree_samples',
            'rupture_mesh_spacing',
            'width_of_mfd_bin',
            'area_source_discretization',
            'reference_vs30_value',
            'reference_vs30_type',
            'reference_depth_to_2pt5km_per_sec',
            'reference_depth_to_1pt0km_per_sec',
            'investigation_time',
            'truncation_level',
            'maximum_distance',
            'mean_hazard_curves',
            'quantile_hazard_curves',
            'poes',
            'export_dir',
            'inputs',
            'hazard_maps',
            'uniform_hazard_spectra',
            'export_multi_curves',
        )

    def is_valid(self):
        super_valid = super(ClassicalHazardForm, self).is_valid()
        all_valid = super_valid

        if self.has_vulnerability():
            if self.instance.intensity_measure_types_and_levels is not None:
                msg = (
                    '`intensity_measure_types_and_levels` is ignored when a '
                    '`vulnerability_file` is specified'
                )
                warnings.warn(msg)

        return all_valid


class EventBasedHazardForm(BaseHazardModelForm):

    calc_mode = 'event_based'

    class Meta:
        model = models.HazardCalculation
        fields = (
            'description',
            'region',
            'region_grid_spacing',
            'sites',
            'random_seed',
            'number_of_logic_tree_samples',
            'rupture_mesh_spacing',
            'intensity_measure_types',
            'intensity_measure_types_and_levels',
            'width_of_mfd_bin',
            'area_source_discretization',
            'reference_vs30_value',
            'reference_vs30_type',
            'reference_depth_to_2pt5km_per_sec',
            'reference_depth_to_1pt0km_per_sec',
            'investigation_time',
            'truncation_level',
            'maximum_distance',
            'ses_per_logic_tree_path',
            'ground_motion_correlation_model',
            'ground_motion_correlation_params',
            'ground_motion_fields',
            'hazard_curves_from_gmfs',
            'mean_hazard_curves',
            'quantile_hazard_curves',
            'poes',
            'export_dir',
            'inputs',
            'hazard_maps',
            'export_multi_curves',
        )

    def is_valid(self):
        super_valid = super(EventBasedHazardForm, self).is_valid()
        all_valid = super_valid

        hc = self.instance

        # contextual validation
        # If a vulnerability model is defined, show warnings if the user also
        # specified `intensity_measure_types_and_levels` or
        # `intensity_measure_types`:
        if self.has_vulnerability():
            if (self.instance.intensity_measure_types_and_levels
                    is not None):
                msg = (
                    '`intensity_measure_types_and_levels` is ignored when '
                    'a `vulnerability_file` is specified'
                )
                warnings.warn(msg)
            if (self.instance.intensity_measure_types is not None):
                msg = (
                    '`intensity_measure_types` is ignored when '
                    'a `vulnerability_file` is specified'
                )
                warnings.warn(msg)
        else:
            if hc.hazard_curves_from_gmfs:
                # The vulnerability model can define the IMTs/IMLs;
                # if there isn't one, we need to check that
                # `intensity_measure_types_and_levels` and
                # `intensity_measure_types` are both defined and valid.
                if hc.intensity_measure_types_and_levels is None:
                    # Not defined
                    msg = '`%s` requires `%s`'
                    msg %= ('hazard_curves_from_gmfs',
                            'intensity_measure_types_and_levels')

                    self._add_error('intensity_measure_types_and_levels', msg)
                    all_valid = False
                else:
                    # IMTs/IMLs is defined
                    # The IMT keys in `intensity_measure_types_and_levels` need
                    # to be a subset of `intensity_measure_types`.
                    imts = set(hc.intensity_measure_types_and_levels.keys())

                    all_imts = set(hc.intensity_measure_types)

                    if not imts.issubset(all_imts):
                        msg = 'Unknown IMT(s) [%s] in `%s`'
                        msg %= (', '.join(sorted(imts - all_imts)),
                                'intensity_measure_types')

                        self._add_error('intensity_measure_types_and_levels',
                                        msg)
                        all_valid = False

                if not hc.ground_motion_fields:
                    msg = ('`hazard_curves_from_gmfs` requires '
                           '`ground_motion_fields` to be `true`')
                    self._add_error('hazard_curves_from_gmfs', msg)
                    all_valid = False

        return all_valid


class DisaggHazardForm(BaseHazardModelForm):

    calc_mode = 'disaggregation'

    class Meta:
        model = models.HazardCalculation
        fields = (
            'description',
            'region',
            'region_grid_spacing',
            'sites',
            'random_seed',
            'intensity_measure_types_and_levels',
            'number_of_logic_tree_samples',
            'rupture_mesh_spacing',
            'width_of_mfd_bin',
            'area_source_discretization',
            'reference_vs30_value',
            'reference_vs30_type',
            'reference_depth_to_2pt5km_per_sec',
            'reference_depth_to_1pt0km_per_sec',
            'investigation_time',
            'truncation_level',
            'maximum_distance',
            'mag_bin_width',
            'distance_bin_width',
            'coordinate_bin_width',
            'num_epsilon_bins',
            'poes_disagg',
            'export_dir',
            'inputs',
        )

    def is_valid(self):
        super_valid = super(DisaggHazardForm, self).is_valid()
        all_valid = super_valid

        if self.has_vulnerability():
            if self.instance.intensity_measure_types_and_levels is not None:
                msg = (
                    '`intensity_measure_types_and_levels` is ignored when a '
                    '`vulnerability_file` is specified'
                )
                warnings.warn(msg)

        return all_valid


class ScenarioHazardForm(BaseHazardModelForm):

    calc_mode = 'scenario'

    class Meta:
        model = models.HazardCalculation
        fields = (
            'description',
            'region',
            'region_grid_spacing',
            'sites',
            'random_seed',
            'intensity_measure_types',
            'rupture_mesh_spacing',
            'reference_vs30_value',
            'reference_vs30_type',
            'reference_depth_to_2pt5km_per_sec',
            'reference_depth_to_1pt0km_per_sec',
            'truncation_level',
            'maximum_distance',
            'number_of_ground_motion_fields',
            'gsim',
            'ground_motion_correlation_model',
            'ground_motion_correlation_params',
            'export_dir',
            'inputs',
        )

    def is_valid(self):
        super_valid = super(ScenarioHazardForm, self).is_valid()
        all_valid = super_valid

        if self.has_vulnerability():
            if self.instance.intensity_measure_types is not None:
                msg = (
                    '`intensity_measure_types` is ignored when a '
                    '`vulnerability_file` is specified'
                )
                warnings.warn(msg)

        return all_valid


class ClassicalRiskForm(BaseOQModelForm):
    calc_mode = 'classical'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'lrem_steps_per_interval',
            'conditional_loss_poes',
            'quantile_loss_curves',
            'insured_losses',
            'poes_disagg',
            'export_dir',
            'inputs',
        )


class ClassicalBCRRiskForm(BaseOQModelForm):
    calc_mode = 'classical_bcr'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'lrem_steps_per_interval',
            'interest_rate',
            'asset_life_expectancy',
            'export_dir',
            'inputs',
        )


class EventBasedBCRRiskForm(BaseOQModelForm):
    calc_mode = 'event_based_bcr'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'loss_curve_resolution',
            'master_seed',
            'asset_correlation',
            'interest_rate',
            'asset_life_expectancy',
            'export_dir',
            'inputs',
        )


class EventBasedRiskForm(BaseOQModelForm):
    calc_mode = 'event_based'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'risk_investigation_time',
            'loss_curve_resolution',
            'conditional_loss_poes',
            'insured_losses',
            'master_seed',
            'asset_correlation',
            'quantile_loss_curves',
            'sites_disagg',
            'mag_bin_width',
            'distance_bin_width',
            'coordinate_bin_width',
            'export_dir',
            'inputs',
        )

    def is_valid(self):
        super_valid = super(EventBasedRiskForm, self).is_valid()
        rc = self.instance          # RiskCalculation instance

        if rc.sites_disagg and not (rc.mag_bin_width
                                    and rc.coordinate_bin_width
                                    and rc.distance_bin_width):
            self._add_error('sites_disagg', "disaggregation requires "
                            "mag_bin_width, coordinate_bin_width, "
                            "distance_bin_width")
            return False

        return super_valid


class ScenarioDamageRiskForm(BaseOQModelForm):
    calc_mode = 'scenario_damage'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'export_dir',
            'inputs',
        )


class ScenarioRiskForm(BaseOQModelForm):
    calc_mode = 'scenario'

    class Meta:
        model = models.RiskCalculation
        fields = (
            'description',
            'region_constraint',
            'maximum_distance',
            'master_seed',
            'asset_correlation',
            'insured_losses',
            'time_event',
            'export_dir',
            'inputs',
        )

    def is_valid(self):
        super_valid = super(ScenarioRiskForm, self).is_valid()
        rc = self.instance          # RiskCalculation instance

        if 'occupants_vulnerability' in self.files:
            if not rc.time_event:
                self._add_error('time_event', "Scenario Risk requires "
                                "time_event when an occupants vulnerability "
                                "model is given")

                return False
        return super_valid

# Silencing 'Missing docstring' and 'Invalid name' for all of the validation
# functions (the latter because some of the function names are very long).
# pylint: disable=C0111,C0103


def description_is_valid(_mdl):
    return True, []


def calculation_mode_is_valid(mdl, expected_calc_mode):
    if not mdl.calculation_mode == expected_calc_mode:
        return False, ['Calculation mode must be "%s"' % expected_calc_mode]
    return True, []


def region_is_valid(mdl):
    valid = True
    errors = []

    if not mdl.region.valid:
        valid = False
        errors.append('Invalid region geomerty: %s' % mdl.region.valid_reason)

    if len(mdl.region.coords) > 1:
        valid = False
        errors.append('Region geometry can only be a single linear ring')

    # There should only be a single linear ring.
    # Even if there are multiple, we can still check for and report errors.
    for ring in mdl.region.coords:
        lons = [lon for lon, _ in ring]
        lats = [lat for _, lat in ring]

        errors.extend(_lons_lats_are_valid(lons, lats))

    if errors:
        valid = False

    return valid, errors


def region_grid_spacing_is_valid(mdl):
    if not mdl.region_grid_spacing > 0:
        return False, ['Region grid spacing must be > 0']
    return True, []


def sites_is_valid(mdl):
    valid = True
    errors = []

    lons = [pt.x for pt in mdl.sites]
    lats = [pt.y for pt in mdl.sites]

    errors.extend(_lons_lats_are_valid(lons, lats))
    if errors:
        valid = False

    return valid, errors


def sites_disagg_is_valid(mdl):
    # sites_disagg is optional in risk event based
    if mdl.calculation_mode == 'event_based' and mdl.sites_disagg is None:
        return True, []
    valid = True
    errors = []

    lons = [pt.x for pt in mdl.sites_disagg]
    lats = [pt.y for pt in mdl.sites_disagg]

    errors.extend(_lons_lats_are_valid(lons, lats))
    if errors:
        valid = False

    return valid, errors


def _lons_lats_are_valid(lons, lats):
    """
    Helper function for validating lons/lats.

    :returns:
        A list of error messages, or an empty list.
    """
    errors = []

    if not all([-180 <= x <= 180 for x in lons]):
        errors.append('Longitude values must in the range [-180, 180]')
    if not all([-90 <= x <= 90 for x in lats]):
        errors.append('Latitude values must be in the range [-90, 90]')

    return errors


def random_seed_is_valid(mdl):
    if not models.MIN_SINT_32 <= mdl.random_seed <= models.MAX_SINT_32:
        return False, [('Random seed must be a value from %s to %s (inclusive)'
                       % (models.MIN_SINT_32, models.MAX_SINT_32))]
    return True, []


def number_of_logic_tree_samples_is_valid(mdl):
    if not mdl.number_of_logic_tree_samples >= 0:
        return False, ['Number of logic tree samples must be >= 0']
    return True, []


def rupture_mesh_spacing_is_valid(mdl):
    if not mdl.rupture_mesh_spacing > 0:
        return False, ['Rupture mesh spacing must be > 0']
    return True, []


def width_of_mfd_bin_is_valid(mdl):
    if not mdl.width_of_mfd_bin > 0:
        return False, ['Width of MFD bin must be > 0']
    return True, []


def area_source_discretization_is_valid(mdl):
    if not mdl.area_source_discretization > 0:
        return False, ['Area source discretization must be > 0']
    return True, []


def reference_vs30_value_is_valid(mdl):
    if not mdl.reference_vs30_value > 0:
        return False, ['Reference VS30 value must be > 0']
    return True, []


def reference_vs30_type_is_valid(mdl):
    if not mdl.reference_vs30_type in ('measured', 'inferred'):
        return False, ['Reference VS30 type must be either '
                       '"measured" or "inferred"']
    return True, []


def reference_depth_to_2pt5km_per_sec_is_valid(mdl):
    if not mdl.reference_depth_to_2pt5km_per_sec > 0:
        return False, ['Reference depth to 2.5 km/sec must be > 0']
    return True, []


def reference_depth_to_1pt0km_per_sec_is_valid(mdl):
    if not mdl.reference_depth_to_1pt0km_per_sec > 0:
        return False, ['Reference depth to 1.0 km/sec must be > 0']
    return True, []


def investigation_time_is_valid(mdl):
    if not mdl.investigation_time > 0:
        return False, ['Investigation time must be > 0']
    return True, []


def _validate_imt(imt):
    """
    Validate an intensity measure type string.

    :returns:
        A pair of values. The first is a `bool` indicating whether or not the
        IMT is valid. The second value is a `list` of error messages. (If the
        IMT is valid, the list should be empty.)
    """
    valid = True
    errors = []

    # SA intensity measure configs need special handling
    valid_imts = list(set(openquake.hazardlib.imt.__all__) - set(['SA']))

    if 'SA' in imt:
        match = re.match(r'^SA\(([^)]+?)\)$', imt)
        if match is None:
            # SA key is not formatted properly
            valid = False
            errors.append(
                '%s: SA must be specified with a period value, in the form'
                ' `SA(N)`, where N is a value >= 0' % imt
            )
        else:
            # there's a match; make sure the period value is valid
            sa_period = match.groups()[0]
            try:
                if float(sa_period) < 0:
                    valid = False
                    errors.append(
                        '%s: SA period values must be >= 0' % imt
                    )
            except ValueError:
                valid = False
                errors.append(
                    '%s: SA period value should be a float >= 0' % imt
                )
    elif not imt in valid_imts:
        valid = False
        errors.append('%s: Invalid intensity measure type' % imt)

    return valid, errors


def intensity_measure_types_and_levels_is_valid(mdl):
    im = mdl.intensity_measure_types_and_levels

    valid = True
    errors = []

    if im is None:
        return valid, errors

    for im_type, imls in im.iteritems():
        # validate IMT:
        valid_imt, imt_errors = _validate_imt(im_type)
        valid &= valid_imt
        errors.extend(imt_errors)

        # validate IML values:
        if not isinstance(imls, list):
            valid = False
            errors.append(
                '%s: IMLs must be specified as a list of floats' % im_type
            )
        else:
            if len(imls) == 0:
                valid = False
                errors.append(
                    '%s: IML lists must have at least 1 value' % im_type
                )
            elif not all([x > 0 for x in imls]):
                valid = False
                errors.append('%s: IMLs must be > 0' % im_type)

    return valid, errors


def intensity_measure_types_is_valid(mdl):
    imts = mdl.intensity_measure_types

    if imts is None:
        return True, []

    if isinstance(imts, str):
        imts = [imts]

    valid = True
    errors = []

    for imt in imts:
        valid_imt, imt_errors = _validate_imt(imt)
        valid &= valid_imt
        errors.extend(imt_errors)

    return valid, errors


# FIXME
# This function and similar ones where different
# checking rules are applied according to
# different calculation modes need to be refactored,
# splitting up the checking rules for each calculation
# mode.
def truncation_level_is_valid(mdl):
    if mdl.calculation_mode == 'disaggregation':
        if mdl.truncation_level is not None:
            if mdl.truncation_level <= 0:
                return False, [
                    'Truncation level must be > 0 for disaggregation'
                    ' calculations']
        else:
            return False, [
                'Truncation level must be set for disaggregation'
                ' calculations and it must be > 0']
    else:
        if mdl.truncation_level is not None:
            if mdl.truncation_level < 0:
                return False, ['Truncation level must be >= 0']

    return True, []


def maximum_distance_is_valid(mdl):
    if not mdl.maximum_distance > 0:
        return False, ['Maximum distance must be > 0']
    return True, []


def mean_hazard_curves_is_valid(_mdl):
    # The validation form should normalize the type to a boolean.
    # We don't need to check anything here.
    return True, []


def quantile_hazard_curves_is_valid(mdl):
    qhc = mdl.quantile_hazard_curves

    if qhc is not None:
        if not all([0.0 <= x <= 1.0 for x in qhc]):
            return False, ['Quantile hazard curve values must in the range '
                           '[0, 1]']
    return True, []


def quantile_loss_curves_is_valid(mdl):
    qlc = mdl.quantile_loss_curves

    if qlc is not None:
        if not all([0.0 <= x <= 1.0 for x in qlc]):
            return False, ['Quantile loss curve values must in the range '
                           '[0, 1]']
    return True, []


def poes_is_valid(mdl):
    phm = mdl.poes
    error_msg = '`poes` values must be in the range [0, 1]'
    return _validate_poe_list(phm, error_msg)


def _validate_poe_list(poes, error_msg):
    if poes is not None:
        if not all([0.0 <= x <= 1.0 for x in poes]):
            return False, [error_msg]
    return True, []


def ses_per_logic_tree_path_is_valid(mdl):
    sps = mdl.ses_per_logic_tree_path

    if not sps > 0:
        return False, ['`Stochastic Event Sets Per Sample` '
                       '(ses_per_logic_tree_path) must be > 0']
    return True, []


def ground_motion_correlation_model_is_valid(_mdl):
    # No additional validation is required;
    # the model form and fields will take care of validation based on the
    # valid choices defined for this field.
    return True, []


def ground_motion_correlation_params_is_valid(_mdl):
    # No additional validation is required;
    # it is not appropriate to do detailed checks on the correlation model
    # parameters at this point because the parameters are specific to a given
    # correlation model.
    # Field normalization should make sure that the input is properly formed.
    return True, []


def ground_motion_fields_is_valid(_mdl):
    # This parameter is a simple True or False;
    # field normalization should cover all of validation necessary.
    return True, []


def hazard_curves_from_gmfs_is_valid(_mdl):
    # This parameter is a simple True or False;
    # field normalization should cover all of validation necessary.
    return True, []


def conditional_loss_poes_is_valid(mdl):
    value = mdl.conditional_loss_poes

    if value is not None:
        if not all([0.0 <= x <= 1.0 for x in value]):
            return (
                False,
                ['PoEs for conditional loss poes must be in the range [0, 1]'])
    return True, []


def lrem_steps_per_interval_is_valid(mdl):
    value = mdl.lrem_steps_per_interval
    msg = 'loss conditional exceedence matrix steps per interval must be > 0'

    if value is None or not value > 0:
        return False, [msg]
    return True, []


def region_constraint_is_valid(_mdl):
    # At this stage, we just use the region_is_valid implementation to
    # check for a consistent geometry. Further validation occurs after
    # we have loaded the exposure.
    _mdl.region = _mdl.region_constraint
    return region_is_valid(_mdl)


def mag_bin_width_is_valid(mdl):
    # mag_bin_width is optional in risk event based
    if mdl.calculation_mode == 'event_based' and mdl.sites_disagg is None:
        return True, []

    if not mdl.mag_bin_width > 0.0:
        return False, ['Magnitude bin width must be > 0.0']
    return True, []


def distance_bin_width_is_valid(mdl):
    # distance_bin_width is optional in risk event based
    if mdl.calculation_mode == 'event_based' and mdl.sites_disagg is None:
        return True, []

    if not mdl.distance_bin_width > 0.0:
        return False, ['Distance bin width must be > 0.0']
    return True, []


def coordinate_bin_width_is_valid(mdl):
    # coordinate_bin_width is optional in risk event based
    if mdl.calculation_mode == 'event_based' and mdl.sites_disagg is None:
        return True, []

    if not mdl.coordinate_bin_width > 0.0:
        return False, ['Coordinate bin width must be > 0.0']
    return True, []


def num_epsilon_bins_is_valid(mdl):
    if not mdl.num_epsilon_bins > 0:
        return False, ['Number of epsilon bins must be > 0']
    return True, []


def asset_life_expectancy_is_valid(mdl):
    if mdl.is_bcr:
        if mdl.asset_life_expectancy is None or mdl.asset_life_expectancy <= 0:
            return False, ['Asset Life Expectancy must be > 0']
    return True, []


def taxonomies_from_model_is_valid(_mdl):
    return True, []


def interest_rate_is_valid(mdl):
    if mdl.is_bcr:
        if mdl.interest_rate is None:
            return False, "Interest Rate is mandatory for BCR analysis"
    return True, []


def insured_losses_is_valid(_mdl):
    # The validation form should normalize the type to a boolean.
    # We don't need to check anything here.
    return True, []


def loss_curve_resolution_is_valid(mdl):
    if mdl.calculation_mode == 'event_based':
        if (mdl.loss_curve_resolution is not None and
                mdl.loss_curve_resolution < 1):
            return False, ['Loss Curve Resolution must be >= 1']
    return True, []


def asset_correlation_is_valid(_mdl):
    # The validation form should check if it is in the list
    # We don't need to check anything here.
    if _mdl.asset_correlation is not None:
        if not (_mdl.asset_correlation >= 0 and _mdl.asset_correlation <= 1):
            return False, ['Asset Correlation must be >= 0 and <= 1']
    return True, []


def master_seed_is_valid(_mdl):
    return True, []


def export_multi_curves_is_valid(_mdl):
    return True, []


def gsim_is_valid(mdl):
    if mdl.gsim in AVAILABLE_GSIMS:
        return True, []
    return False, ['The gsim %r is not in in openquake.hazardlib.gsim' %
                   mdl.gsim]


def number_of_ground_motion_fields_is_valid(mdl):
    gmfno = mdl.number_of_ground_motion_fields
    if gmfno > 0:
        return True, []
    return False, ['The number_of_ground_motion_fields must be a positive '
                   'integer, got %r' % gmfno]


def poes_disagg_is_valid(mdl):
    # poes_disagg optional in classical risk
    if mdl.calculation_mode == 'classical':
        return True, []
    poesd = mdl.poes_disagg
    if len(poesd) == 0:
        return False, ['`poes_disagg` must contain at least 1 value']
    error_msg = 'PoEs for disaggregation must be in the range [0, 1]'
    return _validate_poe_list(poesd, error_msg)


def hazard_maps_is_valid(mdl):
    if mdl.hazard_maps and mdl.poes is None:
        return False, ['`poes` are required to compute hazard maps']
    return True, []


def uniform_hazard_spectra_is_valid(mdl):
    if mdl.uniform_hazard_spectra and mdl.poes is None:
        return False, ['`poes` are required to compute UHS']
    return True, []


def time_event_is_valid(_mdl):
    # Any string is allowed, or `None`.
    return True, []


def risk_investigation_time_is_valid(_mdl):
    if _mdl.calculation_mode != 'event_based':
        return False, ['`risk_investigation_time` is only used '
                       'in event based calculations']
    if _mdl.risk_investigation_time is not None:
        if _mdl.risk_investigation_time <= 0:
            return False, ['Risk investigation time must be > 0']
    return True, []

########NEW FILE########
__FILENAME__ = logs
# -*- coding: utf-8 -*-

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Set up some system-wide loggers
"""
import logging


# Place the new level between info and warning
logging.PROGRESS = 25
logging.addLevelName(logging.PROGRESS, "PROGRESS")

LEVELS = {'debug': logging.DEBUG,
          'info': logging.INFO,
          'warn': logging.WARNING,
          'progress': logging.PROGRESS,
          'error': logging.ERROR,
          'critical': logging.CRITICAL}

LOG = logging.getLogger()


def _log_progress(msg, *args, **kwargs):
    """
    Log the message using the progress reporting logging level.

    ``args`` and ``kwargs`` are the same as :meth:`logging.Logger.debug`,
    except that this method has an additional possible keyword: ``indent``.

    Normally, progress messages are logged with a '** ' prefix. If ``indent``
    is `True`, messages will be logged with a '**  >' prefix.

    If ``indent`` is not specified, it will default to `False`.
    """
    indent = kwargs.get('indent')

    if indent is None:
        indent = False
    else:
        # 'indent' is an invalid kwarg for the logger's _log method
        # we need to remove it before we call _log:
        del kwargs['indent']

    if indent:
        prefix = '**  >'
    else:
        prefix = '** '

    msg = '%s %s' % (prefix, msg)
    LOG._log(logging.PROGRESS, msg, args, **kwargs)
LOG.progress = _log_progress


def set_level(level):
    """
    Initialize logs to write records with level `level` or above.
    """
    logging.root.setLevel(LEVELS.get(level, logging.WARNING))


class tracing(object):
    """
    Simple context manager util to handle tracing. E.g.

    with log("exports"):
       do_export()
    """
    def __init__(self, msg):
        self.msg = msg

    def __enter__(self):
        LOG.debug('starting ' + self.msg)

    def __exit__(self, *args):
        LOG.debug('ending ' + self.msg)

########NEW FILE########
__FILENAME__ = performance
import os
import time
import atexit
from datetime import datetime
import psutil

import numpy
from django.db import connections

from openquake.engine import logs
from openquake.engine.db import models
from openquake.engine.writer import CacheInserter


# this is not thread-safe
class PerformanceMonitor(object):
    """
    Measure the resident memory occupied by a list of processes during
    the execution of a block of code. Should be used as a context manager,
    as follows::

     with PerformanceMonitor([os.getpid()]) as mm:
         do_something()
     deltamemory, = mm.mem

    At the end of the block the PerformanceMonitor object will have the
    following 5 public attributes:

    .start_time: when the monitor started (a datetime object)
    .duration: time elapsed between start and stop (in seconds)
    .exc: None unless an exception happened inside the block of code
    .mem: an array with the memory deltas (in bytes)

    The memory array has the same length as the number of processes.
    The behaviour of the PerformanceMonitor can be customized by subclassing it
    and by overriding the method on_exit(), called at end and used to display
    or store the results of the analysis.
    """
    def __init__(self, pids=None):
        pids = pids or [os.getpid()]
        self._procs = [psutil.Process(pid) for pid in pids if pid]
        self._start_time = None  # seconds from the epoch
        self.start_time = None  # datetime object
        self.duration = None  # seconds
        self.mem = None  # bytes
        self.exc = None  # exception

    def measure_mem(self):
        "An array of memory measurements (in bytes), one per process"
        mem = []
        for proc in list(self._procs):
            try:
                rss = proc.get_memory_info().rss
            except psutil.AccessDenied:
                # no access to information about this process
                # don't not try to check it anymore
                self._procs.remove(proc)
            else:
                mem.append(rss)
        return numpy.array(mem)

    def __enter__(self):
        "Call .start"
        self.exc = None
        self._start_time = time.time()
        self.start_time = datetime.fromtimestamp(self._start_time)
        self.start_mem = self.measure_mem()
        return self

    def __exit__(self, etype, exc, tb):
        "Call .stop"
        self.exc = exc
        self.stop_mem = self.measure_mem()
        self.mem = self.stop_mem - self.start_mem
        self.duration = time.time() - self._start_time
        self.on_exit()

    def on_exit(self):
        "Save the results: to be overridden in subclasses"
        print 'start_time =', self.start_time
        print 'duration =', self.duration
        print 'mem =', self.mem
        if self.exc:
            print 'exc = %s(%s)' % (self.exc.__class__.__name__, self.exc)


class EnginePerformanceMonitor(PerformanceMonitor):
    """
    Performance monitor specialized for the engine. It takes in input a
    string, a job_id, and a celery task; the on_exit method
    send the relevant info to the uiapi.performance table.
    For efficiency reasons the saving on the database is delayed and
    done in chunks of 1,000 rows each. That means that hundreds of
    concurrents task can log simultaneously on the uiapi.performance table
    without problems. You can save more often by calling the .cache.flush()
    method; it is automatically called for you by the oqtask decorator;
    it is also called at the end of the main engine process.
    """

    # globals per process
    cache = CacheInserter(models.Performance, 1000)  # store at most 1k objects
    pgpid = None
    pypid = None

    @classmethod
    def store_task_id(cls, job_id, task):
        with cls('storing task id', job_id, task, flush=True):
            pass

    @classmethod
    def monitor(cls, method):
        """
        A decorator to add monitoring to calculator methods. The only
        constraints are:
        1) the method has no keyword arguments
        2) there is an attribute self.job.id
        """
        def newmeth(self, *args):
            with cls(method.__name__, self.job.id, flush=True):
                return method(self, *args)
        newmeth.__name__ = method.__name__
        return newmeth

    def __init__(self, operation, job_id, task=None, tracing=False,
                 profile_pymem=True, profile_pgmem=False, flush=False):
        self.operation = operation
        self.job_id = job_id
        if task:
            self.task = task
            self.task_id = task.request.id
        else:
            self.task = None
            self.task_id = None
        self.tracing = tracing
        self.profile_pymem = profile_pymem
        self.profile_pgmem = profile_pgmem
        self.flush = flush
        if self.profile_pymem and self.pypid is None:
            self.__class__.pypid = os.getpid()
        if self.profile_pgmem and self.pgpid is None:
            # this may be slow
            pgpid = connections['job_init'].cursor().\
                connection.get_backend_pid()
            try:
                psutil.Process(pgpid)
            except psutil.error.NoSuchProcess:  # db on a different machine
                pass
            else:
                self.__class__.pgpid = pgpid
        if tracing:
            self.tracer = logs.tracing(operation)

        super(EnginePerformanceMonitor, self).__init__(
            [self.pypid, self.pgpid])

    def copy(self, operation):
        """
        Return a copy of the monitor usable for a different operation
        in the same task.
        """
        return self.__class__(operation, self.job_id, self.task, self.tracing,
                              self.profile_pymem, self.profile_pgmem)

    def on_exit(self):
        """
        Save the memory consumption on the uiapi.performance table.
        """
        n_measures = len(self.mem)
        if n_measures == 2:
            pymemory, pgmemory = self.mem
        elif n_measures == 1:
            pymemory, = self.mem
            pgmemory = None
        elif n_measures == 0:  # profile_pymem was False
            pymemory = pgmemory = None
        else:
            raise ValueError(
                'Got %d memory measurements, must be <= 2' % n_measures)
        if self.exc is None:  # save only valid calculations
            perf = models.Performance(
                oq_job_id=self.job_id,
                task_id=self.task_id,
                task=getattr(self.task, '__name__', None),
                operation=self.operation,
                start_time=self.start_time,
                duration=self.duration,
                pymemory=pymemory,
                pgmemory=pgmemory)
            self.cache.add(perf)
            if self.flush:
                self.cache.flush()

    def __enter__(self):
        super(EnginePerformanceMonitor, self).__enter__()
        if self.tracing:
            self.tracer.__enter__()
        return self

    def __exit__(self, etype, exc, tb):
        super(EnginePerformanceMonitor, self).__exit__(etype, exc, tb)
        if self.tracing:
            self.tracer.__exit__(etype, exc, tb)

## makes sure the performance results are flushed in the db at the end
atexit.register(EnginePerformanceMonitor.cache.flush)


class DummyMonitor(PerformanceMonitor):
    """
    This class makes it easy to disable the monitoring
    in client code. Disabling the monitor can improve the performance.
    """
    def __init__(self, operation='', job_id=0, *args, **kw):
        self.operation = operation
        self.job_id = job_id
        self._procs = []

    def copy(self, operation):
        return self.__class__(operation, self.job_id)

    def __enter__(self):
        return self

    def __exit__(self, etype, exc, tb):
        pass


class LightMonitor(object):
    """
    in situations where a `PerformanceMonitor` is overkill or affects
    the performance (as in short loops), this helper can aid in
    measuring roughly the performance of a small piece of code. Please
    note that it does not prevent the common traps in measuring the
    performance as stated in the "Algorithms" chapter in the Python
    Cookbook.
    """
    def __init__(self, operation, job_id, task=None):
        self.operation = operation
        self.job_id = job_id
        if task is not None:
            self.task = task
            self.task_id = task.request.id
        else:
            self.task = None
            self.task_id = None
        self.t0 = time.time()
        self.start_time = datetime.fromtimestamp(self.t0)
        self.duration = 0

    def __enter__(self):
        self.t0 = time.time()
        return self

    def __exit__(self, etype, exc, tb):
        self.duration += time.time() - self.t0

    def copy(self, operation):
        return self.__class__(operation, self.job_id, self.task)

    def flush(self):
        models.Performance.objects.create(
            oq_job_id=self.job_id,
            task_id=self.task_id,
            task=getattr(self.task, '__name__', None),
            operation=self.operation,
            start_time=self.start_time,
            duration=self.duration)
        self.__init__(self.operation, self.job_id, self.task)

########NEW FILE########
__FILENAME__ = settings
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""Django settings for OpenQuake."""

from openquake.engine.utils import config


# DEBUG = True
DB_SECTION = config.get_section('database')


def _db_cfg(db_name):
    """
    Helper method to create db config items for the various roles and schemas.

    :param db_name: The name of the database configuration. Configurations for
        this name will be loaded from the site specific config file. If an item
        doesn't exist in the config file, a default value will be used instead.

    :returns: Configuration dict, structured like so::
        {'ENGINE': 'django.db.backends.postgresql_psycopg2',
         'NAME': 'openquake',
         'USER': 'openquake',
         'PASSWORD': 'secret',
         'HOST': 'localhost',
         'PORT': '5432',
        }


    """

    return dict(
        ENGINE='django.contrib.gis.db.backends.postgis',
        NAME=DB_SECTION.get('name', 'openquake'),
        USER=DB_SECTION.get('%s_user' % db_name, 'openquake'),
        PASSWORD=DB_SECTION.get('%s_password' % db_name, ''),
        HOST=DB_SECTION.get('host', ''),
        PORT=DB_SECTION.get('port', '5432'),
    )


_DB_NAMES = (
    'admin',
    'job_init',
)

DATABASES = dict((db, _db_cfg(db)) for db in _DB_NAMES)

DEFAULT_USER = 'admin'
# We need a 'default' database to make Django happy:
DATABASES['default'] = {
    'ENGINE': 'django.db.backends.postgresql_psycopg2',
    'NAME': DB_SECTION.get('name', 'openquake'),
    'USER': DB_SECTION.get('%s_user' % DEFAULT_USER, 'oq_admin'),
    'PASSWORD': DB_SECTION.get('%s_password' % DEFAULT_USER, 'openquake'),
    'HOST': '',
    'PORT': '5432',
}

DATABASE_ROUTERS = ['openquake.engine.db.routers.OQRouter']

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# On Unix systems, a value of None will cause Django to use the same
# timezone as the operating system.
TIME_ZONE = 'Europe/Zurich'

LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# Make this unique, and don't share it with anybody.
SECRET_KEY = 'change-me-in-production'

USE_I18N = False
USE_L10N = False

try:
    from local_settings import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = bulk_insert_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest

from openquake.engine import writer

from openquake.engine.db.models import GmfData
from openquake.engine.writer import CacheInserter


class DummyConnection(object):
    @property
    def connection(self):
        return self

    @property
    def description(self):
        # a mock to the test insertion in the GmfData table
        return [['id'], ['gmf_id'], ['task_no'], ['imt'], ['sa_period'],
                ['sa_damping'], ['gmvs'], ['rupture_ids'], ['site_id']]

    def cursor(self):
        return self

    def execute(self, sql, values=()):
        self.sql = sql
        self.values = values

    def copy_from(self, stringio, table, columns):
        self.data = stringio.getvalue()
        self.table = table
        self.columns = columns


class CacheInserterTestCase(unittest.TestCase):
    """
    Unit tests for the CacheInserter class.
    """
    def setUp(self):
        self.connections = writer.connections
        writer.connections = dict(
            admin=DummyConnection(), job_init=DummyConnection())

    def tearDown(self):
        writer.connections = self.connections

    # this test is probably too strict and testing implementation details
    def test_insert_gmf(self):
        cache = CacheInserter(GmfData, 10)
        gmf1 = GmfData(
            gmf_id=1, imt='PGA', gmvs=[], rupture_ids=[],
            site_id=1)
        gmf2 = GmfData(
            gmf_id=1, imt='PGA', gmvs=[], rupture_ids=[],
            site_id=2)
        cache.add(gmf1)
        cache.add(gmf2)
        cache.flush()
        connection = writer.connections['job_init']
        self.assertEqual(
            connection.data,
            '1\t\\N\tPGA\t\\N\t\\N\t{}\t{}\t1\n1\t\\N\tPGA\t\\N\t\\N\t{}\t{}\t2\n')
        self.assertEqual(connection.table, '"hzrdr"."gmf_data"')
        self.assertEqual(
            connection.columns,
            ['gmf_id', 'task_no', 'imt', 'sa_period', 'sa_damping',
             'gmvs', 'rupture_ids', 'site_id'])

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import getpass
import unittest

import numpy

from nose.plugins.attrib import attr

from openquake.engine.calculators.hazard.classical import core
from openquake.engine.db import models
from openquake.engine.tests.utils import helpers
from openquake.commonlib.general import WeightedSequence


class ClassicalHazardCalculatorTestCase(unittest.TestCase):
    """
    Tests for the main methods of the classical hazard calculator.
    """

    def setUp(self):
        self.job, self.calc = self._setup_a_new_calculator()
        models.JobStats.objects.create(oq_job=self.job)

    def _setup_a_new_calculator(self):
        cfg = helpers.get_data_path('simple_fault_demo_hazard/job.ini')
        job = helpers.get_job(cfg, username=getpass.getuser())
        calc = core.ClassicalHazardCalculator(job)
        return job, calc

    def test_initialize_sources(self):
        self.calc.initialize_site_model()
        self.calc.initialize_sources()
        # after splitting/grouping the source model contains 21 blocks
        blocks = self.calc.source_blocks_per_ltpath[
            ('b1',), 'Active Shallow Crust']
        self.assertEqual(21, len(blocks))

    @attr('slow')
    def test_initialize_site_model(self):
        # we need a slightly different config file for this test
        cfg = helpers.get_data_path(
            'simple_fault_demo_hazard/job_with_site_model.ini')
        self.job = helpers.get_job(cfg)
        self.calc = core.ClassicalHazardCalculator(self.job)

        self.calc.initialize_site_model()
        # If the site model isn't valid for the calculation geometry, a
        # `RuntimeError` should be raised here

        # Okay, it's all good. Now check the count of the site model records.
        sm_nodes = models.SiteModel.objects.filter(job=self.job)

        self.assertEqual(2601, len(sm_nodes))

        num_pts_to_compute = len(
            self.job.hazard_calculation.points_to_compute())

        hazard_site = models.HazardSite.objects.filter(
            hazard_calculation=self.job.hazard_calculation)

        # The site model is good. Now test that `hazard_site` was computed.
        # For now, just test the length.
        self.assertEqual(num_pts_to_compute, len(hazard_site))

    def test_initialize_site_model_no_site_model(self):
        patch_path = 'openquake.engine.calculators.hazard.general.\
store_site_model'
        with helpers.patch(patch_path) as store_sm_patch:
            self.calc.initialize_site_model()
            # We should never try to store a site model in this case.
            self.assertEqual(0, store_sm_patch.call_count)

    def _check_logic_tree_realization_source_blocks_per_ltpath(self, ltr):
        # the logic tree for this sample calculation only contains a single
        # source model
        path = tuple(ltr.sm_lt_path)
        sources = WeightedSequence.merge(
            self.calc.source_blocks_per_ltpath[path, 'Active Shallow Crust'])
        self.assertEqual(22, len(sources))

    def test_initialize_realizations_montecarlo(self):
        # We need initalize sources first (read logic trees, parse sources,
        # etc.)
        self.calc.initialize_site_model()
        self.calc.initialize_sources()

        # No realizations yet:
        ltrs = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=self.job.hazard_calculation.id)
        self.assertEqual(0, len(ltrs))

        self.calc.initialize_realizations()

        # We expect 2 logic tree realizations
        ltr1, ltr2 = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=self.job.hazard_calculation.id
        ).order_by("id")

        # Check each ltr contents, just to be thorough.
        self.assertEqual(0, ltr1.ordinal)
        self.assertEqual(['b1'], ltr1.sm_lt_path)
        self.assertEqual(['b1'], ltr1.gsim_lt_path)

        self.assertEqual(1, ltr2.ordinal)
        self.assertEqual(['b1'], ltr2.sm_lt_path)
        self.assertEqual(['b1'], ltr2.gsim_lt_path)

        for ltr in (ltr1, ltr2):
            self._check_logic_tree_realization_source_blocks_per_ltpath(ltr)

    def test_initialize_realizations_enumeration(self):
        self.calc.initialize_site_model()
        # enumeration is triggered by zero value used as number of realizations
        self.calc.job.hazard_calculation.number_of_logic_tree_samples = 0
        self.calc.initialize_sources()
        self.calc.initialize_realizations()

        [ltr] = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=self.job.hazard_calculation.id)

        # Check each ltr contents, just to be thorough.
        self.assertEqual(0, ltr.ordinal)
        self.assertEqual(['b1'], ltr.sm_lt_path)
        self.assertEqual(['b1'], ltr.gsim_lt_path)

        self._check_logic_tree_realization_source_blocks_per_ltpath(ltr)

    @attr('slow')
    def test_complete_calculation_workflow(self):
        # Test the calculation workflow, from pre_execute through clean_up
        hc = self.job.hazard_calculation

        self.calc.pre_execute()

        # Test the job stats:
        job_stats = models.JobStats.objects.get(oq_job=self.job.id)
        # num sources * num lt samples / block size (items per task):
        self.assertEqual(120, job_stats.num_sites)

        # Update job status to move on to the execution phase.
        self.job.is_running = True

        self.job.status = 'executing'
        self.job.save()
        self.calc.execute()

        self.job.status = 'post_executing'
        self.job.save()
        self.calc.post_execute()

        lt_rlzs = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=self.job.hazard_calculation.id)

        self.assertEqual(2, len(lt_rlzs))

        # Now we test that the htemp results were copied to the final location
        # in `hzrdr.hazard_curve` and `hzrdr.hazard_curve_data`.
        for rlz in lt_rlzs:

            # get hazard curves for this realization
            [pga_curves] = models.HazardCurve.objects.filter(
                lt_realization=rlz.id, imt='PGA')
            [sa_curves] = models.HazardCurve.objects.filter(
                lt_realization=rlz.id, imt='SA', sa_period=0.025)

            # check that the multi-hazard-curve outputs have been
            # created for this realization

            self.assertEqual(
                1,
                models.HazardCurve.objects.filter(
                    lt_realization=rlz.id, imt=None, statistics=None).count())

            # In this calculation, we have 120 sites of interest.
            # We should have exactly that many curves per realization
            # per IMT.
            pga_curve_data = models.HazardCurveData.objects.filter(
                hazard_curve=pga_curves.id)
            self.assertEqual(120, len(pga_curve_data))
            sa_curve_data = models.HazardCurveData.objects.filter(
                hazard_curve=sa_curves.id)
            self.assertEqual(120, len(sa_curve_data))

        # test post processing
        self.job.status = 'post_processing'
        self.job.save()
        self.calc.post_process()

        # Test for the correct number of mean/quantile curves
        self.assertEqual(
            1,
            models.HazardCurve.objects.filter(
                output__oq_job=self.job,
                lt_realization__isnull=True, statistics="mean",
                imt="PGA").count())
        self.assertEqual(
            1,
            models.HazardCurve.objects.filter(
                output__oq_job=self.job,
                lt_realization__isnull=True, statistics="mean",
                imt="SA", sa_period=0.025).count())
        self.assertEqual(
            1,
            models.HazardCurve.objects.filter(
                output__oq_job=self.job,
                lt_realization__isnull=True, statistics="mean",
                imt=None).count())

        for quantile in hc.quantile_hazard_curves:
            self.assertEqual(
                1,
                models.HazardCurve.objects.filter(
                    lt_realization__isnull=True, statistics="quantile",
                    output__oq_job=self.job,
                    quantile=quantile,
                    imt="PGA").count())
            self.assertEqual(
                1,
                models.HazardCurve.objects.filter(
                    lt_realization__isnull=True, statistics="quantile",
                    output__oq_job=self.job,
                    quantile=quantile,
                    imt="SA", sa_period=0.025).count())
            self.assertEqual(
                1,
                models.HazardCurve.objects.filter(
                    lt_realization__isnull=True, statistics="quantile",
                    output__oq_job=self.job,
                    quantile=quantile,
                    imt=None).count())

        # Test for the correct number of maps.
        # The expected count is:
        # (num_poes * num_imts * num_rlzs)
        # +
        # (num_poes * num_imts * (1 mean + num_quantiles))
        # Thus:
        # (2 * 2 * 2) + (2 * 2 * (1 + 2)) = 20
        hazard_maps = models.HazardMap.objects.filter(output__oq_job=self.job)
        self.assertEqual(20, hazard_maps.count())

        # test for the correct number of UH Spectra:
        # The expected count is:
        # (num_hazard_maps_PGA_or_SA / num_poes)
        # (20 / 2) = 10
        uhs = models.UHS.objects.filter(output__oq_job=self.job)
        self.assertEqual(10, uhs.count())
        # Now test the number of curves in each UH Spectra
        # It should be equal to the number of sites (120)
        for u in uhs:
            self.assertEqual(120, u.uhsdata_set.count())

        self.job.status = 'clean_up'
        self.job.save()
        self.calc.clean_up()
        self.assertEqual(0, len(self.calc.source_blocks_per_ltpath))


def update_result_matrix(current, new):
    return 1 - (1 - current) * (1 - new)


class HelpersTestCase(unittest.TestCase):
    """
    Tests for helper functions in the classical hazard calculator core module.
    """

    def test_update_result_matrix_with_scalars(self):
        init = 0.0
        result = update_result_matrix(init, 0.2)
        # The first time we apply this formula on a 0.0 value,
        # result is equal to the first new value we apply.
        self.assertAlmostEqual(0.2, result)

        result = update_result_matrix(result, 0.3)
        self.assertAlmostEqual(0.44, result)

    def test_update_result_matrix_numpy_arrays(self):
        init = numpy.zeros((4, 4))
        first = numpy.array([0.2] * 16).reshape((4, 4))

        result = update_result_matrix(init, first)
        numpy.testing.assert_allclose(first, result)

        second = numpy.array([0.3] * 16).reshape((4, 4))
        result = update_result_matrix(result, second)

        expected = numpy.array([0.44] * 16).reshape((4, 4))
        numpy.testing.assert_allclose(expected, result)

########NEW FILE########
__FILENAME__ = post_processing_test
# -*- coding: utf-8 -*-
# unittest.TestCase base class does not honor the following coding
# convention
# pylint: disable=C0103,R0904
# pylint: enable=W0511,W0142,I0011,E1101,E0611,F0401,E1103,R0801,W0232

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Test classical calculator post processing features
"""

import decimal
import mock
import numpy
import unittest

from nose.plugins.attrib import attr
from scipy.stats import mstats

from openquake.engine.tests.utils import helpers

from openquake.engine.db import models
from openquake.engine.calculators import post_processing
from openquake.engine.calculators.hazard.classical import (
    post_processing as post_proc)
from openquake.engine.calculators.hazard.classical.post_processing import (
    hazard_curves_to_hazard_map
)

aaae = numpy.testing.assert_array_almost_equal


# package prefix used for mock.patching
MOCK_PREFIX = "openquake.engine.calculators.hazard.classical.post_processing"


class HazardMapsTestCase(unittest.TestCase):

    def test_compute_hazard_map(self):
        curves = [
            [0.8, 0.5, 0.1],
            [0.98, 0.15, 0.05],
            [0.6, 0.5, 0.4],
            [0.1, 0.01, 0.001],
            [0.8, 0.2, 0.1],
        ]
        imls = [0.005, 0.007, 0.0098]
        poe = 0.2

        expected = [[0.00847798, 0.00664814, 0.0098, 0, 0.007]]
        actual = post_proc.compute_hazard_maps(curves, imls, poe)
        aaae(expected, actual)

    def test_compute_hazard_map_poes_list_of_one(self):
        curves = [
            [0.8, 0.5, 0.1],
            [0.98, 0.15, 0.05],
            [0.6, 0.5, 0.4],
            [0.1, 0.01, 0.001],
            [0.8, 0.2, 0.1],
        ]

        # NOTE(LB): Curves may be passed as a generator or iterator;
        # let's make sure that works, too.
        curves = iter(curves)

        imls = [0.005, 0.007, 0.0098]
        poe = [0.2]
        expected = [[0.00847798, 0.00664814, 0.0098, 0, 0.007]]
        actual = post_proc.compute_hazard_maps(curves, imls, poe)
        aaae(expected, actual)

    def test_compute_hazard_map_multi_poe(self):
        curves = [
            [0.8, 0.5, 0.1],
            [0.98, 0.15, 0.05],
            [0.6, 0.5, 0.4],
            [0.1, 0.01, 0.001],
            [0.8, 0.2, 0.1],
        ]
        imls = [0.005, 0.007, 0.0098]
        poes = [0.1, 0.2]
        expected = [
            [0.0098, 0.00792555, 0.0098, 0.005,  0.0098],
            [0.00847798, 0.00664814, 0.0098, 0, 0.007]
        ]
        actual = post_proc.compute_hazard_maps(curves, imls, poes)
        aaae(expected, actual)


class HazardMapTaskFuncTestCase(unittest.TestCase):

    MOCK_HAZARD_MAP = numpy.array([
        [0.0098, 0.0084],
        [0.0091, 0.00687952],
    ])

    TEST_POES = [0.1, 0.02]

    @classmethod
    def setUpClass(cls):
        cfg = helpers.get_data_path(
            'calculators/hazard/classical/haz_map_test_job2.ini')
        cls.job = helpers.run_job(cfg)
        models.JobStats.objects.create(oq_job=cls.job)

    def _test_maps(self, curve, hm_0_1, hm_0_02, lt_rlz=None):
        self.assertEqual(lt_rlz, hm_0_1.lt_realization)
        self.assertEqual(lt_rlz, hm_0_02.lt_realization)

        self.assertEqual(
            curve.investigation_time, hm_0_1.investigation_time)
        self.assertEqual(
            curve.investigation_time, hm_0_02.investigation_time)

        self.assertEqual(curve.imt, hm_0_1.imt)
        self.assertEqual(curve.imt, hm_0_02.imt)

        self.assertEqual(curve.statistics, hm_0_1.statistics)
        self.assertEqual(curve.statistics, hm_0_02.statistics)

        self.assertEqual(curve.quantile, hm_0_1.quantile)
        self.assertEqual(curve.quantile, hm_0_02.quantile)

        self.assertIsNone(hm_0_1.sa_period)
        self.assertIsNone(hm_0_02.sa_period)

        self.assertIsNone(hm_0_1.sa_damping)
        self.assertIsNone(hm_0_02.sa_damping)

        self.assertEqual(0.1, hm_0_1.poe)
        self.assertEqual(0.02, hm_0_02.poe)

        aaae([0.0, 0.001], hm_0_1.lons)
        aaae([0.0, 0.001], hm_0_1.lats)
        # our mock hazard map results:
        aaae([0.0098, 0.0084], hm_0_1.imls)

        aaae([0.0, 0.001], hm_0_02.lons)
        aaae([0.0, 0.001], hm_0_02.lats)
        # our mock hazard map results:
        aaae([0.0091, 0.00687952], hm_0_02.imls)

    @attr('slow')
    def test_hazard_curves_to_hazard_map_logic_tree(self):
        lt_haz_curves = models.HazardCurve.objects.filter(
            output__oq_job=self.job,
            imt__isnull=False,
            lt_realization__isnull=False)

        with mock.patch('%s.compute_hazard_maps' % MOCK_PREFIX) as compute:
            compute.return_value = self.MOCK_HAZARD_MAP

            for curve in lt_haz_curves:
                hazard_curves_to_hazard_map(
                    self.job.id, curve.id, self.TEST_POES)

                lt_rlz = curve.lt_realization
                # There should be two maps: 1 for each PoE
                hm_0_1, hm_0_02 = models.HazardMap.objects.filter(
                    output__oq_job=self.job,
                    lt_realization=lt_rlz).order_by('-poe')

                self._test_maps(curve, hm_0_1, hm_0_02, lt_rlz=lt_rlz)

    @attr('slow')
    def test_hazard_curves_to_hazard_map_mean(self):
        mean_haz_curves = models.HazardCurve.objects.filter(
            output__oq_job=self.job,
            imt__isnull=False,
            statistics='mean')

        with mock.patch('%s.compute_hazard_maps' % MOCK_PREFIX) as compute:
            compute.return_value = self.MOCK_HAZARD_MAP

            for curve in mean_haz_curves:
                hazard_curves_to_hazard_map(
                    self.job.id, curve.id, self.TEST_POES)

                hm_0_1, hm_0_02 = models.HazardMap.objects.filter(
                    output__oq_job=self.job,
                    statistics='mean').order_by('-poe')

                self._test_maps(curve, hm_0_1, hm_0_02)

    @attr('slow')
    def test_hazard_curves_to_hazard_map_quantile(self):
        with mock.patch('%s.compute_hazard_maps' % MOCK_PREFIX) as compute:
            compute.return_value = self.MOCK_HAZARD_MAP

            for quantile in (0.1, 0.9):
                quantile_haz_curves = models.HazardCurve.objects.filter(
                    output__oq_job=self.job,
                    imt__isnull=False,
                    statistics='quantile',
                    quantile=quantile)

                for curve in quantile_haz_curves:
                    hazard_curves_to_hazard_map(
                        self.job.id, curve.id, self.TEST_POES)

                    hm_0_1, hm_0_02 = models.HazardMap.objects.filter(
                        output__oq_job=self.job,
                        statistics='quantile',
                        quantile=quantile).order_by('-poe')

                    self._test_maps(curve, hm_0_1, hm_0_02)


class Bug1086719TestCase(unittest.TestCase):
    """
    Tests for bug https://bugs.launchpad.net/openquake/+bug/1086719.

    Here's a brief summary of the bug:

    With certain calculation parameters, hazard map creation was causing
    calculations to crash. The issue was isolated to an uncommitted
    transaction.
    """

    @attr('slow')
    def test(self):
        # The bug can be reproduced with any hazard calculation profile which
        # the following parameters set:
        #
        # * number_of_logic_tree_samples = 1
        # * mean_hazard_curves = false
        # * quantile_hazard_curves =
        # * poes = at least one PoE
        cfg = helpers.get_data_path(
            'calculators/hazard/classical/haz_map_1rlz_no_stats.ini'
        )
        job = helpers.run_job(cfg)
        self.assertEqual(job.status, 'complete')


class MeanCurveTestCase(unittest.TestCase):

    def test_compute_mean_curve(self):
        curves = [
            [1.0, 0.85, 0.67, 0.3],
            [0.87, 0.76, 0.59, 0.21],
            [0.62, 0.41, 0.37, 0.0],
        ]

        expected_mean_curve = numpy.array([0.83, 0.67333333, 0.54333333, 0.17])
        numpy.testing.assert_allclose(
            expected_mean_curve, post_processing.mean_curve(curves))

    def test_compute_mean_curve_weighted(self):
        curves = [
            [1.0, 0.85, 0.67, 0.3],
            [0.87, 0.76, 0.59, 0.21],
            [0.62, 0.41, 0.37, 0.0],
        ]
        weights = [0.5, 0.3, 0.2]

        expected_mean_curve = numpy.array([0.885, 0.735, 0.586, 0.213])
        numpy.testing.assert_allclose(
            expected_mean_curve,
            post_processing.mean_curve(curves, weights=weights))

    def test_compute_mean_curve_weights_None(self):
        # If all weight values are None, ignore the weights altogether
        curves = [
            [1.0, 0.85, 0.67, 0.3],
            [0.87, 0.76, 0.59, 0.21],
            [0.62, 0.41, 0.37, 0.0],
        ]
        weights = [None, None, None]

        expected_mean_curve = numpy.array([0.83, 0.67333333, 0.54333333, 0.17])
        numpy.testing.assert_allclose(
            expected_mean_curve,
            post_processing.mean_curve(curves, weights=weights))

    def test_compute_mean_curve_invalid_weights(self):
        curves = [
            [1.0, 0.85, 0.67, 0.3],
            [0.87, 0.76, 0.59, 0.21],
            [0.62, 0.41, 0.37, 0.0],
        ]
        weights = [0.6, None, 0.4]

        self.assertRaises(
            ValueError, post_processing.mean_curve, curves, weights
        )


class QuantileCurveTestCase(unittest.TestCase):

    def test_compute_quantile_curve(self):
        expected_curve = numpy.array([
            9.9178000e-01, 9.8892000e-01, 9.6903000e-01, 9.4030000e-01,
            8.8405000e-01, 7.8782000e-01, 6.4897250e-01, 4.8284250e-01,
            3.4531500e-01, 3.2337000e-01, 1.8880500e-01, 9.5574000e-02,
            4.3707250e-02, 1.9643000e-02, 8.1923000e-03, 2.9157000e-03,
            7.9955000e-04, 1.5233000e-04, 1.5582000e-05])

        quantile = 0.75

        curves = [
            [9.8161000e-01, 9.7837000e-01, 9.5579000e-01, 9.2555000e-01,
             8.7052000e-01, 7.8214000e-01, 6.5708000e-01, 5.0526000e-01,
             3.7044000e-01, 3.4740000e-01, 2.0502000e-01, 1.0506000e-01,
             4.6531000e-02, 1.7548000e-02, 5.4791000e-03, 1.3377000e-03,
             2.2489000e-04, 2.2345000e-05, 4.2696000e-07],
            [9.7309000e-01, 9.6857000e-01, 9.3853000e-01, 9.0089000e-01,
             8.3673000e-01, 7.4057000e-01, 6.1272000e-01, 4.6467000e-01,
             3.3694000e-01, 3.1536000e-01, 1.8340000e-01, 9.2412000e-02,
             4.0202000e-02, 1.4900000e-02, 4.5924000e-03, 1.1126000e-03,
             1.8647000e-04, 1.8882000e-05, 4.7123000e-07],
            [9.9178000e-01, 9.8892000e-01, 9.6903000e-01, 9.4030000e-01,
             8.8405000e-01, 7.8782000e-01, 6.4627000e-01, 4.7537000e-01,
             3.3168000e-01, 3.0827000e-01, 1.7279000e-01, 8.8360000e-02,
             4.2766000e-02, 1.9643000e-02, 8.1923000e-03, 2.9157000e-03,
             7.9955000e-04, 1.5233000e-04, 1.5582000e-05],
            [9.8885000e-01, 9.8505000e-01, 9.5972000e-01, 9.2494000e-01,
             8.6030000e-01, 7.5574000e-01, 6.1009000e-01, 4.4217000e-01,
             3.0543000e-01, 2.8345000e-01, 1.5760000e-01, 8.0225000e-02,
             3.8681000e-02, 1.7637000e-02, 7.2685000e-03, 2.5474000e-03,
             6.8347000e-04, 1.2596000e-04, 1.2853000e-05],
            [9.9178000e-01, 9.8892000e-01, 9.6903000e-01, 9.4030000e-01,
             8.8405000e-01, 7.8782000e-01, 6.4627000e-01, 4.7537000e-01,
             3.3168000e-01, 3.0827000e-01, 1.7279000e-01, 8.8360000e-02,
             4.2766000e-02, 1.9643000e-02, 8.1923000e-03, 2.9157000e-03,
             7.9955000e-04, 1.5233000e-04, 1.5582000e-05],
        ]
        actual_curve = post_processing.quantile_curve(curves, quantile)

        # TODO(LB): Check with our hazard experts to see if this is reasonable
        # tolerance. Better yet, get a fresh set of test data. (This test data
        # was just copied verbatim from from some old tests in
        # `tests/hazard_test.py`.
        numpy.testing.assert_allclose(expected_curve, actual_curve, atol=0.005)

        # Since this implementation is an optimized but equivalent version of
        # scipy's `mquantiles`, compare algorithms just to prove they are the
        # same:
        scipy_curve = mstats.mquantiles(curves, prob=quantile, axis=0)[0]
        numpy.testing.assert_allclose(scipy_curve, actual_curve)

    def test_compute_weighted_quantile_curve_case1(self):
        expected_curve = numpy.array([0.69909, 0.60859, 0.50328])

        quantile = 0.3

        curves = [
            [9.9996e-01, 9.9962e-01, 9.9674e-01],
            [6.9909e-01, 6.0859e-01, 5.0328e-01],
            [1.0000e+00, 9.9996e-01, 9.9947e-01],
        ]
        weights = [0.5, 0.3, 0.2]

        actual_curve = post_processing.weighted_quantile_curve(
            curves, weights, quantile)

        numpy.testing.assert_allclose(expected_curve, actual_curve)

    def test_compute_weighted_quantile_curve_case2(self):
        expected_curve = numpy.array([0.89556, 0.83045, 0.73646])

        quantile = 0.3

        curves = [
            [9.2439e-01, 8.6700e-01, 7.7785e-01],
            [8.9556e-01, 8.3045e-01, 7.3646e-01],
            [9.1873e-01, 8.6697e-01, 7.8992e-01],
        ]
        weights = [0.2, 0.3, 0.5]

        actual_curve = post_processing.weighted_quantile_curve(
            curves, weights, quantile)

        numpy.testing.assert_allclose(expected_curve, actual_curve)

    def test_weighted_quantile_with_decimal_weights(self):
        # NOTE(LB): This is a test for a bug I found.
        # In the case of end-branch enumeration with _more_ than 1 branch,
        # numpy.interp (used in `quantile_curves_weighted`) cannot handle the
        # `weights` input properly. `weights` is passed as a list of
        # `decimal.Decimal` types. Numpy throws back this error:
        # TypeError: array cannot be safely cast to required type
        # This doesn't appear to be a problem when there is only a single end
        # branch in the logic tree (and so the single weight is
        # decimal.Decimal(1.0)).
        # This test ensures that `weighted_quantile_curve` works when weights
        # are passed as `Decimal` types.
        expected_curve = numpy.array([0.89556, 0.83045, 0.73646])

        quantile = 0.3

        curves = [
            [9.2439e-01, 8.6700e-01, 7.7785e-01],
            [8.9556e-01, 8.3045e-01, 7.3646e-01],
            [9.1873e-01, 8.6697e-01, 7.8992e-01],
        ]
        weights = [decimal.Decimal(x) for x in ('0.2', '0.3', '0.5')]

        actual_curve = post_processing.weighted_quantile_curve(
            curves, weights, quantile)

        numpy.testing.assert_allclose(expected_curve, actual_curve)


class UHSTestCase(unittest.TestCase):

    def setUp(self):
        self.lons = [0.0, 1.0, 2.0]
        self.lats = [6.0, 7.0, 8.0]
        map1_imls = [0.01, 0.02, 0.03]
        map2_imls = [0.05, 0.10, 0.15]
        map3_imls = [1.25, 2.17828, 3.14]

        self.map1 = models.HazardMap(
            imt='PGA',
            poe=0.1,
            lons=list(self.lons),
            lats=list(self.lats),
            imls=map1_imls,
        )

        self.map2 = models.HazardMap(
            imt='SA',
            sa_period=0.025,
            poe=0.1,
            lons=list(self.lons),
            lats=list(self.lats),
            imls=map2_imls,
        )

        self.map3 = models.HazardMap(
            imt='SA',
            sa_period=0.1,
            poe=0.1,
            lons=list(self.lons),
            lats=list(self.lats),
            imls=map3_imls,
        )

        # an invalid map type for calculating UHS
        self.map_pgv = models.HazardMap(
            imt='PGV',
            poe=0.1,
            lons=list(self.lons),
            lats=list(self.lats),
            imls=[0.0, 0.0, 0.0],
        )

    def test_make_uhs(self):
        # intentionally out of order to set sorting
        # the PGV map will get filtered out/ignored
        maps = [self.map2, self.map_pgv, self.map3, self.map1]

        # they need to be sorted in ascending order by SA period
        # PGA is considered to be SA period = 0.0
        expected = {
            'periods': [0.0, 0.025, 0.1],  # again, 0.0 is PGA
            'uh_spectra': [
                # triples of (lon, lat, [imls])
                (0.0, 6.0, (0.01, 0.05, 1.25)),
                (1.0, 7.0, (0.02, 0.10, 2.17828)),
                (2.0, 8.0, (0.030, 0.15, 3.14)),
            ]
        }

        actual = post_proc.make_uhs(maps)

        self.assertEqual(expected, actual)

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import getpass
import mock
import unittest

from nose.plugins.attrib import attr

from openquake.engine.calculators.hazard.disaggregation import core
from openquake.engine.db import models

from openquake.engine.tests.utils import helpers


class DisaggHazardCalculatorTestcase(unittest.TestCase):

    def setUp(self):
        self.job, self.calc = self._setup_a_new_calculator()
        models.JobStats.objects.create(oq_job=self.job, num_sites=0)

    def _setup_a_new_calculator(self):
        cfg = helpers.get_data_path('disaggregation/job.ini')
        job = helpers.get_job(cfg, username=getpass.getuser())
        calc = core.DisaggHazardCalculator(job)
        return job, calc

    @attr('slow')
    def test_workflow(self):
        # Test `pre_execute` to ensure that all stats are properly initialized.
        # Then test the core disaggregation function.
        self.calc.pre_execute()
        job_stats = models.JobStats.objects.get(oq_job=self.job.id)
        self.assertEqual(2, job_stats.num_sites)

        with mock.patch.dict(os.environ, {'OQ_NO_DISTRIBUTE': '1'}):
            # to test the disagg function, we first need to compute the hazard
            # curves
            self.calc.execute()
            with mock.patch(
                    'openquake.engine.calculators.hazard.disaggregation.'
                    'core.save_disagg_result') as save:
                save.__name__ = 'save_disagg_result'
                save.task_func = save
                # Some of these tasks will not compute anything, since the
                # hazard  curves for these few are all 0.0s.
                # 2 poes * 2 imts * 2 sites = 8
                self.calc.post_execute()
                self.assertEqual(8, save.call_count)

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import os
import getpass
import unittest
import mock
import numpy

from nose.plugins.attrib import attr

from openquake.hazardlib.imt import PGA
from openquake.hazardlib.source.rupture import Rupture
from openquake.hazardlib.site import Site, SiteCollection
from openquake.hazardlib.geo.point import Point
from openquake.hazardlib.geo.mesh import Mesh
from openquake.hazardlib.geo.surface.complex_fault import ComplexFaultSurface
from openquake.hazardlib.source.complex_fault import ComplexFaultSource
from openquake.hazardlib.gsim import get_available_gsims

from openquake.engine.db import models
from openquake.engine.calculators.hazard.event_based import core

from openquake.engine.tests.utils import helpers


def make_mock_points(n):
    points = []
    for _ in range(n):
        point = mock.Mock()
        point.wkt2d = 'XXX'
        points.append(point)
    return points


def make_site_coll(lon, lat, n):
    assert n <= 1000
    sites = []
    for i in range(n):
        site = Site(Point(lon - float(i) / 1000, lat),
                    800., 'measured', 50., 2.5, i)
        sites.append(site)
    return SiteCollection(sites)


class FakeRupture(object):
    def __init__(self, id, trt, mag=5.0, rake=90.):
        hypocenter = Point(17.788328, -77.219496, 7.8125)
        lons = numpy.array(
            [-78.18106621, -78.18013243, -78.17919864, -78.15399318,
             -78.15305962, -78.15212606])
        lats = numpy.array(
            [15.615, 15.615, 15.615, 15.56553731,
             15.56553731,  15.56553731])
        surface = ComplexFaultSurface(Mesh(lons, lats, None))
        self.rupture = Rupture(mag, rake, trt, hypocenter,
                               surface, ComplexFaultSource)
        self.id = id


class RuptureCollectorTestCase(unittest.TestCase):
    """Tests for the routines used by the event-based hazard calculator"""

    # test a case with 5 sites and 2 ruptures
    def test_compute_gmf(self):
        hc = mock.Mock()
        hc.ground_motion_correlation_model = None
        hc.truncation_level = None
        hc.maximum_distance = 200.

        trt = 'Subduction Interface'
        gsim = get_available_gsims()['AkkarBommer2010']()
        num_sites = 5
        site_coll = make_site_coll(-78, 15.5, num_sites)
        params = dict(truncation_level=3,
                      correl_model=None,
                      maximum_distance=200,
                      num_sites=num_sites)
        rup_id, rup_seed = 42, 44
        rup = FakeRupture(rup_id, trt)
        pga = PGA()
        rlz = mock.Mock()
        rlz.id = 1
        coll = core.RuptureCollector(
            params, [pga], [gsim], trt_model_id=1, task_no=0)
        coll.calc_gmf(site_coll, rup.rupture, rup.id, rup_seed)
        expected_rups = {
            ('AkkarBommer2010', pga, 0): [rup_id],
            ('AkkarBommer2010', pga, 1): [rup_id],
            ('AkkarBommer2010', pga, 2): [rup_id],
            ('AkkarBommer2010', pga, 3): [rup_id],
            ('AkkarBommer2010', pga, 4): [rup_id],
        }
        expected_gmvs = {
            ('AkkarBommer2010', pga, 0): [0.1027847118266612],
            ('AkkarBommer2010', pga, 1): [0.02726361912605336],
            ('AkkarBommer2010', pga, 2): [0.0862595971325641],
            ('AkkarBommer2010', pga, 3): [0.04727148908077005],
            ('AkkarBommer2010', pga, 4): [0.04750575818347277],
        }
        numpy.testing.assert_equal(coll.ruptures_per_site, expected_rups)
        for i, gmvs in expected_gmvs.iteritems():
            numpy.testing.assert_allclose(gmvs, expected_gmvs[i])


class EventBasedHazardCalculatorTestCase(unittest.TestCase):
    """
    Tests for the core functionality of the event-based hazard calculator.
    """

    def setUp(self):
        self.cfg = helpers.get_data_path('event_based_hazard/job_2.ini')
        self.job = helpers.get_job(self.cfg, username=getpass.getuser())
        self.calc = core.EventBasedHazardCalculator(self.job)
        hc = self.job.hazard_calculation
        hc._site_collection = make_site_coll(0, 0, n=5)
        models.JobStats.objects.create(oq_job=self.job)

    @unittest.skip  # temporarily skipped
    def test_donot_save_trivial_gmf(self):
        ses = mock.Mock()

        # setup two ground motion fields on a region made by three
        # locations. On the first two locations the values are
        # nonzero, in the third one is zero. Then, we will expect the
        # cache inserter to add only two entries.
        gmvs = numpy.matrix([[1., 1.],
                             [1., 1.],
                             [0., 0.]])
        gmf_dict = {PGA: dict(rupture_ids=[1, 2], gmvs=gmvs)}
        points = make_mock_points(3)
        with helpers.patch('openquake.engine.writer.CacheInserter') as m:
            core._save_gmfs(
                ses, gmf_dict, points)
            self.assertEqual(2, m.add.call_count)

    @unittest.skip  # temporarily skipped
    def test_save_only_nonzero_gmvs(self):
        ses = mock.Mock()

        gmvs = numpy.matrix([[0.0, 0, 1]])
        gmf_dict = {PGA: dict(rupture_ids=[1, 2, 3], gmvs=gmvs)}

        points = make_mock_points(1)
        with helpers.patch('openquake.engine.writer.CacheInserter') as m:
            core._save_gmfs(ses, gmf_dict, points)
            self.assertEqual(1, m.add.call_count)

    def test_initialize_ses_db_records(self):
        hc = self.job.hazard_calculation
        self.calc.pre_execute()

        outputs = models.Output.objects.filter(
            oq_job=self.job, output_type='ses')
        # there is a single source model realization in this test
        self.assertEqual(1, len(outputs))

        ses_coll = models.SESCollection.objects.get(
            lt_model__hazard_calculation=hc)
        self.assertEqual(hc.ses_per_logic_tree_path, len(ses_coll))
        for ses in ses_coll:
            # The only metadata in in the SES is investigation time.
            self.assertEqual(hc.investigation_time, ses.investigation_time)

    @attr('slow')
    def test_complete_event_based_calculation_cycle(self):
        # run the calculation in process (to easy debugging)
        # and check the outputs
        with mock.patch.dict(os.environ, {'OQ_NO_DISTRIBUTE': '1'}):
            job = helpers.run_job(self.cfg)
        hc = job.hazard_calculation
        [rlz1, rlz2] = models.LtRealization.objects.filter(
            lt_model__hazard_calculation=hc.id)

        # check that the parameters are read correctly from the files
        self.assertEqual(hc.ses_per_logic_tree_path, 5)

        # check that we generated the right number of ruptures
        # (this is fixed if the seeds are fixed correctly)
        num_ruptures = models.SESRupture.objects.filter(
            rupture__ses_collection__output__oq_job=job.id).count()
        self.assertEqual(num_ruptures, 96)

        # check that we generated the right number of rows in GmfData
        # 242 = 121 sites * 2 IMTs
        num_gmf1 = models.GmfData.objects.filter(
            gmf__lt_realization=rlz1).count()
        num_gmf2 = models.GmfData.objects.filter(
            gmf__lt_realization=rlz2).count()

        # with concurrent_tasks=64, this test generates several tasks, but
        # only 15 give nonzero contributions
        self.assertEqual(num_gmf1, 242 * 15)
        self.assertEqual(num_gmf2, 242 * 15)

        # Now check for the correct number of hazard curves:
        curves = models.HazardCurve.objects.filter(output__oq_job=job)
        # ((2 IMTs * 2 real) + (2 IMTs * (1 mean + 2 quantiles))) = 10
        # + 3 mean and quantiles multi-imt curves
        self.assertEqual(13, curves.count())

        # Finally, check for the correct number of hazard maps:
        maps = models.HazardMap.objects.filter(output__oq_job=job)
        # ((2 poes * 2 realizations * 2 IMTs)
        # + (2 poes * 2 IMTs * (1 mean + 2 quantiles))) = 20
        self.assertEqual(20, maps.count())

    def test_task_arg_gen(self):
        self.calc.pre_execute()
        # this is also testing the splitting of fault sources
        expected = [  # source_id, seed
            ('3-0', 540589706),
            ('3-1', 721420855),
            ('3-2', 1007747341),
            ('3-3', 573154379),
            ('3-4', 1310571686),
            ('3-5', 2015354266),
            ('3-6', 425466075),
            ('3-7', 41871302),
            ('3-8', 930268948),
            ('3-9', 1920723121),
            ('4', 1760832373),
        ]

        # utility to present the generated arguments in a nicer way
        def process_args(arg_gen):
            for args in arg_gen:
                # args is (job_id, sitecol, src_seed_pairs, ...)
                for src, seed in args[2]:
                    if src.__class__.__name__ != 'PointSource':
                        yield src.source_id, seed

        actual = list(process_args(self.calc.task_arg_gen()))
        self.assertEqual(expected, actual)

########NEW FILE########
__FILENAME__ = post_processing_test
# Copyright (c) 2012-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest

import numpy

from openquake.engine.calculators.hazard.event_based import (
    post_processing as pp)

from openquake.engine.tests.calculators.hazard.event_based import _pp_test_data as test_data


class GmvsToHazCurveTestCase(unittest.TestCase):
    """
    Tests for
    :func:`openquake.engine.calculators.hazard.event_based.\
post_processing.gmvs_to_haz_curve`.
    """

    def test_gmvs_to_haz_curve_site_1(self):
        expected_poes = [0.63578, 0.39347, 0.07965]
        imls = [0.01, 0.1, 0.2]
        gmvs = test_data.SITE_1_GMVS
        invest_time = 1.0  # years
        duration = 1000.0  # years

        actual_poes = pp.gmvs_to_haz_curve(gmvs, imls, invest_time, duration)
        numpy.testing.assert_array_almost_equal(
            expected_poes, actual_poes, decimal=6)

    def test_gmvs_to_haz_curve_case_2(self):
        expected_poes = [0.63578, 0.28609, 0.02664]
        imls = [0.01, 0.1, 0.2]
        gmvs = test_data.SITE_2_GMVS
        invest_time = 1.0  # years
        duration = 1000.0  # years

        actual_poes = pp.gmvs_to_haz_curve(gmvs, imls, invest_time, duration)
        numpy.testing.assert_array_almost_equal(
            expected_poes, actual_poes, decimal=6)

########NEW FILE########
__FILENAME__ = _pp_test_data
"""
Test input data for event-based GMF post-processing.
"""

SITE_1_GMVS = [
    1.162211578688180647e-01,
    1.351968466120511136e-01,
    1.564700129489147440e-01,
    3.148238038358503604e-02,
    1.400746716987386342e-01,
    8.852703019021931907e-02,
    7.578870508857667165e-02,
    2.239887510439858842e-01,
    3.120020075560779871e-01,
    1.917180058829747735e-01,
    9.517803721594945976e-02,
    1.023035424336563304e-01,
    6.568098236719485672e-02,
    1.498359444078432856e-01,
    1.880613093593130891e-01,
    5.705700935720926792e-02,
    9.646960952080663176e-02,
    1.279295429597159250e-01,
    2.352043473737223667e-01,
    5.441859000026726817e-02,
    5.564855445197395417e-02,
    7.699366702666611550e-02,
    6.961852894981848672e-02,
    1.031714205808499651e-01,
    1.191721739798427271e-01,
    1.135419953216618011e-01,
    6.431278405145467170e-02,
    1.242003043107282323e-01,
    8.976090885200702085e-02,
    1.354835367823576942e-01,
    1.772203335734167939e-01,
    9.054418904580661953e-02,
    9.455793390015687638e-02,
    6.397700447883672425e-02,
    1.411676560022228633e-01,
    1.111088477990836154e-01,
    6.539112464718874196e-02,
    5.508107904853195430e-02,
    2.994232750853145775e-01,
    6.355104099082478641e-02,
    4.121619891570797029e-02,
    1.291821013538576590e-01,
    6.988215351032486811e-02,
    7.063707166284963201e-02,
    3.520174471701794511e-02,
    5.660950141882731962e-02,
    8.889785195737461010e-02,
    8.329063178317827609e-02,
    8.124519923551920542e-02,
    4.409739517997540020e-02,
    4.831975450795947952e-02,
    9.374995898011635742e-02,
    1.135151021180849373e-01,
    1.230929577540876979e-01,
    6.186735889808680361e-02,
    5.353535731289792154e-02,
    1.319888728876296158e-01,
    1.203522453272935916e-01,
    5.412703482946266964e-02,
    1.799937553605412965e-01,
    1.352833647945360374e-01,
    5.486154166217371148e-02,
    9.772288097381687244e-02,
    1.640520771495691810e-01,
    4.687029655283352775e-02,
    5.848603882616899841e-02,
    1.228921010690026683e-01,
    8.219662836889447821e-02,
    9.772144550636760463e-02,
    1.291813575929322278e-01,
    1.884873754741301921e-01,
    6.851829908489012744e-02,
    1.117550838970112898e-01,
    6.400637512420895292e-02,
    9.273739421494518742e-02,
    5.652937627145285943e-02,
    7.690525837317269131e-02,
    9.951163791863830432e-02,
    1.095172219602036368e-01,
    1.262965441226285290e-01,
    7.629250685420013722e-02,
    6.753897205277571703e-02,
    5.249078719478204175e-02,
    1.413991254848239176e-01,
    7.966552009470734275e-02,
    1.210743642519971214e-01,
    2.962111960546179659e-02,
    2.035010784704399944e-01,
    5.655930767653837704e-02,
    7.325808542446035632e-02,
    2.302217214995237737e-01,
    1.708106117010727565e-01,
    1.166405336944835863e-01,
    1.002705623600420731e-01,
    6.733622922532456201e-02,
    1.489485592391828483e-01,
    1.433663616676449093e-01,
    2.103109930133405525e-01,
    7.153288366632429285e-02,
    1.266742231289951504e-01,
    1.249223912884085685e-01,
    1.774608142417450629e-01,
    5.196419206832086912e-02,
    7.539377117624490587e-02,
    7.783200689621211366e-02,
    1.938619174516314492e-01,
    9.584482229257748620e-02,
    1.997992510226276330e-01,
    1.278252426829379063e-01,
    1.056506109872110827e-01,
    1.131130279065483968e-01,
    7.013253131302117638e-02,
    1.242424151605723559e-01,
    1.480977598598809120e-01,
    8.492405176025101532e-02,
    9.844861175707650403e-02,
    7.599128675276649314e-02,
    2.336495934388010398e-01,
    1.562463185450904402e-01,
    2.164220066620140825e-01,
    1.114154362855851593e-01,
    1.167857801906223292e-01,
    8.603784464615914229e-02,
    1.050220453285786171e-01,
    8.658292342690300514e-02,
    8.698474048076730580e-02,
    1.059503223153003687e-01,
    1.511967527947702949e-01,
    1.329013188358832698e-01,
    1.167447516215431580e-01,
    7.776730721386712619e-02,
    7.048235043423303758e-02,
    8.293905102254237049e-02,
    4.416915116344881376e-02,
    8.848199071779638825e-02,
    1.003784361311899492e-01,
    1.107788654880910717e-01,
    8.125093095305140067e-02,
    1.919988788469677854e-01,
    1.541908290878265808e-01,
    1.497930000688546259e-01,
    1.401474879563223419e-01,
    1.772081503633134159e-01,
    1.658475015318502666e-01,
    1.184668751109146378e-01,
    5.746342874419924779e-02,
    8.137895501508930762e-02,
    1.199533624304908591e-01,
    1.733946743376866273e-01,
    3.550165778724235360e-02,
    2.108120191976639024e-01,
    4.320860360601894851e-02,
    3.923305220577428393e-02,
    6.040849108727885247e-02,
    1.165643757736507385e-01,
    1.085178287864278085e-01,
    3.616709648144155603e-02,
    8.808740285191074548e-02,
    2.680906888535599109e-01,
    1.917995205065610720e-01,
    1.592147638907827900e-01,
    1.170753784749018755e-01,
    1.005764027300294211e-01,
    9.579200785089672654e-02,
    7.446438976229377560e-02,
    1.408408869699127874e-01,
    5.908988025912640524e-02,
    1.257331522546410174e-01,
    7.587845068049327291e-02,
    2.164845714688332645e-01,
    1.114820081267727386e-01,
    9.855927843729525573e-02,
    4.690640099315811218e-02,
    7.294942180496125084e-02,
    1.174920987599474553e-01,
    1.187047865335330715e-01,
    9.316148685291539278e-02,
    1.246834717121346331e-01,
    3.847671959665922847e-02,
    9.280983195615753745e-02,
    6.932315788217280339e-02,
    5.946617220464728987e-02,
    1.613337254458448722e-01,
    9.665286781001723726e-02,
    2.164500478717142906e-01,
    9.883972342309688131e-02,
    1.193949095437217417e-01,
    9.574759744733535305e-02,
    2.216839774776789840e-01,
    8.942401062272962997e-02,
    1.261777078100350669e-01,
    5.652479143465356098e-02,
    7.277837293515677930e-02,
    1.055775572204079027e-01,
    1.822047209042594873e-01,
    8.197265458917885050e-02,
    5.837167361156768430e-02,
    4.051010461339248775e-02,
    1.339931090835347893e-01,
    5.351821505613615915e-02,
    1.066028478148834807e-01,
    1.714482609336563101e-01,
    1.046740425660642598e-01,
    1.499792316566722350e-01,
    1.822963447619425081e-01,
    8.982671574850499552e-02,
    1.348229182007438243e-01,
    1.142426021761410448e-01,
    5.598092664868485613e-02,
    9.915048301510438755e-02,
    4.390109828439910100e-02,
    1.624208819027227746e-01,
    6.810170517414500246e-02,
    1.072342758285337316e-01,
    1.482124128101850180e-01,
    8.944323860870409537e-02,
    8.490893953627226542e-02,
    1.709917128226309535e-01,
    1.256626717953188976e-01,
    6.961805370762225165e-02,
    2.139855376717088309e-01,
    1.162320316014317095e-01,
    2.667884788468412038e-01,
    7.321829567889366619e-02,
    2.641677307868395830e-01,
    1.511948808661968957e-01,
    5.168833771956528322e-02,
    5.141216639689545620e-02,
    5.173766381556997584e-02,
    5.440123558939360948e-02,
    1.067055630116791609e-01,
    1.800266956457464984e-01,
    1.683567112957949641e-01,
    1.421408475954576145e-01,
    7.199318818362207550e-02,
    9.466785859789783597e-02,
    8.996034869105350162e-02,
    3.998333562985863926e-02,
    1.738116998050321937e-01,
    1.789191119476343794e-01,
    1.113197428257883537e-01,
    9.907671977266827024e-02,
    2.285419794080845168e-01,
    1.314480765563848519e-01,
    1.133473756740023958e-01,
    1.392347536270623354e-01,
    1.576049061263647688e-01,
    5.688348203993154328e-02,
    1.754448443659354928e-01,
    7.549961213590337950e-02,
    5.675966970268470774e-02,
    7.701088648915786761e-02,
    6.548373785076888609e-02,
    3.093437628376877746e-01,
    8.782319130107398009e-02,
    9.146485270985094118e-02,
    1.281298633558402567e-01,
    1.253203107293148211e-01,
    5.913996691520165966e-02,
    7.598904905870812465e-02,
    1.416857413004533051e-01,
    8.281668203367673065e-02,
    6.061079514823287245e-02,
    1.611386061397225844e-01,
    8.833872510365453623e-02,
    5.390838915568656881e-02,
    1.304116170705343913e-01,
    1.535149267266175954e-01,
    2.741912721842305611e-01,
    8.031074239969764050e-02,
    5.223622729931465258e-02,
    1.065388631232631195e-01,
    9.780229395796641301e-02,
    5.859608889103784524e-02,
    2.492552248448665630e-01,
    1.679965933883280915e-01,
    9.614936388986111693e-02,
    7.671364272344326218e-02,
    7.355772318685002698e-02,
    1.766668689288904415e-01,
    9.435800392643683976e-02,
    3.084153173213189936e-01,
    8.747071206567377799e-02,
    5.397766831564505124e-02,
    1.298516454356997141e-01,
    8.929671634306576844e-02,
    1.503540490184450407e-01,
    7.598407595090193867e-02,
    1.350723566237031770e-01,
    8.503512211095365370e-02,
    1.084090064238464601e-01,
    9.919933730406400862e-02,
    4.957816197554028304e-02,
    5.728134579196628856e-02,
    6.392815828232589759e-02,
    7.265906404381813899e-02,
    8.945692249454686995e-02,
    9.310966555436533953e-02,
    5.104632956771341934e-02,
    2.010306565516087607e-01,
    8.382215569265624722e-02,
    6.140537549181237370e-02,
    9.750747784138652507e-02,
    8.979059734854349939e-02,
    1.304307060803802376e-01,
    1.698845770982520265e-01,
    2.002070746103872534e-01,
    1.167939854961286134e-01,
    3.631676957915100368e-02,
    7.074550434967800949e-02,
    1.400704754601510837e-01,
    8.909939009871373217e-02,
    8.468437452842009217e-02,
    6.606301789034726979e-02,
    3.063351486052629316e-02,
    1.335908428143799398e-01,
    1.350074037959012319e-01,
    1.269383066397467841e-01,
    1.854471902579818698e-01,
    1.509471443634759213e-01,
    1.586295640218259861e-01,
    1.071886206122662361e-01,
    1.211196969713509014e-01,
    1.320470538116096104e-01,
    1.880454430607532701e-01,
    7.037679809567748412e-02,
    8.682745381079723379e-02,
    1.486065221224656119e-01,
    5.313990163358166957e-02,
    1.210668429915307265e-01,
    9.231912287264526507e-02,
    7.013769655261349600e-02,
    1.000663311925411025e-01,
    1.470079280372484365e-01,
    6.203495606847350685e-02,
    7.497018140930329644e-02,
    3.813615240778202992e-02,
    1.852223845604132335e-01,
    7.252762218384599391e-02,
    1.783465022453484816e-01,
    8.690654441622486237e-02,
    8.366758833318471700e-02,
    3.614934720049244737e-02,
    2.546367934182351411e-01,
    9.183562724495428209e-02,
    9.929279800359623598e-02,
    1.074958473572258794e-01,
    5.792745416811149323e-02,
    1.997287705027047666e-01,
    5.466626405956995227e-02,
    9.685727523132274530e-02,
    2.209632129575598847e-01,
    1.114466197231354649e-01,
    1.823937194950570884e-01,
    2.380829762004952888e-01,
    3.308260440365448640e-01,
    1.767061411178799391e-01,
    1.790090981003116810e-01,
    1.519283063897581276e-01,
    9.409226155497571464e-02,
    1.088860303581834221e-01,
    1.004204526834218958e-01,
    8.610363272031469617e-02,
    3.675557413392764594e-02,
    5.814189642913684952e-02,
    6.369646424817944352e-02,
    1.448821893783325265e-01,
    1.689316987593607877e-01,
    6.305930546535935821e-02,
    7.265146432084999828e-02,
    1.727313800821876077e-01,
    2.174434424470180482e-01,
    1.099114885378583151e-01,
    3.278040395976551669e-02,
    5.764003711680112940e-02,
    2.486297129367662506e-01,
    1.463578905318674295e-01,
    2.994212535198946257e-01,
    1.029501026319120105e-01,
    6.097994937134754850e-02,
    1.009502667466466025e-01,
    2.290218873733526617e-01,
    6.430143971734293695e-02,
    1.458288267650640624e-01,
    5.144769313110594822e-02,
    7.516646768254606203e-02,
    1.122708286233313479e-01,
    1.145865224955565642e-01,
    9.225350350518200793e-02,
    5.673214560504084153e-02,
    1.618052850387720343e-01,
    6.810911647013136172e-02,
    2.052381886369031572e-01,
    5.316974413920579762e-02,
    7.827387488665449311e-02,
    6.109382078281390405e-02,
    8.076362423937960555e-02,
    1.398037522735781357e-01,
    7.595302164623260466e-02,
    6.586860796175268706e-02,
    6.090197300404572023e-02,
    6.028646687909387791e-02,
    8.139477058806013354e-02,
    1.347396777817916891e-01,
    2.175983478246736602e-01,
    1.105965272257299348e-01,
    5.999271793599898067e-02,
    8.079387611919436329e-02,
    7.457580170656610830e-02,
    1.013059970143015409e-01,
    7.628758903435781114e-02,
    1.564614982414784805e-01,
    5.304352930091509044e-02,
    7.244211794013452610e-02,
    7.084180562675768122e-02,
    1.378436080501820316e-01,
    1.971841041468624101e-01,
    5.692371467523400785e-02,
    1.318167852156275355e-01,
    4.961765261842961128e-02,
    9.544685681750227080e-02,
    8.742010812823326760e-02,
    9.164401565634125446e-02,
    1.321043758556596504e-01,
    4.744559033779554430e-02,
    1.623275631403976182e-01,
    2.176829997784655579e-01,
    8.798879930167312347e-02,
    2.067252839924201280e-01,
    7.750709917926972004e-02,
    1.594489761128047578e-01,
    1.767927548387643599e-01,
    7.002367472504789081e-02,
    1.676811179390445272e-01,
    7.416804931892402586e-02,
    1.032724286583003442e-01,
    8.201690671941286315e-02,
    8.049020849913302023e-02,
    1.770527456357670060e-01,
    1.656540100519027781e-01,
    8.912851274243141497e-02,
    9.313733169157441261e-02,
    3.767256944891776010e-02,
    1.117318046091391459e-01,
    9.779587224857567984e-02,
    1.102260961769340880e-01,
    1.697414370595022037e-01,
    7.007059112888096297e-02,
    8.128601912455750422e-02,
    5.751355848327788928e-02,
    1.202448085133578370e-01,
    7.827523233273420145e-02,
    1.288940738273695596e-01,
    1.280832423438552914e-01,
    1.101208768786597625e-01,
    3.605679865685023494e-02,
    1.307149246933397013e-01,
    6.793330890403340150e-02,
    7.605636307068372459e-02,
    7.514912629407517752e-02,
    5.570622428814608201e-02,
    7.712584095236256487e-02,
    2.105472222564823692e-01,
    1.211855564842043431e-01,
    1.834688209763199185e-01,
    9.065699384018481577e-02,
    5.973546191923071597e-02,
    3.471328955443116898e-02,
    1.200620440165231367e-01,
    1.030400952688289434e-01,
    1.626563193647087568e-01,
    6.565009895299334097e-02,
    9.175343959926433868e-02,
    6.282700746528506464e-02,
    1.212123643306482063e-01,
    1.384443543948213895e-01,
    3.268702296937675555e-02,
    8.792400927974303171e-02,
    1.315160519616186130e-01,
    5.292549896530940323e-02,
    1.359280173519758772e-01,
    1.913934610383205837e-01,
    9.306074193210465972e-02,
    6.723625730410663637e-02,
    2.180353010330036978e-01,
    8.739873425447707012e-02,
    3.448241130872638227e-02,
    7.207826897694767643e-02,
    9.304847023818194063e-02,
    1.117551501442081707e-01,
    2.355582922875792962e-01,
    3.348007050881222635e-01,
    8.184278334142511269e-02,
    1.504974976657937058e-01,
    1.383035861074734529e-01,
    2.544395088221143575e-01,
    1.392191728260693817e-01,
    1.691767612311713687e-01,
    1.059499829034482171e-01,
    1.111743361485149212e-01,
    1.862056144677719793e-01,
    1.157903961461247966e-01,
    1.990530083427209940e-01,
    9.067945404065842763e-02,
    1.993276788151432388e-01,
    2.057666825794149668e-01,
    6.930153336273947584e-02,
    1.121120133567250488e-01,
    1.969975276708244849e-01,
    7.289476249676481168e-02,
    1.495928031747237874e-01,
    5.627330359853443403e-02,
    2.094086467141587637e-01,
    6.553800259423180063e-02,
    1.780521758166425106e-01,
    2.416109271368841460e-01,
    5.349921664568980245e-02,
    2.201785857524723999e-01,
    1.351955635756319496e-01,
    6.695236598782766668e-02,
    4.023193820993048453e-02,
    1.208328676148466702e-01,
    3.865346778007916484e-02,
    8.692001489830013250e-02,
    1.364061766752426574e-01,
    1.166107287817966520e-01,
    9.002930046231927275e-02,
    2.411794301300776433e-01,
    1.160334910835651989e-01,
    5.611745849015403048e-02,
    7.094503236592025752e-02,
    1.671340913281569307e-01,
    6.084836555769201905e-02,
    1.454002395964234251e-01,
    7.595424489026937620e-02,
    1.285369748939918788e-01,
    1.301559097913304874e-01,
    8.068780439512011904e-02,
    1.044254813502241835e-01,
    1.460028860926712857e-01,
    1.793831092368066704e-01,
    6.530218431578439786e-02,
    9.412744258373595407e-02,
    2.518873446977966668e-01,
    2.020524742132212836e-01,
    6.820437916915768339e-02,
    5.570277682375553846e-02,
    1.319737523035126314e-01,
    9.947829535807960200e-02,
    8.463154926686330959e-02,
    6.789127044265792776e-02,
    5.584661102784224673e-02,
    3.161189041709815822e-02,
    1.047474658959869492e-01,
    1.529727751260889501e-01,
    9.201740353971593622e-02,
    1.666939539312228824e-01,
    6.255029550357167623e-02,
    1.262373637560208484e-01,
    6.058144274654293088e-02,
    1.829023093835362712e-01,
    7.695286679643631444e-02,
    6.313188144038402172e-02,
    1.193237902345961610e-01,
    1.736157028855588313e-01,
    3.837669405313947485e-02,
    1.329088391357540477e-01,
    1.693322133498885163e-01,
    6.172010696015631237e-02,
    1.196836197634397192e-01,
    1.229400308603462305e-01,
    1.333713677495987071e-01,
    2.065738301141189226e-01,
    4.010521780424728538e-02,
    4.116674061516536337e-02,
    1.031256152913145180e-01,
    1.361990321776709312e-01,
    2.115221147276351021e-01,
    1.482561719375368181e-01,
    5.836688781037883272e-02,
    1.310386603572732955e-01,
    9.114740967719954512e-02,
    1.167905569783837388e-01,
    1.394906791160816784e-01,
    2.245781966272779318e-01,
    5.414586973474153769e-02,
    5.722585130577949319e-02,
    8.112880515280457283e-02,
    1.799026115439411877e-01,
    1.938270582600613023e-01,
    1.707583931043892556e-01,
    5.270671323790759866e-02,
    1.362545889021068435e-01,
    5.423047381203333073e-02,
    7.310349991628843636e-02,
    5.905477563205293462e-02,
    7.403235742899928751e-02,
    4.057390547315890134e-02,
    7.867509687619606007e-02,
    9.904780975916036012e-02,
    8.101891643015141087e-02,
    8.246174477499074174e-02,
    9.701921232539734152e-02,
    6.557652143625687657e-02,
    2.201218229918538716e-01,
    1.060427921294842118e-01,
    1.880169435713601456e-01,
    8.778568787274884655e-02,
    9.125719423675844599e-02,
    5.745591558728124770e-02,
    1.292014742825743079e-01,
    1.486504584726587030e-01,
    1.508137554930119861e-01,
    1.071543357738683200e-01,
    6.267857351175308567e-02,
    1.492082114446289520e-01,
    2.161434720134719767e-01,
    6.873562224109185104e-02,
    1.367545908450035763e-01,
    9.138921533712754397e-02,
    5.762993507495817874e-02,
    1.369580694966049395e-01,
    1.082865302585725858e-01,
    1.771451586011000834e-01,
    1.134056961360822430e-01,
    1.147375128971159369e-01,
    1.726953488949269366e-01,
    1.719956795850010045e-01,
    1.158484135581280411e-01,
    1.117213387918264828e-01,
    7.438480368208709448e-02,
    5.268416174651954775e-02,
    1.336619983477770057e-01,
    1.235361891902991560e-01,
    1.123586712314090380e-01,
    1.269423353384117881e-01,
    1.631068058894023232e-01,
    5.240750822668117925e-02,
    2.269350798006274095e-01,
    2.870035016389942406e-01,
    1.418608637299907382e-01,
    1.355642824169054073e-01,
    1.275435345122418462e-01,
    3.001325115208282601e-01,
    9.522950717273498400e-02,
    4.336631017978156283e-02,
    3.217672626861203811e-02,
    7.885242216523677405e-02,
    8.867678446814551696e-02,
    3.708325724146042252e-02,
    1.414714457569427875e-01,
    2.085145826047934137e-01,
    1.000675874826206097e-01,
    7.394374864019073734e-02,
    8.283867358566018857e-02,
    9.398699019304423974e-02,
    6.236914022789341594e-02,
    1.067385531065882659e-01,
    7.286707613320914101e-02,
    1.655099274971107426e-01,
    2.730308575923121039e-01,
    4.555544551500500389e-02,
    2.146778972163784771e-01,
    7.587654316094330498e-02,
    6.679611260148772545e-02,
    2.019994922637818879e-01,
    9.917161283524945914e-02,
    5.546967156478519595e-02,
    8.853594739375086897e-02,
    6.327567867907864241e-02,
    1.306627250830028064e-01,
    1.431654162219468118e-01,
    1.547150011108195944e-01,
    9.292220029483641563e-02,
    2.948206597773979365e-02,
    6.047689520279039832e-02,
    6.873409728062600255e-02,
    3.010504770574252698e-01,
    9.326360284624546360e-02,
    5.475776634768006468e-02,
    4.689069968019343032e-02,
    3.125048064180869778e-01,
    1.104670481332159643e-01,
    9.321920861520102686e-02,
    1.108890782150438598e-01,
    1.114588146988781392e-01,
    3.955201083573231524e-02,
    6.538049420140024104e-02,
    1.208943666353374385e-01,
    7.021129795811587027e-02,
    8.381694576768902727e-02,
    9.313909619332023937e-02,
    1.360893523499166480e-01,
    6.219146549938069091e-02,
    1.322252142106225548e-01,
    2.386468069687004978e-01,
    1.267883870774636768e-01,
    1.939421771280816498e-01,
    4.195057417464329724e-02,
    5.964365620017615666e-02,
    1.137894822464752820e-01,
    1.728987460250203945e-01,
    4.359087817871230147e-02,
    1.615919710958221567e-01,
    6.772970969825395371e-02,
    1.002983974593278510e-01,
    1.403471448611690042e-01,
    9.885384810040692694e-02,
    6.930750722864444102e-02,
    5.717555588417283507e-02,
    9.360883730004526670e-02,
    8.267566203119922075e-02,
    1.087151937211036368e-01,
    9.875263170771876597e-02,
    6.010366131549999685e-02,
    1.272078825093186094e-01,
    1.197075914412539938e-01,
    6.798864818908383068e-02,
    1.052842742075548593e-01,
    1.149677067405718706e-01,
    7.968773611975858706e-02,
    3.805904954543700147e-01,
    2.597347152154009242e-01,
    3.166010205366909247e-02,
    6.887709636111892519e-02,
    6.276699622370524034e-02,
    5.502098799560199704e-02,
    7.978202628646660632e-02,
    1.544122349081126777e-01,
    2.156998780029375229e-01,
    3.282692080838203741e-01,
    1.424098851682632783e-01,
    1.742365099144979612e-01,
    3.047502389883098295e-01,
    7.447993881821890483e-02,
    1.458553882588722039e-01,
    1.534087966065515451e-01,
    1.359142185678490766e-01,
    4.752481637454366442e-02,
    4.819000520000234211e-02,
    2.084761202946778524e-01,
    1.247414558948886643e-01,
    1.757537505268608469e-01,
    8.496836059039823696e-02,
    8.181591308572283561e-02,
    1.012360044286990945e-01,
    7.095576519980005548e-02,
    1.262474299609505912e-01,
    7.870600832109518608e-02,
    1.751576058064825692e-01,
    1.061287168737181930e-01,
    6.513105621174626603e-02,
    1.267726502099148500e-01,
    1.014998615252417774e-01,
    1.081759551662752755e-01,
    1.092082402342356984e-01,
    9.366141628031141031e-02,
    4.938302155614985450e-02,
    1.465521535008776910e-01,
    5.108006503927341874e-02,
    9.627336421671320310e-02,
    5.214588093893226389e-02,
    5.396821153810097554e-02,
    1.146192423683607825e-01,
    2.887757641717425550e-01,
    8.224695472615330127e-02,
    1.166812833918415282e-01,
    4.343297263750139220e-02,
    1.168389734038772082e-01,
    5.468275829032311075e-02,
    5.946928569885615717e-02,
    4.707796269980532577e-02,
    8.898226039275997090e-02,
    9.442892325424472044e-02,
    7.064774771148268906e-02,
    1.036709887944850528e-01,
    7.203397053987810328e-02,
    7.383585961404512399e-02,
    4.672573150252873342e-02,
    8.491713280264266694e-02,
    1.186841902279647670e-01,
    9.859995369454420400e-02,
    8.895080191276187598e-02,
    1.536784506033898046e-01,
    6.757124158621491405e-02,
    8.343042626970621767e-02,
    1.925579159344926183e-01,
    3.429210941041460070e-01,
    1.279131489072011241e-01,
    6.297147355475521402e-02,
    1.112277619146927604e-01,
    9.436368608346984432e-02,
    1.390929668058213386e-01,
    8.276978342411279999e-02,
    9.621091492769762166e-02,
    4.720583138278983126e-02,
    1.596485916573744113e-01,
    7.388328528930325489e-02,
    6.783873449967867120e-02,
    8.006077933363048293e-02,
    1.187925189994156333e-01,
    1.286942759786977586e-01,
    6.882254843807490696e-02,
    8.892802753449678688e-02,
    2.396580638435872324e-01,
    1.627541439580451610e-01,
    9.888711886580787980e-02,
    1.102294108076200957e-01,
    1.161878380184630383e-01,
    1.933451878630053355e-01,
    1.407594899028325464e-01,
    6.676128752457878568e-02,
    4.361601132088804150e-02,
    5.545769486176469615e-02,
    8.197919036445304453e-02,
    1.449905575338128194e-01,
    1.213376196325707396e-01,
    1.757944891644780960e-01,
    1.356656568759523618e-01,
    7.524300232330555227e-02,
    2.775573756580542173e-01,
    4.755210595727370898e-02,
    8.290632518425339326e-02,
    1.739056508497609510e-01,
    1.652128312762572715e-01,
    2.537959828535595319e-01,
    8.748492777656728669e-02,
    1.142167936387979560e-01,
    9.759337696048436817e-02,
    6.903608729872952321e-02,
    2.462247226627097274e-01,
    7.189254189848243159e-02,
    1.175241174000760114e-01,
    6.936867538963091551e-02,
    1.167105055500654437e-01,
    6.201341675237213164e-02,
    1.942073184149412657e-01,
    5.006824471907331264e-02,
    1.900900541278782441e-01,
    6.100632607320288781e-02,
    9.611649223235907424e-02,
    1.146405697428676956e-01,
    7.924131007029650009e-02,
    1.401261022385790755e-01,
    2.293907390625631593e-01,
    1.325477712606108194e-01,
    1.067184649994849938e-01,
    1.733594611561041776e-01,
    9.546114995509749623e-02,
    8.223151033938241872e-02,
    3.626408852154541090e-02,
    3.743101306412002782e-02,
    1.753371825663122274e-01,
    1.285910306223186084e-01,
    7.270885593699857874e-02,
    1.609816795950768376e-01,
    8.487525557628423922e-02,
    1.074539856244130925e-01,
    1.073133794208651209e-01,
    1.260929616124256136e-01,
    1.700206425474634353e-01,
    5.656624636752153462e-02,
    8.209126196434021749e-02,
    3.299856983858427684e-02,
    2.187218435834237529e-01,
    6.641242678111507203e-02,
    6.896732202179607640e-02,
    4.270053900421579701e-02,
    6.350302515361887223e-02,
    4.386333965278505637e-02,
    7.173474325811324270e-02,
    6.003774193101470147e-02,
    1.251648429315637689e-01,
    6.142458741206544698e-02,
    8.860449240455084419e-02,
    6.663875181671011227e-02,
    1.370809098143769278e-01,
    9.303472441134280624e-02,
    7.804494546336142313e-02,
    4.182138370135112665e-02,
    1.150346099698132257e-01,
    4.907118041455584001e-02,
    6.898608893394071739e-02,
    5.591696796721089963e-02,
    7.053654278592866311e-02,
    1.567438827684415337e-01,
    5.927511534898223522e-02,
    5.991703173218611983e-02,
    1.301436888220021126e-01,
    7.110452374981110490e-02,
    6.439807501314399885e-02,
    1.068999526543799039e-01,
    1.027392347272014228e-01,
    1.428296624174224816e-01,
    8.205295269992517182e-02,
    5.222733559177052121e-02,
    1.385351927620228352e-01,
    8.936755903645385302e-02,
    1.811287244061352275e-01,
    4.329907152975404766e-02,
    1.593351310822873623e-01,
    9.962004184631219339e-02,
    9.646414190358827689e-02,
    8.166699688644246669e-02,
    1.583953992631194352e-01,
    5.487868480329614884e-02,
    2.390959210792514555e-01,
    6.700714470925908661e-02,
    1.753604888959596486e-01,
    1.758355524546161841e-01,
    1.912456915255054402e-01,
    6.940436317141303324e-02,
    3.017956420214186752e-01,
    1.797198392127511701e-01,
    6.829117125955221079e-02,
    8.916688040367932999e-02,
    1.142010926742815929e-01,
    6.583924474528968229e-02,
    1.184364905621759223e-01,
    8.624794829107852723e-02,
    5.120195717945248692e-02,
    1.070476691214538451e-01,
    6.547162666913922124e-02,
    9.796647733299321259e-02,
    1.157251878168728682e-01,
    5.691864201497019021e-02,
    2.045017499269820016e-01,
    4.531574680708309877e-02,
    1.232794753732400356e-01,
    9.732976643075878787e-02,
    8.144200702206427778e-02,
    1.561188862494380936e-01,
    9.164985644293260902e-02,
    1.988683955188884356e-01,
    1.512674567674839043e-01,
    1.598522669692960529e-01,
    1.320434484101178918e-01,
    1.168722842412366486e-01,
    9.541008390532781935e-02,
    2.178367582953917025e-01,
    7.630325332453337384e-02,
    9.222295269283070640e-02,
    6.505388302671606671e-02,
    1.529683437503843724e-01,
    1.460120865307423799e-01,
    9.836619371530778977e-02,
    5.542032334331478682e-02,
    1.852316729417544938e-01,
    4.282852187604899163e-02,
    7.962592068905707809e-02,
    1.032717096464169648e-01,
    9.815019203168834361e-02,
    8.585030427340900605e-02,
    1.148320006581630831e-01,
    1.004505394361804582e-01,
    1.048450156966107272e-01,
    1.205580261950428417e-01,
    2.251316573887141015e-01,
    1.851926409159018738e-01,
    3.114168705412859867e-02,
    1.025725254326259051e-01,
    1.143495886217069102e-01,
    8.847169692816904407e-02,
    6.070406901811251782e-02,
    8.661809450269933319e-02,
    1.305607404888376177e-01,
    6.853378470019375057e-02,
    1.321238009769663546e-01,
    1.328858772759775320e-01,
    4.219362886523717049e-02,
    8.441604779010106197e-02,
    1.954545094931338656e-01,
    7.237135097994709665e-02,
    4.575223562788920117e-02,
    1.189176707237017544e-01,
    1.428982334912543006e-01,
    6.192028311508825100e-02,
    2.077732657658316051e-01,
    6.805102947330561591e-02,
    1.854449334990446729e-01,
    1.942553344891398137e-01,
    1.591781870638514784e-01,
    1.357192723222629127e-01,
    5.384196350736345643e-02,
    1.252511045679207680e-01,
    1.128841931514092956e-01,
    6.631298157087443801e-02,
    1.806179679637341662e-01,
    9.137671097211268190e-02,
    1.264544991868781221e-01,
    1.744124220717262475e-01,
    8.771654807396669917e-02,
    7.095858601021676204e-02,
    6.147889882638685188e-02,
    3.704767569219881429e-02,
    8.402687924254177709e-02,
    4.429642052075603853e-02,
    1.609772133517488601e-01,
    1.310399372140196028e-01,
    8.126903611605990063e-02,
    1.075191786652874015e-01,
    8.170938533436358786e-02,
    6.667076639058976129e-02,
    6.996213647711754802e-02,
    6.698986414995325434e-02,
    1.058448812308084436e-01,
    1.856447362977222404e-01,
    1.440165642534352586e-01,
    1.592181003444543830e-01,
    1.576757798081807327e-01,
]

SITE_2_GMVS = [
    9.198179909309620039e-02,
    1.069998734406542445e-01,
    1.238362580366503957e-01,
    2.491634088419533363e-02,
    1.108603678236252249e-01,
    7.006362399569532329e-02,
    5.998203401872778673e-02,
    1.772731288815456996e-01,
    2.469301330490840940e-01,
    1.517328464371472674e-01,
    7.532748131071195785e-02,
    8.096687435575207481e-02,
    5.198240178570294467e-02,
    1.185858064759850727e-01,
    1.488387991642443620e-01,
    4.515706492505885961e-02,
    7.634968024968376288e-02,
    1.012482557769227576e-01,
    1.861495739904089064e-01,
    4.306892053152679839e-02,
    4.404236069649430929e-02,
    6.093568625328280725e-02,
    5.509872436692691416e-02,
    8.165374578980334752e-02,
    9.431734432446571559e-02,
    8.986140900518783947e-02,
    5.089955813738913232e-02,
    9.829679593543182159e-02,
    7.104016201385823792e-02,
    1.072267708329248809e-01,
    1.402588428551102828e-01,
    7.166007944318722456e-02,
    7.483670820488404207e-02,
    5.063380954432339376e-02,
    1.117253967431312139e-01,
    8.793572446814108823e-02,
    5.175299747536830919e-02,
    4.359323930150189874e-02,
    2.369748506875237193e-01,
    5.029668600594380118e-02,
    3.262005126747146572e-02,
    1.022395776384065202e-01,
    5.530736676737411928e-02,
    5.590483740964424797e-02,
    2.785998582067072613e-02,
    4.480289029768787235e-02,
    7.035710630056381254e-02,
    6.591934118970647905e-02,
    6.430050887804066029e-02,
    3.490033844398332530e-02,
    3.824207255270619527e-02,
    7.419724643965784627e-02,
    8.984012471157265534e-02,
    9.742040018815906199e-02,
    4.896415662118520223e-02,
    4.236989693641032839e-02,
    1.044609622817349454e-01,
    9.525129720866656879e-02,
    4.283817279473545181e-02,
    1.424538332536024743e-01,
    1.070683471869596781e-01,
    4.341948915018935262e-02,
    7.734156645279677278e-02,
    1.298369890464270149e-01,
    3.709491696703586666e-02,
    4.628805263781537310e-02,
    9.726143464700652552e-02,
    6.505350570755276396e-02,
    7.734043036982407582e-02,
    1.022389889980137717e-01,
    1.491760039253405279e-01,
    5.422796103735921802e-02,
    8.844718004143170553e-02,
    5.065705457863041439e-02,
    7.339586456989666274e-02,
    4.473947624001326645e-02,
    6.086571631706085006e-02,
    7.875725603068463310e-02,
    8.667605186783465532e-02,
    9.995583902848721725e-02,
    6.038076169997004528e-02,
    5.345288476066215738e-02,
    4.154318482559452713e-02,
    1.119085904045540175e-01,
    6.305029134427871107e-02,
    9.582280930742205494e-02,
    2.344326904346877641e-02,
    1.610584136171175629e-01,
    4.476316508066530836e-02,
    5.797920635985586574e-02,
    1.822061363193274997e-01,
    1.351859476928538306e-01,
    9.231370890752438330e-02,
    7.935789740077697774e-02,
    5.329242645544352008e-02,
    1.178834964508686106e-01,
    1.134655351696470943e-01,
    1.664480363227634208e-01,
    5.661381674903265965e-02,
    1.002547484105853065e-01,
    9.886828275011062817e-02,
    1.404491683081135200e-01,
    4.112641762060568335e-02,
    5.966946845454582787e-02,
    6.159917998253433558e-02,
    1.534296187529430588e-01,
    7.585520011924337669e-02,
    1.581286480320395205e-01,
    1.011657086118465110e-01,
    8.361587039820581047e-02,
    8.952190804582739470e-02,
    5.550552518506066818e-02,
    9.833012404712011822e-02,
    1.172101417965997622e-01,
    6.721209124417883152e-02,
    7.791593711236988185e-02,
    6.014236477327495473e-02,
    1.849190832028144993e-01,
    1.236592178652382024e-01,
    1.712845225529377735e-01,
    8.817837013685377034e-02,
    9.242866245188577357e-02,
    6.809358885908435577e-02,
    8.311839987571792432e-02,
    6.852498472382768935e-02,
    6.884299786531131171e-02,
    8.385307322488348702e-02,
    1.196628014564671133e-01,
    1.051831063511504544e-01,
    9.239619089793367490e-02,
    6.154797421852398892e-02,
    5.578238520535532680e-02,
    6.564108694166059799e-02,
    3.495712883031753399e-02,
    7.002797806185992457e-02,
    7.944327276380970526e-02,
    8.767456404614416832e-02,
    6.430504517504694040e-02,
    1.519551398733720859e-01,
    1.220324261367525531e-01,
    1.185518187096138570e-01,
    1.109179973508007955e-01,
    1.402492006040383654e-01,
    1.312579554852937314e-01,
    9.375914425098548499e-02,
    4.547872052621755590e-02,
    6.440636823678018430e-02,
    9.493560626952797510e-02,
    1.372310721318417137e-01,
    2.809734831366802327e-02,
    1.668445673044873234e-01,
    3.419691533677615691e-02,
    3.105051431232299167e-02,
    4.780955372152569799e-02,
    9.225343466226774980e-02,
    8.588509449130414530e-02,
    2.862400154446521558e-02,
    6.971568637186116579e-02,
    2.121770625335853977e-01,
    1.517973601785981008e-01,
    1.260085572489888506e-01,
    9.265786143502083294e-02,
    7.959995098191996465e-02,
    7.581340078207468580e-02,
    5.893392102009840927e-02,
    1.114667794308366600e-01,
    4.676595547744348547e-02,
    9.950994949716453208e-02,
    6.005306205834240463e-02,
    1.713340386960967854e-01,
    8.823105759784331814e-02,
    7.800352288877103468e-02,
    3.712349138013931044e-02,
    5.773491835277751866e-02,
    9.298766955464346506e-02,
    9.394743630622331976e-02,
    7.373150744713134752e-02,
    9.867919280412125460e-02,
    3.045192421586480000e-02,
    7.345319452496791579e-02,
    5.486495658573887352e-02,
    4.706376708737702169e-02,
    1.276855831848662803e-01,
    7.649471775790872563e-02,
    1.713067154218118060e-01,
    7.822547760694542607e-02,
    9.449362563385282909e-02,
    7.577825271701552068e-02,
    1.754490443256991050e-01,
    7.077352807380413169e-02,
    9.986178749750629358e-02,
    4.473584762758951727e-02,
    5.759954383865320549e-02,
    8.355805289729376084e-02,
    1.442036746092839605e-01,
    6.487624442754399978e-02,
    4.619753970208494154e-02,
    3.206122165806703228e-02,
    1.060471902499241781e-01,
    4.235632990914629803e-02,
    8.436950646738761161e-02,
    1.356906072976872824e-01,
    8.284297739006718508e-02,
    1.186992093982762730e-01,
    1.442761891791271289e-01,
    7.109224406882667935e-02,
    1.067039324206646295e-01,
    9.041589564181809480e-02,
    4.430541256400799544e-02,
    7.847142444556276519e-02,
    3.474498169188595248e-02,
    1.285459997271982679e-01,
    5.389825293511490284e-02,
    8.486924236538034849e-02,
    1.173008824571962483e-01,
    7.078874582567737384e-02,
    6.720013086127307744e-02,
    1.353292779373992671e-01,
    9.945416860865644026e-02,
    5.509834824236391204e-02,
    1.693562092813062914e-01,
    9.199040497439184283e-02,
    2.111464445169398130e-01,
    5.794771525746288121e-02,
    2.090722858529830974e-01,
    1.196613199417316531e-01,
    4.090809610538868879e-02,
    4.068952372508454624e-02,
    4.094713463448226148e-02,
    4.305518559751281282e-02,
    8.445079914050078462e-02,
    1.424799034352404181e-01,
    1.332438385432659356e-01,
    1.124956172025508572e-01,
    5.697811907064196518e-02,
    7.492370674842283029e-02,
    7.119800620972349670e-02,
    3.164431685604660038e-02,
    1.375611217842241385e-01,
    1.416033200052807250e-01,
    8.810263473071121287e-02,
    7.841304543892894940e-02,
    1.808767251999048031e-01,
    1.040329557087235762e-01,
    8.970737969022625491e-02,
    1.101956250458016456e-01,
    1.247344552165319115e-01,
    4.501972887431137521e-02,
    1.388536538639728590e-01,
    5.975323497404747691e-02,
    4.492173913011403286e-02,
    6.094931438406769875e-02,
    5.182629505079511256e-02,
    2.448262981182426634e-01,
    6.950657940517124267e-02,
    7.238872732220788531e-02,
    1.014067968788050361e-01,
    9.918321117398667952e-02,
    4.680559594240563809e-02,
    6.014059377796666939e-02,
    1.121354289497343143e-01,
    6.554423951770216306e-02,
    4.796966477042260979e-02,
    1.275311584213891791e-01,
    6.991459226207448474e-02,
    4.266512837832593663e-02,
    1.032126626575801814e-01,
    1.214974915660141652e-01,
    2.170052938174779344e-01,
    6.356094456367779311e-02,
    4.134171654226811987e-02,
    8.431886657395012974e-02,
    7.740441687768821744e-02,
    4.637515040161819396e-02,
    1.972699673192319647e-01,
    1.329588276758657539e-01,
    7.609622580278072035e-02,
    6.071406447913647864e-02,
    5.821635096386973407e-02,
    1.398208101021117533e-01,
    7.467847609798866637e-02,
    2.440914913883707182e-01,
    6.922761407037678061e-02,
    4.271995851329871141e-02,
    1.027694800275178177e-01,
    7.067278258931193602e-02,
    1.189958539671258225e-01,
    6.013665787325286893e-02,
    1.069013473778675782e-01,
    6.729999650059574623e-02,
    8.579896837730438541e-02,
    7.851008956880127510e-02,
    3.923802359107837756e-02,
    4.533461322391828946e-02,
    5.059515082575347794e-02,
    5.750511829106474959e-02,
    7.079957576801733377e-02,
    7.369049412081984940e-02,
    4.039998668776937502e-02,
    1.591032287198065331e-01,
    6.634000921910232496e-02,
    4.859852556364366999e-02,
    7.717108830566288269e-02,
    7.106365860754108865e-02,
    1.032277704185139561e-01,
    1.344530490507145071e-01,
    1.584514149705436858e-01,
    9.243515643952468497e-02,
    2.874245846793426970e-02,
    5.599065511958509905e-02,
    1.108570467624537120e-01,
    7.051661117184701566e-02,
    6.702240166106228358e-02,
    5.228475907917180809e-02,
    2.424451675655090593e-02,
    1.057288215825504113e-01,
    1.068499412650137509e-01,
    1.004637540415322661e-01,
    1.467698869077043167e-01,
    1.194652519482684166e-01,
    1.255454080448099863e-01,
    8.483310910868875621e-02,
    9.585868732791683056e-02,
    1.045070088549984177e-01,
    1.488262419783305257e-01,
    5.569884711145869222e-02,
    6.871851527416780736e-02,
    1.176127954018584276e-01,
    4.205691842619664611e-02,
    9.581685669877632039e-02,
    7.306482888517859542e-02,
    5.550961315010218000e-02,
    7.919626116721931175e-02,
    1.163476078691044746e-01,
    4.909679933046263989e-02,
    5.933422356837206313e-02,
    3.018238652307774286e-02,
    1.465919672165919563e-01,
    5.740109025539479048e-02,
    1.411501351329197962e-01,
    6.878111055640381832e-02,
    6.621767879264668644e-02,
    2.860995409540993684e-02,
    2.015291432592964393e-01,
    7.268217224585259151e-02,
    7.858405788442567041e-02,
    8.507625991917026897e-02,
    4.584596771338254884e-02,
    1.580728670955697523e-01,
    4.326493910491831241e-02,
    7.665649348536174301e-02,
    1.748786041537137292e-01,
    8.820304988312702155e-02,
    1.443532552082592846e-01,
    1.884278292002450339e-01,
    2.618281840874481237e-01,
    1.398518915907479010e-01,
    1.416745384337364799e-01,
    1.202417805084221164e-01,
    7.446815758223397641e-02,
    8.617650307490484773e-02,
    7.947652624481824080e-02,
    6.814565601734247358e-02,
    2.908974490991034917e-02,
    4.601568539071775737e-02,
    5.041177944578510012e-02,
    1.146652182781357776e-01,
    1.336989052654066479e-01,
    4.990750800135575899e-02,
    5.749910358424165091e-02,
    1.367061160905520267e-01,
    1.720929252817144095e-01,
    8.698808927823362125e-02,
    2.594364560212886889e-02,
    4.561849504012455148e-02,
    1.967749136498633811e-01,
    1.158331437188641067e-01,
    2.369732507445553304e-01,
    8.147858643424189806e-02,
    4.826182731816824889e-02,
    7.989584103752786493e-02,
    1.812565424281439019e-01,
    5.089057980435369305e-02,
    1.154144227389847788e-01,
    4.071764091982351536e-02,
    5.948957191355482704e-02,
    8.885535983132190896e-02,
    9.068808712833059671e-02,
    7.301289524774715045e-02,
    4.489995552318632593e-02,
    1.280587931969881799e-01,
    5.390411851373189817e-02,
    1.624332279905429666e-01,
    4.208053690846862149e-02,
    6.194889094280903175e-02,
    4.835194943951011404e-02,
    6.391937229882012350e-02,
    1.106459519927000396e-01,
    6.011208032760240877e-02,
    5.213089574377227642e-02,
    4.820011388592674445e-02,
    4.771297916997602195e-02,
    6.441888527655349750e-02,
    1.066380528197996003e-01,
    1.722155232284844906e-01,
    8.753025468179918833e-02,
    4.748049519921691136e-02,
    6.394331477523605700e-02,
    5.902209662652455752e-02,
    8.017737936162289136e-02,
    6.037686955223199592e-02,
    1.238295191766777548e-01,
    4.198064573451106007e-02,
    5.733341897840101703e-02,
    5.606687157520456183e-02,
    1.090946200712488573e-01,
    1.560589223561252847e-01,
    4.505157049631237226e-02,
    1.043247656204491641e-01,
    3.926927797235382073e-02,
    7.554023526219320883e-02,
    6.918756420947086427e-02,
    7.253052362204856496e-02,
    1.045523756783557218e-01,
    3.755022612345996352e-02,
    1.284721437460222826e-01,
    1.722825199711542909e-01,
    6.963764781060315101e-02,
    1.636101712316164192e-01,
    6.134203578528477779e-02,
    1.261939216176306278e-01,
    1.399204409497388502e-01,
    5.541937198379740664e-02,
    1.327091485302874241e-01,
    5.869938603847346670e-02,
    8.173368738440789372e-02,
    6.491126722913422020e-02,
    6.370298079017039372e-02,
    1.401262074529683876e-01,
    1.311048190447254569e-01,
    7.053965992830943388e-02,
    7.371239014320606076e-02,
    2.981548951940677858e-02,
    8.842875593673893420e-02,
    7.739933449515264130e-02,
    8.723708160614765394e-02,
    1.343397626360212094e-01,
    5.545650339180159538e-02,
    6.433281527475336048e-02,
    4.551839508869030132e-02,
    9.516626767001926113e-02,
    6.194996527673301417e-02,
    1.020116218121135676e-01,
    1.013698992550285116e-01,
    8.715380709286574268e-02,
    2.853670769429195522e-02,
    1.034527117272665692e-01,
    5.376497778823575296e-02,
    6.019386862085439327e-02,
    5.947584728596055220e-02,
    4.408800277562850006e-02,
    6.104029367332314271e-02,
    1.666349970378434575e-01,
    9.591081102544957160e-02,
    1.452041309891203558e-01,
    7.174935740361629288e-02,
    4.727689310400653294e-02,
    2.747340401874310417e-02,
    9.502162096766773869e-02,
    8.154980999456895585e-02,
    1.287323337968676318e-01,
    5.195795948920130941e-02,
    7.261712584937524140e-02,
    4.972365862001780490e-02,
    9.593202776421667910e-02,
    1.095700733414778638e-01,
    2.586974037132098309e-02,
    6.958637054844457603e-02,
    1.040867540031317345e-01,
    4.188723208405657761e-02,
    1.075785494867010778e-01,
    1.514759894306857146e-01,
    7.365177412459705808e-02,
    5.321330491388395756e-02,
    1.725613444451949696e-01,
    6.917064812123241424e-02,
    2.729067828445507979e-02,
    5.704545521306098044e-02,
    7.364206184409986811e-02,
    8.844723247195693250e-02,
    1.864296993183045259e-01,
    2.649738804564518158e-01,
    6.477345943350412694e-02,
    1.191093846262807898e-01,
    1.094586640201206107e-01,
    2.013730048038095133e-01,
    1.101832938134088696e-01,
    1.338928569301523763e-01,
    8.385280460156768756e-02,
    8.798755441297030633e-02,
    1.473701323756680115e-01,
    9.164087805117636376e-02,
    1.575380434853584088e-01,
    7.176713325171003854e-02,
    1.577554280362821471e-01,
    1.628514979900288207e-01,
    5.484784212707682904e-02,
    8.872966745126159571e-02,
    1.559112587099431568e-01,
    5.769165891880448371e-02,
    1.183933753518580378e-01,
    4.453681068766864731e-02,
    1.657338854957907393e-01,
    5.186924221138593943e-02,
    1.409171941183105792e-01,
    1.912199823691843603e-01,
    4.234129385198760753e-02,
    1.742576206489477864e-01,
    1.069988579973365078e-01,
    5.298862264004426670e-02,
    3.184107029572582487e-02,
    9.563167977845020429e-02,
    3.059180938131595556e-02,
    6.879177160296931492e-02,
    1.079569827738384169e-01,
    9.229012017771795473e-02,
    7.125257723696140633e-02,
    1.908784793957400139e-01,
    9.183327253516034017e-02,
    4.441346900262996278e-02,
    5.614856910220222819e-02,
    1.322762110794488410e-01,
    4.815768693500989406e-02,
    1.150752227209959899e-01,
    6.011304844897804450e-02,
    1.017290002744467570e-01,
    1.030102862915752704e-01,
    6.385936562008939776e-02,
    8.264625668759800758e-02,
    1.155521798427327496e-01,
    1.419705449255535268e-01,
    5.168260674923445125e-02,
    7.449600117266666488e-02,
    1.993531260481652689e-01,
    1.599119336801056446e-01,
    5.397951299958613641e-02,
    4.408527432254161216e-02,
    1.044489952823010187e-01,
    7.873086747318960854e-02,
    6.698059376063640602e-02,
    5.373170696750630831e-02,
    4.419911012581204113e-02,
    2.501883999968850317e-02,
    8.290108737714300957e-02,
    1.210684123818770441e-01,
    7.282603684788595266e-02,
    1.319278697760190300e-01,
    4.950465835762405536e-02,
    9.990900145870745819e-02,
    4.794643417485246195e-02,
    1.447557723901623516e-01,
    6.090339541529111306e-02,
    4.996494735986945629e-02,
    9.443733913556300041e-02,
    1.374060024445062367e-01,
    3.037276023559713073e-02,
    1.051890581995446605e-01,
    1.340158875883787903e-01,
    4.884761589470162230e-02,
    9.472212176926564364e-02,
    9.729936808803206494e-02,
    1.055551207548797998e-01,
    1.634903049313554957e-01,
    3.174077899670346703e-02,
    3.258090810673019283e-02,
    8.161749375947847940e-02,
    1.077930408212003227e-01,
    1.674065636360712594e-01,
    1.173355150777480593e-01,
    4.619375203887864795e-02,
    1.037089283176469562e-01,
    7.213749095708921200e-02,
    9.243244298152634986e-02,
    1.103981741112683157e-01,
    1.777396383038652161e-01,
    4.285307944774999211e-02,
    4.529069279864624681e-02,
    6.420839021971540272e-02,
    1.423816986063418388e-01,
    1.534020298765770840e-01,
    1.351446199298445283e-01,
    4.171407682393583199e-02,
    1.078370105041655119e-01,
    4.292003829915545104e-02,
    5.785686156983561174e-02,
    4.673817235418158827e-02,
    5.859199436912090886e-02,
    3.211171606005556317e-02,
    6.226643312798842839e-02,
    7.839016496601829143e-02,
    6.412142014821527181e-02,
    6.526332881075526393e-02,
    7.678465659719120595e-02,
    5.189972747287720928e-02,
    1.742126964635505038e-01,
    8.392625731654462795e-02,
    1.488036864096571044e-01,
    6.947689778030391450e-02,
    7.222437858999568305e-02,
    4.547277433102504124e-02,
    1.022549101034218172e-01,
    1.176475682832378300e-01,
    1.193596829752066185e-01,
    8.480597479704292396e-02,
    4.960618240189423184e-02,
    1.180889949799951572e-01,
    1.710640797475788122e-01,
    5.439995876357864302e-02,
    1.082327308626702123e-01,
    7.232886505831935542e-02,
    4.561049990395255743e-02,
    1.083937715268189633e-01,
    8.570203612990213993e-02,
    1.401993465523091176e-01,
    8.975353669919444721e-02,
    9.080758661565192791e-02,
    1.366775996526807813e-01,
    1.361238550241007095e-01,
    9.168679521490662176e-02,
    8.842047289496710039e-02,
    5.887093360047179064e-02,
    4.169622872477608128e-02,
    1.057851367500922679e-01,
    9.777119023073779780e-02,
    8.892488177789496795e-02,
    1.004669425053026227e-01,
    1.290888657895645220e-01,
    4.147727471548205008e-02,
    1.796049643642166349e-01,
    2.271453744813870246e-01,
    1.122740274323667520e-01,
    1.072906759675045230e-01,
    1.009427541615983276e-01,
    2.375361670994162733e-01,
    7.536821657192595281e-02,
    3.432173025558694912e-02,
    2.546587235392788404e-02,
    6.240677503655105846e-02,
    7.018214516824367033e-02,
    2.934908565574341488e-02,
    1.119658273901630169e-01,
    1.650262895055940593e-01,
    7.919725544247706306e-02,
    5.852186603827713718e-02,
    6.556164445974006771e-02,
    7.438484186381404129e-02,
    4.936128525346947238e-02,
    8.447690874341336198e-02,
    5.766974688852625347e-02,
    1.309907866873584736e-01,
    2.160868980295180819e-01,
    3.605429436254069253e-02,
    1.699041686865108702e-01,
    6.005155237556838305e-02,
    5.286495782846013791e-02,
    1.598700017709915044e-01,
    7.848814738058679241e-02,
    4.390078604612675933e-02,
    7.007068140622842167e-02,
    5.007875390733716736e-02,
    1.034113989907666648e-01,
    1.133064993800042275e-01,
    1.224472756064470813e-01,
    7.354212705792766147e-02,
    2.333321676828211538e-02,
    4.786369131338505389e-02,
    5.439875185246394185e-02,
    2.382626795788131691e-01,
    7.381232588806686812e-02,
    4.333735746002539968e-02,
    3.711106477003579818e-02,
    2.473280669946548993e-01,
    8.742778005417747855e-02,
    7.377719062254635418e-02,
    8.776179054684309089e-02,
    8.821270144596872476e-02,
    3.130295017819970105e-02,
    5.174458414042383547e-02,
    9.568035241819097814e-02,
    5.556786407289479590e-02,
    6.633588588837310840e-02,
    7.371378663630645534e-02,
    1.077062360769862892e-01,
    4.922066678535522155e-02,
    1.046480117010279848e-01,
    1.888740660937209548e-01,
    1.003451020567222712e-01,
    1.534931392819920970e-01,
    3.320126349048661135e-02,
    4.720423460222832740e-02,
    9.005727947330051386e-02,
    1.368385757976395278e-01,
    3.449946182304990117e-02,
    1.278899685130497621e-01,
    5.360384171297261774e-02,
    7.937992714611066813e-02,
    1.110760133407110301e-01,
    7.823665640824466194e-02,
    5.485257006942016733e-02,
    4.525088710878426290e-02,
    7.408555742999722860e-02,
    6.543263097951637630e-02,
    8.604129652974765174e-02,
    7.815654994511964104e-02,
    4.756829996584546749e-02,
    1.006771065319909120e-01,
    9.474109385742128520e-02,
    5.380877538148645667e-02,
    8.332593768126642098e-02,
    9.098977068823822789e-02,
    6.306787394275202574e-02,
    3.012136441553920907e-01,
    2.055638304637424640e-01,
    2.505699650363610656e-02,
    5.451192670748682828e-02,
    4.967616346450961368e-02,
    4.354568098028697987e-02,
    6.314249872992185042e-02,
    1.222076550370065351e-01,
    1.707130027500330383e-01,
    2.598046078709474460e-01,
    1.127085436646863836e-01,
    1.378973324954072266e-01,
    2.411908104360451222e-01,
    5.894622712824965083e-02,
    1.154354445050028433e-01,
    1.214134961940814689e-01,
    1.075676286095341438e-01,
    3.761292859114087617e-02,
    3.813938406641134010e-02,
    1.649958489855839205e-01,
    9.872508366897289611e-02,
    1.390981338274656365e-01,
    6.724715892021468355e-02,
    6.475219330170067789e-02,
    8.012198459474095813e-02,
    5.615706347093623457e-02,
    9.991696823219739509e-02,
    6.229089760903930295e-02,
    1.386263224558895724e-01,
    8.399426139348023512e-02,
    5.154716952615508857e-02,
    1.003326473073825331e-01,
    8.033081103296033088e-02,
    8.561452283962954490e-02,
    8.643151209932280199e-02,
    7.412717041413918739e-02,
    3.908358211776615659e-02,
    1.159868907449167902e-01,
    4.042668620982063643e-02,
    7.619436432902790268e-02,
    4.127021303187573820e-02,
    4.271247406358165410e-02,
    9.071398286729769056e-02,
    2.285480097606833738e-01,
    6.509333588105431501e-02,
    9.234595975189914541e-02,
    3.437448943391107486e-02,
    9.247076156313584205e-02,
    4.327799326000634356e-02,
    4.706623122389096830e-02,
    3.725927177264499318e-02,
    7.042391031360024922e-02,
    7.473460432353606908e-02,
    5.591328683639653491e-02,
    8.204912287864417342e-02,
    5.701039576249802149e-02,
    5.843647860186494680e-02,
    3.698050274455818437e-02,
    6.720661532068659960e-02,
    9.393113561471772011e-02,
    7.803571481844148727e-02,
    7.039901288838078253e-02,
    1.216269105173996140e-01,
    5.347842408410431736e-02,
    6.602997980843930415e-02,
    1.523975828675155630e-01,
    2.714006619885439009e-01,
    1.012352809066634080e-01,
    4.983799452116592643e-02,
    8.802983757535724763e-02,
    7.468297317095032306e-02,
    1.100834096183821442e-01,
    6.550712219275235670e-02,
    7.614493961100692043e-02,
    3.736047186154664579e-02,
    1.263519048734607908e-01,
    5.847401306644472302e-02,
    5.369012804469311395e-02,
    6.336311438417094322e-02,
    9.401687108211718991e-02,
    1.018534942739547644e-01,
    5.446875542791618924e-02,
    7.038098838815670666e-02,
    1.896744128498799031e-01,
    1.288097558623128580e-01,
    7.826298814434935747e-02,
    8.723970492964071288e-02,
    9.195542850931121004e-02,
    1.530206595059519781e-01,
    1.114023587280297545e-01,
    5.283739595172117637e-02,
    3.451935313782063947e-02,
    4.389130723253351768e-02,
    6.488141708611333547e-02,
    1.147509849155386313e-01,
    9.603115952497394414e-02,
    1.391303759187453304e-01,
    1.073709075302958227e-01,
    5.955014430914176010e-02,
    2.196693548123382633e-01,
    3.763452659413824702e-02,
    6.561518648138427168e-02,
    1.376354781774944414e-01,
    1.307556535549841692e-01,
    2.008636941289183020e-01,
    6.923886491908066987e-02,
    9.039546979388894676e-02,
    7.723907210190117389e-02,
    5.463775812019250333e-02,
    1.948715138191818852e-01,
    5.689846381209059251e-02,
    9.301301031167552580e-02,
    5.490098085449516413e-02,
    9.236908726788774227e-02,
    4.907975230492468699e-02,
    1.537029820767673582e-01,
    3.962589352183169128e-02,
    1.504444241393798343e-01,
    4.828270283287904724e-02,
    7.607020993569028888e-02,
    9.073086215427995926e-02,
    6.271455556299629364e-02,
    1.109010718887851327e-01,
    1.815484655391698954e-01,
    1.049032954919670163e-01,
    8.446101025930241590e-02,
    1.372032030944552450e-01,
    7.555154738930329461e-02,
    6.508111261201168785e-02,
    2.870076469594345578e-02,
    2.962431270389787735e-02,
    1.387684462631836313e-01,
    1.017717820125911138e-01,
    5.754452546957655040e-02,
    1.274069608469830805e-01,
    6.717347210741882180e-02,
    8.504312896807769695e-02,
    8.493184792594085653e-02,
    9.979471616673178669e-02,
    1.345607363689479696e-01,
    4.476865662189315198e-02,
    6.497011476882356973e-02,
    2.611631029074889046e-02,
    1.731047000621892540e-01,
    5.256129442765129084e-02,
    5.458333469158914431e-02,
    3.379481388652112034e-02,
    5.025868446498387571e-02,
    3.471509808952952247e-02,
    5.677357602830181149e-02,
    4.751612888364965120e-02,
    9.906016810678262841e-02,
    4.861373060050026146e-02,
    7.012493050792256710e-02,
    5.274041658006814459e-02,
    1.084909919782155036e-01,
    7.363118287932381156e-02,
    6.176770757993452210e-02,
    3.309901728698108614e-02,
    9.104272042220748484e-02,
    3.883677930009470786e-02,
    5.459818753227788091e-02,
    4.425479325597941882e-02,
    5.582527507237660530e-02,
    1.240530089207348280e-01,
    4.691255750010912134e-02,
    4.742059428856610498e-02,
    1.030006141564313721e-01,
    5.627479658693169268e-02,
    5.096706061496229095e-02,
    8.460464642088035003e-02,
    8.131169763703885445e-02,
    1.130407711807777521e-01,
    6.493979537494172505e-02,
    4.133467931021915509e-02,
    1.096419662445867610e-01,
    7.072885016349481080e-02,
    1.433520904783888439e-01,
    3.426851505698168265e-02,
    1.261038203751647935e-01,
    7.884305097954805164e-02,
    7.634535297160703615e-02,
    6.463433541613929079e-02,
    1.253600812404225828e-01,
    4.343305687736970233e-02,
    1.892295119062054609e-01,
    5.303197658214635346e-02,
    1.387868917697552273e-01,
    1.391628749522504371e-01,
    1.513590390759560755e-01,
    5.492922550834436479e-02,
    2.388524311805535483e-01,
    1.422370457035866398e-01,
    5.404820352692153718e-02,
    7.057002553964297764e-02,
    9.038304345956053865e-02,
    5.210765658898364183e-02,
    9.373509677538922891e-02,
    6.825987279232063887e-02,
    4.052315623777918291e-02,
    8.472155479321366589e-02,
    5.181670980576857932e-02,
    7.753435778081690932e-02,
    9.158926972485752216e-02,
    4.504755580203630844e-02,
    1.618503826747459129e-01,
    3.586458778243735435e-02,
    9.756801725278652082e-02,
    7.703044080594206866e-02,
    6.445626996848151380e-02,
    1.235583631496975782e-01,
    7.253514624041378089e-02,
    1.573919339474467161e-01,
    1.197187592418865465e-01,
    1.265131011819896345e-01,
    1.045041554045446353e-01,
    9.249712501369684869e-02,
    7.551113179530538622e-02,
    1.724042102491733508e-01,
    6.038926686110660663e-02,
    7.298871618486946211e-02,
    5.148609176259647674e-02,
    1.210649052243401441e-01,
    1.155594614157418737e-01,
    7.785070837176334690e-02,
    4.386173000610676248e-02,
    1.465993183912153719e-01,
    3.389610434877590034e-02,
    6.301895088406027357e-02,
    8.173311833133860282e-02,
    7.767975650868791382e-02,
    6.794516234876152305e-02,
    9.088236778641951841e-02,
    7.950033803346630290e-02,
    8.297829196129179186e-02,
    9.541415993333395018e-02,
    1.781776679836399035e-01,
    1.465684269767258963e-01,
    2.464670335900571649e-02,
    8.117975762609949686e-02,
    9.050056874198486589e-02,
    7.001983116927365525e-02,
    4.804348522202737948e-02,
    6.855282043711009854e-02,
    1.033306845440763316e-01,
    5.424021693621456502e-02,
    1.045677494505514032e-01,
    1.051708853193953214e-01,
    3.339362612160129418e-02,
    6.681003778009637906e-02,
    1.546900560423743187e-01,
    5.727741134232377379e-02,
    3.621004146538107926e-02,
    9.411592086763420628e-02,
    1.130950409097363496e-01,
    4.900604283867479594e-02,
    1.644395834548534052e-01,
    5.385814627795382920e-02,
    1.467681008237332885e-01,
    1.537409838052921363e-01,
    1.259796089713434153e-01,
    1.074133408126796280e-01,
    4.261255661987224663e-02,
    9.912843881282557001e-02,
    8.934079960689339517e-02,
    5.248258974498540980e-02,
    1.429478586041593169e-01,
    7.231896863316170099e-02,
    1.000808506120229868e-01,
    1.380365559982608403e-01,
    6.942217794101190886e-02,
    5.615929596647996347e-02,
    4.865671469816525152e-02,
    2.932092507830965852e-02,
    6.650203514261318616e-02,
    3.505785459031858509e-02,
    1.274034260938965712e-01,
    1.037099388701389763e-01,
    6.431937428255998790e-02,
    8.509472519459901352e-02,
    6.466788322939504008e-02,
    5.276575411891776257e-02,
    5.537066829798439893e-02,
    5.301830009704300989e-02,
    8.376962318164057897e-02,
    1.469262322795129860e-01,
    1.139801299707373372e-01,
    1.260111978440141278e-01,
    1.247905473161231155e-01,
]

########NEW FILE########
__FILENAME__ = general_test
# -*- coding: utf-8 -*-
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest
import openquake.hazardlib

from openquake.hazardlib import geo as hazardlib_geo

from openquake.engine import engine
from openquake.engine.calculators.hazard import general
from openquake.engine.utils import get_calculator_class
from openquake.engine.db import models

from openquake.engine.tests.utils import helpers


class StoreSiteModelTestCase(unittest.TestCase):

    def test_store_site_model(self):
        # Setup
        site_model = helpers.get_data_path('site_model.xml')

        exp_site_model = [
            dict(lon=-122.5, lat=37.5, vs30=800.0, vs30_type="measured",
                 z1pt0=100.0, z2pt5=5.0),
            dict(lon=-122.6, lat=37.6, vs30=801.0, vs30_type="measured",
                 z1pt0=101.0, z2pt5=5.1),
            dict(lon=-122.7, lat=37.7, vs30=802.0, vs30_type="measured",
                 z1pt0=102.0, z2pt5=5.2),
            dict(lon=-122.8, lat=37.8, vs30=803.0, vs30_type="measured",
                 z1pt0=103.0, z2pt5=5.3),
            dict(lon=-122.9, lat=37.9, vs30=804.0, vs30_type="measured",
                 z1pt0=104.0, z2pt5=5.4),
        ]

        job = models.OqJob.objects.create(user_name="openquake")
        ids = general.store_site_model(job, site_model)

        actual_site_model = models.SiteModel.objects.filter(
            job=job).order_by('id')

        for i, exp in enumerate(exp_site_model):
            act = actual_site_model[i]

            self.assertAlmostEqual(exp['lon'], act.location.x)
            self.assertAlmostEqual(exp['lat'], act.location.y)
            self.assertAlmostEqual(exp['vs30'], act.vs30)
            self.assertEqual(exp['vs30_type'], act.vs30_type)
            self.assertAlmostEqual(exp['z1pt0'], act.z1pt0)
            self.assertAlmostEqual(exp['z2pt5'], act.z2pt5)

        # last, check that the `store_site_model` function returns all of the
        # newly-inserted records
        for i, s in enumerate(ids):
            self.assertEqual(s, actual_site_model[i].id)


class ClosestSiteModelTestCase(unittest.TestCase):

    def setUp(self):
        self.hc = models.HazardCalculation.objects.create(
            maximum_distance=200,
            calculation_mode="classical",
            inputs={'site_model': ['fake']})
        self.job = models.OqJob.objects.create(
            user_name="openquake", hazard_calculation=self.hc)

    def test_get_closest_site_model_data(self):
        # This test scenario is the following:
        # Site model data nodes arranged 2 degrees apart (longitudinally) along
        # the same parallel (indicated below by 'd' characters).
        #
        # The sites of interest are located at (-0.0000001, 0) and
        # (0.0000001, 0) (from left to right).
        # Sites of interest are indicated by 's' characters.
        #
        # To illustrate, a super high-tech nethack-style diagram:
        #
        # -1.........0.........1   V  oh no, a vampire!
        #  d        s s        d

        sm1 = models.SiteModel(
            job=self.job, vs30_type='measured', vs30=0.0000001,
            z1pt0=0.0000001, z2pt5=0.0000001, location='POINT(-1 0)'
        )
        sm1.save()
        sm2 = models.SiteModel(
            job=self.job, vs30_type='inferred', vs30=0.0000002,
            z1pt0=0.0000002, z2pt5=0.0000002, location='POINT(1 0)'
        )
        sm2.save()

        # NOTE(larsbutler): I tried testing the site (0, 0), but the result
        # actually alternated between the the two site model nodes on each test
        # run. It's very strange indeed. It must be a PostGIS thing.
        # (Or we can blame the vampire.)
        #
        # Thus, I decided to not include this in my test case, since it caused
        # the test to intermittently fail.
        point1 = hazardlib_geo.Point(-0.0000001, 0)
        point2 = hazardlib_geo.Point(0.0000001, 0)

        res1 = self.hc.get_closest_site_model_data(point1)
        res2 = self.hc.get_closest_site_model_data(point2)

        self.assertEqual(sm1, res1)
        self.assertEqual(sm2, res2)


class ParseRiskModelsTestCase(unittest.TestCase):
    def test(self):
        # check that if risk models are provided, then the ``points to
        # compute`` and the imls are got from there

        username = helpers.default_user()

        job = engine.prepare_job(username)

        cfg = helpers.get_data_path('classical_job-sd-imt.ini')
        params = engine.parse_config(open(cfg, 'r'))

        haz_calc = engine.create_calculation(models.HazardCalculation, params)
        haz_calc = models.HazardCalculation.objects.get(id=haz_calc.id)
        job.hazard_calculation = haz_calc
        job.is_running = True
        job.save()

        calc = get_calculator_class(
            'hazard',
            job.hazard_calculation.calculation_mode)(job)
        calc.parse_risk_models()

        self.assertEqual([(1.0, -1.0), (0.0, 0.0)],
                         [(point.latitude, point.longitude)
                          for point in haz_calc.points_to_compute()])
        self.assertEqual(['PGA'], haz_calc.get_imts())

        self.assertEqual(
            3, haz_calc.oqjob.exposuremodel.exposuredata_set.count())

        return job


class InitializeSourcesTestCase(unittest.TestCase):
    # this is a based on a demo with 3 realizations, 2 sources and 2 sites
    @classmethod
    def setUpClass(cls):
        cfg = helpers.get_data_path(
            'calculators/hazard/classical/haz_map_test_job.ini')
        job = helpers.get_job(cfg)
        models.JobStats.objects.create(oq_job=job)
        hc = job.hazard_calculation
        cls.calc = get_calculator_class('hazard', hc.calculation_mode)(job)
        cls.calc.initialize_site_model()
        assert len(hc.site_collection) == 2, len(hc.site_collection)

    def test_few_sites(self):
        # site_collection is smaller than FILTERING_THRESHOLD:
        # prefiltering is enabled and sources are filtered
        m1, m2, m3 = self.calc.initialize_sources()
        self.assertEqual(
            [m1.get_num_sources(), m2.get_num_sources(), m3.get_num_sources()],
            [1, 1, 1])  # 1 source instead of 2

    def test_many_sites(self):
        # site_collection is bigger than FILTERING_THRESHOLD:
        # sources are not prefiltered
        ft = models.FILTERING_THRESHOLD
        try:
            models.FILTERING_THRESHOLD = 0
            m1, m2, m3 = self.calc.initialize_sources()
            self.assertEqual(
                [m1.get_num_sources(), m2.get_num_sources(),
                 m3.get_num_sources()],
                [2, 2, 2])  # the original 2 sources
        finally:
            models.FILTERING_THRESHOLD = ft

########NEW FILE########
__FILENAME__ = base_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import unittest

from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path
from openquake.engine.calculators.risk import base, hazard_getters
from openquake.engine.tests.utils.tasks import fake_risk_task
from openquake.engine.db import models


class BaseRiskCalculatorTestCase(unittest.TestCase):
    """
    An abstract class that just setup a risk job
    """
    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('classical_psha_based_risk/job.ini'),
            get_data_path('simple_fault_demo_hazard/job.ini'))
        models.JobStats.objects.create(oq_job=self.job)
        self.job.is_running = True
        self.job.save()

    @property
    def hazard_calculation(self):
        "A shortcut to a the corresponding hazard calculation"
        return self.job.risk_calculation.get_hazard_calculation()


class FakeWorkflow:
    """Fake Workflow class used in FakeRiskCalculator"""


class FakeRiskCalculator(base.RiskCalculator):
    """
    Fake Risk Calculator. Used to test the base class
    """
    output_builders = []
    getter_class = hazard_getters.GroundMotionValuesGetter
    core_calc_task = fake_risk_task

    @property
    def calculation_parameters(self):
        return base.make_calc_params()

    def agg_result(self, acc, res):
        newacc = dict((key, acc.get(key, 0) + res[key]) for key in res)
        return newacc

    def get_workflow(self, vulnerability_functions):
        FakeWorkflow.vulnerability_functions = vulnerability_functions
        return FakeWorkflow()


class RiskCalculatorTestCase(BaseRiskCalculatorTestCase):
    """
    Integration test for the base class supporting the risk
    calculators.
    """
    def setUp(self):
        super(RiskCalculatorTestCase, self).setUp()
        self.calculator = FakeRiskCalculator(self.job)

    def test(self):
        self.calculator.pre_execute()
        # there are 2 assets and 1 taxonomy; will generate a supertask
        # for the taxonomy and 1 subtask, for the two assets
        self.assertEqual(self.calculator.taxonomies_asset_count, {'VF': 2})

        self.calculator.execute()
        self.assertEqual(self.calculator.acc, {self.job.id: 1})

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from openquake.engine.tests.calculators.risk import base_test

from openquake.engine.calculators.risk.classical import core as classical
from openquake.engine.db import models


class ClassicalRiskCalculatorTestCase(base_test.BaseRiskCalculatorTestCase):
    """
    Integration test for the classical risk calculator
    """
    def setUp(self):
        super(ClassicalRiskCalculatorTestCase, self).setUp()
        self.calculator = classical.ClassicalRiskCalculator(self.job)

    def test_complete_workflow(self):
        # Test the complete risk classical calculation workflow and test
        # for the presence of the outputs
        self.job.is_running = True
        self.job.save()
        self.calculator.pre_execute()
        self.job.status = 'executing'
        self.job.save()
        self.calculator.execute()

        # 1 loss curve + 3 loss maps + 1 mean + 2 quantile
        self.assertEqual(4,
                         models.Output.objects.filter(oq_job=self.job).count())
        self.assertEqual(1,
                         models.LossCurve.objects.filter(
                             output__oq_job=self.job).count())
        self.assertEqual(2,
                         models.LossCurveData.objects.filter(
                             loss_curve__output__oq_job=self.job).count())
        self.assertEqual(3,
                         models.LossMap.objects.filter(
                             output__oq_job=self.job).count())
        self.assertEqual(6,
                         models.LossMapData.objects.filter(
                             loss_map__output__oq_job=self.job).count())

        files = self.calculator.export(exports=['xml'])
        self.assertEqual(4, len(files))

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from openquake.engine.tests.calculators.risk import base_test
from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path

from openquake.engine.db import models
from openquake.engine.calculators.risk.classical_bcr import (
    core as classical_bcr)


class ClassicalBCRRiskCalculatorTestCase(base_test.BaseRiskCalculatorTestCase):
    """
    Integration test for the classical bcr risk calculator
    """

    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('classical_bcr/job.ini'),
            get_data_path('simple_fault_demo_hazard/job.ini'))

        self.calculator = classical_bcr.ClassicalBCRRiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)

    def shortDescription(self):
        """
        Use method names instead of docstrings for verbose output
        """
        return None

    def test_complete_workflow(self):
        # Test the complete risk classical calculation workflow and test
        # for the presence of the outputs
        self.job.is_running = True
        self.job.save()
        self.calculator.pre_execute()

        self.job.status = 'executing'
        self.job.save()
        self.calculator.execute()

        self.assertEqual(1, models.Output.objects.filter(
            oq_job=self.job).count())

        self.assertEqual(1, models.BCRDistribution.objects.filter(
            output__oq_job=self.job).count())

        self.assertEqual(3, models.BCRDistributionData.objects.filter(
            bcr_distribution__output__oq_job=self.job).count())

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path
from openquake.engine.tests.calculators.risk import base_test

from openquake.engine.db import models
from openquake.engine.calculators.risk.event_based import core as event_based


class EventBasedRiskCalculatorTestCase(base_test.BaseRiskCalculatorTestCase):
    """
    Integration test for the event based risk calculator
    """
    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('event_based_risk/job.ini'),
            get_data_path('event_based_hazard/job.ini'), output_type="gmf")

        self.calculator = event_based.EventBasedRiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)
        self.job.is_running = True
        self.job.save()
        self.calculator.pre_execute()
        self.job.status = 'executing'
        self.job.save()

    def test_calculator_parameters(self):
        # Test that the specific calculation parameters are present

        params = self.calculator.calculator_parameters

        self.assertEqual([0.1, 0.2, 0.3], params.conditional_loss_poes)
        self.assertTrue(params.insured_losses)

    def test_complete_workflow(self):
        # Test the complete risk classical calculation workflow and test
        # for the presence of the outputs
        self.calculator.execute()
        self.calculator.post_process()

        # 1 loss curve + 3 loss maps + 1 aggregate curve + 1 insured
        # curve + 1 event loss table
        self.assertEqual(7,
                         models.Output.objects.filter(oq_job=self.job).count())
        self.assertEqual(1,
                         models.LossCurve.objects.filter(
                             output__oq_job=self.job,
                             insured=False,
                             aggregate=False).count())
        self.assertEqual(1,
                         models.LossCurve.objects.filter(
                             output__oq_job=self.job,
                             insured=True,
                             aggregate=False).count())
        self.assertEqual(1,
                         models.LossCurve.objects.filter(
                             output__oq_job=self.job,
                             insured=False,
                             aggregate=True).count())
        # 2 individual loss curves, 2 insured ones
        self.assertEqual(4,
                         models.LossCurveData.objects.filter(
                             loss_curve__output__oq_job=self.job).count())
        self.assertEqual(1,
                         models.AggregateLossCurveData.objects.filter(
                             loss_curve__output__oq_job=self.job).count())
        self.assertEqual(3,
                         models.LossMap.objects.filter(
                             output__oq_job=self.job).count())
        self.assertEqual(6,
                         models.LossMapData.objects.filter(
                             loss_map__output__oq_job=self.job).count())

        files = self.calculator.export(exports=['xml'])
        self.assertEqual(7, len(files))

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from openquake.engine.tests.calculators.risk import base_test
from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path

from openquake.engine.db import models
from openquake.engine.calculators.risk.event_based_bcr import core


class EventBasedBCRRiskCalculatorTestCase(
        base_test.BaseRiskCalculatorTestCase):
    """
    Integration test for the event based bcr risk calculator
    """

    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('event_based_bcr/job.ini'),
            get_data_path('event_based_hazard/job.ini'), output_type="gmf")

        self.calculator = core.EventBasedBCRRiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)

    def shortDescription(self):
        """
        Use method names instead of docstrings for verbose output
        """
        return None

    def test_complete_workflow(self):
        # Test the complete risk classical calculation workflow and test
        # for the presence of the outputs
        self.job.is_running = True
        self.job.save()
        self.calculator.pre_execute()

        self.job.status = 'executing'
        self.job.save()
        self.calculator.execute()

        self.assertEqual(1, models.Output.objects.filter(
            oq_job=self.job).count())

        self.assertEqual(1, models.BCRDistribution.objects.filter(
            output__oq_job=self.job).count())

        self.assertEqual(3, models.BCRDistributionData.objects.filter(
            bcr_distribution__output__oq_job=self.job).count())

########NEW FILE########
__FILENAME__ = hazard_getters_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import numpy
from openquake.engine.tests.utils import helpers
import unittest
import cPickle as pickle

from openquake.engine.db import models
from openquake.engine.calculators.risk import hazard_getters
from openquake.engine.calculators.risk.base import RiskCalculator

from openquake.engine.tests.utils.helpers import get_data_path


class HazardCurveGetterTestCase(unittest.TestCase):

    hazard_demo = get_data_path('simple_fault_demo_hazard/job.ini')
    risk_demo = get_data_path('classical_psha_based_risk/job.ini')
    hazard_output_type = 'curve'
    getter_class = hazard_getters.HazardCurveGetter
    taxonomy = 'VF'
    imt = 'PGA'

    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            self.risk_demo, self.hazard_demo, self.hazard_output_type)

        # need to run pre-execute to parse exposure model
        calc = RiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)
        self.job.is_running = True
        self.job.save()
        calc.pre_execute()

        self.builder = hazard_getters.GetterBuilder(
            self.taxonomy, self.job.risk_calculation)

        self.assets = models.ExposureData.objects.filter(
            exposure_model=self.job.risk_calculation.exposure_model).order_by(
            'asset_ref').filter(taxonomy=self.taxonomy)

        ho = self.job.risk_calculation.hazard_output
        self.nbytes = self.builder.calc_nbytes([ho])
        [self.getter] = self.builder.make_getters(
            self.getter_class, [ho], self.assets)

    def test_nbytes(self):
        self.assertEqual(self.nbytes, 0)

    def test_is_pickleable(self):
        pickle.dumps(self.getter)  # raises an error if not

    def test_call(self):
        # the exposure model in this example has three assets of taxonomy VF
        # called a1, a2 and a3; only a2 and a3 are within the maximum distance
        [a1, a2, a3] = self.assets
        self.assertEqual(self.getter.assets, [a2, a3])

        values = self.getter.get_data(self.imt)
        numpy.testing.assert_allclose(
            [[(0.1, 0.1), (0.2, 0.2), (0.3, 0.3)],
             [(0.1, 0.1), (0.2, 0.2), (0.3, 0.3)]], values)


class GroundMotionValuesGetterTestCase(HazardCurveGetterTestCase):

    hazard_demo = get_data_path('event_based_hazard/job.ini')
    risk_demo = get_data_path('event_based_risk/job.ini')
    hazard_output_type = 'gmf'
    getter_class = hazard_getters.GroundMotionValuesGetter
    taxonomy = 'RM'

    def test_nbytes(self):
        # 1 asset * 3 ruptures
        self.assertEqual(self.builder.epsilons_shape.values(), [(1, 3)])
        self.assertEqual(self.nbytes, 24)

    def test_call(self):
        # the exposure model in this example has two assets of taxonomy RM
        # (a1 and a3); the asset a3 has no hazard data within the
        # maximum distance; there is one realization and three ruptures
        a1, a3 = self.assets
        self.assertEqual(self.getter.assets, [a1])
        rupture_ids = self.getter.rupture_ids
        self.assertEqual(len(rupture_ids), 3)
        [gmvs] = self.getter.get_data(self.imt)
        numpy.testing.assert_allclose([0.1, 0.2, 0.3], gmvs)


class ScenarioGetterTestCase(GroundMotionValuesGetterTestCase):

    hazard_demo = get_data_path('scenario_hazard/job.ini')
    risk_demo = get_data_path('scenario_risk/job.ini')
    hazard_output_type = 'gmf_scenario'
    getter_class = hazard_getters.ScenarioGetter
    taxonomy = 'RM'

    def test_nbytes(self):
        # 10 realizations * 1 asset
        self.assertEqual(self.getter.num_samples, 10)
        self.assertEqual(self.nbytes, 80)

    def test_call(self):
        # the exposure model in this example has two assets of taxonomy RM
        # (a1 and a3) but the asset a3 has no hazard data within the
        # maximum distance; there are 10 realizations
        a1, a3 = self.assets
        self.assertEqual(self.getter.assets, [a1])

        [gmvs] = self.getter.get_data(self.imt)
        numpy.testing.assert_allclose([0.1, 0.2, 0.3], gmvs)

########NEW FILE########
__FILENAME__ = loaders_test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest

import StringIO

from openquake.engine.calculators.risk import loaders


class ParseVulnerabilityModelTestCase(unittest.TestCase):

    def test_one_taxonomy_many_imts(self):
        # Should raise a ValueError if a vulnerabilityFunctionID is used for
        # multiple IMTs.
        # In this test input, we've defined two functions in separate sets
        # with the same ID and different IMTs.
        vuln_content = StringIO.StringIO("""\
<?xml version='1.0' encoding='utf-8'?>
<nrml xmlns="http://openquake.org/xmlns/nrml/0.4"
      xmlns:gml="http://www.opengis.net/gml">
    <vulnerabilityModel>
        <discreteVulnerabilitySet vulnerabilitySetID="PAGER"
                                  assetCategory="population"
                                  lossCategory="fatalities">
            <IML IMT="PGA">0.005 0.007 0.0098 0.0137</IML>
            <discreteVulnerability vulnerabilityFunctionID="A"
                                   probabilisticDistribution="LN">
                <lossRatio>0.01 0.06 0.18 0.36</lossRatio>
                <coefficientsVariation>0.30 0.30 0.30 0.30</coefficientsVariation>
            </discreteVulnerability>
        </discreteVulnerabilitySet>
        <discreteVulnerabilitySet vulnerabilitySetID="PAGER"
                                  assetCategory="population"
                                  lossCategory="fatalities">
            <IML IMT="MMI">0.005 0.007 0.0098 0.0137</IML>
            <discreteVulnerability vulnerabilityFunctionID="A"
                                   probabilisticDistribution="LN">
                <lossRatio>0.01 0.06 0.18 0.36</lossRatio>
                <coefficientsVariation>0.30 0.30 0.30 0.30</coefficientsVariation>
            </discreteVulnerability>
        </discreteVulnerabilitySet>
    </vulnerabilityModel>
</nrml>
""")
        with self.assertRaises(ValueError) as ar:
            loaders.vulnerability(vuln_content)
        expected_error = ('Error creating vulnerability function for taxonomy '
                          'A. A taxonomy can not be associated with different '
                          'vulnerability functions')
        self.assertEqual(expected_error, ar.exception.message)

    def test_lr_eq_0_cov_gt_0(self):
        # If a vulnerability function loss ratio is 0 and its corresponding CoV
        # is > 0, a ValueError should be raised
        vuln_content = StringIO.StringIO("""\
<?xml version='1.0' encoding='utf-8'?>
<nrml xmlns="http://openquake.org/xmlns/nrml/0.4"
      xmlns:gml="http://www.opengis.net/gml">
    <vulnerabilityModel>
        <discreteVulnerabilitySet vulnerabilitySetID="PAGER"
                                  assetCategory="population"
                                  lossCategory="fatalities">
            <IML IMT="PGV">0.005 0.007 0.0098 0.0137</IML>
            <discreteVulnerability vulnerabilityFunctionID="A"
                                   probabilisticDistribution="LN">
                <lossRatio>0.00 0.06 0.18 0.36</lossRatio>
                <coefficientsVariation>0.30 0.30 0.30 0.30</coefficientsVariation>
            </discreteVulnerability>
        </discreteVulnerabilitySet>
    </vulnerabilityModel>
</nrml>
""")
        with self.assertRaises(ValueError) as ar:
            loaders.vulnerability(vuln_content)
        expected_error = ("Invalid vulnerability function with ID 'A': It is "
                          "not valid to define a loss ratio = 0.0 with a "
                          "corresponding coeff. of varation > 0.0")
        self.assertEqual(expected_error, ar.exception.message)

########NEW FILE########
__FILENAME__ = core_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path
from openquake.engine.tests.calculators.risk import base_test

from openquake.engine.db import models
from openquake.engine.calculators.risk.scenario import core as scenario


class ScenarioRiskCalculatorTestCase(base_test.BaseRiskCalculatorTestCase):
    """
    Integration test for the scenario risk calculator
    """

    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('scenario_risk/job.ini'),
            get_data_path('scenario_hazard/job.ini'),
            output_type="gmf_scenario")

        self.calculator = scenario.ScenarioRiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)
        self.job.is_running = True
        self.job.save()
        self.calculator.pre_execute()
        self.job.status = 'executing'
        self.job.save()

    def test_complete_workflow(self):
        """
        Test the complete risk scenario calculation workflow and test
        for the presence of the outputs
        """
        self.calculator.execute()
        self.calculator.post_process()

        self.assertEqual(
            2, models.Output.objects.filter(oq_job=self.job).count())

        # One Loss map
        self.assertEqual(1, models.LossMap.objects.filter(
                         output__oq_job=self.job).count())

        # One Aggregate Loss
        self.assertEqual(1, models.AggregateLoss.objects.filter(
                         output__oq_job=self.job).count())

        files = self.calculator.export(exports=['xml'])
        self.assertEqual(2, len(files))

########NEW FILE########
__FILENAME__ = core_test
from cStringIO import StringIO
import unittest
import mock

from openquake.nrmllib.risk import parsers
from openquake.engine.calculators.risk.scenario_damage.core import \
    ScenarioDamageRiskCalculator

FRAGILITY_FILE = StringIO('''<?xml version='1.0' encoding='utf-8'?>
<nrml xmlns="http://openquake.org/xmlns/nrml/0.4">

    <fragilityModel format="continuous">
        <description>Fragility model for Pavia (continuous)</description>
        <!-- limit states apply to the entire fragility model -->
        <limitStates>
            slight
            moderate
            extensive
            complete
        </limitStates>

        <!-- fragility function set, each with its own, distinct taxonomy -->
        <ffs noDamageLimit="0.05" type="lognormal">
            <taxonomy>RC/DMRF-D/LR</taxonomy>
            <IML IMT="PGA" minIML="0.1" maxIML="9.9" imlUnit="m"/>

            <!-- fragility function in continuous format, 1 per limit state -->
            <ffc ls="slight">
                <params mean="11.19" stddev="8.27" />
            </ffc>

            <ffc ls="moderate">
                <params mean="27.98" stddev="20.677" />
            </ffc>

            <ffc ls="extensive">
                <params mean="48.05" stddev="42.49" />
            </ffc>

            <ffc ls="complete">
                <params mean="108.9" stddev="123.7" />
            </ffc>
        </ffs>
 </fragilityModel>
</nrml>
''')

FMParser = parsers.FragilityModelParser


class FakeJob:
    class rc:
        inputs = dict(fragility=mock.Mock())
    risk_calculation = rc


class ScenarioDamageCalculatorTestCase(unittest.TestCase):
    """
    Test the ``get_risk_models method`` of the calculator, i.e.
    the parsing of the inputs and the setting of the taxonomies,
    depending on the parameter ``taxonomies_from_fragility``.
    """

    def setUp(self):
        self.calculator = ScenarioDamageRiskCalculator(FakeJob())
        self.calculator.taxonomies = {'RC/DMRF-D/LR': 2, 'RC': 1}

    def test_taxonomies_from_fragility_true(self):
        self.calculator.job.rc.taxonomies_from_model = True
        with mock.patch('openquake.nrmllib.risk.parsers.FragilityModelParser',
                        lambda p: FMParser(FRAGILITY_FILE)):
            self.calculator.get_risk_models()
        self.assertEqual(sorted(self.calculator.risk_models), ['RC/DMRF-D/LR'])

    def test_taxonomies_from_fragility_false(self):
        self.calculator.job.rc.taxonomies_from_model = False
        with mock.patch('openquake.nrmllib.risk.parsers.FragilityModelParser',
                        lambda p: FMParser(FRAGILITY_FILE)):
            with self.assertRaises(RuntimeError) as cm:
                self.calculator.get_risk_models()
        self.assertEqual(str(cm.exception),
                         'The following taxonomies are in the exposure '
                         "model but not in the risk model: ['RC']")

########NEW FILE########
__FILENAME__ = validation_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest
import mock

from openquake.engine.calculators.risk import validation
from openquake.engine.db import models

from openquake.risklib.workflows import RiskModel


class HazardIMTTestCase(unittest.TestCase):
    def test_get_error(self):
        vf1 = mock.Mock()
        vf1.imt = 'PGA'
        vf2 = mock.Mock()
        vf2.imt = 'PGV'
        calc = mock.Mock()
        workflow = mock.Mock()
        workflow.vulnerability_functions = dict(
            structural=vf1, nonstructural=vf2)
        calc.risk_models = {
            'tax1': RiskModel('tax1', workflow),
            'tax2': RiskModel('tax2', workflow)}
        calc.hc.get_imts = mock.Mock(return_value=['PGA', 'PGV'])
        val = validation.HazardIMT(calc)

        self.assertIsNone(val.get_error())
        calc.hc.get_imts = mock.Mock(return_value=['PGA'])
        self.assertEqual(("There is no hazard output for: PGV. "
                          "The available IMTs are: PGA."), val.get_error())


class EmptyExposureTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()
        calc.taxonomies_asset_count = {}
        val = validation.EmptyExposure(calc)

        self.assertEqual(('Region of interest is not covered by '
                          'the exposure input. '
                          'This configuration is invalid. '
                          'Change the region constraint input or use a proper '
                          'exposure'), val.get_error())

        calc.taxonomies_asset_count = {'RM': 1, 'RC': 2}
        self.assertIsNone(val.get_error())


class OrphanTaxonomiesTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()
        val = validation.OrphanTaxonomies(calc)

        calc.rc.taxonomies_from_model = True
        calc.risk_models = {'RM': mock.Mock()}
        calc.taxonomies_asset_count = {'RC': 1, 'RM': 2}

        self.assertIsNone(val.get_error())

        calc.rc.taxonomies_from_model = False
        self.assertEqual("The following taxonomies are in the exposure model "
                         "but not in the risk model: set(['RC'])",
                         val.get_error())

        calc.risk_models = {'RM': mock.Mock(), 'RC': mock.Mock()}
        self.assertIsNone(val.get_error())


class ExposureLossTypesTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()
        val = validation.ExposureLossTypes(calc)

        calc.loss_types = models.LOSS_TYPES
        calc.risk_models = {'RM': mock.Mock()}

        calc.rc.exposure_model.supports_loss_type = mock.Mock(
            return_value=False)

        self.assertEqual(("Invalid exposure "
                          "for computing loss type structural. "),
                         val.get_error())

        calc.rc.exposure_model.supports_loss_type = mock.Mock(
            return_value=True)
        self.assertIsNone(val.get_error())


class NoRiskModelsTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()
        calc.risk_models = None

        val = validation.NoRiskModels(calc)
        self.assertEqual(
            'At least one risk model of type %s must be defined' % (
                models.LOSS_TYPES), val.get_error())

        calc.risk_models = {'RM': mock.Mock()}
        self.assertIsNone(val.get_error())


class RequireClassicalHazardTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()

        val = validation.RequireClassicalHazard(calc)

        calc.rc.hazard_calculation.calculation_mode = 'classical'
        self.assertIsNone(val.get_error())

        calc.rc.hazard_calculation.calculation_mode = 'event_based'
        self.assertEqual(("The provided hazard calculation ID "
                          "is not a classical calculation"), val.get_error())

        calc.rc.hazard_calculation = None
        calc.rc.hazard_output.is_hazard_curve = mock.Mock(return_value=True)
        self.assertIsNone(val.get_error())

        calc.rc.hazard_output.is_hazard_curve = mock.Mock(return_value=False)
        self.assertEqual("The provided hazard output is not an hazard curve",
                         val.get_error())


class RequireScenarioHazardTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()

        val = validation.RequireScenarioHazard(calc)

        calc.rc.hazard_calculation.calculation_mode = 'scenario'
        self.assertIsNone(val.get_error())

        calc.rc.hazard_calculation.calculation_mode = 'event_based'
        self.assertEqual(("The provided hazard calculation ID "
                          "is not a scenario calculation"), val.get_error())

        calc.rc.hazard_calculation = None
        calc.rc.hazard_output.output_type = "gmf_scenario"
        self.assertIsNone(val.get_error())

        calc.rc.hazard_output.output_type = "gmf"
        self.assertEqual(("The provided hazard is not a "
                          "gmf scenario collection"),
                         val.get_error())


class RequireEventBasedHazardTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()

        output = mock.Mock()
        output.output_type = "gmf"
        calc.rc.hazard_outputs.return_value = [output]

        val = validation.RequireEventBasedHazard(calc)

        calc.rc.hazard_calculation.calculation_mode = 'event_based'
        self.assertIsNone(val.get_error())

        output.output_type = "ses"
        calc.rc.inputs = {}
        self.assertIsNotNone(val.get_error())

        calc.rc.inputs = {'source_model_logic_tree': None}
        self.assertIsNone(val.get_error())

        calc.rc.hazard_calculation.calculation_mode = 'classical'
        self.assertEqual(("The provided hazard calculation ID "
                          "is not a event based calculation"), val.get_error())

        calc.rc.hazard_calculation = None
        calc.rc.hazard_output.output_type = "gmf"
        self.assertIsNone(val.get_error())
        calc.rc.hazard_output.output_type = "ses"
        self.assertIsNone(val.get_error())

        calc.rc.hazard_output.output_type = "gmf_scenario"
        self.assertEqual(("The provided hazard is not a "
                          "gmf or ses collection"),
                         val.get_error())


class ExposureHasInsuranceBoundsTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()

        val = validation.ExposureHasInsuranceBounds(calc)

        calc.rc.insured_losses = True
        calc.rc.exposure_model.has_insurance_bounds = mock.Mock(
            return_value=True)
        self.assertIsNone(val.get_error())

        calc.rc.insured_losses = True
        calc.rc.exposure_model.has_insurance_bounds = mock.Mock(
            return_value=False)
        self.assertEqual("Deductible or insured limit missing in exposure",
                         val.get_error())

        calc.rc.insured_losses = False
        calc.rc.exposure_model.has_insurance_bounds = mock.Mock(
            return_value=True)
        self.assertIsNone(val.get_error())

        calc.rc.insured_losses = False
        calc.rc.exposure_model.has_insurance_bounds = mock.Mock(
            return_value=False)
        self.assertIsNone(val.get_error())


class ExposureHasRetrofittedCostsTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()

        val = validation.ExposureHasRetrofittedCosts(calc)

        calc.rc.exposure_model.has_retrofitted_costs = mock.Mock(
            return_value=True)
        self.assertIsNone(val.get_error())

        calc.rc.exposure_model.has_retrofitted_costs = mock.Mock(
            return_value=False)
        self.assertEqual("Some assets do not have retrofitted costs",
                         val.get_error())


class ExposureHasTimeEventTestCase(unittest.TestCase):
    def test_get_error(self):
        calc = mock.Mock()
        val = validation.ExposureHasTimeEvent(calc)

        calc.rc.time_event = "night"

        calc.rc.exposure_model.has_time_event = mock.Mock(
            return_value=True)
        self.assertIsNone(val.get_error())

        calc.rc.exposure_model.has_time_event = mock.Mock(
            return_value=False)
        self.assertEqual("Some assets are missing an "
                         "occupancy with period=night", val.get_error())

########NEW FILE########
__FILENAME__ = celery_node_monitor_test
import os
import time
import mock
import signal
import unittest
from openquake.engine.celery_node_monitor import CeleryNodeMonitor


class CeleryNodeMonitorTestCase(unittest.TestCase):
    def setUp(self):
        self.patch = mock.patch('celery.task.control.inspect')
        self.inspect = self.patch.start()

    def test_all_nodes_were_down(self):
        ping = self.inspect().ping
        ping.return_value = {}
        mon = CeleryNodeMonitor(no_distribute=False, interval=1)
        with self.assertRaises(SystemExit), mock.patch('sys.stderr') as stderr:
            mon.__enter__()
        self.assertEqual(ping.call_count, 1)  # called only once
        self.assertTrue(stderr.write.called)  # an error message was printed

    def test_all_nodes_are_up(self):
        ping = self.inspect().ping
        ping.return_value = {'node1': []}
        mon = CeleryNodeMonitor(no_distribute=False, interval=1)
        with mon:
            time.sleep(1.1)
        # one ping was done in the thread, plus one at the beginning
        self.assertEqual(ping.call_count, 2)

    def test_one_node_went_down(self):
        ping = self.inspect().ping
        ping.return_value = {'node1': []}
        mon = CeleryNodeMonitor(no_distribute=False, interval=1)
        with mon, mock.patch('os.kill') as kill, \
                mock.patch('openquake.engine.logs.LOG') as log:
            time.sleep(1.1)
            ping.return_value = {}
            time.sleep(1)
            # two pings was done in the thread, plus 1 at the beginning
            self.assertEqual(ping.call_count, 3)

            # check that kill was called with a SIGABRT
            pid, signum = kill.call_args[0]
            self.assertEqual(pid, os.getpid())
            self.assertEqual(signum, signal.SIGABRT)
            self.assertTrue(log.critical.called)

    def test_no_distribute(self):
        with CeleryNodeMonitor(no_distribute=True, interval=0.1):
            time.sleep(0.5)
        self.assertIsNone(self.inspect.call_args)

    def tearDown(self):
        self.patch.stop()

########NEW FILE########
__FILENAME__ = fields_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import numpy
import pickle
import unittest

from django import forms

from openquake.engine.db import fields


class FloatArrayFormFieldTestCase(unittest.TestCase):

    def setUp(self):
        self.form_field = fields.FloatArrayFormField()

    def test_clean(self):
        # a general succesful case with some mixed input which can be cast to
        # floats
        value = [0.0, 1, -17L, '5.1']

        expected = [0.0, 1.0, -17.0, 5.1]

        self.assertEqual(expected, self.form_field.clean(value))

    def test_clean_empty_list(self):
        self.assertEqual([], self.form_field.clean([]))

    def test_clean_no_list_tuple_or_string(self):
        value = object()

        self.assertRaises(forms.ValidationError, self.form_field.clean, value)

    def test_clean_list_of_non_floats(self):
        value = ['a', 5]

        self.assertRaises(forms.ValidationError, self.form_field.clean, value)

    def test_clean_str_list(self):
        value = '1.1 -5.78, 0 ,  7'

        expected = [1.1, -5.78, 0.0, 7.0]

        self.assertEqual(expected, self.form_field.clean(value))

    def test_clean_str_list_invalid(self):
        value = 'a 5'

        self.assertRaises(forms.ValidationError, self.form_field.clean, value)


class FloatArrayFieldTestCase(unittest.TestCase):
    """Test for the custom
       :py:class:`openquake.engine.db.models.FloatArrayField` type"""

    def test_get_prep_value(self):
        """Test the proper behavior of
        :py:method:`openquake.engine.db.models.FloatArrayField.get_prep_value`.
        """
        expected = '{3.14, 10, -0.111}'

        faf = fields.FloatArrayField()
        actual = faf.get_prep_value([3.14, 10, -0.111])

        self.assertEqual(expected, actual)


class CharArrayFieldTestCase(unittest.TestCase):
    """Tests for the custom
       :py:class:`openquake.engine.db.models.CharArrayField` type"""

    def test_get_prep_value(self):
        """Test the proper behavior of
        :py:method:`openquake.engine.db.models.CharArrayField.get_prep_value`.
        """
        expected = '{"MagPMF", "MagDistPMF", "LatLonPMF"}'

        caf = fields.CharArrayField()
        actual = caf.get_prep_value(['MagPMF', 'MagDistPMF', 'LatLonPMF'])

        self.assertEqual(expected, actual)


class PickleFieldTestCase(unittest.TestCase):
    def test_to_python(self):
        field = fields.PickleField()
        data = {'foo': None, (1, False): 'baz'}
        self.assertEqual(field.to_python(buffer(pickle.dumps(data))), data)
        self.assertIs(data, data)
        empty_buffer = buffer('')
        self.assertIs(empty_buffer, empty_buffer)

    def test_get_prep_value(self):
        field = fields.PickleField()
        data = {'foo': None, (1, False): 'baz'}
        self.assertEqual(pickle.loads(field.get_prep_value(data)), data)


class NumpyListFieldTestCase(unittest.TestCase):

    def setUp(self):
        self.field = fields.NumpyListField()

    def test_to_python(self):
        value = pickle.dumps(
            [[1, 2, 3], [4, 5, 6]], protocol=pickle.HIGHEST_PROTOCOL
        )
        pvalue = self.field.to_python(value)

        self.assertTrue(isinstance(pvalue, numpy.ndarray))
        numpy.testing.assert_array_equal(
            pvalue, numpy.array([[1, 2, 3], [4, 5, 6]])
        )

    def test_to_python_raises(self):
        # A ValueError is raised when value to be converted is not a list or
        # tuple.
        value = pickle.dumps(
            'not a list or tuple', protocol=pickle.HIGHEST_PROTOCOL
        )
        self.assertRaises(ValueError, self.field.to_python, value)

    def test_to_python_none(self):
        self.assertIsNone(self.field.to_python(None))

    def test_get_prep_value(self):
        value = numpy.array([[0.1, 0.2, 0.3], [6.2, 6.3, 6.7]])
        expected = bytearray(
            pickle.dumps(value.tolist(), protocol=pickle.HIGHEST_PROTOCOL)
        )

        actual = self.field.get_prep_value(value)

        self.assertEqual(pickle.loads(expected), pickle.loads(actual))


class OqNullBooleanFieldTestCase(unittest.TestCase):

    def test_to_python(self):
        field = fields.OqNullBooleanField()

        true_cases = ['T', 't', 'TRUE', 'True', 'true', 'Y', 'y', 'YES', 'yes']
        false_cases = ['F', 'f', 'FALSE', 'False', 'false', 'N', 'n', 'NO',
                       'no']

        for tc in true_cases:
            self.assertEqual(True, field.to_python(tc))

        for fc in false_cases:
            self.assertEqual(False, field.to_python(fc))


class NullFloatFieldTestCase(unittest.TestCase):

    def test_get_prep_value_empty_str(self):
        field = fields.NullFloatField()
        self.assertIsNone(field.get_prep_value(''))
        self.assertIsNone(field.get_prep_value('  '))
        self.assertIsNone(field.get_prep_value('\t'))


class NullCharFieldTestCase(unittest.TestCase):
    def test_to_python(self):
        field = fields.NullTextField().formfield()

        self.assertEqual('', field.clean(''))
        self.assertEqual('foo', field.clean('foo'))
        self.assertIsNone(field.clean(None))

########NEW FILE########
__FILENAME__ = models_manager_test
# -*- coding: utf-8 -*-
# unittest.TestCase base class does not honor the following coding
# convention
# pylint: disable=C0103,R0904
# pylint: enable=W0511,W0142,I0011,E1101,E0611,F0401,E1103,R0801,W0232

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Test Django custom model managers
"""

import mock
import random
import unittest
from collections import namedtuple

from django.contrib.gis.geos.point import Point
from django.contrib.gis.geos.polygon import Polygon

from openquake.engine.db import models
from openquake.engine.calculators.risk import base

from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path


class TestCaseWithAJob(unittest.TestCase):
    """
    Abstract test case class to just setup a job
    """
    def setUp(self):
        cfg = helpers.get_data_path('simple_fault_demo_hazard/job.ini')
        self.job = helpers.get_job(cfg, username="test_user")
        for i in range(0, random.randint(1, 10)):
            lt_model = models.LtSourceModel.objects.create(
                hazard_calculation=self.job.hazard_calculation,
                ordinal=i, sm_lt_path=[i])
            models.LtRealization(
                lt_model=lt_model, ordinal=i,
                weight=1 / (i + 1), gsim_lt_path=[i]).save()


class OutputManagerTestCase(TestCaseWithAJob):
    """
    Test the OutputManager
    """

    def setUp(self):
        super(OutputManagerTestCase, self).setUp()
        self.manager = models.Output.objects

    def test_create_output(self):
        output = self.manager.create_output(
            self.job, "fake output", "hazard_curve")
        self.assertEqual(1, self.manager.filter(pk=output.id).count())

        output = self.manager.create_output(
            self.job, "another fake output", "hazard_map")
        self.assertEqual(1, self.manager.filter(pk=output.id).count())


class ExposureContainedInTestCase(unittest.TestCase):
    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('classical_psha_based_risk/job.ini'),
            get_data_path('simple_fault_demo_hazard/job.ini'))
        calculator = base.RiskCalculator(self.job)
        models.JobStats.objects.create(oq_job=self.job)
        calculator.pre_execute()
        self.rc = self.job.risk_calculation

        common_fake_args = dict(
            exposure_model=self.rc.exposure_model, taxonomy="test")

        asset = models.ExposureData(site=Point(0.5, 0.5),
                                    asset_ref="test1",
                                    **common_fake_args)
        asset.save()

        asset = models.ExposureData(site=Point(179.1, 0),
                                    asset_ref="test2",
                                    **common_fake_args)
        asset.save()

    def test_simple_inclusion(self):
        self.rc.region_constraint = Polygon(
            ((0, 0), (0, 1), (1, 1), (1, 0), (0, 0)))

        results = models.ExposureData.objects.get_asset_chunk(
            self.rc, "test", 0, 10)

        self.assertEqual(1, len(list(results)))
        self.assertEqual("test1", results[0].asset_ref)

    def test_inclusion_of_a_pole(self):
        self.rc.region_constraint = Polygon(
            ((-1, 0), (-1, 1), (1, 1), (1, 0), (-1, 0)))

        results = models.ExposureData.objects.get_asset_chunk(
            self.rc, "test", 0, 10)

        self.assertEqual(1, len(results))
        self.assertEqual("test1", results[0].asset_ref)

        self.rc.region_constraint = Polygon(
            ((179, 10), (-179, 10), (-179, -10), (179, -10), (179, 10)))

        results = models.ExposureData.objects.get_asset_chunk(
            self.rc, "test",  0, 10)

        self.assertEqual(1, len(list(results)))
        self.assertEqual("test2", results[0].asset_ref)


class AssetManagerTestCase(unittest.TestCase):
    base = 'openquake.engine.db.models.AssetManager.'

    def setUp(self):
        self.manager = models.ExposureData.objects

    def test_get_asset_chunk_query_args(self):
        rc = mock.Mock()
        rc.exposure_model.id = 0
        rc.region_constraint.wkt = "REGION CONSTRAINT"

        p1 = mock.patch(self.base + '_get_people_query_helper')
        m1 = p1.start()
        m1.return_value = ("occupants_fields", "occupants_cond",
                           "occupancy_join", ("occ_arg1", "occ_arg2"))
        p2 = mock.patch(self.base + '_get_cost_types_query_helper')
        m2 = p2.start()
        m2.return_value = ("cost_type_fields", "cost_type_joins")

        try:
            query, args = self.manager._get_asset_chunk_query_args(
                rc, "taxonomy", 0, 1)
            self.assertEqual("""
            SELECT riski.exposure_data.*,
                   occupants_fields AS people,
                   cost_type_fields
            FROM riski.exposure_data
            occupancy_join
            ON riski.exposure_data.id = riski.occupancy.exposure_data_id
            cost_type_joins
            WHERE exposure_model_id = %s AND
                  taxonomy = %s AND
                  ST_COVERS(ST_GeographyFromText(%s), site) AND
                  occupants_cond
            GROUP BY riski.exposure_data.id
            ORDER BY ST_X(geometry(site)), ST_Y(geometry(site))
            LIMIT %s OFFSET %s
            """, query)

            self.assertEqual(args,
                             (0, 'taxonomy',
                              'SRID=4326; REGION CONSTRAINT',
                              'occ_arg1', 'occ_arg2', 1, 0))
        finally:
            p1.stop()
            p2.stop()

    def test_get_people_query_helper_population_no_event(self):
        field, cond, join, args = self.manager._get_people_query_helper(
            "population", None)
        self.assertEqual("number_of_units", field)
        self.assertEqual("1 = 1", cond)
        self.assertEqual("", join)
        self.assertEqual(args, ())

    def test_get_people_query_helper_population_time_event(self):
        field, cond, join, args = self.manager._get_people_query_helper(
            "population", "day")
        self.assertEqual("number_of_units", field)
        self.assertEqual("1 = 1", cond)
        self.assertEqual("", join)
        self.assertEqual(args, ())

    def test_get_people_query_helper_buildings_no_event(self):
        field, cond, join, args = self.manager._get_people_query_helper(
            "buildings", None)
        self.assertEqual("AVG(riski.occupancy.occupants)", field)
        self.assertEqual("1 = 1", cond)
        self.assertEqual("LEFT JOIN riski.occupancy", join)
        self.assertEqual(args, ())

    def test_get_people_query_helper_buildings_time_event(self):
        field, cond, join, args = self.manager._get_people_query_helper(
            "buildings", "day")
        self.assertEqual("AVG(riski.occupancy.occupants)", field)
        self.assertEqual("riski.occupancy.period = %s", cond)
        self.assertEqual("LEFT JOIN riski.occupancy", join)
        self.assertEqual(args, ("day",))

    def test_get_cost_types_query_helper_no_cost_types(self):
        fields, joins = self.manager._get_cost_types_query_helper([])

        self.assertEqual("", fields)
        self.assertEqual("", joins)

    def test_get_cost_types_query_helper_one_cost_type(self):
        cost = namedtuple("cost", "name id")
        fields, joins = self.manager._get_cost_types_query_helper(
            [cost("structural", 1)])

        self.assertEqual(
            "max(structural.converted_cost) AS structural, "
            "max(structural.converted_retrofitted_cost) AS retrofitted_structural, "
            "max(structural.deductible_absolute) AS deductible_structural, "
            "max(structural.insurance_limit_absolute) AS insurance_limit_structural",
            fields)
        self.assertEqual("""
            LEFT JOIN riski.cost AS structural
            ON structural.cost_type_id = '1' AND
            structural.exposure_data_id = riski.exposure_data.id""",
            joins)

    def test_get_cost_types_query_helper_several_cost_types(self):
        cost = namedtuple("cost", "name id")
        fields, joins = self.manager._get_cost_types_query_helper(
            [cost("structural", 1), cost("nonstructural", 2)])

        self.assertEqual(
            "max(structural.converted_cost) AS structural, "
            "max(structural.converted_retrofitted_cost) AS retrofitted_structural, "
            "max(structural.deductible_absolute) AS deductible_structural, "
            "max(structural.insurance_limit_absolute) AS insurance_limit_structural, "
            "max(nonstructural.converted_cost) AS nonstructural, "
            "max(nonstructural.converted_retrofitted_cost) AS retrofitted_nonstructural, "
            "max(nonstructural.deductible_absolute) AS deductible_nonstructural, "
            "max(nonstructural.insurance_limit_absolute) AS insurance_limit_nonstructural",
            fields)
        self.assertEqual("""
            LEFT JOIN riski.cost AS structural
            ON structural.cost_type_id = '1' AND
            structural.exposure_data_id = riski.exposure_data.id
            LEFT JOIN riski.cost AS nonstructural
            ON nonstructural.cost_type_id = '2' AND
            nonstructural.exposure_data_id = riski.exposure_data.id""",
            joins)

########NEW FILE########
__FILENAME__ = models_test
# -*- encoding: utf-8 -*-
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import getpass
import unittest
import mock

import numpy

from nose.plugins.attrib import attr

from django.contrib.gis.db import models as djm

from openquake.hazardlib.geo.mesh import Mesh
from openquake.hazardlib.geo.point import Point
from openquake.hazardlib.geo.surface.planar import PlanarSurface
from openquake.hazardlib.geo.surface.simple_fault import SimpleFaultSurface

from openquake.engine.calculators.hazard.classical import core as cls_core
from openquake.engine.calculators.hazard.scenario import core as scen_core
from openquake.engine.db import models

from openquake.engine.tests.utils import helpers


class HazardCalculationGeometryTestCase(unittest.TestCase):
    """Test special geometry handling in the HazardCalculation constructor."""

    def test_sites_from_wkt(self):
        # should succeed with no errors
        hjp = models.HazardCalculation.create(sites='MULTIPOINT(1 2, 3 4)')
        expected_wkt = (
            'MULTIPOINT (1.0000000000000000 2.0000000000000000,'
            ' 3.0000000000000000 4.0000000000000000)'
        )

        self.assertEqual(expected_wkt, hjp.sites.wkt)

    def test_sites_invalid_str(self):
        self.assertRaises(
            ValueError, models.HazardCalculation.create, sites='a 5')

    def test_sites_odd_num_of_coords_in_str_list(self):
        self.assertRaises(
            ValueError, models.HazardCalculation.create, sites='1 2, 3')

    def test_sites_valid_str_list(self):
        hjp = models.HazardCalculation.create(sites='1 2, 3 4')
        expected_wkt = (
            'MULTIPOINT (1.0000000000000000 2.0000000000000000,'
            ' 3.0000000000000000 4.0000000000000000)'
        )

        self.assertEqual(expected_wkt, hjp.sites.wkt)

    def test_region_from_wkt(self):
        hjp = models.HazardCalculation.create(
            region='POLYGON((1 2, 3 4, 5 6, 1 2))')
        expected_wkt = (
            'POLYGON ((1.0000000000000000 2.0000000000000000, '
            '3.0000000000000000 4.0000000000000000, '
            '5.0000000000000000 6.0000000000000000, '
            '1.0000000000000000 2.0000000000000000))'
        )

        self.assertEqual(expected_wkt, hjp.region.wkt)

    def test_region_invalid_str(self):
        self.assertRaises(
            ValueError, models.HazardCalculation.create,
            region='0, 0, 5a 5, 1, 3, 0, 0'
        )

    def test_region_odd_num_of_coords_in_str_list(self):
        self.assertRaises(
            ValueError, models.HazardCalculation.create,
            region='1 2, 3 4, 5 6, 1'
        )

    def test_region_valid_str_list(self):
        # note that the last coord (with closes the ring) can be ommitted
        # in this case
        hjp = models.HazardCalculation.create(region='1 2, 3 4, 5 6')
        expected_wkt = (
            'POLYGON ((1.0000000000000000 2.0000000000000000, '
            '3.0000000000000000 4.0000000000000000, '
            '5.0000000000000000 6.0000000000000000, '
            '1.0000000000000000 2.0000000000000000))'
        )

        self.assertEqual(expected_wkt, hjp.region.wkt)

    def test_points_to_compute_none(self):
        hc = models.HazardCalculation.create()
        self.assertIsNone(hc.points_to_compute())

        hc = models.HazardCalculation.create(region='1 2, 3 4, 5 6')
        # There's no region grid spacing
        self.assertIsNone(hc.points_to_compute())

    def test_points_to_compute_region(self):
        lons = [
            6.761295081695822, 7.022590163391642,
            7.28388524508746, 7.54518032678328,
            7.806475408479099, 8.067770490174919,
            8.329065571870737, 6.760434846130313,
            7.020869692260623, 7.281304538390934,
            7.541739384521245, 7.802174230651555,
            8.062609076781865, 8.323043922912175,
            6.759582805761787, 7.019165611523571,
            7.278748417285356, 7.53833122304714,
            7.797914028808925, 8.057496834570708,
            8.317079640332492, 6.758738863707749,
            7.017477727415495, 7.276216591123242,
            7.534955454830988, 7.793694318538734,
            8.05243318224648, 8.311172045954226,
        ]

        lats = [
            46.5, 46.5,
            46.5, 46.5,
            46.5, 46.5,
            46.5, 46.320135678816236,
            46.320135678816236, 46.320135678816236,
            46.320135678816236, 46.320135678816236,
            46.320135678816236, 46.320135678816236,
            46.140271357632486, 46.140271357632486,
            46.140271357632486, 46.140271357632486,
            46.140271357632486, 46.140271357632486,
            46.140271357632486, 45.96040703644873,
            45.96040703644873, 45.96040703644873,
            45.96040703644873, 45.96040703644873,
            45.96040703644873, 45.96040703644873,
        ]

        hc = models.HazardCalculation.create(
            region='6.5 45.8, 6.5 46.5, 8.5 46.5, 8.5 45.8',
            region_grid_spacing=20)
        mesh = hc.points_to_compute(save_sites=False)

        numpy.testing.assert_array_almost_equal(lons, mesh.lons)
        numpy.testing.assert_array_almost_equal(lats, mesh.lats)

    def test_points_to_compute_sites(self):
        lons = [6.5, 6.5, 8.5, 8.5]
        lats = [45.8, 46.5, 46.5, 45.8]
        hc = models.HazardCalculation.create(
            sites='6.5 45.8, 6.5 46.5, 8.5 46.5, 8.5 45.8')

        mesh = hc.points_to_compute(save_sites=False)

        numpy.testing.assert_array_equal(lons, mesh.lons)
        numpy.testing.assert_array_equal(lats, mesh.lats)


class ProbabilisticRuptureTestCase(unittest.TestCase):

    @classmethod
    def setUpClass(self):
        cfg = helpers.get_data_path('simple_fault_demo_hazard/job.ini')
        job = helpers.get_job(cfg)

        lt_model = models.LtSourceModel.objects.create(
            hazard_calculation=job.hazard_calculation, ordinal=0,
            sm_lt_path='foo')
        lt_rlz = models.LtRealization.objects.create(
            lt_model=lt_model, ordinal=0, gsim_lt_path='bar')
        output = models.Output.objects.create(
            oq_job=job, display_name='test', output_type='ses')
        ses_coll = models.SESCollection.objects.create(
            output=output, lt_model=lt_rlz.lt_model, ordinal=0)

        self.mesh_lons = numpy.array(
            [0.1 * x for x in range(16)]).reshape((4, 4))
        self.mesh_lats = numpy.array(
            [0.2 * x for x in range(16)]).reshape((4, 4))
        self.mesh_depths = numpy.array(
            [0.3 * x for x in range(16)]).reshape((4, 4))

        sfs = SimpleFaultSurface(
            Mesh(self.mesh_lons, self.mesh_lats, self.mesh_depths))

        ps = PlanarSurface(
            10, 20, 30,
            Point(3.9, 2.2, 10), Point(4.90402718, 3.19634248, 10),
            Point(5.9, 2.2, 90), Point(4.89746275, 1.20365263, 90))

        self.fault_rupture = models.ProbabilisticRupture.objects.create(
            ses_collection=ses_coll, magnitude=5, rake=0, surface=sfs,
            tectonic_region_type='Active Shallow Crust',
            is_from_fault_source=True, is_multi_surface=False)
        self.source_rupture = models.ProbabilisticRupture.objects.create(
            ses_collection=ses_coll, magnitude=5, rake=0, surface=ps,
            tectonic_region_type='Active Shallow Crust',
            is_from_fault_source=False, is_multi_surface=False)

    def test_fault_rupture(self):
        # Test loading a fault rupture from the DB, just to illustrate a use
        # case.
        # Also, we should that planar surface corner points are not valid and
        # are more or less disregarded for this type of rupture.
        fault_rupture = models.ProbabilisticRupture.objects.get(
            id=self.fault_rupture.id)
        self.assertIs(None, fault_rupture.top_left_corner)
        self.assertIs(None, fault_rupture.top_right_corner)
        self.assertIs(None, fault_rupture.bottom_right_corner)
        self.assertIs(None, fault_rupture.bottom_left_corner)

    def test_source_rupture(self):
        source_rupture = models.ProbabilisticRupture.objects.get(
            id=self.source_rupture.id)
        self.assertEqual((3.9, 2.2, 10.), source_rupture.top_left_corner)
        self.assertEqual((4.90402718, 3.19634248, 10.0),
                         source_rupture.top_right_corner)
        self.assertEqual((4.89746275, 1.20365263, 90.0),
                         source_rupture.bottom_left_corner)
        self.assertEqual((5.9, 2.2, 90.0), source_rupture.bottom_right_corner)


def get_tags(gmf_data):
    """
    Get the rupture tags associated to a given gmf_data record
    """
    for r_id in gmf_data.rupture_ids:
        yield models.SESRupture.objects.get(pk=r_id).tag


class GmfsPerSesTestCase(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cfg = helpers.get_data_path('event_based_hazard/job.ini')
        job = helpers.get_job(cfg)
        lt_model = models.LtSourceModel.objects.create(
            hazard_calculation=job.hazard_calculation,
            ordinal=1, sm_lt_path="test_sm")
        lt_model_2 = models.LtSourceModel.objects.create(
            hazard_calculation=job.hazard_calculation,
            ordinal=2, sm_lt_path="test_sm_2")
        rlz1 = models.LtRealization.objects.create(
            lt_model=lt_model, ordinal=1, weight=None,
            gsim_lt_path="test_gsim")
        rlz2 = models.LtRealization.objects.create(
            lt_model=lt_model, ordinal=2, weight=None,
            gsim_lt_path="test_gsim_2")
        ses_coll = models.SESCollection.objects.create(
            output=models.Output.objects.create_output(
                job, "Test SES Collection 1", "ses"),
            lt_model=lt_model, ordinal=1)
        # create a second SESCollection; this is to avoid regressions
        # in models.Gmf.__iter__ which should yield a single
        # GmfSet even if there are several SES collections
        models.SESCollection.objects.create(
            output=models.Output.objects.create_output(
                job, "Test SES Collection 2", "ses"),
            lt_model=lt_model_2, ordinal=2)

        gmf_data1 = helpers.create_gmf_data_records(job, rlz1, ses_coll)[0]
        points = [(15.3, 38.22), (15.7, 37.22),
                  (15.4, 38.09), (15.56, 38.1), (15.2, 38.2)]
        gmf_data2 = helpers.create_gmf_data_records(
            job, rlz2, ses_coll, points)[0]
        cls.gmf1 = gmf_data1.gmf  # a Gmf instance
        cls.ruptures1 = tuple(get_tags(gmf_data1))
        cls.ruptures2 = tuple(get_tags(gmf_data2))
        cls.investigation_time = job.hazard_calculation.investigation_time

    def test_branch_lt(self):
        [gmfset] = self.gmf1  # exhaust Gmf.__iter__
        expected = """\
GMFsPerSES(investigation_time=%f, stochastic_event_set_id=1,
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=%s
<X= 15.31000, Y= 38.22500, GMV=0.1000000>
<X= 15.48000, Y= 38.09100, GMV=0.1000000>
<X= 15.48100, Y= 38.25000, GMV=0.1000000>
<X= 15.56500, Y= 38.17000, GMV=0.1000000>
<X= 15.71000, Y= 37.22500, GMV=0.1000000>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=%s
<X= 15.31000, Y= 38.22500, GMV=0.2000000>
<X= 15.48000, Y= 38.09100, GMV=0.2000000>
<X= 15.48100, Y= 38.25000, GMV=0.2000000>
<X= 15.56500, Y= 38.17000, GMV=0.2000000>
<X= 15.71000, Y= 37.22500, GMV=0.2000000>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=%s
<X= 15.31000, Y= 38.22500, GMV=0.3000000>
<X= 15.48000, Y= 38.09100, GMV=0.3000000>
<X= 15.48100, Y= 38.25000, GMV=0.3000000>
<X= 15.56500, Y= 38.17000, GMV=0.3000000>
<X= 15.71000, Y= 37.22500, GMV=0.3000000>))""" % (
            (self.investigation_time,) + self.ruptures1)
        self.assertEqual(str(gmfset), expected)


class PrepGeometryTestCase(unittest.TestCase):

    def test__prep_geometry(self):
        the_input = {
            # with commas between every value
            'sites': '-1.1, -1.2, 1.3, 0.0',
            # with no commas
            'region': '-1 1 1 1 1 -1 -1 -1',
            # with randomly placed commas
            'region_constraint': (
                '-0.5 0.5 0.0, 2.0 0.5 0.5, 0.5 -0.5 -0.5, -0.5'),
            'something': 'else',
        }

        expected = {
            'sites': 'MULTIPOINT(-1.1 -1.2, 1.3 0.0)',
            'region': (
                'POLYGON((-1.0 1.0, 1.0 1.0, 1.0 -1.0, -1.0 -1.0, -1.0 1.0))'),
            'region_constraint': (
                'POLYGON((-0.5 0.5, 0.0 2.0, 0.5 0.5, 0.5 -0.5, -0.5 -0.5, '
                '-0.5 0.5))'),
            'something': 'else',
        }

        self.assertEqual(expected, models._prep_geometry(the_input))


class FloatFieldTestCase(unittest.TestCase):
    def test_truncate_small_numbers(self):
        # workaround a postgres error "out of range for type double precision"
        self.assertEqual(djm.FloatField().get_prep_value(1e-301), 0)


class GetSiteCollectionTestCase(unittest.TestCase):

    @attr('slow')
    def test_get_site_collection_with_site_model(self):
        cfg = helpers.get_data_path(
            'simple_fault_demo_hazard/job_with_site_model.ini')
        job = helpers.get_job(cfg)
        models.JobStats.objects.create(oq_job=job)
        calc = cls_core.ClassicalHazardCalculator(job)

        # Bootstrap the `hazard_site` table:
        calc.initialize_site_model()
        calc.initialize_sources()

        site_coll = job.hazard_calculation.site_collection
        # Since we're using a pretty big site model, it's a bit excessive to
        # check each and every value.
        # Instead, we'll just test that the lenth of each site collection attr
        # is equal to the number of points of interest in the calculation.
        expected_len = len(job.hazard_calculation.points_to_compute())

        self.assertEqual(expected_len, len(site_coll))
        self.assertEqual(expected_len, len(site_coll.vs30))
        self.assertEqual(expected_len, len(site_coll.vs30measured))
        self.assertEqual(expected_len, len(site_coll.z1pt0))
        self.assertEqual(expected_len, len(site_coll.z2pt5))

    def test_site_collection_and_ses_collection(self):
        cfg = helpers.get_data_path('scenario_hazard/job.ini')
        job = helpers.get_job(cfg, username=getpass.getuser())
        models.JobStats.objects.create(oq_job=job)

        calc = scen_core.ScenarioHazardCalculator(job)
        calc.initialize_site_model()
        site_coll = job.hazard_calculation.site_collection

        # all of the parameters should be the same:
        self.assertTrue((site_coll.vs30 == 760).all())
        self.assertTrue((site_coll.vs30measured).all())
        self.assertTrue((site_coll.z1pt0 == 100).all())
        self.assertTrue((site_coll.z2pt5 == 5).all())

        # just for sanity, make sure the meshes are correct (the locations)
        job_mesh = job.hazard_calculation.points_to_compute()
        self.assertTrue((job_mesh.lons == site_coll.mesh.lons).all())
        self.assertTrue((job_mesh.lats == site_coll.mesh.lats).all())

        # test SESCollection
        calc.initialize_sources()
        calc.create_ruptures()
        ses_coll = models.SESCollection.objects.get(
            output__oq_job=job, output__output_type='ses')
        expected_tags = [
            'scenario-0000000000',
            'scenario-0000000001',
            'scenario-0000000002',
            'scenario-0000000003',
            'scenario-0000000004',
            'scenario-0000000005',
            'scenario-0000000006',
            'scenario-0000000007',
            'scenario-0000000008',
            'scenario-0000000009',
        ]
        expected_seeds = [
            511025145, 1168723362, 794472670, 1296908407, 1343724121,
            140722153, 28278046, 1798451159, 556958504, 503221907]
        for ses in ses_coll:  # there is a single ses
            self.assertEqual(ses.ordinal, 1)
            for ses_rup, tag, seed in zip(ses, expected_tags, expected_seeds):
                self.assertEqual(ses_rup.ses_id, 1)
                self.assertEqual(ses_rup.tag, tag)
                self.assertEqual(ses_rup.seed, seed)


class LossFractionTestCase(unittest.TestCase):
    def test_display_taxonomy_value(self):
        lf = models.LossFraction(variable="taxonomy")
        rc = mock.Mock()

        self.assertEqual("RC", lf.display_value("RC", rc))

    def test_display_magnitude_distance_value(self):
        rc = mock.Mock()
        rc.mag_bin_width = 2
        rc.distance_bin_width = 10

        lf = models.LossFraction(variable="magnitude_distance")

        self.assertEqual("12.0000,14.0000|300.0000,310.0000",
                         lf.display_value("6, 30", rc))
        self.assertEqual("14.0000,16.0000|210.0000,220.0000",
                         lf.display_value("7, 21", rc))
        self.assertEqual("0.0000,2.0000|0.0000,10.0000",
                         lf.display_value("0, 0", rc))

    def test_display_coordinate_value(self):
        rc = mock.Mock()
        rc.coordinate_bin_width = 0.5

        lf = models.LossFraction(variable="coordinate")

        self.assertEqual("3.0000,3.5000|15.0000,15.5000",
                         lf.display_value("6, 30", rc))
        self.assertEqual("3.5000,4.0000|10.5000,11.0000",
                         lf.display_value("7, 21", rc))
        self.assertEqual("0.0000,0.5000|0.0000,0.5000",
                         lf.display_value("0.0, 0.0", rc))

########NEW FILE########
__FILENAME__ = routers_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import unittest

from openquake.engine.db import routers
from openquake.engine.db import models as oq


class OQRouterTestCase(unittest.TestCase):
    '''
    Tests for the :py:class:`openquake.engine.db.routers.OQRouter` class.
    '''
    def setUp(self):
        self.router = routers.OQRouter()

    def _db_for_read_helper(self, classes, expected_db):
        '''
        Common test logic
        '''
        for cls in classes:
            self.assertEqual(expected_db, self.router.db_for_read(cls()))

    def _db_for_write_helper(self, classes, expected_db):
        '''
        Common test logic
        '''
        for cls in classes:
            self.assertEqual(expected_db, self.router.db_for_write(cls()))

    def test_admin_correct_read_db(self):
        '''
        For each model in the 'admin' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.RevisionInfo]
        expected_db = 'admin'

        self._db_for_read_helper(classes, expected_db)

    def test_admin_correct_write_db(self):
        '''
        For each model in the 'admin' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.RevisionInfo]
        expected_db = 'admin'

        self._db_for_write_helper(classes, expected_db)

    def test_hzrdi_read_schema(self):
        '''
        For each model in the 'hzrdi' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.SiteModel]
        expected_db = 'job_init'

        self._db_for_read_helper(classes, expected_db)

    def test_hzrdi_write_schema(self):
        '''
        For each model in the 'hzrdi' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.SiteModel]
        expected_db = 'job_init'

        self._db_for_write_helper(classes, expected_db)

    def test_uiapi_read_schema(self):
        '''
        For each model in the 'uiapi' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.OqJob, oq.Output]
        expected_db = 'job_init'

        self._db_for_read_helper(classes, expected_db)

    def test_uiapi_write_schema(self):
        '''
        For each model in the 'uiapi' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.OqJob]
        expected_db = 'job_init'

        self._db_for_write_helper(classes, expected_db)
        self._db_for_write_helper([oq.Output], 'job_init')

    def test_hzrdr_read_schema(self):
        '''
        For each model in the 'hzrdr' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.HazardMap, oq.HazardCurve, oq.HazardCurveData,
                   oq.GmfData]
        expected_db = 'job_init'

        self._db_for_read_helper(classes, expected_db)

    def test_hzrdr_write_schema(self):
        '''
        For each model in the 'hzrdr' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.HazardMap, oq.HazardCurve, oq.HazardCurveData,
                   oq.GmfData]

        expected_db = 'job_init'

        self._db_for_write_helper(classes, expected_db)

    def test_riskr_read_schema(self):
        '''
        For each model in the 'riskr' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.LossMap, oq.LossMapData, oq.LossCurve, oq.LossCurveData,
                   oq.AggregateLossCurveData, oq.BCRDistribution,
                   oq.BCRDistributionData]
        expected_db = 'job_init'

        self._db_for_read_helper(classes, expected_db)

    def test_riskr_write_schema(self):
        '''
        For each model in the 'riskr' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.LossMap, oq.LossMapData, oq.LossCurve, oq.LossCurveData,
                   oq.AggregateLossCurveData, oq.BCRDistribution,
                   oq.BCRDistributionData]
        expected_db = 'job_init'

        self._db_for_write_helper(classes, expected_db)

    def test_riski_correct_read_db(self):
        '''
        For each model in the 'riski' schema, test for proper db routing
        for read operations.
        '''
        classes = [oq.ExposureModel, oq.ExposureData, oq.Occupancy]
        expected_db = 'job_init'

        self._db_for_read_helper(classes, expected_db)

    def test_riski_correct_write_db(self):
        '''
        For each model in the 'riski' schema, test for proper db routing
        for write operations.
        '''
        classes = [oq.ExposureModel, oq.ExposureData]
        expected_db = 'job_init'

        self._db_for_write_helper(classes, expected_db)

########NEW FILE########
__FILENAME__ = engine_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import getpass
import os
import shutil
import subprocess
import tempfile
import unittest
import warnings

from openquake.engine.db import models
from django.core import exceptions

from openquake.engine import engine
from openquake.engine.tests.utils import helpers


class PrepareJobTestCase(unittest.TestCase):

    def test_prepare_job_default_user(self):
        job = engine.prepare_job()

        self.assertEqual('openquake', job.user_name)
        self.assertEqual('pre_executing', job.status)
        self.assertEqual('progress', job.log_level)

        # Check the make sure it's in the database.
        try:
            models.OqJob.objects.get(id=job.id)
        except exceptions.ObjectDoesNotExist:
            self.fail('Job was not found in the database')

    def test_prepare_job_specified_user(self):
        user_name = helpers.random_string()
        job = engine.prepare_job(user_name=user_name)

        self.assertEqual(user_name, job.user_name)
        self.assertEqual('pre_executing', job.status)
        self.assertEqual('progress', job.log_level)

        try:
            models.OqJob.objects.get(id=job.id)
        except exceptions.ObjectDoesNotExist:
            self.fail('Job was not found in the database')

    def test_prepare_job_explicit_log_level(self):
        # By default, a job is created with a log level of 'progress'
        # (just to show calculation progress).
        # In this test, we'll specify 'debug' as the log level.
        job = engine.prepare_job(log_level='debug')

        self.assertEqual('debug', job.log_level)


class ParseConfigTestCase(unittest.TestCase):

    def test_parse_config_no_files(self):
        # sections are there just for documentation
        # when we parse the file, we ignore these
        source = StringIO.StringIO("""
[general]
CALCULATION_MODE = classical
region = 1 1 2 2 3 3
[foo]
bar = baz
""")
        # Add a 'name' to make this look like a real file:
        source.name = 'path/to/some/job.ini'
        exp_base_path = os.path.dirname(
            os.path.join(os.path.abspath('.'), source.name))

        expected_params = {
            'base_path': exp_base_path,
            'calculation_mode': 'classical',
            'region': '1 1 2 2 3 3',
            'bar': 'baz',
            'inputs': {},
        }

        params = engine.parse_config(source)

        self.assertEqual(expected_params, params)

    def test_parse_config_with_files(self):
        temp_dir = tempfile.mkdtemp()
        site_model_input = helpers.touch(dir=temp_dir, content="foo")
        job_config = helpers.touch(dir=temp_dir, content="""
[general]
calculation_mode = classical
[site]
site_model_file = %s
maximum_distance=0
truncation_level=0
random_seed=0
    """ % site_model_input)

        try:
            exp_base_path = os.path.dirname(job_config)

            expected_params = {
                'base_path': exp_base_path,
                'calculation_mode': 'classical',
                'truncation_level': '0',
                'random_seed': '0',
                'maximum_distance': '0',
                'inputs': {'site_model': site_model_input},
            }

            params = engine.parse_config(open(job_config, 'r'))
            self.assertEqual(expected_params, params)
            self.assertEqual(['site_model'], params['inputs'].keys())
            self.assertEqual([site_model_input], params['inputs'].values())
        finally:
            shutil.rmtree(temp_dir)

    def test__parse_sites_csv(self):
        expected_wkt = 'MULTIPOINT(0.1 0.2, 2 3, 4.1 5.6)'
        source = StringIO.StringIO("""\
0.1,0.2
2,3
4.1,5.6
""")
        wkt = engine._parse_sites_csv(source)
        self.assertEqual(expected_wkt, wkt)

    def test_parse_config_with_sites_csv(self):
        sites_csv = helpers.touch(content='1.0,2.1\n3.0,4.1\n5.0,6.1')
        try:
            source = StringIO.StringIO("""
[general]
calculation_mode = classical
[geometry]
sites_csv = %s
[misc]
maximum_distance=0
truncation_level=3
random_seed=5
""" % sites_csv)
            source.name = 'path/to/some/job.ini'
            exp_base_path = os.path.dirname(
                os.path.join(os.path.abspath('.'), source.name))

            expected_params = {
                'base_path': exp_base_path,
                'sites': 'MULTIPOINT(1.0 2.1, 3.0 4.1, 5.0 6.1)',
                'calculation_mode': 'classical',
                'truncation_level': '3',
                'random_seed': '5',
                'maximum_distance': '0',
                'inputs': {},
            }

            params = engine.parse_config(source)
            self.assertEqual(expected_params, params)
        finally:
            os.unlink(sites_csv)


class CreateHazardCalculationTestCase(unittest.TestCase):

    def setUp(self):
        # Just the bare minimum set of params to satisfy not null constraints
        # in the db.
        self.params = {
            'base_path': 'path/to/job.ini',
            'calculation_mode': 'classical',
            'region': '1 1 2 2 3 3',
            'width_of_mfd_bin': '1',
            'rupture_mesh_spacing': '1',
            'area_source_discretization': '2',
            'investigation_time': 50,
            'truncation_level': 0,
            'maximum_distance': 200,
            'number_of_logic_tree_samples': 1,
            'intensity_measure_types_and_levels': dict(PGA=[1, 2, 3, 4]),
            'random_seed': 37,
        }

    def test_create_hazard_calculation(self):
        hc = engine.create_calculation(models.HazardCalculation, self.params)

        # Normalize/clean fields by fetching a fresh copy from the db.
        hc = models.HazardCalculation.objects.get(id=hc.id)

        self.assertEqual(hc.calculation_mode, 'classical')
        self.assertEqual(hc.width_of_mfd_bin, 1.0)
        self.assertEqual(hc.rupture_mesh_spacing, 1.0)
        self.assertEqual(hc.area_source_discretization, 2.0)
        self.assertEqual(hc.investigation_time, 50.0)
        self.assertEqual(hc.truncation_level, 0.0)
        self.assertEqual(hc.maximum_distance, 200.0)

    def test_create_hazard_calculation_warns(self):
        # If unknown parameters are specified in the config file, we expect
        # `create_hazard_calculation` to raise warnings and ignore those
        # parameters.

        # Add some random unknown params:
        self.params['blargle'] = 'spork'
        self.params['do_science'] = 'true'

        expected_warnings = [
            "Unknown parameter 'blargle'. Ignoring.",
            "Unknown parameter 'do_science'. Ignoring.",
        ]

        with warnings.catch_warnings(record=True) as w:
            engine.create_calculation(
                models.HazardCalculation, self.params)
        actual_warnings = [msg.message.message for msg in w]
        self.assertEqual(sorted(expected_warnings), sorted(actual_warnings))


class CreateRiskCalculationTestCase(unittest.TestCase):

    def test_create_risk_calculation(self):
        # we need an hazard output to create a risk calculation
        hazard_cfg = helpers.get_data_path('simple_fault_demo_hazard/job.ini')
        hazard_job = helpers.get_job(hazard_cfg, 'openquake')
        hc = hazard_job.hazard_calculation
        lt_model = models.LtSourceModel.objects.create(
            hazard_calculation=hazard_job.hazard_calculation,
            ordinal=1, sm_lt_path="test_sm")
        rlz = models.LtRealization.objects.create(
            lt_model=lt_model, ordinal=1, weight=None,
            gsim_lt_path="test_gsim")
        hazard_output = models.HazardCurve.objects.create(
            lt_realization=rlz,
            output=models.Output.objects.create_output(
                hazard_job, "Test Hazard output", "hazard_curve"),
            investigation_time=hc.investigation_time,
            imt="PGA", imls=[0.1, 0.2, 0.3])
        params = {
            'hazard_output_id': hazard_output.output.id,
            'base_path': 'path/to/job.ini',
            'export_dir': '/tmp/xxx',
            'calculation_mode': 'classical',
            # just some sample params
            'lrem_steps_per_interval': 5,
            'conditional_loss_poes': '0.01, 0.02, 0.05',
            'region_constraint': '-0.5 0.5, 0.5 0.5, 0.5 -0.5, -0.5, -0.5',
        }

        rc = engine.create_calculation(models.RiskCalculation, params)

        # Normalize/clean fields by fetching a fresh copy from the db.
        rc = models.RiskCalculation.objects.get(id=rc.id)

        self.assertEqual(rc.calculation_mode, 'classical')
        self.assertEqual(rc.lrem_steps_per_interval, 5)
        self.assertEqual(rc.conditional_loss_poes, [0.01, 0.02, 0.05])
        self.assertEqual(
            rc.region_constraint.wkt,
            ('POLYGON ((-0.5000000000000000 0.5000000000000000, '
             '0.5000000000000000 0.5000000000000000, '
             '0.5000000000000000 -0.5000000000000000, '
             '-0.5000000000000000 -0.5000000000000000, '
             '-0.5000000000000000 0.5000000000000000))'))


class OpenquakeCliTestCase(unittest.TestCase):
    """
    Run "openquake --version" as a separate
    process using `subprocess`.
    """

    def test_run_version(self):
        args = [helpers.RUNNER, "--version"]

        print 'Running:', ' '.join(args)  # this is useful for debugging
        return subprocess.check_call(args)


class DeleteHazCalcTestCase(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.hazard_cfg = helpers.get_data_path(
            'simple_fault_demo_hazard/job.ini')
        cls.risk_cfg = helpers.get_data_path(
            'classical_psha_based_risk/job.ini')

    def test_del_haz_calc(self):
        hazard_job = helpers.get_job(
            self.hazard_cfg, username=getpass.getuser())
        hazard_calc = hazard_job.hazard_calculation

        models.Output.objects.create_output(
            hazard_job, 'test_curves_1', output_type='hazard_curve'
        )
        models.Output.objects.create_output(
            hazard_job, 'test_curves_2', output_type='hazard_curve'
        )

        # Sanity check: make sure the hazard calculation and outputs exist in
        # the database:
        hazard_calcs = models.HazardCalculation.objects.filter(
            id=hazard_calc.id
        )
        self.assertEqual(1, hazard_calcs.count())

        outputs = models.Output.objects.filter(oq_job=hazard_job.id)
        self.assertEqual(2, outputs.count())

        # Delete the calculation
        engine.del_haz_calc(hazard_calc.id)

        # Check that the hazard calculation and its outputs were deleted:
        outputs = models.Output.objects.filter(oq_job=hazard_job.id)
        self.assertEqual(0, outputs.count())

        hazard_calcs = models.HazardCalculation.objects.filter(
            id=hazard_calc.id
        )
        self.assertEqual(0, hazard_calcs.count())

    def test_del_haz_calc_does_not_exist(self):
        self.assertRaises(RuntimeError, engine.del_haz_calc, -1)

    def test_del_haz_calc_no_access(self):
        # Test the case where we try to delete a hazard calculation which does
        # not belong to current user.
        # In this case, deletion is now allowed and should raise an exception.
        hazard_job = helpers.get_job(
            self.hazard_cfg, username=helpers.random_string())
        hazard_calc = hazard_job.hazard_calculation

        self.assertRaises(RuntimeError, engine.del_haz_calc, hazard_calc.id)

    def test_del_haz_calc_referenced_by_risk_calc(self):
        # Test the case where a risk calculation is referencing the hazard
        # calculation we want to delete.
        # In this case, deletion is not allowed and should raise an exception.
        risk_job, _ = helpers.get_fake_risk_job(
            self.risk_cfg, self.hazard_cfg,
            output_type='curve', username=getpass.getuser()
        )
        risk_calc = risk_job.risk_calculation

        hazard_job = risk_job.risk_calculation.hazard_output.oq_job
        hazard_calc = hazard_job.hazard_calculation

        risk_calc.hazard_output = None
        risk_calc.hazard_calculation = hazard_calc
        risk_calc.save(using='admin')

        self.assertRaises(RuntimeError, engine.del_haz_calc, hazard_calc.id)

    def test_del_haz_calc_output_referenced_by_risk_calc(self):
        # Test the case where a risk calculation is referencing one of the
        # belonging to the hazard calculation we want to delete.
        # In this case, deletion is not allowed and should raise an exception.
        risk_job, _ = helpers.get_fake_risk_job(
            self.risk_cfg, self.hazard_cfg,
            output_type='curve', username=getpass.getuser()
        )
        hazard_job = risk_job.risk_calculation.hazard_output.oq_job
        hazard_calc = hazard_job.hazard_calculation

        self.assertRaises(RuntimeError, engine.del_haz_calc, hazard_calc.id)


class DeleteRiskCalcTestCase(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.hazard_cfg = helpers.get_data_path(
            'simple_fault_demo_hazard/job.ini')
        cls.risk_cfg = helpers.get_data_path(
            'classical_psha_based_risk/job.ini')

    def test_del_risk_calc(self):
        risk_job, _ = helpers.get_fake_risk_job(
            self.risk_cfg, self.hazard_cfg,
            output_type='curve', username=getpass.getuser()
        )
        risk_calc = risk_job.risk_calculation

        models.Output.objects.create_output(
            risk_job, 'test_curves_1', output_type='loss_curve'
        )
        models.Output.objects.create_output(
            risk_job, 'test_curves_2', output_type='loss_curve'
        )

        # Sanity check: make sure the risk calculation and outputs exist in
        # the database:
        risk_calcs = models.RiskCalculation.objects.filter(
            id=risk_calc.id
        )
        self.assertEqual(1, risk_calcs.count())

        outputs = models.Output.objects.filter(oq_job=risk_job.id)
        self.assertEqual(2, outputs.count())

        # Delete the calculation
        engine.del_risk_calc(risk_calc.id)

        # Check that the risk calculation and its outputs were deleted:
        outputs = models.Output.objects.filter(oq_job=risk_job.id)
        self.assertEqual(0, outputs.count())

        risk_calcs = models.RiskCalculation.objects.filter(
            id=risk_calc.id
        )
        self.assertEqual(0, risk_calcs.count())

    def test_del_risk_calc_does_not_exist(self):
        self.assertRaises(RuntimeError, engine.del_risk_calc, -1)

    def test_del_risk_calc_no_access(self):
        # Test the case where we try to delete a risk calculation which does
        # not belong to current user.
        # In this case, deletion is now allowed and should raise an exception.
        risk_job, _ = helpers.get_fake_risk_job(
            self.risk_cfg, self.hazard_cfg,
            output_type='curve', username=helpers.random_string()
        )
        risk_calc = risk_job.risk_calculation

        self.assertRaises(RuntimeError, engine.del_risk_calc, risk_calc.id)

########NEW FILE########
__FILENAME__ = import_gmf_scenario_test
import os
import unittest
from openquake.engine.db import models
from openquake.engine.tools import import_gmf_scenario
from openquake import nrmllib
from nose.tools import assert_equal
from io import StringIO


class ImportGMFScenarioTestCase(unittest.TestCase):

    # test that the example file gmf-scenario.xml can be imported
    def test_import_gmf_scenario(self):
        repodir = os.path.dirname(os.path.dirname(nrmllib.__path__[0]))
        fileobj = open(os.path.join(repodir, 'examples', 'gmf-scenario.xml'))
        out = import_gmf_scenario.import_gmf_scenario(fileobj)
        hc = out.oq_job.hazard_calculation
        n = models.GmfData.objects.filter(gmf__output=out).count()
        assert_equal(hc.calculation_mode, 'scenario')
        assert_equal(hc.number_of_ground_motion_fields, n)
        assert_equal(n, 9)  # 9 rows entered
        assert_equal(hc.description,
                     'Scenario importer, file gmf-scenario.xml')

    # test that a tab-separated file can be imported
    def test_import_gmf_scenario_csv(self):
        test_data = StringIO(unicode('''\
SA	0.025	5.0	{0.2}	POINT(0.0 0.0)
SA	0.025	5.0	{1.4}	POINT(1.0 0.0)
SA	0.025	5.0	{0.6}	POINT(0.0 1.0)
PGA	\N	\N	{0.2,0.3}	POINT(0.0 0.0)
PGA	\N	\N	{1.4,1.5}	POINT(1.0 0.0)
PGA	\N	\N	{0.6,0.7}	POINT(0.0 1.0)
PGV	\N	\N	{0.2}	POINT(0.0 0.0)
PGV	\N	\N	{1.4}	POINT(1.0 0.0)
'''))
        test_data.name = 'test_data'
        out = import_gmf_scenario.import_gmf_scenario(test_data)
        n = models.GmfData.objects.filter(gmf__output=out).count()
        assert_equal(n, 8)  # 8 rows entered

########NEW FILE########
__FILENAME__ = import_hazard_curves_test
import os
import unittest

from openquake.engine.tools.import_hazard_curves import import_hazard_curves
from openquake import nrmllib
from openquake.engine.db.models import HazardCurve, HazardCurveData


class ImportHazardCurvesTestCase(unittest.TestCase):

    def test_import_hazard_curves_pga(self):
        repodir = os.path.dirname(os.path.dirname(nrmllib.__path__[0]))
        fileobj = open(os.path.join(
                       repodir, 'examples', 'hazard-curves-pga.xml'))
        out = import_hazard_curves(fileobj)
        [hc] = HazardCurve.objects.filter(output=out)
        data = HazardCurveData.objects.filter(hazard_curve=hc)
        self.assertEqual(len(data), 2)  # 2 rows entered
        self.assertEqual(out.oq_job.hazard_calculation.description,
                         'HazardCurve importer, file hazard-curves-pga.xml')

    def test_import_hazard_curves_sa(self):
        repodir = os.path.dirname(os.path.dirname(nrmllib.__path__[0]))
        fileobj = open(os.path.join(
                       repodir, 'examples', 'hazard-curves-sa.xml'))
        out = import_hazard_curves(fileobj)
        [hc] = HazardCurve.objects.filter(output=out)
        data = HazardCurveData.objects.filter(hazard_curve=hc)
        self.assertEqual(len(data), 2)  # 2 rows entered
        self.assertEqual(out.oq_job.hazard_calculation.description,
                         'HazardCurve importer, file hazard-curves-sa.xml')

########NEW FILE########
__FILENAME__ = core_test

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import os
import shutil
import tempfile
import unittest
import openquake.nrmllib

from openquake.engine.export import core as export


def number_of(elem_name, tree):
    """
    Given an element name (including the namespaces prefix, if applicable),
    return the number of occurrences of the element in a given XML document.
    """
    expr = '//%s' % elem_name
    return len(tree.xpath(expr, namespaces=openquake.nrmllib.PARSE_NS_MAP))


class BaseExportTestCase(unittest.TestCase):
    def _test_exported_file(self, filename):
        self.assertTrue(os.path.exists(filename))
        self.assertTrue(os.path.isabs(filename))
        self.assertTrue(os.path.getsize(filename) > 0)


@export.makedirsdeco
def _decorated(_output, _target_dir):
    """Just a test function for exercising the `makedirsdeco` decorator."""
    return []


class UtilsTestCase(unittest.TestCase):
    """Tests for misc. export utilties."""

    def test_makedirsdeco(self):
        temp_dir = tempfile.mkdtemp()

        try:
            target_dir = os.path.join(temp_dir, 'some', 'nonexistent', 'dir')

            self.assertFalse(os.path.exists(target_dir))

            _decorated(None, target_dir)

            self.assertTrue(os.path.exists(target_dir))
        finally:
            shutil.rmtree(temp_dir)

    def test_makedirsdeco_dir_already_exists(self):
        # If the dir already exists, this should work with no errors.
        # The decorator should just gracefully pass through.
        temp_dir = tempfile.mkdtemp()
        try:
            _decorated(None, temp_dir)
        finally:
            shutil.rmtree(temp_dir)

    def test_makedirsdeco_target_exists_as_file(self):
        # If a file exists with the exact path of the target dir,
        # we should get a RuntimeError.
        _, temp_file = tempfile.mkstemp()

        try:
            self.assertRaises(RuntimeError, _decorated, None, temp_file)
        finally:
            os.unlink(temp_file)

########NEW FILE########
__FILENAME__ = hazard_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import mock
import os
import shutil
import tempfile
import unittest

from collections import namedtuple
from lxml import etree
from nose.plugins.attrib import attr

from openquake.engine.db import models
from openquake.engine.export import core as export_core
from openquake.engine.export import hazard
from openquake import nrmllib

from openquake.engine.tests.export.core_test import \
    BaseExportTestCase, number_of
from openquake.engine.tests.utils import helpers


def check_export(output_id, target):
    """
    Call hazard.export by checking that the exported file is valid
    according to our XML schema.
    """
    out_file = hazard.export(output_id, target, 'xml')
    nrmllib.assert_valid(out_file)
    return out_file


class GetResultExportDestTestCase(unittest.TestCase):

    def setUp(self):
        self.Location = namedtuple('Location', 'x, y')

        self.FakeHazardCurve = namedtuple(
            'HazardCurve',
            'output, lt_realization, imt, sa_period, statistics, quantile'
        )
        self.FakeHazardMap = namedtuple(
            'HazardMap',
            'output, lt_realization, imt, sa_period, poe, statistics, quantile'
        )
        self.FakeUHS = namedtuple(
            'UHS',
            'output, lt_realization, poe, statistics, quantile'
        )
        self.FakeDisagg = namedtuple(
            'Disagg',
            'output, lt_realization, imt, sa_period, location, poe'
        )
        self.FakeGMF = namedtuple(
            'GMF',
            'output, lt_realization'
        )
        self.FakeSES = namedtuple(
            'SES',
            'output, ordinal, sm_lt_path'
        )
        self.FakeOutput = namedtuple(
            'Output',
            'output_type'
        )
        self.FakeLTR = namedtuple(
            'LtRealization',
            'sm_lt_path, gsim_lt_path, ordinal, weight'
        )

        self.ltr_mc = self.FakeLTR(['B1', 'B3'], ['B2', 'B4'], 3, None)
        self.ltr_enum = self.FakeLTR(['B10', 'B9'], ['B7', 'B8'], 0, 0.6)

        self.target_dir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.target_dir)

    def test_hazard_curve(self):
        output = self.FakeOutput('hazard_curve')

        curves = [
            self.FakeHazardCurve(output, self.ltr_mc, 'PGA', None, None, None),
            self.FakeHazardCurve(output, self.ltr_enum, 'SA', 0.025, None,
                                 None),
            self.FakeHazardCurve(output, None, 'SA', 0.025, 'mean', None),
            self.FakeHazardCurve(output, None, 'SA', 0.025, 'quantile', 0.85),
        ]
        expected_paths = [
            '%s/calc_7/hazard_curve/PGA/'
            'hazard_curve-smltp_B1_B3-gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/hazard_curve/SA-0.025/'
            'hazard_curve-smltp_B10_B9-gsimltp_B7_B8.xml',
            '%s/calc_7/hazard_curve/SA-0.025/'
            'hazard_curve-mean.xml',
            '%s/calc_7/hazard_curve/SA-0.025/'
            'hazard_curve-quantile_0.85.xml',
        ]
        expected_paths = [x % self.target_dir for x in expected_paths]

        for i, curve in enumerate(curves):
            self.assertEqual(
                expected_paths[i],
                hazard._get_result_export_dest(7, self.target_dir, curve)
            )

    def test_hazard_curve_multi(self):
        output = self.FakeOutput('hazard_curve_multi')

        curves = [
            self.FakeHazardCurve(output, self.ltr_mc, None, None, None, None),
            self.FakeHazardCurve(output, self.ltr_enum, None, None, None,
                                 None),
            self.FakeHazardCurve(output, None, None, None, 'mean', None),
            self.FakeHazardCurve(output, None, None, None, 'quantile', 0.85),
        ]
        expected_paths = [
            '%s/calc_7/hazard_curve_multi/'
            'hazard_curve_multi-smltp_B1_B3-gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/hazard_curve_multi/'
            'hazard_curve_multi-smltp_B10_B9-gsimltp_B7_B8.xml',
            '%s/calc_7/hazard_curve_multi/'
            'hazard_curve_multi-mean.xml',
            '%s/calc_7/hazard_curve_multi/'
            'hazard_curve_multi-quantile_0.85.xml',
        ]
        expected_paths = [x % self.target_dir for x in expected_paths]

        for i, curve in enumerate(curves):
            self.assertEqual(
                expected_paths[i],
                hazard._get_result_export_dest(7, self.target_dir, curve)
            )

    def test_hazard_map(self):
        output = self.FakeOutput('hazard_map')

        maps = [
            self.FakeHazardMap(output, self.ltr_mc, 'PGA', None, 0.1,
                               None, None),
            self.FakeHazardMap(output, self.ltr_mc, 'SA', 0.025, 0.2,
                               None, None),
            self.FakeHazardMap(output, None, 'SA', 0.025, 0.3,
                               'mean', None),
            self.FakeHazardMap(output, None, 'SA', 0.025, 0.4,
                               'quantile', 0.85),
        ]
        expected_paths = [
            '%s/calc_7/hazard_map/PGA/'
            'hazard_map-poe_0.1-smltp_B1_B3-gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/hazard_map/SA-0.025/'
            'hazard_map-poe_0.2-smltp_B1_B3-gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/hazard_map/SA-0.025/'
            'hazard_map-poe_0.3-mean.xml',
            '%s/calc_7/hazard_map/SA-0.025/'
            'hazard_map-poe_0.4-quantile_0.85.xml',
        ]
        expected_paths = [x % self.target_dir for x in expected_paths]

        for i, hmap in enumerate(maps):
            self.assertEqual(
                expected_paths[i],
                hazard._get_result_export_dest(7, self.target_dir, hmap)
            )

    def test_uhs(self):
        output = self.FakeOutput('uh_spectra')

        uh_spectra = [
            self.FakeUHS(output, self.ltr_mc, 0.1, None, None),
            self.FakeUHS(output, None, 0.2, 'mean', None),
            self.FakeUHS(output, None, 0.3, 'quantile', 0.85),
        ]
        expected_paths = [
            '%s/calc_7/uh_spectra/'
            'uh_spectra-poe_0.1-smltp_B1_B3-gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/uh_spectra/uh_spectra-poe_0.2-mean.xml',
            '%s/calc_7/uh_spectra/uh_spectra-poe_0.3-quantile_0.85.xml',
        ]
        expected_paths = [x % self.target_dir for x in expected_paths]

        for i, uhs in enumerate(uh_spectra):
            self.assertEqual(
                expected_paths[i],
                hazard._get_result_export_dest(7, self.target_dir, uhs)
            )

    def test_disagg(self):
        output = self.FakeOutput('disagg_matrix')

        matrices = [
            self.FakeDisagg(output, self.ltr_mc, 'PGA', None,
                            self.Location(33.333, -89.999001), 0.1),
            self.FakeDisagg(output, self.ltr_enum, 'SA', 0.025,
                            self.Location(40.1, 10.1), 0.02),
        ]

        expected_paths = [
            '%s/calc_7/disagg_matrix/PGA/'
            'disagg_matrix(0.1)-lon_33.333-lat_-89.999001-smltp_B1_B3-'
            'gsimltp_B2_B4-ltr_3.xml',
            '%s/calc_7/disagg_matrix/SA-0.025/'
            'disagg_matrix(0.02)-lon_40.1-lat_10.1-'
            'smltp_B10_B9-gsimltp_B7_B8.xml'
        ]
        expected_paths = [x % self.target_dir for x in expected_paths]

        for i, matrix in enumerate(matrices):
            self.assertEqual(
                expected_paths[i],
                hazard._get_result_export_dest(7, self.target_dir, matrix)
            )

    def test_gmf(self):
        output = self.FakeOutput('gmf')

        gmf = self.FakeGMF(output, self.ltr_enum)
        expected_path = (
            '%s/calc_8/gmf/gmf-smltp_B10_B9-gsimltp_B7_B8.xml'
            % self.target_dir
        )

        self.assertEqual(
            expected_path,
            hazard._get_result_export_dest(8, self.target_dir, gmf)
        )

    def test_ses(self):
        output = self.FakeOutput('ses')

        ses = self.FakeSES(output, 1, self.ltr_mc.sm_lt_path)
        expected_path = (
            '%s/calc_8/ses/ses-smltp_B1_B3.xml'
            % self.target_dir
        )
        self.assertEqual(
            expected_path,
            hazard._get_result_export_dest(8, self.target_dir, ses)
        )


class ClassicalExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_classical_hazard_export(self):
        # Run a hazard calculation to compute some curves and maps
        # Call the exporter and verify that files were created
        # Since the hazard curve XML writer is concerned with correctly
        # generating XML, we won't test that here.
        target_dir = tempfile.mkdtemp()

        try:
            cfg = helpers.get_data_path('simple_fault_demo_hazard/job.ini')

            # run the calculation to create something to export
            helpers.run_job(cfg)

            job = models.OqJob.objects.latest('id')
            self.assertEqual(job.status, 'complete')

            outputs = export_core.get_outputs(job.id)

            # 10 hazard curves, 20 maps, 10 uhs, 5 multi curves
            expected_outputs = 45
            self.assertEqual(expected_outputs, outputs.count())

            # Number of curves:
            # (2 imts * 2 realizations)
            # + (2 imts * (1 mean + 2 quantiles)
            # = 10
            curves = outputs.filter(output_type='hazard_curve')
            self.assertEqual(10, curves.count())

            # Number of multi-curves
            # (2 realizations + 1 mean + 2 quantiles)
            multi_curves = outputs.filter(output_type="hazard_curve_multi")
            self.assertEqual(5, multi_curves.count())

            # Number of maps:
            # (2 poes * 2 imts * 2 realizations)
            # + (2 poes * 2 imts * (1 mean + 2 quantiles))
            # = 20
            # Number of UHS:
            maps = outputs.filter(output_type='hazard_map')
            self.assertEqual(20, maps.count())

            # Number of UHS:
            # (20 maps_PGA_SA / 2 poes)
            # = 10
            uhs = outputs.filter(output_type='uh_spectra')
            self.assertEqual(10, uhs.count())

            # Test hazard curve export:
            hc_files = []
            for curve in curves:
                hc_files.append(check_export(curve.id, target_dir))

            self.assertEqual(10, len(hc_files))

            # Test multi hazard curve export:
            hc_files = []
            for curve in multi_curves:
                hc_files.append(check_export(curve.id, target_dir))

            self.assertEqual(5, len(hc_files))

            for f in hc_files:
                self._test_exported_file(f)

            # Test hazard map export:
            hm_files = []
            for haz_map in maps:
                hm_files.append(check_export(haz_map.id, target_dir))

            self.assertEqual(20, len(hm_files))

            for f in hm_files:
                self._test_exported_file(f)

            # Test UHS export:
            uhs_files = []
            for u in uhs:
                uhs_files.append(check_export(u.id, target_dir))
            for f in uhs_files:
                self._test_exported_file(f)
        finally:
            shutil.rmtree(target_dir)


class EventBasedExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_export_for_event_based(self):
        # Run an event-based hazard calculation to compute SESs and GMFs
        # Call the exporters for both SES and GMF results  and verify that
        # files were created
        # Since the XML writers (in `openquake.nrmllib.writers`) are concerned
        # with correctly generating the XML, we don't test that here...
        # but we should still have an end-to-end QA test.
        target_dir = tempfile.mkdtemp()

        try:
            cfg = helpers.get_data_path('event_based_hazard/job.ini')

            # run the calculation in process to create something to export
            os.environ['OQ_NO_DISTRIBUTE'] = '1'
            try:
                job = helpers.run_job(cfg)
            finally:
                del os.environ['OQ_NO_DISTRIBUTE']
            self.assertEqual(job.status, 'complete')

            outputs = export_core.get_outputs(job.id)
            # 2 GMFs, 1 SES,
            # ((2 imts * 2 realizations)
            # + ((2 imts + 1 multi) * (1 mean + 3 quantiles))
            # hazard curves,
            # (2 poes * 2 imts * 2 realizations)
            # + (2 poes * 2 imts * (1 mean + 3 quantiles)) hazard maps
            # Total: 41
            self.assertEqual(43, len(outputs))

            #######
            # SESs:
            ses_outputs = outputs.filter(output_type='ses')
            self.assertEqual(1, len(ses_outputs))

            exported_files = []
            for ses_output in ses_outputs:
                out_file = check_export(ses_output.id, target_dir)
                exported_files.append(out_file)

            self.assertEqual(1, len(exported_files))

            for f in exported_files:
                self._test_exported_file(f)

            #######
            # GMFs:
            gmf_outputs = outputs.filter(output_type='gmf')
            self.assertEqual(2, len(gmf_outputs))

            exported_files = []
            for gmf_output in gmf_outputs:
                out_file = check_export(gmf_output.id, target_dir)
                exported_files.append(out_file)

            self.assertEqual(2, len(exported_files))
            # Check the file paths exist, are absolute, and the files aren't
            # empty.
            for f in exported_files:
                self._test_exported_file(f)

            ################
            # Hazard curves:
            haz_curves = outputs.filter(output_type='hazard_curve')
            self.assertEqual(12, haz_curves.count())
            for curve in haz_curves:
                exported_file = check_export(curve.id, target_dir)
                self._test_exported_file(exported_file)

            ##############
            # Hazard maps:
            haz_maps = outputs.filter(output_type='hazard_map')
            self.assertEqual(24, haz_maps.count())
            for hmap in haz_maps:
                exported_file = check_export(hmap.id, target_dir)
                self._test_exported_file(exported_file)
        finally:
            shutil.rmtree(target_dir)


class ScenarioExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_export_for_scenario(self):
        target_dir = tempfile.mkdtemp()

        try:
            cfg = helpers.get_data_path('scenario_hazard/job.ini')

            # run the calculation in process to create something to export
            os.environ['OQ_NO_DISTRIBUTE'] = '1'
            try:
                helpers.run_job(cfg)
            finally:
                del os.environ['OQ_NO_DISTRIBUTE']
            job = models.OqJob.objects.latest('id')
            self.assertEqual(job.status, 'complete')

            outputs = export_core.get_outputs(job.id)

            self.assertEqual(2, len(outputs))  # 1 GMF, 1 SES

            gmf_outputs = outputs.filter(output_type='gmf_scenario')
            self.assertEqual(1, len(gmf_outputs))

            exported_file = check_export(gmf_outputs[0].id, target_dir)

            # Check the file paths exist, is absolute, and the file isn't
            # empty.
            self._test_exported_file(exported_file)

            # Check for the correct number of GMFs in the file:
            tree = etree.parse(exported_file)
            self.assertEqual(20, number_of('nrml:gmf', tree))
        finally:
            shutil.rmtree(target_dir)


class DisaggExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_disagg_hazard_export(self):
        target_dir = tempfile.mkdtemp()

        try:
            cfg = helpers.get_data_path('disaggregation/job.ini')

            # run the calculation in process to create something to export
            os.environ['OQ_NO_DISTRIBUTE'] = '1'
            try:
                helpers.run_job(cfg)
            finally:
                del os.environ['OQ_NO_DISTRIBUTE']

            job = models.OqJob.objects.latest('id')
            self.assertEqual(job.status, 'complete')

            outputs = export_core.get_outputs(job.id)

            # Test curve export:
            curves = outputs.filter(output_type='hazard_curve')
            self.assertEqual(4, len(curves))
            curve_files = []
            for curve in curves:
                curve_files.append(check_export(curve.id, target_dir))

            self.assertEqual(4, len(curve_files))
            for f in curve_files:
                self._test_exported_file(f)

            # Test disagg matrix export:
            matrices = outputs.filter(output_type='disagg_matrix')
            self.assertEqual(8, len(matrices))
            disagg_files = []
            for matrix in matrices:
                disagg_files.append(check_export(matrix.id, target_dir))

            self.assertEqual(8, len(disagg_files))
            for f in disagg_files:
                self._test_exported_file(f)
        finally:
            shutil.rmtree(target_dir)


class Bug1202290TestCase(unittest.TestCase):
    """
    Tests to specifically address
    https://bugs.launchpad.net/oq-engine/+bug/1202290.
    """

    def test(self):
        output = mock.Mock()
        output.oq_job.hazard_calculation.id = 1202290
        output.hazard_curve = mock.Mock()
        output.hazard_curve.__iter__ = lambda x: iter([])
        target = mock.Mock()

        with mock.patch('openquake.engine.export.hazard'
                        '._get_result_export_dest') as gred:
            with mock.patch('openquake.nrmllib.hazard.writers'
                            '.MultiHazardCurveXMLWriter') as mhcxw:
                mhcxw.return_value
                mhcxw.serialize = mock.Mock()
                hazard.export_hazard_curve_multi_xml(output, target)

        self.assertEqual(1, gred.call_count)
        self.assertEqual(
            ((1202290, target, output.hazard_curve), {}),
            gred.call_args
        )
        self.assertEqual(1, mhcxw.call_count)
        self.assertEqual(1, mhcxw.return_value.serialize.call_count)

        self.assertEqual(
            ((gred.return_value, []), {}),
            mhcxw.call_args
        )

########NEW FILE########
__FILENAME__ = risk_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import shutil
import tempfile
import unittest
import mock

from nose.plugins.attrib import attr

from openquake.engine.db import models
from openquake.engine.export import risk

from openquake.engine.tests.export.core_test import BaseExportTestCase
from openquake.engine.tests.utils import helpers


class ExportTestCase(unittest.TestCase):

    def setUp(self):
        self.output_mock = mock.Mock()
        self.output_patch = helpers.patch(
            'openquake.engine.db.models.Output.objects.get')
        m = self.output_patch.start()
        m.return_value = self.output_mock

        self.output_mock.hazard_metadata.investigation_time = 30
        self.output_mock.hazard_metadata.statistics = "mean"
        self.output_mock.hazard_metadata.quantile = None
        self.output_mock.hazard_metadata.sm_path = None
        self.output_mock.hazard_metadata.gsim_path = None
        rc = self.output_mock.oq_job.risk_calculation
        rc.exposure_model.category = "air"
        rc.exposure_model.unit = mock.Mock(return_value="bucks")
        rc.interest_rate = 0.3
        rc.asset_life_expectancy = 10

    def tearDown(self):
        self.output_patch.stop()

    def test_export_agg_loss_curve(self):
        writer = 'openquake.nrmllib.risk.writers.AggregateLossCurveXMLWriter'

        self.output_mock.loss_curve.id = 0
        self.output_mock.loss_curve.loss_type = "structural"
        self.output_mock.output_type = 'agg_loss_curve'
        with mock.patch(writer) as m:
            ret = risk.export_agg_loss_curve_xml(self.output_mock, "/tmp/")

            self.assertEqual([(('/tmp/loss-curves-0.xml', ),
                              {'gsim_tree_path': None,
                               'investigation_time': 30,
                               'quantile_value': None,
                               'source_model_tree_path': None,
                               'statistics': 'mean',
                               'unit': 'bucks',
                               'loss_type': 'structural'})], m.call_args_list)
            self.assertEqual('/tmp/loss-curves-0.xml', ret)

    def test_export_loss_curve(self):
        writer = 'openquake.nrmllib.risk.writers.LossCurveXMLWriter'

        self.output_mock.loss_curve.id = 0
        self.output_mock.loss_curve.insured = False
        self.output_mock.loss_curve.loss_type = "structural"
        self.output_mock.output_type = 'loss_curve'

        with mock.patch(writer) as m:
            ret = risk.export_loss_curve_xml(self.output_mock, "/tmp/")

            self.assertEqual([(('/tmp/loss-curves-0.xml', ),
                              {'gsim_tree_path': None,
                               'investigation_time': 30,
                               'insured': False,
                               'quantile_value': None,
                               'source_model_tree_path': None,
                               'statistics': 'mean',
                               'unit': 'bucks',
                               'loss_type': 'structural'})], m.call_args_list)
            self.assertEqual('/tmp/loss-curves-0.xml', ret)

    def test_export_loss_map(self):
        writer = 'openquake.nrmllib.risk.writers.LossMapXMLWriter'

        self.output_mock.loss_map.id = 0
        self.output_mock.loss_map.poe = 0.1
        self.output_mock.loss_map.loss_type = "structural"
        self.output_mock.output_type = 'loss_map'

        with mock.patch(writer) as m:
            ret = risk.export_loss_map_xml(self.output_mock, "/tmp/")

            self.assertEqual([(('/tmp/loss-maps-0.xml', ),
                              {'gsim_tree_path': None,
                               'investigation_time': 30,
                               'loss_category': 'air',
                               'poe': 0.1,
                               'quantile_value': None,
                               'source_model_tree_path': None,
                               'statistics': 'mean',
                               'unit': 'bucks',
                               'loss_type': 'structural'})], m.call_args_list)
            self.assertEqual('/tmp/loss-maps-0.xml', ret)

    def test_export_bcr_distribution(self):
        writer = 'openquake.nrmllib.risk.writers.BCRMapXMLWriter'

        self.output_mock.bcr_distribution.id = 0
        self.output_mock.bcr_distribution.loss_type = "structural"
        self.output_mock.output_type = 'bcr_distribution'

        with mock.patch(writer) as m:
            ret = risk.export_bcr_distribution_xml(self.output_mock, "/tmp/")

            self.assertEqual([(('/tmp/bcr-distribution-0.xml', ),
                              {'asset_life_expectancy': 10,
                               'gsim_tree_path': None,
                               'interest_rate': 0.3,
                               'quantile_value': None,
                               'source_model_tree_path': None,
                               'statistics': 'mean',
                               'unit': 'bucks',
                               'loss_type': 'structural'})], m.call_args_list)
            self.assertEqual('/tmp/bcr-distribution-0.xml', ret)

    def test_export_aggregate_loss(self):
        writer = 'csv.writer'

        self.output_mock.aggregate_loss.id = 0
        self.output_mock.aggregate_loss.mean = 1
        self.output_mock.aggregate_loss.std_dev = 2
        self.output_mock.aggregate_loss.loss_type = "structural"
        self.output_mock.output_type = 'aggregate_loss'

        with mock.patch(writer) as m:
            ret = risk.export_aggregate_loss(self.output_mock, "/tmp/")

            self.assertEqual([], m.writerow.call_args_list)
            self.assertEqual("/tmp/aggregate-loss-0.csv", ret)


class ClassicalExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_classical_risk_export(self):
        target_dir = tempfile.mkdtemp()
        try:
            haz_cfg = helpers.get_data_path(
                'end-to-end-hazard-risk/job_haz_classical.ini'
            )
            risk_cfg = helpers.get_data_path(
                'end-to-end-hazard-risk/job_risk_classical.ini'
            )

            haz_job = helpers.run_job(haz_cfg)
            # Run the risk on all outputs produced by the haz calc:
            risk_job = helpers.run_job(
                risk_cfg, hazard_calculation_id=haz_job.hazard_calculation.id
            )

            risk_outputs = models.Output.objects.filter(oq_job=risk_job)

            loss_curve_outputs = risk_outputs.filter(output_type='loss_curve')
            loss_map_outputs = risk_outputs.filter(output_type='loss_map')

            # 16 logic tree realizations + 1 mean + 2 quantiles = 19
            # + 19 insured loss curves
            self.assertEqual(38, loss_curve_outputs.count())
            # make sure the mean and quantile curve sets got created correctly
            loss_curves = models.LossCurve.objects.filter(
                output__oq_job=risk_job,
                insured=False
            )
            # sanity check
            self.assertEqual(19, loss_curves.count())

            insured_curves = models.LossCurve.objects.filter(
                output__oq_job=risk_job,
                insured=True
            )
            # sanity check
            self.assertEqual(19, insured_curves.count())

            # mean
            self.assertEqual(1, loss_curves.filter(statistics='mean').count())
            # quantiles
            self.assertEqual(
                2, loss_curves.filter(statistics='quantile').count()
            )

            # mean
            self.assertEqual(
                1, insured_curves.filter(statistics='mean').count())
            # quantiles
            self.assertEqual(
                2, insured_curves.filter(statistics='quantile').count()
            )

            # 16 logic tree realizations = 16 loss map + 1 mean loss
            # map + 2 quantile loss map
            self.assertEqual(19, loss_map_outputs.count())

            # 19 loss fractions
            loss_fraction_outputs = risk_outputs.filter(
                output_type="loss_fraction")
            self.assertEqual(19, loss_fraction_outputs.count())

            # Now try to export everything, just to do a "smoketest" of the
            # exporter code:
            loss_curve_files = []
            for o in loss_curve_outputs:
                loss_curve_files.append(risk.export(o.id, target_dir, 'xml'))

            loss_map_files = []
            for o in loss_map_outputs:
                loss_map_files.append(risk.export(o.id, target_dir, 'xml'))

            self.assertEqual(38, len(loss_curve_files))
            self.assertEqual(19, len(loss_map_files))

            for f in loss_curve_files:
                self._test_exported_file(f)
            for f in loss_map_files:
                self._test_exported_file(f)
        finally:
            shutil.rmtree(target_dir)


class EventBasedExportTestCase(BaseExportTestCase):

    @attr('slow')
    def test_event_based_risk_export(self):
        target_dir = tempfile.mkdtemp()
        try:
            haz_cfg = helpers.get_data_path(
                'end-to-end-hazard-risk/job_haz_event_based.ini'
            )
            risk_cfg = helpers.get_data_path(
                'end-to-end-hazard-risk/job_risk_event_based.ini'
            )

            haz_job = helpers.run_job(haz_cfg)
            # Run the risk on all outputs produced by the haz calc:
            risk_job = helpers.run_job(
                risk_cfg, hazard_calculation_id=haz_job.hazard_calculation.id
            )

            risk_outputs = models.Output.objects.filter(oq_job=risk_job)

            agg_loss_curve_outputs = risk_outputs.filter(
                output_type='agg_loss_curve')
            loss_curve_outputs = risk_outputs.filter(output_type='loss_curve')
            loss_map_outputs = risk_outputs.filter(output_type='loss_map')

            # (1 mean + 2 quantiles) * 2 (as there also insured curves)
            self.assertEqual(6, loss_curve_outputs.count())

            # 16 rlzs + 16 (due to insured curves)
            event_loss_curve_outputs = risk_outputs.filter(
                output_type='event_loss_curve')
            self.assertEqual(32, event_loss_curve_outputs.count())
            self.assertEqual(16, agg_loss_curve_outputs.count())

            # make sure the mean and quantile curve sets got created correctly
            loss_curves = models.LossCurve.objects.filter(
                output__oq_job=risk_job
            )
            # sanity check (16 aggregate loss curve + 38 loss curves)
            self.assertEqual(54, loss_curves.count())
            # mean
            self.assertEqual(2, loss_curves.filter(statistics='mean').count())
            # quantiles
            self.assertEqual(
                4, loss_curves.filter(statistics='quantile').count()
            )

            # 16 logic tree realizations = 16 loss map + 1 mean loss
            # map + 2 quantile loss map
            self.assertEqual(19, loss_map_outputs.count())

            # 16 event loss table (1 per rlz)
            event_loss_tables = risk_outputs.filter(output_type="event_loss")
            self.assertEqual(16, event_loss_tables.count())

            # 32 loss fractions
            loss_fraction_outputs = risk_outputs.filter(
                output_type="loss_fraction")
            self.assertEqual(32, loss_fraction_outputs.count())

            # Now try to export everything, just to do a "smoketest" of the
            # exporter code:
            loss_curve_files = []
            for o in loss_curve_outputs:
                loss_curve_files.append(risk.export(o.id, target_dir, 'xml'))
            for o in loss_fraction_outputs:
                loss_curve_files.append(risk.export(o.id, target_dir, 'xml'))
            for o in event_loss_curve_outputs:
                loss_curve_files.append(risk.export(o.id, target_dir, 'xml'))

            agg_loss_curve_files = []
            for o in agg_loss_curve_outputs:
                agg_loss_curve_files.append(
                    risk.export(o.id, target_dir, 'xml')
                )

            event_loss_table_files = []
            for o in event_loss_tables:
                event_loss_table_files.append(
                    risk.export(o.id, target_dir, 'xml')
                )

            loss_map_files = []
            for o in loss_map_outputs:
                loss_map_files.append(risk.export(o.id, target_dir, 'xml'))

            self.assertEqual(70, len(loss_curve_files))
            self.assertEqual(16, len(agg_loss_curve_files))
            self.assertEqual(16, len(event_loss_table_files))
            self.assertEqual(19, len(loss_map_files))

            for f in loss_curve_files:
                self._test_exported_file(f)
            for f in loss_map_files:
                self._test_exported_file(f)
        finally:
            shutil.rmtree(target_dir)

########NEW FILE########
__FILENAME__ = logictree_test
# -*- coding: utf-8 -*-

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Tests for python logic tree processor.
"""

import os
import os.path
import unittest

from openquake.nrmllib.parsers import SourceModelParser

from openquake.commonlib import logictree
from openquake.commonlib.source import NrmlHazardlibConverter
from openquake.commonlib.general import deep_eq

from openquake.engine.calculators.hazard.general import make_gsim_lt

from openquake.engine.tests.utils import helpers


class LogicTreeProcessorTestCase(unittest.TestCase):
    def setUp(self):
        # this is an example with number_of_logic_tree_samples = 1
        cfg = helpers.get_data_path('classical_job.ini')
        job = helpers.get_job(cfg)
        self.source_model_lt = logictree.SourceModelLogicTree.from_hc(
            job.hazard_calculation)
        self.gmpe_lt = make_gsim_lt(
            job.hazard_calculation,
            ['Active Shallow Crust', 'Subduction Interface'])

    def test_sample_source_model(self):
        [(sm_name, weight, branch_ids)] = self.source_model_lt
        self.assertEqual(sm_name, 'example-source-model.xml')
        self.assertIsNone(weight)
        self.assertEqual(('b1', 'b5', 'b8'), branch_ids)

    def test_sample_gmpe(self):
        [(value, weight, branch_ids)] = self.gmpe_lt
        self.assertEqual(value,
                         {'Subduction Interface': 'SadighEtAl1997',
                          'Active Shallow Crust': 'ChiouYoungs2008'})
        self.assertIsNone(weight)
        self.assertEqual(('b2', 'b3'), branch_ids)


class LogicTreeProcessorParsePathTestCase(unittest.TestCase):
    def setUp(self):
        cfg = helpers.get_data_path('classical_job.ini')
        job = helpers.get_job(cfg)
        self.uncertainties_applied = []

        def apply_uncertainty(branchset, value, source):
            fingerprint = (branchset.uncertainty_type, value)
            self.uncertainties_applied.append(fingerprint)
        self.original_apply_uncertainty = logictree.BranchSet.apply_uncertainty
        logictree.BranchSet.apply_uncertainty = apply_uncertainty

        self.source_model_lt = logictree.SourceModelLogicTree.from_hc(
            job.hazard_calculation)
        self.gmpe_lt = make_gsim_lt(
            job.hazard_calculation,
            ['Active Shallow Crust', 'Subduction Interface'])

    def tearDown(self):
        logictree.BranchSet.apply_uncertainty = self.original_apply_uncertainty

    def test_parse_source_model_logictree_path(self):
        make_apply_un = self.source_model_lt.make_apply_uncertainties
        make_apply_un(['b1', 'b5', 'b8'])(None)
        self.assertEqual(self.uncertainties_applied,
                         [('maxMagGRRelative', -0.2),
                          ('bGRRelative', -0.1)])
        del self.uncertainties_applied[:]
        make_apply_un(['b1', 'b3', 'b6'])(None)
        self.assertEqual(self.uncertainties_applied,
                         [('maxMagGRRelative', 0.2),
                          ('bGRRelative', 0.1)])


class _BaseSourceModelLogicTreeBlackboxTestCase(unittest.TestCase):
    JOB_CONFIG = None

    def _do_test(self, path, expected_result, expected_branch_ids):
        cfg = helpers.get_data_path(self.JOB_CONFIG)
        job = helpers.get_job(cfg)

        nrml_to_hazardlib = NrmlHazardlibConverter(job.hazard_calculation)
        base_path = job.hazard_calculation.base_path

        source_model_lt = logictree.SourceModelLogicTree.from_hc(
            job.hazard_calculation)

        [branch] = source_model_lt.root_branchset.branches
        all_branches = source_model_lt.branches
        path = iter(path)
        while branch.child_branchset is not None:
            nextbranch = all_branches[next(path)]
            branch.child_branchset.sample = (
                lambda nextbranch: lambda rnd: nextbranch)(nextbranch)
            branch = nextbranch
        assert list(path) == []

        [(sm_path, weight, branch_ids)] = source_model_lt
        branch_ids = list(branch_ids)
        self.assertEqual(expected_branch_ids, branch_ids)
        modify_source = source_model_lt.make_apply_uncertainties(branch_ids)

        expected_result_path = os.path.join(base_path, expected_result)
        e_nrml_sources = SourceModelParser(expected_result_path).parse()
        e_hazardlib_sources = [
            nrml_to_hazardlib(source)
            for source in e_nrml_sources]

        original_sm_path = os.path.join(base_path, sm_path)
        a_nrml_sources = SourceModelParser(original_sm_path).parse()
        a_hazardlib_sources = [
            nrml_to_hazardlib(source)
            for source in a_nrml_sources]
        for i, source in enumerate(a_hazardlib_sources):
            modify_source(source)

        self.assertEqual(len(e_hazardlib_sources), len(a_hazardlib_sources))
        for i in xrange(len(e_hazardlib_sources)):
            expected_source = e_hazardlib_sources[i]
            actual_source = a_hazardlib_sources[i]
            self.assertTrue(*deep_eq(expected_source, actual_source))


class RelSMLTBBTestCase(_BaseSourceModelLogicTreeBlackboxTestCase):
    JOB_CONFIG = helpers.get_data_path(
        'LogicTreeRelativeUncertaintiesTest/rel_uncert.ini')

    def test_b4(self):
        self._do_test(['b2', 'b4'], 'result_b4.xml', ['b1', 'b2', 'b4'])

    def test_b5(self):
        self._do_test(['b2', 'b5'], 'result_b5.xml', ['b1', 'b2', 'b5'])

    def test_b6(self):
        self._do_test(['b3', 'b6'], 'result_b6.xml', ['b1', 'b3', 'b6'])

    def test_b7(self):
        self._do_test(['b3', 'b7'], 'result_b7.xml', ['b1', 'b3', 'b7'])


class AbsSMLTBBTestCase(_BaseSourceModelLogicTreeBlackboxTestCase):
    JOB_CONFIG = helpers.get_data_path(
        'LogicTreeAbsoluteUncertaintiesTest/abs_uncert.ini')

    def test_b4(self):
        self._do_test(['b2', 'b4'], 'result_b4.xml', ['b1', 'b2', 'b4'])

    def test_b5(self):
        self._do_test(['b2', 'b5'], 'result_b5.xml', ['b1', 'b2', 'b5'])

    def test_b7(self):
        self._do_test(['b3', 'b7'], 'result_b7.xml', ['b1', 'b3', 'b7'])

    def test_b8(self):
        self._do_test(
            ['b3', 'b6', 'b8'], 'result_b8.xml', ['b1', 'b3', 'b6', 'b8'])

    def test_b9(self):
        self._do_test(
            ['b3', 'b6', 'b9'], 'result_b9.xml', ['b1', 'b3', 'b6', 'b9'])

########NEW FILE########
__FILENAME__ = validation_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


import itertools
import json
import unittest
import warnings

from openquake.engine import engine
from openquake.engine.db import models
from openquake.engine.job import validation

from openquake.engine.tests.utils import helpers
from openquake.engine.tests.utils.helpers import get_data_path

from openquake.commonlib.general import deep_eq


VALID_IML_IMT = {
    "PGV": [0.005, 0.007, 0.0098],
    "IA": [0.005, 0.007, 0.0098],
    "PGD": [0.005, 0.007, 0.0098],
    "MMI": [0.005, 0.007, 0.0098],
    "PGA": [0.007, 0.005, 0.0098],
    "RSD": [0.005, 0.007, 0.0098],
    "SA(0)": [0.005, 0.007, 0.0098],
    "SA(0.025)": [0.005, 0.007, 0.0098],
    "SA(2.5)": [0.005, 0.007, 0.0098],
    "SA(0.45)": [0.005, 0.007, 0.0098],
}

VALID_IML_IMT_STR = json.dumps(VALID_IML_IMT)

INVALID_IML_IMT = {
    "PGZ": [0.005, 0.007, 0.0098],
    "IA": [0.0, 0.007, 0.0098],
    "PGD": [],
    "MMI": (0.005, 0.007, 0.0098),
    "PGA": [-0.001, 0.6, 0.0098],
    "RSD": [0.005, 0.007, 0.0098],
    "SA(-0.1)": [0.005, 0.007, 0.0098],
    "SA(0.025)": [],
    "SA<2.5>": [0.005, 0.007, 0.0098],
    "SA(0.45)": [0.005, 0.007, 0.0098],
    "SA(2x)": [0.005, 0.007, 0.0098],
}


class ClassicalHazardFormTestCase(unittest.TestCase):
    """Tests for classical hazard job param validation."""

    def setUp(self):
        self.hc = models.HazardCalculation.create(
            description='',
            region=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'
            ),
            region_grid_spacing=0.001,
            calculation_mode='classical',
            random_seed=37,
            number_of_logic_tree_samples=1,
            rupture_mesh_spacing=0.001,
            width_of_mfd_bin=0.001,
            area_source_discretization=0.001,
            reference_vs30_value=0.001,
            reference_vs30_type='measured',
            reference_depth_to_2pt5km_per_sec=0.001,
            reference_depth_to_1pt0km_per_sec=0.001,
            investigation_time=1.0,
            intensity_measure_types_and_levels=VALID_IML_IMT,
            truncation_level=0.0,
            maximum_distance=100.0,
            mean_hazard_curves=True,
            quantile_hazard_curves=[0.0, 0.5, 1.0],
            poes=[1.0, 0.5, 0.0],
        )

    def test_hazard_calculation_is_valid_region_only(self):
        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_hazard_calculation_is_valid_with_site_model(self):
        form = validation.ClassicalHazardForm(
            instance=self.hc, files=dict(site_model_file=object())
        )
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_hazard_calculation_is_valid_sites_only(self):
        self.hc.region = None
        self.hc.region_grid_spacing = None
        self.hc.sites = 'MULTIPOINT((-122.114 38.113))'
        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_hazard_calculation_is_not_valid_missing_geom(self):
        expected_errors = {
            'region': [
                'Must specify either `region`, `sites` or `exposure_file`.'],
            'sites': [
                'Must specify either `region`, `sites` or `exposure_file`.'],
        }
        self.hc.region = None
        self.hc.sites = None
        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )
        self.assertFalse(form.is_valid())

        self.assertEqual(expected_errors, dict(form.errors))

    def test_hazard_calculation_is_not_valid(self):
        # test with an invalid job profile
        # several parameters are given invalid values
        expected_errors = {
            'area_source_discretization': [
                'Area source discretization must be > 0',
            ],
            'calculation_mode': [
                'Calculation mode must be "classical"',
            ],
            'investigation_time': ['Investigation time must be > 0'],
            'maximum_distance': ['Maximum distance must be > 0'],
            'number_of_logic_tree_samples': [
                'Number of logic tree samples must be >= 0',
            ],
            'poes': [
                '`poes` values must be in the range [0, 1]',
            ],
            'quantile_hazard_curves': [
                'Quantile hazard curve values must in the range [0, 1]'
            ],
            'random_seed': [
                'Random seed must be a value from -2147483648 to 2147483647 '
                '(inclusive)',
            ],
            'rupture_mesh_spacing': ['Rupture mesh spacing must be > 0'],
            'truncation_level': ['Truncation level must be >= 0'],
            'width_of_mfd_bin': ['Width of MFD bin must be > 0'],
            'intensity_measure_types_and_levels': [
                'SA(-0.1): SA period values must be >= 0',
                ('SA<2.5>: SA must be specified with a period value, in the '
                 'form `SA(N)`, where N is a value >= 0'),
                'IA: IMLs must be > 0',
                'PGD: IML lists must have at least 1 value',
                'SA(2x): SA period value should be a float >= 0',
                'PGA: IMLs must be > 0',
                'PGZ: Invalid intensity measure type',
                'SA(0.025): IML lists must have at least 1 value',
                'MMI: IMLs must be specified as a list of floats',
            ],
            'region': ['Cannot specify `region` and `sites`. Choose one.'],
            'reference_vs30_value': ['Reference VS30 value must be > 0'],
            'reference_vs30_type': [
                'Reference VS30 type must be either "measured" or "inferred"',
            ],
            'reference_depth_to_1pt0km_per_sec': [
                'Reference depth to 1.0 km/sec must be > 0',
            ],
            'reference_depth_to_2pt5km_per_sec': [
                'Reference depth to 2.5 km/sec must be > 0',
            ],

        }

        hc = models.HazardCalculation.create(
            description='',
            region=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'
            ),
            region_grid_spacing=0,
            sites='-122.0  38.113 , -122.114,38.113',
            calculation_mode='Classical',
            random_seed=2147483648,
            number_of_logic_tree_samples=-1,
            rupture_mesh_spacing=0,
            width_of_mfd_bin=0,
            area_source_discretization=0,
            reference_vs30_type=None,
            reference_vs30_value=0,
            reference_depth_to_2pt5km_per_sec=0,
            reference_depth_to_1pt0km_per_sec=0,
            investigation_time=0,
            intensity_measure_types_and_levels=INVALID_IML_IMT,
            truncation_level=-0.1,
            maximum_distance=0,
            quantile_hazard_curves=[0.0, -0.1, 1.1],
            poes=[1.00001, -0.5, 0.0],
        )

        form = validation.ClassicalHazardForm(
            instance=hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_calculation_is_not_valid_region_only(self):
        expected_errors = {
            'region_grid_spacing': ['Region grid spacing must be > 0'],
            'region': [
                'Invalid region geomerty: Self-intersection[0 0]',
                'Region geometry can only be a single linear ring',
                'Longitude values must in the range [-180, 180]',
                'Latitude values must be in the range [-90, 90]'],
        }

        self.hc.region_grid_spacing = 0
        self.hc.region = (
            'POLYGON((-180.001 90.001, 180.001 -90.001, -179.001 -89.001, '
            '179.001 89.001, -180.001 90.001), (1 1, 2 2, 3 3, 4 4, 1 1))'
        )

        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_calculation_is_not_valid_missing_grid_spacing(self):
        expected_errors = {
            'region': ['`region` requires `region_grid_spacing`'],
        }

        self.hc.region_grid_spacing = None

        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_calculation_is_not_valid_sites_only(self):
        expected_errors = {
            'sites': [
                'Longitude values must in the range [-180, 180]',
                'Latitude values must be in the range [-90, 90]',
            ],
        }

        self.hc.region = None
        self.hc.region_grid_spacing = None
        self.hc.sites = 'MULTIPOINT((-180.001 90.001), (180.001 -90.001))'

        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_calculation_is_not_valid_missing_export_dir(self):
        # When the user specifies '--exports' on the command line the
        # 'export_dir' parameter must be present in the .ini file.
        err = ('--exports specified on the command line but the '
               '"export_dir" parameter is missing in the .ini file')
        expected_errors = {
            'export_dir': [err],
        }

        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None, exports=['xml']
        )
        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_classical_hc_hazard_maps_uhs_no_poes(self):
        # Test that errors are reported if `hazard_maps` and
        # `uniform_hazard_spectra` are `true` but no `poes` are
        # specified.
        expected_errors = {
            'hazard_maps': ['`poes` are required to compute hazard maps'],
            'uniform_hazard_spectra': ['`poes` are required to compute UHS'],
        }

        self.hc.hazard_maps = True
        self.hc.uniform_hazard_spectra = True
        self.hc.poes = None

        form = validation.ClassicalHazardForm(
            instance=self.hc, files=None
        )
        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_is_valid_warns(self):
        # `is_valid` should warn if we specify a `vulnerability_file` as well
        # as `intensity_measure_types_and_levels`
        form = validation.ClassicalHazardForm(
            instance=self.hc, files=dict(
                structural_vulnerability=object())
        )

        with warnings.catch_warnings(record=True) as w:
            form.is_valid()

        expected_warnings = [
            '`intensity_measure_types_and_levels` is ignored when a '
            '`vulnerability_file` is specified',
        ]

        actual_warnings = [m.message.message for m in w]
        self.assertEqual(expected_warnings, actual_warnings)


class EventBasedHazardFormTestCase(unittest.TestCase):

    def setUp(self):
        subset_iml_imt = VALID_IML_IMT.copy()
        subset_iml_imt.pop('PGA')

        self.hc = models.HazardCalculation.create(
            description='',
            region=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'
            ),
            region_grid_spacing=0.001,
            calculation_mode='event_based',
            random_seed=37,
            number_of_logic_tree_samples=1,
            rupture_mesh_spacing=0.001,
            width_of_mfd_bin=0.001,
            area_source_discretization=0.001,
            reference_vs30_value=0.001,
            reference_vs30_type='measured',
            reference_depth_to_2pt5km_per_sec=0.001,
            reference_depth_to_1pt0km_per_sec=0.001,
            investigation_time=1.0,
            intensity_measure_types=VALID_IML_IMT.keys(),
            # intensity_measure_types_and_levels just needs to be a subset of
            # intensity_measure_types
            intensity_measure_types_and_levels=subset_iml_imt,
            truncation_level=0.0,
            maximum_distance=100.0,
            ses_per_logic_tree_path=5,
            ground_motion_correlation_model='JB2009',
            ground_motion_correlation_params={"vs30_clustering": True},
            ground_motion_fields=True,
            hazard_curves_from_gmfs=True,
            mean_hazard_curves=True,
            quantile_hazard_curves=[0.5, 0.95],
            poes=[0.1, 0.2],
        )

    def test_valid_event_based_params(self):
        form = validation.EventBasedHazardForm(
            instance=self.hc, files=None
        )

        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_ses_per_logic_tree_path_is_not_valid(self):
        expected_errors = {
            'ses_per_logic_tree_path': [
                '`Stochastic Event Sets Per Sample` (ses_per_logic_tree_path) '
                'must be > 0'],
        }

        self.hc.ses_per_logic_tree_path = -1

        form = validation.EventBasedHazardForm(
            instance=self.hc, files=None
        )
        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_invalid_imts(self):
        expected_errors = {
            'intensity_measure_types': [
                'SA(-0.1): SA period values must be >= 0',
                ('SA<2.5>: SA must be specified with a period value, in the '
                 'form `SA(N)`, where N is a value >= 0'),
                'SA(2x): SA period value should be a float >= 0',
                'PGZ: Invalid intensity measure type',
            ],
        }

        self.hc.intensity_measure_types = INVALID_IML_IMT.keys()
        self.hc.intensity_measure_types_and_levels = None
        self.hc.hazard_curves_from_gmfs = False

        form = validation.EventBasedHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_curves_from_gmf_no_iml_imt(self):
        # Test a configuration where the user has requested to post-process
        # GMFs into hazard curves.
        # In this case, the configuration is missing the required
        # `intensity_measure_types_and_levels`.
        expected_errors = {
            'intensity_measure_types_and_levels': [
                '`hazard_curves_from_gmfs` requires '
                '`intensity_measure_types_and_levels`'],
        }

        self.hc.intensity_measure_types_and_levels = None

        form = validation.EventBasedHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_hazard_curves_from_gmf_invalid_iml_imt(self):
        # Test a configuration where the user has requested to post-process
        # GMFs into hazard curves.
        # In this case, the configuration has the required
        # `intensity_measure_types_and_levels`, but the IMTs are not a subset
        # of `intensity_measure_types`.
        expected_errors = {
            'intensity_measure_types_and_levels': [
                'Unknown IMT(s) [SA(0)] in `intensity_measure_types`'],
        }
        iml_imt = VALID_IML_IMT.keys()
        iml_imt.pop()

        self.hc.intensity_measure_types = iml_imt
        self.hc.intensity_measure_types_and_levels = VALID_IML_IMT

        form = validation.EventBasedHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_is_valid_warns(self):
        # `is_valid` should warn if we specify a `vulnerability_file` as well
        # as `intensity_measure_types` and `intensity_measure_types_and_levels`
        subset_iml_imt = VALID_IML_IMT.copy()
        subset_iml_imt.pop('PGA')

        # intensity_measure_types_and_levels just needs to be a subset of
        # intensity_measure_types
        self.hc.intensity_measure_types_and_levels = subset_iml_imt

        form = validation.EventBasedHazardForm(
            instance=self.hc, files=dict(
                structural_vulnerability=object())
        )

        with warnings.catch_warnings(record=True) as w:
            form.is_valid()

        expected_warnings = [
            '`intensity_measure_types_and_levels` is ignored when a '
            '`vulnerability_file` is specified',
            '`intensity_measure_types` is ignored when a `vulnerability_file` '
            'is specified',
        ]

        actual_warnings = [m.message.message for m in w]
        self.assertEqual(sorted(expected_warnings), sorted(actual_warnings))

    def test_gmfs_false_hazard_curves_true(self):
        # An error should be raised if `hazard_curves_from_gmfs` is `True`, but
        # `ground_motion_fields` is `False`.
        # GMFs are needed to compute hazard curves.
        expected_errors = {
            'hazard_curves_from_gmfs': ['`hazard_curves_from_gmfs` requires '
                                        '`ground_motion_fields` to be `true`'],
        }
        self.hc.ground_motion_fields = False
        self.hc.hazard_curves_from_gmfs = True

        form = validation.EventBasedHazardForm(instance=self.hc, files=None)

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)


class DisaggHazardFormTestCase(unittest.TestCase):

    def setUp(self):
        self.hc = models.HazardCalculation.create(
            description='',
            sites='MULTIPOINT((-122.114 38.113))',
            calculation_mode='disaggregation',
            random_seed=37,
            number_of_logic_tree_samples=1,
            rupture_mesh_spacing=0.001,
            width_of_mfd_bin=0.001,
            area_source_discretization=0.001,
            reference_vs30_value=0.001,
            reference_vs30_type='measured',
            reference_depth_to_2pt5km_per_sec=0.001,
            reference_depth_to_1pt0km_per_sec=0.001,
            investigation_time=1.0,
            intensity_measure_types_and_levels=VALID_IML_IMT_STR,
            truncation_level=0.1,
            maximum_distance=100.0,
            mag_bin_width=0.3,
            distance_bin_width=10.0,
            coordinate_bin_width=0.02,  # decimal degrees
            num_epsilon_bins=4,
            poes_disagg=[0.02, 0.1],
        )

    def test_valid_disagg_calc(self):
        form = validation.DisaggHazardForm(
            instance=self.hc, files=None
        )
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_disagg_calc(self):
        expected_errors = {
            'mag_bin_width': ['Magnitude bin width must be > 0.0'],
            'distance_bin_width': ['Distance bin width must be > 0.0'],
            'coordinate_bin_width': ['Coordinate bin width must be > 0.0'],
            'num_epsilon_bins': ['Number of epsilon bins must be > 0'],
            'truncation_level': ['Truncation level must be > 0 for'
                                 ' disaggregation calculations'],
            'poes_disagg': ['PoEs for disaggregation must be in the range'
                            ' [0, 1]'],
        }

        self.hc.mag_bin_width = 0.0
        self.hc.distance_bin_width = 0.0
        self.hc.coordinate_bin_width = 0.0  # decimal degrees
        self.hc.num_epsilon_bins = 0
        self.hc.poes_disagg = [1.00001, -0.5, 0.0]
        self.hc.truncation_level = 0.0

        form = validation.DisaggHazardForm(instance=self.hc, files=None)

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

        # test with an empty `poes_disagg` list
        self.hc.poes_disagg = []
        form = validation.DisaggHazardForm(instance=self.hc, files=None)
        expected_errors['poes_disagg'] = [(
            '`poes_disagg` must contain at least 1 value')]
        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_invalid_disagg_calc_truncation_not_set(self):
        expected_errors = {
            'mag_bin_width': ['Magnitude bin width must be > 0.0'],
            'distance_bin_width': ['Distance bin width must be > 0.0'],
            'coordinate_bin_width': ['Coordinate bin width must be > 0.0'],
            'num_epsilon_bins': ['Number of epsilon bins must be > 0'],
            'truncation_level': ['Truncation level must be set for'
                                 ' disaggregation calculations'],
            'poes_disagg': ['PoEs for disaggregation must be in the range'
                            ' [0, 1]'],
        }

        self.hc.truncation_level = None
        self.hc.mag_bin_width = 0.0
        self.hc.distance_bin_width = 0.0
        self.hc.coordinate_bin_width = 0.0  # decimal degrees
        self.hc.num_epsilon_bins = 0
        self.hc.poes_disagg = [1.00001, -0.5, 0.0]

        form = validation.DisaggHazardForm(instance=self.hc, files=None)

        self.assertFalse(form.is_valid())
        deep_eq(expected_errors, dict(form.errors))

    def test_is_valid_warns(self):
        # `is_valid` should warn if we specify a `vulnerability_file` as well
        # as `intensity_measure_types_and_levels`
        form = validation.DisaggHazardForm(
            instance=self.hc, files=dict(structural_vulnerability=object()))

        with warnings.catch_warnings(record=True) as w:
            form.is_valid()

        expected_warnings = [
            '`intensity_measure_types_and_levels` is ignored when a '
            '`vulnerability_file` is specified',
        ]

        actual_warnings = [m.message.message for m in w]
        self.assertEqual(expected_warnings, actual_warnings)


class ScenarioFormTestCase(unittest.TestCase):

    def setUp(self):
        self.hc = models.HazardCalculation.create(
            description='',
            sites='MULTIPOINT((-122.114 38.113))',
            calculation_mode='scenario',
            random_seed=37,
            rupture_mesh_spacing=0.001,
            reference_vs30_value=0.001,
            reference_vs30_type='measured',
            reference_depth_to_2pt5km_per_sec=0.001,
            reference_depth_to_1pt0km_per_sec=0.001,
            intensity_measure_types=VALID_IML_IMT.keys(),
            truncation_level=0.1,
            maximum_distance=100.0,
            gsim='BooreAtkinson2008',
            ground_motion_correlation_model='JB2009',
            number_of_ground_motion_fields=10,
        )

    def test_valid_scenario_calc(self):
        form = validation.ScenarioHazardForm(
            instance=self.hc, files=None
        )
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_scenario_calc(self):
        expected_errors = {
            'gsim': ["The gsim u'BooreAtkinson208' is not in in \
openquake.hazardlib.gsim"],
            'number_of_ground_motion_fields': [
                'The number_of_ground_motion_fields must be a positive '
                'integer, got -10']
        }
        self.hc.number_of_ground_motion_fields = -10
        self.hc.gsim = 'BooreAtkinson208'
        form = validation.ScenarioHazardForm(
            instance=self.hc, files=None
        )

        self.assertFalse(form.is_valid())
        equal, err = deep_eq(expected_errors, dict(form.errors))
        self.assertTrue(equal, err)

    def test_is_valid_warns(self):
        # `is_valid` should warn if we specify a `vulnerability_file` as well
        # as `intensity_measure_types`
        form = validation.ScenarioHazardForm(
            instance=self.hc, files=dict(structural_vulnerability=object()))

        with warnings.catch_warnings(record=True) as w:
            form.is_valid()

        expected_warnings = [
            '`intensity_measure_types` is ignored when a '
            '`vulnerability_file` is specified',
        ]

        actual_warnings = [m.message.message for m in w]
        self.assertEqual(expected_warnings, actual_warnings)


class ClassicalRiskFormTestCase(unittest.TestCase):
    def setUp(self):
        job, _ = helpers.get_fake_risk_job(
            get_data_path('classical_psha_based_risk/job.ini'),
            get_data_path('simple_fault_demo_hazard/job.ini')
        )
        self.compulsory_arguments = dict(
            lrem_steps_per_interval=5)
        self.other_args = dict(
            calculation_mode="classical",
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            hazard_output=job.risk_calculation.hazard_output)

    def test_valid_form(self):
        args = dict(self.compulsory_arguments.items())
        args.update(self.other_args)

        rc = models.RiskCalculation.create(**args)

        form = validation.ClassicalRiskForm(
            instance=rc, files=None)
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_form(self):

        def powerset(iterable):
            s = list(iterable)
            return itertools.chain.from_iterable(
                itertools.combinations(s, r) for r in range(len(s) + 1))

        # for each set of compulsory arguments, we set them to None
        for fields in list(powerset(self.compulsory_arguments))[1:]:
            arguments = dict(self.compulsory_arguments.items())
            for field in fields:
                arguments[field] = None

            # then we set other not-compulsory arguments
            arguments.update(self.other_args)

            rc = models.RiskCalculation.create(**arguments)

            form = validation.ClassicalRiskForm(instance=rc, files=None)

            self.assertFalse(form.is_valid(), fields)


class ClassicalBCRRiskFormTestCase(unittest.TestCase):
    def setUp(self):
        job, _ = helpers.get_fake_risk_job(
            get_data_path('classical_psha_based_risk/job.ini'),
            get_data_path('simple_fault_demo_hazard/job.ini')
        )
        self.compulsory_arguments = dict(
            calculation_mode="classical_bcr",
            lrem_steps_per_interval=5,
            interest_rate=0.05,
            asset_life_expectancy=40)

        self.other_args = dict(
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            hazard_output=job.risk_calculation.hazard_output)

    def test_valid_form(self):
        args = dict(self.compulsory_arguments.items())
        args.update(self.other_args)

        rc = models.RiskCalculation.create(**args)

        form = validation.ClassicalBCRRiskForm(
            instance=rc, files=None)
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_form(self):
        def powerset(iterable):
            s = list(iterable)
            return itertools.chain.from_iterable(
                itertools.combinations(s, r) for r in range(len(s) + 1))

        for fields in list(powerset(self.compulsory_arguments))[1:]:
            compulsory_arguments = dict(self.compulsory_arguments.items())
            for field in fields:
                compulsory_arguments[field] = None
            compulsory_arguments.update(self.other_args)
            rc = models.RiskCalculation.create(**compulsory_arguments)
            form = validation.ClassicalBCRRiskForm(instance=rc, files=None)

            self.assertFalse(form.is_valid(), fields)


class EventBasedBCRRiskForm(unittest.TestCase):

    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('event_based_bcr/job.ini'),
            get_data_path('event_based_hazard/job.ini')
        )

    def test_valid_form(self):
        region_constraint = (
            'POLYGON((-122.0 38.113, -122.114 38.113, '
            '-122.57 38.111, -122.0 38.113))'
        )

        rc = models.RiskCalculation.create(
            calculation_mode="event_based_bcr",
            region_constraint=region_constraint,
            hazard_output=self.job.risk_calculation.hazard_output,
            interest_rate=0.05,
            asset_life_expectancy=40,
        )

        form = validation.EventBasedBCRRiskForm(
            instance=rc, files=None)

        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_form(self):
        region_constraint = (
            'POLYGON((-122.0 38.113, -122.114 38.113, '
            '-122.57 38.111, -122.0 38.113))'
        )

        rc = models.RiskCalculation.create(
            calculation_mode="event_based_bcr",
            region_constraint=region_constraint,
            hazard_output=self.job.risk_calculation.hazard_output,
        )

        form = validation.EventBasedBCRRiskForm(
            instance=rc, files=None)

        self.assertFalse(form.is_valid())


class EventBasedRiskValidationTestCase(unittest.TestCase):
    def setUp(self):
        self.job, _ = helpers.get_fake_risk_job(
            get_data_path('event_based_risk/job.ini'),
            get_data_path('event_based_hazard/job.ini')
        )

    def test_valid_form_with_default_resolution(self):
        rc = models.RiskCalculation.create(
            calculation_mode="event_based",
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            hazard_output=self.job.risk_calculation.hazard_output)

        form = validation.EventBasedRiskForm(
            instance=rc, files=None)
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_valid_form_with_custom_resolution(self):
        rc = models.RiskCalculation.create(
            calculation_mode="event_based",
            loss_curve_resolution=60,
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            hazard_output=self.job.risk_calculation.hazard_output)

        form = validation.EventBasedRiskForm(
            instance=rc, files=None)
        self.assertTrue(form.is_valid(), dict(form.errors))

    def test_invalid_form(self):
        rc = models.RiskCalculation.create(
            calculation_mode="event_based",
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            hazard_output=self.job.risk_calculation.hazard_output,
            sites_disagg='-180.1 38.113, -122.114 38.113',
            coordinate_bin_width=0.0,
            loss_curve_resolution=0,
            mag_bin_width=0.0,
        )

        expected_errors = {
            'coordinate_bin_width': ['Coordinate bin width must be > 0.0'],
            'distance_bin_width': ['Distance bin width must be > 0.0'],
            'loss_curve_resolution': ['Loss Curve Resolution must be >= 1'],
            'mag_bin_width': ['Magnitude bin width must be > 0.0'],
            'sites_disagg': ['Longitude values must in the range [-180, 180]',
                             'disaggregation requires mag_bin_width, '
                             'coordinate_bin_width, distance_bin_width'],
        }

        form = validation.EventBasedRiskForm(
            instance=rc, files=None)
        self.assertFalse(form.is_valid())
        self.assertEqual(expected_errors, dict(form.errors))


class ScenarioRiskValidationTestCase(unittest.TestCase):

    def test_invalid_form(self):
        rc = models.RiskCalculation.create(
            calculation_mode='scenario',
            region_constraint=(
                'POLYGON((-122.0 38.113, -122.114 38.113, -122.57 38.111, '
                '-122.0 38.113))'),
            maximum_distance=100,
            master_seed=666,
            asset_correlation='foo',
            insured_losses=True,
        )

        form = validation.ScenarioRiskForm(
            instance=rc,
            files=dict(occupants_vulnerability=object())
        )

        expected_errors = {
            'asset_correlation': [u'Enter a number.',
                                  u'Asset Correlation must be >= 0 and <= 1'],
            'time_event': ['Scenario Risk requires time_event when an '
                           'occupants vulnerability model is given'],
        }
        self.assertFalse(form.is_valid())
        self.assertEqual(expected_errors, dict(form.errors))


class ValidateTestCase(unittest.TestCase):
    """
    Tests for :func:`openquake.engine.job.validation.validate`.
    """

    def test_validate_warns(self):
        # Test that `validate` raises warnings if unnecessary parameters are
        # specified for a given calculation.
        # For example, `ses_per_logic_tree_path` is an event-based hazard
        # param; if this param is specified for a classical hazard job, a
        # warning should be raised.
        cfg_file = helpers.get_data_path('simple_fault_demo_hazard/job.ini')
        job = engine.prepare_job()
        params = engine.parse_config(open(cfg_file, 'r'))
        # Add a few superfluous parameters:
        params['ses_per_logic_tree_path'] = 5
        params['ground_motion_correlation_model'] = 'JB2009'
        calculation = engine.create_calculation(
            models.HazardCalculation, params)
        job.hazard_calculation = calculation
        job.save()

        with warnings.catch_warnings(record=True) as w:
            validation.validate(job, 'hazard', params, ['xml'])

        expected_warnings = [
            "Unknown parameter '%s' for calculation mode 'classical'."
            " Ignoring." % x for x in ('ses_per_logic_tree_path',
                                       'ground_motion_correlation_model')
        ]

        actual_warnings = [m.message.message for m in w]
        self.assertEqual(sorted(expected_warnings), sorted(actual_warnings))

########NEW FILE########
__FILENAME__ = openquake_test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Tests for code in :mod:`openquake.engine.__init__`.
"""

import os
import unittest

import openquake.engine

from mock import patch


class NoDistributeTestCase(unittest.TestCase):

    def test_no_distribute_not_set(self):
        with patch.dict('os.environ'):
            if openquake.engine.NO_DISTRIBUTE_VAR in os.environ:
                os.environ.pop(openquake.engine.NO_DISTRIBUTE_VAR)

            self.assertFalse(openquake.engine.no_distribute())

    def test_no_distribute_set_true(self):
        with patch.dict('os.environ'):
            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = '1'
            self.assertTrue(openquake.engine.no_distribute())

            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = 'true'
            self.assertTrue(openquake.engine.no_distribute())

            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = 'yes'
            self.assertTrue(openquake.engine.no_distribute())

            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = 't'
            self.assertTrue(openquake.engine.no_distribute())

    def test_no_distribute_set_false(self):
        with patch.dict('os.environ'):
            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = '0'
            self.assertFalse(openquake.engine.no_distribute())

            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = 'false'
            self.assertFalse(openquake.engine.no_distribute())

            os.environ[openquake.engine.NO_DISTRIBUTE_VAR] = 'blarg'
            self.assertFalse(openquake.engine.no_distribute())

########NEW FILE########
__FILENAME__ = performance_monitor_test
import os
import mock
import time
import unittest

from nose.plugins.attrib import attr

import uuid
from datetime import datetime
from openquake.engine.performance import \
    PerformanceMonitor, EnginePerformanceMonitor, LightMonitor
from openquake.engine.db.models import Performance
from openquake.engine import engine

flush = EnginePerformanceMonitor.cache.flush


class TestCase(unittest.TestCase):

    def _check_result(self, pmon):
        # check that the attributes start_time, duration and mem are populated
        self.assertGreater(datetime.now(), pmon.start_time)
        self.assertGreaterEqual(pmon.duration, 0)
        self.assertGreaterEqual(pmon.mem[0], 0)

    # the base monitor does not save on the engine db
    @attr('slow')
    def test_performance_monitor(self):
        ls = []
        with PerformanceMonitor([os.getpid()]) as pmon:
            for _ in range(1000 * 1000):
                ls.append(range(50))  # 50 million of integers
        self._check_result(pmon)

    def test_light_monitor(self):
        mon = LightMonitor('test', 1)
        with mon:
            time.sleep(.1)
        with mon:
            time.sleep(.1)
        self.assertGreater(mon.duration, .2)

    # Skip the following two tests as they always fail on Mac
    @unittest.skip
    def test_engine_performance_monitor(self):
        job = engine.prepare_job()
        mock_task = mock.Mock()
        mock_task.__name__ = 'mock_task'
        mock_task.request.id = task_id = str(uuid.uuid1())
        with EnginePerformanceMonitor(
                'test', job.id, mock_task, profile_pgmem=True) as pmon:
            pass
        self._check_result(pmon)
        # check that one record was stored on the db, as it should
        flush()
        self.assertEqual(len(Performance.objects.filter(task_id=task_id)), 1)

    @unittest.skip
    def test_engine_performance_monitor_no_task(self):
        job = engine.prepare_job()
        operation = str(uuid.uuid1())
        with EnginePerformanceMonitor(
                operation, job.id, profile_pgmem=True) as pmon:
            pass
        self._check_result(pmon)
        flush()
        records = Performance.objects.filter(operation=operation)
        self.assertEqual(len(records), 1)

########NEW FILE########
__FILENAME__ = tools_dbmaint_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Unit tests for the tools/dbmaint.py tool.
"""

from distutils import version
import os
import shutil
import sys
import tempfile
import unittest
from openquake.engine.tests.utils.helpers import patch
from tools.dbmaint import (
    error_occurred, find_scripts, psql, run_cmd, run_scripts, scripts_to_run,
    version_key, script_sort_key)


def touch(path):
    """Create an empty file with the given `path`."""
    open(path, "w+").close()


class RunCmdTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.run_cmd()."""

    def setUp(self):
        self.orig_env = os.environ.copy()
        os.environ["LANG"] = 'C'

    def tearDown(self):
        os.environ.clear()
        os.environ.update(self.orig_env)

    def test_run_cmd_with_success(self):
        """Invoke a command without errors."""
        code, out, err = run_cmd(["echo", "-n", "Hello world!"])
        self.assertEqual(0, code)
        self.assertEqual("Hello world!", out)
        self.assertEqual("", err)

    def test_run_cmd_with_errors(self):
        """Invoke a command with errors."""
        # The expected error message varies between Linux and OSX.
        if sys.platform == 'darwin':
            expected_error = ('ls terminated with exit code: 1\nls: '
                '/this/does/not/exist: No such file or directory\n')
        else:
            expected_error = ('ls terminated with exit code: 2\nls: cannot '
                'access /this/does/not/exist: No such file or directory\n')

        try:
            code, out, err = run_cmd(["ls", "-AF", "/this/does/not/exist"])
        except Exception, e:
            self.assertEqual(expected_error, e.args[0])
        else:
            self.fail("exception not raised")

    def test_run_cmd_with_errors_and_ignore_exit_code(self):
        """Invoke a command with errors but ignore the exit code."""
        # Both the expected exit code and error message vary between Linux and
        # OSX.
        if sys.platform == 'darwin':
            expected_code = 1
            expected_error = ("ls: /this/does/not/exist: No such file or "
                "directory\n")
        else:
            expected_code = 2
            expected_error = ("ls: cannot access /this/does/not/exist: No such"
                " file or directory\n")

        code, out, err = run_cmd(
            ["ls", "-AF", "/this/does/not/exist"], ignore_exit_code=True)
        self.assertEqual(expected_code, code)
        self.assertEqual("", out)
        self.assertEqual(expected_error, err)


class PsqlTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.psql()."""

    def test_psql_cmd_with_script(self):
        """Tests the psql command params with an SQL script file."""
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.args = cmds
        fake_runner.args = []

        config = {"dryrun": False, "path": "/tmp", "host": "localhost",
                  "db": "0penquark", "user": "postgres"}
        psql(config, script="xxx", runner=fake_runner)
        self.assertEqual(
            ["psql", "--set", "ON_ERROR_STOP=1", "-d", "0penquark", "-U",
             "postgres", "-f", "/tmp/xxx"],
            fake_runner.args)

    def test_psql_cmd_with_command(self):
        """Tests the psql command params with an SQL command."""
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.args = cmds
        fake_runner.args = []

        config = {"dryrun": False, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "chuckn"}
        psql(config, cmd="SELECT * from admin.revision_info",
             runner=fake_runner)
        self.assertEqual(
            ["psql", "--set", "ON_ERROR_STOP=1", "-d", "openquake", "-U",
             "chuckn", "-c", "SELECT * from admin.revision_info"],
            fake_runner.args)

    def test_psql_with_non_local_host(self):
        """
        The `-h` flag *is* specified in the `psql` command when the host in
        the configuration is not `localhost`.
        """
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.args = cmds
        fake_runner.args = []

        config = {"dryrun": False, "path": "/tmp", "host": "gozilla",
                  "db": "openquake", "user": "postgres"}
        psql(config, cmd="SELECT * from admin.revision_info",
             runner=fake_runner)
        self.assertTrue("-h" in fake_runner.args)

    def test_psql_with_local_host(self):
        """
        Does not specify the `-h` flag in the `psql` command when the host in
        the configuration is `localhost`.
        """
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.args = cmds
        fake_runner.args = []

        config = {"dryrun": False, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        psql(config, cmd="SELECT * from admin.revision_info",
             runner=fake_runner)
        self.assertTrue("-h" not in fake_runner.args)

    def test_psql_with_local_host_ip(self):
        """
        Does not specify the `-h` flag in the `psql` command when the host in
        the configuration is `127.0.0.1`.
        """
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.args = cmds
        fake_runner.args = []

        config = {"dryrun": False, "path": "/tmp", "host": "127.0.0.1",
                  "db": "openquake", "user": "postgres"}
        psql(config, cmd="SELECT * from admin.revision_info",
             runner=fake_runner)
        self.assertTrue("-h" not in fake_runner.args)

    def test_psql_with_dry_run_flag(self):
        """
        Does not call the psql command if the `dryrun` flag is set in
        the configuration.
        """
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.number_of_calls += 1
        fake_runner.number_of_calls = 0

        config = {"dryrun": True, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        psql(config, cmd="SELECT * from admin.revision_info",
             runner=fake_runner)
        self.assertEqual(0, fake_runner.number_of_calls)

    def test_psql_with_ignored_dry_run_flag(self):
        """
        Calls the psql command if the `dryrun` flag is set in the configuration
        but the 'ignore_dryrun' parameter is set to `True`.
        """
        def fake_runner(cmds):
            """Fake command runner function to be used in this test."""
            fake_runner.number_of_calls += 1
        fake_runner.number_of_calls = 0

        config = {"dryrun": True, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        psql(config, cmd="SELECT * from admin.revision_info",
             ignore_dryrun=True, runner=fake_runner)
        self.assertEqual(1, fake_runner.number_of_calls)

    def test_psql_with_both_script_and_command(self):
        """
        Raises an `Exception` if both a command and a script are passed.
        """
        config = {"dryrun": True, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        try:
            psql(config, cmd="SELECT * from admin.revision_info", script="xxx")
        except Exception, e:
            self.assertEqual(
                "Please specify either an SQL script or a command.", e.args[0])
        else:
            self.fail("exception not raised")

    def test_psql_with_neither_script_nor_command(self):
        """
        Raises an `Exception` if neither a command nor a script are passed.
        """
        config = {"dryrun": True, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        try:
            psql(config)
        except Exception, e:
            self.assertEqual(
                "Neither SQL script nor command specified.", e.args[0])
        else:
            self.fail("exception not raised")


class FindScriptsTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.find_scripts()."""

    def setUp(self):
        self.tdir = tempfile.mkdtemp()
        self.top = "%s/schema/upgrades/openquake/hzrdi/0.3.9-1" % self.tdir
        self.path1 = "%s/1" % self.top
        os.makedirs(self.path1)
        self.path1d = "%s/1/too_deep" % self.top
        os.makedirs(self.path1d)
        self.path2 = "%s/2" % self.top
        os.makedirs(self.path2)

    def tearDown(self):
        shutil.rmtree(self.tdir)

    def test_with_files_in_dir_too_deep_in_hierarchy(self):
        """
        Files too far up or too far down will be ignored.
        """
        touch("%s/01-too-far-up.sql" % self.top)
        touch("%s/01-too-far-down.sql" % self.path1d)
        self.assertEqual([], find_scripts(self.top))

    def test_with_non_sql_files(self):
        """
        Files with extensions other than ".sql" are ignored.
        """
        touch("%s/01-not-a-sql-file.txt" % self.path1)
        touch("%s/01-no-sql-extensionsql" % self.path2)
        self.assertEqual([], find_scripts(self.top))

    def test_with_files_at_level_two(self):
        """
        Files that are at level 3 relative to the artefact subdirectory will be
        found.
        """
        touch("%s/01-a.sql" % self.path1)
        touch("%s/02-b.sql" % self.path1)
        touch("%s/03-c.py" % self.path1)
        touch("%s/01-a.sql" % self.path2)
        self.assertEqual(["1/01-a.sql", "1/02-b.sql", "1/03-c.py",
                          "2/01-a.sql"],
                         list(sorted(find_scripts(self.top))))


class ScriptsToRunTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.scripts_to_run()."""

    def setUp(self):
        self.tdir = tempfile.mkdtemp()
        self.path = "%s/schema/upgrades" % self.tdir
        self.top = "%s/" % self.path
        # older revision
        self.path_38_1 = "%s/0.3.8/1" % self.top
        os.makedirs(self.path_38_1)
        self.path_38_5 = "%s/0.3.8/5" % self.top
        os.makedirs(self.path_38_5)
        # current revision
        self.path_39_1 = "%s/0.3.9-1/1" % self.top
        os.makedirs(self.path_39_1)
        self.path_39_1d = "%s/0.3.9-1/1/too_deep" % self.top
        os.makedirs(self.path_39_1d)
        self.path_39_2 = "%s/0.3.9-1/2" % self.top
        os.makedirs(self.path_39_2)
        self.path_39_3 = "%s/0.3.9-1/3" % self.top
        os.makedirs(self.path_39_3)
        # newer revision
        self.path_42_1 = "%s/0.4.2/1" % self.top
        os.makedirs(self.path_42_1)

    def tearDown(self):
        shutil.rmtree(self.tdir)

    def test_scripts_to_run_with_no_upgrades(self):
        """No upgrades are available."""
        artefact = "openquake/hzrdi"
        rev_info = {"step": "2", "id": "3", "revision": "0.3.9-1"}
        config = {"dryrun": True, "path": self.path, "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        touch("%s/01-a.sql" % self.path_38_1)
        touch("%s/01-a.sql" % self.path_38_5)
        touch("%s/01-a.sql" % self.path_39_1)
        touch("%s/01-a.sql" % self.path_39_2)
        self.assertEqual([], scripts_to_run(artefact, rev_info, config))

    def test_scripts_to_run_with_available_upgrades(self):
        """Upgrades are available."""
        artefact = "openquake/hzrdi"
        rev_info = {"step": "2", "id": "3", "revision": "0.3.9-1"}
        config = {"dryrun": True, "path": self.path, "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        touch("%s/01-a.sql" % self.path_38_1)
        touch("%s/01-a.sql" % self.path_39_1)
        touch("%s/01-a.sql" % self.path_38_5)
        touch("%s/01-b.sql" % self.path_39_2)
        touch("%s/01-c.sql" % self.path_39_3)
        touch("%s/02-d.sql" % self.path_39_3)
        touch("%s/01-a.sql" % self.path_42_1)
        touch("%s/02-b.sql" % self.path_42_1)
        self.assertEqual(["0.3.9-1/3/01-c.sql", "0.3.9-1/3/02-d.sql",
                          "0.4.2/1/01-a.sql", "0.4.2/1/02-b.sql"],
                         scripts_to_run(artefact, rev_info, config))


class ErrorOccuredTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.error_occurred()."""

    def test_error_occured_with_error(self):
        """A psql error is detected correctly."""
        output = '''
            psql:/tmp/openquake/hzrdi/0.3.9-1/5/55-eee.sql:1: ERROR:  relation
            "admin.dbm_test" does not exist
            LINE 1: INSERT INTO admin.dbm_test(name) VALUES('5/55-eee.sql');
        '''
        self.assertTrue(error_occurred(output))

    def test_error_occured_with_no_error(self):
        """A psql error is not detected (which is correct)."""
        output = '''
            LINE 1: INSERT INTO admin.dbm_test(name) VALUES('5/55-eee.sql');
        '''
        self.assertFalse(error_occurred(output))


class RunScriptsTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.run_scripts()."""

    def test_run_scripts_with_available_upgrades(self):
        """
        The `psql` function is called for every upgrade script and at the
        very end to update the revision step.
        """

        artefact = "openquake/hzrdi"
        rev_info = {"step": "2", "id": "3", "revision": "0.3.9-1"}
        config = {"dryrun": True, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        scripts = ["0.3.9-1/3/01-c.sql", "0.3.9-1/3/02-d.sql",
                   "0.4.2/2/01-a.sql"]
        with patch('tools.dbmaint.psql') as mock_psql:
            # Make all the calls pass.
            mock_psql.return_value = (0, "", "")

            # Run the actual function that is to be tested.
            run_scripts(artefact, rev_info, scripts, config)

            # The mock was called four times.
            self.assertEqual(4, mock_psql.call_count)
            # The first call executed an SQL script.
            self.assertEqual({"script": "0.3.9-1/3/01-c.sql"},
                             mock_psql.call_args_list[0][1])
            # The second call executed the second SQL script.
            self.assertEqual({"script": "0.3.9-1/3/02-d.sql"},
                             mock_psql.call_args_list[1][1])
            # The third call executed the second SQL script.
            self.assertEqual({"script": "0.4.2/2/01-a.sql"},
                             mock_psql.call_args_list[2][1])
            # The last call executed the command to update the revision step.
            self.assertEqual(
                {"cmd": "UPDATE admin.revision_info SET step=2, "
                        "revision='0.4.2', "
                        "last_update=timezone('UTC'::text, now()) WHERE "
                        "artefact='openquake/hzrdi' AND revision = '0.3.9-1'"},
                mock_psql.call_args_list[3][1])

    def test_run_scripts_with_failing_upgrades(self):
        """Upgrades are available but the second one will fail."""
        def fail_on_first_even_script(
            config, script=None, cmd=None, ignore_dryrun=False, runner=None):
            """Pretend that the second SQL script failed on execution."""
            if script and script.find("02-d.sql") >= 0:
                return(1, "", '02-d.sql:1: ERROR:  relation "admin.dbm_test" ')
            else:
                return(0, "All goood", "")

        artefact = "openquake/hzrdi"
        rev_info = {"step": "2", "id": "3", "revision": "0.3.9-1"}
        config = {"dryrun": False, "path": "/tmp", "host": "localhost",
                  "db": "openquake", "user": "postgres"}
        scripts = ["0.3.9-1/3/01-c.sql", "0.3.9-1/3/02-d.sql",
                   "0.4.2/1/01-a.sql"]
        with patch('tools.dbmaint.psql') as mock_psql:
            # Make all the calls pass.
            mock_psql.side_effect = fail_on_first_even_script

            # Run the actual function that is to be tested.
            run_scripts(artefact, rev_info, scripts, config)

            # The mock was called thrice.
            self.assertEqual(3, mock_psql.call_count)
            # The first call executed an SQL script.
            self.assertEqual({"script": "0.3.9-1/3/01-c.sql"},
                             mock_psql.call_args_list[0][1])
            # The second call executed the second SQL script.
            self.assertEqual({"script": "0.3.9-1/3/02-d.sql"},
                             mock_psql.call_args_list[1][1])
            # Please note how the step is assigned a -1 value which indicates
            # a database upgrade failure.
            self.assertEqual(
                {"cmd": "UPDATE admin.revision_info SET step=-1, "
                        "revision='0.3.9-1', "
                        "last_update=timezone('UTC'::text, now()) WHERE "
                        "artefact='openquake/hzrdi' AND revision = '0.3.9-1'"},
                mock_psql.call_args_list[2][1])


class VersionKeyTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.version_key()."""

    def test_version_with_dash(self):
        self.assertEquals('3.9.1', str(version_key('3.9.1-1')))

    def test_plain_version(self):
        self.assertEquals('3.9.1', str(version_key('3.9.1')))


class ScriptSortKeyTestCase(unittest.TestCase):
    """Tests the behaviour of dbmaint.script_sort_key()."""

    def test_sanity(self):
        self.assertEquals((version.StrictVersion("0.3.9"), 7, "01-a.sql"),
                          script_sort_key("0.3.9-1/7/01-a.sql"))

    def test_different_revision(self):
        self.assertTrue(script_sort_key("0.3.9-1/1/01-a.sql") <
                        script_sort_key("0.4.2/1/01-a.sql"))
        self.assertTrue(script_sort_key("0.3.9-1/1/01-a.sql") <
                        script_sort_key("0.3.10/1/01-a.sql"))
        self.assertTrue(script_sort_key("0.3.9-1/4/01-a.sql") <
                        script_sort_key("0.4.2/1/01-a.sql"))
        self.assertTrue(script_sort_key("0.3.9-1/1/04-a.sql") <
                        script_sort_key("0.4.2/1/01-a.sql"))

    def test_different_step(self):
        self.assertTrue(script_sort_key("0.4.2/1/01-a.sql") <
                        script_sort_key("0.4.2/2/01-a.sql"))
        self.assertTrue(script_sort_key("0.4.2/9/01-a.sql") <
                        script_sort_key("0.4.2/10/01-a.sql"))

    def test_different_file(self):
        self.assertTrue(script_sort_key("0.3.9-1/1/01-a.sql") <
                        script_sort_key("0.3.9-1/1/02-a.sql"))
        self.assertTrue(script_sort_key("0.4.2/1/01-a.sql") <
                        script_sort_key("0.4.2/1/02-a.sql"))

########NEW FILE########
__FILENAME__ = helpers
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Helper functions for our unit and smoke tests.
"""

import functools
import logging
import mock as mock_module
import numpy
import os
import csv
import random
import shutil
import string
import sys
import tempfile
import textwrap
import time

from openquake.hazardlib.source.rupture import ParametricProbabilisticRupture
from openquake.hazardlib.geo import Point
from openquake.hazardlib.geo.surface.planar import PlanarSurface
from openquake.hazardlib.tom import PoissonTOM

from django.core import exceptions

from openquake.engine.db import models
from openquake.engine import engine
from openquake.engine import logs
from openquake.engine.utils import config
from openquake.engine.job.validation import validate


CD = os.path.dirname(__file__)  # current directory

RUNNER = os.path.abspath(os.path.join(CD, '../../../../bin/openquake'))

DATA_DIR = os.path.abspath(os.path.join(CD, '../data'))

OUTPUT_DIR = os.path.abspath(os.path.join(CD, '../data/output'))

WAIT_TIME_STEP_FOR_TASK_SECS = 0.5
MAX_WAIT_LOOPS = 10


#: Wraps mock.patch() to make mocksignature=True by default.
patch = functools.partial(mock_module.patch, mocksignature=True)


def default_user():
    """Return the default user to be used for test setups."""
    return "openquake"


def _patched_mocksignature(func, mock=None, skipfirst=False):
    """
    Fixes arguments order and support of staticmethods in mock.mocksignature.
    """
    static = False
    if isinstance(func, staticmethod):
        static = True
        func = func.__func__

    if mock is None:
        mock = mock_module.Mock()
    signature, func = mock_module._getsignature(func, skipfirst)

    checker = eval("lambda %s: None" % signature)
    mock_module._copy_func_details(func, checker)

    def funcopy(*args, **kwargs):
        checker(*args, **kwargs)
        return mock(*args, **kwargs)

    if not hasattr(mock_module, '_setup_func'):
        # compatibility with mock < 0.8
        funcopy.mock = mock
    else:
        mock_module._setup_func(funcopy, mock)
    if static:
        funcopy = staticmethod(funcopy)
    return funcopy
mock_module.mocksignature = _patched_mocksignature


def get_data_path(file_name):
    return os.path.join(DATA_DIR, file_name)


def demo_file(file_name):
    """
    Take a file name and return the full path to the file in the demos
    directory.
    """
    return os.path.join(
        os.path.dirname(__file__), "../../demos", file_name)


def run_job(cfg, exports=None, hazard_calculation_id=None,
            hazard_output_id=None):
    """
    Given the path to a job config file and a hazard_calculation_id
    or a output, run the job.
    """
    if exports is None:
        exports = []

    job = get_job(cfg, hazard_calculation_id=hazard_calculation_id,
                  hazard_output_id=hazard_output_id)
    job.is_running = True
    job.save()

    logfile = os.path.join(tempfile.gettempdir(), 'qatest.log')
    job_type = 'risk' if (
        hazard_calculation_id or hazard_output_id) else 'hazard'
    engine.run_calc(job, 'error', logfile, exports, job_type)
    return job


def timeit(method):
    """Decorator for timing methods"""

    def _timed(*args, **kw):
        """Wrapped function for timed methods"""
        timestart = time.time()
        result = method(*args, **kw)
        timeend = time.time()

        print '%r (%r, %r) %2.2f sec' % (
            method.__name__, args, kw, timeend - timestart)
        return result
    try:
        import nose
        return nose.tools.make_decorator(method)(_timed)
    except ImportError:
        pass
    return _timed


def assertDeepAlmostEqual(test_case, expected, actual, *args, **kwargs):
    """
    Assert that two complex structures have almost equal contents.

    Compares lists, dicts and tuples recursively. Checks numeric values
    using test_case's :py:meth:`unittest.TestCase.assertAlmostEqual` and
    checks all other values with :py:meth:`unittest.TestCase.assertEqual`.
    Accepts additional positional and keyword arguments and pass those
    intact to assertAlmostEqual() (that's how you specify comparison
    precision).

    :param test_case: TestCase object on which we can call all of the basic
        'assert' methods.
    :type test_case: :py:class:`unittest.TestCase` object
    """
    is_root = not '__trace' in kwargs
    trace = kwargs.pop('__trace', 'ROOT')
    try:
        if isinstance(expected, (int, float, long, complex)):
            test_case.assertAlmostEqual(expected, actual, *args, **kwargs)
        elif isinstance(expected, (list, tuple, numpy.ndarray)):
            test_case.assertEqual(len(expected), len(actual))
            for index in xrange(len(expected)):
                v1, v2 = expected[index], actual[index]
                assertDeepAlmostEqual(test_case, v1, v2,
                                      __trace=repr(index), *args, **kwargs)
        elif isinstance(expected, dict):
            test_case.assertEqual(set(expected), set(actual))
            for key in expected:
                assertDeepAlmostEqual(test_case, expected[key], actual[key],
                                      __trace=repr(key), *args, **kwargs)
        else:
            test_case.assertEqual(expected, actual)
    except AssertionError as exc:
        exc.__dict__.setdefault('traces', []).append(trace)
        if is_root:
            trace = ' -> '.join(reversed(exc.traces))
            exc = AssertionError("%s\nTRACE: %s" % (exc.message, trace))
        raise exc


def assertModelAlmostEqual(test_case, expected, actual):
    """
    Assert that two Django models are equal. For values which are numbers,
    we use :py:meth:`unittest.TestCase.assertAlmostEqual` for number
    comparisons with a reasonable precision tolerance.

    If the `expected` input value contains nested models, this function
    will recurse through them and check for equality.

    :param test_case: TestCase object on which we can call all of the basic
        'assert' methods.
    :type test_case: :py:class:`unittest.TestCase` object
    :type expected: dict
    :type actual: dict
    """

    from django.contrib.gis.db import models as gis_models

    test_case.assertEqual(type(expected), type(actual))

    def getattr_or_none(model, field):
        try:
            return getattr(model, field.name)
        except exceptions.ObjectDoesNotExist:
            return None

    for field in expected._meta.fields:
        if field.name == 'last_update':
            continue

        exp_val = getattr_or_none(expected, field)
        act_val = getattr_or_none(actual, field)

        # If it's a number, use assertAlmostEqual to compare
        # the values with a reasonable tolerance.
        if isinstance(exp_val, (int, float, long, complex)):
            test_case.assertAlmostEqual(exp_val, act_val)
        elif isinstance(exp_val, gis_models.Model):
            # make a recursive call in case there are nested models
            assertModelAlmostEqual(test_case, exp_val, act_val)
        else:
            test_case.assertEqual(exp_val, act_val)


# preserve stdout/stderr (note: we want the nose-manipulated stdout/stderr,
# otherwise we could just use __stdout__/__stderr__)
STDOUT = sys.stdout
STDERR = sys.stderr


def cleanup_loggers():
    root = logging.getLogger()

    for h in list(root.handlers):
        if (isinstance(h, logging.FileHandler) or
            isinstance(h, logging.StreamHandler) or
                isinstance(h, logs.AMQPHandler)):
            root.removeHandler(h)

    # restore the damage created by redirect_stdouts_to_logger; this is only
    # necessary because tests perform multiple log initializations, sometimes
    # for AMQP, sometimes for console
    sys.stdout = STDOUT
    sys.stderr = STDERR


def touch(content=None, dir=None, prefix="tmp", suffix="tmp"):
    """Create temporary file with the given content.

    Please note: the temporary file must be deleted bu the caller.

    :param string content: the content to write to the temporary file.
    :param string dir: directory where the file should be created
    :param string prefix: file name prefix
    :param string suffix: file name suffix
    :returns: a string with the path to the temporary file
    """
    if dir is not None:
        if not os.path.exists(dir):
            os.makedirs(dir)
    fh, path = tempfile.mkstemp(dir=dir, prefix=prefix, suffix=suffix)
    if content:
        fh = os.fdopen(fh, "w")
        fh.write(content)
        fh.close()
    return path


class ConfigTestCase(object):
    """Class which contains various configuration- and environment-related
    testing helpers."""

    def setup_config(self):
        self.orig_env = os.environ.copy()
        os.environ.clear()
        # Move the local configuration file out of the way if it exists.
        # Otherwise the tests that follow will break.
        local_path = "%s/openquake.cfg" % os.path.abspath(config.OQDIR)
        if os.path.isfile(local_path):
            shutil.move(local_path, "%s.test_bakk" % local_path)

    def teardown_config(self):
        os.environ.clear()
        os.environ.update(self.orig_env)
        # Move the local configuration file back into place if it was stashed
        # away.
        local_path = "%s/openquake.cfg" % os.path.abspath(config.OQDIR)
        if os.path.isfile("%s.test_bakk" % local_path):
            shutil.move("%s.test_bakk" % local_path, local_path)
        config.cfg.cfg.clear()
        config.cfg._load_from_file()

    def prepare_config(self, section, data=None):
        """Set up a configuration with the given `max_mem` value."""
        if data is not None:
            data = '\n'.join(["%s=%s" % item for item in data.iteritems()])
            content = """
                [%s]
                %s""" % (section, data)
        else:
            content = ""
        site_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_SITE_CFG_PATH"] = site_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()


def random_string(length=16):
    """Generate a random string of the given length."""
    result = ""
    while len(result) < length:
        result += random.choice(string.letters + string.digits)
    return result


def get_job(cfg, username="openquake", hazard_calculation_id=None,
            hazard_output_id=None):
    """
    Given a path to a config file and a hazard_calculation_id
    (or, alternatively, a hazard_output_id, create a
    :class:`openquake.engine.db.models.OqJob` object for a risk calculation.
    """
    if hazard_calculation_id is None and hazard_output_id is None:
        return engine.job_from_file(cfg, username, 'error', [])

    job = engine.prepare_job(username)
    params = engine.parse_config(open(cfg, 'r'))

    params.update(
        dict(hazard_output_id=hazard_output_id,
             hazard_calculation_id=hazard_calculation_id)
    )

    risk_calc = engine.create_calculation(
        models.RiskCalculation, params)
    risk_calc = models.RiskCalculation.objects.get(id=risk_calc.id)
    job.risk_calculation = risk_calc
    job.save()
    return job


def create_gmf(hazard_job, rlz=None):
    """
    Returns the created Gmf object.
    """
    hc = hazard_job.hazard_calculation

    rlz = rlz or models.LtRealization.objects.create(
        lt_model=models.LtSourceModel.objects.create(
            hazard_calculation=hc, ordinal=0, sm_lt_path="test_sm"),
        ordinal=0, weight=None, gsim_lt_path="test_gsim")

    gmf = models.Gmf.objects.create(
        output=models.Output.objects.create_output(
            hazard_job, "Test Hazard output", "gmf"),
        lt_realization=rlz)

    return gmf


def create_gmf_data_records(hazard_job, rlz=None, ses_coll=None, points=None):
    """
    Returns the created records.
    """
    gmf = create_gmf(hazard_job, rlz)
    ses_coll = ses_coll or models.SESCollection.objects.create(
        output=models.Output.objects.create_output(
            hazard_job, "Test SES Collection", "ses"),
        lt_model=gmf.lt_realization.lt_model,
        ordinal=0)
    ruptures = create_ses_ruptures(hazard_job, ses_coll, 3)
    records = []
    if points is None:
        points = [(15.310, 38.225), (15.71, 37.225),
                  (15.48, 38.091), (15.565, 38.17),
                  (15.481, 38.25)]
    for site_id in hazard_job.hazard_calculation.save_sites(points):
        records.append(models.GmfData.objects.create(
            gmf=gmf,
            task_no=0,
            imt="PGA",
            gmvs=[0.1, 0.2, 0.3],
            rupture_ids=[r.id for r in ruptures],
            site_id=site_id))

    return records


# NB: create_gmf_from_csv and populate_gmf_data_from_csv
# will be unified in the future
def create_gmf_from_csv(job, fname):
    """
    Populate the gmf_data table for an event_based calculation.
    """
    hc = job.hazard_calculation
    hc.investigation_time = 50
    hc.ses_per_logic_tree_path = 1
    hc.save()

    # tricks to fool the oqtask decorator
    job.is_running = True
    job.status = 'post_processing'
    job.save()

    gmf = create_gmf(job)

    ses_coll = models.SESCollection.objects.create(
        output=models.Output.objects.create_output(
            job, "Test SES Collection", "ses"),
        lt_model=gmf.lt_realization.lt_model,
        ordinal=0)
    with open(fname, 'rb') as csvfile:
        gmfreader = csv.reader(csvfile, delimiter=',')
        locations = gmfreader.next()

        gmv_matrix = numpy.array(
            [map(float, row) for row in gmfreader]).transpose()

        ruptures = create_ses_ruptures(job, ses_coll, len(gmv_matrix[0]))

        for i, gmvs in enumerate(gmv_matrix):
            point = tuple(map(float, locations[i].split()))
            [site_id] = job.hazard_calculation.save_sites([point])
            models.GmfData.objects.create(
                gmf=gmf,
                task_no=0,
                imt="PGA", gmvs=gmvs,
                rupture_ids=[r.id for r in ruptures],
                site_id=site_id)

    return gmf


def populate_gmf_data_from_csv(job, fname):
    """
    Populate the gmf_data table for a scenario calculation.
    """
    # tricks to fool the oqtask decorator
    job.is_running = True
    job.status = 'post_processing'
    job.save()

    gmf = models.Gmf.objects.create(
        output=models.Output.objects.create_output(
            job, "Test Hazard output", "gmf_scenario"))

    with open(fname, 'rb') as csvfile:
        gmfreader = csv.reader(csvfile, delimiter=',')
        locations = gmfreader.next()

        gmv_matrix = numpy.array(
            [map(float, row) for row in gmfreader]).transpose()

        for i, gmvs in enumerate(gmv_matrix):
            point = tuple(map(float, locations[i].split()))
            [site_id] = job.hazard_calculation.save_sites([point])
            models.GmfData.objects.create(
                task_no=0,
                imt="PGA",
                gmf=gmf,
                gmvs=gmvs,
                site_id=site_id)

    return gmf


def get_fake_risk_job(risk_cfg, hazard_cfg, output_type="curve",
                      username="openquake"):
    """
    Takes in input the paths to a risk job config file and a hazard job config
    file.

    Creates fake hazard outputs suitable to be used by a risk
    calculation and then creates a :class:`openquake.engine.db.models.OqJob`
    object for a risk calculation. It also returns the input files
    referenced by the risk config file.

    :param output_type: gmf, gmf_scenario, or curve
    """

    hazard_job = get_job(hazard_cfg, username)
    hc = hazard_job.hazard_calculation

    lt_model = models.LtSourceModel.objects.create(
        hazard_calculation=hazard_job.hazard_calculation,
        ordinal=1, sm_lt_path="test_sm")

    rlz = models.LtRealization.objects.create(
        lt_model=lt_model, ordinal=1, weight=None,
        gsim_lt_path="test_gsim")

    if output_type == "curve":
        models.HazardCurve.objects.create(
            lt_realization=rlz,
            output=models.Output.objects.create_output(
                hazard_job, "Test Hazard output", "hazard_curve_multi"),
            investigation_time=hc.investigation_time)

        hazard_output = models.HazardCurve.objects.create(
            lt_realization=rlz,
            output=models.Output.objects.create_output(
                hazard_job, "Test Hazard output", "hazard_curve"),
            investigation_time=hc.investigation_time,
            imt="PGA", imls=[0.1, 0.2, 0.3])

        for point in ["POINT(-1.01 1.01)", "POINT(0.9 1.01)",
                      "POINT(0.01 0.01)", "POINT(0.9 0.9)"]:
            models.HazardSite.objects.create(
                hazard_calculation=hc, location=point)
            models.HazardCurveData.objects.create(
                hazard_curve=hazard_output,
                poes=[0.1, 0.2, 0.3],
                location="%s" % point)

    elif output_type == "gmf_scenario":
        hazard_output = models.Gmf.objects.create(
            output=models.Output.objects.create_output(
                hazard_job, "Test gmf scenario output", "gmf_scenario"))

        site_ids = hazard_job.hazard_calculation.save_sites(
            [(15.48, 38.0900001), (15.565, 38.17), (15.481, 38.25)])
        for site_id in site_ids:
            models.GmfData.objects.create(
                gmf=hazard_output,
                task_no=0,
                imt="PGA",
                site_id=site_id,
                gmvs=[0.1, 0.2, 0.3])

    elif output_type in ("ses", "gmf"):
        hazard_output = create_gmf_data_records(hazard_job, rlz)[0].gmf

    else:
        raise RuntimeError('Unexpected output_type: %s' % output_type)

    hazard_job.status = "complete"
    hazard_job.save()
    job = engine.prepare_job(username)
    params = engine.parse_config(open(risk_cfg, 'r'))

    params.update(dict(hazard_output_id=hazard_output.output.id))

    risk_calc = engine.create_calculation(models.RiskCalculation, params)
    job.risk_calculation = risk_calc
    job.save()
    error_message = validate(job, 'risk', params, [])

    # reload risk calculation to have all the types converted properly
    job.risk_calculation = models.RiskCalculation.objects.get(id=risk_calc.id)
    if error_message:
        raise RuntimeError(error_message)
    return job, set(params['inputs'])


def create_ses_ruptures(job, ses_collection, num):
    """
    :param job:
        the current job, an instance of `openquake.engine.db.models.OqJob`
    :param ses_collection:
        an instance of :class:`openquake.engine.db.models.SESCollection`
        to be associated with the newly created SES object
    :param int num:
        the number of ruptures to create
    :returns:
        a list of newly created ruptures associated with `job`.

    It also creates a father :class:`openquake.engine.db.models.SES`.
    Each rupture has a magnitude ranging from 0 to 10 and no geographic
    information.
    """
    rupture = ParametricProbabilisticRupture(
        mag=1 + 10. / float(num), rake=0,
        tectonic_region_type="test region type",
        hypocenter=Point(0, 0, 0.1),
        surface=PlanarSurface(
            10, 11, 12, Point(0, 0, 1), Point(1, 0, 1),
            Point(1, 0, 2), Point(0, 0, 2)),
        occurrence_rate=1,
        temporal_occurrence_model=PoissonTOM(10),
        source_typology=object())
    ses = models.SES(ses_collection, ordinal=1)
    seed = 42
    pr = models.ProbabilisticRupture.create(rupture, ses_collection)
    return [models.SESRupture.create(pr, ses, 'test', 1, i, seed + i)
            for i in range(num)]


class MultiMock(object):
    """
    Context-managed multi-mock object. This is useful if you need to mock
    multiple things at once. So instead of creating individual patch+mock
    objects for each, you can define them basically as a dictionary. You can
    also use the mock context managers without having to nest `with`
    statements.

    Example usage:

    .. code-block:: python

        # First, define your mock targets as a dictionary.
        # The value of each item is the path to the function/method you wish to
        # mock. The key is basically a shortcut to the mock.
        mocks = {
            'touch': 'openquake.engine.engine.touch_log_file',
            'job': 'openquake.engine.engine.job_from_file',
        }
        multi_mock = MultiMock(**mocks)

        # To start mocking, start the context manager using `with`:
        # with multi_mock:
        with multi_mock:
            # You can mock return values, for example, just as you would with
            # any other Mock object:
            multi_mock['job'].return_value = 'foo'

            # call the function under test which will calls the mocked
            # functions
            engine.run_job('job.ini', 'debug', 'oq.log', ['geojson'])

            # To test the mocks, you can simply access each mock from
            # `multi_mock` like a dict:
            assert multi_mock['touch'].call_count == 1
    """

    def __init__(self, **mocks):
        # dict of mock names -> mock paths
        self._mocks = mocks
        self.active_patches = {}
        self.active_mocks = {}

    def __enter__(self):
        for key, value in self._mocks.iteritems():
            the_patch = mock_module.patch(value)
            self.active_patches[key] = the_patch
            self.active_mocks[key] = the_patch.start()
        return self

    def __exit__(self, *args):
        for each_mock in self.active_mocks.itervalues():
            each_mock.stop()
        for each_patch in self.active_patches.itervalues():
            each_patch.stop()

    def __iter__(self):
        return self.active_mocks.itervalues()

    def __getitem__(self, key):
        return self.active_mocks.get(key)

########NEW FILE########
__FILENAME__ = tasks
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Task functions for our unit tests.
"""

import sys
import functools
import traceback

from celery.task import task

from openquake.engine.utils.tasks import Pickled, oqtask


# mimic the behavior of oqtask
def test_task(func):
    @functools.wraps(func)
    def wrapper(*args):
        try:
            res = func(*[a.unpickle() for a in args])
            return Pickled((res, None))
        except:
            exctype, exc, tb = sys.exc_info()
            tb_str = ''.join(traceback.format_tb(tb))
            err_msg = '\n%s%s: %s' % (tb_str, exctype.__name__, exc)
            return Pickled((err_msg, exctype))
    return task(wrapper)


@test_task
def reflect_args(*args):
    """Merely returns the parameters received."""
    return args


@test_task
def just_say_hello(*args):
    """Merely returns 'hello'."""
    return "hello"


@test_task
def just_say_1(*args):
    """Merely returns 1."""
    return 1


@test_task
def single_arg_called_a(a):
    """Takes a single argument called `a` and merely returns `True`."""
    return True


@test_task
def failing_task(data):
    """
    Takes a single argument called `data` and raises a `NotImplementedError`
    exception throwing it back.
    """
    raise NotImplementedError(data)


@test_task
def reflect_data_to_be_processed(data):
    """Merely returns the data received."""
    return data


@oqtask
def fake_risk_task(job_id, risk_model, outputdict, params):
    """Used in tests.calculators.risk.base"""
    return {job_id: 1}

########NEW FILE########
__FILENAME__ = utils_config_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Test related to code in openquake/utils/config.py
"""


import os
import textwrap
import unittest

from openquake.engine.utils import config

from openquake.engine.tests.utils.helpers import ConfigTestCase
from openquake.engine.tests.utils.helpers import patch
from openquake.engine.tests.utils.helpers import touch


class ConfigTestCase(ConfigTestCase, unittest.TestCase):
    """Tests the behaviour of the utils.config.Config class."""

    def setUp(self):
        self.setup_config()

    def tearDown(self):
        self.teardown_config()

    def test_get_paths_with_global_env_var_set(self):
        # _get_paths() will honour the OQ_SITE_CFG_PATH environment
        # variable
        os.environ["OQ_SITE_CFG_PATH"] = "/a/b/c/d"
        self.assertEqual(
            ["/a/b/c/d", "%s/openquake.cfg" % config.OQDIR],
            config.cfg._get_paths())

    def test_get_paths_with_local_env_var_set(self):
        # _get_paths() will honour the OQ_LOCAL_CFG_PATH
        # variable
        os.environ["OQ_LOCAL_CFG_PATH"] = "/e/f/g/h"
        self.assertEqual(
            ["/etc/openquake/openquake.cfg", "/e/f/g/h"],
            config.cfg._get_paths())

    def test_get_paths_with_no_environ(self):
        # _get_paths() will return the hard-coded paths if the OQ_SITE_CFG_PATH
        # and the OQ_LOCAL_CFG_PATH variables are not set
        self.assertEqual(
            ["/etc/openquake/openquake.cfg",
             "%s/openquake.cfg" % config.OQDIR],
            config.cfg._get_paths())

    def test_load_from_file_with_no_config_files(self):
        # In the absence of config files the `cfg` dict will be empty
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual([], config.cfg.cfg.keys())

    def test_load_from_file_with_global(self):
        # The config data in the global file is loaded correctly
        content = '''
            [A]
            a=1
            b=c

            [B]
            b=2'''
        site_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_SITE_CFG_PATH"] = site_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual(["A", "B"], sorted(config.cfg.cfg))
        self.assertEqual({"a": "1", "b": "c"}, config.cfg.cfg.get("A"))
        self.assertEqual({"b": "2"}, config.cfg.cfg.get("B"))

    def test_load_from_file_with_local(self):
        # The config data in the local file is loaded correctly
        content = '''
            [C]
            c=3
            d=e

            [D]
            d=4'''
        local_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_LOCAL_CFG_PATH"] = local_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual(["C", "D"], sorted(config.cfg.cfg))
        self.assertEqual({"c": "3", "d": "e"}, config.cfg.cfg.get("C"))
        self.assertEqual({"d": "4"}, config.cfg.cfg.get("D"))

    def test_load_from_file_with_local_and_global(self):
        # The config data in the local and global files is loaded correctly
        content = '''
            [A]
            a=1
            b=c

            [B]
            b=2'''
        site_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_SITE_CFG_PATH"] = site_path
        content = '''
            [C]
            c=3
            d=e

            [D]
            d=4'''
        local_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_LOCAL_CFG_PATH"] = local_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual(["A", "B", "C", "D"],
                         sorted(config.cfg.cfg))
        self.assertEqual({"a": "1", "b": "c"}, config.cfg.cfg.get("A"))
        self.assertEqual({"b": "2"}, config.cfg.cfg.get("B"))
        self.assertEqual({"c": "3", "d": "e"}, config.cfg.cfg.get("C"))
        self.assertEqual({"d": "4"}, config.cfg.cfg.get("D"))

    def test_load_from_file_with_local_overriding_global(self):
        # The config data in the local and global files is loaded correctly.
        # The local data will override the global one
        content = '''
            [A]
            a=1
            b=c

            [B]
            b=2'''
        site_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_SITE_CFG_PATH"] = site_path
        content = '''
            [A]
            a=2
            d=e

            [D]
            c=d-1
            d=4'''
        local_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_LOCAL_CFG_PATH"] = local_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual(["A", "B", "D"],
                         sorted(config.cfg.cfg))
        self.assertEqual({"a": "2", "b": "c", "d": "e"},
                         config.cfg.cfg.get("A"))
        self.assertEqual({"b": "2"}, config.cfg.cfg.get("B"))
        self.assertEqual({"c": "d-1", "d": "4"}, config.cfg.cfg.get("D"))

    def test_get_with_unknown_section(self):
        # get() will return `None` for a section name that is not known
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertTrue(config.cfg.get("Anything") is None)

    def test_get_with_known_section(self):
        # get() will correctly return configuration data for known sections
        content = '''
            [E]
            f=6
            g=h'''
        site_path = touch(content=textwrap.dedent(content))
        os.environ["OQ_SITE_CFG_PATH"] = site_path
        config.cfg.cfg.clear()
        config.cfg._load_from_file()
        self.assertEqual({"f": "6", "g": "h"}, config.cfg.get("E"))


class GetSectionTestCase(unittest.TestCase):
    """Tests the behaviour of utils.config.get_section()"""

    def tearDown(self):
        config.cfg.cfg.clear()
        config.cfg._load_from_file()

    def test_get_section_merely_calls_get_on_config_data_dict(self):
        orig_method = config.cfg.get

        def fake_get(section):
            self.assertEqual("f@k3", section)
            return {"this": "is", "so": "fake"}

        config.cfg.get = fake_get
        self.assertEqual({"this": "is", "so": "fake"},
                         config.get_section("f@k3"))
        config.cfg.get = orig_method


class GetTestCase(unittest.TestCase):
    """Tests the behaviour of utils.config.get()"""

    def tearDown(self):
        config.cfg.cfg.clear()
        config.cfg._load_from_file()

    def test_get_with_empty_section_data(self):
        # config.get() returns `None` if the section data dict is empty
        with patch('openquake.engine.utils.config.get_section') as mock:
            mock.return_value = dict()
            self.assertTrue(config.get("whatever", "key") is None)
            self.assertEqual(1, mock.call_count)
            self.assertEqual([("whatever",), {}], mock.call_args)

    def test_get_with_nonempty_section_data_and_known_key(self):
        # config.get() correctly returns the configuration datum for known
        # sections/keys
        with patch('openquake.engine.utils.config.get_section') as mock:
            mock.return_value = dict(a=11)
            self.assertEqual(11, config.get("hmmm", "a"))
            self.assertEqual(1, mock.call_count)
            self.assertEqual([("hmmm",), {}], mock.call_args)

    def test_get_with_unknown_key(self):
        """config.get() returns `None` if the `key` is not known."""
        with patch('openquake.engine.utils.config.get_section') as mock:
            mock.return_value = dict(b=1)
            self.assertTrue(config.get("arghh", "c") is None)
            self.assertEqual(1, mock.call_count)
            self.assertEqual([("arghh",), {}], mock.call_args)


class IsReadableTestCase(unittest.TestCase):
    """Tests the behaviour of utils.config.Config.is_readable()."""

    def setUp(self):
        self.orig_env = os.environ.copy()
        os.environ.clear()

    def tearDown(self):
        os.environ.clear()
        os.environ.update(self.orig_env)

    def test_is_readable_no_file_present(self):
        # When no config file is present is_readable() returns `False`
        os.environ["OQ_SITE_CFG_PATH"] = "/this/does/not/exist.cfg"
        os.environ["OQ_LOCAL_CFG_PATH"] = "/nor/does/this.cfg"
        self.assertFalse(config.cfg.is_readable())

    def test_is_readable_all_files_lack_permissions(self):
        # When we miss read permissions for all config files is_readable()
        # returns `False`
        os.environ["OQ_SITE_CFG_PATH"] = "/etc/sudoers"
        os.environ["OQ_LOCAL_CFG_PATH"] = "/etc/passwd-"
        self.assertFalse(config.cfg.is_readable())

    def test_is_readable_one_plus_files_have_permissions(self):
        # When at least one config file is present and we have permission to
        # read it is_readable() returns `True`.
        os.environ["OQ_SITE_CFG_PATH"] = "/etc/passwd"
        os.environ["OQ_LOCAL_CFG_PATH"] = "/etc/passwd-"
        self.assertTrue(config.cfg.is_readable())


class FlagSetTestCase(ConfigTestCase, unittest.TestCase):
    """
    Tests for openquake.engine.utils.config.flag_set()
    """

    def setUp(self):
        self.setup_config()

    def tearDown(self):
        self.teardown_config()

    def test_flag_set_with_absent_key(self):
        # flag_set() returns False if the setting
        # is not present in the configuration file.
        self.prepare_config("a")
        self.assertFalse(config.flag_set("a", "z"))

    def test_flag_set_with_number(self):
        # flag_set() returns False if the setting is present but
        # not equal to 'true'
        self.prepare_config("b", {"y": "123"})
        self.assertFalse(config.flag_set("b", "y"))

    def test_flag_set_with_text_but_not_true(self):
        """
        flag_set() returns False if the setting is present but
        not equal to 'true'.
        """
        self.prepare_config("c", {"x": "blah"})
        self.assertFalse(config.flag_set("c", "x"))

    def test_flag_set_with_true(self):
        """
        flag_set() returns True if the setting is present and equal to 'true'.
        """
        self.prepare_config("d", {"w": "true"})
        self.assertTrue(config.flag_set("d", "w"))

    def test_flag_set_with_True(self):
        """
        flag_set() returns True if the setting is present and equal to 'true'.
        """
        self.prepare_config("e", {"v": " True 	 "})
        self.assertTrue(config.flag_set("e", "v"))

########NEW FILE########
__FILENAME__ = utils_tasks_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Unit tests for the utils.tasks module.
"""

import unittest

from openquake.engine.utils import tasks

from openquake.engine.tests.utils.tasks import failing_task, just_say_hello


class MapReduceTestCase(unittest.TestCase):
    """
    Tests the behaviour of utils.tasks.map_reduce and utils.tasks.parallelize
    """

    def test_single_item(self):
        expected = ["hello"] * 5
        result = tasks.map_reduce(
            just_say_hello, [(i, ) for i in range(5)],
            lambda lst, val: lst + [val], [])
        self.assertEqual(expected, result)

    def test_type_error(self):
        try:
            tasks.map_reduce(just_say_hello, range(5),
                             lambda lst, val: lst + [val], [])
        except TypeError as exc:
            # the message depend on the OQ_NO_DISTRIBUTE flag
            self.assertIn('int', str(exc))
        else:
            raise Exception("Exception not raised.")

    def test_failing_subtask(self):
        try:
            tasks.parallelize(failing_task, [(42, )], None)
        except RuntimeError as exc:
            self.assertIn('NotImplementedError: 42', str(exc))
        else:
            raise Exception("Exception not raised.")

    def test_parallelize(self):
        lst = []
        res = tasks.parallelize(just_say_hello, [(i, ) for i in range(5)],
                                lst.append)
        self.assertEqual(res, None)
        self.assertEqual(lst, ['hello'] * 5)

########NEW FILE########
__FILENAME__ = utils_version_test
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Unit tests for the utils.version module.
"""


from datetime import datetime
from datetime import timedelta
import unittest

from openquake.engine.utils import version


class VersionInfoTestCase(unittest.TestCase):
    """Tests the behaviour of utils.version.info()."""

    def __init__(self, *args, **kwargs):
        super(VersionInfoTestCase, self).__init__(*args, **kwargs)

    def test_info_with_major_number_only(self):
        """Only the major version number is set."""
        self.assertEqual(
            "OpenQuake version 2.0.0", version.info((2, -1, -1, -1)))

    def test_info_with_minor_number_only(self):
        """Only the minor version number is set."""
        self.assertEqual(
            "OpenQuake version 0.2.0", version.info((-1, 2, -1, -1)))

    def test_info_with_sprint_number_only(self):
        """Only the sprint number is set."""
        self.assertEqual(
            "OpenQuake version 0.0.2", version.info((-1, -1, 2, -1)))

    def test_info_with_all_data_in_place(self):
        """All the version information is in place."""
        self.assertEqual(
            "OpenQuake version 0.3.2, released 2011-04-08T06:04:11Z",
            version.info((0, 3, 2, 1302242651)))

    def test_info_with_malformed_version_information(self):
        """The version information is malformed."""
        self.assertEqual(
            "The OpenQuake version is not available.", version.info((-1,)))

    def test_info_with_data_not_integer(self):
        """The version information is malformed (non-integers)."""
        self.assertEqual(
            "The OpenQuake version is not available.",
            version.info(("2", "-1", "-1", "-1")))

    def test_info_with_data_not_a_tuple(self):
        """The version information is malformed (not in a tuple)."""
        self.assertEqual(
            "The OpenQuake version is not available.",
            version.info([2, -1, -1, -1]))

    def test_info_with_datum_less_than_minus_one(self):
        """The version information is malformed (datum less than -1)."""
        self.assertEqual(
            "The OpenQuake version is not available.",
            version.info([2, -1, -1, -2]))

    def test_info_with_release_date_more_than_a_month_in_future(self):
        """The release date is ignored since it is too far in the future."""
        today_plus_60_days = int(
            (datetime.today() + timedelta(days=60)).strftime("%s"))
        self.assertEqual(
            "OpenQuake version 0.3.2",
            version.info((0, 3, 2, today_plus_60_days)))

########NEW FILE########
__FILENAME__ = extract_gmvs
# -*- coding: utf-8 -*-

# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Extract ground motion values generated from a hazard calculation and
display them to standard output in CSV format
"""

import sys
import csv
from openquake.engine.db import models
from openquake.hazardlib.imt import from_string


def extract(hc_id, a_writer):
    hc = models.HazardCalculation.objects.get(pk=hc_id)

    for lt in models.LtRealization.objects.filter(
            lt_model__hazard_calculation=hc):

        for imt in hc.intensity_measure_types:
            imt_type, sa_period, _ = from_string(imt)

            if imt_type == "PGA":
                imt_type_fix = "SA"
                sa_period_fix = 0
            else:
                imt_type_fix = imt_type
                sa_period_fix = sa_period

            ruptures = sorted(
                [r.id for r in models.SESRupture.objects.filter(
                    rupture__ses_collection__lt_realization=lt)])

            for site in hc.hazardsite_set.all().order_by('id'):
                gmvs = []
                gmvs_data = dict()

                for ses_coll in models.SESCollection.objects.filter(
                        lt_realization=lt).order_by('id'):
                    for ses in ses_coll:
                        for gmf in models.GmfData.objects.filter(
                                ses_id=ses.ordinal,
                                site=site,
                                imt=imt_type, sa_period=sa_period):
                            gmvs_data.update(
                                dict(zip(gmf.rupture_ids, gmf.gmvs)))
                gmvs.extend([gmvs_data.get(r, 0.0) for r in ruptures])
                a_writer.writerow([lt.id, site.location.x, site.location.y,
                                   imt_type_fix, sa_period_fix] + gmvs)


if __name__ == "__main__":
    if len(sys.argv) != 2 or sys.argv[1] in ['-h', '--help']:
        print "Usage:\n %s <hazard_calculation ID>" % sys.argv[0]
        sys.exit(1)

    extract(sys.argv[1], csv.writer(sys.stdout, delimiter=','))

########NEW FILE########
__FILENAME__ = import_gmf_scenario
import os
import time
import argparse
from openquake.nrmllib.hazard.parsers import GMFScenarioParser
from openquake.hazardlib.imt import from_string
from openquake.engine.db import models
from openquake.engine import writer, engine


def import_rows(hc, gmf_coll, rows):
    """
    Import a list of records into the gmf_data and hazard_site tables.

    :param hc: :class:`openquake.engine.db.models.HazardCalculation` instance
    :param gmf_coll: :class:`openquake.engine.db.models.Gmf` instance
    :param rows: a list of records (imt_type, sa_period, sa_damping, gmvs, wkt)
    """
    gmfs = []
    site_id = {}  # dictionary wkt -> site id
    for imt_type, sa_period, sa_damping, gmvs, wkt in rows:
        if wkt not in site_id:  # create a new site
            site_id[wkt] = models.HazardSite.objects.create(
                hazard_calculation=hc, location=wkt).id
        gmfs.append(
            models.GmfData(
                imt=imt_type, sa_period=sa_period, sa_damping=sa_damping,
                gmvs=gmvs, site_id=site_id[wkt], gmf=gmf_coll, task_no=0))
    del site_id
    writer.CacheInserter.saveall(gmfs)


def import_gmf_scenario(fileobj):
    """
    Parse the file with the GMF fields and import it into the table
    gmf_scenario. It also creates a new output record, unrelated to a job.
    Works both with XML files and tab-separated files with format
    (imt, gmvs, location).
    :returns: the generated :class:`openquake.engine.db.models.Output` object
    and the generated :class:`openquake.engine.db.models.HazardCalculation`
    object.
    """
    t0 = time.time()
    fname = fileobj.name

    job = engine.prepare_job()
    hc = models.HazardCalculation.objects.create(
        base_path=os.path.dirname(fname),
        description='Scenario importer, file %s' % os.path.basename(fname),
        calculation_mode='scenario',
        maximum_distance=100,
        intensity_measure_types_and_levels={},
        inputs={},
    )
    # XXX: probably the maximum_distance should be entered by the user

    out = models.Output.objects.create(
        oq_job=job,
        display_name='Imported from %r' % fname,
        output_type='gmf_scenario')

    gmf_coll = models.Gmf.objects.create(output=out)

    rows = []
    if fname.endswith('.xml'):
        # convert the XML into a tab-separated StringIO
        for imt, gmvs, loc in GMFScenarioParser(fileobj).parse():
            hc.intensity_measure_types_and_levels[imt] = []
            imt_type, sa_period, sa_damping = from_string(imt)
            sa_period = '\N' if sa_period is None else str(sa_period)
            sa_damping = '\N' if sa_damping is None else str(sa_damping)
            gmvs = '{%s}' % str(gmvs)[1:-1]
            rows.append([imt_type, sa_period, sa_damping, gmvs, loc])
    else:  # assume a tab-separated file
        for line in fileobj:
            rows.append(line.split('\t'))
    import_rows(hc, gmf_coll, rows)
    hc.number_of_ground_motion_fields = len(rows)
    hc.save()  # update intensity_measure_types_and_levels
    job.hazard_calculation = hc
    job.duration = time.time() - t0
    job.status = 'complete'
    job.save()
    return out

if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('fname')
    arg = p.parse_args()
    print 'Imported', import_gmf_scenario(arg.fname)

########NEW FILE########
__FILENAME__ = import_hazard_curves
import os
import argparse
from cStringIO import StringIO

from django.db import connections

from openquake.nrmllib.hazard.parsers import HazardCurveXMLParser
from openquake.engine.db import models
from openquake.engine import engine


def import_hazard_curves(fileobj):
    """
    Parse the file with the hazard curves and import it into the tables
    hazard_curve and hazard_curve_data. It also creates a new output record,
    unrelated to a job.

    :param fileobj:
        a file-like object associated to an XML file
    :returns:
        the generated :class:`openquake.engine.db.models.Output` object
        and the generated :class:`openquake.engine.db.models.HazardCalculation`
        object.
    """
    fname = fileobj.name
    curs = connections['job_init'].cursor().cursor.cursor  # DB API cursor
    job = engine.prepare_job()
    hc = models.HazardCalculation.objects.create(
        base_path=os.path.dirname(fname),
        description='HazardCurve importer, file %s' % os.path.basename(fname),
        calculation_mode='classical', maximum_distance=100)
    # XXX: what about the maximum_distance?

    out = models.Output.objects.create(
        display_name='Imported from %r' % fname, output_type='hazard_curve',
        oq_job=job)

    f = StringIO()
    # convert the XML into a tab-separated StringIO
    hazcurve = HazardCurveXMLParser(fileobj).parse()
    haz_curve = models.HazardCurve.objects.create(
        investigation_time=hazcurve.investigation_time,
        imt=hazcurve.imt,
        imls=hazcurve.imls,
        quantile=hazcurve.quantile_value,
        statistics=hazcurve.statistics,
        sa_damping=hazcurve.sa_damping,
        sa_period=hazcurve.sa_period,
        output=out)
    hazard_curve_id = str(haz_curve.id)
    for node in hazcurve:
        loc = node.location
        poes = node.poes
        poes = '{%s}' % str(poes)[1:-1]
        print >> f, '\t'.join([hazard_curve_id, poes,
                               'SRID=4326;POINT(%s %s)' % (loc.x, loc.y)])
    f.reset()
    ## import the file-like object with a COPY FROM
    try:
        curs.copy_expert(
            'copy hzrdr.hazard_curve_data (hazard_curve_id, poes, location) '
            'from stdin', f)
    except:
        curs.connection.rollback()
        raise
    else:
        curs.connection.commit()
    finally:
        f.close()
    job.hazard_calculation = hc
    job.save()
    return out

if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('fname')
    arg = p.parse_args()
    print 'Imported', import_hazard_curves(arg.fname)

########NEW FILE########
__FILENAME__ = load_hazards
"""
Load Hazard Calculations dump produced with the
--dump-hazard-calculation option.

The general workflow to load a calculation is the following:

1) Create a "staging" table for each target table (e.g. load_gmf,
load_oq_job, etc.)

2) Use "COPY FROM" statements to populate such table from the content
of the dump

3) INSERT the data SELECTed from the temporary tables INTO the
effective tables in the proper order RETURNING the id of the newly
created rows.

4) If the table is referenced in other tables, we create a temporary
table which maps the old id to the new one. Such table is used in the
SELECT at step 3 to insert the proper foreign key values
"""

import gzip
import logging
import os

from openquake.engine.db import models
from django.db.models import fields

log = logging.getLogger()


def quote_unwrap(name):
    if name.startswith("\""):
        return name[1:-1]
    else:
        return name


def load_tablename(original_tablename):
    _schema, tname = map(quote_unwrap, original_tablename.split('.'))
    return "htemp.load_%s" % tname


def transfer_data(curs, model, **foreign_keys):
    def model_table(model, load=False):
        original_tablename = "\"%s\"" % model._meta.db_table
        if load:
            return load_tablename(original_tablename)
        else:
            return "{}.{}".format(
                *map(quote_unwrap, original_tablename.split('.')))

    def model_fields(model):
        fs = ", ".join([f.column for f in model._meta.fields
                        if f.column != "id"
                        if not isinstance(f, fields.related.ForeignKey)])
        if fs:
            fs = ", " + fs
        return fs

    conn = curs.connection

    # FIXME(lp). In order to avoid alter the table, we should use a
    # data modifying CTE. I am not using data modifying CTE as I
    # consider it a maintenance nightmare at this moment.
    curs.execute(
        "ALTER TABLE %s ADD load_id INT" % model_table(model))
    args = dict(
        table=model_table(model),
        fields=model_fields(model),
        load_table=model_table(model, load=True),
        fk_fields="", fk_joins="", new_fk_ids="")

    if foreign_keys:
        for fk, id_mapping in foreign_keys.iteritems():
            if fk is not None:
                curs.execute(
                    "CREATE TABLE temp_%s_translation("
                    "%s INT NOT NULL, new_id INT NOT NULL)" % (fk, fk))
                ids = ", ".join(["(%d, %d)" % (old_id, new_id)
                                 for old_id, new_id in id_mapping])
                curs.execute(
                    "INSERT INTO temp_%s_translation VALUES %s" % (fk, ids))

                args['fk_fields'] += ", %s" % fk
                args['fk_joins'] += (
                    "JOIN temp_%s_translation USING(%s) " % (fk, fk))
                args['new_fk_ids'] += ", temp_%s_translation.new_id" % fk

    query = """
INSERT INTO %(table)s (load_id %(fields)s %(fk_fields)s)
SELECT id %(fields)s %(new_fk_ids)s
FROM %(load_table)s AS load
%(fk_joins)s
RETURNING  load_id, %(table)s.id
""" % args

    curs.execute(query)
    old_new_ids = curs.fetchall()
    curs.execute(
        "ALTER TABLE %s DROP load_id" % model_table(model))

    for fk in foreign_keys:
        curs.execute("DROP TABLE temp_%s_translation" % fk)

    conn.commit()

    return old_new_ids


def safe_load(curs, filename, original_tablename):
    """
    Load a postgres table into the database, by skipping the ids
    which are already taken. Assume that the first field of the table
    is an integer id and that gzfile.name has the form
    '/some/path/tablename.csv.gz' The file is loaded in blocks to
    avoid memory issues.

    :param curs: a psycopg2 cursor
    :param filename: the path to the csv dump
    :param str tablename: full table name
    """
    # keep in memory the already taken ids
    conn = curs.connection
    tablename = load_tablename(original_tablename)
    try:
        curs.execute("DROP TABLE IF EXISTS %s" % tablename)
        curs.execute(
            "CREATE TABLE %s AS SELECT * FROM %s WHERE 0 = 1" % (
                tablename, original_tablename))
        curs.copy_expert(
            """COPY %s FROM stdin
               WITH (FORMAT 'csv', HEADER true, ENCODING 'utf8')""" % (
                       tablename), gzip.GzipFile(os.path.abspath(filename)))
    except Exception as e:
        conn.rollback()
        log.error(str(e))
        raise
    else:
        conn.commit()


def hazard_load(conn, directory):
    """
    Import a tar file generated by the HazardDumper.

    :param conn: the psycopg2 connection to the db
    :param directory: the pathname to the directory with the .gz files
    """
    filenames = os.path.join(directory, 'FILENAMES.txt')
    curs = conn.cursor()

    created = []
    for line in open(filenames):
        fname = line.rstrip()
        tname = fname[:-7]  # strip .csv.gz

        fullname = os.path.join(directory, fname)
        log.info('Importing %s...', fname)
        created.append(tname)
        safe_load(curs, fullname, tname)
    hc_ids = transfer_data(curs, models.HazardCalculation)
    lt_ids = transfer_data(
        curs, models.LtRealization, hazard_calculation_id=hc_ids)
    transfer_data(
        curs, models.HazardSite, hazard_calculation_id=hc_ids)
    job_ids = transfer_data(
        curs, models.OqJob, hazard_calculation_id=hc_ids)
    out_ids = transfer_data(
        curs, models.Output, oq_job_id=job_ids)
    ses_collection_ids = transfer_data(
        curs, models.SESCollection,
        output_id=out_ids, lt_realization_id=lt_ids)
    ses_ids = transfer_data(
        curs, models.SES, ses_collection_id=ses_collection_ids)
    transfer_data(curs, models.SESRupture, ses_id=ses_ids)

    curs = conn.cursor()
    try:
        for tname in reversed(created):
            query = "DROP TABLE %s" % load_tablename(tname)
            curs.execute(query)
            log.info("Dropped %s" % load_tablename(tname))
    except Exception:
        conn.rollback()
    else:
        conn.commit()
    log.info('Loaded %s', directory)
    return [new_id for _, new_id in hc_ids]

########NEW FILE########
__FILENAME__ = save_hazards
#  -*- coding: utf-8 -*-
#  vim: tabstop=4 shiftwidth=4 softtabstop=4

#  Copyright (c) 2014, GEM Foundation

#  OpenQuake is free software: you can redistribute it and/or modify it
#  under the terms of the GNU Affero General Public License as published
#  by the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.

#  OpenQuake is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.

#  You should have received a copy of the GNU Affero General Public License
#  along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
A script to dump hazard outputs. If you launch it with a given
hazard_calculation_id, it will dump all the hazard outputs relevant for
risk calculations in a directory named hc<hazard-calculation-id>.
The directory can then be moved around and restored in a different
database with the companion script restore_hazards.py.
Internally the dump and restore procedures are based on
COPY TO and COPY FROM commands, so they are quite performant
even for large datasets. They cannot trivially be extended to perform
binary dump/restore since the geography type has no binary form in
PostGIS 1.5.

To restore a hazard computation and all of its outputs into a new database
run ``python restore_hazards.py <directory> <host> <dbname> <user> <password>``

The <user> must have sufficient permissions to write on <dbname>.  If
your database already contains a hazard calculation with the same id,
the restore will not override it. If you think that the hazard
calculation on your database is not important and can removed together
with all of its outputs, then remove it by using ``bin/openquake
--delete-hazard-calculation`` (which must be run by a user with
sufficient permissions). Then run again ``restore_hazards.py``.
"""

import gzip
import itertools
import os
from openquake.engine.db import models
import logging

log = logging.getLogger()


# return a string which is a valid SQL tuple
def _tuplestr(tup):
    return '(%s)' % ', '.join(str(x) for x in tup)


class Copier(object):
    """
    Small wrapper around a psycopg2 cursor, which a .copy method
    writing directly to csv files. It remembers the copied filenames,
    which are stored in the attribute .filenames.
    """
    def __init__(self, psycopg2_cursor):
        self._cursor = psycopg2_cursor
        self.filenames = []

    def tuplestr(self, query, *args):
        """Retrieve tuples of ids a strings"""
        self._cursor.execute(query, args)
        return _tuplestr(row[0] for row in self._cursor)

    def fetchall(self, query, *args):
        """Dispatch to .fetchall"""
        self._cursor.execute(query, args)
        return self._cursor.fetchall()

    def copy(self, query, dest, name, mode):
        """
        Performs a COPY TO/FROM operation. <Works directly with csv files.

        :param str query: the COPY query
        :param str dest: the destination directory
        :param str name: the destination file name
        :param chr mode: 'w' (for COPY TO) or 'r' (for COPY FROM)
        """
        fname = os.path.join(dest, name + ".gz")
        log.info('%s\n(-> %s)', query, fname)
        # here is some trick to avoid storing filename and timestamp info
        with gzip.GzipFile(fname, mode) as fileobj:
            self._cursor.copy_expert(query, fileobj)
            if fname not in self.filenames:
                self.filenames.append(fname)


class HazardDumper(object):
    """
    Class to dump a hazard_calculation and related tables.
    The typical usage is

     hd = HazardDumper(conn, '/tmp/somedir')
     hd.dump(42)  # dump the hazard computation #42
     print hd.mktar()  # generate a tarfile
    """

    def __init__(self, conn, outdir=None):
        self.conn = conn
        self.curs = Copier(conn.cursor())
        outdir = outdir or "/tmp/hc-dump"
        if os.path.exists(outdir):
            # cleanup previously dumped archives, if any
            for fname in os.listdir(outdir):
                if fname.endswith('.csv.gz'):
                    os.remove(os.path.join(outdir, fname))
        else:
            os.mkdir(outdir)
        self.outdir = outdir

    def _copy(self, query, filename, mode='w'):
        self.curs.copy(
            """copy (%s)
               to stdout
               with (format 'csv', header true, encoding 'utf8')""" % query,
            self.outdir, filename, mode)

    def hazard_calculation(self, hc_id, job_id):
        """Dump hazard_calculation, oqjob, lt_realization, hazard_site"""
        self._copy(
            "select * from uiapi.hazard_calculation where id = %s" % hc_id,
            'uiapi.hazard_calculation.csv')
        self._copy(
            """select * from uiapi.oq_job where id = %s""" % job_id,
            'uiapi.oq_job.csv')
        self._copy(
            """select * from hzrdr.lt_realization
               where hazard_calculation_id = %s""" % hc_id,
            'hzrdr.lt_realization.csv')
        self._copy(
            """select * from hzrdi.hazard_site
               where hazard_calculation_id = %s""" % hc_id,
               'hzrdi.hazard_site.csv')

    def output(self, ids):
        """Dump output"""
        self._copy(
            """select * from uiapi.output where id in %s""" % ids,
            'uiapi.output.csv')

    def hazard_curve(self, output):
        """Dump hazard_curve, hazard_curve_data"""
        self._copy(
            """select * from hzrdr.hazard_curve
               where output_id in %s""" % output,
            'hzrdr.hazard_curve.csv', mode='a')

        ids = self.curs.tuplestr(
            'select id from hzrdr.hazard_curve where output_id in %s' % output)

        self._copy(
            """select * from hzrdr.hazard_curve_data
               where hazard_curve_id in {}""".format(ids),
            'hzrdr.hazard_curve_data.csv', mode='a')

    def gmf(self, output):
        """Dump gmf, gmf_data"""
        self._copy(
            """select * from hzrdr.gmf where output_id in %s)""" % output,
               'hzrdr.gmf.csv', mode='a')

        coll_ids = self.curs.tuplestr('select id from hzrdr.gmf '
                                      'where output_id in %s' % output)
        self._copy(
            """select * from hzrdr.gmf_data where gmf_id in %s""" % coll_ids,
            'hzrdr.gmf_data.csv', mode='a')

    def ses(self, output):
        """Dump ses_collection, ses, ses_rupture"""
        self._copy(
            """select * from hzrdr.ses_collection
               where output_id in %s""" % output,
            'hzrdr.ses_collection.csv', 'a')

        coll_ids = self.curs.tuplestr('select id from hzrdr.ses_collection '
                                      'where output_id in %s' % output)
        self._copy(
            """select * from hzrdr.ses
               where ses_collection_id in %s""" % coll_ids,
            'hzrdr.ses.csv', mode='a')

        ses_ids = self.curs.tuplestr(
            'select id from hzrdr.ses where ses_collection_id in %s'
            % coll_ids)
        self._copy(
            """select * from hzrdr.ses_rupture where ses_id in %s""" % ses_ids,
            'hzrdr.ses_rupture.csv', 'a')

    def dump(self, hazard_calculation_id):
        """
        Dump all the data associated to a given hazard_calculation_id
        and relevant for risk.
        """
        hc = models.HazardCalculation.objects.get(pk=hazard_calculation_id)

        outputs = hc.oqjob.output_set.all().values_list('output_type', 'id')

        if not outputs:
            raise RuntimeError(
                'No outputs for hazard calculation %s' % hazard_calculation_id)

        # sort the outputs to prevent foreign key errors
        ordering = {
            'hazard_curve': 1,
            'hazard_curve_multi': 2,
            'ses': 1,
            'gmf': 2,
            'gmf_scenario': 2
        }
        outputs = sorted(outputs, key=lambda o: ordering[o[0]])

        # dump data and collect generated filenames
        self.hazard_calculation(hc.id, hc.oqjob.id)
        all_outs = [output_id for _output_type, output_id in outputs]
        self.output(_tuplestr(all_outs))

        for output_type, output_group in itertools.groupby(outputs, lambda x: x[0]):
            output_ids = [output_id for output_type, output_id in output_group]
            ids = _tuplestr(output_ids)
            print "Dumping %s %s in %s" % (output_type, ids, self.outdir)
            if output_type == 'ses':
                self.ses(ids)
            else:
                log.warn("Unsupported output type %s" % output_type)
        # save FILENAMES.txt
        filenames = os.path.join(
            self.outdir, 'FILENAMES.txt')
        with open(filenames, 'w') as f:
            f.write('\n'.join(map(os.path.basename, self.curs.filenames)))


def main(hazard_calculation_id, outdir=None):
    """
    Dump a hazard_calculation and its relative outputs
    """
    logging.basicConfig(level=logging.WARN)

    assert models.HazardCalculation.objects.filter(
        pk=hazard_calculation_id).exists(), ("The provided hazard calculation "
                                             "does not exist")

    hc = HazardDumper(models.getcursor('admin').connection, outdir)
    hc.dump(hazard_calculation_id)
    log.info('Written %s' % hc.outdir)
    return hc.outdir

########NEW FILE########
__FILENAME__ = config
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Various utility functions concerned with configuration.
"""

import ConfigParser
import os
import pwd
import sys

import openquake.engine
from openquake.commonlib.general import str2bool

OQDIR = os.path.dirname(os.path.dirname(openquake.engine.__path__[0]))
#: Environment variable name for specifying a custom openquake.cfg.
#: The file name doesn't matter.
OQ_CONFIG_FILE_VAR = "OQ_CONFIG_FILE"


# singleton
class _Config(object):
    """
    Load the configuration, make each section available in a separate
    dict.

    The configuration locations are specified via environment variables:
        - OQ_SITE_CFG_PATH
        - OQ_LOCAL_CFG_PATH

    In the absence of these environment variables the following hard-coded
    paths will be used in order:
        - /etc/openquake/openquake.cfg
        - ./openquake.cfg

    Please note: settings in the site configuration file are overridden
    by settings with the same key names in the local configuration.
    """
    GLOBAL_PATH = "/etc/openquake/openquake.cfg"
    LOCAL_PATH = os.path.join(OQDIR, "openquake.cfg")
    cfg = dict()

    def __init__(self):
        self._load_from_file()
        self.job_id = -1

    def get(self, name):
        """A dict with key/value pairs for the given `section` or `None`."""
        return self.cfg.get(name)

    def _get_paths(self):
        """Return the paths for the global/local configuration files."""
        global_path = os.environ.get("OQ_SITE_CFG_PATH", self.GLOBAL_PATH)
        local_path = os.environ.get(
            "OQ_LOCAL_CFG_PATH", os.path.abspath(self.LOCAL_PATH))
        paths = [global_path, local_path]

        # User specified
        user_path = os.environ.get(OQ_CONFIG_FILE_VAR)
        if user_path is not None:
            paths.append(user_path)
        return paths

    def _load_from_file(self):
        """Load the config files, set up the section dictionaries."""
        config = ConfigParser.SafeConfigParser()
        config.read(self._get_paths())
        for section in config.sections():
            self.cfg[section] = dict(config.items(section))

    def is_readable(self):
        """Return `True` if at least one config file is readable."""
        for path in self._get_paths():
            if os.access(path, os.R_OK):
                return True
        else:
            return False

    def exists(self):
        """Return `True` if at least one config file exists."""
        return any(os.path.exists(path) for path in self._get_paths())


cfg = _Config()  # the only instance of _Config


def get_section(section):
    """A dictionary of key/value pairs for the given `section` or `None`."""
    return cfg.get(section)


def get(section, key):
    """The configuration value for the given `section` and `key` or `None`."""
    data = get_section(section)
    return data.get(key) if data else None


def abort_if_no_config_available():
    """Call sys.exit() if no openquake configuration file is readable."""
    if not cfg.exists():
        msg = ('Could not find a configuration file in %s. '
               'Probably your are not in the right directory'
               % cfg._get_paths())
        print msg
        sys.exit(2)
    if not cfg.is_readable():
        msg = (
            "\nYou are not authorized to read any of the OpenQuake "
            "configuration files.\n"
            "Please contact a system administrator or run the following "
            "command:\n\n"
            "   sudo gpasswd --add %s openquake"
            % pwd.getpwuid(os.geteuid()).pw_name)
        print msg
        sys.exit(2)


def flag_set(section, setting):
    """True if the given boolean setting is enabled in openquake.cfg

    :param string section: name of the configuration file section
    :param string setting: name of the configuration file setting

    :returns: True if the setting is enabled in openquake.cfg, False otherwise
    """
    setting = get(section, setting)
    if setting is None:
        return False
    return str2bool(setting)


def refresh():
    """
    Re-parse config files and refresh the cached configuration.

    NOTE: Use with caution. Calling this during some phases of a calculation
    could cause undesirable side-effects.
    """
    cfg._load_from_file()

########NEW FILE########
__FILENAME__ = tasks
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""Utility functions related to splitting work into tasks."""

import sys
import cPickle
import traceback
import psutil

from celery.result import ResultSet
from celery.app import current_app
from celery.task import task

from openquake.engine import logs, no_distribute
from openquake.engine.db import models
from openquake.engine.utils import config
from openquake.engine.writer import CacheInserter
from openquake.engine.performance import EnginePerformanceMonitor


class JobNotRunning(Exception):
    pass


ONE_MB = 1024 * 1024


def check_mem_usage(mem_percent=80):
    """
    Display a warning if we are running out of memory

    :param int mem_percent: the memory limit as a percentage
    """
    used_mem_percent = psutil.phymem_usage().percent
    if used_mem_percent > mem_percent:
        logs.LOG.warn('Using over %d%% of the memory!', used_mem_percent)


class Pickled(object):
    """
    An utility to manually pickling/unpickling objects.
    The reason is that celery does not use the HIGHEST_PROTOCOL,
    so relying on celery is slower. Moreover Pickled instances
    have a nice string representation and length giving the size
    of the pickled bytestring.

    :param obj: the object to pickle
    """
    def __init__(self, obj):
        self.clsname = obj.__class__.__name__
        self.pik = cPickle.dumps(obj, cPickle.HIGHEST_PROTOCOL)

    def __repr__(self):
        """String representation of the pickled object"""
        return '<Pickled %s %dK>' % (self.clsname, len(self) / 1024)

    def __len__(self):
        """Length of the pickled bytestring"""
        return len(self.pik)

    def unpickle(self):
        """Unpickle the underlying object"""
        return cPickle.loads(self.pik)


def pickle_sequence(objects):
    """
    Convert an iterable of objects into a list of pickled objects.
    If the iterable contains copies, the pickling will be done only once.
    If the iterable contains objects already pickled, they will not be
    pickled again.

    :param objects: a sequence of objects to pickle
    """
    cache = {}
    out = []
    for obj in objects:
        obj_id = id(obj)
        if obj_id not in cache:
            if isinstance(obj, Pickled):  # already pickled
                cache[obj_id] = obj
            else:  # pickle the object
                cache[obj_id] = Pickled(obj)
        out.append(cache[obj_id])
    return out


def safely_call(func, args):
    """
    Call the given function with the given arguments safely, i.e.
    by trapping the exceptions. Return a pair (result, exc_type)
    where exc_type is None if no exceptions occur, otherwise it
    is the exception class and the result is a string containing
    error message and traceback.

    :param func: the function to call
    :param args: the arguments
    """
    try:
        return func(*args), None
    except:
        etype, exc, tb = sys.exc_info()
        tb_str = ''.join(traceback.format_tb(tb))
        return '\n%s%s: %s' % (tb_str, etype.__name__, exc), etype


def aggregate_result_set(rset, agg, acc):
    """
    Loop on a set of celery AsyncResults and update the accumulator
    by using the aggregation function.

    :param rset: a :class:`celery.result.ResultSet` instance
    :param agg: the aggregation function, (acc, val) -> new acc
    :param acc: the initial value of the accumulator
    :returns: the final value of the accumulator
    """
    backend = current_app().backend
    for task_id, result_dict in rset.iter_native():
        check_mem_usage()  # log a warning if too much memory is used
        result_pik = result_dict['result']
        result, exctype = result_pik.unpickle()  # this is always negligible
        if exctype:
            raise RuntimeError(result)
        acc = agg(acc, result)
        del backend._cache[task_id]  # work around a celery bug
    return acc


def log_percent_gen(taskname, todo, progress):
    """
    Generator factory. Each time the generator object is called
    log a message if the percentage is bigger than the last one.
    Yield the number of calls done at the current iteration.

    :param str taskname:
        the name of the task
    :param int todo:
        the number of times the generator object will be called
    :param progress:
        a logging function for the progress report
    """
    progress('spawned %d tasks of kind %s', todo, taskname)
    yield 0
    done = 1
    prev_percent = 0
    while done < todo:
        percent = int(float(done) / todo * 100)
        if percent > prev_percent:
            progress('%s %3d%%', taskname, percent)
            prev_percent = percent
        yield done
        done += 1
    progress('%s 100%%', taskname)
    yield done


class OqTaskManager(object):
    """
    A manager to submit several tasks of the same type.
    The usage is::

      oqm = OqTaskManager(do_something, logs.LOG.progress)
      oqm.send(arg1, arg2)
      oqm.send(arg3, arg4)
      print oqm.aggregate_results(agg, acc)

    Progress report is built-in.
    """
    def __init__(self, oqtask, progress, name=None, distribute=None):
        self.oqtask = oqtask
        self.progress = progress
        self.name = name or oqtask.__name__
        self.distribute = (not no_distribute() if distribute is None
                           else distribute)
        self.results = []
        self.sent = 0

    def aggregate_results(self, agg, acc):
        """
        Loop on a set of results and update the accumulator
        by using the aggregation function.

        :param results: a list of results
        :param agg: the aggregation function, (acc, val) -> new acc
        :param acc: the initial value of the accumulator
        :returns: the final value of the accumulator
        """
        logs.LOG.info('Sent %dM of data', self.sent / ONE_MB)
        log_percent = log_percent_gen(
            self.name, len(self.results), self.progress)
        log_percent.next()

        def agg_and_percent(acc, val):
            res = agg(acc, val)
            log_percent.next()
            return res

        if self.distribute:
            return aggregate_result_set(
                ResultSet(self.results), agg_and_percent, acc)
        return reduce(agg_and_percent, self.results, acc)

    def submit(self, *args):
        """
        Submit an oqtask with the given arguments to celery and return
        an AsyncResult. If the variable OQ_NO_DISTRIBUTE is set, the
        task function is run in process and the result is returned.
        """
        if self.distribute:
            piks = pickle_sequence(args)
            self.sent += sum(len(p) for p in piks)
            check_mem_usage()  # log a warning if too much memory is used
            res = self.oqtask.delay(*piks)
        else:
            res = self.oqtask.task_func(*args)
        self.results.append(res)


def map_reduce(task, task_args, agg, acc, name=None, distribute=None):
    """
    Given a task and an iterable of positional arguments, apply the
    task function to the arguments in parallel and return an aggregate
    result depending on the initial value of the accumulator
    and on the aggregation function. To save memory, the order is
    not preserved and there is no list with the intermediated results:
    the accumulator is incremented as soon as a task result comes.

    NB: if the environment variable OQ_NO_DISTRIBUTE is set the
    tasks are run sequentially in the current process and then
    map_reduce(task, task_args, agg, acc) is the same as
    reduce(agg, itertools.starmap(task, task_args), acc).
    Users of map_reduce should be aware of the fact that when
    thousands of tasks are spawned and large arguments are passed
    or large results are returned they may incur in memory issue:
    this is way the calculators limit the queue with the
    `concurrent_task` concept.

    :param task: a `celery` task callable.
    :param task_args: an iterable over positional arguments
    :param agg: the aggregation function, (acc, val) -> new acc
    :param acc: the initial value of the accumulator
    :returns: the final value of the accumulator
    """
    oqm = OqTaskManager(task, logs.LOG.progress, name, distribute)
    for args in task_args:
        oqm.submit(*args)
    return oqm.aggregate_results(agg, acc)


# used to implement BaseCalculator.parallelize, which takes in account
# the `concurrent_task` concept to avoid filling the Celery queue
def parallelize(task, task_args, side_effect=lambda val: None):
    """
    Given a celery task and an iterable of positional arguments, apply the
    callable to the arguments in parallel. It is possible to pass a
    function side_effect(val) which takes the return value of the
    callable and does something with it (such as saving or printing
    it). Notice that the order is not preserved. parallelize returns None.

    NB: if the environment variable OQ_NO_DISTRIBUTE is set the
    tasks are run sequentially in the current process.

    :param task: a celery task
    :param task_args: an iterable over positional arguments
    :param side_effect: a function val -> None
    """
    map_reduce(task, task_args, lambda acc, val: side_effect(val), None)


def oqtask(task_func):
    """
    Task function decorator which sets up logging and catches (and logs) any
    errors which occur inside the task. Also checks to make sure the job is
    actually still running. If it is not running, the task doesn't get
    executed, so we don't do useless computation.

    :param task_func: the function to decorate
    """
    def wrapped(*args):
        """
        Initialize logs, make sure the job is still running, and run the task
        code surrounded by a try-except. If any error occurs, log it as a
        critical failure.
        """
        # job_id is always assumed to be the first argument
        job_id = args[0]
        job = models.OqJob.objects.get(id=job_id)
        if job.is_running is False:
            # the job was killed, it is useless to run the task
            raise JobNotRunning(job_id)

        # it is important to save the task id soon, so that
        # the revoke functionality can work
        EnginePerformanceMonitor.store_task_id(job_id, tsk)

        with EnginePerformanceMonitor(
                'total ' + task_func.__name__, job_id, tsk, flush=True):
            # tasks write on the celery log file
            logs.set_level(job.log_level)
            check_mem_usage()  # log a warning if too much memory is used
            try:
                # run the task
                return task_func(*args)
            finally:
                # save on the db
                CacheInserter.flushall()
                # the task finished, we can remove from the performance
                # table the associated row 'storing task id'
                models.Performance.objects.filter(
                    oq_job=job,
                    operation='storing task id',
                    task_id=tsk.request.id).delete()
    celery_queue = config.get('amqp', 'celery_queue')
    f = lambda *args: Pickled(
        safely_call(wrapped, [a.unpickle() for a in args]))
    f.__name__ = task_func.__name__
    tsk = task(f, queue=celery_queue)
    tsk.task_func = task_func
    return tsk

########NEW FILE########
__FILENAME__ = version
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
Utility functions related to OpenQuake version information.
"""


from datetime import datetime
from datetime import timedelta


def info(version_data):
    """Return a string with the OpenQuake version infomation.

    Version info data set to -1 will be ignored and assumed to have value
    zero.
    Release dates that lie more than 30 days in the future are ignored.

    :param version_data: A 4-tuple of integers that are the major, minor and
        sprint number respectively. The last datum is the number of seconds
        since epoch and represents the release date.

    :returns: A string with human readable OpenQuake version information.
    """
    error = "The OpenQuake version is not available."
    if not isinstance(version_data, tuple) or len(version_data) != 4:
        return error

    data = []
    for datum in version_data:
        if not isinstance(datum, int):
            return error
        if datum < -1:
            return error
        data.append(str(datum if datum > 0 else 0))

    result = "OpenQuake version %s" % ".".join(data[:3])

    seconds_since_epoch = version_data[-1]

    # Our versioning start on the date below.
    start = int(datetime(year=2011, month=4, day=8).strftime("%s"))
    # The release date should not be more than 30 days in the future.
    end = int((datetime.today() + timedelta(days=30)).strftime("%s"))

    if end > seconds_since_epoch > start:
        release_date = datetime.utcfromtimestamp(
            seconds_since_epoch).isoformat()
        result += ", released %sZ" % release_date

    return result

########NEW FILE########
__FILENAME__ = writer
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


# vim: tabstop=4 shiftwidth=4 softtabstop=4
"""
Base classes for the output methods of the various codecs.
"""

import logging
import weakref
import atexit
from cStringIO import StringIO

from django.db import transaction
from django.db import connections
from django.db import router
from django.contrib.gis.db.models.fields import GeometryField
from django.contrib.gis.geos.point import Point

LOGGER = logging.getLogger('serializer')


class CacheInserter(object):
    """
    Bulk insert bunches of Django objects by converting them in strings
    and by using COPY FROM.
    """
    instances = weakref.WeakSet()

    @classmethod
    def flushall(cls):
        """
        Flush the caches of all the instances of CacheInserter.
        """
        for instance in cls.instances:
            instance.flush()

    @classmethod
    def saveall(cls, objects, block_size=1000):
        """
        Save a sequence of Django objects in the database in a single
        transaction, by using a COPY FROM. Returns the ids of the inserted
        objects.
        """
        self = cls(objects[0].__class__, block_size)
        curs = connections[self.alias].cursor()
        seq = self.tname.replace('"', '') + '_id_seq'
        with transaction.commit_on_success(using=self.alias):
            reserve_ids = "select nextval('%s') "\
                "from generate_series(1, %d)" % (seq, len(objects))
            curs.execute(reserve_ids)
            ids = [i for (i,) in curs.fetchall()]
            stringio = StringIO()
            for i, obj in zip(ids, objects):
                stringio.write('%d\t%s\n' % (i, self.to_line(obj)))
            stringio.reset()
            curs.copy_from(stringio, self.tname)
            stringio.close()
        return ids

    def __init__(self, dj_model, max_cache_size):
        self.table = dj_model
        self.max_cache_size = max_cache_size
        self.alias = router.db_for_write(dj_model)
        self.tname = '"%s"' % dj_model._meta.db_table
        self._fields = {}
        self.nlines = 0
        self.stringio = StringIO()
        self.instances.add(self)

    @property
    def fields(self):
        """
        Returns the field names as introspected from the db, except the
        id field. The introspection is done only once per table.
        NB: we cannot trust the ordering in the Django model.
        """
        try:
            return self._fields[self.tname]
        except KeyError:
            # introspect the field names from the database
            # by relying on the DB API 2.0
            # NB: we cannot trust the ordering in the Django model
            curs = connections[self.alias].cursor()
            curs.execute('select * from %s where 1=0' % self.tname)
            names = self._fields[self.tname] = [
                r[0] for r in curs.description if r[0] != 'id']
            return names

    def add(self, obj):
        """
        :param obj: a Django model object

        Append an object to the list of objects to save. If the list exceeds
        the max_cache_size, flush it on the database.
        """
        assert isinstance(obj, self.table), 'Expected instance of %s, got %r' \
            % (self.table.__name__, obj)
        line = self.to_line(obj)
        self.stringio.write(line + '\n')
        self.nlines += 1
        if self.nlines >= self.max_cache_size:
            self.flush()

    def flush(self):
        """
        Save the pending objects on the database with a COPY FROM.
        """
        if not self.nlines:
            return

        # save the StringIO object with a COPY FROM
        with transaction.commit_on_success(using=self.alias):
            curs = connections[self.alias].cursor()
            self.stringio.reset()
            curs.copy_from(self.stringio, self.tname, columns=self.fields)
            self.stringio.close()
            self.stringio = StringIO()

        ## TODO: should we add an assert that the number of rows stored
        ## in the db is the expected one? I (MS) have seen a case where
        ## this fails silently (it was for True/False not converted in t/f)

        LOGGER.debug('saved %d rows in %s', self.nlines, self.tname)
        self.nlines = 0

    def to_line(self, obj):
        """
        Convert the fields of a Django object into a line string suitable
        for import via COPY FROM. The encoding is UTF8.
        """
        cols = []
        for f in self.fields:
            col = getattr(obj, f)
            if col is None:
                col = r'\N'
            elif isinstance(col, bool):
                col = 't' if col else 'f'
            elif isinstance(col, Point):
                col = 'SRID=4326;' + col.wkt
            elif isinstance(col, GeometryField):
                col = col.wkt()
            elif isinstance(col, (tuple, list)):
                # for numeric arrays; this is fragile
                col = self.array_to_pgstring(col)
            else:
                col = unicode(col).encode('utf8')
            cols.append(col)
        return '\t'.join(cols)

    @staticmethod
    def array_to_pgstring(a):
        """
        Convert a Python list/array into the Postgres string-representation
        of it.
        """
        ls = []
        for n in a:
            s = str(n)
            if s.endswith('L'):  # strip the trailing "L"
                s = s[:-1]
            ls.append(s)
        return '{%s}' % ','.join(ls)


# just to make sure that flushall is always called
atexit.register(CacheInserter.flushall)

########NEW FILE########
__FILENAME__ = dbsettings
PLATFORM_DATABASES = {
    'platform': {
        'HOST': 'localhost',
        'NAME': "oqplatform",
        'USER': 'oqplatform',
        'PASSWORD': 'openquake',
        'PORT': 5432
    }
}


# Load more settings from a file called local_settings.py if it exists
try:
    from local_settings import *
except ImportError:
    pass

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
import os
import sys
import imp
import subprocess
from django.core.management import execute_manager
try:
    imp.find_module('settings')  # Assumed to be in the same directory.
except ImportError:
    sys.exit("""Error: Can't find the file 'settings.py' in the directory
        containing %r. It appears you've customized things.\nYou'll have to
        run django-admin.py, passing it your settings module.\n""" % __file__)
import settings
from openquake.server import executor

CELERYD = [sys.executable, '-m', 'celery.bin.celeryd', '-l', 'INFO', '--purge',
           '--logfile', '/tmp/celery.log']


# the code here is run in development mode
if __name__ == "__main__":
    # the django autoreloader sets the variable RUN_MAIN; at the beginning
    # it is None, and only at that moment celery must be run
    run_celery = settings.RUN_CELERY and os.environ.get("RUN_MAIN") is None
    if run_celery:
        cel = subprocess.Popen(CELERYD)
        print 'Starting celery, logging on /tmp/celery.log'
    try:
        with executor:
            execute_manager(settings)
    finally:
        if run_celery:
            print '\nKilling celery'
            cel.kill()

########NEW FILE########
__FILENAME__ = routers
APPS = (
    'auth',
    'contenttypes',
    'sessions',
    'sites',
    'admin',
)


class DefaultRouter(object):

    def db_for_read(self, model, **hints):
        if model._meta.app_label in APPS:
            return 'default'
        return None

    def db_for_write(self, model, **hints):
        if model._meta.app_label in APPS:
            return 'default'
        return None

    def allow_syncdb(self, db, model):
        if not db == 'default':
            return False
        return True

########NEW FILE########
__FILENAME__ = settings
import os

from openquake.engine import settings as oqe_settings

DEBUG = False
TEMPLATE_DEBUG = DEBUG
PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))

ADMINS = (
    # ('Your Name', 'your_email@example.com'),
)

MANAGERS = ADMINS

DATABASES = oqe_settings.DATABASES

DATABASE_ROUTERS = ['openquake.engine.db.routers.OQRouter', ]

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# On Unix systems, a value of None will cause Django to use the same
# timezone as the operating system.
# If running in a Windows environment this must be set to the same as your
# system time zone.
TIME_ZONE = 'Europe/Zurich'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# If you set this to False, Django will not format dates, numbers and
# calendars according to the current locale
USE_L10N = True

# Make this unique, and don't share it with anybody.
SECRET_KEY = 'f_6=^^_0%ygcpgmemxcp0p^xq%47yqe%u9pu!ad*2ym^zt+xq$'

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
)

ROOT_URLCONF = 'openquake.server.urls'

INSTALLED_APPS = ('openquake.server',)

# A sample logging configuration. The only tangible logging
# performed by this configuration is to send an email to
# the site admins on every HTTP 500 error.
# See http://docs.djangoproject.com/en/dev/topics/logging for
# more details on how to customize your logging configuration.
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'simple': {
            'format': '%(levelname)s %(message)s'
        },
    },
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'class': 'django.utils.log.AdminEmailHandler'
        },
        'console': {
            'level': 'DEBUG',
            'class': 'logging.StreamHandler',
            'formatter': 'simple'
        },
    },
    'loggers': {
        'django': {
            'handlers': ['console'],
            'level': 'INFO',
            'propagate': True,
        },
        'django.request': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': True,
        },
        'openquake.server': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': True,
        },
    },
}

FILE_UPLOAD_MAX_MEMORY_SIZE = 1

CONCURRENT_JOBS = 1  # recommended setting for development
RUN_CELERY = True  # recommended setting for development

########NEW FILE########
__FILENAME__ = tasks
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2014, GEM Foundation.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public
# License along with this program. If not, see
# <https://www.gnu.org/licenses/agpl.html>.

import sys
import logging
import traceback
import collections
import shutil
import tempfile
import psycopg2
import urllib
import urllib2

from openquake.engine import engine
from openquake.engine.db import models as oqe_models

from openquake.server.dbsettings import PLATFORM_DATABASES as DATABASES


DEFAULT_LOG_LEVEL = 'progress'


# FIXME. Configure logging by using the configuration stored in settings
logger = logging.getLogger(__name__)


class ProgressHandler(logging.Handler):
    """
    A logging handler to update the status of the job as seen
    from the platform.
    """
    def __init__(self, callback_url, calc):
        logging.Handler.__init__(self)
        self.callback_url = callback_url
        self.calc = calc

    def emit(self, record):
        """
        Update the status field on icebox_calculation with the percentage
        """
        update_calculation(
            self.callback_url,
            status=record.getMessage(),
            description=self.calc.description)


def safely_call(func, *args):
    """
    Call the given procedure with the given arguments safely, i.e.
    by trapping the exceptions and logging them.
    """
    try:
        func(*args)
    except Exception as e:
        logger.error(str(e), exc_info=True)


def run_calc(job_type, calc_id, calc_dir,
             callback_url=None, foreign_calc_id=None,
             dbname="platform", log_file=None):
    """
    Run a calculation given the calculation ID. It is assumed that the
    entire calculation profile is already loaded into the oq-engine database
    and is ready to execute. This function never fails; errors are trapped
    but not logged since the engine already logs them.

    :param job_type:
        'hazard' or 'risk'
    :param calc_id:
        the calculation id on the engine
    :param calc_dir:
        the directory with the input files
    :param callback_url:
        the URL to call at the end of the calculation
    :param foreign_calc_id:
        the calculation id on the platform
    :param dbname:
        the platform database name
    """
    if job_type == 'hazard':
        job = oqe_models.OqJob.objects.get(hazard_calculation=calc_id)
    else:
        job = oqe_models.OqJob.objects.get(risk_calculation=calc_id)
    update_calculation(callback_url, status="started", engine_id=calc_id)

    exports = []
    progress_handler = ProgressHandler(callback_url, job.calculation)
    logging.root.addHandler(progress_handler)
    try:
        engine.run_calc(job, DEFAULT_LOG_LEVEL, log_file, exports, job_type)
    except:  # catch the errors before task spawning
        # do not log the errors, since the engine already does that
        exctype, exc, tb = sys.exc_info()
        einfo = ''.join(traceback.format_tb(tb))
        einfo += '%s: %s' % (exctype.__name__, exc)
        update_calculation(callback_url, status="failed", einfo=einfo)
    finally:
        logging.root.removeHandler(progress_handler)

    shutil.rmtree(calc_dir)

    # If requested to, signal job completion and trigger a migration of
    # results.
    if not None in (callback_url, foreign_calc_id):
        _trigger_migration(job, callback_url, foreign_calc_id, dbname)


def _trigger_migration(job, callback_url, foreign_calc_id, dbname="platform"):
    """
    Helper function to initiate a post-calculation migration of results.

    :param OqJob job:
        The job with which the calculation has been run
    :param str callback_url:
        A URL to POST a request to for pulling results out of the
        oq-engine-server.
    :param str foreign_calc_id:
        The id of the foreign calculation
    :param str dbname:
        a key to extract database connection settings from settings.DATABASES
        in order to get a cursor from the platform database
    """
    if not dbname in DATABASES:
        logger.error('Unknow key %s in PLATFORM_DATABASES; '
                     'you forgot to set local_settings.py' % dbname)
        return
    host = DATABASES[dbname]['HOST']
    if host == 'localhost':
        msg = ('Using localhost as database host; probably '
               'you forgot to override PLATFORM_DATABASES in '
               'local_settings.py ')
        logger.error(msg)
        return

    update_calculation(
        callback_url,
        description=job.calculation.description,
        status="transfering outputs")

    platform_connection = psycopg2.connect(
        host=host,
        database=DATABASES[dbname]['NAME'],
        user=DATABASES[dbname]['USER'],
        password=DATABASES[dbname]['PASSWORD'],
        port=DATABASES[dbname]['PORT'])

    try:
        for output in job.output_set.all():
            copy_output(platform_connection, output, foreign_calc_id)
        update_calculation(callback_url, status="creating layers")
    finally:
        platform_connection.close()


def update_calculation(callback_url=None, **query):
    """
    Update a foreign calculation by POSTing `query` data to
    `callback_url`.
    """
    if callback_url is None:
        return
    try:  # post to an external service
        url = urllib2.urlopen(callback_url, data=urllib.urlencode(query))
    finally:
        url.close()


#: Simple structure that holds all the query components needed to
#transfer calculation outputs from the engine database to the platform
#one
DbInterface = collections.namedtuple(
    'DbInterface',
    'export_query target_table fields import_query')


DBINTERFACE = {
    'hazard_curve': DbInterface(
        """SELECT ST_AsText(location) as location, imls, poes
            FROM hzrdr.hazard_curve_data
            JOIN hzrdr.hazard_curve hc ON hc.id = hazard_curve_id
            JOIN uiapi.output o ON o.id = hc.output_id
            WHERE o.id = %(output_id)d""",
        "icebox_hazardcurve",
        "location varchar, imls float[], poes float[]",
        """INSERT INTO
           icebox_hazardcurve(output_layer_id, location, imls, poes)
           SELECT %s, ST_GeomFromText(location, 4326), imls, poes
           FROM temp_icebox_hazardcurve"""),
    'hazard_map': DbInterface(
        """SELECT unnest(lons) as longitude, unnest(lats) as latitude,
                  unnest(imls)
           FROM hzrdr.hazard_map
           JOIN uiapi.output o ON o.id = output_id
            WHERE o.id = %(output_id)d""",
        "icebox_hazardmap",
        "longitude float, latitude float, iml float",
        """INSERT INTO
           icebox_hazardmap(output_layer_id, iml, location)
           SELECT %s, iml, ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)
           FROM temp_icebox_hazardmap"""),
    'ses': DbInterface(
        """SELECT tag, magnitude, St_AsText(hypocenter)
           FROM hzrdr.ses_rupture r
           JOIN hzrdr.probabilistic_rupture pr ON r.id = r.rupture_id
           JOIN hzrdr.ses_collection sc ON pr.ses_collection_id = sc.id
           JOIN uiapi.output o ON o.id = sc.output_id
           WHERE o.id = %(output_id)d""",
        "icebox_ses",
        "tag varchar, magnitude float, hypocenter varchar",
        """INSERT INTO
           icebox_ses(output_layer_id, hypocenter, rupture_tag, magnitude)
           SELECT %s, St_GeomFromText(hypocenter, 4326), tag, magnitude
           FROM temp_icebox_ses"""),
    # TODO: instead of the region_constraint, we should specify the convex
    # hull of the exposure
    'aggregate_loss': DbInterface(
        """SELECT St_AsText(region_constraint), mean, std_dev
           FROM riskr.aggregate_loss al
           JOIN uiapi.output o ON o.id = al.output_id
           JOIN uiapi.risk_calculation rc ON rc.id = %(calculation_id)d
           WHERE o.id = %(output_id)d""",
        "icebox_aggregateloss",
        "region varchar, mean_loss float, stddev_loss float",
        """INSERT INTO
           icebox_aggregateloss(
               output_layer_id, region, mean_loss, stddev_loss)
           SELECT %s, St_GeomFromText(region, 4326), mean_loss, stddev_loss
           FROM temp_icebox_aggregateloss"""),
    'agg_loss_curve': DbInterface(
        """SELECT ST_AsText(region_constraint) as region, losses, poes,
                  average_loss, stddev_loss
           FROM riskr.aggregate_loss_curve_data
           JOIN riskr.loss_curve lc ON lc.id = loss_curve_id
           JOIN uiapi.output o ON o.id = lc.output_id
           JOIN uiapi.risk_calculation rc ON rc.id = %(calculation_id)d
           WHERE o.id = %(output_id)d""",
        "icebox_aggregatelosscurve",
        ("region varchar, losses float[], poes float[], "
         "average_loss float, stddev_loss float"),
        """INSERT INTO icebox_aggregatelosscurve(
               output_layer_id, region, losses, poes, mean_loss, stddev_loss)
           SELECT %s, St_GeomFromText(region, 4326),
                  losses, poes, average_loss, stddev_loss
           FROM temp_icebox_aggregatelosscurve"""),
    'loss_curve': DbInterface(
        """SELECT ST_AsText(location) as location,
                  loss_ratios, poes, average_loss_ratio, stddev_loss_ratio,
                  asset_ref
           FROM riskr.loss_curve_data
           JOIN riskr.loss_curve lc ON lc.id = loss_curve_id
           JOIN uiapi.output o ON o.id = lc.output_id
           WHERE o.id = %(output_id)d""",
        "icebox_losscurve",
        ("location varchar, losses float[], poes float[], "
         "average_loss float, stddev_loss float, asset_ref varchar"),
        """INSERT INTO icebox_losscurve(
               output_layer_id, location, losses, poes,
               average_loss, stddev_loss, asset_ref)
           SELECT %s, St_GeomFromText(location, 4326),
                  losses, poes, average_loss, stddev_loss, asset_ref
           FROM temp_icebox_losscurve"""),
    'loss_map': DbInterface(
        """SELECT ST_AsText(location) as location,
                  value, std_dev, asset_ref
           FROM riskr.loss_map_data
           JOIN riskr.loss_map lm ON lm.id = loss_map_id
           JOIN uiapi.output o ON o.id = lm.output_id
           WHERE o.id = %(output_id)d""",
        "icebox_lossmap",
        "location varchar, loss float, stddev_loss float, asset_ref varchar",
        """INSERT INTO icebox_lossmap(
               output_layer_id, location, loss, stddev_loss, asset_ref)
           SELECT %s, St_GeomFromText(location, 4326),
                  loss, stddev_loss, asset_ref
           FROM temp_icebox_lossmap"""),
    'bcr_distribution': DbInterface(
        """SELECT ST_AsText(location) as location,
                  average_annual_loss_original,
                  average_annual_loss_retrofitted,
                  bcr,
                  asset_ref
           FROM riskr.bcr_distribution_data
           JOIN riskr.bcr_distribution bd ON bd.id = bcr_distribution_id
           JOIN uiapi.output o ON o.id = bd.output_id
           WHERE o.id = %(output_id)d""",
        "icebox_bcrdistribution",
        ("location varchar, average_annual_loss_original float, "
         "average_annual_loss_retrofitted float, bcr float, "
         "asset_ref varchar"),
        """INSERT INTO icebox_bcrdistribution(
               output_layer_id, location,
               average_annual_loss_original,
               average_annual_loss_retrofitted, bcr, asset_ref)
           SELECT %s, St_GeomFromText(location, 4326),
                  average_annual_loss_original,
                  average_annual_loss_retrofitted, bcr, asset_ref
           FROM temp_icebox_bcrdistribution"""),
}


def copy_output(platform_connection, output, foreign_calculation_id):
    """
    Copy `output` data from the engine database to the platform one.

    :param platform_connection: a psycopg2 connection handler
    :param output: a :class:`openquake.engine.db.models.Output` object
    :param foreign_calculation_id: the id of the foreign (platform) calculation
    """

    # the workflow is the following:
    # 1) Insert a pointer to the output into the output_layer table
    # 2) Create a temporary table on the platform
    # 3) Copy data from the engine to a temporary file
    # 4) Copy data to the temporary table from the temporary file
    # 5) Move data from the temporary table to the persistent one
    #    by considering foreign key issues
    engine_cursor = oqe_models.getcursor('admin')
    platform_cursor = platform_connection.cursor()

    with tempfile.TemporaryFile() as temporary_file:
        try:
            platform_cursor.execute(
                """INSERT INTO
                   icebox_outputlayer(display_name, calculation_id, engine_id)
                   VALUES(%s, %s, %s) RETURNING id""",
                (output.display_name, foreign_calculation_id, output.id))

            [[output_layer_id]] = platform_cursor.fetchall()

            iface = DBINTERFACE.get(output.output_type)

            if iface is None:
                # FIXME. Implement proper logging
                print "Output type %s not supported" % output.output_type
                return

            logger.info("Copying to temporary stream")
            engine_cursor.copy_expert(
                """COPY (%s) TO STDOUT
                   WITH (FORMAT 'csv', HEADER true,
                         ENCODING 'utf8', DELIMITER '|')""" % (
                iface.export_query % {
                    'output_id': output.id,
                    'calculation_id': output.oq_job.calculation.id}),
                temporary_file)

            temporary_file.seek(0)

            temp_table = "temp_%s" % iface.target_table
            platform_cursor.execute("DROP TABLE IF EXISTS %s" % temp_table)
            platform_cursor.execute("CREATE TABLE %s(%s)" % (
                temp_table, iface.fields))

            import_query = """COPY %s FROM STDIN
                              WITH (FORMAT 'csv',
                                    HEADER true,
                                    ENCODING 'utf8',
                                    DELIMITER '|')""" % temp_table
            logger.info("Copying from temporary stream")
            platform_cursor.copy_expert(import_query, temporary_file)

            platform_cursor.execute(iface.import_query % output_layer_id)
            platform_cursor.execute("DROP TABLE IF EXISTS %s" % temp_table)
        except Exception as e:
            # FIXME. Implement proper logging
            print str(e)
            platform_connection.rollback()
            raise
        else:
            platform_connection.commit()

########NEW FILE########
__FILENAME__ = views_test
import zipfile
import json
import mock
import os
import StringIO
import shutil
import tempfile

from collections import namedtuple
from django.utils.datastructures import MultiValueDict
from django.core.exceptions import ObjectDoesNotExist
from django.test.client import RequestFactory
from django.utils import unittest

from openquake.engine.utils.tasks import oqtask
from openquake.server import views, executor, tasks
from openquake.server._test_utils import MultiMock

FakeOutput = namedtuple('FakeOutput', 'id, display_name, output_type')

DATADIR = os.path.join(os.path.dirname(__file__), 'data')
TMPDIR = tempfile.gettempdir()


def write(archive, fname):
    archive.write(os.path.join(DATADIR, fname), fname)


class BaseViewTestCase(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.factory = RequestFactory()


class UtilsTestCase(BaseViewTestCase):

    def setUp(self):
        self.request = self.factory.get('does not matter')

    def test__get_base_url_http(self):
        self.request.is_secure = lambda: False
        self.request.META['HTTP_HOST'] = 'www.openquake.org:8080'
        self.assertEqual('http://www.openquake.org:8080',
                         views._get_base_url(self.request))

    def test__get_base_url_https(self):
        self.request.is_secure = lambda: True
        self.request.META['HTTP_HOST'] = 'www.openquake.org'
        self.assertEqual('https://www.openquake.org',
                         views._get_base_url(self.request))


class CalcHazardTestCase(BaseViewTestCase):

    def setUp(self):
        self.request = self.factory.get('/v1/calc/hazard/')
        self.request.META['HTTP_HOST'] = 'www.openquake.org'

    def test_get(self):
        expected_content = [
            {u'url': u'http://www.openquake.org/v1/calc/hazard/1',
             u'status': u'executing',
             u'description': u'description 1',
             u'id': 1},
            {u'url': u'http://www.openquake.org/v1/calc/hazard/2',
             u'status': u'pre_executing',
             u'description': u'description 2',
             u'id': 2},
            {u'url': u'http://www.openquake.org/v1/calc/hazard/3',
             u'status': u'complete',
             u'description': u'description e',
             u'id': 3},
        ]
        with mock.patch('openquake.server.views._get_calcs') as ghc:
            ghc.return_value = [
                (1, 'executing', 'description 1'),
                (2, 'pre_executing', 'description 2'),
                (3, 'complete', 'description e'),
            ]
            response = views.calc(self.request, 'hazard')

            self.assertEqual(200, response.status_code)
            self.assertEqual(expected_content, json.loads(response.content))

    def test_404_no_calcs(self):
        with mock.patch('openquake.server.views._get_calcs') as ghc:
            ghc.return_value = []
            response = views.calc(self.request, 'hazard')

        self.assertEqual(404, response.status_code)


class CalcRiskTestCase(BaseViewTestCase):

    def test_get(self):
        expected_content = [
            {u'url': u'http://www.openquake.org/v1/calc/risk/1',
             u'status': u'executing',
             u'description': u'description 1',
             u'id': 1},
            {u'url': u'http://www.openquake.org/v1/calc/risk/2',
             u'status': u'pre_executing',
             u'description': u'description 2',
             u'id': 2},
            {u'url': u'http://www.openquake.org/v1/calc/risk/3',
             u'status': u'complete',
             u'description': u'description e',
             u'id': 3},
        ]
        with mock.patch('openquake.server.views._get_calcs') as grc:
            grc.return_value = [
                (1, 'executing', 'description 1'),
                (2, 'pre_executing', 'description 2'),
                (3, 'complete', 'description e'),
            ]
            request = self.factory.get('/v1/calc/risk/')
            request.META['HTTP_HOST'] = 'www.openquake.org'
            response = views.calc(request, 'risk')

            self.assertEqual(200, response.status_code)
            self.assertEqual(expected_content, json.loads(response.content))


class CalcToResponseDataTestCase(unittest.TestCase):
    """
    Tests for `openquake.server.views._calc_to_response_data`.
    """

    def setUp(self):
        Field = namedtuple('Field', 'name')
        self.calc = mock.Mock()
        field_names = [
            'base_path', 'export_dir', 'owner',
            'region', 'sites', 'region_constraint', 'sites_disagg',
            'hazard_calculation', 'hazard_output',
            'description', 'maximum_distance',
        ]
        self.calc._meta.fields = [Field(name) for name in field_names]

        # general stuff
        self.calc.base_path = '/foo/bar/'
        self.calc.export_dir = '/tmp/outputs/'
        self.calc.owner = object()

        # geometry
        self.calc.region.geojson = (
            '{ "type": "Polygon", "coordinates": '
            '[[[1, 1], [2, 3], [3, 1], [1, 1]]] }'
        )
        self.calc.sites.geojson = (
            '{ "type": "MultiPoint", "coordinates": '
            '[[100.0, 0.0], [101.0, 1.0]] }'
        )
        self.calc.region_constraint.geojson = (
            '{ "type": "Polygon", "coordinates": '
            '[[[2, 2], [3, 4], [4, 1], [1, 1]]] }'
        )
        self.calc.sites_disagg.geojson = (
            '{ "type": "MultiPoint", "coordinates": '
            '[[100.1, 0.1], [101.1, 1.1]] }'
        )

        # risk inputs
        self.calc.hazard_calculation = object()
        self.calc.hazard_output = object()

        # some sample parameters
        self.calc.description = 'the description'
        self.calc.maximum_distance = 195.5

    def test(self):
        expected = {
            'description': 'the description',
            'maximum_distance': 195.5,
            'owner': self.calc.owner,
            'region': {
                u'coordinates': [[[1, 1], [2, 3], [3, 1], [1, 1]]],
                u'type': u'Polygon',
            },
            'region_constraint': {
                u'coordinates': [[[2, 2], [3, 4], [4, 1], [1, 1]]],
                u'type': u'Polygon',
            },
            'sites': {
                u'coordinates': [[100.0, 0.0], [101.0, 1.0]],
                u'type': u'MultiPoint',
            },
            'sites_disagg': {
                u'coordinates': [[100.1, 0.1], [101.1, 1.1]],
                u'type': u'MultiPoint',
            },
        }

        response_data = views._calc_to_response_data(self.calc)

        self.assertEqual(expected, response_data)


class CalcHazardResultsTestCase(BaseViewTestCase):

    def setUp(self):
        self.request = self.factory.get('/v1/calc/hazard/1/results')
        self.request.META['HTTP_HOST'] = 'www.openquake.org'

    def test(self):
        expected_content = [
            {'id': 1, 'name': 'output1', 'type': 'hazard_curve',
             'url': 'http://www.openquake.org/v1/calc/hazard/result/1'},
            {'id': 2, 'name': 'output2', 'type': 'hazard_curve',
             'url': 'http://www.openquake.org/v1/calc/hazard/result/2'},
            {'id': 3, 'name': 'output3', 'type': 'hazard_map',
             'url': 'http://www.openquake.org/v1/calc/hazard/result/3'},
        ]
        with mock.patch('openquake.engine.engine.get_outputs') as gho:
            with mock.patch('openquake.engine.db.models'
                            '.HazardCalculation.objects.get') as hc_get:
                hc_get.return_value.oqjob.status = 'complete'
                gho.return_value = [
                    FakeOutput(1, 'output1', 'hazard_curve'),
                    FakeOutput(2, 'output2', 'hazard_curve'),
                    FakeOutput(3, 'output3', 'hazard_map'),
                ]
                self.request = self.factory.get('/v1/calc/hazard/1/results')
                self.request.META['HTTP_HOST'] = 'www.openquake.org'
                response = views.calc_results(self.request, 'hazard', 7)

                self.assertEqual(1, gho.call_count)
                self.assertEqual((('hazard', 7), {}), gho.call_args)
                self.assertEqual(200, response.status_code)
                self.assertEqual(expected_content,
                                 json.loads(response.content))

    def test_404_no_outputs(self):
        with mock.patch('openquake.engine.engine.get_outputs') as gho:
            with mock.patch('openquake.engine.db.models'
                            '.HazardCalculation.objects.get') as hc_get:
                hc_get.return_value.oqjob.status = 'complete'
                gho.return_value = []
                response = views.calc_results(self.request, 'hazard', 7)

        self.assertEqual(404, response.status_code)

    def test_404_calc_not_exists(self):
        with mock.patch('openquake.engine.db.models'
                        '.HazardCalculation.objects.get') as hc_get:
            hc_get.side_effect = ObjectDoesNotExist
            response = views.calc_results(self.request, 'hazard', 7)

        self.assertEqual(404, response.status_code)

    def test_404_calc_not_complete(self):
        with mock.patch('openquake.engine.db.models'
                        '.HazardCalculation.objects.get') as hc_get:
            hc_get.return_value.oqjob.status = 'pre_executing'
            response = views.calc_results(self.request, 'hazard', 7)

        self.assertEqual(404, response.status_code)


class CalcRiskResultsTestCase(BaseViewTestCase):

    def setUp(self):
        self.request = self.factory.get('/v1/calc/risk/1/results')
        self.request.META['HTTP_HOST'] = 'www.openquake.org'

    def test(self):
        expected_content = [
            {'id': 1, 'name': 'output1', 'type': 'loss_curve',
             'url': 'http://www.openquake.org/v1/calc/risk/result/1'},
            {'id': 2, 'name': 'output2', 'type': 'loss_curve',
             'url': 'http://www.openquake.org/v1/calc/risk/result/2'},
            {'id': 3, 'name': 'output3', 'type': 'loss_map',
             'url': 'http://www.openquake.org/v1/calc/risk/result/3'},
        ]
        with mock.patch('openquake.engine.engine.get_outputs') as gro:
            with mock.patch('openquake.engine.db.models'
                            '.RiskCalculation.objects.get') as rc_get:
                rc_get.return_value.oqjob.status = 'complete'

                gro.return_value = [
                    FakeOutput(1, 'output1', 'loss_curve'),
                    FakeOutput(2, 'output2', 'loss_curve'),
                    FakeOutput(3, 'output3', 'loss_map'),
                ]
                response = views.calc_results(self.request, 'risk', 1)

            self.assertEqual(200, response.status_code)
            self.assertEqual(expected_content, json.loads(response.content))

    def test_404_no_outputs(self):
        with mock.patch('openquake.engine.engine.get_outputs') as gro:
            with mock.patch('openquake.engine.db.models'
                            '.RiskCalculation.objects.get') as rc_get:
                rc_get.return_value.oqjob.status = 'complete'
                gro.return_value = []
                response = views.calc_results(self.request, 'risk', 1)

        self.assertEqual(404, response.status_code)

    def test_404_calc_not_exists(self):
        with mock.patch('openquake.engine.db.models'
                        '.RiskCalculation.objects.get') as hc_get:
            hc_get.side_effect = ObjectDoesNotExist
            response = views.calc_results(self.request, 'risk', 1)

        self.assertEqual(404, response.status_code)

    def test_404_calc_not_complete(self):
        with mock.patch('openquake.engine.db.models'
                        '.RiskCalculation.objects.get') as hc_get:
            hc_get.return_value.oqjob.status = 'pre_executing'
            response = views.calc_results(self.request, 'risk', 1)

        self.assertEqual(404, response.status_code)


class GetResultTestCase(BaseViewTestCase):
    """
    Tests for :func:`engine.views.get_result` and the helper function
    :func:`engine.views._get_result`.
    """

    def test_hazard_default_export_type(self):
        with mock.patch('openquake.engine.export.hazard.export') as export:
            with mock.patch('openquake.engine.db.models'
                            '.Output.objects.get') as output_get:
                output_get.return_value.oq_job.status = 'complete'
                ret_val = StringIO.StringIO()
                ret_val.write('Fake result file content')
                ret_val.close()
                export.return_value = ret_val

                request = self.factory.get('/v1/calc/hazard/result/37')
                response = views.get_result(request, 'hazard', 37)

                self.assertEqual(200, response.status_code)
                self.assertEqual('Fake result file content', response.content)

                # Test the call to the export function, including the handling
                # for the default export type:
                self.assertEqual(1, export.call_count)
                self.assertEqual(37, export.call_args[0][0])
                self.assertEqual('xml', export.call_args[1]['export_type'])

    def test_hazard(self):
        with mock.patch('openquake.engine.export.hazard.export') as export:
            with mock.patch('openquake.engine.db.models'
                            '.Output.objects.get') as output_get:
                output_get.return_value.oq_job.status = 'complete'

                ret_val = StringIO.StringIO()
                ret_val.write('Fake result file content')
                ret_val.close()
                export.return_value = ret_val

                request = self.factory.get(
                    '/v1/calc/hazard/result/37?export_type=csv'
                )
                response = views.get_result(request, 'hazard', 37)

                self.assertEqual(200, response.status_code)
                self.assertEqual('Fake result file content', response.content)

                self.assertEqual(1, export.call_count)
                self.assertEqual(37, export.call_args[0][0])
                self.assertEqual('csv', export.call_args[1]['export_type'])

    def test_risk_default_export_type(self):
        with mock.patch('openquake.engine.export.risk.export') as export:
            with mock.patch('openquake.engine.db.models'
                            '.Output.objects.get') as output_get:
                output_get.return_value.oq_job.status = 'complete'

                ret_val = StringIO.StringIO()
                ret_val.write('Fake result file content')
                ret_val.close()
                export.return_value = ret_val

                request = self.factory.get('/v1/calc/risk/result/37')
                response = views.get_result(request, 'risk', 37)

                self.assertEqual(200, response.status_code)
                self.assertEqual('Fake result file content', response.content)

                self.assertEqual(1, export.call_count)
                self.assertEqual(37, export.call_args[0][0])
                self.assertEqual('xml', export.call_args[1]['export_type'])

    def test_risk(self):
        with mock.patch('openquake.engine.export.risk.export') as export:
            with mock.patch('openquake.engine.db.models'
                            '.Output.objects.get') as output_get:
                output_get.return_value.oq_job.status = 'complete'

                ret_val = StringIO.StringIO()
                ret_val.write('Fake result file content')
                ret_val.close()
                export.return_value = ret_val

                request = self.factory.get(
                    '/v1/calc/risk/result/37?export_type=csv'
                )
                response = views.get_result(request, 'risk', 37)

                self.assertEqual(200, response.status_code)
                self.assertEqual('Fake result file content', response.content)

                self.assertEqual(1, export.call_count)
                self.assertEqual(37, export.call_args[0][0])
                self.assertEqual('csv', export.call_args[1]['export_type'])


class IsSourceModelTestCase(unittest.TestCase):

    def test_true(self):
        content = '''<?xml version='1.0' encoding='utf-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
    <sourceModel name="Some Source Model">
        <pointSource id="1" name="point" tectonicRegion="Stable Continental Crust">
            <pointGeometry>
                <gml:Point>
                    <gml:pos>-122.0 38.0</gml:pos>
                </gml:Point>
                <upperSeismoDepth>0.0</upperSeismoDepth>
                <lowerSeismoDepth>10.0</lowerSeismoDepth>
            </pointGeometry>
            <magScaleRel>WC1994</magScaleRel>
            <ruptAspectRatio>0.5</ruptAspectRatio>
            <truncGutenbergRichterMFD aValue="-3.5" bValue="1.0" minMag="5.0" maxMag="6.5" />
            <nodalPlaneDist>
                <nodalPlane probability="0.3" strike="0.0" dip="90.0" rake="0.0" />
                <nodalPlane probability="0.7" strike="90.0" dip="45.0" rake="90.0" />
            </nodalPlaneDist>
            <hypoDepthDist>
                <hypoDepth probability="0.5" depth="4.0" />
                <hypoDepth probability="0.5" depth="8.0" />
            </hypoDepthDist>
        </pointSource>
    </sourceModel>
</nrml>
'''
        xml_file = StringIO.StringIO(content)

        self.assertTrue(views._is_source_model(xml_file))

    def test_false(self):
        content = '''<?xml version='1.0' encoding='utf-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">

    <pointRupture>
        <magnitude type="Mw">6.5</magnitude>
        <strike>20.0</strike>
        <dip>90.0</dip>
        <rake>10.0</rake>

        <hypocenterLocation>
            <gml:Point srsName="urn:ogc:def:crs:EPSG::4979">
                <gml:pos>-124.704 40.363 30.0</gml:pos>
            </gml:Point>
        </hypocenterLocation>
    </pointRupture>
</nrml>
'''
        xml_file = StringIO.StringIO(content)

        self.assertFalse(views._is_source_model(xml_file))

    def test_not_nrml(self):
        content = '''<html><head></head><body></body></html>'''
        xml_file = StringIO.StringIO(content)

        with self.assertRaises(AssertionError) as ar:
            views._is_source_model(xml_file)
        self.assertEqual('Input file is not a NRML artifact',
                         ar.exception.message)


class FakeTempUploadedFile(object):

    def __init__(self, path, name):
        self.path = path
        self.name = name

    def temporary_file_path(self):
        return self.path

FakeUser = namedtuple('FakeUser', 'id')
FakeJob = namedtuple(
    'FakeJob', 'status, owner, hazard_calculation, risk_calculation'
)
FakeJob.calculation = property(
    lambda self: self.risk_calculation or self.hazard_calculation)

FakeCalc = namedtuple('FakeCalc', 'id, description')


class RunCalcTestCase(BaseViewTestCase):

    def setUp(self):
        self.request = mock.Mock()
        self.request.user.username = 'openquake'
        self.request.method = 'POST'
        self.request.POST = dict(database="platform")
        self.request.POST['hazard_calculation_id'] = 666
        self.request.META = dict()
        self.request.META['HTTP_HOST'] = 'www.openquake.org'
        self.executor_call_data = dict(count=0, args=None)
        self.executor_submit = executor.submit

        def submit(func, *args, **kwargs):
            self.executor_call_data['args'] = (args, kwargs)
            self.executor_call_data['count'] += 1
        executor.submit = submit

    def tearDown(self):
        executor.submit = self.executor_submit

    def test(self):
        # prepare an archive with the test files
        if os.path.exists('archive.zip'):
            os.remove('archive.zip')
        archive = zipfile.ZipFile('archive.zip', 'w')
        write(archive, 'job.ini')
        write(archive, 'exposure_model.xml')
        write(archive, 'source_model.xml')
        write(archive, 'source_model_logic_tree.xml')
        write(archive, 'gsim_logic_tree.xml')
        write(archive, 'vulnerability_model.xml')
        archive.close()

        archive_file = open('archive.zip')
        self.request.FILES = MultiValueDict({
            'archive': [archive_file]})

        # Set up the mocks:
        mocks = dict(
            mkdtemp='tempfile.mkdtemp',
            job_from_file='openquake.engine.engine.job_from_file',
            run_calc='openquake.server.tasks.run_calc',
        )
        multi_mock = MultiMock(**mocks)

        temp_dir = tempfile.mkdtemp()

        # Set up expected test values for job_from_file:
        jff_exp_call_args = ((os.path.join(temp_dir, 'job.ini'),
                              'platform', 'progress',  [], None, 666), {})

        try:
            with multi_mock:
                multi_mock['mkdtemp'].return_value = temp_dir

                fake_job = FakeJob(
                    'pending', FakeUser(1), None,
                    FakeCalc(777, 'Fake Calc Desc'),
                )
                multi_mock['job_from_file'].return_value = fake_job

                # Call the function under test
                views.run_calc(self.request, 'risk')

            self.assertEqual(1, multi_mock['mkdtemp'].call_count)

            self.assertEqual(1, multi_mock['job_from_file'].call_count)
            self.assertEqual(jff_exp_call_args,
                             multi_mock['job_from_file'].call_args)

            self.assertEqual({
                'count': 1,
                'args': ((multi_mock['run_calc'],
                          'risk', 777, temp_dir, None, None, 'platform', None),
                         {})
                }, self.executor_call_data)
        finally:
            archive_file.close()
            shutil.rmtree(temp_dir)


class SubmitJobTestCase(unittest.TestCase):
    """
    Test that the notification facility update_calculation works
    well when submitting a job, including the case of failed jobs.
    """
    def setUp(self):
        self.rmtree = mock.patch('shutil.rmtree')
        self.nd = mock.patch.dict(os.environ, {'OQ_NO_DISTRIBUTE': '1'})
        self.uc = mock.patch('openquake.server.tasks.update_calculation')
        self.rmtree.start()
        self.nd.start()
        self.uc.start()

    def tearDown(self):
        self.rmtree.stop()
        self.nd.stop()
        self.uc.stop()

    def run_job(self, job_ini, hazard_calculation_id=None):
        cfg_file = os.path.join(DATADIR, job_ini)
        job, future = views.submit_job(
            cfg_file, DATADIR, 'openquake',
            hazard_calculation_id=hazard_calculation_id,
            logfile=os.path.join(TMPDIR, 'server_tests.log'))
        future.result()  # wait
        return job

    def test_haz_risk_ok(self):
        job = self.run_job('job.ini')
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw, {
            'status': '**  complete (hazard)',
            'description': u'Virtual Island Seismic Hazard, ses=5'})

        self.run_job('job_risk.ini', job.hazard_calculation.id)
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw, {
            'status': '**  complete (risk)',
            'description': u'Virtual Island seismic risk'})

    def test_invalid(self):
        # IOError for non-existing files, RuntimeError for validation errors
        with self.assertRaises(IOError):
            self.run_job('job_invalid.ini')
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw['status'], 'failed')
        self.assertIn('IOError:', kw['einfo'])

    def test_error_invalid_task(self):
        # the error here is to use a function instead of an oqtask
        p = mock.patch(
            'openquake.engine.calculators.hazard.event_based.core.'
            'compute_ruptures', lambda *args: None)
        with p:
            self.run_job('job.ini')
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw['status'], 'failed')
        self.assertIn("AttributeError: 'function' object has no attribute "
                      "'request'", kw['einfo'])

    def test_error_in_celery(self):
        # the error here is inside the celery task
        p = mock.patch(
            'openquake.engine.calculators.hazard.event_based.core.'
            'EventBasedHazardCalculator.core_calc_task',
            oqtask(lambda *args: 1 / 0))
        with p:
            self.run_job('job.ini')
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw['status'], 'failed')
        self.assertIn("ZeroDivisionError", kw['einfo'])

    def test_error_in_pre_execute(self):
        # the error here is in the pre_execute phase
        p = mock.patch(
            'openquake.engine.calculators.hazard.event_based.core.'
            'EventBasedHazardCalculator.pre_execute', lambda self: 1 / 0)
        with p:
            self.run_job('job.ini')
        args, kw = tasks.update_calculation.call_args
        self.assertEqual(kw['status'], 'failed')
        self.assertIn("ZeroDivisionError", kw['einfo'])

    @classmethod
    def tearDownClass(cls):
        executor.shutdown()

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls.defaults import patterns
from django.conf.urls.defaults import include
from django.conf.urls.defaults import url

urlpatterns = patterns(
    '',
    url(r'^v1/calc/', include('openquake.server.v1.calc_urls')),
)

########NEW FILE########
__FILENAME__ = calc_urls
from django.conf.urls.defaults import patterns
from django.conf.urls.defaults import url

# each url is prefixed with /v1/calc/
urlpatterns = patterns(
    'openquake.server.views',
    url(r'^(hazard|risk)$', 'calc'),
    url(r'^(hazard|risk)/(\d+)$', 'calc_info'),
    url(r'^(hazard|risk)/(\d+)/results$', 'calc_results'),
    url(r'^(hazard|risk)/result/(\d+)$', 'get_result'),
    url(r'^(hazard|risk)/run$', 'run_calc'),
)

########NEW FILE########
__FILENAME__ = views
import zipfile
import StringIO
import json
import logging
import os
import shutil
import tempfile
import urlparse

from xml.etree import ElementTree as etree

from django.core.exceptions import ObjectDoesNotExist
from django.http import HttpResponse
from django.http import HttpResponseNotFound
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from openquake import nrmllib
from openquake.engine import engine as oq_engine
from openquake.engine.db import models as oqe_models
from openquake.engine.export import hazard as hazard_export
from openquake.engine.export import risk as risk_export
from openquake.engine.utils.tasks import safely_call
from openquake.server import tasks, executor

METHOD_NOT_ALLOWED = 405
NOT_IMPLEMENTED = 501
JSON = 'application/json'

IGNORE_FIELDS = ('base_path', 'export_dir')
GEOM_FIELDS = ('region', 'sites', 'region_constraint', 'sites_disagg')
RISK_INPUTS = ('hazard_calculation', 'hazard_output')

DEFAULT_LOG_LEVEL = 'progress'

#: For exporting calculation outputs, the client can request a specific format
#: (xml, geojson, csv, etc.). If the client does not specify give them (NRML)
#: XML by default.
DEFAULT_EXPORT_TYPE = 'xml'

EXPORT_CONTENT_TYPE_MAP = dict(xml='application/xml',
                               geojson='application/json')
DEFAULT_CONTENT_TYPE = 'text/plain'

LOGGER = logging.getLogger('openquake.server')

ACCESS_HEADERS = {'Access-Control-Allow-Origin': '*',
                  'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                  'Access-Control-Max-Age': 1000,
                  'Access-Control-Allow-Headers': '*'}


# Credit for this decorator to https://gist.github.com/aschem/1308865.
def cross_domain_ajax(func):
    def wrap(request, *args, **kwargs):
        # Firefox sends 'OPTIONS' request for cross-domain javascript call.
        if not request.method == "OPTIONS":
            response = func(request, *args, **kwargs)
        else:
            response = HttpResponse()
        for k, v in ACCESS_HEADERS.iteritems():
            response[k] = v
        return response
    return wrap


def _get_base_url(request):
    """
    Construct a base URL, given a request object.

    This comprises the protocol prefix (http:// or https://) and the host,
    which can include the port number. For example:
    http://www.openquake.org or https://www.openquake.org:8000.
    """
    if request.is_secure():
        base_url = 'https://%s'
    else:
        base_url = 'http://%s'
    base_url %= request.META['HTTP_HOST']
    return base_url


def _calc_to_response_data(calc):
    """
    Extract the calculation parameters into a dictionary.
    """
    fields = [x.name for x in calc._meta.fields if x.name not in IGNORE_FIELDS]
    response_data = {}
    for field_name in fields:
        try:
            value = getattr(calc, field_name)
            if value is not None:
                if field_name in GEOM_FIELDS:
                    response_data[field_name] = json.loads(value.geojson)
                elif field_name in RISK_INPUTS:
                    response_data[field_name] = value.id
                else:
                    response_data[field_name] = value
        except AttributeError:
            # Better that we miss an attribute than crash.
            pass
    return response_data


def create_detect_job_file(*candidates):
    def detect_job_file(files):
        for candidate in candidates:
            try:
                file_idx = [os.path.basename(f)
                            for f in files].index(candidate)
                return files[file_idx]
            except ValueError:
                pass
        raise RuntimeError("No job file found in %s for %s" % (
            str(files), str(candidates)))
    return detect_job_file


def _prepare_job(request, hazard_output_id, hazard_calculation_id,
                 detect_job_file):
    """
    Creates a temporary directory, move uploaded files there and
    select the job file by using the `detect_job_file` callable which
    accepts in input a list holding all the filenames ending with .ini

    :returns: job_file and temp_dir
    """
    temp_dir = tempfile.mkdtemp()
    files = []

    try:
        archive = zipfile.ZipFile(request.FILES['archive'])
    except KeyError:
        # move each file to a new temp dir, using the upload file names,
        # not the temporary ones
        for each_file in request.FILES.values():
            new_path = os.path.join(temp_dir, each_file.name)
            shutil.move(each_file.temporary_file_path(), new_path)
            files.append(new_path)
    else:  # extract the files from the archive into temp_dir
        archive.extractall(temp_dir)
        archive.close()
        for fname in os.listdir(temp_dir):
            files.append(os.path.join(temp_dir, fname))
    job_file = detect_job_file([f for f in files if f.endswith('.ini')])
    return job_file, temp_dir


def _is_source_model(tempfile):
    """
    Return true if an uploaded NRML file is a seismic source model.
    """
    tree = etree.iterparse(tempfile, events=('start', 'end'))
    # pop off the first elements, which should be a <nrml> element
    # and something else
    _, nrml_elem = tree.next()
    _, model_elem = tree.next()

    assert nrml_elem.tag == '{%s}nrml' % nrmllib.NAMESPACE, (
        "Input file is not a NRML artifact"
    )

    if model_elem.tag == '{%s}sourceModel' % nrmllib.NAMESPACE:
        return True
    return False


@require_http_methods(['GET'])
@cross_domain_ajax
def calc_info(request, job_type, calc_id):
    """
    Get a JSON blob containing all of parameters for the given calculation
    (specified by ``calc_id``). Also includes the current job status (
    executing, complete, etc.).
    """
    try:
        response_data = _get_calc_info(job_type, calc_id)
    except ObjectDoesNotExist:
        return HttpResponseNotFound()

    return HttpResponse(content=json.dumps(response_data), content_type=JSON)


# helper function to get job info and calculation params from the
# oq-engine DB, as a dictionary
def _get_calc_info(job_type, calc_id):
    if job_type == 'hazard':
        job = oqe_models.OqJob.objects\
            .select_related()\
            .get(hazard_calculation=calc_id)
        calc = job.hazard_calculation
    else:  # risk
        job = oqe_models.OqJob.objects\
            .select_related()\
            .get(risk_calculation=calc_id)
        calc = job.risk_calculation

    response_data = _calc_to_response_data(calc)
    response_data['status'] = job.status
    return response_data


@require_http_methods(['GET'])
@cross_domain_ajax
def calc(request, job_type):
    """
    Get a list of calculations and report their id, status, description,
    and a url where more detailed information can be accessed.

    Responses are in JSON.
    """
    base_url = _get_base_url(request)

    calc_data = _get_calcs(job_type)
    if not calc_data:
        return HttpResponseNotFound()

    response_data = []
    for hc_id, status, desc in calc_data:
        url = urlparse.urljoin(base_url, 'v1/calc/%s/%d' % (job_type, hc_id))
        response_data.append(
            dict(id=hc_id, status=status, description=desc, url=url)
        )

    return HttpResponse(content=json.dumps(response_data),
                        content_type=JSON)


@csrf_exempt
@cross_domain_ajax
@require_http_methods(['POST'])
def run_calc(request, job_type):
    """
    Run a calculation.

    :param request:
        a `django.http.HttpRequest` object.
    :param job_type:
        string 'hazard' or 'risk'
    """
    callback_url = request.POST.get('callback_url')
    foreign_calc_id = request.POST.get('foreign_calculation_id')

    hazard_output_id = request.POST.get('hazard_output_id')
    hazard_calculation_id = request.POST.get('hazard_calculation_id')

    einfo, exctype = safely_call(
        _prepare_job, (request, hazard_output_id, hazard_calculation_id,
                       create_detect_job_file("job.ini", "job_risk.ini")))
    if exctype:
        tasks.update_calculation(callback_url, status="failed", einfo=einfo)
        raise exctype(einfo)
    else:
        job_file, temp_dir = einfo
    job, _fut = submit_job(job_file, temp_dir, request.POST['database'],
                           callback_url, foreign_calc_id,
                           hazard_output_id, hazard_calculation_id)
    try:
        response_data = _get_calc_info(job_type, job.calculation.id)
    except ObjectDoesNotExist:
        return HttpResponseNotFound()

    return HttpResponse(content=json.dumps(response_data), content_type=JSON)


def submit_job(job_file, temp_dir, dbname,
               callback_url=None, foreign_calc_id=None,
               hazard_output_id=None, hazard_calculation_id=None,
               logfile=None):
    """
    Create a job object from the given job.ini file in the job directory
    and submit it to the job queue.
    """
    job, exctype = safely_call(
        oq_engine.job_from_file, (job_file, "platform", DEFAULT_LOG_LEVEL, [],
                                  hazard_output_id, hazard_calculation_id))
    if exctype:
        tasks.update_calculation(callback_url, status="failed", einfo=job)
        raise exctype(job)

    calc = job.calculation
    job_type = 'risk' if job.calculation is job.risk_calculation else 'hazard'
    future = executor.submit(
        tasks.safely_call, tasks.run_calc, job_type, calc.id, temp_dir,
        callback_url, foreign_calc_id, dbname, logfile)
    return job, future


def _get_calcs(job_type):
    """
    Helper function for get job+calculation data from the oq-engine database.

    Gets all calculation records available.
    """
    if job_type == 'risk':
        return oqe_models.OqJob.objects\
            .select_related()\
            .filter(risk_calculation__isnull=False)\
            .values_list('risk_calculation',
                         'status',
                         'risk_calculation__description')
    else:
        return oqe_models.OqJob.objects\
            .select_related()\
            .filter(hazard_calculation__isnull=False)\
            .values_list('hazard_calculation',
                         'status',
                         'hazard_calculation__description')


@require_http_methods(['GET'])
@cross_domain_ajax
def calc_results(request, job_type, calc_id):
    """
    Get a summarized list of calculation results for a given ``calc_id``.
    Result is a JSON array of objects containing the following attributes:

        * id
        * name
        * type (hazard_curve, hazard_map, etc.)
        * url (the exact url where the full result can be accessed)
    """
    calc_class = oqe_models.RiskCalculation if job_type == 'risk' \
        else oqe_models.HazardCalculation
    # If the specified calculation doesn't exist OR is not yet complete,
    # throw back a 404.
    try:
        calc = calc_class.objects.get(id=calc_id)
        if not calc.oqjob.status == 'complete':
            return HttpResponseNotFound()
    except ObjectDoesNotExist:
        return HttpResponseNotFound()

    base_url = _get_base_url(request)

    results = oq_engine.get_outputs(job_type, calc_id)
    if not results:
        return HttpResponseNotFound()

    response_data = []
    for result in results:
        url = urlparse.urljoin(base_url,
                               'v1/calc/%s/result/%d' % (job_type, result.id))
        datum = dict(
            id=result.id,
            name=result.display_name,
            type=result.output_type,
            url=url,
        )
        response_data.append(datum)

    return HttpResponse(content=json.dumps(response_data))


@cross_domain_ajax
@require_http_methods(['GET'])
def get_result(request, job_type, result_id):
    """
    Download a specific result, by ``result_id``.

    Parameters for the GET request can include an `export_type`, such as 'xml',
    'geojson', 'csv', etc.
    """
    return _get_result(
        request, result_id,
        risk_export.export if job_type == 'risk' else hazard_export.export)


def _get_result(request, result_id, export_fn):
    """
    The common abstracted functionality for getting hazard or risk results.
    The functionality is the same, except for the hazard/risk specific
    ``export_fn``.

    :param request:
        `django.http.HttpRequest` object. Can contain a `export_type` GET
        param (the default is 'xml' if no param is specified).
    :param result_id:
        The id of the requested artifact.
    :param export_fn:
        Export function, which accepts the following params:

            * result_id (int)
            * target (a file path or file-like)
            * export_type (option kwarg)

    :returns:
        If the requested ``result_id`` is not available in the format
        designated by the `export_type`.

        Otherwise, return a `django.http.HttpResponse` containing the content
        of the requested artifact.
    """
    # If the result for the requested ID doesn't exist, OR
    # the job which it is related too is not complete,
    # throw back a 404.
    try:
        output = oqe_models.Output.objects.get(id=result_id)
        job = output.oq_job
        if not job.status == 'complete':
            return HttpResponseNotFound()
    except ObjectDoesNotExist:
        return HttpResponseNotFound()

    export_type = request.GET.get('export_type', DEFAULT_EXPORT_TYPE)

    content = StringIO.StringIO()
    try:
        content = export_fn(result_id, content, export_type=export_type)
    except NotImplementedError, err:
        # Throw back a 404 if the exact export parameters are not supported
        return HttpResponseNotFound(err.message)

    # Just in case the original StringIO object was closed:
    resp_content = StringIO.StringIO()
    # NOTE(LB): We assume that `content` was written to by using normal
    # file-like `write` calls; thus, the buflist should be populated with all
    # of the content. The exporter/writer might have closed the file object (if
    # so, we cannot read from it normally) so instead we should look at the
    # buflist.
    # NOTE(LB): This might be a really stupid implementation, but it's the best
    # I could come up with so far.
    resp_content.writelines(content.buflist)
    del content

    # TODO(LB): A possible necessary optimization--in the future--would be to
    # iteratively stream large files.
    # TODO(LB): Large files could pose a memory consumption problem.
    # TODO(LB): We also may want to limit the maximum file size of results sent
    # via http.
    resp_value = resp_content.getvalue()
    resp_content.close()
    # TODO: Need to look at `content_type`, otherwise XML gets treated at HTML
    # in the browser
    content_type = EXPORT_CONTENT_TYPE_MAP.get(export_type,
                                               DEFAULT_CONTENT_TYPE)
    return HttpResponse(resp_value, content_type=content_type)

########NEW FILE########
__FILENAME__ = _test_utils
import mock


class MultiMock(object):
    """
    Context-managed multi-mock object. This is useful if you need to mock
    multiple things at once. So instead of creating individual patch+mock
    objects for each, you can define them basically as a dictionary. You can
    also use the mock context managers without having to nest `with`
    statements.

    Example usage:

    .. code-block:: python

        # First, define your mock targets as a dictionary.
        # The value of each item is the path to the function/method you wish to
        # mock. The key is basically a shortcut to the mock.
        mocks = {
            'touch': 'openquake.engine.engine.touch_log_file',
            'job': 'openquake.engine.engine.haz_job_from_file',
        }
        multi_mock = MultiMock(**mocks)

        # To start mocking, start the context manager using `with`:
        # with multi_mock:
        with multi_mock:
            # You can mock return values, for example, just as you would with
            # any other Mock object:
            multi_mock['job'].return_value = 'foo'

            # call the function under test which will calls the mocked
            # functions
            engine.run_hazard('job.ini', 'debug', 'oq.log', ['geojson'])

            # To test the mocks, you can simply access each mock from
            # `multi_mock` like a dict:
            assert multi_mock['touch'].call_count == 1
    """

    def __init__(self, **mocks):
        # dict of mock names -> mock paths
        self._mocks = mocks
        self.active_patches = {}
        self.active_mocks = {}

    def __enter__(self):
        for key, value in self._mocks.iteritems():
            the_patch = mock.patch(value)
            self.active_patches[key] = the_patch
            self.active_mocks[key] = the_patch.start()
        return self

    def __exit__(self, *args):
        for each_mock in self.active_mocks.itervalues():
            each_mock.stop()
        for each_patch in self.active_patches.itervalues():
            each_patch.stop()

    def __iter__(self):
        return self.active_mocks.itervalues()

    def __getitem__(self, key):
        return self.active_mocks.get(key)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase1TestCase(qa_utils.BaseQATestCase):

    EXPECTED_PGA_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.45701348633 0.0586267877384 0.00686616439666</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_SA_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="SA" investigationTime="1.0" saPeriod="0.1" saDamping="5.0">
    <IMLs>0.1 0.4 0.6</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.608674764713 0.330830463746 0.201471216873</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_pga = [0.4570, 0.0587, 0.0069]
            expected_curve_sa = [
                0.608675003748, 0.330831513139, 0.201472214825
            ]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            curves = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id
            )

            [pga_curve] = curves.filter(hazard_curve__imt='PGA')
            numpy.testing.assert_array_almost_equal(
                expected_curve_pga, pga_curve.poes, decimal=4
            )

            [sa_curve] = curves.filter(
                hazard_curve__imt='SA', hazard_curve__sa_period=0.1
            )
            numpy.testing.assert_array_almost_equal(
                expected_curve_sa, sa_curve.poes, decimal=4
            )

            # Test the exports as well:
            exported_file = hazard_export.export(
                pga_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_PGA_XML), exported_file)

            exported_file = hazard_export.export(
                sa_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_SA_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase10TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML_B1_B2 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b2" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.00994026570298 0.000753551720765 9.69007927378e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B3 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b3" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0430003905349 0.0011965867367 7.37146638196e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes_b1_b2 = [0.00995, 0.00076, 9.7E-5, 0.0]
            expected_curve_poes_b1_b3 = [0.043, 0.0012, 7.394E-5, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values for the two curves:
            curve_b1_b2, curve_b1_b3 = models.HazardCurveData.objects\
                .filter(hazard_curve__output__oq_job=job.id)\
                .order_by('hazard_curve__lt_realization__lt_model__sm_lt_path')

            # Sanity check, to make sure we have the curves ordered correctly:
            self.assertEqual(
                ['b1', 'b2'],
                curve_b1_b2.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b3'],
                curve_b1_b3.hazard_curve.lt_realization.sm_lt_path)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b2, curve_b1_b2.poes, decimal=4)
            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b3, curve_b1_b3.poes, decimal=4)

            # Test the exports as well:
            exported_file_b1_b2 = hazard_export.export(
                curve_b1_b2.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B2),
                exported_file_b1_b2)

            exported_file_b1_b3 = hazard_export.export(
                curve_b1_b3.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B3),
                exported_file_b1_b3)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase11TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML_B1_B2 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b2" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0055270921432 0.000421641883053 5.75154102291e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B3 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b3" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.00994026570298 0.000753551720765 9.69007927378e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B4 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b4" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0180244115988 0.00133514260633 0.00013735539542 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_MEAN = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves statistics="mean" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0106744601702 0.000803487930336 9.71146367725e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>"""

    EXPECTED_XML_QUANTILE_0_9 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves statistics="quantile" quantileValue="0.9" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0139823386509 0.00104434716355 0.000117128094079 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_QUANTILE_0_1 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves statistics="quantile" quantileValue="0.1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0055270921432 0.000421641883053 5.75154102291e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()
        aaae = numpy.testing.assert_array_almost_equal

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes_b1_b2 = [0.0055, 0.00042, 5.77E-5, 0.0]
            expected_curve_poes_b1_b3 = [0.00995, 0.00076, 9.7E-5, 0.0]
            expected_curve_poes_b1_b4 = [0.018, 0.0013, 0.00014, 0.0]

            expected_mean_poes = [0.01067, 0.0008, 9.774E-5, 0.0]

            expected_q0_1_poes = [0.0055, 0.00042, 5.77E-5, 0.0]
            expected_q0_9_poes = [0.013975, 0.00103, 0.0001185, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values for the two curves:
            curve_b1_b2, curve_b1_b3, curve_b1_b4 = (
                models.HazardCurveData.objects
                .filter(hazard_curve__output__oq_job=job.id,
                        hazard_curve__lt_realization__isnull=False)
                .order_by(
                    'hazard_curve__lt_realization__lt_model__sm_lt_path'))

            # Sanity check, to make sure we have the curves ordered correctly:
            self.assertEqual(
                ['b1', 'b2'],
                curve_b1_b2.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b3'],
                curve_b1_b3.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b4'],
                curve_b1_b4.hazard_curve.lt_realization.sm_lt_path)

            aaae(expected_curve_poes_b1_b2, curve_b1_b2.poes, decimal=4)
            aaae(expected_curve_poes_b1_b3, curve_b1_b3.poes, decimal=4)
            aaae(expected_curve_poes_b1_b4, curve_b1_b4.poes, decimal=4)

            # Test the mean curve:
            [mean_curve] = models.HazardCurveData.objects\
                .filter(hazard_curve__output__oq_job=job.id,
                        hazard_curve__statistics='mean')
            aaae(expected_mean_poes, mean_curve.poes, decimal=4)

            # Test the quantile curves:
            quantile_0_1_curve, quantile_0_9_curve = \
                models.HazardCurveData.objects\
                    .filter(hazard_curve__output__oq_job=job.id,
                            hazard_curve__statistics='quantile')\
                    .order_by('hazard_curve__quantile')
            aaae(expected_q0_1_poes, quantile_0_1_curve.poes, decimal=4)
            aaae(expected_q0_9_poes, quantile_0_9_curve.poes, decimal=4)

            # Test the exports as well:
            exported_file_b1_b2 = hazard_export.export(
                curve_b1_b2.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B2),
                exported_file_b1_b2)

            exported_file_b1_b3 = hazard_export.export(
                curve_b1_b3.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B3),
                exported_file_b1_b3)

            exported_file_b1_b4 = hazard_export.export(
                curve_b1_b4.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B4),
                exported_file_b1_b4)

            exported_file_mean = hazard_export.export(
                mean_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_MEAN),
                exported_file_mean)

            q01_file = hazard_export.export(
                quantile_0_1_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_QUANTILE_0_1),
                q01_file)

            q09_file = hazard_export.export(
                quantile_0_9_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_QUANTILE_0_9),
                q09_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase12TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1_b2" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.751664728823 0.0780348539189 0.00686616439666</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()
        aaae = numpy.testing.assert_array_almost_equal

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.75421006, 0.08098179, 0.00686616]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            aaae(expected_curve_poes, curve.poes, decimal=2)

            # Test the exports as well:
            exported_file = hazard_export.export(
                curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML),
                exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
from nose.plugins.attrib import attr
from qa_tests._utils import BaseQATestCase, compare_hazard_curve_with_csv

CURRENTDIR = os.path.dirname(__file__)


# this test is described in https://bugs.launchpad.net/oq-engine/+bug/1226061
# the CSV files with the expected hazard_curves were provided by Damiano
class ClassicalHazardCase13TestCase(BaseQATestCase):

    @attr('qa', 'hazard', 'classical')
    def test(self):
        cfg = os.path.join(CURRENTDIR, 'job.ini')
        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        lt_paths = [
            ['aFault_aPriori_D2.1', 'BooreAtkinson2008'],
            ['aFault_aPriori_D2.1', 'ChiouYoungs2008'],
            ['bFault_stitched_D2.1_Char', 'BooreAtkinson2008'],
            ['bFault_stitched_D2.1_Char', 'ChiouYoungs2008']]

        csvdir = os.path.join(CURRENTDIR, 'expected_results')
        for sm_path, gsim_path in lt_paths:

            fname = '%s_%s_expected_curves_PGA.dat' % (sm_path, gsim_path)
            compare_hazard_curve_with_csv(
                hc, [sm_path], [gsim_path], 'PGA', None, None,
                os.path.join(csvdir, fname), ' ', rtol=0.3)

            fname = '%s_%s_expected_curves_SA02.dat' % (sm_path, gsim_path)
            compare_hazard_curve_with_csv(
                hc, [sm_path], [gsim_path], 'SA', 0.2, 5.0,
                os.path.join(csvdir, fname), ' ', rtol=0.3)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.
import os
from nose.plugins.attrib import attr
from qa_tests._utils import BaseQATestCase, compare_hazard_curve_with_csv

CURRENTDIR = os.path.dirname(__file__)


# this test is described in https://bugs.launchpad.net/oq-engine/+bug/1226102
# the CSV files with the expected hazard_curves were provided by Damiano
class ClassicalHazardCase14TestCase(BaseQATestCase):

    @attr('qa', 'hazard', 'classical')
    def test(self):
        cfg = os.path.join(CURRENTDIR, 'job.ini')
        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        compare_hazard_curve_with_csv(
            hc, ['simple_fault'], ['AbrahamsonSilva2008'], 'PGA', None, None,
            os.path.join(CURRENTDIR, 'AS2008_expected_curves.dat'), ' ',
            rtol=0.01)

        compare_hazard_curve_with_csv(
            hc, ['simple_fault'], ['CampbellBozorgnia2008'], 'PGA', None, None,
            os.path.join(CURRENTDIR, 'CB2008_expected_curves.dat'), ' ',
            rtol=0.01)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
from nose.plugins.attrib import attr
from qa_tests._utils import BaseQATestCase, compare_hazard_curve_with_csv

CURRENTDIR = os.path.dirname(__file__)


# this test is described in https://bugs.launchpad.net/oq-engine/+bug/1226061
# the CSV files with the expected hazard_curves were provided by Damiano
class ClassicalHazardCase15TestCase(BaseQATestCase):

    @attr('qa', 'hazard', 'classical')
    def test(self):
        cfg = os.path.join(CURRENTDIR, 'job.ini')
        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        lt_paths = [
            [['SM1'], ['BA2008', 'C2003']],
            [['SM1'], ['BA2008', 'T2002']],
            [['SM1'], ['CB2008', 'C2003']],
            [['SM1'], ['CB2008', 'T2002']],
            [['SM2', 'a3pt2b0pt8'], ['BA2008', 'C2003']],
            [['SM2', 'a3pt2b0pt8'], ['BA2008', 'T2002']],
            [['SM2', 'a3pt2b0pt8'], ['CB2008', 'C2003']],
            [['SM2', 'a3pt2b0pt8'], ['CB2008', 'T2002']],
            [['SM2', 'a3b1'], ['BA2008', 'C2003']],
            [['SM2', 'a3b1'], ['BA2008', 'T2002']],
            [['SM2', 'a3b1'], ['CB2008', 'C2003']],
            [['SM2', 'a3b1'], ['CB2008', 'T2002']],
        ]

        csvdir = os.path.join(CURRENTDIR, 'expected_results')
        j = '_'.join
        for sm_path, gsim_path in lt_paths:

            fname = '%s_PGA.txt' % j(sm_path + gsim_path)
            compare_hazard_curve_with_csv(
                hc, sm_path, gsim_path, 'PGA', None, None,
                os.path.join(csvdir, fname), ' ', rtol=1e-7)

            fname = '%s_SA(0.1).txt' % j(sm_path + gsim_path)
            compare_hazard_curve_with_csv(
                hc, sm_path, gsim_path, 'SA', 0.1, 5.0,
                os.path.join(csvdir, fname), ' ', rtol=1e-7)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase2TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.00994026570298 0.000753551720765 9.69007927378e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.0095, 0.00076, 0.000097, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=3)

            # Test the export as well:
            exported_file = hazard_export.export(
                actual_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                 StringIO.StringIO(self.EXPECTED_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase3TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.632120558829 0.484816655418 0.0454104263504</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.63212, 0.47291, 0.04084]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=2)

            # Test the export as well:
            exported_file = hazard_export.export(
                actual_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase4TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.632120558829 0.611128414527 0.251495554108</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.63212, 0.61186, 0.25110]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=3)

            # Test the export as well:
            exported_file = hazard_export.export(
                actual_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase5TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.632120558829 0.547883067773 0.153809112325</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.632120, 0.54811, 0.15241]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=3)

            # Test the export as well:
            exported_file = hazard_export.export(
                actual_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase6TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.864664716763 0.824184571746 0.366622358502</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.86466, 0.82460, 0.36525]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=2)

            # Test the export as well:
            exported_file = hazard_export.export(
                actual_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase7TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML_B1 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.864664716763 0.824184571746 0.366622358502</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B2 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b2" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.632120558829 0.611128414527 0.251495554108</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_MEAN = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves statistics="mean" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.12 0.2</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.794901469383 0.76026772458 0.332084317184</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes_b1 = [0.86466, 0.82460, 0.36525]
            expected_curve_poes_b2 = [0.63212, 0.61186, 0.25110]
            expected_mean_poes = [0.794898, 0.760778, 0.331005]

            job = self.run_hazard(cfg)

            # Test the poe values for the two curves.
            actual_curve_b1, actual_curve_b2 = (
                models.HazardCurveData.objects
                .filter(hazard_curve__output__oq_job=job.id,
                        hazard_curve__lt_realization__isnull=False)
                .order_by('hazard_curve__lt_realization__lt_model__sm_lt_path')
            )

            # Sanity check, to make sure we have the curves ordered correctly:
            self.assertEqual(
                ['b1'], actual_curve_b1.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b2'], actual_curve_b2.hazard_curve.lt_realization.sm_lt_path)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1, actual_curve_b1.poes, decimal=3)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b2, actual_curve_b2.poes, decimal=3)

            # Test the mean curve:
            [mean_curve] = models.HazardCurveData.objects\
                .filter(hazard_curve__output__oq_job=job.id,
                        hazard_curve__statistics='mean')
            numpy.testing.assert_array_almost_equal(
                expected_mean_poes, mean_curve.poes, decimal=3)

            # Test the exports as well:
            exported_file_b1 = hazard_export.export(
                actual_curve_b1.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1), exported_file_b1)

            exported_file_b2 = hazard_export.export(
                actual_curve_b2.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B2), exported_file_b2)

            # mean:
            exported_file_mean = hazard_export.export(
                mean_curve.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_MEAN), exported_file_mean)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase8TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML_B1_B2 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b2" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0948022879866 0.0123017499076 0.00224907994938 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B3 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b3" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.00994026570298 0.000753551720765 9.69007927378e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B4 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b4" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.000999249229013 4.54145190526e-05 4.06200711345e-06 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes_b1_b2 = [0.095163, 0.012362, 0.002262, 0.0]
            expected_curve_poes_b1_b3 = [0.009950, 0.00076, 9.99995E-6, 0.0]
            expected_curve_poes_b1_b4 = [0.0009995, 4.5489E-5, 4.07365E-6, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values for the three curves:
            curve_b1_b2, curve_b1_b3, curve_b1_b4 = (
                models.HazardCurveData.objects
                .filter(hazard_curve__output__oq_job=job.id)
                .order_by('hazard_curve__lt_realization__lt_model__sm_lt_path')
            )

            # Sanity check, to make sure we have the curves ordered correctly:
            self.assertEqual(
                ['b1', 'b2'],
                curve_b1_b2.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b3'],
                curve_b1_b3.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b4'],
                curve_b1_b4.hazard_curve.lt_realization.sm_lt_path)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b2, curve_b1_b2.poes, decimal=3)
            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b3, curve_b1_b3.poes, decimal=3)
            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b4, curve_b1_b4.poes, decimal=3)

            # Test the exports as well:
            exported_file_b1_b2 = hazard_export.export(
                curve_b1_b2.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B2),
                exported_file_b1_b2)

            exported_file_b1_b3 = hazard_export.export(
                curve_b1_b3.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B3),
                exported_file_b1_b3)

            exported_file_b1_b4 = hazard_export.export(
                curve_b1_b4.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B4),
                exported_file_b1_b4)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import StringIO
import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from openquake.engine.export import hazard as hazard_export
from qa_tests import _utils as qa_utils


class ClassicalHazardCase9TestCase(qa_utils.BaseQATestCase):

    EXPECTED_XML_B1_B2 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b2" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.00994026570298 0.000753551720765 9.69007927378e-05 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    EXPECTED_XML_B1_B3 = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <hazardCurves sourceModelTreePath="b1_b3" gsimTreePath="b1" IMT="PGA" investigationTime="1.0">
    <IMLs>0.1 0.4 0.6 1.0</IMLs>
    <hazardCurve>
      <gml:Point>
        <gml:pos>0.0 0.0</gml:pos>
      </gml:Point>
      <poEs>0.0099470354334 0.000760384267168 0.000103737829121 0.0</poEs>
    </hazardCurve>
  </hazardCurves>
</nrml>
"""

    @attr('qa', 'hazard', 'classical')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes_b1_b2 = [0.00995, 0.00076, 9.7E-5, 0.0]
            expected_curve_poes_b1_b3 = [0.00995, 0.00076, 0.000104, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values for the two curves:
            curve_b1_b2, curve_b1_b3 = models.HazardCurveData.objects \
                .filter(hazard_curve__output__oq_job=job.id) \
                .order_by('hazard_curve__lt_realization__lt_model__sm_lt_path')

            # Sanity check, to make sure we have the curves ordered correctly:
            self.assertEqual(
                ['b1', 'b2'],
                curve_b1_b2.hazard_curve.lt_realization.sm_lt_path)
            self.assertEqual(
                ['b1', 'b3'],
                curve_b1_b3.hazard_curve.lt_realization.sm_lt_path)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b2, curve_b1_b2.poes, decimal=4)
            numpy.testing.assert_array_almost_equal(
                expected_curve_poes_b1_b3, curve_b1_b3.poes, decimal=4)

            # Test the exports as well:
            exported_file_b1_b2 = hazard_export.export(
                curve_b1_b2.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B2),
                exported_file_b1_b2)

            exported_file_b1_b3 = hazard_export.export(
                curve_b1_b3.hazard_curve.output.id, result_dir)
            self.assert_xml_equal(
                StringIO.StringIO(self.EXPECTED_XML_B1_B3),
                exported_file_b1_b3)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os.path
from qa_tests import _utils


class DisaggHazardCase1TestCase(_utils.DisaggHazardTestCase):
    working_dir = os.path.dirname(__file__)
    imts = ['PGA', 'SA-0.025']
    fnames = [
        'disagg_matrix(0.02)-lon_10.1-lat_40.1-smltp_b1-gsimltp_b1-ltr_0.xml',
        'disagg_matrix(0.02)-lon_10.1-lat_40.1-smltp_b1-gsimltp_b1-ltr_1.xml',
        'disagg_matrix(0.1)-lon_10.1-lat_40.1-smltp_b1-gsimltp_b1-ltr_0.xml',
        'disagg_matrix(0.1)-lon_10.1-lat_40.1-smltp_b1-gsimltp_b1-ltr_1.xml',
    ]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os.path
from qa_tests import _utils


class DisaggHazardCase2TestCase(_utils.DisaggHazardTestCase):
    working_dir = os.path.dirname(__file__)
    imts = ['PGA']
    fnames = '''\
disagg_matrix(0.02)-lon_0.0-lat_0.0-smltp_source_model_1-gsimltp_ChiouYoungs2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.02)-lon_0.0-lat_0.0-smltp_source_model_2-gsimltp_BooreAtkinson2008.xml
disagg_matrix(0.02)-lon_0.0-lat_0.0-smltp_source_model_2-gsimltp_ChiouYoungs2008.xml
disagg_matrix(0.02)-lon_-3.0-lat_-3.0-smltp_source_model_1-gsimltp_BooreAtkinson2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.02)-lon_-3.0-lat_-3.0-smltp_source_model_1-gsimltp_ChiouYoungs2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.1)-lon_0.0-lat_0.0-smltp_source_model_1-gsimltp_BooreAtkinson2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.1)-lon_0.0-lat_0.0-smltp_source_model_1-gsimltp_ChiouYoungs2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.1)-lon_0.0-lat_0.0-smltp_source_model_2-gsimltp_BooreAtkinson2008.xml
disagg_matrix(0.1)-lon_0.0-lat_0.0-smltp_source_model_2-gsimltp_ChiouYoungs2008.xml
disagg_matrix(0.1)-lon_-3.0-lat_-3.0-smltp_source_model_1-gsimltp_BooreAtkinson2008_YoungsEtAl1997SSlab.xml
disagg_matrix(0.1)-lon_-3.0-lat_-3.0-smltp_source_model_1-gsimltp_ChiouYoungs2008_YoungsEtAl1997SSlab.xml'''.split()

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.
"""
This is a regression test with the goal of avoiding the reintroduction
of a dependence from the configuration parameter concurrent_tasks.
We use a source model with 398 sources and a single SES.
Due to the distance filtering only 7 sources are relevant, but some
of them are area sources generating a lot of point sources.
We test the independence from the parameter concurrent_tasks.
"""

import os
from nose.plugins.attrib import attr
from qa_tests import _utils as qa_utils
from openquake.engine.db import models
from openquake.engine.calculators.hazard.event_based.core import \
    EventBasedHazardCalculator


class EventBasedHazardTestCase(qa_utils.BaseQATestCase):
    DEBUG = False
    # if the test fails and you want to debug it, set this flag:
    # then you will see in /tmp a few files which you can diff
    # to see the problem
    expected_tags = [
        'smlt=00|ses=0001|src=1-389|rup=002-01',
        'smlt=00|ses=0001|src=2-126|rup=001-01',
        'smlt=00|ses=0001|src=2-315|rup=002-01',
        'smlt=00|ses=0001|src=2-52|rup=002-01',
        'smlt=00|ses=0001|src=3-255|rup=002-01']

    expected_gmfs = '''\
GMFsPerSES(investigation_time=5.000000, stochastic_event_set_id=1,
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=smlt=00|ses=0001|src=1-389|rup=002-01
<X=131.00000, Y= 40.00000, GMV=0.0019092>
<X=131.00000, Y= 40.10000, GMV=0.0017119>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=smlt=00|ses=0001|src=2-126|rup=001-01
<X=131.00000, Y= 40.00000, GMV=0.0001250>
<X=131.00000, Y= 40.10000, GMV=0.0001781>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=smlt=00|ses=0001|src=2-315|rup=002-01
<X=131.00000, Y= 40.00000, GMV=0.0015655>
<X=131.00000, Y= 40.10000, GMV=0.0016485>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=smlt=00|ses=0001|src=2-52|rup=002-01
<X=131.00000, Y= 40.00000, GMV=0.0002431>
<X=131.00000, Y= 40.10000, GMV=0.0001554>)
GMF(imt=PGA sa_period=None sa_damping=None rupture_id=smlt=00|ses=0001|src=3-255|rup=002-01
<X=131.00000, Y= 40.00000, GMV=0.0002826>
<X=131.00000, Y= 40.10000, GMV=0.0004627>))'''

    @attr('qa', 'hazard', 'event_based')
    def test_4(self):
        tags_4, gmfs_4 = self.run_with_concurrent_tasks(4)
        self.assertEqual(tags_4, self.expected_tags)
        if self.DEBUG:  # write the output on /tmp so you can diff it
            open('/tmp/4-got.txt', 'w').write(gmfs_4)
            open('/tmp/4-exp.txt', 'w').write(self.expected_gmfs)
        self.assertEqual(gmfs_4, self.expected_gmfs)

    @attr('qa', 'hazard', 'event_based')
    def test_8(self):
        tags_8, gmfs_8 = self.run_with_concurrent_tasks(8)
        self.assertEqual(tags_8, self.expected_tags)
        if self.DEBUG:  # write the output on /tmp so you can diff it
            open('/tmp/8-got.txt', 'w').write(gmfs_8)
            open('/tmp/8-exp.txt', 'w').write(self.expected_gmfs)
        self.assertEqual(gmfs_8, self.expected_gmfs)

    def run_with_concurrent_tasks(self, n):
        orig = EventBasedHazardCalculator.concurrent_tasks.im_func
        EventBasedHazardCalculator.concurrent_tasks = lambda self: n
        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            job = self.run_hazard(cfg)
            tags = models.SESRupture.objects.filter(
                rupture__ses_collection__output__oq_job=job
                ).values_list('tag', flat=True)
            # gets the GMFs for all the ruptures in the only existing SES
            [gmfs_per_ses] = list(models.Gmf.objects.get(output__oq_job=job))
        finally:
            EventBasedHazardCalculator.concurrent_tasks = orig
        return map(str, tags), str(gmfs_per_ses)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class EventBasedHazardCase1TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.4570, 0.0587, 0.0069]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id,
                hazard_curve__imt__isnull=False)

            numpy.testing.assert_array_almost_equal(
                expected_curve_poes, actual_curve.poes, decimal=2)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class EventBasedHazardCase12TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        result_dir = tempfile.mkdtemp()
        aaae = numpy.testing.assert_array_almost_equal

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.75421006, 0.08098179, 0.00686616]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id,
                hazard_curve__imt__isnull=False)

            aaae(expected_curve_poes, curve.poes, decimal=2)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class EventBasedHazardCase13TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        aaae = numpy.testing.assert_array_almost_equal

        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        expected_curve_poes = [0.54736, 0.02380, 0.00000]

        job = self.run_hazard(cfg)

        # Test the poe values of the single curve:
        [curve] = models.HazardCurveData.objects.filter(
            hazard_curve__output__oq_job=job.id,
            hazard_curve__imt__isnull=False)

        aaae(expected_curve_poes, curve.poes, decimal=2)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import shutil
import tempfile

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class EventBasedHazardCase2TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        result_dir = tempfile.mkdtemp()

        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            expected_curve_poes = [0.0095, 0.00076, 0.000097, 0.0]

            job = self.run_hazard(cfg)

            # Test the poe values of the single curve:
            [actual_curve] = models.HazardCurveData.objects.filter(
                hazard_curve__output__oq_job=job.id,
                hazard_curve__imt__isnull=False)

            self.assert_equals_var_tolerance(
                expected_curve_poes, actual_curve.poes
            )
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os

import numpy

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class EventBasedHazardCase4TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        expected_curve_poes = [0.63212, 0.61186, 0.25110]

        job = self.run_hazard(cfg)

        # Test the poe values of the single curve:
        [actual_curve] = models.HazardCurveData.objects.filter(
            hazard_curve__output__oq_job=job.id,
            hazard_curve__imt__isnull=False)

        # NOTE(LB): The expected/actual results are precise, however Dr.
        # Monelli has intructed use that 1 digits of tolerance in this case
        # still tells us something useful.
        numpy.testing.assert_array_almost_equal(
            expected_curve_poes, actual_curve.poes, decimal=1)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os

import numpy

from nose.plugins.attrib import attr
from openquake.engine.db import models
from qa_tests import _utils as qa_utils

GET_GMF_OUTPUTS = '''
select gsim_lt_path, array_concat(gmvs order by site_id, task_no) as gmf
from hzrdr.gmf_data as a, hzrdr.lt_realization as b, hzrdr.gmf as c
where lt_realization_id=b.id and a.gmf_id=c.id and c.output_id in
(select id from uiapi.output where oq_job_id=%d and output_type='gmf')
group by gsim_lt_path, c.output_id, imt, sa_period, sa_damping
order by c.output_id;
'''

# this is an example with 4 realizations for source_model 1,
# 0 realization for source model 2 and 0 realizations
# for source model 3, i.e. a total of 4 realizations
# only two sites are affected by the ground motion shaking
EXPECTED_GMFS = [
    # (gsim_lt_path, gmf) pairs
    (['b1_1'], [0.00340147507904, 0.00161568401851]),
    (['b1_2'], [0.00142871347159, 0.000564978445291]),
    (['b1_3'], [0.00243873007462, 0.001041715044]),
    (['b1_4'], [0.00364899520179, 0.0016602107846])
]


class EventBasedHazardCase5TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        cursor = models.getcursor('job_init')
        cursor.execute(GET_GMF_OUTPUTS % job.id)
        actual_gmfs = cursor.fetchall()
        self.assertEqual(len(actual_gmfs), len(EXPECTED_GMFS))
        for (actual_path, actual_gmf), (expected_path, expected_gmf) in zip(
                actual_gmfs, EXPECTED_GMFS):
            self.assertEqual(actual_path, expected_path)
            numpy.testing.assert_almost_equal(actual_gmf, expected_gmf)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os

from nose.plugins.attrib import attr
from qa_tests import _utils as qa_utils
from qa_tests.hazard.event_based.spatial_correlation import _utils as sc_utils


class EBHazardSpatialCorrelCase1TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')

        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        site_1 = 'POINT(0.0 0.0)'
        site_2 = 'POINT(0.008993 0.0)'

        gmvs_site_1 = sc_utils.get_gmvs_for_location(site_1, job)
        gmvs_site_2 = sc_utils.get_gmvs_for_location(site_2, job)

        joint_prob_0_5 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 0.5, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )
        joint_prob_1_0 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 1.0, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )

        numpy.testing.assert_almost_equal(joint_prob_0_5, 0.99, decimal=1)
        numpy.testing.assert_almost_equal(joint_prob_1_0, 0.41, decimal=1)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os

from nose.plugins.attrib import attr
from qa_tests import _utils as qa_utils
from qa_tests.hazard.event_based.spatial_correlation import _utils as sc_utils


class EBHazardSpatialCorrelCase2TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')

        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        site_1 = 'POINT(0.0 0.0)'
        site_2 = 'POINT(0.008993 0.0)'

        gmvs_site_1 = sc_utils.get_gmvs_for_location(site_1, job)
        gmvs_site_2 = sc_utils.get_gmvs_for_location(site_2, job)

        joint_prob_0_5 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 0.5, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )
        joint_prob_1_0 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 1.0, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )

        numpy.testing.assert_almost_equal(joint_prob_0_5, 0.99, decimal=1)
        numpy.testing.assert_almost_equal(joint_prob_1_0, 0.64, decimal=1)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import numpy
import os

from nose.plugins.attrib import attr
from qa_tests import _utils as qa_utils
from qa_tests.hazard.event_based.spatial_correlation import _utils as sc_utils


class EBHazardSpatialCorrelCase3TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'event_based')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')

        job = self.run_hazard(cfg)
        hc = job.hazard_calculation

        site_1 = 'POINT(0.0 0.0)'
        site_2 = 'POINT(0.008993 0.0)'

        gmvs_site_1 = sc_utils.get_gmvs_for_location(site_1, job)
        gmvs_site_2 = sc_utils.get_gmvs_for_location(site_2, job)

        joint_prob_0_5 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 0.5, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )
        joint_prob_1_0 = sc_utils.joint_prob_of_occurrence(
            gmvs_site_1, gmvs_site_2, 1.0, hc.investigation_time,
            hc.ses_per_logic_tree_path
        )

        numpy.testing.assert_almost_equal(joint_prob_0_5, 0.95, decimal=1)
        numpy.testing.assert_almost_equal(joint_prob_1_0, 0.22, decimal=1)

########NEW FILE########
__FILENAME__ = _utils
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import itertools
import math
from openquake.engine.db import models


def joint_prob_of_occurrence(gmvs_site_1, gmvs_site_2, gmv, time_span,
                             num_ses, delta_gmv=0.1):
    """
    Compute the Poissonian probability of a ground shaking value to be in the
    range [``gmv`` - ``delta_gmv`` / 2, ``gmv`` + ``delta_gmv`` / 2] at two
    different locations within a given ``time_span``.

    :param gmvs_site_1, gmvs_site_2:
        Lists of ground motion values (as floats) for two different sites.
    :param gmv:
        Reference value for computing joint probability.
    :param time_span:
        `investigation_time` parameter from the calculation which produced
        these ground motion values.
    :param num_ses:
        `ses_per_logic_tree_path` parameter from the calculation which produced
        these ground motion values. In other words, the total number of
        stochastic event sets.
    """
    assert len(gmvs_site_1) == len(gmvs_site_2)

    half_delta = float(delta_gmv) / 2
    gmv_close = lambda v: (gmv - half_delta <= v <= gmv + half_delta)
    count = 0
    for gmv_site_1, gmv_site_2 in itertools.izip(gmvs_site_1, gmvs_site_2):
        if gmv_close(gmv_site_1) and gmv_close(gmv_site_2):
            count += 1

    prob = 1 - math.exp(- (float(count) / (time_span * num_ses)) * time_span)
    return prob


def get_gmvs_for_location(location, job):
    """
    Get a list of GMVs (as floats) for a given ``location`` and ``job_id``.

    :param str location:
        Location as a POINT string Well Known Text format
    :param job:
        The job that generated the GMVs
    :returns:
        `list` of ground motion values, as floats
    """
    [output] = job.output_set.filter(output_type='gmf')

    [site] = models.HazardSite.objects.filter(
        hazard_calculation=job.hazard_calculation).extra(
        where=["location::geometry ~= 'SRID=4326;%s'::geometry"
               % location])
    gmvs = []
    for gmf in models.GmfData.objects.filter(site=site, gmf=output.gmf):
        gmvs.extend(gmf.gmvs)
    return gmvs

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import tempfile
import shutil
import numpy
from nose.plugins.attrib import attr
from numpy.testing import assert_almost_equal

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase1TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')

        actual = map(numpy.median, models.get_gmvs_per_site(output, 'PGA'))
        expected_medians = [0.48155582, 0.21123045, 0.14484586]
        assert_almost_equal(actual, expected_medians, decimal=2)

    @attr('qa', 'hazard', 'scenario')
    def test_export(self):
        result_dir = tempfile.mkdtemp()
        try:
            cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
            job = self.run_hazard(cfg)
            [output] = export.core.get_outputs(job.id, 'gmf_scenario')
            exported_file = export.hazard.export(
                output.id, result_dir)
            expected = os.path.join(os.path.dirname(__file__), 'expected.xml')
            self.assert_xml_equal(open(expected), exported_file)
        finally:
            shutil.rmtree(result_dir)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import numpy
from nose.plugins.attrib import attr
from numpy.testing import assert_almost_equal

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase2TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        actual = map(numpy.median, models.get_gmvs_per_site(output, 'PGA'))
        expected_medians = [0.37412136, 0.19021782, 0.1365383]
        assert_almost_equal(actual, expected_medians, decimal=2)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import numpy
from nose.plugins.attrib import attr
from numpy.testing import assert_almost_equal

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


# job.ini contains intensity_measure_types = PGA, SA(0.1)
class ScenarioHazardCase3TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        actual = map(numpy.median, models.get_gmvs_per_site(output, 'PGA'))
        expected_medians_pga = [0.48155582, 0.21123045, 0.14484586]
        assert_almost_equal(actual, expected_medians_pga, decimal=2)

        actual = map(numpy.median, models.get_gmvs_per_site(output, 'SA(0.1)'))
        expected_medians_sa = [0.93913177, 0.40880148, 0.2692668]
        assert_almost_equal(actual, expected_medians_sa, decimal=2)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import numpy
from nose.plugins.attrib import attr
from numpy.testing import assert_almost_equal

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase4TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        actual = map(numpy.median, models.get_gmvs_per_site(output, 'PGA'))
        expected_medians = [0.41615372, 0.22797466, 0.1936226]
        assert_almost_equal(actual, expected_medians, decimal=2)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
from nose.plugins.attrib import attr

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase5TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        gmfs = list(models.get_gmvs_per_site(output, 'PGA', sort=lambda x: x))
        realizations = 1e5
        first_value = 0.5
        second_value = 1.0
        gmfs_within_range_fst = qa_utils.count(first_value, gmfs[0], gmfs[1])
        gmfs_within_range_snd = qa_utils.count(second_value, gmfs[0], gmfs[1])

        self.assertAlmostEqual(gmfs_within_range_fst / realizations,
                               0.03, places=2)
        self.assertAlmostEqual(gmfs_within_range_snd / realizations,
                               0.003, places=3)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
from nose.plugins.attrib import attr

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase6TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        gmfs = list(models.get_gmvs_per_site(output, 'PGA', sort=lambda x: x))
        realizations = 2e4
        first_value = 0.5
        second_value = 1.0
        gmfs_within_range_fst = qa_utils.count(first_value, gmfs[0], gmfs[1])
        gmfs_within_range_snd = qa_utils.count(second_value, gmfs[0], gmfs[1])
        self.assertAlmostEqual(gmfs_within_range_fst / realizations,
                               0.05, places=2)
        self.assertAlmostEqual(gmfs_within_range_snd / realizations,
                               0.006, places=3)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
from nose.plugins.attrib import attr

from openquake.engine import export
from openquake.engine.db import models
from qa_tests import _utils as qa_utils


class ScenarioHazardCase6TestCase(qa_utils.BaseQATestCase):

    @attr('qa', 'hazard', 'scenario')
    def test(self):
        cfg = os.path.join(os.path.dirname(__file__), 'job.ini')
        job = self.run_hazard(cfg)
        [output] = export.core.get_outputs(job.id, 'gmf_scenario')
        gmfs = list(models.get_gmvs_per_site(output, 'PGA', sort=lambda x: x))
        realizations = 1e5
        first_value = 0.5
        second_value = 1.0
        gmfs_within_range_fst = qa_utils.count(first_value, gmfs[0], gmfs[1])
        gmfs_within_range_snd = qa_utils.count(second_value, gmfs[0], gmfs[1])

        self.assertAlmostEqual(gmfs_within_range_fst / realizations,
                               0.02, places=2)
        self.assertAlmostEqual(gmfs_within_range_snd / realizations,
                               0.002, places=3)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


from nose.plugins.attrib import attr as noseattr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models


class ClassicalRiskCase1TestCase(risk.BaseRiskQATestCase):
    EXPECTED_LOSS_CURVE_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossCurves investigationTime="50.0" statistics="mean" unit="USD" lossType="structural">
    <lossCurve assetRef="a1">
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <poEs>0.0393347533677 0.039319630829 0.0384540639673 0.0353555683375 0.0310809359515 0.0269219661169 0.0233091854249 0.0202549286473 0.0176926044553 0.0155616221765 0.0138044829893 0.0111599850445 0.00927277820929 0.00780386210329 0.00660104748954 0.00562104810103 0.00426294495221 0.00347810187546 0.00291642896185 0.00237546166034 0.00185477228722 0.00113319071162 0.000862358303707 0.000784269030445 0.000660062215756 0.000374938542786 0.000230249004394 0.000122823654476 5.72790058706e-05 2.35807221323e-05 8.66392324538e-06</poEs>
      <losses>0.0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.24 0.28 0.32 0.36 0.4 0.48 0.56 0.64 0.72 0.8 0.96 1.12 1.28 1.44 1.6 1.68 1.76 1.84 1.92 2.0</losses>
      <lossRatios>0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.12 0.14 0.16 0.18 0.2 0.24 0.28 0.32 0.36 0.4 0.48 0.56 0.64 0.72 0.8 0.84 0.88 0.92 0.96 1.0</lossRatios>
      <averageLoss>9.3929e-03</averageLoss>
    </lossCurve>
  </lossCurves>
</nrml>
    """

    EXPECTED_LOSS_MAP_0_01_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossMap investigationTime="50.0" poE="0.01" statistics="mean"
    lossCategory="single_asset" unit="USD" lossType="structural">
    <node>
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <loss assetRef="a1" value="0.264586283238"/>
    </node>
  </lossMap>
</nrml>
    """

    EXPECTED_LOSS_MAP_0_02_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossMap investigationTime="50.0" poE="0.02"
           statistics="mean" lossCategory="single_asset" unit="USD" lossType="structural">
    <node>
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <loss assetRef="a1" value="0.141989823521"/>
    </node>
  </lossMap>
</nrml>"""

    EXPECTED_LOSS_MAP_0_05_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossMap investigationTime="50.0" poE="0.05" statistics="mean"
           lossCategory="single_asset" unit="USD" lossType="structural">
    <node>
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <loss assetRef="a1" value="0.0"/>
    </node>
  </lossMap>
</nrml>
    """

    @noseattr('qa', 'risk', 'classical')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("simple_fault_demo_hazard/job.ini"))

        hazard_curve = [
            (0.001, 0.0398612669790014),
            (0.01, 0.039861266979001400), (0.05, 0.039728757480298900),
            (0.10, 0.029613426625612500), (0.15, 0.019827328756491600),
            (0.20, 0.013062270161451900), (0.25, 0.008655387950000430),
            (0.30, 0.005898520593689670), (0.35, 0.004061698589511780),
            (0.40, 0.002811727179526820), (0.45, 0.001995117417776690),
            (0.50, 0.001358705972845710), (0.55, 0.000989667841573727),
            (0.60, 0.000757544444296432), (0.70, 0.000272824002045979),
            (0.80, 0.00), (0.9, 0.00), (1.0, 0.00)]

        models.HazardSite.objects.create(
            hazard_calculation=job.hazard_calculation,
            location="POINT(1 1)")
        models.HazardCurveData.objects.create(
            hazard_curve=models.HazardCurve.objects.create(
                output=models.Output.objects.create_output(
                    job, "Test Hazard curve", "hazard_curve"),
                investigation_time=50,
                imt="PGA", imls=[hz[0] for hz in hazard_curve],
                statistics="mean"),
            poes=[hz[1] for hz in hazard_curve],
            location="POINT(1 1)")

        return job

    def actual_data(self, job):
        return ([curve.loss_ratios
                for curve in models.LossCurveData.objects.filter(
                    loss_curve__output__oq_job=job).order_by('asset_ref')] +
                [point.value
                 for point in models.LossMapData.objects.filter(
                    loss_map__output__oq_job=job).order_by(
                        'asset_ref', 'loss_map__poe')])

    def expected_data(self):
        return [[
            0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,
            0.07, 0.08, 0.09, 0.10, 0.12, 0.14, 0.16,
            0.18, 0.20, 0.24, 0.28, 0.32, 0.36, 0.40,
            0.48, 0.56, 0.64, 0.72, 0.80, 0.84, 0.88,
            0.92, 0.96, 1.00],
            0.264586283238, 0.141989823521, 0]

    def expected_outputs(self):
        return [self.EXPECTED_LOSS_CURVE_XML,
                self.EXPECTED_LOSS_MAP_0_01_XML,
                self.EXPECTED_LOSS_MAP_0_02_XML,
                self.EXPECTED_LOSS_MAP_0_05_XML]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models


class ClassicalRiskCase2TestCase(risk.BaseRiskQATestCase):
    EXPECTED_LOSS_CURVE_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml" xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossCurves investigationTime="50.0" statistics="mean" unit="USD" lossType="structural">
    <lossCurve assetRef="a1">
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <poEs>0.0393347533677 0.0391254281716 0.0376741689433 0.0347597109836 0.0310305310068 0.0271795287863 0.023629919279 0.0205495084461 0.0179532864059 0.0157897693715 0.0139899994698 0.011228361585 0.00925277823514 0.00777698111944 0.00661872190264 0.00567849220587 0.00429320981949 0.00342379135052 0.00285158950286 0.00237111635038 0.00190153868716 0.00114593052735 0.000834074579572 0.000755265952957 0.000655382394931 0.000422046545858 0.000266286103069 0.00012403689013 3.28497166703e-05 2.17866446601e-06 0.0</poEs>
      <losses>0.0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.24 0.28 0.32 0.36 0.4 0.48 0.56 0.64 0.72 0.8 0.96 1.12 1.28 1.44 1.6 1.68 1.76 1.84 1.92 2.0</losses>
      <lossRatios>0.0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.12 0.14 0.16 0.18 0.2 0.24 0.28 0.32 0.36 0.4 0.48 0.56 0.64 0.72 0.8 0.84 0.88 0.92 0.96 1.0</lossRatios>
      <averageLoss>9.3932e-03</averageLoss>
    </lossCurve>
  </lossCurves>
</nrml>"""

    EXPECTED_LOSS_MAP_0_01_XML = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <lossMap investigationTime="50.0" poE="0.01"
           statistics="mean" lossCategory="single_asset" unit="USD" lossType="structural">
    <node>
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <loss assetRef="a1" value="0.264870863284"/>
    </node>
  </lossMap>
</nrml>
    """

    @noseattr('qa', 'risk', 'classical')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("simple_fault_demo_hazard/job.ini"))

        hazard_curve = [
            (0.001, 0.0398612669790014),
            (0.01, 0.039861266979001400), (0.05, 0.039728757480298900),
            (0.10, 0.029613426625612500), (0.15, 0.019827328756491600),
            (0.20, 0.013062270161451900), (0.25, 0.008655387950000430),
            (0.30, 0.005898520593689670), (0.35, 0.004061698589511780),
            (0.40, 0.002811727179526820), (0.45, 0.001995117417776690),
            (0.50, 0.001358705972845710), (0.55, 0.000989667841573727),
            (0.60, 0.000757544444296432), (0.70, 0.000272824002045979),
            (0.80, 0.00), (0.9, 0.00), (1.0, 0.00)]

        models.HazardSite.objects.create(
            hazard_calculation=job.hazard_calculation,
            location="POINT(1 1)")
        models.HazardCurveData.objects.create(
            hazard_curve=models.HazardCurve.objects.create(
                output=models.Output.objects.create_output(
                    job, "Test Hazard curve", "hazard_curve"),
                investigation_time=50,
                imt="PGA", imls=[hz[0] for hz in hazard_curve],
                statistics="mean"),
            poes=[hz[1] for hz in hazard_curve],
            location="POINT(1 1)")

        return job

    def actual_data(self, job):
        return ([curve.loss_ratios
                for curve in models.LossCurveData.objects.filter(
                        loss_curve__output__oq_job=job).order_by(
                            'asset_ref')] +
                [point.value
                 for point in models.LossMapData.objects.filter(
                        loss_map__output__oq_job=job).order_by(
                            'asset_ref', 'loss_map__poe')])

    def expected_data(self):
        return [[
            0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,
            0.07, 0.08, 0.09, 0.10, 0.12, 0.14, 0.16,
            0.18, 0.20, 0.24, 0.28, 0.32, 0.36, 0.40,
            0.48, 0.56, 0.64, 0.72, 0.80, 0.84, 0.88,
            0.92, 0.96, 1.00],
            0.264870863283]

    def expected_outputs(self):
        return [self.EXPECTED_LOSS_CURVE_XML, self.EXPECTED_LOSS_MAP_0_01_XML]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


from nose.plugins.attrib import attr

from qa_tests import risk

from openquake.engine.db import models


class ClassicalRiskCase3TestCase(
        risk.CompleteTestCase, risk.FixtureBasedQATestCase):
    hazard_calculation_fixture = "Classical PSHA - Loss fractions QA test"

    @attr('qa', 'risk', 'classical')
    def test(self):
        self._run_test()

    items_per_output = None

    def expected_output_data(self):
        b1 = models.Output.HazardMetadata(
            investigation_time=50.0,
            statistics=None, quantile=None,
            sm_path=('b1',), gsim_path=('b1',))

        values = self._csv('fractions', dtype="f4, f4, S3, f4, f4")

        return zip(
            [("loss_fraction", b1, None, None, "taxonomy", 0.1,
              "structural") + ('%.5f' % lon, '%.5f' % lat, taxonomy)
             for lon, lat, taxonomy, _loss, _fraction in values],
            [models.LossFractionData(absolute_loss=v) for v in values['f3']])

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models


class ClassicalBCRCase1TestCase(risk.BaseRiskQATestCase):
    EXPECTED_BCR_DISTRIBUTION = """<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <bcrMap interestRate="0.05" assetLifeExpectancy="40.0" statistics="mean" unit="USD" lossType="structural">
    <node>
      <gml:Point>
        <gml:pos>1.0 1.0</gml:pos>
      </gml:Point>
      <bcr assetRef="a1" ratio="0.486045805147" aalOrig="0.00939290422672" aalRetr="0.00658230148253"/>
    </node>
  </bcrMap>
</nrml>
    """

    @attr('qa', 'risk', 'classical_bcr')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("simple_fault_demo_hazard/job.ini"))

        hazard_curve = [
            (0.001, 0.0398612669790014),
            (0.01, 0.039861266979001400), (0.05, 0.039728757480298900),
            (0.10, 0.029613426625612500), (0.15, 0.019827328756491600),
            (0.20, 0.013062270161451900), (0.25, 0.008655387950000430),
            (0.30, 0.005898520593689670), (0.35, 0.004061698589511780),
            (0.40, 0.002811727179526820), (0.45, 0.001995117417776690),
            (0.50, 0.001358705972845710), (0.55, 0.000989667841573727),
            (0.60, 0.000757544444296432), (0.70, 0.000272824002045979),
            (0.80, 0.00), (0.9, 0.00), (1.0, 0.00)]

        models.HazardSite.objects.create(
            hazard_calculation=job.hazard_calculation,
            location="POINT(1 1)")
        models.HazardCurveData.objects.create(
            hazard_curve=models.HazardCurve.objects.create(
                output=models.Output.objects.create_output(
                    job, "Test Hazard curve", "hazard_curve"),
                investigation_time=50, imt="PGA",
                imls=[hz[0] for hz in hazard_curve], statistics="mean"),
            poes=[hz[1] for hz in hazard_curve],
            location="POINT(1 1)")

        return job

    def actual_data(self, job):
        [result] = models.BCRDistributionData.objects.filter(
            bcr_distribution__output__oq_job=job)

        return [result.average_annual_loss_original,
            result.average_annual_loss_retrofitted, result.bcr]

    def expected_data(self):
        return [0.009379, 0.006586, 0.483091]

    def expected_outputs(self):
        return [self.EXPECTED_BCR_DISTRIBUTION]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr
from qa_tests import risk

from openquake.engine.db import models


class EventBaseQATestCase1(risk.CompleteTestCase, risk.FixtureBasedQATestCase):
    hazard_calculation_fixture = "PEB QA test 1"

    @noseattr('qa', 'risk', 'event_based')
    def test(self):
        self._run_test()

    def expected_output_data(self):
        branches = dict(
            b1=models.Output.HazardMetadata(
                investigation_time=50.0,
                statistics=None, quantile=None,
                sm_path=('b1',), gsim_path=('b1',)),
            b2=models.Output.HazardMetadata(
                investigation_time=50.0,
                statistics=None, quantile=None,
                sm_path=('b1',), gsim_path=('b2',)))

        assets = ["a0", "a1", "a2", "a3"]
        costs = ["nonstructural", "structural", "contents"]

        def gen_loss_curves(branches, costs):
            for branch, metadata in branches:
                for cost in costs:
                    csv_name = "%s_%s" % (branch, cost)
                    data = self._csv(csv_name)
                    yield csv_name, None
                    for i, asset in enumerate(assets):
                        descriptor = (u'event_loss_curve', metadata, None,
                                      None, False, False, cost, asset)
                        asset_value = data[i * 2 + 1, 0]
                        curve = models.LossCurveData(
                            asset_value=asset_value,
                            loss_ratios=data[i * 2, 2:],
                            poes=data[i * 2 + 1, 2:])
                        yield descriptor, curve

        loss_curves = (
            list(gen_loss_curves(branches.items(), costs)) +
            list(gen_loss_curves([("b1", branches["b1"])], ["fatalities"])))

        data = self._csv("aggregates")

        aggregate_loss_curves = [
            ('aggregates', None)] + [
            ((u'agg_loss_curve', branch, None,
              None, True, False, "structural"),
             models.AggregateLossCurveData(
                 losses=data[i * 2, 2:],
                 poes=data[i * 2 + 1, 2:]))
            for i, branch in enumerate(branches.values())]

        #NB: the event loss table check has been temporarily removed
        # we check only the first 10 values of the event loss table
        #data = self._csv('event_loss_table')[1:, 0:3]
        #data = sorted(data, key=lambda v: -v[2])[0:10]
        #event_loss_table_b1 = [('event_loss_table', None)] + [
        #    ((u'event_loss', branches["b1"], "structural", i),
        #     models.EventLossData(rupture_id=i, aggregate_loss=j))
        #    for i, _m, j in data]
        return loss_curves + aggregate_loss_curves  # + event_loss_table_b1

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models


# FIXME(lp). This is just a regression test
class EventBasedRiskCase1TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf"

    @noseattr('qa', 'risk', 'event_based')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("event_based_hazard/job.ini"))
        helpers.create_gmf_from_csv(job, self._test_path('gmf.csv'))

        return job

    def actual_data(self, job):
        return ([curve.average_loss_ratio
                 for curve in models.LossCurveData.objects.filter(
                loss_curve__output__oq_job=job,
                loss_curve__aggregate=False,
                loss_curve__insured=False).order_by('asset_ref')] +
                [curve.average_loss_ratio
                 for curve in models.LossCurveData.objects.filter(
                loss_curve__output__oq_job=job,
                loss_curve__aggregate=False,
                loss_curve__insured=True).order_by('asset_ref')] +
                [curve.stddev_loss_ratio
                 for curve in models.LossCurveData.objects.filter(
                loss_curve__output__oq_job=job,
                loss_curve__aggregate=False,
                loss_curve__insured=False).order_by('asset_ref')] +
                [curve.average_loss
                 for curve in models.AggregateLossCurveData.objects.filter(
                loss_curve__output__oq_job=job,
                loss_curve__aggregate=True,
                loss_curve__insured=False)] +
                [curve.stddev_loss
                 for curve in models.AggregateLossCurveData.objects.filter(
                loss_curve__output__oq_job=job,
                loss_curve__aggregate=True,
                loss_curve__insured=False)] +
                [[point.value
                  for point in models.LossMapData.objects.filter(
                loss_map__output__oq_job=job).order_by(
                    'asset_ref', 'loss_map__poe')]] +
                [[el.aggregate_loss
                 for el in models.EventLossData.objects.filter(
                event_loss__output__oq_job=job).order_by(
                    '-aggregate_loss')[0:10]]])

    def expected_data(self):
        # FIXME(lp). Event Loss Table data do not come from a reliable
        # implementation. This is just a regression test
        expected_event_loss_table = [330.0596974, 222.4581073, 162.7511019,
                                     115.9594444, 115.8300568, 107.8437644,
                                     105.7095923, 105.0259645, 96.6493404,
                                     93.7629886]

        return [
            [0.07021910798], [0.015239297], [0.04549904],
            [0.0569212190852481], [0.00779260434], [0.0],
            [0.0059876], [0.0022866], [0.0053434],
            [278.904436],
            [46.0207855534291],
            [263.37280611, 262.51974659, 261.66668707,
             30.96750422, 30.89424056, 30.82097689, 49.45179882, 49.29162539,
             49.13145195],
            expected_event_loss_table]

    def actual_xml_outputs(self, job):
        """
        Event Loss is in CSV format
        """
        return models.Output.objects.none()

    def expected_outputs(self):
        return []

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models

# FIXME(lp). This is a regression test. Data has not been validated
# by an alternative reliable implemantation


class EventBasedRiskCase2TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf"

    @noseattr('qa', 'risk', 'event_based')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("event_based_hazard/job.ini"))
        helpers.create_gmf_from_csv(job, self._test_path('gmf.csv'))

        return job

    def actual_data(self, job):
        data = ([curve.poes
                for curve in models.LossCurveData.objects.filter(
                    loss_curve__output__oq_job=job,
                    loss_curve__aggregate=False,
                    loss_curve__insured=False).order_by('asset_ref')] +
                [curve.loss_ratios
                 for curve in models.LossCurveData.objects.filter(
                     loss_curve__output__oq_job=job,
                     loss_curve__aggregate=False,
                     loss_curve__insured=False).order_by('asset_ref')] +
                [curve.losses
                 for curve in models.AggregateLossCurveData.objects.filter(
                     loss_curve__output__oq_job=job,
                     loss_curve__aggregate=True,
                     loss_curve__insured=False)] +
                [[el.aggregate_loss
                 for el in models.EventLossData.objects.filter(
                     event_loss__output__oq_job=job).order_by(
                     '-aggregate_loss')[0:10]]] +
                list(
                    models.LossFraction.objects.get(
                        variable="coordinate",
                        output__oq_job=job).iteritems())[0][1].values())
        return data

    def expected_data(self):
        poes_1 = [0.999996627985, 0.999849266925, 0.550671035883,
                  0.329679953964, 0.329679953964, 0.181269246922,
                  0.181269246922, 0.181269246922, 0.0]
        poes_2 = [0.999998484856, 0.999998484856, 0.999917275934,
                  0.993262053001, 0.925726421786, 0.834701111778,
                  0.698805788088, 0.550671035883, 0.0]
        poes_3 = [0.999997239227, 0.999997239227, 0.997521247823,
                  0.834701111778, 0.698805788088, 0.451188363906,
                  0.329679953964, 0.181269246922, 0.0]
        losses_1 = [0.0, 0.0110999061875, 0.0221998123751, 0.0332997185626,
                    0.0443996247501, 0.0554995309376, 0.0665994371252,
                    0.0776993433127, 0.0887992495002]
        losses_2 = [0.0, 0.00197070366487, 0.00394140732973, 0.0059121109946,
                    0.00788281465947, 0.00985351832434, 0.0118242219892,
                    0.0137949256541, 0.0157656293189]
        losses_3 = [0.0, 0.00627183973737, 0.0125436794747, 0.0188155192121,
                    0.0250873589495, 0.0313591986868, 0.0376310384242,
                    0.0439028781616, 0.0501747178989]
        expected_aggregate_losses = [
            0.0, 41.5569324154, 83.1138648309, 124.670797246, 166.227729662,
            207.784662077, 249.341594493, 290.898526908, 332.455459323]

        expected_event_loss_table = [
            332.455459323, 221.05497273, 164.077337072, 118.399426421,
            115.406897066, 108.925518713, 104.67570929, 104.295934513,
            96.7416846911, 94.4460331092]

        loss_fraction = [2847.6635362969, 1.0]

        return [poes_1, poes_2, poes_3, losses_1, losses_2, losses_3,
                expected_aggregate_losses, expected_event_loss_table,
                loss_fraction]

    def actual_xml_outputs(self, job):
        """
        do not check file outputs
        """
        return []

    def expected_outputs(self):
        return []

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import unittest
from nose.plugins.attrib import attr as noseattr
from qa_tests import risk


# the GMF-on-the-fly feature will be removed, this test must be
# changed; for the moment it is skipped (MS)
class EventBaseDumpRestoreSESTestCase(
        risk.CompleteTestCase, risk.FixtureBasedQATestCase):
    hazard_calculation_fixture = ("QA (regression) test for Risk Event "
                                  "Based from Stochastic Event Set")
    dump_restore = True

    @noseattr('qa', 'risk', 'event_based')
    def test(self):
        raise unittest.SkipTest
        self._run_test()

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr

from qa_tests import risk
from openquake.engine.tests.utils import helpers

from openquake.engine.db import models


# FIXME(lp). This is a regression test. Data has not been validated
# by an alternative reliable implemantation


class EventBasedBCRCase1TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf"

    check_exports = False

    @noseattr('qa', 'risk', 'event_based_bcr')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("event_based_hazard/job.ini"))

        job.hazard_calculation = models.HazardCalculation.objects.create(
            truncation_level=job.hazard_calculation.truncation_level,
            maximum_distance=job.hazard_calculation.maximum_distance,
            intensity_measure_types_and_levels=(
                job.hazard_calculation.intensity_measure_types_and_levels),
            calculation_mode="event_based",
            investigation_time=50,
            ses_per_logic_tree_path=1)
        job.save()

        helpers.create_gmf_from_csv(job, self._test_path('gmf.csv'))

        return job

    def actual_data(self, job):
        data = [(result.average_annual_loss_original,
                 result.average_annual_loss_retrofitted, result.bcr)
                for result in models.BCRDistributionData.objects.filter(
                    bcr_distribution__output__oq_job=job).order_by(
                    'asset_ref')]
        return data

    def expected_data(self):
        return [(0.2311358, 0.0, 39.970994),
                (0.3344297, 0.0, 28.916953),
                (0.2479981, 0.0, 21.443528)]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.tests.utils import helpers
from openquake.engine.db import models
from openquake.engine.tools.import_gmf_scenario import import_gmf_scenario


class ScenarioRiskCase1TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf_scenario"

    @attr('qa', 'risk', 'scenario')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("scenario_hazard/job.ini"))
        fname = self._test_path('gmf_scenario.csv')
        helpers.populate_gmf_data_from_csv(job, fname)
        return job

    def actual_data(self, job):
        maps = models.LossMapData.objects.filter(
            loss_map__output__oq_job=job,
            loss_map__insured=False).order_by('asset_ref', 'value')
        agg = models.AggregateLoss.objects.get(output__oq_job=job,
                                               insured=False)
        insured_maps = models.LossMapData.objects.filter(
            loss_map__output__oq_job=job,
            loss_map__insured=True).order_by('asset_ref', 'value')
        insured_agg = models.AggregateLoss.objects.get(
            output__oq_job=job,
            insured=True)

        return [[[m.value, m.std_dev] for m in maps],
                [agg.mean, agg.std_dev],
                [[m.value, m.std_dev] for m in insured_maps],
                [insured_agg.mean, insured_agg.std_dev]]

    def expected_data(self):
        return [[[440.14707, 182.6159],
                 [432.2254, 186.8644],
                 [180.7175, 92.2122]],
                [1053.09, 246.62],
                [[147.49208753, 141.02819028],
                 [104.85957932, 145.36984417],
                 [76.75091081, 69.96900115]],
                [329.10257766, 193.67786848]]


class ImportGmfScenarioTestCase(risk.BaseRiskQATestCase):
    output_type = "gmf_scenario"

    @attr('qa', 'risk', 'scenario')
    def test(self):
        # check that the imported GMFs can be read by the risk calculator
        self._run_test()

    def get_hazard_job(self):
        with open(self._test_path('gmf-scenario.xml')) as data:
            output = import_gmf_scenario(data)
        job = output.oq_job
        # this is needed to make happy the GetterBuilder
        job.hazard_calculation.number_of_ground_motion_fields = 3
        job.hazard_calculation.save()
        return job

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.tests.utils import helpers
from openquake.engine.db import models


class ScenarioRiskCase2TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf_scenario"

    @attr('qa', 'risk', 'scenario')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("scenario_hazard/job.ini"))
        fname = self._test_path('gmf_scenario.csv')
        helpers.populate_gmf_data_from_csv(job, fname)
        # this is needed to make happy the GetterBuilder
        job.hazard_calculation.number_of_ground_motion_fields = 1000
        job.hazard_calculation.save()
        return job

    def actual_data(self, job):
        maps = models.LossMapData.objects.filter(
            loss_map__output__oq_job=job).order_by('asset_ref', 'value')
        agg = models.AggregateLoss.objects.get(output__oq_job=job)
        data = [[[m.value, m.std_dev] for m in maps],
                [agg.mean, agg.std_dev]]
        return data

    def expected_data(self):
        return [[[522.675485, 248.86320],
                 [516.1156, 369.162],
                 [200.0154, 94.8534]],
                [1238.806, 519.7878]]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.db import models


class ScenarioRiskCase3TestCase(risk.FixtureBasedQATestCase):
    hazard_calculation_fixture = 'Scenario QA Test 3'

    @attr('qa', 'risk', 'scenario')
    def test(self):
        self._run_test()

    def actual_data(self, job):
        maps = models.LossMapData.objects.filter(
            loss_map__output__oq_job=job).order_by('asset_ref', 'value')
        agg = models.AggregateLoss.objects.get(output__oq_job=job)
        return [[[m.value, m.std_dev] for m in maps],
                [agg.mean, agg.std_dev]]

    def expected_data(self):
        return [[[138.78573075, 58.27789895],
                 [196.7998648, 228.41381954],
                 [252.37890689, 264.81701577]],
                [587.96450244, 358.39744028]]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr as noseattr

from qa_tests import risk


class ScenarioOccupantsQATestCase1(risk.FixtureBasedQATestCase):
    output_type = "gmf_scenario"
    hazard_calculation_fixture = 'Scenario QA Test for occupants'

    @noseattr('qa', 'risk', 'scenario')
    def test(self):
        self._run_test()

    def actual_data(self, job):
        latest_loss_map = job.output_set.filter(
            output_type="loss_map", loss_map__loss_type="fatalities").latest(
            'last_update').loss_map
        latest_aggregated = job.output_set.filter(
            output_type="aggregate_loss",
            aggregate_loss__loss_type="fatalities").latest(
            'last_update').aggregate_loss

        data = [(d.value, d.std_dev)
                for d in latest_loss_map.lossmapdata_set.all().order_by(
                    'asset_ref')] + [
            (latest_aggregated.mean, latest_aggregated.std_dev)]

        return data

    def expected_data(self):
        return [(0.471915785, 0.07295834),
                (1.5895092, 1.85708354),
                (0.9215899, 0.4393920),
                (2.983015, 1.8925195)]

# For NIGHT:

# Asset 1
# Mean: 0.8981603187500
# Std Dev: 0.479301017326327

# Asset 2
# Mean: 1.67389744650000
# Std Dev: 1.64243715167681

# Asset 3
# Mean: 1.60533374061429
# Std Dev: 1.13698143346693

# Mean: 4.1774
# Std Dev: 3.1036

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.tests.utils import helpers


class ScenarioDamageRiskCase1TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf_scenario"

    EXPECTED_DMG_DIST_PER_ASSET = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerAsset>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <gml:Point>
        <gml:pos>15.48 38.09</gml:pos>
      </gml:Point>
      <asset assetRef="a1">
        <damage ds="no_damage" mean="875.81199" stddev="757.540852525"/>
        <damage ds="LS1" mean="1448.2960791" stddev="256.15318195"/>
        <damage ds="LS2" mean="675.8919309" stddev="556.765558354"/>
      </asset>
    </DDNode>
    <DDNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <asset assetRef="a2">
        <damage ds="no_damage" mean="344.90869" stddev="300.611885456"/>
        <damage ds="LS1" mean="747.62374" stddev="144.648244809"/>
        <damage ds="LS2" mean="907.46757" stddev="417.307715071"/>
      </asset>
    </DDNode>
    <DDNode>
      <gml:Point>
        <gml:pos>15.48 38.25</gml:pos>
      </gml:Point>
      <asset assetRef="a3">
        <damage ds="no_damage" mean="224.41751" stddev="220.650927184"/>
        <damage ds="LS1" mean="465.6442715" stddev="136.927805797"/>
        <damage ds="LS2" mean="309.9382185" stddev="246.84411292"/>
      </asset>
    </DDNode>
  </dmgDistPerAsset>
</nrml>
'''

    EXPECTED_DMG_DIST_PER_TAXONOMY = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerTaxonomy>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <taxonomy>RM</taxonomy>
      <damage ds="no_damage" mean="1100.2295" stddev="880.276624388"/>
      <damage ds="LS1" mean="1913.9403506" stddev="296.218619036"/>
      <damage ds="LS2" mean="985.8301494" stddev="616.562370911"/>
    </DDNode>
    <DDNode>
      <taxonomy>RC</taxonomy>
      <damage ds="no_damage" mean="344.90869" stddev="300.611885456"/>
      <damage ds="LS1" mean="747.62374" stddev="144.648244809"/>
      <damage ds="LS2" mean="907.46757" stddev="417.307715071"/>
    </DDNode>
  </dmgDistPerTaxonomy>
</nrml>
'''

    EXPECTED_DMG_DIST_TOTAL = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <totalDmgDist>
    <damageStates>no_damage LS1 LS2</damageStates>
    <damage ds="no_damage" mean="1445.13819" stddev="824.78005267"/>
    <damage ds="LS1" mean="2661.5640906" stddev="373.99997281"/>
    <damage ds="LS2" mean="1893.2977194" stddev="661.810775842"/>
  </totalDmgDist>
</nrml>
'''

    EXPECTED_COLLAPSE_MAP = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <collapseMap>
    <CMNode>
      <gml:Point>
        <gml:pos>15.48 38.09</gml:pos>
      </gml:Point>
      <cf assetRef="a1" mean="675.8919309" stdDev="556.765558354"/>
    </CMNode>
    <CMNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <cf assetRef="a2" mean="907.46757" stdDev="417.307715071"/>
    </CMNode>
    <CMNode>
      <gml:Point>
        <gml:pos>15.48 38.25</gml:pos>
      </gml:Point>
      <cf assetRef="a3" mean="309.9382185" stdDev="246.84411292"/>
    </CMNode>
  </collapseMap>
</nrml>
'''

    @attr('qa', 'risk', 'scenario_damage')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("scenario_hazard/job.ini"))
        fname = self._test_path('gmf_scenario.csv')
        helpers.populate_gmf_data_from_csv(job, fname)
        return job

    def expected_outputs(self):
        return [self.EXPECTED_DMG_DIST_PER_ASSET,
                self.EXPECTED_COLLAPSE_MAP,
                self.EXPECTED_DMG_DIST_PER_TAXONOMY,
                self.EXPECTED_DMG_DIST_TOTAL]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from openquake.engine.tests.utils import helpers
from qa_tests import risk


class ScenarioDamageRiskCase2TestCase(risk.BaseRiskQATestCase):
    output_type = "gmf_scenario"

    EXPECTED_DMG_DIST_PER_ASSET = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerAsset>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <gml:Point>
        <gml:pos>15.48 38.09</gml:pos>
      </gml:Point>
      <asset assetRef="a1">
        <damage ds="no_damage" mean="1562.60873676" stddev="968.935196007"/>
        <damage ds="LS1" mean="1108.0179559" stddev="652.736659161"/>
        <damage ds="LS2" mean="329.37330734" stddev="347.391647796"/>
      </asset>
    </DDNode>
    <DDNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <asset assetRef="a2">
        <damage ds="no_damage" mean="56.7203646693" stddev="117.780724159"/>
        <damage ds="LS1" mean="673.104377189" stddev="485.202354464"/>
        <damage ds="LS2" mean="1270.17525814" stddev="575.872927586"/>
      </asset>
    </DDNode>
    <DDNode>
      <gml:Point>
        <gml:pos>15.48 38.25</gml:pos>
      </gml:Point>
      <asset assetRef="a3">
        <damage ds="no_damage" mean="417.329450348" stddev="304.476603429"/>
        <damage ds="LS1" mean="387.208726145" stddev="181.141228526"/>
        <damage ds="LS2" mean="195.461823506" stddev="253.91290672"/>
      </asset>
    </DDNode>
  </dmgDistPerAsset>
</nrml>
'''

    EXPECTED_DMG_DIST_PER_TAXONOMY = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerTaxonomy>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <taxonomy>RM</taxonomy>
      <damage ds="no_damage" mean="1979.93818711" stddev="1103.59950218"/>
      <damage ds="LS1" mean="1495.22668204" stddev="745.325447387"/>
      <damage ds="LS2" mean="524.835130846" stddev="401.918282572"/>
    </DDNode>
    <DDNode>
      <taxonomy>RC</taxonomy>
      <damage ds="no_damage" mean="56.7203646693" stddev="117.780724159"/>
      <damage ds="LS1" mean="673.104377189" stddev="485.202354464"/>
      <damage ds="LS2" mean="1270.17525814" stddev="575.872927586"/>
    </DDNode>
  </dmgDistPerTaxonomy>
</nrml>
'''

    EXPECTED_DMG_DIST_TOTAL = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <totalDmgDist>
    <damageStates>no_damage LS1 LS2</damageStates>
    <damage ds="no_damage" mean="2036.65855178" stddev="1075.31798196"/>
    <damage ds="LS1" mean="2168.33105923" stddev="1076.43488958"/>
    <damage ds="LS2" mean="1795.01038899" stddev="687.09098615"/>
  </totalDmgDist>
</nrml>
'''

    EXPECTED_COLLAPSE_MAP = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <collapseMap>
    <CMNode>
      <gml:Point>
        <gml:pos>15.48 38.09</gml:pos>
      </gml:Point>
      <cf assetRef="a1" mean="329.37330734" stdDev="347.391647796"/>
    </CMNode>
    <CMNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <cf assetRef="a2" mean="1270.17525814" stdDev="575.872927586"/>
    </CMNode>
    <CMNode>
      <gml:Point>
        <gml:pos>15.48 38.25</gml:pos>
      </gml:Point>
      <cf assetRef="a3" mean="195.461823506" stdDev="253.91290672"/>
    </CMNode>
  </collapseMap>
</nrml>
'''

    @attr('qa', 'risk', 'scenario_damage')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("scenario_hazard/job.ini"))
        fname = self._test_path('../case_1/gmf_scenario.csv')
        helpers.populate_gmf_data_from_csv(job, fname)
        return job

    def expected_outputs(self):
        return [self.EXPECTED_DMG_DIST_PER_ASSET,
                self.EXPECTED_COLLAPSE_MAP,
                self.EXPECTED_DMG_DIST_PER_TAXONOMY,
                self.EXPECTED_DMG_DIST_TOTAL]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.tests.utils import helpers


class ScenarioDamageRiskCase3TestCase(risk.BaseRiskQATestCase):
    """
    There are not enough taxonomies in the fragility_function (RM is missing)
    but the parameter taxonomies_from_model is set, so assets of
    unknown taxonomy are simply ignored.
    """

    output_type = "gmf_scenario"

    EXPECTED_DMG_DIST_PER_ASSET = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerAsset>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <asset assetRef="a2">
        <damage ds="no_damage" mean="344.90869" stddev="300.611885456"/>
        <damage ds="LS1" mean="747.62374" stddev="144.648244809"/>
        <damage ds="LS2" mean="907.46757" stddev="417.307715071"/>
      </asset>
    </DDNode>
  </dmgDistPerAsset>
</nrml>
'''

    EXPECTED_DMG_DIST_PER_TAXONOMY = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <dmgDistPerTaxonomy>
    <damageStates>no_damage LS1 LS2</damageStates>
    <DDNode>
      <taxonomy>RC</taxonomy>
      <damage ds="no_damage" mean="344.90869" stddev="300.611885456"/>
      <damage ds="LS1" mean="747.62374" stddev="144.648244809"/>
      <damage ds="LS2" mean="907.46757" stddev="417.307715071"/>
    </DDNode>
  </dmgDistPerTaxonomy>
</nrml>
'''

    EXPECTED_DMG_DIST_TOTAL = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <totalDmgDist>
    <damageStates>no_damage LS1 LS2</damageStates>
    <damage ds="no_damage" mean="344.90869" stddev="300.611885456"/>
    <damage ds="LS1" mean="747.62374" stddev="144.648244809"/>
    <damage ds="LS2" mean="907.46757" stddev="417.307715071"/>
  </totalDmgDist>
</nrml>
'''

    EXPECTED_COLLAPSE_MAP = '''<?xml version='1.0' encoding='UTF-8'?>
<nrml xmlns:gml="http://www.opengis.net/gml"
      xmlns="http://openquake.org/xmlns/nrml/0.4">
  <collapseMap>
    <CMNode>
      <gml:Point>
        <gml:pos>15.56 38.17</gml:pos>
      </gml:Point>
      <cf assetRef="a2" mean="907.46757" stdDev="417.307715071"/>
    </CMNode>
  </collapseMap>
</nrml>
'''

    @attr('qa', 'risk', 'scenario_damage')
    def test(self):
        self._run_test()

    def get_hazard_job(self):
        job = helpers.get_job(
            helpers.get_data_path("scenario_hazard/job.ini"))
        helpers.populate_gmf_data_from_csv(
            job, self._test_path('../case_1/gmf_scenario.csv'))
        return job

    def expected_outputs(self):
        return [self.EXPECTED_DMG_DIST_PER_ASSET,
                self.EXPECTED_COLLAPSE_MAP,
                self.EXPECTED_DMG_DIST_PER_TAXONOMY,
                self.EXPECTED_DMG_DIST_TOTAL]

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

from nose.plugins.attrib import attr

from qa_tests import risk
from openquake.engine.db import models


# FIXME(lp). This is a regression test meant to exercise the sd-imt
# logic in the SR calculator. Data has not been validated


class ScenarioDamageRiskCase4TestCase(risk.FixtureBasedQATestCase):
    output_type = 'gmf_scenario'
    hazard_calculation_fixture = 'Scenario Damage QA Test 4'

    @attr('qa', 'risk', 'scenario_damage')
    def test(self):
        self._run_test()

    def actual_data(self, job):
        per_asset = models.DmgDistPerAsset.objects.filter(
            dmg_state__risk_calculation=job.risk_calculation).order_by(
            'exposure_data', 'dmg_state')
        totals = models.DmgDistTotal.objects.filter(
            dmg_state__risk_calculation=job.risk_calculation).order_by(
            'dmg_state')
        data = [[[m.mean, m.stddev] for m in per_asset],
                [[total.mean, total.stddev] for total in totals]]
        return data

    def expected_data(self):
        return [[[2839.99487945302, 260.107936318162],
                 [154.207170037728, 240.669053166515],
                 [5.79795050924058, 24.2361847686849],
                 [162.093574777342, 262.704324187903],
                 [436.941603707819, 294.180204333271],
                 [400.964821514838, 376.14843653929],
                 [506.964203111531, 645.131915664689],
                 [774.865676844215, 535.513668479813],
                 [718.170120044255, 744.033150727349]],
                [[3509.0526573419, 736.44555857085],
                 [1366.01445058976, 657.913617460034],
                 [1124.93289206833, 838.747142746392]]]

########NEW FILE########
__FILENAME__ = _utils
# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import csv
import numpy
import unittest
import os
import shutil
import filecmp

from nose.plugins.attrib import attr

import openquake.engine
import openquake.nrmllib
from openquake.nrmllib import PARSE_NS_MAP
from openquake.engine.db import models

from lxml import etree

from openquake.engine.tests.utils import helpers


class BaseQATestCase(unittest.TestCase):
    """
    Base QA test case class with some general functionality built in for
    running QA tests.
    """

    def run_hazard(self, cfg, exports=None):
        """
        Given the path to job config file, run the job and assert that it was
        successful. If this assertion passes, return the completed job.

        :param str cfg:
            Path to a job config file.
        :param list exports:
            A list of export format types. Currently only 'xml' is supported.
        :returns:
            The completed :class:`~openquake.engine.db.models.OqJob`.
        :raises:
            :exc:`AssertionError` if the job was not successfully run.
        """
        completed_job = helpers.run_job(cfg, exports=exports)
        self.assertEqual('complete', completed_job.status)

        return completed_job

    def assert_xml_equal(self, a, b):
        """
        Compare two XML artifacts for equality.

        :param a, b:
            Paths to XML files, or a file-like object containing the XML
            contents.
        """
        contents_a = etree.tostring(etree.parse(a), pretty_print=True)
        contents_b = etree.tostring(etree.parse(b), pretty_print=True)

        self.assertEqual(contents_a, contents_b)

    def assert_equals_var_tolerance(self, expected, actual):
        """
        Assert (almost) equal with a variable tolerance. For extremely low
        values (< 0.0005), tolerance is 3 digits. For everything >= 0.0005,
        tolerance is 2 digits.
        """
        for i, exp in enumerate(expected):
            act = actual[i]

            if exp < 0.0005:
                tolerance = 3
            else:
                tolerance = 2
            numpy.testing.assert_almost_equal(act, exp, decimal=tolerance)


def validates_against_xml_schema(
        xml_instance_path, schema_path=openquake.nrmllib.nrml_schema_file()):
    """
    Check whether an XML file validates against an XML schema.
    """
    xml_doc = etree.parse(xml_instance_path)
    xmlschema = etree.XMLSchema(etree.parse(schema_path))
    return xmlschema.validate(xml_doc)


def count(gmf_value, gmfs_site_one, gmfs_site_two,
          delta_prob=0.1, div_factor=2.0):
    """
    Count the number of pairs of gmf values
    within the specified range.
    See https://bugs.launchpad.net/openquake/+bug/1097646
    attached Scenario Hazard script.
    """

    i = 0
    lower_bound = gmf_value - delta_prob / div_factor
    upper_bound = gmf_value + delta_prob / div_factor

    for v1, v2 in zip(gmfs_site_one, gmfs_site_two):
        if ((lower_bound <= v1 <= upper_bound) and
                (lower_bound <= v2 <= upper_bound)):
            i += 1
    return i


def compare_hazard_curve_with_csv(
        hc, sm_lt_path, gsim_lt_path, imt, sa_period, sa_damping,
        csv_name, csv_delimiter, rtol):
    """
    This is useful in tests that compares the hazard curves in the db with
    the expected values in the csv. The csv is expected to have the form
    `lon, lat, poe1, poe2, ...`
    """
    rlzs = list(
        models.LtRealization.objects.filter(lt_model__hazard_calculation=hc))
    # there is some contorsion here since is seems that Django filtering
    # with CharArrayFields does not work, so I get all the realizations
    # and I extract by hand the one with the given lt_paths
    [rlz] = [r for r in rlzs
             if r.sm_lt_path == sm_lt_path
             and r.gsim_lt_path == gsim_lt_path]
    curves = models.HazardCurveData.objects.filter(
        hazard_curve__lt_realization=rlz,
        hazard_curve__imt=imt,
        hazard_curve__sa_period=sa_period,
        hazard_curve__sa_damping=sa_damping)

    data = []  # read computed data from db
    for hazard_curve_data in curves:
        loc = hazard_curve_data.location
        data.append([loc.x, loc.y] + hazard_curve_data.poes)
    data.sort()  # expects the csv values to be sorted by lon, lat

    # read expected data from csv
    with open(csv_name) as f:
        reader = csv.reader(f, delimiter=csv_delimiter)
        expected_data = [map(float, row) for row in reader]

    try:
        numpy.testing.assert_allclose(expected_data, data, rtol=rtol)
    except:
        # to debug the test, in case it breaks, comment the assert and
        # uncomment the following, lines then compare the expected file with
        # the file generated from the computed data and stored in /tmp:
        #import os
        #tmp = os.path.join('/tmp', os.path.basename(csv_name))
        #print 'saving', tmp
        #print >>open(tmp, 'w'), '\n'.join(' '.join(map(str, r)) for r in data)
        raise

aac = lambda a, b: numpy.testing.assert_allclose(a, b, atol=1e-5)


class DisaggHazardTestCase(BaseQATestCase):
    fnames = []  # to be overridden
    working_dir = os.path.dirname(__file__)
    imts = []

    @attr('qa', 'hazard', 'disagg')
    def test(self):
        cfg = os.path.join(self.working_dir, 'job.ini')
        expected = os.path.join(self.working_dir, 'expected_output')
        job = self.run_hazard(cfg, exports=['xml'])
        hc = job.hazard_calculation
        export_dir = os.path.join(hc.export_dir, 'calc_%d' % hc.id)

        # compare the directories and print a report
        dc = filecmp.dircmp(expected, export_dir)
        dc.report_full_closure()

        # compare the disagg files
        for fname in self.fnames:
            for imt in self.imts:
                exp = os.path.join(expected, 'disagg_matrix', imt, fname)
                got = os.path.join(export_dir, 'disagg_matrix', imt, fname)
                self.assert_disagg_xml_almost_equal(exp, got)

        # remove the export_dir if the test passes
        shutil.rmtree(export_dir)

    def assert_disagg_xml_almost_equal(self, expected, actual):
        """
        A special helper function to test that values in the ``expected`` and
        ``actual`` XML are almost equal to a certain precision.

        :param expected, actual:
            Paths to XML files, or file-like objects containing the XML
            contents.
        """
        exp_tree = etree.parse(expected)
        act_tree = etree.parse(actual)

        # First, compare the <disaggMatrices> container element, check attrs,
        # etc.
        [exp_dms] = exp_tree.xpath(
            '//nrml:disaggMatrices', namespaces=PARSE_NS_MAP
        )
        [act_dms] = act_tree.xpath(
            '//nrml:disaggMatrices', namespaces=PARSE_NS_MAP
        )
        self.assertEqual(exp_dms.attrib, act_dms.attrib)

        # Then, loop over each <disaggMatrix>, check attrs, then loop over each
        # <prob> and compare indices and values.
        exp_dm = exp_tree.xpath(
            '//nrml:disaggMatrix', namespaces=PARSE_NS_MAP
        )
        act_dm = act_tree.xpath(
            '//nrml:disaggMatrix', namespaces=PARSE_NS_MAP
        )
        self.assertEqual(len(exp_dm), len(act_dm))

        for i, matrix in enumerate(exp_dm):
            act_matrix = act_dm[i]

            self.assertEqual(matrix.attrib['type'], act_matrix.attrib['type'])
            self.assertEqual(matrix.attrib['dims'], act_matrix.attrib['dims'])
            self.assertEqual(matrix.attrib['poE'], act_matrix.attrib['poE'])
            aac(float(act_matrix.attrib['iml']), float(matrix.attrib['iml']))

            # compare probabilities
            exp_probs = matrix.xpath('./nrml:prob', namespaces=PARSE_NS_MAP)
            act_probs = act_matrix.xpath(
                './nrml:prob', namespaces=PARSE_NS_MAP
            )
            self.assertEqual(len(exp_probs), len(act_probs))

            for j, prob in enumerate(exp_probs):
                act_prob = act_probs[j]

                self.assertEqual(prob.attrib['index'],
                                 act_prob.attrib['index'])
                aac(float(act_prob.attrib['value']),
                    float(prob.attrib['value']))

########NEW FILE########
__FILENAME__ = dbmaint
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.


"""
OpenQuake database maintenance tool, performs schema upgrades and/or data
migration.

  -h | --help     : prints this help string
       --host H   : database host machine name [default: localhost]
  -d | --db D     : database to use [default: openquake]
  -n | --dryrun   : don't do anything just show what needs done
  -p | --path P   : path to schema upgrade files [default: db/schema/upgrades]
  -U | --user U   : database user to use [default: postgres]
"""

from distutils import version
import getopt
import logging
import subprocess
import re
import sys
import os


logging.basicConfig(level=logging.INFO)


def run_cmd(cmds, ignore_exit_code=False):
    """Run the given command and return the exit code, stdout and stderr.

    :param list cmds: the strings that comprise the command to run
    :param bool ignore_exit_code: if `True` no `Exception` will be raised for
        non-zero command exit code.
    :returns: an `(exit code, stdout, stderr)` triple
    :raises Exception: when the command terminates with a non-zero command
        exit code.
    """
    p = subprocess.Popen(
        cmds, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, err = p.communicate()
    if p.returncode != 0 and not ignore_exit_code:
        error = ("%s terminated with exit code: %s\n%s"
                 % (cmds[0], p.returncode, err))
        logging.error(error)
        raise Exception(error)
    return (p.returncode, out, err)


def psql(config, script=None, cmd=None, ignore_dryrun=False, runner=run_cmd):
    """Runs the `psql` tool either with a command or SQL script.

    If the `dryrun` configuration flag is set the command will not be run but
    merely printed.
    Please note that `file` and `cmd` are mutually exclusive.

    :param dict config: the configuration to use: database, host, user, path.
    :param string script: the script to run, relative to the path in `config`.
    :param string cmd: the command to run.
    :param bool ignore_dryrun: if `True` the `dryrun` flag in the
        configuration will be disregarded.
    :param runner: function to use for running the `psql` tool.
    :returns: a triple (exit code, stdout, stderr) with psql execution outcome
    """
    if script and cmd:
        raise Exception("Please specify either an SQL script or a command.")

    if not script and not cmd:
        raise Exception("Neither SQL script nor command specified.")

    if config['host'] in ["localhost", "127.0.0.1"]:
        psql_cmd = "psql --set ON_ERROR_STOP=1 -d %(db)s -U %(user)s" % config
    else:
        psql_cmd =\
        "psql --set ON_ERROR_STOP=1 -d %(db)s -U %(user)s -h %(host)s" % config

    cmds = psql_cmd.split()

    if cmd:
        cmds.extend(['-c', "%s" % cmd])
    else:
        cmds.extend(["-f", "%s/%s" % (config['path'], script)])

    if config['dryrun'] and not ignore_dryrun:
        if cmd:
            cmds[-1] = '"%s"' % cmds[-1]
        print " ".join(cmds)
        return (-1, "", "")
    else:
        return runner(cmds)


def find_scripts(path):
    """Find all SQL scripts at level 2 of the given `path`."""
    result = []
    cmd = "find %s -mindepth 2 -maxdepth 2 -type f" \
          "     ( -name *.sql -o -name *.py )" % path

    code, out, err = run_cmd(cmd.split(), ignore_exit_code=True)

    if code == 0:
        prefix_length = len(path) + 1
        for file in out.split('\n'):
            result.append(file[prefix_length:])
    return [r for r in result if r]


def version_key(string):
    """Returns a version representation useful for version comparison"""

    # remove the trailing '-<release>' number if any
    string = string.split('-', 1)[0]

    return version.StrictVersion(string)


def script_sort_key(script):
    """
    Return a sort key to order upgrade scripts first by revision, then
    by step, then by file name
    """

    revision, step, name = script.rsplit('/', 3)

    return version_key(revision), int(step), name


def scripts_to_run(artefact, rev_info, config):
    """The SQL scripts that need to run given the `artefact` and `rev_info`.

    :param str artefact:
        DEPRECATED: name of the revision controlled database artefact
    :param dict rev_info: current revision info: revision and step
    :param dict config: the configuration to use: database, host, user, path.

    :return: a sorted list of SQL script paths (relative to the path in
        `config`)
    """
    result = []
    revision = rev_info['revision']

    # find upgrade scripts for this revision
    path = "%s/%s" % (config['path'], revision)
    files = find_scripts(path)
    step = int(rev_info['step'])
    for script in files:
        spath, sfile = script.split('/')
        if (int(spath) > step):
            result.append(os.path.join(revision, script))

    # find upgrade scripts for revisions newer than the current one
    path = config['path']
    current_revision_key = version_key(revision)
    if os.path.isdir(path):
        dirs = [os.path.join(path, d)
                    for d in os.listdir(path)
                    if os.path.isdir(os.path.join(path, d))]

        for dir_name in dirs:
            path_revision = os.path.basename(dir_name)

            if version_key(path_revision) > current_revision_key:
                result.extend(os.path.join(path_revision, s)
                                  for s in find_scripts(dir_name))

    return sorted(result, key=script_sort_key)


def error_occurred(output):
    """Detect psql errors in `output`."""
    regex = re.compile('sql:\d+:\s+ERROR:\s+')
    return regex.search(output) is not None


def run_scripts(artefact, rev_info, scripts, config):
    """Run the SQL `scripts` for the given `artefact`.

    Once all scripts complete the step of the `artefact` at hand will be
    updated to the highest step encountered.

    :param str artefact: name of the revision controlled database artefact
    :param dict rev_info: current revision info: revision and step
    :param list scripts: a sorted list of SQL script paths (relative to the
        path in `config`)
    :param dict config: the configuration to use: database, host, user, path.
    :returns: True for success, False for failure
    """
    max_step = 0
    max_revision = '0.0.0'

    for script in scripts:
        # Keep track of the max. step/revision applied.
        revision, step, _ = script.split('/')
        step = int(step)
        if ((version_key(revision), step) >
            (version_key(max_revision), max_step)):
            max_step = step
            max_revision = revision

        # Run the SQL script.
        results = psql(config, script=script)
        if script_failed(results, script, config):
            # A step of '-1' indicates a broken upgrade.
            max_step = -1
            break

    if max_step != 0:
        cmd = ("UPDATE admin.revision_info SET step=%s, revision='%s', "
               "last_update=timezone('UTC'::text, now()) "
               "WHERE artefact='%s' AND revision = '%s'")
        cmd %= (max_step, max_revision, artefact, rev_info['revision'])
        code, out, err = psql(config, cmd=cmd)

    return max_step != -1


def script_failed((code, out, err), script, config):
    """Log SQL script output and return `True` in case of errors."""
    if not config['dryrun']:
        logging.info("%s (%s)" % (script, code))
        out = out.strip()
        if out:
            logging.info(out)
        err = err.strip()
        if err:
            logging.info(err)
            if error_occurred(err):
                return True
    return False


def perform_upgrade(config):
    """Perform the upgrades for all artefacts in the database.

    :param dict config: the configuration to use: database, host, user, path.
    :returns: True for success, False for failure
    """
    # Get the revision information from the database.
    cmd = "SELECT artefact, id, revision, step FROM admin.revision_info"
    code, out, err = psql(config, cmd=cmd, ignore_dryrun=True)
    # Throw away the psql header and footer.
    db_rev_data = out.split('\n')[2:-3]

    rev_data = dict()
    columns = ("id", "revision", "step")

    # Extract the revision info from the psql input.
    #     artefact     | id | revision | step
    # -----------------+----+----------+------
    #  openquake/admin |  1 | 0.3.9-1  |    0
    #  openquake/eqcat |  2 | 0.3.9-1  |    0
    #  openquake/uiapi |  4 | 0.3.9-1  |    0
    #  openquake/hzrdi |  3 | 0.3.9-1  |    0
    # (4 rows)
    for info in db_rev_data:
        info = [d.strip() for d in info.split('|')]
        rev_data[info[0]] = dict(zip(columns, info[1:]))
    logging.debug(rev_data)

    # Run upgrade scripts (if any) for all artefacts.
    for artefact, rev_info in rev_data.iteritems():
        scripts = scripts_to_run(artefact, rev_info, config)
        if scripts:
            logging.debug("%s: %s" % (artefact, scripts))
            if not run_scripts(artefact, rev_info, scripts, config):
                return False

    return True


def main(cargs):
    """Run SQL scripts that upgrade the database and/or migrate data."""
    def strip_dashes(arg):
        return arg.split('-')[-1]

    config = dict(
        db="openquake", user="postgres",
        path="openquake/engine/db/schema/upgrades",
        host="localhost", dryrun=False)
    longopts = ["%s" % k if isinstance(v, bool) else "%s=" % k
                for k, v in config.iteritems()] + ["help"]
    s2l = dict(d="db", p="path", n="dryrun", U="user")

    try:
        opts, args = getopt.getopt(cargs[1:], "hd:np:U:", longopts)
    except getopt.GetoptError, e:
        print e
        print __doc__
        sys.exit(101)

    for opt, arg in opts:
        if opt in ("-h", "--help"):
            print __doc__
            sys.exit(0)
        opt = strip_dashes(opt)
        if opt not in config:
            opt = s2l[opt]
        config[opt] = arg if arg else not config[opt]

    success = perform_upgrade(config)
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
__FILENAME__ = test_migration
#!/usr/bin/env python

import os
import re
import subprocess
import sys

db_admin_user = 'postgres'
original_db = 'original'


def info(message):
    print message


def quiet_check_call(*args, **kwargs):
    try:
        with open('/tmp/subprocess.out', 'w') as out:
            subprocess.check_call(*args,
                                   stderr=subprocess.STDOUT,
                                   stdout=out,
                                   **kwargs)
    except:
        with open('/tmp/subprocess.out') as fh:
            print fh.read()

        raise
    finally:
        os.unlink('/tmp/subprocess.out')


def psql(*args):
    quiet_check_call(['psql', '-U', db_admin_user] + list(args))


def load(path):
    quiet_check_call('zcat %s | psql -U %s -d %s' % (
            path, db_admin_user, original_db), shell=True)


def pg_dump(to_file):
    pg = subprocess.Popen(['pg_dump', '--schema-only', '-U', db_admin_user,
                           '--exclude-schema', 'public',
                           original_db], stdout=subprocess.PIPE)

    comments = []
    columns = []
    alters = []
    alter_start = None
    in_table = False

    with open(to_file, 'w') as out:
        for line in pg.stdout:
            # skip SQL comments and blank lines
            if line.startswith('--') or line == '\n':
                continue

            # normalize conditions (for some reason pg_dump dumps them
            # differently before and after an upgrade)
            if line.startswith('    CONSTRAINT'):
                line = re.sub(r"\('(\w+)'::character varying\)::text",
                              r"'\1'::character varying", line)
                line = re.sub(r"ANY \(\(ARRAY\[(.*?)\]\)::text\[\]\)",
                              r"ANY (ARRAY[\1])", line)

            # reorder table columns
            if in_table:
                if line == ');\n':
                    # table finished
                    in_table = False
                else:
                    # accumulate table lines
                    line = re.sub(r",\n", r"\n", line)
                    columns.append(line)
                    continue

                # write table lines sorted
                for column in sorted(columns):
                    out.write(column)

            # ignore the name of UNIQUE keys until we start naming
            # them explicitly
            if alter_start:
                if re.match(r"^    ADD CONSTRAINT \w+_key UNIQUE ", line):
                    line = re.sub(r"    ADD CONSTRAINT (\w+)_key UNIQUE ",
                                  r"    ADD CONSTRAINT <elided>_key UNIQUE ",
                                  line)
                    alters.append(alter_start + line)
                    alter_start = None
                    continue
                else:
                    line = alter_start + line
                    alter_start = None
            elif line.startswith('ALTER TABLE ONLY '):
                alter_start = line
                continue

            # move comments to the end
            if line.startswith('COMMENT '):
                comments.append(line)
                continue

            if line.startswith('CREATE TABLE'):
                in_table = True
                columns = []

            out.write(line)

        # write out alters sorted
        for alter in sorted(alters):
            out.write(alter)

        # write out comments sorted
        for comment in sorted(comments):
            out.write(comment)


# DB from oq_create_db
info('Creating a fresh database using oq_create_db...')

psql('-c', "DROP DATABASE IF EXISTS %s" % original_db)
psql('-c', "CREATE DATABASE %s" % original_db)

quiet_check_call(['bin/oq_create_db', '--yes', '--no-tab-spaces',
                  '--db-name=%s' % original_db,
                  '--db-user=%s' % db_admin_user,
                  '--schema-path=%s' % 'openquake/db/schema'])

info('Dumping the database...')
pg_dump('/tmp/fresh.sql')

# DB from dbmaint.py
info('Loading database from old dump...')
psql('-c', "DROP DATABASE IF EXISTS %s" % original_db)
psql('-c', "CREATE DATABASE %s" % original_db)
load('tools/test_migration_base.sql.gz')

info('Upgrading database using dbmaint...')
quiet_check_call(['tools/dbmaint.py', '--db', original_db,
                  '-U', db_admin_user])

info('Dumping the database...')
pg_dump('/tmp/after.sql')

info('Comparing new and upgraded version...')
res = subprocess.call(['diff', '-u', '/tmp/fresh.sql', '/tmp/after.sql'])

sys.exit(res)

########NEW FILE########

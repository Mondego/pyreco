__FILENAME__ = conf
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# gaffer documentation build configuration file, created by
# sphinx-quickstart on Tue Oct  9 21:10:46 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os
import gaffer

CURDIR = os.path.abspath(os.path.dirname(__file__))
sys.path.append(os.path.join(CURDIR, '..', '..'))
sys.path.append(os.path.join(CURDIR, '..'))
sys.path.append(os.path.join(CURDIR, '.'))


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'gaffer'
copyright = '2012-2013 (c) Benoit Chesneau <benoitc@e-engura.org>'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.5'
# The full version, including alpha/beta/rc tags.
release = '0.5.2'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'gafferdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'gaffer.tex', 'gaffer Documentation',
   'Author', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'gaffer', 'gaffer Documentation',
     ['Author'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'gaffer', 'gaffer Documentation',
   'Author', 'gaffer', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = 'gaffer'
epub_author = 'Author'
epub_publisher = 'Author'
epub_copyright = '2012, Author'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = gafferpm_ext
import os
from gaffer.pm.commands import get_commands

_HEADER = """\
.. _gaffer:

Gaffer
======

The **gaffer** command line tool is an interface to the :doc:`gaffer
HTTP api <http>` and include support for loading/unloading Procfile
applications, scaling them up and down, ... .

It can also be used as a manager for Procfile-based applications similar to
foreman but using the :doc:`gaffer framework <processframework>`. It is
running your application directly using a Procfile or export it to a
gafferd configuration file or simply to a JSON file that you could send
to gafferd using the :doc:`HTTP api <http>`.

Example of use
--------------

For example using the following **Procfile**::

    dummy: python -u dummy_basic.py
    dummy1: python -u dummy_basic.py


You can launch all the programs in this procfile using the following
command line::

    $ gaffer start


.. image:: _static/gafferp.png


Or load them on a gaffer node::

    $ gaffer load

and then scale them up and down::

    $ gaffer scale dummy=3 dummy1+2
    Scaling dummy processes... done, now running 3
    Scaling dummy1 processes... done, now running 3


.. image:: _static/gaffer_ps.png


"""

_BOTTOM = """\


Command line usage
------------------

::

    $ gaffer
    usage: gaffer [options] command [args]

    manage Procfiles applications.

    optional arguments:
      -h, --help            show this help message and exit
      -c CONCURRENCY, --concurrency CONCURRENCY
                            Specify the number of each process type to run. The
                            value passed in should be in the format
                            process=num,process=num
      -e ENVS [ENVS ...], --env ENVS [ENVS ...]
                            Specify one or more .env files to load
      -f FILE, --procfile FILE
                            Specify an alternate Procfile to load
      -d ROOT, --directory ROOT
                            Specify an alternate application root. This defaults
                            to the directory containing the Procfile
      --endpoint ENDPOINT   Gaffer node URL to connect
      --version             show program's version number and exit

    Commands:
    ---------

        start 	Start a process
        run   	Run one-off command
        export	Export a Procfile
        load  	Load a Procfile application to gafferd
        unload	Unload a Procfile application to gafferd
        scale 	Scaling your process
        ps    	List your process informations
        help  	Get help on a command
"""

def generate_commands(app):
    path = os.path.join(app.srcdir, "pm")
    ext = app.config['source_suffix']
    if not os.path.exists(path):
        os.makedirs(path)

    tocname = os.path.join(app.srcdir, "gaffer%s" % ext)

    with open(tocname, "w") as toc:
        toc.write(_HEADER)
        toc.write("gaffer commands\n")
        toc.write("-------------------\n\n")

        commands = get_commands()
        for name, cmd in commands.items():
            toc.write("- **%s**: :doc:`pm/%s`\n" % (name, name))

            # write the command file
            refline = ".. _gaffer_%s:" % name
            fname = os.path.join(path, "%s%s" % (name, ext))
            with open(fname, "w") as f:
                f.write("\n".join([refline, "\n", cmd.desc, ""]))

        toc.write("\n")
        toc.write(".. toctree::\n")
        toc.write("   :hidden:\n")
        toc.write("   :glob:\n\n")
        toc.write("   pm/*\n")
        toc.write(_BOTTOM)


def setup(app):
    app.connect('builder-inited', generate_commands)

########NEW FILE########
__FILENAME__ = dummy
#!/usr/bin/env python
import os
import signal
import sys
import time


class Dummy(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        print("hello, dummy (pid: %s) is alive" % os.getpid())
        print("env %s" % os.environ)

        i = 0
        while self.alive:
            sys.stdout.write("STDOUT %s\n" % i)
            sys.stdout.flush()
            sys.stderr.write("STDERR %s\n" % i)
            sys.stderr.flush()
            time.sleep(1.0)
            i += 1
if __name__ == "__main__":
    Dummy().run()

########NEW FILE########
__FILENAME__ = echo
#!/usr/bin/env python
import os
import signal
import sys
import time


class Echo(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        while self.alive:
            c = sys.stdin.readline()
            sys.stdout.write(c)
            sys.stdout.flush()
            time.sleep(0.1)
if __name__ == "__main__":
    Echo().run()

########NEW FILE########
__FILENAME__ = app
class DummyApp(object):

    def start(self, loop, manager):
        print("start dummy app")

    def stop(sef):
        print("stop dummy")

    def restart(self):
        print("restart dummy")

########NEW FILE########
__FILENAME__ = main
from gaffer import Plugin

__all__ = ['DummyPlugin']

from .app import DummyApp


class DummyPlugin(Plugin):
    name = "dummy"
    version = "1.0"
    description = "test"


    def app(self, cfg):
        return DummyApp()

########NEW FILE########
__FILENAME__ = main
from gaffer import Plugin

__all__ = ['DummyPlugin1']

class DummyApp1(object):

    def start(self, loop, manager):
        print("start dummy 1 app")

    def stop(sef):
        print("stop dummy 1")

    def restart(self):
        print("restart dummy")

class DummyPlugin1(Plugin):
    name = "dummy1"
    version = "1.0"
    description = "test"


    def app(self, cfg):
        return DummyApp1()

########NEW FILE########
__FILENAME__ = echo_client
from websocket import create_connection
ws = create_connection("ws://localhost:5000/wstreams/2")
ws.send("ECHO\n")
print "Sent"
print "Receiving..."
result =  ws.recv()
print "Received '%s'" % result
ws.close()

########NEW FILE########
__FILENAME__ = echo_stream
#!/usr/bin/env python
from __future__ import print_function

import os
import signal
import sys
import time

PY3 = sys.version_info[0] == 3

class Echo(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        i = 0
        try:
            if PY3:
                stream = os.fdopen(3, 'wb+', buffering=0)
            else:
                stream =  os.fdopen(3, "w+")

            while self.alive:
                c = stream.readline()
                if PY3:
                    stream.write(c)
                else:
                    print(c, file=stream)

                stream.flush()
                time.sleep(0.1)
        except Exception as e:
            sys.stdout.write(str(e))
            sys.stdout.flush()

if __name__ == "__main__":
    Echo().run()

########NEW FILE########
__FILENAME__ = dummy
#!/usr/bin/env python
import os
import signal
import sys
import time


class Dummy(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        print("hello, dummy (pid: %s) is alive" % os.getpid())
        print("env %s" % os.environ)

        i = 0
        while self.alive:
            sys.stdout.write("STDOUT %s\n" % i)
            sys.stdout.flush()
            sys.stderr.write("STDERR %s\n" % i)
            sys.stderr.flush()
            time.sleep(1.0)
            i += 1
if __name__ == "__main__":
    Dummy().run()

########NEW FILE########
__FILENAME__ = dummy_basic
#!/usr/bin/env python
import os
import signal
import sys
import time


class Dummy(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        print("hello, dummy (pid: %s) is alive" % os.getpid())
        while self.alive:
            time.sleep(0.01)

if __name__ == "__main__":
    Dummy().run()

########NEW FILE########
__FILENAME__ = echo
#!/usr/bin/env python
import os
import signal
import sys
import time


class Echo(object):

    def __init__(self):
        # init signal handling
        signal.signal(signal.SIGQUIT, self.handle_quit)
        signal.signal(signal.SIGTERM, self.handle_quit)
        signal.signal(signal.SIGINT, self.handle_quit)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        self.alive = True

    def handle_quit(self, *args):
        self.alive = False
        sys.exit(0)

    def handle_chld(self, *args):
        return

    def run(self):
        while self.alive:
            c = sys.stdin.readline()
            sys.stdout.write(c)
            sys.stdout.flush()
            time.sleep(0.1)
if __name__ == "__main__":
    Echo().run()

########NEW FILE########
__FILENAME__ = chat
# -*- coding: utf-8 -*-
"""
    Simple sockjs-tornado chat application. By default will listen on port 8080.
"""

from tornado import ioloop
import tornado.web

from gaffer import sockjs
from gaffer import tornado_pyuv

class IndexHandler(tornado.web.RequestHandler):
    """Regular HTTP handler to serve the chatroom page"""
    def get(self):
        self.render('index.html')


class ChatConnection(sockjs.SockJSConnection):
    """Chat connection implementation"""
    # Class level variable
    participants = set()

    def on_open(self, info):
        # Send that someone joined
        self.broadcast(self.participants, "Someone joined.")

        # Add client to the clients list
        self.participants.add(self)

    def on_message(self, message):
        # Broadcast message
        self.broadcast(self.participants, message)

    def on_close(self):
        # Remove client from the clients list and broadcast leave message
        self.participants.remove(self)

        self.broadcast(self.participants, "Someone left.")

if __name__ == "__main__":
    import logging
    logging.getLogger().setLevel(logging.DEBUG)

    import pyuv
    from gaffer.tornado_pyuv import IOLoop

    loop = pyuv.Loop.default_loop()
    ioloop = IOLoop(_loop=loop)

    # 1. Create chat router
    ChatRouter = sockjs.SockJSRouter(ChatConnection, '/chat',
            io_loop=ioloop)

    # 2. Create Tornado application
    app = tornado.web.Application(
            [(r"/", IndexHandler)] + ChatRouter.urls
    )

    # 3. Make Tornado app listen on port 8080
    app.listen(8080, io_loop=ioloop)

    # 4. Start IOLoop
    while True:
        try:
            if not loop.run(pyuv.UV_RUN_ONCE):
                break
        except KeyboardInterrupt:
            break

########NEW FILE########
__FILENAME__ = chat_console
# -*- coding: utf-8 -*-

import sys

import pyuv
from tornado import ioloop

from gaffer.httpclient import WebSocket


class Input(object):

    def __init__(self, chat):
        self.chat = chat
        self.tty = pyuv.TTY(chat.loop, True)
        self.tty.start_read(self.on_input)
        self.tty.unref()

    def stop(self):
        self.tty.stop()

    def on_input(self, handle, data, err):
        self.chat.write_message(data)


class ChatReader(WebSocket):

    def on_open(self):
        print("connection openned")
        self.console = Input(self)

    def on_message(self, data):
        print("received %s" % data)

    def on_close(self):
        print('closed')
        self.console.stop()

loop = pyuv.Loop.default_loop()

ws = ChatReader(loop, "ws://localhost:8080/chat/websocket")
ws.start()

try:
    while True:
        try:
            if not loop.run(pyuv.UV_RUN_ONCE):
                break
        except KeyboardInterrupt:
            break

finally:
    ws.close()

########NEW FILE########
__FILENAME__ = apps
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from .base import Command


class Apps(Command):
    """
    usage: gaffer apps

    """

    name = "apps"
    short_desc = "list of applications"

    def run(self, config, args):
        sessions = config.server.sessions()
        for session in sessions:
            print(session)

########NEW FILE########
__FILENAME__ = base
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import OrderedDict
import copy
import json
import os
import re
import sys

try:
    input = raw_input
except NameError:
    pass

KNOWN_COMMANDS = []
def get_commands():
    commands = OrderedDict()
    for c in KNOWN_COMMANDS:
        cmd = c()
        commands[c.name] = cmd.copy()
    return commands

class CommandMeta(type):

    def __new__(cls, name, bases, attrs):
        super_new = type.__new__
        parents = [b for b in bases if isinstance(b, CommandMeta)]
        if not parents:
            return super_new(cls, name, bases, attrs)
        attrs["order"] = len(KNOWN_COMMANDS)
        new_class = super_new(cls, name, bases, attrs)
        KNOWN_COMMANDS.append(new_class)
        return new_class

VALID_APPNAME = re.compile(r'^[a-z][a-z0-9_+-]*$')

class Command(object):

    name = None
    short_descr = None
    order = 0

    def copy(self):
        return copy.copy(self)

    def run(self, config, args):
        raise NotImplementedError

    def parse_concurrency(self, args):
        if not args["--concurrency"]:
            return {}

        settings = {}
        for setting in args["--concurrency"]:
            kv = setting.split("=")
            if len(kv) == 2:
                key = kv[0].strip()
                try:
                    val = int(kv[1].strip())
                except ValueError:
                    continue
                settings[key] = val
        return settings

    def default_appname(self, config, args):
        if args['--app']:
            appname = args['--app']

            # appname is relative to current dir
            if appname == ".":
                if config.use_procfile:
                    appname = config.procfile.get_appname()
                else:
                    appname = os.path.split(os.getcwd())[1]

        elif config.procfile is not None:
            appname = config.procfile.get_appname()
        else:
            appname = "default"

        if not VALID_APPNAME.match(appname):
            raise ValueError("Invalid APP name: %r" % appname)

        return appname

    def parse_name(self, name, default="default"):
        if "." in name:
            appname, name = name.split(".", 1)
        elif "/" in name:
            appname, name = name.split("/", 1)
        else:
            appname = default

        return appname, name

    def use_procfile(self, config, appname):
        return (config.use_procfile and
                appname == config.procfile.get_appname())

    def confirm(self, prompt, resp=True):
        if resp:
            prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')
        else:
            prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')

        while True:
            ret = input(prompt).lower()
            if not ret:
                return resp

            if ret not in ('y', 'n'):
                print('please enter y or n.')
                continue

            if ret == "y":
                return True

            return False

    def load_jsonconfig(self, fname):
        if fname == "-":
            content = []
            while True:
                data = sys.stdin.readline()
                if not data:
                    break
                content.append(data)
            content = ''.join(content)
        else:
            if not os.path.isfile(fname):
                raise RuntimeError("%r not found" % fname)

            with open(fname, 'rb') as f:
                content = f.read()

        if isinstance(content, bytes):
            content = content.decode('utf-8')

        # parse the config
        obj = json.loads(content)
        if "jobs" in obj:
            configs = obj['jobs']
        else:
            configs = [obj]

        return configs

Command = CommandMeta('Command', (Command,), {})

########NEW FILE########
__FILENAME__ = dev_run
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os

from .base import Command

from ...manager import Manager
from ...console_output import ConsoleOutput
from ...process import ProcessConfig
from ...sig_handler import SigHandler


class DevRun(Command):
    """
    usage: gaffer dev:run [-v] [-c concurrency|--concurrency concurrency]
                          (<args>...)

      -h, --help
      -c concurrency,--concurrency concurrency  Specify the number processesses
                                                to run.
      -v  verbose
    """

    name = "dev:run"
    short_descr = "run one-off commands locally"

    def run(self, config, args):
        if not config.use_procfile:
            raise RuntimeError("procfile not found")

        m = Manager()
        m.start(apps=[SigHandler(), ConsoleOutput()])

        if not args['--concurrency']:
            numprocesses = 1
        else:
            numprocesses = int(args['--concurrency'])

        # parse command
        cmd_str = " ".join(args['<args>'])
        cmd, cmd_args = config.procfile.parse_cmd(cmd_str)
        name = os.path.basename(cmd)

        appname = config.procfile.get_appname()

        # if verbose the display stdout/stderr
        if args['-v']:
            redirect_output = ['out', 'err']
        else:
            redirect_output = []

        params = dict(args=cmd_args, env=config.procfile.env,
                numprocesses=numprocesses,
                redirect_output=redirect_output)

        config = ProcessConfig(name, cmd, **params)
        m.load(config, sessionid=appname)
        m.run()

########NEW FILE########
__FILENAME__ = dev_start
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from .base import Command

from ...manager import Manager
from ...console_output import ConsoleOutput
from ...process import ProcessConfig
from ...sig_handler import SigHandler


class Start(Command):
    """
    usage: gaffer dev:start [-v] [-c job=v|--concurrency job=v]...
                            [<job>]

      -c job=v,--concurrency job=v  Specify the number of each process
                                    type to run.
      -v  verbose
    """

    name = "dev:start"
    short_descr = "start a process locally from a procfile"

    def run(self, config, args):
        if not config.use_procfile:
            raise RuntimeError("procfile not found")

        concurrency = self.parse_concurrency(args)
        m = Manager()
        m.start(apps=[SigHandler(), ConsoleOutput()])

        # if verbose the display stdout/stderr
        if args['-v']:
            redirect_output = ['out', 'err']
        else:
            redirect_output = []

        # load job configs
        if args['<job>']:
            name = args['<job>']
            cmd_str = config.procfile.cfg[name]
            cmd, cmd_args = config.procfile.parse_cmd(cmd_str)
            appname = config.procfile.get_appname()
            params = dict(args=cmd_args, env=config.procfile.env,
                    numprocesses=concurrency.get(name, 1),
                    redirect_output=redirect_output)

            pconfig = ProcessConfig(name, cmd, **params)
            m.load(pconfig, sessionid=appname)
        else:
            appname = config.procfile.get_appname()

            # add processes
            for name, cmd_str in config.procfile.processes():
                cmd, cmd_args = config.procfile.parse_cmd(cmd_str)
                params = dict(args=cmd_args, env=config.procfile.env,
                        numprocesses=concurrency.get(name, 1),
                        redirect_output=redirect_output)
                pconfig = ProcessConfig(name, cmd, **params)
                m.load(pconfig, sessionid=appname)

        # run the gaffer manager
        m.run()

########NEW FILE########
__FILENAME__ = export
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from io import StringIO
import json
import sys

from .base import Command

class Export(Command):
    """
    usage: gaffer export [-c concurrency|--concurrency concurrency]...
                         [--format=format] [--out=filename] [<name>]

      -h, --help
      -c concurrency,--concurrency concurrency  Specify the number processesses
                                                to run.
      --format=format                           json or ini
      --out=filename
    """

    name = "export"
    short_descr = "export a Procfile"

    def run(self, config, args):
        concurrency = self.parse_concurrency(args)
        if args['--format'] == "json":
            if not args['<name>']:
                print("you should provide a process type to export")
                sys.exit(1)

            try:
                obj = config.procfile.as_dict(args["<name>"], concurrency)
            except KeyError:
                raise KeyError("%r is not found" % args["<name>"])

            if args['--out']:
                with open(args['--out'], 'w') as f:
                    json.dump(obj, f, indent=True)

            else:
                print(json.dumps(obj, indent=True))
        else:
            config = config.procfile.as_configparser(concurrency)
            if args['--out']:
                with open(args['--out'], 'w') as f:
                    config.write(f, space_around_delimiters=True)
            else:
                buf = StringIO()
                config.write(buf, space_around_delimiters=True)
                print(buf.getvalue())

########NEW FILE########
__FILENAME__ = info
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...httpclient import GafferNotFound
from .base import Command

class Info(Command):
    """
    usage: gaffer ps:info <target>  [--app APP]

      <target> job name or process id

      -h, --help
      --app APP  application name
    """

    name = "ps:info"
    short_descr = "get job info"

    def run(self, config, args):
        if args['<target>'].isdigit():
            # get a process info
            try:
                p = config.server.get_process(int(args['<target>']))
            except GafferNotFound:
                raise RuntimeError('process %r not found' % args['<target>'])

            lines = ["%s: %s" % (k, v) for k, v in p.info.items()]
        else:
            # get a job info
            appname, name = self.parse_name(args['<target>'],
                    self.default_appname(config, args))

            try:
                job = config.server.get_job("%s.%s" % (appname, name))
            except GafferNotFound:
                raise RuntimeError('job %r not found' % args['<target>'])
            info = job.info()
            pids = [str(pid) for pid in info['processes']]

            lines = ["active: %s" % info['active'],
                     "running: %s" % info['running'],
                     "num_processes: %s" % info['max_processes'],
                     "pids: %s" % ",".join(pids)]

        # finally print the result
        print("\n".join(lines))


########NEW FILE########
__FILENAME__ = key
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...httpclient.base import GafferNotFound
from ...httpclient.keys import Keys
from .base import Command

class Key(Command):
    """
    usage: gaffer key <apikey>

      <apikey> API key
    """

    name = "key"
    short_descr = "Fetch the key infos"

    def run(self, config, args):
        server = config.get('server')

        keys = Keys(server)
        try:
            key = keys.get_key(args['<apikey>'])
        except GafferNotFound:
            raise RuntimeError("Key %r not found" % args['<apikey>'])


        permissions = key.get('permissions', {})
        print("Key %r found." % args['<apikey>'])
        print("Label: %s" % key.get("label", ""))
        print("Permisssions:")
        display_permissions(permissions)

def display_permissions(permissions):
    is_admin = permissions.get('admin') == True
    print("  admin: %s" % permissions.get('admin', False))
    if is_admin:
        print("  create_user: True")
        print("  create_key: True")
        print("  manage: *")
        print("  read: *")
        print("  write: *")
    else:
        print("  create_key: %s" % permissions.get('create_key', False))
        print("  create_user: %s" % permissions.get('create_user',
            False))
        print("  manage: %s" % ",".join(permissions.get("manage", [])))
        print("  read: %s" % ",".join(permissions.get("read", [])))
        print("  write: %s" % ",".join(permissions.get("write", [])))

########NEW FILE########
__FILENAME__ = key_add
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...gafferd.util import confirm
from ...httpclient.base import GafferConflict
from ...httpclient.keys import Keys
from .base import Command
from .key import display_permissions

class KeyCreate(Command):
    """
    usage: gaffer key:create <args>... [--label LABEL]

      <args> permission to add to this key.
      --label LABEL  the key label

    Arguments:

        +a: give admin right
        +u: give create_user permission
        +k: give create_key permission
        +m manage all

        <session>+rwm where r is for read, w for write and m for manage
            permissions. You can mix them or just use one of them. (ex.
            <session>+r will oonly give the read permission on the session)
        <session.jobname>+rwm

    Example:

        gaffer key:create default+rw test+m

    will create a key with read andwrite permission on the default session and
    manae permissions on the test session.
    """

    name = "key:create"
    short_descr = "Create a key"

    def run(self, config, args):
        server = config.get('server')

        # parse permissions
        permissions = {"admin": False, "create_user": False,
                "create_key": False, "manage": [], "read": [], "write": []}
        is_admin = False
        for arg in args['<args>']:
            if arg == "+a":
                permissions['admin'] = True
                is_admin = True
            elif arg == "+u":
                permissions['create_user'] = True
            elif arg == "+k":
                permissions['create_key'] = True
            elif arg == "+m":
                permissions['manage'].append('*')
            else:
                try:
                    target, perms = arg.split("+")
                except ValueError:
                    continue

                for c in perms:
                    if c == "m":
                        permissions['manage'].append(target)
                    elif c == "r":
                        permissions['read'].append(target)
                    elif c == "w":
                        permissions['write'].append(target)

        if is_admin:
            permissions['create_key'] = True
            permissions['create_user'] = True

        print("A key with the following permissions will be created:\n")
        display_permissions(permissions)
        print("")

        try:
            if not confirm("Create the key"):
                return
        except KeyboardInterrupt:
            return

        keys = Keys(server)
        try:
            key = keys.create_key(permissions, label=args['--label'])
        except GafferConflict:
            raise RuntimeError("The key already exists")
        print("The key %r has been created." % key)

########NEW FILE########
__FILENAME__ = key_delete
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...gafferd.util import confirm
from ...httpclient.base import GafferNotFound
from ...httpclient.keys import Keys
from .base import Command

class KeyDelete(Command):
    """
    usage: gaffer key:delete <apikey>

      <apikey> the KEY to delete

    """

    name = "key:delete"
    short_descr = "Delete a key"

    def run(self, config, args):
        server = config.get('server')
        try:
            if not confirm("Delete the key %r" % args['<apikey>']):
                return
        except KeyboardInterrupt:
            return

        keys = Keys(server)
        try:
            key = keys.delete_key(args['<apikey>'])
        except GafferNotFound:
            raise RuntimeError("Key %r not found" % args['<apikey>'])
        print("The key %r has been deleted." % args['<apikey>'])

########NEW FILE########
__FILENAME__ = kill
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
from .base import Command
from ...httpclient import GafferNotFound

class kill(Command):

    """
    usage: gaffer ps:kill <proc> <sig> [--app=appname] [--no-input]

      <proc>  pid or job label
      <sig>   signal number or signal name

      --app=appname  name of the procfile application.
      --no-input        don't prompt a confirmation
    """

    name = "ps:kill"
    short_descr = "send a signal to a process or pid"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get("server", "procfile")
        name = args['<proc>']

        if name.isdigit():
            # we want to stop a process from a job
            try:
                process = server.get_process(int(name))
            except GafferNotFound:
                raise RuntimeError("%r not found" % name)
            process.kill(args['<sig>'])
        else:
            # we want to stop a job
            appname, job_name = self.parse_name(name, appname)
            if (self.use_procfile(config, appname) and
                    job_name not in procfile.cfg):
                raise RuntimeError("%r not found" % name)

            pname = "%s.%s" % (appname, name)
            try:
                job = server.get_job(pname)
            except GafferNotFound:
                raise RuntimeError("%r not found" % name)
            job.kill(args['<sig>'])

        print("%r sent to %r" % (args['<sig>'], name))

########NEW FILE########
__FILENAME__ = load
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os

from .base import Command
from ...httpclient import GafferConflict
from ...process import ProcessConfig


class Load(Command):
    """
    usage: gaffer load [-c concurrency|--concurrency concurrency]...
                       [--app APP] [<file>]

    Args

        <file>  Path to a job configuration  or stdin ``-``
    Options

    -h, --help
    -c concurrency,--concurrency concurrency  Specify the number processesses
                                              to run.
    --app APP                                 application name
    """

    name = "load"
    short_descr = "load a Procfile application"

    def run(self, config, args):
        if args['<file>']:
            self.load_file(config, args)
        elif config.use_procfile:
            self.load_procfile(config, args)
        else:
            raise RuntimeError("procfile or job file is missing")

    def load_file(self, config, args):
        fname = args['<file>']
        server = config.get("server")

        # load configs
        configs = self.load_jsonconfig(fname)

        for conf in configs:
            try:
                name = conf.pop('name')
                cmd = conf.pop('cmd')
            except KeyError:
                raise ValueError("invalid job config")

            # parse job name and eventually extract the appname
            appname, name = self.parse_name(name, self.default_appname(config,
                args))

            # always force the appname if specified
            if args['--app']:
                appname = args['--app']

            # finally load the config
            pname = "%s.%s" % (appname, name)
            start = conf.get('start', True)
            pconfig = ProcessConfig(name, cmd, **conf)
            try:
                server.load(pconfig, sessionid=appname, start=start)
                print("%r has been loaded in %s" % (pname, server.uri))
            except GafferConflict:
                print("%r already loaded" % pname)

        print("%r has been loaded" % fname)

    def load_procfile(self, config, args):
        procfile, server = config.get("procfile", "server")
        appname = self.default_appname(config, args)

        # parse the concurrency settings
        concurrency = self.parse_concurrency(args)

        # finally send the processes
        for name, cmd_str in procfile.processes():
            if name in procfile.redirect_input:
                redirect_input = True
            else:
                redirect_input = False

            cmd, args = procfile.parse_cmd(cmd_str)
            params = dict(args=args, env=procfile.env,
                    numprocesses=concurrency.get(name, 1),
                    redirect_output=['out', 'err'],
                    redirect_input=redirect_input,
                    cwd=os.path.abspath(procfile.root))

            config = ProcessConfig(name, cmd, **params)
            try:
                server.load(config, sessionid=appname)
            except GafferConflict:
                print("%r already loaded" % name)

        print("==> %r has been loaded in %s" % (appname, server.uri))

########NEW FILE########
__FILENAME__ = login
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

try:
    import configparser
except ImportError:
    import ConfigParser as configparser

try:
    input = raw_input
except NameError:
    pass

from getpass import getpass
import os
import sys

from ...httpclient import GafferUnauthorized
from .base import Command


class Login(Command):
    """
    usage: gaffer login [-u USERNAME] [-p PASSWORD]

      -u USERNAME  user name
      -p PASSWORD  password
    """

    name = "login"
    short_descr = "login to the gafferd node"

    def run(self, config, args):
        username = args["-u"]
        password = args["-p"]

        server = config.get("server")

        try:
            if username is None:
                while True:
                    username = input("username: ")
                    if username and username is not None:
                        break

                    print("username is empty. Please enter a username.")

            if password is None:
                while True:
                    password = getpass("password: ")
                    if password and password is not None:
                        break
                    print("password is empty. Please enter a password.")
        except KeyboardInterrupt:
            print("\nnot authenticated")
            return

        try:
            api_key = server.authenticate(username, password)
        except GafferUnauthorized:
            print("unauthorized: username or password is wrong")
            sys.exit(1)

        # store the key to use it later
        self.save_key(config, api_key)
        print("User %r authenticated." % username)

    def save_key(self, config, api_key):
        cfg = configparser.RawConfigParser()

        if os.path.isfile(config.user_config_path):
            with open(config.user_config_path) as f:
                cfg.readfp(f)

        section = "node \"%s\"" % config.server.uri

        if not cfg.has_section(section):
            cfg.add_section(section)

        cfg.set(section, "key", api_key)

        with open(config.user_config_path, 'w') as f:
            cfg.write(f)

########NEW FILE########
__FILENAME__ = logs
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import copy
from datetime import datetime
import sys

import pyuv

from .base import Command
from ...console_output import colored, GAFFER_COLORS

class Logs(Command):

    """
    usage: gaffer logs  [--ps JOB] [--app APP] [--no-color]

      --ps JOB      job name
      --app APP     name of the procfile application.
      --no-color    return with colors
    """

    name = "logs"
    short_descr = "Get logs for an app"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server  = config.get("server")

        self.nocolor = args["--no-color"]
        self._balance = copy.copy(GAFFER_COLORS)
        self._process_colors = {}

        socket = server.socket()
        socket.start()

        socket.subscribe('EVENTS')

        if args['--ps']:
            pattern = "job.%s.%s." % (appname, args['--ps'])
        else:
            pattern = "job.%s." % appname

        socket['EVENTS'].bind(pattern, self._on_event)

        while True:
            try:
                if not server.loop.run(pyuv.UV_RUN_ONCE):
                    break
            except KeyboardInterrupt:
                break

    def _on_event(self, event, msg):
        name = msg['name']
        if not 'pid' in msg:
            line = self._print(name, '%s %s' % (event, name))
        else:
            line = self._print(name, '%s process with pid %s' % (event,
                msg['pid']))

        self._write(name, line)

    def _write(self, name, lines):
        if not self.nocolor:
            sys.stdout.write(colored(self._get_process_color(name), lines))
        else:
            if not isinstance(lines, list):
                lines = [lines]

            sys.stdout.write(''.join(lines))
        sys.stdout.flush()

    def _print(self, name, line):
        now = datetime.now().strftime('%H:%M:%S')
        prefix = '{time} {name} | '.format(time=now, name=name)
        return ''.join([prefix, line, '\n'])

    def _set_process_color(self, name):
        code = self._balance.pop(0)
        self._process_colors[name] = code
        self._balance.append(code)

    def _get_process_color(self, name):
        if name not in self._process_colors:
            self._set_process_color(name)
        return self._process_colors[name]

########NEW FILE########
__FILENAME__ = lookup
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from datetime import datetime
import copy
import sys

from ...console_output import colored, GAFFER_COLORS
from ...lookupd.client import LookupServer
from ...sig_handler import BaseSigHandler
from .base import Command

import pyuv

class SigHandler(BaseSigHandler):

    def __init__(self, channel):
        super(SigHandler, self).__init__()
        self.channel = channel

    def handle_quit(self, h, *args):
        self.stop()
        self.channel.close()

    def handle_reload(self, h, *args):
        return

class Lookup(Command):
    """
    usage: gaffer lookup (-L ADDR|--lookupd-address=ADDR) [--no-color]


      -h, --help
      --no-color  return with colors
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup"
    short_descr = "watch lookup events on one lookupd server"


    def run(self, config, args):
        self.nocolor = args["--no-color"]
        self._balance = copy.copy(GAFFER_COLORS)
        self._event_colors = {}

        loop = pyuv.Loop.default_loop()

        # connect to the lookupd server channel
        s = LookupServer(args['--lookupd-address'], loop=loop,
                **config.client_options)
        channel = self.channel = s.lookup()

        # initialize the signal handler
        self._sig_handler = SigHandler(channel)
        self._sig_handler.start(loop)

        # bind to all events

        channel.bind_all(self._on_event)
        channel.start()

        loop.run()

    def _on_event(self, event, msg):
        if event == "add_node":
            line = self._print(event, "add node")
        elif event == "identify":
            line = self._print(event, "%s: %s" % (msg['name'],
                msg['origin']))
        elif event == "remove_node":
            line = self._print(event, "%s: node %s" % (msg['name'],
                msg['origin']))
        else:
            uri = msg['node']['origin']
            if event == "add_job":
                line = "load %s in %s" % (msg["job_name"], uri)
            elif event == "remove_job":
                line = "unload %s in %s" % (msg["job_name"], uri)
            elif event == "add_process":
                line = "%s: process id %s spanwned on %s" % (msg["job_name"],
                        msg["pid"], uri)
            elif event == "remove_process":
                line = "%s: process %s exited on %s" % (msg["job_name"],
                        msg["pid"], uri)

            line = self._print(event, line)

        self._write(event, line)

    def _write(self, event, lines):
        if not isinstance(lines, list):
            lines = [lines]

        if not self.nocolor:
            sys.stdout.write(colored(self._get_event_color(event), lines))
        else:
            sys.stdout.write(''.join(lines))
        sys.stdout.flush()

    def _print(self, event, line):
        now = datetime.now().strftime('%H:%M:%S')
        prefix = '{time} {event} | '.format(time=now, event=event)
        return ''.join([prefix, line, '\n'])

    def _set_event_color(self, event):
        code = self._balance.pop(0)
        self._event_colors[event] = code
        self._balance.append(code)

    def _get_event_color(self, event):
        if event not in self._event_colors:
            self._set_event_color(event)
        return self._event_colors[event]

########NEW FILE########
__FILENAME__ = lookup_job
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy

from ...console_output import colored, GAFFER_COLORS
from ...lookupd.client import LookupServer
from .base import Command

import pyuv

class LookupJob(Command):
    """
    usage: gaffer lookup:job <job> (-L ADDR|--lookupd-address=ADDR)...

      -h, --help
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup:job"
    short_descr = "find a job on different lookupd servers"


    def run(self, config, args):
        lookupd_addresses = set(args['--lookupd-address'])
        job_name = args['<job>']

        sources = []
        loop = pyuv.Loop.default_loop()
        for addr in lookupd_addresses:
            s = LookupServer(addr, loop=loop, **config.client_options)
            resp = s.find_job(job_name)
            new_sources = resp.get('sources', [])
            if not new_sources:
                continue
            sources.extend(new_sources)

        loop.run()
        if not sources:
            print("no job found")
            return

        balance = copy.copy(GAFFER_COLORS)
        for source in sources:
            name = source['node_info']['name']
            version = source['node_info']['version']

            pids = [str(pid) for pid in source['pids']]
            lines = ["=== %s (Protocol: %s)" % (name, version),
                    "Origin: %s" % source['node_info']['origin'],
                    "Pids: %s" % (",".join(pids)), ""]
            color, balance = self.get_color(balance)
            print(colored(color, "\n".join(lines)))

    def get_color(self, balance):
        code = balance.pop(0)
        balance.append(code)
        return code, balance

########NEW FILE########
__FILENAME__ = lookup_jobs
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...lookupd.client import LookupServer
from .base import Command

import pyuv

class LookupJobs(Command):
    """
    usage: gaffer lookup:jobs (-L ADDR|--lookupd-address=ADDR)...

      -h, --help
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup:jobs"
    short_descr = "list all jobs in  lookupd servers"


    def run(self, config, args):
        lookupd_addresses = set(args['--lookupd-address'])

        loop = pyuv.Loop.default_loop()
        all_jobs = {}
        for addr in lookupd_addresses:
            s = LookupServer(addr, loop=loop, **config.client_options)
            resp = s.jobs()

            for job in resp['jobs']:
                job_name = job['name']
                if job_name in all_jobs:
                    all_jobs[job_name] = list(
                            set(all_jobs[job_name] + job['sources'])
                    )
                else:
                    all_jobs[job_name] = job['sources']
        loop.run()
        print("%s job(s) found\n" % len(all_jobs))
        for job_name, sources in all_jobs.items():
            lines = ["=== %s" % job_name]

            for source in sources:
                name = source['node_info']['name']
                version = source['node_info']['version']
                origin = source['node_info']['origin']
                lines.append("%s - name: %s, protocol: %s" % (origin,
                    name, version))

            lines.append("")
            print("\n".join(lines))

########NEW FILE########
__FILENAME__ = lookup_nodes
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy

from ...console_output import colored, GAFFER_COLORS
from ...httpclient import GafferNotFound
from ...lookupd.client import LookupServer
from ...util import is_ssl
from .base import Command

import pyuv

class LookupNodes(Command):
    """
    usage: gaffer lookup:nodes (-L ADDR|--lookupd-address=ADDR)...

      -h, --help
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup:nodes"
    short_descr = "list all gaffer nodes"

    def run(self, config, args):
        lookupd_addresses = set(args['--lookupd-address'])

        loop = pyuv.Loop.default_loop()
        all_nodes = []
        for addr in lookupd_addresses:
            # connect to the lookupd server channel
            s = LookupServer(addr, loop=loop, **config.client_options)

            # get list of nodes on this lookupd server
            resp = s.nodes()
            for node in resp['nodes']:
                if node not in all_nodes:
                    all_nodes.append(node)

        print("%s node(s) found\n" % len(all_nodes))

        balance = copy.copy(GAFFER_COLORS)
        for node in all_nodes:
            line = "%s - %s (protocol: %s)" % (node['origin'], node['name'],
                    node['version'])

            color, balance = self.get_color(balance)
            print(colored(color, line))

    def get_color(self, balance):
        code = balance.pop(0)
        balance.append(code)
        return code, balance


########NEW FILE########
__FILENAME__ = lookup_session
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...lookupd.client import LookupServer
from .base import Command

import pyuv

class LookupSession(Command):
    """
    usage: gaffer lookup:session <sid> (-L ADDR|--lookupd-address=ADDR)...

      <sid>  a session id (or application name)

      -h, --help
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup:session"
    short_descr = "list all jobs for a session in lookupd servers"


    def run(self, config, args):
        lookupd_addresses = set(args['--lookupd-address'])

        loop = pyuv.Loop.default_loop()
        all_jobs = {}
        for addr in lookupd_addresses:
            # connect to the lookupd server channel
            s = LookupServer(addr, loop=loop, **config.client_options)
            resp = s.find_session(args['<sid>'])

            for job in resp['jobs']:
                job_name = job['name']
                if job_name in all_jobs:
                    all_jobs[job_name] = list(
                            set(all_jobs[job_name] + job['sources'])
                    )
                else:
                    all_jobs[job_name] = job['sources']
        loop.run()
        print("%s job(s) found\n" % len(all_jobs))
        for job_name, sources in all_jobs.items():
            lines = ["=== %s" % job_name]

            for source in sources:
                name = source['node_info']['name']
                origin = source['node_info']['origin']
                version = source['node_info']['version']

                lines.append("%s - name: %s, protocol: %s" % (origin, name,
                    version))
            lines.append("")
            print("\n".join(lines))

########NEW FILE########
__FILENAME__ = lookup_sessions
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...lookupd.client import LookupServer
from .base import Command

import pyuv

class LookupSessions(Command):
    """
    usage: gaffer lookup:sessions [<nodeid>] (-L ADDR|--lookupd-address=ADDR)...

      <node>  a gafferd id

      -h, --help
      -L ADDR --lookupd-address=ADDR  lookupd HTTP address
    """

    name = "lookup:sessions"
    short_descr = "list all sessions in lookupd servers"


    def run(self, config, args):
        lookupd_addresses = set(args['--lookupd-address'])

        sessions = {}
        loop = pyuv.Loop.default_loop()
        for addr in lookupd_addresses:
            s = LookupServer(addr, loop=loop, **config.client_options)
            if args['<nodeid>']:
                resp = s.sessions(args['<nodeid>'])
            else:
                resp = s.sessions()

            for session in resp['sessions']:
                sid = session['sessionid']
                if sid not in sessions:
                     sessions[sid] = session["jobs"]
                else:
                    current_jobs = sessions[sid]
                    new_jobs = session["jobs"]

                    for job_name, new in new_jobs.items():
                        if job_name in current_jobs:
                            current = current_jobs[job_name]
                            sessions[sid][job_name] = list(set(current + new))
        loop.run()

        print("%s session(s) found\n" % len(sessions))
        for sid, jobs in sessions.items():
            lines = ["*** session: %s" % sid, ""]
            for job_name, sources in jobs.items():
                lines.extend(["=== job: %s" % job_name])
                for source in sources:
                    version = source['node_info']['version']
                    name = source['node_info']['name']
                    origin = source['node_info']['origin']

                    lines.append("%s - name: %s, protocol: %s" % (origin,
                        name, version))
                lines.append("")
            print("\n".join(lines))

########NEW FILE########
__FILENAME__ = ps
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy

from .base import Command
from ...console_output import colored, GAFFER_COLORS

class Ps(Command):
    """
    usage: gaffer ps [--app APP]


      -h, --help
      --app APP  application name
    """

    name = "ps"
    short_descr = "list your processes informations"

    def run(self, config, args):
        balance = copy.copy(GAFFER_COLORS)
        appname = self.default_appname(config, args)
        server = config.get("server")

        if not appname in server.sessions():
            raise RuntimeError("%r not loaded" % appname)

        for pname in server.jobs(appname):
            lines = []
            try:
                job = server.get_job(pname)
            except:
                # we just ignore
                continue

            if not job.active:
                continue

            appname, name = self.parse_name(pname)

            color, balance = self.get_color(balance)
            stats = job.stats()

            # recreate cmd line
            args = job.config.get('args') or []
            cmd = " ".join([job.config['cmd']] + args)
            lines = ["=== %s: `%s`" % (name, cmd),
                     "Total CPU: %.2f Total MEM: %.2f" % (stats['cpu'],
                         stats['mem']),
                     ""]

            for info in stats['stats']:
                lines.append("%s.%s: up for %s" % (name, info['pid'],
                    info['ctime']))

            print(colored(color, '\n'.join(lines)))

    def get_color(self, balance):
        code = balance.pop(0)
        balance.append(code)
        return code, balance

########NEW FILE########
__FILENAME__ = recv
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import sys

import pyuv

from .base import Command
from ...httpclient import GafferNotFound

from .send import SigHandler

class Recv(Command):

    """
    usage: gaffer recv <pid> [<stream>][--app APP]

      <pid>     process ID or can be in the form of <pid.stream>
      <stream>  name of the stream to receive from

      --app APP     name of the procfile application.
    """

    name = "recv"
    short_descr = "read a stream from a pid"

    def run(self, config, args):
        server  = config.get("server")

        # get the stream and pid from the command line
        stream = args["<stream>"]
        if "." in args['<pid>']:
            pid, stream = args['<pid>'].split(".", 1)
        else:
            pid = args['<pid>']


        if not pid.isdigit():
            raise RuntimeError("invalid <pid> value")

        # open the process
        try:
            p = server.get_process(int(pid))
        except GafferNotFound:
            print("process %r not found" % pid)
            return

        # test if the stream exist before sending anything to the server
        if stream is not None:
            if (stream not in p.redirect_output and
                    stream not in p.custom_streams):
                raise RuntimeError("can't read on %r" % stream)

        self.socket = p.socket(mode=pyuv.UV_READABLE, stream=stream)
        self.socket.start()
        self.socket.start_read(self._on_read)

        self.sig_handler = SigHandler(self)
        self.sig_handler.start(server.loop)

        # then listen
        server.loop.run()

    def stop(self):
        self.socket.close()


    def _on_read(self, channel, data):
        sys.stdout.write(data.decode('utf-8'))
        sys.stdout.flush()

########NEW FILE########
__FILENAME__ = reload
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import sys

from .base import Command
from ...httpclient import GafferNotFound

class Reload(Command):
    """
    usage: gaffer reload [<jobs>]... [--app APP] [--from-file JSON]
                         [--no-input]

      --app APP         name of the procfile application.
      --no-input        don't prompt a confirmation
      --from-file JSON  unload from a JSON config file
    """

    name = "reload"
    short_descr = "reload a job from a gafferd node"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get('server', 'procfile')

        if not args['<jobs>'] and not args['--from-file']:
            # unload from a procfile
            if not args["--no-input"]:
                if not self.confirm("Do you want to reload %r?" % appname):
                    return
            apps = server.sessions()
            if appname not in apps:
                raise RuntimeError("%r not found" % appname)

            # unload the complete app
            server.jobs_walk(lambda s, job: self._reload(s, job, appname))
            print("==> app %r reloaded" % appname)
        elif args['--from-file']:
            # unload from a JSON config file
            fname = args['--from-file']

            # load configs
            configs = self.load_jsonconfig(fname)

            # finally unload all jobs from the give config
            for conf in configs:
                try:
                    job_name = conf.pop('name')
                except KeyError:
                    raise ValueError("invalid job config")

                # parse job name and eventually extract the appname
                appname, name = self.parse_name(job_name,
                        self.default_appname(config, args))
                # always force the appname if specified
                if args['--app']:
                    appname = args['--app']

                # unload the job
                pname = "%s.%s" % (appname, name)
                if not args["--no-input"]:
                    if not self.confirm("Do you want to reload %r?" %
                            pname):
                        continue

                try:
                    server.reload(name, appname)
                    print("job %r reloaded" % pname)
                except GafferNotFound:
                    sys.stderr.write("%r not found in %r\n" % (name,
                        appname))
                    sys.stderr.flush()
        else:
            # unload all jobs given on the command line. it can be either a
            # job specified in the Procfile or any job in the gafferd node.
            for job_name in args['<jobs>']:
                appname, name = self.parse_name(job_name, appname)
                if (self.use_procfile(config, appname) and
                        name not in procfile.cfg):
                    print("Ignore %r" % name)
                    continue

                if not args["--no-input"]:
                    if not self.confirm("Do you want to reload %r?" %
                            job_name):
                        continue

                # unload the job
                try:
                    server.reload(name, appname)
                    print("job %r reloaded" % job_name)
                except GafferNotFound:
                    sys.stderr.write("%r not found in %r\n" % (job_name,
                        appname))
                    sys.stderr.flush()

    def _reload(self, server, job, appname):
        try:
            server.reload(job, appname)
            print("job %r reloaded" % job.name)
        except GafferNotFound:
            sys.stderr.write("%r not found in %r\n" % (job.name, appname))
            sys.stderr.flush()

########NEW FILE########
__FILENAME__ = run
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from .base import Command
from ...httpclient import GafferNotFound


class Run(Command):
    """
    usage: gaffer run <job> [--graceful-timeout=TIMEOUT]
                            [--app=appname] [-p k=v]...

      <job>  job name: the config to use to launch this process

      -h --help
      -p k=v                 an environnemnt variable to add to the process  environmenet
      --graceful-timeout=T  graceful time before we send a  SIGKILL to the
                            process (which definitely kill it).  This is a time
                            we let to a process to exit cleanly.
      --app=appname         name of the procfile application (or session).

    """

    name = "run"
    short_descr = "run one-off command using a job config"

    def run(self, config, args):
        appname, name = self.parse_name(args['<job>'],
                default=self.default_appname(config, args))
        pname = "%s.%s" % (appname, name)
        server, procfile = config.get("server", "procfile")

        graceful_timeout = args['--graceful-timeout']
        if graceful_timeout is not None:
            if not graceful_timeout.isdigit():
                raise RuntimeError('invalid graceful timeout value')
            graceful_timeout = int(graceful_timeout)

        env = config.env.copy()
        env.update(self.parse_env(args))

        try:
            job = server.get_job(pname)
            pid = job.commit(graceful_timeout=graceful_timeout, env=env)
            print("%r: new process %r launched" % (pname, pid))
        except GafferNotFound:
            print("%r not found" % pname)

    def parse_env(self, args):
        if not args['-p']:
            return {}

        env = {}
        for kvs in set(args['-p']):
            kv = kvs.split("=")
            if len(kv) == 2:
                key = kv[0].strip()
                val = kv[1].strip()
                env[key] = val
        return env

########NEW FILE########
__FILENAME__ = scale
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from .base import Command
from ...httpclient import GafferNotFound

class Scale(Command):
    """
    usage: gaffer ps:scale [--app=appname](<args>)...

      --app=appname  name of the procfile application.
    """

    name = "ps:scale"
    short_descr = "scaling your process"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get("server", "procfile")
        use_procfile = self.use_procfile(config, appname)
        ops = self.parse_scaling(args["<args>"])

        for name, op, val in ops:
            appname, name = self.parse_name(name, default=appname)
            pname = "%s.%s" % (appname, name)

            # if we are using a procfile check first if the app contains this
            # job. We don't need to make an HTTP request in this case.
            if use_procfile and name not in procfile.cfg:
                continue

            try:
                job = server.get_job(pname)
                ret = job.scale("%s%s" % (op, val))
                print("Scaling %s processes... done, now running %s" %
                        (pname,ret))
            except GafferNotFound:
                print("%r not found" % pname)

    def parse_scaling(self, args):
        ops = []
        for arg in args:
            if "=" in arg:
                op = "="
            elif "+" in arg:
                op = "+"
            elif "-" in arg:
                op = "-"

            k,v = arg.split(op)
            try:
                v = int(v)
            except:
                continue

            ops.append((k, op, v))
        return ops

########NEW FILE########
__FILENAME__ = send
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import os
import sys

import pyuv

from .base import Command
from ...httpclient import GafferNotFound
from ...sig_handler import BaseSigHandler


class SigHandler(BaseSigHandler):

    def __init__(self, command):
        super(SigHandler, self).__init__()
        self.command = command

    def handle_quit(self, h, *args):
        self.stop()
        self.command.stop()

    def handle_reload(self, h, *args):
        return

class Send(Command):

    """
    usage: gaffer send [--app APP] [-s STREAM|--stream STREAM] <pid> <data>...

      <data>  file inpur or data input

      --app APP                  name of the procfile application or session.
      -s STREAM --stream STREAM  name of the stream where to send the data
    """

    name = "send"
    short_descr = "send some data to a pid"

    def run(self, config, args):
        server  = config.get("server")

        stream = args["--stream"]
        if stream == "stdin":
            stream = None

        if "." in args['<pid>']:
            pid, stream = args['<pid>'].split(".", 1)
        else:
            pid = args['<pid>']


        if not pid.isdigit():
            raise RuntimeError("invalid <pid> value")

        try:
            p = server.get_process(int(pid))
        except GafferNotFound:
            print("process %r not found" % pid)
            return

        # make sure we can send some data before sending it.
        if not stream:
            if not p.redirect_input:
                raise RuntimeError("can't write on this process, (no stdin)")
        elif stream not in p.custom_streams:
            raise RuntimeError("stream %r not found" % stream)

        # make pid & stream available in the instance
        self.pid = int(pid)
        self.stream = stream

        self.socket = p.socket(mode=pyuv.UV_WRITABLE, stream=stream)
        self.socket.start()

        data = " ".join(args["<data>"])
        self.args = args
        self.tty = None
        self.should_close = False

        if data.strip() == "-":
            self.tty = pyuv.TTY(server.loop, sys.stdin.fileno(), True)
            self.tty.start_read(self._on_tty_read)
            self.sig_handler = SigHandler(self)
            self.sig_handler.start(server.loop)
        else:
            data = data.strip()
            if not data.endswith('\n'):
                data = data + os.linesep

            self.socket.write(data, self._on_done)

        server.loop.run()

    def _on_done(self, channel, result, error):
        if error is not None:
            print("Error (%s): %s" % (error['errno'], error['reason']))
        self.stop()

    def _on_tty_done(self, channel, result, error):
        if error is not None:
            print("Error (%s): %s" % (error['errno'], error['reason']))
            self.stop()
        elif self.should_close:
            self.stop()

    def _on_tty_read(self, handle, data, error):
        if not data:
            self.should_close = True
            return

        data = data.strip()
        if data == b".":
            self.stop()
        else:
            data = data.decode('utf-8') + os.linesep
            self.socket.write(data, self._on_tty_done)

    def stop(self):
        if self.tty is not None and not self.tty.closed:
            self.tty.close()
            pyuv.TTY.reset_mode()
        self.socket.close()

########NEW FILE########
__FILENAME__ = start
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import sys

from .base import Command
from ...httpclient import GafferNotFound

class Start(Command):

    """
    usage: gaffer ps:start [<args>]... [--app=appname] [--no-input]

      <args>  job label

      --app=appname  name of the procfile application.
      --no-input        don't prompt a confirmation
    """

    name = "ps:start"
    short_descr = "start a job"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get("server", "procfile")

        if not args['<args>']:
            if (not args["--no-input"] and
                    self.confirm("Do you want to start all jobs in %r" %
                        appname)):

                apps = server.sessions()
                if appname not in apps:
                    raise RuntimeError("%r not found\n" % appname)

                #  stop all the jobs the complete app
                server.jobs_walk(lambda s, job: self._start(s, job))
                print("==> all jobs in %r started" % appname)
        else:
            for name in args['<args>']:
                # confirm that we can stop this job or pid
                if (not args["--no-input"] and
                    not self.confirm("Do you want to start %r" % name)):
                    continue

                # we want to stop a job
                appname, job_name = self.parse_name(name, appname)
                if (self.use_procfile(config, appname) and
                        job_name not in procfile.cfg):
                    print("Ignore %r" % name)
                    continue

                pname = "%s.%s" % (appname, job_name)
                try:
                    job = server.get_job(pname)
                except GafferNotFound:
                    sys.stderr.write("%r not found\n" % name)
                    sys.stderr.flush()
                    continue
                job.start()
                print("%r started" % name)

    def _start(self, server, job):
        try:
            job.start()
            print("job %r started" % job.name)
        except GafferNotFound:
            sys.stderr.write("%r not found\n" % job.name)
            sys.stderr.flush()

########NEW FILE########
__FILENAME__ = stop
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import sys

from .base import Command
from ...httpclient import GafferNotFound

class Stop(Command):

    """
    usage: gaffer ps:stop [<args>]... [--app=appname] [--no-input]

      <args>  pid or job label

      --app=appname  name of the procfile application.
      --no-input        don't prompt a confirmation
    """

    name = "ps:stop"
    short_descr = "stop a process or pid"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get("server", "procfile")

        if not args['<args>']:
            if (not args["--no-input"] and
                    self.confirm("Do you want to stop all jobs in %r" %
                        appname)):

                apps = server.sessions()
                if appname not in apps:
                    raise RuntimeError("%r not found\n" % appname)

                #  stop all the jobs the complete app
                server.jobs_walk(lambda s, job: self._stop(s, job))
                print("==> all jobs in %r stopped" % appname)
        else:
            for name in args['<args>']:
                # confirm that we can stop this job or pid
                if (not args["--no-input"] and
                    not self.confirm("Do you want to stop %r" % name)):
                    continue

                if name.isdigit():
                    # we want to stop a process from a job
                    try:
                        process = server.get_process(int(name))
                    except GafferNotFound:
                        sys.stderr.write("%r not found\n" % name)
                        sys.stderr.flush()
                        continue
                    process.stop()
                else:
                    # we want to stop a job
                    appname, job_name = self.parse_name(name, appname)
                    if (self.use_procfile(config, appname) and
                            job_name not in procfile.cfg):
                        print("Ignore %r" % name)
                        continue

                    pname = "%s.%s" % (appname, job_name)
                    try:
                        job = server.get_job(pname)
                    except GafferNotFound:
                        sys.stderr.write("%r not found\n" % name)
                        sys.stderr.flush()
                        continue
                    job.stop()

                print("%r stopped" % name)

    def _stop(self, server, job):
        try:
            job.stop()
            print("job %r stopped" % job.name)
        except GafferNotFound:
            pass

########NEW FILE########
__FILENAME__ = unload
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import sys

from .base import Command
from ...httpclient import GafferNotFound

class UnLoad(Command):
    """
    usage: gaffer unload [<jobs>]... [--app APP] [--from-file JSON]
                         [--no-input]

      --app APP         name of the procfile application.
      --no-input        don't prompt a confirmation
      --from-file JSON  unload from a JSON config file
    """

    name = "unload"
    short_descr = "unload a job from a gafferd node"

    def run(self, config, args):
        appname = self.default_appname(config, args)
        server, procfile = config.get('server', 'procfile')

        if not args['<jobs>'] and not args['--from-file']:
            # unload from a procfile
            if not args["--no-input"]:
                if not self.confirm("Do you want to unload %r?" % appname):
                    return
            apps = server.sessions()
            if appname not in apps:
                raise RuntimeError("%r not found" % appname)

            # unload the complete app
            server.jobs_walk(lambda s, job: self._unload(server, job, appname))
            print("==> app %r unloaded" % appname)
        elif args['--from-file']:
            # unload from a JSON config file
            fname = args['--from-file']

            # load configs
            configs = self.load_jsonconfig(fname)

            # finally unload all jobs from the give config
            for conf in configs:
                try:
                    job_name = conf.pop('name')
                except KeyError:
                    raise ValueError("invalid job config")

                # parse job name and eventually extract the appname
                appname, name = self.parse_name(job_name,
                        self.default_appname(config, args))
                # always force the appname if specified
                if args['--app']:
                    appname = args['--app']

                # unload the job
                pname = "%s.%s" % (appname, name)
                if not args["--no-input"]:
                    if not self.confirm("Do you want to unload %r?" %
                            pname):
                        continue

                try:
                    server.unload(name, appname)
                    print("job %r unloaded" % pname)
                except GafferNotFound:
                    sys.stderr.write("%r not found in %r\n" % (name,
                        appname))
                    sys.stderr.flush()
        else:
            # unload all jobs given on the command line. it can be either a
            # job specified in the Procfile or any job in the gafferd node.
            for job_name in args['<jobs>']:
                appname, name = self.parse_name(job_name, appname)
                if (self.use_procfile(config, appname) and
                        name not in procfile.cfg):
                    print("Ignore %r" % name)
                    continue

                if not args["--no-input"]:
                    if not self.confirm("Do you want to unload %r?" %
                            job_name):
                        continue

                # unload the job
                try:
                    server.unload(name, appname)
                    print("job %r unloaded" % job_name)
                except GafferNotFound:
                    sys.stderr.write("%r not found in %r\n" % (job_name,
                        appname))
                    sys.stderr.flush()

    def _unload(self, server, job, appname):
        try:
            server.unload(job, appname)
        except GafferNotFound:
            sys.stderr.write("%r not found in %r\n" % (job.name, appname))
            sys.stderr.flush()

########NEW FILE########
__FILENAME__ = user
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...httpclient.base import GafferNotFound
from ...httpclient.users import Users
from .base import Command

class User(Command):
    """
    usage: gaffer user <username>

      <username>  name of the user
    """

    name = "user"
    short_descr = "Fetch the user infos"

    def run(self, config, args):
        server = config.get('server')

        users = Users(server)
        try:
            user = users.get_user(args['<username>'])
        except GafferNotFound:
            raise RuntimeError("User %r not found" % args['<username>'])

        print("User %r found" % args['<username>'])
        print("API Key: %s" % user['key'])

########NEW FILE########
__FILENAME__ = user_add
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
from getpass import getpass

try:
    input = raw_input
except NameError:
    pass

from ...gafferd.util import confirm
from ...httpclient.base import GafferConflict
from ...httpclient.users import Users
from .base import Command

class UserAdd(Command):
    """
    usage: gaffer user:add [<username>] [-p PASSWORD] [-k KEY] [-t TYPE]

      <username>  name of the user

      -p PASSWORD  the user password
      -k KEY       the user API key
      -t TYPE      the user type [default 1].
    """

    name = "user:add"
    short_descr = "add a user"

    def run(self, config, args):
        server = config.get('server')

        username = args['<username>']
        password = args['-p']
        api_key = args['-k']
        user_type = args["-t"]

        try:
            if username is None:
                while True:
                    username = input("username: ")
                    if username and username is not None:
                        break

                    print("username is empty. Please enter a username.")

            if password is None:
                while True:
                    password = getpass("password: ")
                    if password and password is not None:
                        break
                    print("password is empty. Please enter a password.")

                # confirm password

                while True:
                    confirm_password = getpass("confirm password: ")
                    if confirm_password == password:
                        break

                    print("Passwords are different.")

            if api_key is None:
                api_key = input("api key: ")
                if not api_key:
                    api_key = None

            if user_type is None or not user_type.isdigit():

                while True:
                    user_type = input("type [1]: ")
                    if user_type.isdigit() or not user_type:
                        break
                    print("invalid user type")


        except KeyboardInterrupt:
            return

        if not user_type:
            user_type = "1"

        print("")
        print("Creating the user %r with the following infos:" % username)
        print("API Key: %s" % api_key)
        print("Type: %s" % user_type)
        print("")

        if not confirm("Confirm the creation of the user %r" % username):
            return

        users = Users(server)
        try:
            user = users.create_user(username, password,
                    user_type=int(user_type), key=api_key)
        except GafferConflict:
            raise RuntimeError("User %r alerady exists" % username)


        print("User %r created" % username)

########NEW FILE########
__FILENAME__ = user_del
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from ...gafferd.util import confirm
from ...httpclient.base import GafferNotFound
from ...httpclient.users import Users
from .base import Command

class UserDelete(Command):
    """
    usage: gaffer user:del <username>

      <username>  name of the user
    """

    name = "user:del"
    short_descr = "delete a user"

    def run(self, config, args):
        server = config.get('server')

        if not confirm("Confirm deletion of the user %r" % args['<username>']):
            return

        users = Users(server)
        try:
            user = users.delete_user(args['<username>'])
        except GafferNotFound:
            raise RuntimeError("User %r not found" % args['<username>'])

        print("User %r deleted" % args['<username>'])

########NEW FILE########
__FILENAME__ = user_setkey
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from getpass import getpass

try:
    input = raw_input
except NameError:
    pass

from ...gafferd.util import confirm
from ...httpclient.base import GafferNotFound
from ...httpclient.users import Users
from .base import Command

class UserSetPassword(Command):
    """
    usage: gaffer user:setkey <username> [-k KEY]

      <username>  name of the user

      -k KEY  the user key
    """

    name = "user:setkey"
    short_descr = "change the key of a user"

    def run(self, config, args):
        server = config.get('server')

        key = args['-k']
        try:
            if key is None:
                key = input("api key: ")
        except KeyboardInterrupt:
            return

        if not confirm("Confirm the change %s's key" % args['<username>']):
            return

        users = Users(server)
        try:
            user = users.set_key(args['<username>'], key)
        except GafferNotFound:
            raise RuntimeError("User %r not found" % args['<username>'])

        print("%s's key changed" % args['<username>'])

########NEW FILE########
__FILENAME__ = user_setpwd
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from getpass import getpass

from ...gafferd.util import confirm
from ...httpclient.base import GafferNotFound
from ...httpclient.users import Users
from .base import Command

class UserSetPassword(Command):
    """
    usage: gaffer user:setpassword <username> [-p PASSWORD]

      <username>  name of the user

      -p PASSWORD  the user password
    """

    name = "user:setpassword"
    short_descr = "change the password of a user"

    def run(self, config, args):
        server = config.get('server')

        password = args['-p']
        try:
            if password is None:
                while True:
                    password = getpass("password: ")
                    if password and password is not None:
                        break
                    print("password is empty. Please enter a password.")

                # confirm password

                while True:
                    confirm_password = getpass("confirm password: ")
                    if confirm_password == password:
                        break
        except KeyboardInterrupt:
            return


        if not confirm("Confirm the change %s's password" % args['<username>']):
            return

        users = Users(server)
        try:
            user = users.set_password(args['<username>'], password)
        except GafferNotFound:
            raise RuntimeError("User %r not found" % args['<username>'])

        print("%s's password changed" % args['<username>'])

########NEW FILE########
__FILENAME__ = main
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
usage: gaffer [--version] [-f procfile|--procfile procfile]
              [-d root|--root root] [-e path|--env path]...
              [--certfile=CERTFILE] [--keyfile=KEYFILE] [--cacert=CACERT]
              [-g url|--gafferd-http-address url]
              [--api-key API_KEY] <command> [<args>...]

Options

    -h --help                           show this help message and exit
    --version                           show version and exit
    -f procfile,--procfile procfile     Specify an alternate Procfile to load
    -d root,--root root                 Specify an alternate application root
                                        This defaults to the  directory
                                        containing the Procfile [default: .]
    -e path,--env path                  Specify one or more .env files to load
    -g url --gafferd-http-address url   gafferd node HTTP address to connect
                                        [default: http://127.0.0.1:5000]
    --api-key API_KEY                   API Key to access to gaffer
    --certfile=CERTFILE                 SSL certificate file
    --keyfile=KEYFILE                   SSL key file
    --cacert=CACERT                     SSL CA certificate

"""
try:
    import configparser
except ImportError:
    import ConfigParser as configparser
import os
import sys

import tornado

from .. import __version__
from ..docopt import docopt, printable_usage
from ..gafferd.util import user_path, default_user_path
from ..httpclient import Server, GafferUnauthorized, GafferForbidden
from ..procfile import Procfile, get_env
from ..util import is_ssl

from .commands import get_commands


class Config(object):

    def __init__(self, args):
        self.args = args

        if self.args["--env"]:
            self.envs = list(set(self.args["--env"]))
        else:
            self.envs = ['.env']

        # initialize the env object
        self.env = get_env(self.envs)

        # initialize the procfile
        self.procfile = self._init_procfile()

        # get user config path
        self.user_config_path = self.get_user_config()

         # load config
        self.user_config = configparser.RawConfigParser()
        if os.path.isfile(self.user_config_path):
            with open(self.user_config_path) as f:
                self.user_config.readfp(f)

        # node config section
        node_section = "node \"%s\"" % args["--gafferd-http-address"]
        if not self.user_config.has_section(node_section):
            self.user_config.add_section(node_section)

        # parse ssl options
        self.parse_ssl_options(node_section)

        # setup default server
        api_key = self.args['--api-key']
        if (api_key is None and
                self.user_config.has_option(node_section, "key")):
            api_key = self.user_config.get(node_section, "key")

        self.server = Server(self.args["--gafferd-http-address"],
                api_key=api_key, **self.client_options)

    def get(self, *attrs):
        ret = []
        for name in attrs:
            ret.append(getattr(self, name))

        if len(ret) == 1:
            return ret[0]

        return tuple(ret)

    @property
    def use_procfile(self):
        return isinstance(self.procfile, Procfile)

    @property
    def gafferd_address(self):
        return self.args["--gafferd-http-address"]

    def _init_procfile(self):
        procfile = "Procfile"

        if self.args['--procfile']:
            procfile = self.args['--procfile']

        if self.args['--root']:
            root = self.args['--root']
        else:
            root = os.path.dirname(procfile) or "."

        if not os.path.isfile(procfile):
            if self.args['--procfile'] is not None:
                raise RuntimeError("procfile %r not found" % procfile)
            else:
                return None
        else:
            return Procfile(procfile, root=root, envs=self.envs)

    def parse_ssl_options(self, node_section):
        self.client_options = {}
        # get client options from the config file
        if self.user_config.has_option(node_section, "certfile"):
            self.client_options['client_cert'] = self.user_config.get(
                    node_section, "cerfile")

        if self.user_config.has_option(node_section, "keyfile"):
            self.client_options['client_key'] = self.user_config.get(
                    node_section, "keyfile")

        if self.user_config.has_option(node_section, "cacert"):
            self.client_options['ca_certs'] = self.user_config.get(
                    node_section, "cacert")

        # override the ssl options from the command line

        if self.args["--certfile"] is not None:
            self.client_options["client_cer"] = self.args["--certfile"]

        if self.args["--keyfile"] is not None:
            self.client_options["client_key"] = self.args["--keyfile"]

        if self.args["--cacert"] is not None:
            self.client_options = {"ca_certs": self.args["--cacert"]}

    def get_user_config(self):
        if 'GAFFER_CONFIG' in os.environ:
            return os.environ.get('GAFFER_CONFIG')

        for path in user_path():
            config_file = os.path.join(path, "gaffer.ini")
            if os.path.isfile(config_file):
                return config_file

        config_dir = default_user_path()
        if not os.path.isdir(config_dir):
            os.makedirs(config_dir)

        return os.path.join(config_dir, "gaffer.ini")

class GafferCli(object):

    def __init__(self, argv=None):
        self.commands = get_commands()

        version_str = "gaffer version %s" % __version__
        self.doc_str = "%s%s" % (__doc__, self._commands_help())
        self.args = docopt(self.doc_str, argv=argv,  version=version_str,
                options_first=True)

    def run(self):
        if self.args['<command>'].lower() == "help":
            self.display_help()
        else:
            cmdname = self.args['<command>']
            if cmdname not in self.commands:
                print("Unknown command: %r" % cmdname)
                self.display_help()
                sys.exit(1)

            # parse command arguments
            cmd = self.commands[cmdname]
            cmd_argv = [cmdname] + self.args['<args>']
            cmd_args =  docopt(cmd.__doc__, argv=cmd_argv)

            try:
                config = Config(self.args)
            except RuntimeError as e:
                sys.stderr.write("config error: %s" % str(e))
                sys.exit(1)

            # finally launch the command
            cmd = self.commands[cmdname]
            try:
                return cmd.run(config, cmd_args)
            except GafferUnauthorized:
                print("Unauthorized access. You need an API key")
                sys.exit(1)
            except GafferForbidden:
                print("Forbidden access. API key permissions aren't enough")
                sys.exit(1)
            except tornado.httpclient.HTTPError as e:
                print("HTTP Error: %s\n" % str(e))
                sys.exit(1)

            except RuntimeError as e:
                sys.stderr.write("%s\n" % str(e))
                sys.exit(1)
            except Exception as e:
                import traceback
                print(traceback.format_exc())
                sys.stderr.write("%s\n" % str(e))
                sys.exit(1)
        sys.exit(0)

    def display_help(self):
        if self.args['<args>']:
            name = self.args['<args>'][0]
            if name in self.commands:
                cmd = self.commands[name]
                print(printable_usage(self.commands[name].__doc__))
                return

        print(self.doc_str)

    def _commands_help(self):
        commands = [name for name in self.commands] + ["help"]
        max_len = len(max(commands, key=len))
        output = ["Commands",
                  " "]
        for name in commands:
            if name == "help":
                desc = "Get help on a command"
                output.append("    %-*s\t%s" % (max_len, name, desc))
            else:
                cmd = self.commands[name]
                # Command name is max_len characters.
                # Used by the %-*s formatting code
                output.append("    %-*s\t%s" % (max_len, name, cmd.short_descr))
        output.append("")
        return '\n'.join(output)


def main():
    cli = GafferCli()
    cli.run()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = console_output
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
module to return all streams from the managed processes to the
console. This application is subscribing to the manager to know when a
process is created or killed and display the information. When an OS process
is spawned it then subscribe to its streams if any are redirected and
print the output on the console. This module is used by
:doc:`gaffer` .


.. note::

    if colorize is set to true, each templates will have a different
    colour
"""

import copy
from datetime import datetime
import sys

from colorama import Fore, Back, Style, init
init()

from .error import ProcessNotFound

GAFFER_COLORS = ['cyan', 'yellow', 'green', 'magenta', 'red', 'blue',
'intense_cyan', 'intense_yellow', 'intense_green', 'intense_magenta',
'intense_red', 'intense_blue']


class Color(object):
    """ wrapper around colorama to ease the output creation. Don't use
    it directly, instead, use the ``colored(name_of_color, lines)`` to
    return the colored ouput.

    Colors are: cyan, yellow, green, magenta, red, blue,
    intense_cyan, intense_yellow, intense_green, intense_magenta,
    intense_red, intense_blue.

    lines can be a list or a string.
    """

    def __init__(self):
        # intialize colors code
        colors = {}
        for color in GAFFER_COLORS:
            c = color.upper()
            if color.startswith('intense_'):
                code = getattr(Fore, c.split('_')[1])
                colors[color] = code + Style.BRIGHT
            else:
                code = getattr(Fore, c)
                colors[color] = code
        self.colors = colors

    def output(self, color_name, lines):
        if not isinstance(lines, list):
            lines = [lines]

        lines.insert(0, self.colors[color_name])
        lines.append(Fore.RESET)
        return ''.join(lines)

_color = Color()
colored = _color.output

def status_bar(s):
    print(''.join([Style.BRIGHT, Fore.WHITE, Back.BLACK, s,
        Style.RESET_ALL]))


class ConsoleOutput(object):
    """ The application that need to be added to the gaffer manager """

    DEFAULT_ACTIONS = ['start', 'stop', 'spawn', 'reap', 'exit', 'stop_pid']

    def __init__(self, colorize=True, output_streams=True, actions=None):
        self.output_streams = output_streams
        self.colorize = colorize

        self.subscribed = actions or self.DEFAULT_ACTIONS

        self._balance = copy.copy(GAFFER_COLORS)
        self._process_colors = {}

    def start(self, loop, manager):
        self.loop = loop
        self.manager = manager

        for action in self.subscribed:
            self.manager.events.subscribe(action, self._on_process)

    def stop(self):
        for action in self.subscribed:
            self.manager.events.unsubscribe(action, self._on_process)

    def restart(self):
        self.stop()
        for action in self.subscribed:
            self.manager.events.subscribe(action, self._on_process)

    def _on_process(self, event, msg):
        name = msg['name']

        if not 'os_pid' in msg:
            line = self._print(name, '%s %s' % (event, name))
        else:
            pid = msg['pid']
            if event == "spawn":

                line = self._print(name, 'spawn process with pid %s' % pid)


                try:
                    p = self.manager.get_process(pid)
                    if p.redirect_output and self.output_streams:
                        for output in p.redirect_output:
                            p.monitor_io(output, self._on_output)
                except ProcessNotFound:
                    pass
            else:
                line = self._print(name,
                        '%s process with pid %s' % (event, pid))
        self._write(name, line)

    def _on_output(self, event, msg):
        data =  msg['data'].decode('utf-8')
        name = msg['name']

        lines = []
        for line in data.splitlines():
            line = line.strip()
            if line:
                lines.append(self._print(name, line))
        self._write(name, lines)

    def _write(self, name, lines):
        if self.colorize:
            sys.stdout.write(colored(self._get_process_color(name), lines))
        else:
            if not isinstance(lines, list):
                lines = [lines]
            sys.stdout.write("".join(lines))
        sys.stdout.flush()

    def _print(self, name, line):
        now = datetime.now().strftime('%H:%M:%S')
        prefix = '{time} {name} | '.format(time=now, name=name)
        return ''.join([prefix, line, '\n'])

    def _set_process_color(self, name):
        code = self._balance.pop(0)
        self._process_colors[name] = code
        self._balance.append(code)

    def _get_process_color(self, name):
        if name not in self._process_colors:
            self._set_process_color(name)
        return self._process_colors[name]

########NEW FILE########
__FILENAME__ = controller
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from .error import ProcessError, CommandError
from .process import ProcessConfig

COMMANDS_TABLE= {
        # generic commands
        "sessions": "sessions",
        "jobs": "jobs",
        "pids": "pids",
        # job config management commands
        "load": "load",
        "unload": "unload",
        "reload": "reload",
        "update": "update",
        # job commands
        "start_job": "start_job",
        "stop_job": "stop_job",
        "scale": "scale",
        "info": "info",
        "stats": "stats",
        "stopall": "stopall",
        "killall": "killall",
        "commit": "commit",
        # process commands
        "process_info": "process_info",
        "process_stats": "process_stats",
        "stop_process": "stop_process",
        "send": "send",
        "kill": "kill"}


class Command(object):

    def __init__(self, name, args=None, kwargs=None):
        self.name = name
        self.args = args or ()
        self.kwargs = kwargs or {}

    def reply(self, result):
        raise NotImplementedError

    def reply_error(self, error):
        raise NotImplementedError

class Controller(object):
    """ a controller is class that allows a client to pass commands to the
    manager asynchronously. It should be the main object used by plugins and
    is used to pass command using a websocket.

    The main method to call is `process_command` it accept a `Command`
    instance. This instance will be passed to accepted commands by the
    controller. Arguments and Named arguments of the manager are passed via
    the command instance properties """

    def __init__(self, manager):
        self.manager = manager

    def process_command(self, cmd):
        if cmd.name not in COMMANDS_TABLE:
            cmd.reply_error({"errno": 404, "reason": "command_not_found"})
            return

        try:
            fun = getattr(self, COMMANDS_TABLE[cmd.name])
            fun(cmd)
        except ProcessError as pe:
            cmd.reply_error({"errno": pe.errno, "reason": pe.reason})
        except CommandError as ce:
            cmd.reply_error({"errno": ce.errno, "reason": ce.reason})
        except Exception as e:
            cmd.reply_error({"errno": 500, "reason": str(e)})

    def sessions(self, cmd):
        cmd.reply({"sessions": self.manager.sessions})

    def jobs(self, cmd):
        if not cmd.args:
            cmd.reply({"jobs": self.manager.jobs()})
        else:
            sessionid = cmd.args[0]
            jobs = self.manager.jobs(sessionid)
            cmd.reply({"sessionid": sessionid, "jobs": jobs})

    def pids(self, cmd):
        if not cmd.args:
            cmd.reply({"pids": self.manager.pids()})
        else:
            sessionid = cmd.args[0]
            pids = self.manager.pids(sessionid)
            cmd.reply({"sessionid": sessionid, "pids": pids})

    def load(self, cmd):
        if not cmd.args:
            raise CommandError("config_missing")

        config = cmd.args[0]
        if not isinstance(config, dict):
            raise CommandError("invalid_config")

        pconfig = ProcessConfig.from_dict(config)
        self.manager.load(pconfig, **cmd.kwargs)
        cmd.reply({"ok": True})

    def unload(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.unload(cmd.args[0], **cmd.kwargs)
        cmd.reply({"ok": True})


    def reload(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.reload(cmd.args[0], **cmd.kwargs)
        cmd.reply({"ok": True})


    def update(self, cmd):
        if not cmd.args:
            raise CommandError("config_missing")

        config = cmd.args[0]
        if not isinstance(config, dict):
            raise CommandError("invalid_config")

        pconfig = ProcessConfig.from_dict(config)
        self.manager.update(pconfig, **cmd.kwargs)
        cmd.reply({"ok": True})

    def start_job(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.start_job(cmd.args[0])
        cmd.reply({"ok": True})

    def stop_job(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.stop_job(cmd.args[0])
        cmd.reply({"ok": True})

    def commit(self, cmd):
        if not cmd.args:
            raise CommandError()

        pid = self.manager.commit(cmd.args[0], *cmd.args[1:])
        cmd.reply({"ok": True, "pid": pid})


    def scale(self, cmd):
        if len(cmd.args) < 2:
            raise CommandError()

        numprocesses = self.manager.scale(cmd.args[0], cmd.args[1])
        cmd.reply({"numprocesses": numprocesses})

    def info(self, cmd):
        if not cmd.args:
            raise CommandError()

        cmd.reply({"info": self.manager.info(cmd.args[0])})

    def stats(self, cmd):
        if not cmd.args:
            raise CommandError()

        cmd.reply({"stats": self.manager.stats(cmd.args[0])})

    def stopall(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.stopall(cmd.args[0])
        cmd.reply({"ok": True})

    def killall(self, cmd):
        if len(cmd.args) < 2:
            raise CommandError()

        self.manager.killall(cmd.args[0], cmd.args[1])
        cmd.reply({"ok": True})

    def process_info(self, cmd):
        if not cmd.args:
            raise CommandError()

        process = self.manager.get_process(cmd.args[0])
        cmd.reply({"info": process.info})

    def process_stats(self, cmd):
        if not cmd.args:
            raise CommandError()

        process = self.manager.get_process(cmd.args[0])
        cmd.reply({"stats": process.stats})

    def stop_process(self, cmd):
        if not cmd.args:
            raise CommandError()

        self.manager.stop_process(cmd.args[0])
        cmd.reply({"ok": True})

    def kill(self, cmd):
        if len(cmd.args) < 2:
            raise CommandError()

        self.manager.kill(cmd.args[0], cmd.args[1])
        cmd.reply({"ok": True})

    def send(self, cmd):
        if len(cmd.args) < 2 or len(cmd.args) > 3:
            raise CommandError()

        self.manager.send(*cmd.args)
        cmd.reply({"ok": True})

########NEW FILE########
__FILENAME__ = docopt
"""Pythonic command-line interface parser that will make you smile.

 * http://docopt.org
 * Repository and issue-tracker: https://github.com/docopt/docopt
 * Licensed under terms of MIT license (see LICENSE-MIT)
 * Copyright (c) 2013 Vladimir Keleshev, vladimir@keleshev.com

"""
import re
import sys

__all__ = ['docopt']
__version__ = '0.6.1'


class DocoptLanguageError(Exception):

    """Error in construction of usage-message by developer."""


class DocoptExit(SystemExit):

    """Exit in case user invoked program with incorrect arguments."""

    usage = ''

    def __init__(self, message=''):
        SystemExit.__init__(self, (message + '\n' + self.usage).strip())


class Pattern(object):

    def __eq__(self, other):
        return repr(self) == repr(other)

    def __hash__(self):
        return hash(repr(self))

    def fix(self):
        self.fix_identities()
        self.fix_repeating_arguments()
        return self

    def fix_identities(self, uniq=None):
        """Make pattern-tree tips point to same object if they are equal."""
        if not hasattr(self, 'children'):
            return self
        uniq = list(set(self.flat())) if uniq is None else uniq
        for i, c in enumerate(self.children):
            if not hasattr(c, 'children'):
                assert c in uniq
                self.children[i] = uniq[uniq.index(c)]
            else:
                c.fix_identities(uniq)

    def fix_repeating_arguments(self):
        """Fix elements that should accumulate/increment values."""
        either = [list(c.children) for c in self.either.children]
        for case in either:
            for e in [c for c in case if case.count(c) > 1]:
                if type(e) is Argument or type(e) is Option and e.argcount:
                    if e.value is None:
                        e.value = []
                    elif type(e.value) is not list:
                        e.value = e.value.split()
                if type(e) is Command or type(e) is Option and e.argcount == 0:
                    e.value = 0
        return self

    @property
    def either(self):
        """Transform pattern into an equivalent, with only top-level Either."""
        # Currently the pattern will not be equivalent, but more "narrow",
        # although good enough to reason about list arguments.
        ret = []
        groups = [[self]]
        while groups:
            children = groups.pop(0)
            types = [type(c) for c in children]
            if Either in types:
                either = [c for c in children if type(c) is Either][0]
                children.pop(children.index(either))
                for c in either.children:
                    groups.append([c] + children)
            elif Required in types:
                required = [c for c in children if type(c) is Required][0]
                children.pop(children.index(required))
                groups.append(list(required.children) + children)
            elif Optional in types:
                optional = [c for c in children if type(c) is Optional][0]
                children.pop(children.index(optional))
                groups.append(list(optional.children) + children)
            elif AnyOptions in types:
                optional = [c for c in children if type(c) is AnyOptions][0]
                children.pop(children.index(optional))
                groups.append(list(optional.children) + children)
            elif OneOrMore in types:
                oneormore = [c for c in children if type(c) is OneOrMore][0]
                children.pop(children.index(oneormore))
                groups.append(list(oneormore.children) * 2 + children)
            else:
                ret.append(children)
        return Either(*[Required(*e) for e in ret])


class ChildPattern(Pattern):

    def __init__(self, name, value=None):
        self.name = name
        self.value = value

    def __repr__(self):
        return '%s(%r, %r)' % (self.__class__.__name__, self.name, self.value)

    def flat(self, *types):
        return [self] if not types or type(self) in types else []

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        pos, match = self.single_match(left)
        if match is None:
            return False, left, collected
        left_ = left[:pos] + left[pos + 1:]
        same_name = [a for a in collected if a.name == self.name]
        if type(self.value) in (int, list):
            if type(self.value) is int:
                increment = 1
            else:
                increment = ([match.value] if type(match.value) is str
                             else match.value)
            if not same_name:
                match.value = increment
                return True, left_, collected + [match]
            same_name[0].value += increment
            return True, left_, collected
        return True, left_, collected + [match]


class ParentPattern(Pattern):

    def __init__(self, *children):
        self.children = list(children)

    def __repr__(self):
        return '%s(%s)' % (self.__class__.__name__,
                           ', '.join(repr(a) for a in self.children))

    def flat(self, *types):
        if type(self) in types:
            return [self]
        return sum([c.flat(*types) for c in self.children], [])


class Argument(ChildPattern):

    def single_match(self, left):
        for n, p in enumerate(left):
            if type(p) is Argument:
                return n, Argument(self.name, p.value)
        return None, None

    @classmethod
    def parse(class_, source):
        name = re.findall('(<\S*?>)', source)[0]
        value = re.findall('\[default: (.*)\]', source, flags=re.I)
        return class_(name, value[0] if value else None)


class Command(Argument):

    def __init__(self, name, value=False):
        self.name = name
        self.value = value

    def single_match(self, left):
        for n, p in enumerate(left):
            if type(p) is Argument:
                if p.value == self.name:
                    return n, Command(self.name, True)
                else:
                    break
        return None, None


class Option(ChildPattern):

    def __init__(self, short=None, long=None, argcount=0, value=False):
        assert argcount in (0, 1)
        self.short, self.long = short, long
        self.argcount, self.value = argcount, value
        self.value = None if value is False and argcount else value

    @classmethod
    def parse(class_, option_description):
        short, long, argcount, value = None, None, 0, False
        options, _, description = option_description.strip().partition('  ')
        options = options.replace(',', ' ').replace('=', ' ')
        for s in options.split():
            if s.startswith('--'):
                long = s
            elif s.startswith('-'):
                short = s
            else:
                argcount = 1
        if argcount:
            matched = re.findall('\[default: (.*)\]', description, flags=re.I)
            value = matched[0] if matched else None
        return class_(short, long, argcount, value)

    def single_match(self, left):
        for n, p in enumerate(left):
            if self.name == p.name:
                return n, p
        return None, None

    @property
    def name(self):
        return self.long or self.short

    def __repr__(self):
        return 'Option(%r, %r, %r, %r)' % (self.short, self.long,
                                           self.argcount, self.value)


class Required(ParentPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        l = left
        c = collected
        for p in self.children:
            matched, l, c = p.match(l, c)
            if not matched:
                return False, left, collected
        return True, l, c


class Optional(ParentPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        for p in self.children:
            m, left, collected = p.match(left, collected)
        return True, left, collected


class AnyOptions(Optional):

    """Marker/placeholder for [options] shortcut."""


class OneOrMore(ParentPattern):

    def match(self, left, collected=None):
        assert len(self.children) == 1
        collected = [] if collected is None else collected
        l = left
        c = collected
        l_ = None
        matched = True
        times = 0
        while matched:
            # could it be that something didn't match but changed l or c?
            matched, l, c = self.children[0].match(l, c)
            times += 1 if matched else 0
            if l_ == l:
                break
            l_ = l
        if times >= 1:
            return True, l, c
        return False, left, collected


class Either(ParentPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        outcomes = []
        for p in self.children:
            matched, _, _ = outcome = p.match(left, collected)
            if matched:
                outcomes.append(outcome)
        if outcomes:
            return min(outcomes, key=lambda outcome: len(outcome[1]))
        return False, left, collected


class TokenStream(list):

    def __init__(self, source, error):
        self += source.split() if hasattr(source, 'split') else source
        self.error = error

    def move(self):
        return self.pop(0) if len(self) else None

    def current(self):
        return self[0] if len(self) else None


def parse_long(tokens, options):
    """long ::= '--' chars [ ( ' ' | '=' ) chars ] ;"""
    long, eq, value = tokens.move().partition('=')
    assert long.startswith('--')
    value = None if eq == value == '' else value
    similar = [o for o in options if o.long == long]
    if tokens.error is DocoptExit and similar == []:  # if no exact match
        similar = [o for o in options if o.long and o.long.startswith(long)]
    if len(similar) > 1:  # might be simply specified ambiguously 2+ times?
        raise tokens.error('%s is not a unique prefix: %s?' %
                           (long, ', '.join(o.long for o in similar)))
    elif len(similar) < 1:
        argcount = 1 if eq == '=' else 0
        o = Option(None, long, argcount)
        options.append(o)
        if tokens.error is DocoptExit:
            o = Option(None, long, argcount, value if argcount else True)
    else:
        o = Option(similar[0].short, similar[0].long,
                   similar[0].argcount, similar[0].value)
        if o.argcount == 0:
            if value is not None:
                raise tokens.error('%s must not have an argument' % o.long)
        else:
            if value is None:
                if tokens.current() is None:
                    raise tokens.error('%s requires argument' % o.long)
                value = tokens.move()
        if tokens.error is DocoptExit:
            o.value = value if value is not None else True
    return [o]


def parse_shorts(tokens, options):
    """shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;"""
    token = tokens.move()
    assert token.startswith('-') and not token.startswith('--')
    left = token.lstrip('-')
    parsed = []
    while left != '':
        short, left = '-' + left[0], left[1:]
        similar = [o for o in options if o.short == short]
        if len(similar) > 1:
            raise tokens.error('%s is specified ambiguously %d times' %
                               (short, len(similar)))
        elif len(similar) < 1:
            o = Option(short, None, 0)
            options.append(o)
            if tokens.error is DocoptExit:
                o = Option(short, None, 0, True)
        else:  # why copying is necessary here?
            o = Option(short, similar[0].long,
                       similar[0].argcount, similar[0].value)
            value = None
            if o.argcount != 0:
                if left == '':
                    if tokens.current() is None:
                        raise tokens.error('%s requires argument' % short)
                    value = tokens.move()
                else:
                    value = left
                    left = ''
            if tokens.error is DocoptExit:
                o.value = value if value is not None else True
        parsed.append(o)
    return parsed


def parse_pattern(source, options):
    tokens = TokenStream(re.sub(r'([\[\]\(\)\|]|\.\.\.)', r' \1 ', source),
                         DocoptLanguageError)
    result = parse_expr(tokens, options)
    if tokens.current() is not None:
        raise tokens.error('unexpected ending: %r' % ' '.join(tokens))
    return Required(*result)


def parse_expr(tokens, options):
    """expr ::= seq ( '|' seq )* ;"""
    seq = parse_seq(tokens, options)
    if tokens.current() != '|':
        return seq
    result = [Required(*seq)] if len(seq) > 1 else seq
    while tokens.current() == '|':
        tokens.move()
        seq = parse_seq(tokens, options)
        result += [Required(*seq)] if len(seq) > 1 else seq
    return [Either(*result)] if len(result) > 1 else result


def parse_seq(tokens, options):
    """seq ::= ( atom [ '...' ] )* ;"""
    result = []
    while tokens.current() not in [None, ']', ')', '|']:
        atom = parse_atom(tokens, options)
        if tokens.current() == '...':
            atom = [OneOrMore(*atom)]
            tokens.move()
        result += atom
    return result


def parse_atom(tokens, options):
    """atom ::= '(' expr ')' | '[' expr ']' | 'options'
             | long | shorts | argument | command ;
    """
    token = tokens.current()
    result = []
    if token in '([':
        tokens.move()
        matching, pattern = {'(': [')', Required], '[': [']', Optional]}[token]
        result = pattern(*parse_expr(tokens, options))
        if tokens.move() != matching:
            raise tokens.error("unmatched '%s'" % token)
        return [result]
    elif token == 'options':
        tokens.move()
        return [AnyOptions()]
    elif token.startswith('--') and token != '--':
        return parse_long(tokens, options)
    elif token.startswith('-') and token not in ('-', '--'):
        return parse_shorts(tokens, options)
    elif token.startswith('<') and token.endswith('>') or token.isupper():
        return [Argument(tokens.move())]
    else:
        return [Command(tokens.move())]


def parse_argv(tokens, options, options_first=False):
    """Parse command-line argument vector.

    If options_first:
        argv ::= [ long | shorts ]* [ argument ]* [ '--' [ argument ]* ] ;
    else:
        argv ::= [ long | shorts | argument ]* [ '--' [ argument ]* ] ;

    """
    parsed = []
    while tokens.current() is not None:
        if tokens.current() == '--':
            return parsed + [Argument(None, v) for v in tokens]
        elif tokens.current().startswith('--'):
            parsed += parse_long(tokens, options)
        elif tokens.current().startswith('-') and tokens.current() != '-':
            parsed += parse_shorts(tokens, options)
        elif options_first:
            return parsed + [Argument(None, v) for v in tokens]
        else:
            parsed.append(Argument(None, tokens.move()))
    return parsed


def parse_defaults(doc):
    # in python < 2.7 you can't pass flags=re.MULTILINE
    split = re.split('\n *(<\S+?>|-\S+?)', doc)[1:]
    split = [s1 + s2 for s1, s2 in zip(split[::2], split[1::2])]
    options = [Option.parse(s) for s in split if s.startswith('-')]
    #arguments = [Argument.parse(s) for s in split if s.startswith('<')]
    #return options, arguments
    return options


def printable_usage(doc):
    usage_pattern = re.compile(r'(usage:)', re.IGNORECASE)
    usage_split = re.split(usage_pattern, doc)
    if len(usage_split) < 3:
        raise DocoptLanguageError('"usage:" (case-insensitive) not found.')
    if len(usage_split) > 3:
        raise DocoptLanguageError('More than one "usage:" (case-insensitive).')
    return re.split(r'\n\s*\n', ''.join(usage_split[1:]))[0].strip()


def formal_usage(printable_usage):
    pu = printable_usage.split()[1:]  # split and drop "usage:"
    return '( ' + ' '.join(') | (' if s == pu[0] else s for s in pu[1:]) + ' )'


def extras(help, version, options, doc):
    if help and any((o.name in ('-h', '--help')) and o.value for o in options):
        print(doc.strip("\n"))
        sys.exit()
    if version and any(o.name == '--version' and o.value for o in options):
        print(version)
        sys.exit()


class Dict(dict):
    def __repr__(self):
        return '{%s}' % ',\n '.join('%r: %r' % i for i in sorted(self.items()))


def docopt(doc, argv=None, help=True, version=None, options_first=False):
    """Parse `argv` based on command-line interface described in `doc`.

    `docopt` creates your command-line interface based on its
    description that you pass as `doc`. Such description can contain
    --options, <positional-argument>, commands, which could be
    [optional], (required), (mutually | exclusive) or repeated...

    Parameters
    ----------
    doc : str
        Description of your command-line interface.
    argv : list of str, optional
        Argument vector to be parsed. sys.argv[1:] is used if not
        provided.
    help : bool (default: True)
        Set to False to disable automatic help on -h or --help
        options.
    version : any object
        If passed, the object will be printed if --version is in
        `argv`.
    options_first : bool (default: False)
        Set to True to require options precede positional arguments,
        i.e. to forbid options and positional arguments intermix.

    Returns
    -------
    args : dict
        A dictionary, where keys are names of command-line elements
        such as e.g. "--verbose" and "<path>", and values are the
        parsed values of those elements.

    Example
    -------
    >>> from docopt import docopt
    >>> doc = '''
    Usage:
        my_program tcp <host> <port> [--timeout=<seconds>]
        my_program serial <port> [--baud=<n>] [--timeout=<seconds>]
        my_program (-h | --help | --version)

    Options:
        -h, --help  Show this screen and exit.
        --baud=<n>  Baudrate [default: 9600]
    '''
    >>> argv = ['tcp', '127.0.0.1', '80', '--timeout', '30']
    >>> docopt(doc, argv)
    {'--baud': '9600',
     '--help': False,
     '--timeout': '30',
     '--version': False,
     '<host>': '127.0.0.1',
     '<port>': '80',
     'serial': False,
     'tcp': True}

    See also
    --------
    * For video introduction see http://docopt.org
    * Full documentation is available in README.rst as well as online
      at https://github.com/docopt/docopt#readme

    """
    if argv is None:
        argv = sys.argv[1:]
    DocoptExit.usage = printable_usage(doc)
    options = parse_defaults(doc)
    pattern = parse_pattern(formal_usage(DocoptExit.usage), options)
    # [default] syntax for argument is disabled
    #for a in pattern.flat(Argument):
    #    same_name = [d for d in arguments if d.name == a.name]
    #    if same_name:
    #        a.value = same_name[0].value
    argv = parse_argv(TokenStream(argv, DocoptExit), list(options),
                      options_first)
    pattern_options = set(pattern.flat(Option))
    for ao in pattern.flat(AnyOptions):
        doc_options = parse_defaults(doc)
        ao.children = list(set(doc_options) - pattern_options)
        #if any_options:
        #    ao.children += [Option(o.short, o.long, o.argcount)
        #                    for o in argv if type(o) is Option]
    extras(help, version, argv, doc)
    matched, left, collected = pattern.fix().match(argv)
    if matched and left == []:  # better error message if left?
        return Dict((a.name, a.value) for a in (pattern.flat() + collected))

    if left and left[0].name is not None:
        msg = "Unknown option: %s" % left[0].name
    else:
        msg = ""

    raise DocoptExit(msg)

########NEW FILE########
__FILENAME__ = error
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json

class ProcessError(Exception):
    """ exception raised on process error """

    def __init__(self, errno=400, reason="bad_request"):
        self.errno = errno
        self.reason = reason

    def __str__(self):
        return "%s: %s" % (self.errno, self.reason)

    def to_dict(self):
        return {"error": self.reason, "errno": self.errno}

    def to_json(self):
        return json.dumps(self.to_dict())

class ProcessNotFound(ProcessError):
    """ exception raised when a process or job isn't found """

    def __init__(self, reason="not_found"):
        ProcessError.__init__(self, errno=404, reason=reason)


class ProcessConflict(ProcessError):
    """ exception raised when a job already exists in the manager """

    def __init__(self, reason="process_conflict"):
        ProcessError.__init__(self, errno=409, reason=reason)

class TopicError(ProcessError):
    """ raised on topic error """


class CommandError(ProcessError):
    """ exception raised on command error """

    def __init__(self, reason="bad_command"):
        ProcessError.__init__(self, errno=400, reason=reason)

class CommandNotFound(ProcessNotFound):
    """ exception raised when a command doesn't exist """

class AlreadyRead(ProcessError):

    def __init__(self,):
        ProcessError.__init__(self, errno=403, reason="already_read")

########NEW FILE########
__FILENAME__ = events
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
Many events happend in gaffer.


Manager events
--------------

Manager events have the following format::

    {
      "event": "<nameofevent">>,
      "name": "<templatename>"
    }

- **create**: a process template is created
- **start**: a process template start to launch OS processes
- **stop**: all OS processes of a process template are stopped
- **restart**: all processes of a process template are restarted
- **update**: a process template is updated
- **delete**: a process template is deleted
- **spawn**: a new process is spawned
- **reap**: a process is reaped
- **exit**: a process exited
- **stop_pid**: a process has been stopped


Processes events
----------------

All processes' events are prefixed by ``proc.<name>`` to make the pattern
matching easier, where ``<name>`` is the name of the process template

Events are:

- **proc.<name>.start** : the template <name> start to spawn processes
- **proc.<name>.spawn** : one OS process using the process <name>
  template is spawned. Message is::

    {
      "event": "proc.<name>.spawn">>,
      "name": "<name>",
      "detach": false,
      "pid": int
    }


  .. note::

    pid is the internal pid
- **proc.<name>.exit**: one OS process of the <name> template has
  exited. Message is::

    {
      "event": "proc.<name>.exit">>,
      "name": "<name>",
      "pid": int,
      "exit_code": int,
      "term_signal": int
    }

- **proc.<name>.stop**: all OS processes in the template <name> are
  stopped.
- **proc.<name>.stop_pid**: One OS process of the template <name> is
  stopped. Message is::

    {
      "event": "proc.<name>.stop_pid">>,
      "name": "<name>",
      "pid": int
    }

- **proc.<name>.stop_pid**: One OS process of the template <name> is
  reapped. Message is::

    {
      "event": "proc.<name>.reap">>,
      "name": "<name>",
      "pid": int
    }


The :mod:`events` Module
------------------------


This module offeres a common way to susbscribe and emit events. All
events in gaffer are using.

Example of usage
++++++++++++++++

::

        event = EventEmitter()

        # subscribe to all events with the pattern a.*
        event.subscribe("a", subscriber)

        # subscribe to all events "a.b"
        event.subscribe("a.b", subscriber2)

        # subscribe to all events (wildcard)
        event.subscribe(".", subscriber3)

        # publish an event
        event.publish("a.b", arg, namedarg=val)

In this example all subscribers will be notified of the event. A
subscriber is just a callable *(event, *args, **kwargs)*

Classes
-------

"""

from collections import deque
import logging

import pyuv


class EventEmitter(object):
    """ Many events happend in gaffer. For example a process will emist
    the events "start", "stop", "exit".

    This object offer a common interface to all events emitters """

    def __init__(self, loop, max_size=10000):
        self.loop = loop
        self._events = {}
        self._wildcards = set()

        self._queue = deque(maxlen=max_size)
        self._wqueue = deque(maxlen=max_size)

        self._event_dispatcher = pyuv.Prepare(self.loop)
        self._event_dispatcher.start(self._send)
        self._event_dispatcher.unref()
        self._spinner = pyuv.Idle(self.loop)

    def close(self):
        """ close the event

        This function clear the list of listeners and stop all idle
        callback """
        self._wqueue.clear()
        self._queue.clear()
        self._events = {}
        self._wildcards = set()

        # close handlers
        if not self._event_dispatcher.closed:
            self._event_dispatcher.close()

        if not self._spinner.closed:
            self._spinner.close()

    def publish(self, evtype, *args, **kwargs):
        """ emit an event **evtype**

        The event will be emitted asynchronously so we don't block here
        """
        if "." in evtype:
            parts = evtype.split(".")
            self._queue.append((parts[0], evtype, args, kwargs))
            key = []
            for part in parts:
                key.append(part)
                self._queue.append((".".join(key), evtype, args, kwargs))
        else:
            self._queue.append((evtype, evtype, args, kwargs))

        # emit the event for wildcards events
        self._wqueue.append((evtype, args, kwargs))

        # send the event for later
        self._dispatch_event()

    def subscribe(self, evtype, listener, once=False):
        """ subcribe to an event """

        if evtype == ".": # wildcard
            self._wildcards.add((once, listener))
            return

        if evtype.endswith("."):
            evtype = evtype[:-1]

        if evtype not in self._events:
            self._events[evtype] = set()

        self._events[evtype].add((once, listener))

    def subscribe_once(self, evtype, listener):
        """ subscribe to event once.
        Once the evennt is triggered we remove ourself from the list of
        listenerrs """

        self.subscribe(evtype, listener, True)

    def unsubscribe(self, evtype, listener, once=False):
        """ unsubscribe from an event"""
        if evtype not in self._events:
            return

        try:
            self._events[evtype].remove((once, listener))
        except KeyError:
            pass

    def unsubscribe_once(self, evtype, listener):
        self.unsubscribe(evtype, listener, True)

    def unsubscribe_all(self, events=[]):
        """ unsubscribe all listeners from a list of events """
        for evtype in events:
            if evtype == ".":
                self._wildcards = set()
            else:
                self._events[evtype] = set()

    ### private methods

    def _dispatch_event(self):
        self._spinner.start(lambda h: None)

    def _send(self, handle):
        lwqueue = len(self._wqueue)
        lqueue = len(self._queue)

        for i in range(lwqueue):
            evtype, args, kwargs = self._wqueue.popleft()
            if self._wildcards:
                self._wildcards = self._send_listeners(evtype,
                        self._wildcards.copy(), *args, **kwargs)

        for i in range(lqueue):
            pattern, evtype, args, kwargs = self._queue.popleft()
            # emit the event to all listeners
            if pattern in self._events:
                self._events[pattern] = self._send_listeners(evtype,
                    self._events[pattern].copy(), *args, **kwargs)

        if not self._spinner.closed:
            self._spinner.stop()

    def _send_listeners(self, evtype, listeners, *args, **kwargs):
        to_remove = []
        for once, listener in listeners:
            try:
                listener(evtype, *args, **kwargs)
            except Exception:
                # we ignore all exception
                logging.error('Uncaught exception', exc_info=True)
                to_remove.append(listener)

            if once:
                # once event
                to_remove.append(listener)

        if to_remove:
            for listener in to_remove:
                try:
                    listeners.remove((True, listener))
                except KeyError:
                    pass
        return listeners

########NEW FILE########
__FILENAME__ = config
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy
import fnmatch
import os
try:
    import configparser
except ImportError:
    import ConfigParser as configparser

import six

from ..gafferd.util import user_path
from ..state import FlappingInfo

PROCESS_DEFAULTS = dict(
        group = None,
        args = None,
        env = {},
        uid = None,
        gid = None,
        cwd = None,
        detach = False,
        shell = False,
        os_env = True,
        numprocesses = 1,
        start = True,
        priority = six.MAXSIZE)


class ConfigError(Exception):
    """ exception raised on config error """


class DefaultConfigParser(configparser.ConfigParser):
    """ object overriding ConfigParser to return defaults values instead
    of raising an error if needed """

    def dget(self, section, option, default=None):
        if not self.has_option(section, option):
            return default
        return self.get(section, option)

    def dgetint(self, section, option, default=None):
        if not self.has_option(section, option):
            return default
        return self.getint(section, option)

    def dgetboolean(self, section, option, default=None):
        if not self.has_option(section, option):
            return default
        return self.getboolean(section, option)


class Config(object):
    """ main gafferd config object """

    def __init__(self, args, config_dir):
        self.args = args
        self.config_dir = config_dir
        self.cfg = None

        # set defaulut
        self.set_defaults()

    def load(self):
        """ load the config """

        # maybe load config from a config file
        config_file = os.path.join(self.config_dir, "gafferd.ini")
        if os.path.isfile(config_file):
            self.parse_config(config_file)

        # if the plugin dir hasn't been set yet, set it to the default path
        if not self.plugin_dir:
            if 'GAFFER_PLUGIN_DIR' in os.environ:
                self.plugin_dir = os.environ.get('GAFFER_PLUGIN_DIR')
            else:
                self.plugin_dir = os.path.join(self.config_dir, "plugins")

        # bind address
        self.bind = self.args['--bind'] or self.bind

        #lookupd address
        if self.args['--lookupd-address'] is not None:
            self.lookupd_addresses = self.args['--lookupd-address']

        # broadcast address
        if self.args['--broadcast-address']:
            self.broadcast_address = self.args['--broadcast-address']

        if (self.broadcast_address is not None and
                not self.broadcast_address.startswith("http://") and
                not self.broadcast_address.startswith("https://")):
            raise ConfigError("invalid broadcast address")

        # set the backlog
        if self.args["--backlog"]:
            try:
                self.backlog = int(self.args["--backlog"])
            except ValueError:
                raise ConfigError("backlog should be an integer")

        # parse SSL options
        self.parse_ssl_options()

        # pidfile
        if self.args["--pidfile"] is not None:
            self.pidfile = self.args["--pidfile"]

        if self.args["--daemon"]:
            self.daemonize = True

        if self.args['-v'] > 0:
            self.logfile = "-"

        if self.args['--error-log'] is not None:
            self.logfile = self.args['--error-log']

        if self.args['--log-level'] is not None:
            self.loglevel = self.args['--log-level']

        if self.args['--require-key']:
            self.require_key = True


    def reload(self):
        """ like reload but track removed processes and webhhoks """
        # store the old processes webhooks list
        old_processes = set([(n, s) for n, s, c, p in self.processes])
        old_webhooks = set(self.webhooks)

        # reload the config
        self.load()

        # get all processes & webhooks to remove
        new_processes = set([(n, s) for n, s, c, p in self.processes])
        removed_processes = old_processes.difference(new_processes)
        removed_webhooks = old_webhooks.difference(set(self.webhooks))

        return (removed_processes, removed_webhooks)

    def set_defaults(self):
        self.plugin_dir = self.args["--plugin-dir"]
        self.webhooks = []
        self.processes = []
        self.ssl_options = {}
        self.client_ssl_options = {}
        self.bind = "0.0.0.0:5000"
        self.lookupd_addresses = []
        self.broadcast_address = None
        self.backlog = 128
        self.daemonize = False
        self.pidfile = None
        self.logfile = None
        self.loglevel = "info"

        # auth(z) API
        self.require_key = False
        self.auth_backend = "default"
        self.keys_backend = "default"
        self.auth_dbname = None
        self.keys_dbname = None

    def parse_ssl_options(self):
        ssl_options = {}
        client_ssl_options = {}
        if self.args["--certfile"] is not None:
            ssl_options['certfile'] = self.args["--certfile"]

        if self.args["--keyfile"] is not None:
            ssl_options["keyfile"] = self.args["--keyfile"]

        client_ssl_options = {}
        if self.args["--client-certfile"] is not None:
            ssl_options['certfile'] = self.args["--client-certfile"]

        if self.args["--keyfile"] is not None:
            client_ssl_options["keyfile"] = self.args["--client-keyfile"]

        if self.args.get("--cacert") is not None:
            client_ssl_options["ca_certs"] = self.args["--cacert"]

        # update the SSL configuration previously loaded in the config file
        if self.ssl_options is not None:
            self.ssl_options.update(ssl_options)
        self.client_ssl_options.update(client_ssl_options)

        # no ssl options
        if not self.ssl_options:
            self.ssl_options = None

    def read_config(self, config_path):
        cfg = DefaultConfigParser()
        with open(config_path) as f:
            cfg.readfp(f)
        cfg_files_read = [config_path]

        # load included config files
        includes = []
        for include_file in cfg.dget('gaffer', 'include', '').split():
            includes.append(include_file)

        for include_dir in cfg.dget('gaffer', 'include_dir', '').split():
            for root, dirnames, filenames in os.walk(include_dir):
                for filename in fnmatch.filter(filenames, '*.ini'):
                    cfg_file = os.path.join(root, filename)
                    includes.append(cfg_file)

        cfg_files_read.extend(cfg.read(includes))
        return cfg, cfg_files_read

    def parse_config(self, config_file):
        cfg, cfg_files_read = self.read_config(config_file)
        self.cfg = cfg

        plugin_dir = cfg.dget('gaffer', 'plugins_dir', "")
        if plugin_dir:
            self.plugin_dir = plugin_dir

        self.bind = cfg.dget('gaffer', 'bind', "0.0.0.0:5000")
        self.broadcast_address = cfg.dget('gaffer', 'broadcast_address')
        self.backlog = cfg.dgetint('gaffer', 'backlog', 128)
        self.daemonize = cfg.dgetboolean('gaffer', 'daemonize', False)
        self.pidfile = cfg.dget('gaffer', 'pidfile')
        self.logfile =  cfg.dget('gaffer', 'error_log', self.logfile)
        self.loglevel = cfg.dget('gaffer', 'log_level', self.loglevel)

        # Collect lookupd addresses
        # they are put in the gaffer section undert the form:
        #
        #    lookupd_address1 = http://127.0.0.1:5010
        #
        self.lookupd_addresses = []
        for k, v in cfg.items('gaffer'):
            if k.startswith('lookupd_address'):
                self.lookupd_addresses.append(v)

        # parse AUTH api
        self.require_key = cfg.dgetboolean('gaffer', 'require_key', True)
        self.auth_backend = cfg.dget('auth', 'auth_backend', 'default')
        self.keys_backend = cfg.dget('auth', 'keys_backend', 'default')
        self.auth_dbname = cfg.dget('auth', 'auth_dbname', None)
        self.keys_dbname = cfg.dget('auth', 'keys_dbname', None)

        processes = []
        webhooks = []
        envs = {}
        for section in cfg.sections():
            if section.startswith('process:') or section.startswith('job:'):
                if section.startswith('process:'):
                    prefix = "process:"
                else:
                    prefix = "job:"

                name = section.split(prefix, 1)[1]
                name, sessionid = self._split_name(name)
                cmd = cfg.dget(section, 'cmd', '')
                if cmd:
                    params = PROCESS_DEFAULTS.copy()
                    for key, val in cfg.items(section):
                        if key == "args":
                            params[key] = val
                        elif key.startswith('env:'):
                            envname = key.split("env:", 1)[1]
                            params['env'][envname] = val
                        elif key == 'uid':
                            params[key] = val
                        elif key == 'gid':
                            params[key] = val
                        elif key == 'cwd':
                            params[key] = val
                        elif key == 'detach':
                            params[key] = cfg.dgetboolean(section, key,
                                    False)
                        elif key == 'shell':
                            params[key] = cfg.dgetboolean(section, key,
                                    False)
                        elif key == 'os_env':
                            params[key] = cfg.dgetboolean(section, key,
                                    True)
                        elif key == 'numprocesses':
                            params[key] = cfg.dgetint(section, key, 1)
                        elif key == 'start':
                            params[key] = cfg.dgetboolean(section, key,
                                    True)
                        elif key == 'flapping':
                            # flapping values are passed in order on one
                            # line
                            values_str = val.split(None)
                            try:
                                values = [float(val) for val in values_str]
                                params['flapping'] = FlappingInfo(*values)
                            except ValueError:
                                pass
                        elif key == "redirect_output":
                            params[key] = [v.strip() for v in val.split(",")]
                        elif key == "redirect_input":
                            params[key] = cfg.dgetboolean(section, key,
                                    False)
                        elif key == "graceful_timeout":
                            params[key] = cfg.dgetint(section, key, 10)
                        elif key == "priority":
                            params[key] = cfg.dgetint(section, key,
                                    six.MAXSIZE)

                    processes.append((name, sessionid, cmd, params))
            elif section == "webhooks":
                for key, val in cfg.items(section):
                    webhooks.append((key, val))
            elif section.startswith('env:'):
                pname = section.split("env:", 1)[1]
                name, sessionid = self._split_name(pname)
                kvs = [(key.upper(), val) for key, val in cfg.items(section)]
                envs[(sessionid, name)] = dict(kvs)
            elif section == "ssl":
                for key, val in cfg.items(section):
                    self.ssl_options[key] = val
            elif section == "lookup_ssl":
                for key, val in cfg.items(section):
                    self.client_ssl_options[key] = val

        # add environment variables
        for name, sessionid, cmd, params in processes:
            if (sessionid, name) in envs:
                params['env'] = envs[(sessionid, name)]

        # sort processes by priority
        processes = sorted(processes, key=lambda p: p[3]['priority'])

        self.webhooks = webhooks
        self.processes = processes

    def _split_name(self, name):
        if "/" in name:
            name, sessionid = name.split("/", 1)
        elif ":" in name:
            name, sessionid = name.split(":", 1)
        elif "." in name:
            name, sessionid = name.split(".", 1)
        else:
            sessionid = "default"
        return name, sessionid

########NEW FILE########
__FILENAME__ = http
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy
import logging
import ssl
import sys

# patch tornado IOLoop
from ..tornado_pyuv import IOLoop, install
install()

from tornado.web import Application
from tornado.httpserver import HTTPServer

from ..httpclient.util import make_uri
from .. import sockjs
from ..util import (bind_sockets, hostname, is_ssl)
from . import http_handlers
from .keys import KeyManager
from .lookup import LookupClient
from .users import AuthManager

LOGGER = logging.getLogger("gaffer")
PROTOCOL_VERSION = "0.1"
LOOKUP_EVENTS = ("load", "unload", "spawn", "stop_process", "exit")

DEFAULT_HANDLERS = [
        (r'/', http_handlers.WelcomeHandler),
        (r'/ping', http_handlers.PingHandler),
        (r'/version', http_handlers.VersionHandler),
        (r'/([0-9^/]+)', http_handlers.ProcessIdHandler),
        (r'/([0-9^/]+)/signal$', http_handlers.ProcessIdSignalHandler),
        (r'/([0-9^/]+)/stats$', http_handlers.ProcessIdStatsHandler),
        (r'/([0-9^/]+)/channel', http_handlers.PidChannel),
        (r'/([0-9^/]+)/channel/([^/]+)$', http_handlers.PidChannel),
        (r'/pids', http_handlers.AllProcessIdsHandler),
        (r'/sessions', http_handlers.SessionsHandler),
        (r'/jobs', http_handlers.AllJobsHandler),
        (r'/jobs/([^/]+)', http_handlers.JobsHandler),
        (r'/jobs/([^/]+)/([^/]+)', http_handlers.JobHandler),
        (r'/jobs/([^/]+)/([^/]+)/stats$', http_handlers.JobStatsHandler),
        (r'/jobs/([^/]+)/([^/]+)/numprocesses$', http_handlers.ScaleJobHandler),
        (r'/jobs/([^/]+)/([^/]+)/signal$', http_handlers.SignalJobHandler),
        (r'/jobs/([^/]+)/([^/]+)/state$', http_handlers.StateJobHandler),
        (r'/jobs/([^/]+)/([^/]+)/pids$', http_handlers.PidsJobHandler),
        (r'/jobs/([^/]+)/([^/]+)/commit$', http_handlers.CommitJobHandler),
        (r'/auth', http_handlers.AuthHandler),
        (r'/keys', http_handlers.KeysHandler),
        (r'/keys/([^/]+)$', http_handlers.KeyHandler),
        (r'/users', http_handlers.UsersHandler),
        (r'/users/([^/]+)', http_handlers.UserHandler),
        (r'/users/([^/]+)/password', http_handlers.UserPasswordHandler),
        (r'/users/([^/]+)/key', http_handlers.UserKeydHandler)
]


class HttpHandler(object):
    """ simple gaffer application that gives an HTTP API access to gaffer.
    """

    def __init__(self, config, plugin_manager=None, **settings):
        self.config = config
        self.plugin_manager = plugin_manager
        self.key_mgr = None
        self.auth_mgr = None

        # custom settings
        if 'manager' in settings:
            del settings['manager']
        self.settings = settings

        self.clients = dict()

        # intialize the config
        self.init_config()


    def init_config(self):
        self.hostname = hostname()
        self.address = self.config.bind
        self.broadcast_address = self.config.broadcast_address
        self.lookupd_addresses = self.config.lookupd_addresses
        self.backlog = self.config.backlog

        # initialize ssl options
        self.ssl_options = self.config.ssl_options
        self.client_options = self.config.client_ssl_options or {}
        # disable SSLv2
        # http://blog.ivanristic.com/2011/09/ssl-survey-protocol-support.html
        if sys.version_info >= (2, 7):
            self.client_options["ciphers"] = "DEFAULT:!SSLv2"
        else:
            # This is really only necessary for pre-1.0 versions
            # of openssl, but python 2.6 doesn't expose version
            # information.
            self.client_options["ssl_version"] = ssl.PROTOCOL_SSLv3

         # set http handlers
        self.handlers = copy.copy(DEFAULT_HANDLERS)
        if self.plugin_manager is not None:
            self.handlers.extend(self.plugin_manager.get_sites() or [])


    def init_app(self):
        # add channel routes
        user_settings = { "manager": self.manager }

        # start the key api if needed
        if self.config.require_key:
            self.key_mgr = KeyManager(self.loop, self.config)
            self.auth_mgr = AuthManager(self.loop, self.config)
            user_settings.update({"require_key": True, "key_mgr": self.key_mgr,
                "auth_mgr": self.auth_mgr})
        else:
            self.key_mgr = None
            self.auth_mgr = None

        channel_router = sockjs.SockJSRouter(http_handlers.ChannelConnection,
                "/channel", io_loop=self.io_loop, user_settings=user_settings)

        handlers = self.handlers + channel_router.urls

        settings = self.settings.copy()
        settings.update(user_settings)

        # create the application
        self.app = Application(handlers, **user_settings)


    def start(self, loop, manager):
        self.loop = loop
        self.io_loop = IOLoop(_loop=loop)
        self.manager = manager

        # init app
        self.init_app()

        # start the server
        self._start_server()

        # start the lookup client
        self._start_lookup()

    def stop(self):
        # stop the server
        self.server.stop()

        # close the api key managers
        if self.config.require_key:
            self.key_mgr.close()
            self.auth_mgr.close()

        for event in LOOKUP_EVENTS:
            self.manager.events.unsubscribe(event, self._on_event)


        # close all lookups clients
        addresses = list(self.clients)
        for addr in addresses:
            client = self.clients.pop(addr)
            if not client.closed:
                client.close()
        self.clients = {}

        # finally close the tornado loop
        self.io_loop.close()

    def restart(self):
        # stop the server
        self.server.stop()

        # close the api key managers
        if self.config.require_key:
            self.key_mgr.close()
            self.auth_mgr.close()


        # close all lookups clients
        addresses = list(self.clients)
        for addr in addresses:
            client = self.clients.pop(addr)
            if not client.closed:
                client.close()
        self.clients = {}

        # reinit the config
        self.init_config()

        # reinit the app
        self.init_app()

        # start the server
        self._start_server()

        # restart lookup clients
        self._start_lookup()

        # notify all jobs
        jobs = self.manager.jobs()
        for _, client in self.clients:
            [client.add_job(job_name) for job_name in jobs]

    def _start_server(self):
        # open API keys managers
        if self.config.require_key:
            self.key_mgr.open()
            self.auth_mgr.open()

        self.server = HTTPServer(self.app, io_loop=self.io_loop,
                ssl_options=self.ssl_options)

        # initialize the socket
        sock = bind_sockets(self.address, backlog=self.backlog)
        self.server.add_sockets(sock)
        self.port = sock[0].getsockname()[1]

        # start the server
        self.server.start()

    def _start_lookup(self):
        if not self.broadcast_address:
            if self.ssl_options:
                scheme = "https"
            else:
                scheme = "http"

            origin = "%s://%s:%s" % (scheme, self.hostname, self.port)
        else:
            origin = self.broadcast_address

        for addr in self.lookupd_addresses:
            if addr.startswith("http"):
                addr = addr.replace("http", "ws")

            addr = make_uri(addr, "/ws")
            if addr in self.clients:
                return

            # initialize the client
            options = {}
            if is_ssl(addr):
                options["ssl_options"] = self.client_options
            client = LookupClient(self.loop, addr, **options)

            # register the client
            self.clients[addr] = client

            # start the client
            client.start(on_exit_cb=self._on_exit_lookup)

            # identify the client
            # node name is its hostname for now
            client.identify(self.hostname, origin, PROTOCOL_VERSION)

            for event in LOOKUP_EVENTS:
                self.manager.events.subscribe(event, self._on_event)

    def _on_event(self, event, msg):
        if not self.clients:
            return

        if event == "load":
            args = (msg['name'],)
            lookup_event = "add_job"

        elif event == "unload":
            args = (msg['name'],)
            lookup_event = "remove_job"
        elif event == "spawn":
            args = (msg['name'], msg['pid'],)
            lookup_event = "add_process"
        elif event in ("stop_process", "exit"):
            args = (msg['name'], msg['pid'],)
            lookup_event = "remove_process"


        for _, client in self.clients.items():
            fun = getattr(client, lookup_event)
            fun(*args)

    def _on_exit_lookup(self, client):
        if client.url not in self.clients:
            return

        LOGGER.info("LOOKUP: %r exited" % client.url)
        try:
            del self.clients[client.url]
        except KeyError:
            pass

########NEW FILE########
__FILENAME__ = auth
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import base64
import json

from tornado.web import HTTPError

from .util import CorsHandler

class AuthHandler(CorsHandler):

    def prepare(self):
        require_key = self.settings.get('require_key', False)
        auth_mgr = self.settings.get("auth_mgr")

        if not require_key:
            raise HTTPError(404)

        # get auth header
        auth_hdr = self.request.headers.get('Authorization', "").encode('utf-8')
        if not auth_hdr or not auth_hdr.startswith(b'Basic '):
            raise HTTPError(401)

        # decode the auth header
        auth_decoded = base64.decodestring(auth_hdr[6:])
        username, password = auth_decoded.split(b':', 2)

        # authenticate the user
        self.user = auth_mgr.authenticate(username.decode('utf-8'),
                password.decode('utf-8'))

        if not self.user.is_authenticated():
            raise HTTPError(401)

        self.set_header("Content-Type", "application/json")
        self.set_header("X-Api-Key", self.user.key or "")

    def head(self):
        self.set_status(200)

    def get(self, *args):
        self.write({"api_key": self.user.key})

########NEW FILE########
__FILENAME__ = channels
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from functools import partial
import json

from ...controller import Command, Controller
from ...error import ProcessError
from ...sockjs import SockJSConnection
from ...sync import increment, decrement
from ..keys import Key, DummyKey, KeyNotFound

class MessageError(Exception):
    """ raised on message error """


class SubscriptionError(Exception):
    """ raised on subscriptionError """


class Subscription(object):

    def __init__(self, topic):
        self.topic = topic
        self.nb = 0
        self.callback = None

        parts = self.topic.split(":", 1)
        self.pid = None
        if len(parts) == 1:
            self.source = parts[0].upper()
            self.target = "."
        else:
            self.source, self.target = parts[0].upper(), parts[1].lower()
            if self.target.isdigit():
                self.pid = int(self.target)
                self.target = self.pid
            elif self.source == "STREAM" and "." in self.target:
                pid, target = self.target.split(".", 1)
                if pid.isdigit():
                    self.pid = int(pid)
                    self.target = target

    def __str__(self):
        return "subscription: %s" % self.topic


class WSCommand(Command):

    def __init__(self, ws, msg):
        self.ws = ws
        self.identity = msg.identity
        super(WSCommand, self).__init__(msg.name, msg.args, msg.kwargs)

    def reply(self, result):
        data = {"id": self.identity, "result": result}
        msg = {"event": "gaffer:command_success", "data": data}
        self.ws.write_message(msg)

    def reply_error(self, error):
        data = {"id": self.identity, "error": error}
        msg = {"event": "gaffer:command_error", "data": data}
        self.ws.write_message(msg)


class Message(object):

    def __init__(self, msg):
        if not isinstance(msg, dict):
            try:
                msg = json.loads(msg)
            except ValueError:
                raise MessageError("invalid_json")

        self.msg = msg
        try:
            self.event = msg['event']
        except KeyError:
            raise MessageError("event_missing")

        if self.event == "NOP":
            self.nop = True
        else:
            self.nop = False
            self.parse_message(msg)

    def parse_message(self, msg):
        try:
            self.data = msg['data']
        except KeyError:
            raise MessageError("data_missing")

        if self.event in ("SUB", "UNSUB"):
            if "topic" not in self.data:
                raise MessageError("topic_missing")

            self.topic = self.data['topic']

        elif self.event == "CMD":
            if "name" not in self.data:
                raise MessageError("cmd_name_mssing")

            if "identity" not in self.data:
                raise MessageError("cmd_identity_missing")

            self.identity = self.data['identity']
            self.name = self.data['name']
            self.args = self.data.get('args', ())
            self.kwargs = self.data.get('kwargs', {})
        else:
            raise MessageError("unknown_cmd")

    def __str__(self):
        if self.event in ("SUB", "UNSUB"):
            return "%s: %s" % (self.event, self.data['event'])
        elif self.event == "CMD":
            return "%s: %s" % (self.event, self.data['cmd'])

        return self.event


class ChannelConnection(SockJSConnection):

    def on_open(self, info):
        self.settings = self.session.server.settings
        self.manager = self.settings.get('manager')

        # initialize key handling
        self.require_key = self.settings.get('require_key', False)
        self.key_mgr = self.settings.get('key_mgr')
        self.api_key = None

        self.ctl = Controller(self.manager)
        self._subscriptions = {}

    def on_close(self):
        if self._subscriptions:
            for _, sub in self._subscriptions.items():
                self.stop_subcription(sub)

            self._subscriptions = []

    def authenticate(self, body):
        if body.startswith("AUTH:"):
            key = body.split("AUTH:")[1]
            try:
                self.api_key = Key.load(self.key_mgr.get_key(key))
            except KeyNotFound:
                raise ProcessError(403, "forbidden")
        else:
            raise ProcessError(401, "unauthorized")

    def on_message(self, raw):
        if not self.api_key and self.require_key:
            try:
                self.authenticate(raw)
            except ProcessError as e:
                self.write_message(_error_msg(error="AUTH_REQUIRED",
                    reason=e.to_json()))
                return self.close()
        else:
            self.api_key = DummyKey()

        try:
            msg = Message(raw)
        except MessageError as e:
            return self.write_message(_error_msg(error="invalid_msg",
                reason=str(e)))

        if msg.nop:
            return

        try:
            if msg.event == "SUB":
                self.add_subscription(msg.topic)
            elif msg.event == "UNSUB":
                self.del_subscription(msg.topic)
            elif msg.event == "CMD":
                command = WSCommand(self, msg)
                self._check_command_authz(command)
                self.ctl.process_command(command)
        except SubscriptionError as e:
            return self.write_message(_error_msg(event="subscription_error",
                reason=str(e)))

        if msg.event == "SUB":
            self.write_message({"event": "gaffer:subscription_success",
                "topic": msg.topic})
        elif msg.event == "UNSUB":
            self.write_message({"event": "gaffer:subscription_success",
                "topic": msg.topic })

    def add_subscription(self, topic):
        if topic in self._subscriptions:
            sub = self._subscriptions[topic]
        else:
            sub = self._subscriptions[topic] = Subscription(topic)

        if not sub.nb:
            self.start_subscription(sub)

        sub.nb = increment(sub.nb)

    def del_subscription(self, topic):
        if topic not in self._subscriptions:
            return

        sub = self._subscriptions[topic]
        sub.nb = decrement(sub.nb)

        if not sub.nb:
            self.stop_subcription(sub)
            del self._subscriptions[sub]

    def start_subscription(self, sub):
        if sub.source == "EVENTS":
            # only managers can read events
            if not self.api_key.can_manage_all():
                raise SubscriptionError("forbidden")

            sub.callback = partial(self._dispatch_event, sub.topic)
            # subscribe to all manager events
            self.manager.events.subscribe(sub.target, sub.callback)
        elif sub.source == "JOB":
            if not self.api_key.can_manage(sub.target):
                raise SubscriptionError("forbidden")

            sub.callback = partial(self._dispatch_process_event, sub.topic)
            self.manager.events.subscribe("job.%s" % sub.target, sub.callback)
        elif sub.source == "PROCESS":
            # can we read this process
            try:
                p = self.manager.get_process(sub.pid)
            except ValueError:
                raise SubscriptionError("invalid_process")
            except ProcessError as e:
                raise SubscriptionError(e.to_json())

            if not self.api_key.can_manage_all(p.name):
                raise SubscriptionError("forbidden")


            sub.callback = partial(self._dispatch_process_event, sub.topic)
            self.manager.events.subscribe("proc.%s" % sub.target, sub.callback)
        elif sub.source == "STATS":
            if sub.pid is not None:
                sub.callback = partial(self._dispatch_event, sub.topic)
                # subscribe to the pid stats
                proc = self.manager.get_process(sub.pid)

                # check if we can read on this process
                self._check_read(proc.name)

                proc.monitor(sub.callback)
            else:
                # check if we can read on this job
                self._check_read(sub.target)

                sub.callback = partial(self._dispatch_event, sub.topic)
                # subscribe to the job processes stats
                state = self.manager._get_locked_state(sub.target)
                for proc in state.running:
                    proc.monitor(sub.callback)
        elif sub.source == "STREAM":
            if not sub.pid:
                raise SubscriptionError("invalid_topic")

            sub.callback = partial(self._dispatch_output, sub.topic)
            proc = self.manager.get_process(sub.pid)

            # check if we can read on this process
            self._check_read(proc.name)


            # get the target to receive the data from
            if sub.target == sub.pid:
                target = proc.redirect_output[0]
            else:
                target = sub.target

            # check if the target exists
            if target in proc.redirect_output:
                proc.monitor_io(target, sub.callback)
            elif target in proc.custom_streams:
                proc.streams[target].subscribe(sub.callback)
            else:
                raise SubscriptionError("stream_not_found")
        else:
            raise SubscriptionError("invalid_topic")

    def stop_subscription(self, sub):
        if sub.source == "EVENTS":
            self.manager.events.unsubscribe(sub.target, sub.callback)
        elif sub.source == "JOB":
            self.manager.events.unsubscribe("job.%s" % sub.target,
                    sub.callback)
        elif sub.source == "PROCESS":
            self.manager.events.unsubscribe("proc.%s" % sub.target,
                    sub.callback)
        elif sub.source == "STATS":
            if sub.pid is not None:
                proc = self.manager.get_process(sub.pid)
                proc.unmonitor(sub.callback)
            else:
                state = self.manager._get_locked_state(sub.target)
                for proc in state.running:
                    proc.monitor(sub.callback)
        elif sub.source == "STREAM":
            if sub.pid:
                proc = self.manager.get_process(sub.pid)
                if sub.target == sub.pid:
                    target = proc.redirect_output[0]
                else:
                    target = sub.target

                if target in proc.redirect_output:
                    proc.unmonitor_io(target, sub.callback)
                elif target in proc.custom_streams:
                    proc.streams[target].unsubscribe(sub.callback)


    def _check_command_authz(self, command):
        if self.api_key.can_manage_all():
            return

        try:
            self._do_check_command_authz(command)
        except ProcessError as pe:
            command.reply_error({"errno": pe.errno, "reason": pe.reason})
        except Exception as e:
            command.reply_error({"errno": 500, "reason": str(e)})

    def _do_check_command_authz(self, command):
        if (command.name in ("process_info", "process_stats", "stop_process",
            "send", "kill",)):

            # if not pid given return and handle the command error later
            if not command.args:
                return

            # get the process instance
            p = self.manager.get_process(command.args[0])

            # we need write permission for 'send'
            if command.name == "send" and self.api_key.can_write(p.name):
                return

            # else we need manage rights tp execute commands.
            if self.api_key.can_manage(p.name):
                return
        elif command.name in ("sessions", "jobs", "pids",):
            # we need manage_all permission for such commands
            if self.api_key.can_manage_all():
                return
        elif command.name in ("load", "unload", "reload", "update",):
            # we need to be an admin
            if self.api_key.is_admin():
                return
        else:
            if not command.args:
                return

            # only manager permission are needed.
            if self.api_key.can_manage(command.args[0]):
                return

        raise ProcessError(403, "forbidden")

    def _check_read(self, pname):
        if not self.api_key.can_read(pname):
            raise SubscriptionError("forbidden")

    def _dispatch_event(self, topic, evtype, ev):
        data = { "event": evtype, "topic": topic}
        data.update(ev)
        msg = { "event": "gaffer:event", "data": data}
        self.write_message(msg)

    def _dispatch_process_events(self, topic, evtype, ev):
        try:
            sub = self._subscriptions[topic]
        except KeyError:
            return

        evtype = evtype.split("proc.%s." % sub.target, 1)[1]
        self._dispatch_event(topic, evtype, ev)

    def _dispatch_output(self, topic, evtype, ev):
        if isinstance( ev['data'], bytes):
            ev['data'] =  ev['data'].decode("utf-8")

        data = { "event": evtype, "topic": topic}
        data.update(ev)
        msg = { "event": "gaffer:event", "data": data}
        self.write_message(msg)


    def write_message(self, msg):
        if isinstance(msg, dict):
            self.send(json.dumps(msg))
        else:
            self.send(msg)

def _error_msg(event="gaffer:error", **data):
    msg =  { "event": event, "data": data }
    return json.dumps(msg)

########NEW FILE########
__FILENAME__ = jobs
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
from tornado.web import HTTPError

from ...error import ProcessError
from ...process import ProcessConfig
from .util import CorsHandler, CorsHandlerWithAuth


class SessionsHandler(CorsHandlerWithAuth):
    """ /sessions """

    def get(self, *args):
        self.preflight()

        if (not self.api_key.is_admin() and
                not self.api_key.can_manage_all()):
            raise HTTPError(403)

        m = self.settings.get('manager')
        self.set_header('Content-Type', 'application/json')
        self.write(json.dumps({"sessions": m.sessions}))


class AllJobsHandler(CorsHandlerWithAuth):
    """ /jobs """

    def get(self, *args):
        self.preflight()

        if (not self.api_key.is_admin() and
                not self.api_key.can_manage_all()):
            raise HTTPError(403)

        m = self.settings.get('manager')
        self.set_header('Content-Type', 'application/json')
        self.write(json.dumps({"jobs": m.jobs()}))

class JobsHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid> """

    def get(self, *args, **kwargs):
        self.preflight()
        m = self.settings.get('manager')
        sessionid = args[0]

        if not self.api_key.can_manage(sessionid):
            raise HTTPError(403)

        try:
            jobs = list(m.jobs(sessionid))
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())


        # send response
        self.set_header('Content-Type', 'application/json')
        self.write(json.dumps({"sessionid": sessionid,
                               "jobs": jobs}))

    def post(self, *args, **kwargs):
        self.preflight()

        # extract the sessionid from the path.
        sessionid = args[0]

        if not self.api_key.can_manage(sessionid):
            raise HTTPError(403)

        self.set_header('Content-Type', 'application/json')
        try:
            name, cmd, settings = self.fetch_body()
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_request"})
            return

        # do we start the job once the config is loaded? True by default.
        if "start" in settings:
            start = settings.pop("start")
        else:
            start = True

        config = ProcessConfig(name, cmd, **settings)

        # load the config
        m = self.settings.get('manager')
        try:
            m.load(config, sessionid=sessionid, start=start)
        except ProcessError as e :
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write({"ok": True})

    def fetch_body(self):
        obj = json.loads(self.request.body.decode('utf-8'))
        if not "name" or not "cmd" in obj:
            raise ValueError

        name = obj.pop("name")
        cmd = obj.pop("cmd")
        return name, cmd, obj


class JobHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid>/<label> """

    def head(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)

        try:
            m.get(pname)
        except ProcessError:
            return self.set_status(404)

        self.set_status(200)

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)

        try:
            info = m.info(pname)
        except ProcessError:
            self.set_status(404)
            return self.write({"error": "not_found"})

        self.write(info)

    def delete(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        sessionid = args[0]
        name = args[1]

        if not self.api_key.can_manage("%s.%s" % (sessionid, name)):
            raise HTTPError(403)

        try:
            m.unload(name, sessionid)
        except ProcessError:
            self.set_status(404)
            return self.write({"error": "not_found"})

        self.write({"ok": True})

    def put(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        sessionid = args[0]
        name = args[1]

        if not self.api_key.can_manage("%s.%s" % (sessionid, name)):
            raise HTTPError(403)

        try:
            cmd, settings = self.fetch_body(name)
        except ValueError as e:
            self.set_status(400)
            return self.write({"error": "bad_request", "reason": str(e)})

        # do we start the job once the config is loaded? True by default.
        if "start" in settings:
            start = settings.pop("start")
        else:
            start = True

        # create config object
        config = ProcessConfig(name, cmd, **settings)

        try:
            m.update(config, sessionid=sessionid, start=start)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.set_status(201)
        self.write({"ok": True})


    def fetch_body(self, name):
        config = json.loads(self.request.body.decode('utf-8'))
        if "cmd" not in config:
            raise ValueError("invalid process config")

        if 'name' in config:
            if config.get('name') != name:
                raise ValueError("template name conflict with the path")

            del config['name']

        cmd = config.pop("cmd")
        return cmd, config


class JobStatsHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid>/<label>/stats """

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)
        try:
            stats = m.stats(pname)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write(stats)


class ScaleJobHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid>/<label>/numprocesses """

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)

        try:
            t = m._get_locked_state()
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write({"numprocesses": t.numprocesses})

    def post(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_manage(pname):
            raise HTTPError(403)

        try:
            n = self.get_scaling_value()
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_request"})

        try:
            ret = m.scale(pname, n)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write({"numprocesses": ret})

    def get_scaling_value(self):
        obj = json.loads(self.request.body.decode('utf-8'))
        if "scale" not in obj:
            raise ValueError("invalid scaling value")
        return obj['scale']


class PidsJobHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid>/<label>/pids """

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)

        try:
            pids = m.pids(pname)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write({"pids": pids})


class SignalJobHandler(CorsHandlerWithAuth):
    """ /<jobs>/<sessionid>/<label>/signal """


    def post(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_manage(pname):
            raise HTTPError(403)

        try:
            m.kill(pname, self.get_signal_value())
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_request"})
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.send_status(202)
        self.write({"ok": True})

    def get_signal_value(self):
        obj = json.loads(self.request.body.decode('utf-8'))
        if "signal" not in obj:
            raise ValueError("invalid signal value")
        return obj['signal']


class StateJobHandler(CorsHandlerWithAuth):

    def get(self, *args):
        self.preflight()

        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_read(pname):
            raise HTTPError(403)

        try:
            t = m._get_locked_state(pname)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write(str(int(t.active)))

    def post(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')
        pname = "%s.%s" % (args[0], args[1])

        if not self.api_key.can_manage(pname):
            raise HTTPError(403)

        try:
            do = self.get_action(m)
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_request"})

        try:
            do(pname)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.set_status(202)
        self.write({"ok": True})

    def get_action(self, m):
        state = self.request.body
        if state == b'1':
            do = m.start_job
        elif state == b'0':
            do = m.stop_job
        elif state == b'2':
            do = m.reload
        else:
            raise ValueError("invalid state")
        return do


class CommitJobHandler(CorsHandlerWithAuth):
    """ /jobs/<sessionid>/<label>/commit """


    def post(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        if not self.api_key.can_manage(args[0]):
            raise HTTPError(403)

        try:
            graceful_timeout, env = self.get_params()
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_request"})

        try:
            pid = m.commit("%s.%s" % (args[0], args[1]),
                    graceful_timeout=graceful_timeout, env=env)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        self.write({"pid": pid})

    def get_params(self):
        obj = json.loads(self.request.body.decode('utf-8'))
        env = obj.get('env')
        graceful_timeout = obj.get('graceful_timeout')
        if graceful_timeout is not None:
            try:
                graceful_timeout = int(obj.get('graceful_timeout'))
            except TypeError as e:
                raise ValueError(str(e))
        return graceful_timeout, env

########NEW FILE########
__FILENAME__ = keys
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import uuid

from tornado.web import HTTPError

from .util import CorsHandlerWithAuth
from ..keys import KeyConflict, KeyNotFound

class KeysHandler(CorsHandlerWithAuth):

    def get(self, *args):
        if not self.api_key.is_admin():
            raise HTTPError(403)

        if self.get_argument("include_keys", "false").lower() == "true":
            include_key = True
        else:
            include_key = False
        self.write({"keys": self.key_mgr.all_keys(include_key)})

    def post(self, *args):
        if not self.api_key.can_create_key():
            raise HTTPError(403)

        try:
            api_key, data, parent = self.fetch_key()
        except ValueError:
            raise HTTPError(400)

        permissions = data.get('permissions', {})
        if permissions.get('admin') == True and not self.api_key.is_admin():
            raise HTTPError(403)

        try:
            self.key_mgr.set_key(api_key, data, parent=parent)
        except KeyConflict:
            raise HTTPError(409)

        location = '%s://%s/keys/%s' % (self.request.protocol,
                self.request.host, api_key)

        self.set_header("Content-Type", "application/json")
        self.set_header("X-Api-Key", api_key)
        self.set_header("Location", location)
        self.write(json.dumps({"ok": True, "api_key": api_key}))

    def fetch_key(self):
        obj = json.loads(self.request.body.decode('utf-8'))
        key = uuid.uuid4().hex

        # if key id was passed in obj, remove it
        if "key" in obj:
            del obj['key']

        # parent key ?
        parent = None
        if "parent" in obj:
            parent = obj.pop('parent')

        return key, obj, parent


class KeyHandler(CorsHandlerWithAuth):

    def head(self, *args):
        if (not self.api_key.can_create_key() and
                self.api_key.api_key != args[0]):
            # only those who can create keys or the key owner can read the key
            # object
            raise HTTPError(403)

        try:
            key_obj = self.key_mgr.has_key(args[0])
        except KeyNotFound:
            raise HTTPError(404)

        self.set_status(200)

    def get(self, *args):
        if (not self.api_key.can_create_key() and
                self.api_key.api_key != args[0]):
            # only those who can create keys or the key owner can read the key
            # object
            raise HTTPError(403)

        try:
            key_obj = self.key_mgr.get_key(args[0])
        except KeyNotFound:
            raise HTTPError(404)

        if self.get_argument("include_keys", "false").lower() == "true":
            # include subkeys
            subkeys = self.key_mgr.all_subkeys(args[0])
            key_obj['keys'] = subkeys

        self.write(key_obj)

    def delete(self, *args):
        if (not self.api_key.can_create_key() and
                self.api_key.api_key != args[0]):
            # only those who can create keys or the key owner can read the key
            # object
            raise HTTPError(403)

        try:
            key_obj = self.key_mgr.delete_key(args[0])
        except KeyNotFound:
            raise HTTPError(404)

        self.write({"ok": True})

########NEW FILE########
__FILENAME__ = misc
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.


from ... import __version__
from .util import CorsHandler

class WelcomeHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.write({"welcome": "gaffer", "version": __version__})

class VersionHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.write({"name": "gaffer", "version": __version__})

class PingHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.set_status(200)
        self.write("OK")

########NEW FILE########
__FILENAME__ = pid
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import pyuv
from tornado import escape, websocket
from tornado.web import HTTPError

from ...message import Message, decode_frame, make_response
from ...error import ProcessError
from ..keys import Key, DummyKey, KeyNotFound
from .util import CorsHandler, CorsHandlerWithAuth

class AllProcessIdsHandler(CorsHandlerWithAuth):

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        if (not self.api_key.is_admin() and
                not self.api_key.can_manage_all()):
            raise HTTPError(403)

        self.write({"pids": list(m.running)})

class ProcessIdHandler(CorsHandlerWithAuth):

    def head(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        try:
            pid = int(args[0])
        except ValueError:
            self.set_status(400)
            self.write({"error": "bad_value"})
            return

        try:
           p = m.get_process(pid)
        except ProcessError:
            self.set_status(404)
            return

        if not self.api_key.can_read(p.name):
            raise HTTPError(403)

        self.set_status(200)

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        try:
            pid = int(args[0])
        except ValueError:
            self.set_status(400)
            self.write({"error": "bad_value"})
            return

        try:
            p = m.get_process(pid)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        if not self.api_key.can_read(p.name):
            raise HTTPError(403)

        self.write(p.info)

    def delete(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        try:
            pid = int(args[0])
        except ValueError:
            self.set_status(400)
            self.write({"error": "bad_value"})
            return

        try:
            p = m.get_process(pid)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        if not self.api_key.can_manage(p.name):
            raise HTTPError(403)

        try:
            m.stop_process(pid)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        # return the response, we set the status to accepted since the result
        # is async.
        self.set_status(202)
        self.write({"ok": True})


class ProcessIdSignalHandler(CorsHandlerWithAuth):

    def post(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        try:
            pid = int(args[0])
        except ValueError:
            self.set_status(400)
            self.write({"error": "bad_value"})
            return

        # get pidnum
        try:
            p = m.get_process(pid)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        if not self.api_key.can_manage(p.name):
            raise HTTPError(403)


        # decode object
        obj = escape.json_decode(self.request.body)
        try:
            p.kill(obj.get('signal'))
        except ValueError:
            self.set_status(400)
            return self.write({"error": "bad_signal"})


        self.set_status(202)
        self.write({"ok": True})

class ProcessIdStatsHandler(CorsHandlerWithAuth):

    def get(self, *args):
        self.preflight()
        self.set_header('Content-Type', 'application/json')
        m = self.settings.get('manager')

        try:
            pid = int(args[0])
        except ValueError:
            self.set_status(400)
            self.write({"error": "bad_value"})
            return

        # get pidnum
        try:
            p = m.get_process(pid)
        except ProcessError as e:
            self.set_status(e.errno)
            return self.write(e.to_dict())

        if not self.api_key.can_read(p.name):
            raise HTTPError(403)


        self.set_status(200)
        self.write({"stats": p.stats})


class PidChannel(websocket.WebSocketHandler):
    """ bi-directionnal stream handler using a wensocket,
    this handler allows you to read and write to a stream if the operation is
    available """

    def open(self, *args):
        self.manager = self.settings.get('manager')
        self.args = args

        # initialize key handling
        self.require_key = self.settings.get('require_key', False)
        self.key_mgr = self.settings.get('key_mgr')
        self.api_key = None

        try:
            process = self.process = self.manager.get_process(int(args[0]))
        except ProcessError as e:
            self.write_error(e.to_json())
            return self.close()

        # subscribe to exit event so we make sure to close the connection when
        # the process exit.
        self.manager.events.subscribe("proc.%s.exit" % process.pid,
                self.on_exit)

        if not self.require_key:
            self.api_key = DummyKey()
            try:
                self.open_stream(self.process, self.args)
            except ProcessError as e:
                self.write_error(e.to_json())
                self.close()
        else:
            self.opened = False


    def open_stream(self, process, args):
        self._write = None
        self._stream = None
        self._io = None

        # mode is the maskk used to handle this stream. It can be
        # pyuv.UV_READABLE or pyuv.UV_WRITABLE.
        mode = self.mode = int(self.get_argument("mode", 3))
        if len(args) == 1:
            # we try to read from stdout and write to stdin

            # test if we need to read
            if mode & pyuv.UV_READABLE:
                if not self.api_key.can_read(self.process.name):
                    raise ProcessError(403, "FORBIDDEN")

                if not process.redirect_output:
                    raise ProcessError(403, "EPERM")

                self.process.monitor_io(process.redirect_output[0],
                    self.on_output)
                self._stream = process.redirect_output[0]

            # test if we need to write
            if mode & pyuv.UV_WRITABLE:
                if not self.api_key.can_write(self.process.name):
                    raise ProcessError(403, "FORBIDDEN")

                if not process.redirect_input:
                    raise ProcessError(403, "EPERM")
                self._write = process.write

        elif len(args) == 2:
            # a stream name is used
            stream = escape.native_str(args[1])

            if stream in process.redirect_output:
                if mode & pyuv.UV_READABLE:
                    self.process.monitor_io(stream, self.on_output)
                if mode & pyuv.UV_WRITABLE:
                    if not process.redirect_input:
                        raise ProcessError(403, "EPERM")
                    self._write = process.write

            elif stream in process.custom_streams:
                self._io = process.streams[stream]
                self._io.subscribe(self.on_output)
                if mode & pyuv.UV_WRITABLE:
                    self._write = self._io.write
            else:
                raise ProcessError(404, "ENOENT")

            self._stream = stream

        self.opened = True

    def authenticate(self, body):
        if body.startswith(b"AUTH:"):
            key = body.split(b"AUTH:")[1].decode('utf-8')
            try:
                self.api_key = Key.load(self.key_mgr.get_key(key))
            except KeyNotFound:
                raise ProcessError(403, "AUTH_REQUIRED")
        else:
            raise ProcessError(403, "AUTH_REQUIRED")

    def close(self):
        self._close_subscriptions()
        super(PidChannel, self).close()

    def on_message(self, frame):
        # decode the coming msg frame
        msg = decode_frame(frame)

        is_auth = msg.body.startswith(b"AUTH:") == True
        if not self.api_key and self.require_key:
            try:
                self.authenticate(msg.body)
            except ProcessError as e:
                self.write_error(e.to_json())
                self.close()

        if not self.opened:
            try:
                self.open_stream(self.process, self.args)
            except ProcessError as e:
                self.write_error(e.to_json())
                self.close()


        if not is_auth:
            # we can write on this stream, return an error
            if not self._write:
                error = ProcessError(403, "EPERM")
                return self.write_error(error.to_json(), msg.id)

            # send the message
            try:
                self._write(msg.body)
            except Exception:
                error = ProcessError(500, "EIO")
                return self.write_error(error.to_json(), msg.id)


        # send OK response
        resp = make_response("OK", id=msg.id)
        self.write_message(resp.encode())

    def on_output(self, evtype, message):
        msg = Message(message['data'])
        self.write_message(msg.encode())

    def on_close(self):
        self.manager.events.unsubscribe("proc.%s.exit" % self.process.pid,
                self.on_exit)

        # close the subscriptions
        self._close_subscriptions()

    def on_exit(self):
        self.close()

    def write_error(self, error_msg, msgid=None):
        msgid = msgid or b"gaffer_error"

        msg = Message(error_msg, id=msgid, type=b'error')
        self.write_message(msg.encode())

    def _close_subscriptions(self):
        self.manager.events.unsubscribe("proc.%s.exit" % self.process.pid,
                self.on_exit)

        # unsubscribe reads
        if self._stream is not None:
            if self._stream in self.process.redirect_output:
                self.process.unmonitor_io(self._stream, self.on_output)
            else:
                self._io.unsubscribe(self.on_output)

########NEW FILE########
__FILENAME__ = user
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import uuid

from tornado.web import HTTPError

from .util import CorsHandlerWithAuth
from ..users import UserNotFound, UserConflict


class UsersHandler(CorsHandlerWithAuth):

    def get(self, *args):
        if not self.api_key.is_admin():
            raise HTTPError(403)

        if self.get_argument("include_user", "false").lower() == "true":
            include_user = True
        else:
            include_user = False
        self.write({"users":  self.auth_mgr.all_users(
            include_user=include_user)})


    def post(self, *args):
        if not self.api_key.can_create_user():
            raise HTTPError(403)

        try:
            username, password, user_type, key, extra = self.fetch_user()
        except ValueError:
            raise HTTPError(400)

        try:
            self.auth_mgr.create_user(username, password, user_type=user_type,
                    key=key, extra=extra)
        except UserConflict:
            raise HTTPError(409)

        self.write({"ok": True})

    def fetch_user(self, update=False):
        obj = json.loads(self.request.body.decode('utf-8'))

        if not update:
            try:
                username = obj.pop('username')
            except KeyError:
                raise ValueError()
        elif "username" in obj:
            del obj['username']

        try:
            password = obj.pop('password')
        except KeyError:
            raise ValueError()

        user_type = obj.pop('user_type', 1)
        if user_type == 0 and not self.api_key.is_admin():
            raise HTTPError(403)

        key = obj.pop('key', None)

        if not update:
            return username, password, user_type, key, obj
        return password, user_type, key, obj


class UserHandler(UsersHandler):

    def head(self, *args):
        # only the key with can_create_user or key associated to a username
        # can fetch the user details
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(404)

        if not self.auth_mgr.has_user(args[0]):
            self.set_status(404)
        else:
            self.set_status(200)


    def get(self, *args):
        # only the key with can_create_user or key associated to a username
        # can fetch the user details
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(403)

        try:
            self.write(self.auth_mgr.get_user(args[0]))
        except UserNotFound:
            raise HTTPError(404)

    def delete(self, *args):
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(403)

        if not self.auth_mgr.has_user(args[0]):
            raise HTTPError(404)

        try:
            self.auth_mgr.delete_user(args[0])
        except UserNotFound:
            raise HTTPError(404)

        self.write({"ok": True})

    def put(self, *args):
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(403)

        try:
            password, user_type, key, extra = self.fetch_user(update=True)
        except ValueError:
            raise HTTPError(400)

        try:
            self.auth_mgr.update_user(args[0], password, user_type=user_type,
                    key=key, extra=extra)
        except UserConflict:
            raise HTTPError(409)

        self.write({"ok": True})


class UserPasswordHandler(CorsHandlerWithAuth):

    def put(self, *args):
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(403)

        obj = json.loads(self.request.body.decode('utf-8'))
        if obj.get("password", None) is None:
            raise HTTPError(400)

        try:
            self.auth_mgr.set_password(args[0], obj['password'])
        except UserNotFound:
            raise HTTPError(404)

        self.write({"ok": True})

class UserKeydHandler(CorsHandlerWithAuth):

    def put(self, *args):
        if not self.api_key.can_create_user() and self.key_username != args[0]:
            raise HTTPError(403)

        obj = json.loads(self.request.body.decode('utf-8'))
        if obj.get("key", None) is None:
            raise HTTPError(400)

        try:
            self.auth_mgr.set_key(args[0], obj['key'])
        except UserNotFound:
            raise HTTPError(404)

        self.write({"ok": True})

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -
#
# this file is part of gaffer. see the notice for more information.

try:
    import httplib
except ImportError:
    import http.client as httplib
import json

import pyuv
from tornado.web import RequestHandler, asynchronous, HTTPError
from ..keys import DummyKey, Key, KeyNotFound
from ..users import UserNotFound

ACCESS_CONTROL_HEADERS = ['X-Requested-With',
            'X-HTTP-Method-Override', 'Content-Type', 'Accept',
            'Authorization']

CORS_HEADERS = {
    'Access-Control-Allow-Methods' : 'POST, GET, PUT, DELETE, OPTIONS',
    'Access-Control-Max-Age'       : '86400', # 24 hours
    'Access-Control-Allow-Headers' : ", ".join(ACCESS_CONTROL_HEADERS),
    'Access-Control-Allow-Credentials': 'true'
}


class CorsHandler(RequestHandler):

    @asynchronous
    def options(self, *args, **kwargs):
        self.preflight()
        self.set_status(204)
        self.finish()

    def preflight(self):
        origin = self.request.headers.get('Origin', '*')

        if origin == 'null':
            origin = '*'

        self.set_header('Access-Control-Allow-Origin', origin)
        for k, v in CORS_HEADERS.items():
            self.set_header(k, v)

    def get_error_html(self, status_code, **kwargs):
        self.set_header("Content-Type", "application/json")

        if status_code == 400:
            resp = {"error": 400, "reason": "bad_request"}
        elif status_code == 404:
            resp = {"error": 404, "reason": "not_found"}
        elif status_code == 409:
            resp = {"error": 409, "reason": "conflict"}
        elif status_code == 401:
            resp = {"error": 401, "reason": "unauthorized"}
        elif status_code == 403:
            resp = {"error": 403, "reason": "forbidden"}
        else:
            resp = {"error": status_code,
                    "reason": httplib.responses[status_code]}

        if self.settings.get("debug") and "exc_info" in kwargs:
            exc_info = traceback.format_exception(*kwargs["exc_info"])
            resp['exc_info'] = exc_info

        return json.dumps(resp)


class CorsHandlerWithAuth(CorsHandler):

    def prepare(self):
        api_key = self.request.headers.get('X-Api-Key', None)
        require_key = self.settings.get('require_key', False)
        self.auth_mgr = self.settings.get('auth_mgr')
        key_mgr = self.key_mgr = self.settings.get('key_mgr')
        self.api_key = DummyKey()
        self.key_username = None

        # if the key API is enable start to use it
        if require_key:
            if api_key is not None:
                try:
                    self.api_key = Key.load(key_mgr.get_key(api_key))
                except KeyNotFound:
                    raise HTTPError(403, "key %s doesn't exist",api_key)

                try:
                    user_obj = self.auth_mgr.user_by_key(api_key)
                    self.key_username = user_obj["username"]
                except UserNotFound:
                    pass
            else:
                raise HTTPError(401)

########NEW FILE########
__FILENAME__ = keys
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import deque
import os
import json
import sqlite3
import uuid

from ..events import EventEmitter
from .util import load_backend


class KeyNotFound(Exception):
    """ exception raised when the key isn't found """


class KeyConflict(Exception):
    """ exception when you try to create a key that already exists """


class InvalidKey(Exception):
    """ exception raised when the key is invalid """


class UnknownPermission(Exception):
    """ raised when the permission is not found """


class Key(object):
    """ instance representing a key """

    def __init__(self, api_key, label="", permissions={}):
        self.api_key = api_key
        self.label = label
        self.permissions = permissions

        # parse permissions
        self.manage = permissions.get('manage', None) or {}
        self.write = permissions.get('write', None) or {}
        self.read = permissions.get('read', None) or {}

    def __str__(self):
        return "Key: %s" % self.api_key

    @classmethod
    def load(cls, obj):
        if not "key" in obj:
            raise InvalidKey()

        key = obj['key']
        label = obj.get('label', "")
        permissions = obj.get("permissions", {})
        return cls(key, label, permissions)

    def dump(self):
        return {"key": self.api_key, "label": self.label, "permissions":
                self.permissions}

    def is_admin(self):
        """ does this key has all rights? """
        return self.permissions.get("admin", False)

    def can_create_key(self):
        """ can we create new keys with this key?

        Note only a user key can create keys able to create other keys. Sub
        keys can't create keys.
        """
        if self.is_admin():
            return True

        return self.permissions.get("create_key", False)

    def can_create_user(self):
        """ can we create users with this key ? """
        if self.is_admin():
            return True

        return self.permissions.get("create_user", False)

    def can_manage_all(self):
        return '*' in self.manage or self.is_admin()

    def can_write_all(self):
        return '*' in self.write or self.can_manage_all()

    def can_read_all(self):
        return '*' in self.read or self.can_write_all()

    def can_manage(self, job_or_session):
        """ test if a user can manage a job or a session

        managing a session means:
        - load/unload/update job in this session
        - start/stop processes and jobs in a session
        - list
        """

        return self.can('manage', job_or_session)

    def can_write(self, job_or_session):
        """ test if a user can write to a process for this job or all the jobs
        of the session """
        if self.can_manage(job_or_session):
            return True

        return self.can('write', job_or_session)

    def can_read(self, job_or_session):
        """ test if a user can read from a process for this job or all the jobs
        of the session """

        if self.can_write(job_or_session):
            return True

        return self.can('read', job_or_session)

    def can(self, permission, what):
        """ test the permission for a job or a session """
        if not hasattr(self, permission):
            raise UnknownPermission("%r does not exist")

        # get all permissions
        permissions = getattr(self, permission, {})

        # check if we we have the permission on all resources
        if '*' in permissions or self.is_admin():
            return True

        if "." in what:
            # we are testing job possibilities. The try first to know if we
            # have the permissions on the session
            session = what.split(".")[0]
            if session in permissions:
                return True

        # test the job permission
        if what in getattr(self, permission, {}):
            return True

        return False


class DummyKey(Key):

    def __init__(self):
        super(DummyKey, self).__init__("dummy")

    def can_create_key(self):
        return False

    def can_create_user(self):
        return False

    def is_admin(self):
        return True

    def can_manage_all(self):
        return True

    def can_write_all(self):
        return True

    def can_read_all(self):
        return True

    def can(self, permissions, what):
        return True


class KeyManager(object):

    def __init__(self, loop, cfg):
        self.loop = loop
        self.cfg = cfg
        self._cache = {}
        self._entries = deque()

        # initialize the db backend
        if not cfg.keys_backend or cfg.keys_backend == "default":
            self._backend = SqliteKeyBackend(loop, cfg)
        else:
            self._backend = load_backend(cfg.keys_backend)

        # initialize the events listenr
        self._emitter = EventEmitter(loop)

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        try:
            self.close()
        except:
            pass

    def subscribe(self, event, listener):
        self._emitter.subscribe(event, listener)

    def unsubscribe(self, event, listener):
        self._emitter.unsubscribe(event, listener)

    def open(self):
        self._backend.open()
        self._emitter.publish("open", self)

    def close(self):
        self._emitter.publish("close", self)
        self._backend.close()
        self._emitter.close()

        # empty the cache
        self._entries.clear()
        self._cache = {}

    def all_keys(self, include_key=False):
        return self._backend.all_keys(include_key=include_key)

    def create_key(self, permissions, key=None, label="", parent=None):
        key = key or uuid.uuid4().hex
        data = {"permissions": permissions}
        if label and label is not None:
            data['label'] = label

        self.set_key(key, data, parent=parent)
        return key


    def set_key(self, key, data, parent=None):
        self._backend.set_key(key, data, parent=parent)
        self._emitter.publish("set", self, key)

    def get_key(self, key):
        if key in self._cache:
            return self._cache[key]

        okey = self._backend.get_key(key)

        # do we need to clean the cache?
        # we only keep last 1000 acceded keys in RAM
        if len(self._cache) >= 1000:
            to_remove = self._entries.popleft()
            self._cache.pop(to_remove)

        # enter last entry in the cache
        self._cache[key] = okey
        self._entries.append(key)
        return okey

    def delete_key(self, key):
        # remove the key and all sub keys from the cache if needed
        self._delete_entry(key)

        for subkey in self.all_subkeys(key):
            self._delete_entry(subkey["key"])

        # then delete the
        self._backend.delete_key(key)
        self._emitter.publish("delete", self, key)

    def has_key(self, key):
        return self._backend.has_key(key)

    def all_subkeys(self, key):
        return self._backend.all_subkeys(key)

    def _delete_entry(self, key):
        if key in self._cache:
            self._entries.remove(key)
            self._cache.pop(key)


class KeyBackend(object):

    def __init__(self, loop, cfg):
        self.loop = loop
        self.cfg = cfg

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        try:
            self.close()
        except:
            pass

    def open(self):
        raise NotImplementedError

    def close(self):
        raise NotImplementedError

    def all_keys(self):
        raise NotImplementedError

    def set_key(self, key, data):
        raise NotImplementedError

    def get_key(self, key):
        raise NotImplementedError

    def delete_key(self, key):
        raise NotImplementedError

    def has_key(self, key):
        raise NotImplementedError

    def all_subkeys(self, key):
        raise NotImplementedError


class SqliteKeyBackend(KeyBackend):
    """ sqlite backend to store API keys in gaffer """


    def __init__(self, loop, cfg):
        super(SqliteKeyBackend, self).__init__(loop, cfg)

        # set dbname
        self.dbname = cfg.keys_dbname or "keys.db"
        if self.dbname != ":memory:":
            self.dbname = os.path.join(cfg.config_dir, self.dbname)

        # intitialize conn
        self.conn = None

    def open(self):
        self.conn = sqlite3.connect(self.dbname)
        with self.conn:
            sql = """CREATE TABLE if not exists keys (key text primary key,
            data text, parent text)"""
            self.conn.execute(sql)

    def close(self):
        self.conn.commit()
        self.conn.close()

    def all_keys(self, include_key=False):
        with self.conn:
            cur = self.conn.cursor()
            rows = cur.execute("SELECT * FROM keys", [])
            if include_key:
                return [self._make_key(row) for row in rows]
            else:
                return [row[0] for row in rows]

    def set_key(self, key, data, parent=None):
        assert self.conn is not None
        if isinstance(data, dict):
            data = json.dumps(data)

        with self.conn:
            cur = self.conn.cursor()
            try:
                cur.execute("INSERT INTO keys VALUES (?, ?, ?)", [key,
                    data, parent])
            except sqlite3.IntegrityError:
                raise KeyConflict()

    def get_key(self, key, subkeys=True):
        assert self.conn is not None

        with self.conn:
            cur = self.conn.cursor()
            cur.execute("SELECT * FROM keys WHERE key=?", [key])
            row = cur.fetchone()

        if not row:
            raise KeyNotFound()
        return self._make_key(row)

    def delete_key(self, key):
        assert self.conn is not None
        with self.conn:
            self.conn.execute("DELETE FROM keys WHERE key=? OR parent=?",
                    [key, key])

    def has_key(self, key):
        try:
            self.get_key(key)
        except KeyNotFound:
            return False
        return True

    def all_subkeys(self, key):
        with self.conn:
            cur = self.conn.cursor()
            rows = cur.execute("SELECT * FROM keys WHERE parent=?", [key])
            return [self._make_key(row) for row in rows]

    def _make_key(self, row):
        obj = json.loads(row[1])
        obj.update({ "key": row[0] })
        return obj

########NEW FILE########
__FILENAME__ = lookup
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import logging
from threading import RLock
import uuid

import pyuv

from ..httpclient.websocket import WebSocket

LOGGER = logging.getLogger("gaffer")

class Message(object):

    def __init__(self, msg, callback=None):
        self.id = uuid.uuid4().hex
        self.msg = msg
        self.msg['msgid'] = self.id
        self.callback = None
        self._result = None

    def __str__(self):
        return "%s: %s" % (self.__class__.__name__, self.id)

    def done(self):
        return self._result is not None

    def result(self):
        return self._result

    def reply(self, msg):
        self._result = msg
        if self.callback is not None:
            try:
                self.callback(msg)
            except Exception:
                LOGGER.exception('exception calling callback for %r', self)

    def to_json(self):
        return json.dumps(self.msg)


class LookupClient(WebSocket):

    def __init__(self, loop, url, **kwargs):
        loop = loop
        self._lock = RLock()

        # initialize the heartbeart. It will PING the lookupd server to say
        # it's alive
        try:
            self.heartbeat_timeout = kwargs.pop('heartbeat')
        except KeyError:
            self.heartbeat_timeout = 15.0
        self._heartbeat = pyuv.Timer(loop)

        # define status
        self.active = False
        self.closed = False

        # dict to maintain list of sent messages to handle replies from the
        # lookupd server
        self.messages = dict()

        super(LookupClient, self).__init__(loop, url, **kwargs)

    def start(self, on_exit_cb=None):
        # set the exit callabck
        self.exit_cb = on_exit_cb

        # already started, return
        if self.active:
            return
        super(LookupClient, self).start()
        self.active = True

    def close(self):
        self._heartbeat.stop()
        self.closed = True
        self.active = False
        super(LookupClient, self).close()

    def ping(self):
        if self.closed:
            return
        return self.write_message({"type": "PING"})

    def identify(self, name, broadcast_address, version,
            callback=None):
        return self.write_message({"type": "IDENTIFY", "name": name,
            "origin": broadcast_address, "version": version},
            callback=callback)

    def add_job(self, job_name, callback=None):
        return self.write_message({"type": "REGISTER_JOB",
            "job_name": job_name}, callback=callback)

    def remove_job(self, job_name, callback=None):
        return self.write_message({"type": "UNREGISTER_JOB",
            "job_name": job_name}, callback=callback)

    def add_process(self, job_name, pid, callback=None):
        return self.write_message({"type": "REGISTER_PROCESS",
            "job_name": job_name, "pid": pid}, callback=callback)

    def remove_process(self, job_name, pid, callback=None):
        return self.write_message({"type": "UNREGISTER_PROCESS",
            "job_name": job_name, "pid": pid}, callback=callback)

    ### websocket methods

    def on_message(self, message):
        try:
            result = json.loads(message)
        except ValueError as e:
            LOGGER.error('invalid json: %r' % str(e))
            return

        msgid = result.get('msgid')
        if not msgid:
            LOGGER.error('invalid message: %r' % str(e))
            return

        try:
            msg = self.messages.pop(msgid)
        except KeyError:
            return

        msg.reply(result)

    def on_open(self):
        # start the heartbeat
        self._heartbeat.start(self.on_heartbeat, self.heartbeat_timeout,
                self.heartbeat_timeout)
        self._heartbeat.unref()

    def on_close(self):
        self.active = False
        self.closed = True
        self._heartbeat.stop()

        # call exit the callback
        if self.exit_cb is not None:
            try:
                self.exit_cb(self)
            except Exception:
                LOGGER.exception('exception calling exit callback for %r',
                        self)

    def on_heartbeat(self, h):
        # on heartbeat send a `PING` message to the channel
        # it will maintain the connection open
        self.ping()

    def write_message(self, message, callback=None):
        if isinstance(message, bytes):
            super(LookupClient, self).write_message(message)
            return

        msg = Message(message, callback=callback)

        # store the message to handle the reply
        with self._lock:
            self.messages[msg.id] = msg

        super(LookupClient, self).write_message(msg.to_json())
        return msg

########NEW FILE########
__FILENAME__ = main
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
usage: gafferd [--version] [-v|-vv] [-c CONFIG|--config=CONFIG]
               [-p PLUGINS_DIR|--plugin-dir=PLUGINS_DIR]
               [--daemon] [--pidfile=PIDFILE]
               [--bind=ADDRESS] [--lookupd-address=LOOKUP]...
               [--broadcast-address=ADDR]
               [--certfile=CERTFILE] [--keyfile=KEYFILE]
               [--cacert=CACERT]
               [--client-certfile=CERTFILE] [--client-keyfile=KEYFILE]
               [--backlog=BACKLOG]
               [--error-log=FILE] [--log-level=LEVEL]
               [--require-key]
               [--create-admin-user] [--username USER] [--password PASSWORD]
               [--list-admins]


Args

    CONFIG                    configuration file path

Options

    -h --help                   show this help message and exit
    --version                   show version and exit
    -v -vv                      verbose mode
    -c CONFIG --config=CONFIG   configuration dir
    -p DIR --plugin-dir=DIR     plugin dir
    --daemon                    Start gaffer in daemon mode
    --pidfile=PIDFILE
    --bind=ADDRESS              default HTTP binding (default: 0.0.0.0:5000)
    --lookupd-address=LOOKUP    lookupd HTTP address
    --broadcast-address=ADDR    the address for this node. This is registered
                                with gaffer_lookupd (defaults to OS hostname)
    --broadcast-port=PORT       The port that will be registered with
                                gaffer_lookupd (defaults to local port)
    --certfile=CERTFILE         SSL certificate file for the default binding
    --keyfile=KEYFILE           SSL key file
    --client-certfile=CERTFILE  SSL client certificate file (to connect to the
                                lookup server)
    --client-keyfile=KEYFILE    SSL client key file
    --cacert=CACERT             SSL CA certificate
    --backlog=BACKLOG           default backlog (default: 128).
    --error-log=FILE            logging file
    --log-level=LEVEL           logging level (critical, error warning, info,
                                debug)
    --require-key               enable the key authorization API
    --create-admin-user         create an admin user and exit
    --username USERNAME         admin username
    --password PASSWORD         admin password
    --list-admins               list admin users
"""



from getpass import getpass
import os
import logging
import sys
import uuid

import pyuv

from .. import __version__
from ..console_output import ConsoleOutput
from ..docopt import docopt
from ..error import ProcessError
from ..manager import Manager
from ..pidfile import Pidfile
from ..process import ProcessConfig
from ..sig_handler import SigHandler
from ..util import daemonize, setproctitle_
from ..webhooks import WebHooks
from .config import ConfigError, Config
from .http import HttpHandler
from .keys import KeyManager
from .plugins import PluginManager
from .users import AuthManager
from .util import user_path, system_path, default_path, is_admin, confirm


LOG_LEVELS = {
        "critical": logging.CRITICAL,
        "error": logging.ERROR,
        "warning": logging.WARNING,
        "info": logging.INFO,
        "debug": logging.DEBUG}

LOG_ERROR_FMT = r"%(asctime)s [%(process)d] [%(levelname)s] %(message)s"
LOG_DATEFMT = r"%Y-%m-%d %H:%M:%S"


class Server(object):
    """ Server object used for gafferd """

    def __init__(self, args):
        self.args = args
        # get config dir
        self.config_dir = self.find_configdir()
        if not os.path.exists(self.config_dir):
            os.makedirs(self.config_dir)
        elif not os.path.isdir(self.config_dir):
            raise RuntimeError("%r isn't a directory" % self.config_dir)

        self.cfg = Config(args, self.config_dir)
        self.plugins = []


    def start(self, loop, manager):
        # the server is responsible of launching plugins instead of the
        # manager so we can manage the configuratino change
        self.plugin_manager.start_apps(self.cfg, loop, manager)

    def stop(self):
        # stop all plugins apps
        self.plugin_manager.stop_apps()


    def restart(self):
        logging.info("reload config")
        try:
            self.do_restart()
        except Exception as e:
            logging.error('Uncaught exception when stopping a plugin',
                        exc_info=True)

    def do_restart(self):
        try:
            jobs_removed, webhooks_removed = self.cfg.reload()
        except ConfigError as e:
            # if on restart we fail to parse the config then just return and
            # do nothing
            logging.error("failed parsing config: %s" % str(e))
            logging.info("config not reloaded")
            return

        # remove jobs config from the manager
        for jobname, sessionid in jobs_removed:
            self.manager.unload(jobname, sessionid=sessionid)

        # load or update job configs
        for name, sessionid, cmd, params in self.cfg.processes:
            if "start" in params:
                # on restart we don't start the loaded jobs. They will be
                # handled later by gaffer so just remove this param
                params.pop("start")

            config = ProcessConfig(name, cmd, **params)

            # so we need to update a job config
            update = True
            try:
                self.manager.get("%s.%s" % (sessionid, name))
            except ProcessError:
                update = False

            if update:
                self.manager.update(config, sessionid=sessionid)
            else:
                self.manager.load(config, sessionid=sessionid)

        # unregister hooks
        for event, url in webhooks_removed:
            self.webhook_app.unregister(event, url)

        # restart plugins
        self.plugin_manager.restart_apps(self.cfg, self.manager.loop,
                self.manager)

    def run(self):
        # load config
        self.cfg.load()

        # check if we need to create an admin
        if self.args['--create-admin-user']:
            return self.create_admin_user()

        # list admin
        if self.args['--list-admins']:
            return self.list_admins()


        # do we need to daemonize the daemon
        if self.cfg.daemonize:
            daemonize()

        # fix the process name
        setproctitle_("gafferd")

        # setup the pidfile
        pidfile = None
        if self.cfg.pidfile:
            pidfile = Pidfile(self.cfg.pidfile)
            try:
                pidfile.create(os.getpid())
            except RuntimeError as e:
                print(str(e))
                sys.exit(1)

        # initialize the plugin manager
        self.plugin_manager = PluginManager(self.cfg.plugin_dir)

        # check if any plugin dependancy is missing
        self.plugin_manager.check_mandatory()

        # initialize the manager
        self.manager = Manager()

        # initialize apps
        self.http_handler = HttpHandler(self.cfg, self.plugin_manager)
        self.webhook_app = WebHooks(hooks=self.cfg.webhooks)

        # setup gaffer apps
        apps = [self,
                SigHandler(),
                self.webhook_app,
                self.http_handler]

        # verbose mode
        if self.args["-v"] == 2:
            apps.append(ConsoleOutput(actions=['.']))
        elif self.args["-v"] == 1:
            apps.append(ConsoleOutput(output_streams=False))

        self.set_logging()

        # really start the server
        self.manager.start(apps=apps)

        # load job configs
        for name, sessionid, cmd, params in self.cfg.processes:
            if "start" in params:
                start = params.pop("start")
            else:
                start = True

            config = ProcessConfig(name, cmd, **params)
            self.manager.load(config, sessionid=sessionid, start=start)

        # run the main loop
        try:
            self.manager.run()
        except KeyboardInterrupt:
            pass
        except Exception as e:
            print("error: %s" % str(e))
            sys.exit(1)
        finally:
            if pidfile is not None:
                pidfile.unlink()

    def find_configdir(self):
        if self.args.get('--config') is not None:
            return self.args.get('--config')

        if 'GAFFERD_CONFIG' in os.environ:
            return os.environ.get('GAFFERD_CONFIG')

        if is_admin():
            default_paths = system_path()
        else:
            default_paths = user_path()

        for path in default_paths:
            if os.path.isdir(path):
                return path

        return default_path()

    def set_logging(self):
        logger = logging.getLogger()

        handlers = []
        if self.cfg.logfile is not None and self.cfg.logfile != "-":
            handlers.append(logging.FileHandler(self.cfg.logfile))
        else:
            handlers.append(logging.StreamHandler())

        loglevel = LOG_LEVELS.get(self.cfg.loglevel.lower(), logging.INFO)
        logger.setLevel(loglevel)

        format = r"%(asctime)s [%(process)d] [%(levelname)s] %(message)s"
        datefmt = r"%Y-%m-%d %H:%M:%S"
        for h in handlers:
            h.setFormatter(logging.Formatter(format, datefmt))
            logger.addHandler(h)

    def create_admin_user(self):
        loop = pyuv.Loop.default_loop()
        key_mgr = KeyManager(loop, self.cfg)
        auth_mgr = AuthManager(loop, self.cfg)


        # open the auth db
        try:
            auth_mgr.open()
        except:
            logging.error("error while creating an admin",
                    exc_info=True)
            sys.exit(1)

        try:
            username = self.args['--username']
            password = self.args['--password']
            confirm_password = None

            if username and username is not None:
                if auth_mgr.has_user(username):
                    print("username %r already exists. " % username
                            + "Please choose another")
                    sys.exit(1)
            elif username == "":
                print("Error: username is empty")
                sys.exit(1)

            if password == "":
                print("Error: password is empty")

            if not username:
                while True:
                    username = input("username: ").lower()
                    if username and username is not None:
                        if auth_mgr.has_user(username):
                            print("username %r already exists. " % username
                                    + "Please enter another")
                            continue
                        break

                    print("username is empty. Please enter a username.")

            if not password:
                while True:
                    password = getpass("password: ")
                    if password and password is not None:
                        break

                    print("password is empty. Please enter a password.")

                # confirm password

                while True:
                    confirm_password = getpass("confirm password: ")
                    if confirm_password == password:
                        break

                    print("Passwords are different.")
            else:
                confirm_password = password

        except KeyboardInterrupt:
            print("")

        if not username or not password or (confirm_password != password):
            print("admin not created.")
            sys.exit(0)

        # create an admin key
        permissions = {"admin": True}
        try:
            key_mgr.open()
            api_key = key_mgr.create_key(permissions, label="admin key")
        except:
            logging.error("error while creating an admin key",
                    exc_info=True)
            sys.exit(0)
        finally:
            try:
                key_mgr.close()
            except:
                pass

        # create a user
        try:
            auth_mgr.create_user(username, password, user_type=0,
                    key=api_key)
        except:
            logging.error("error while creating an admin",
                    exc_info=True)
            sys.exit(0)
        finally:
            try:
                auth_mgr.close()
            except:
                pass


        print("User %r created.\nAPI Key: %s" % (username, api_key))

    def list_admins(self):
        loop = pyuv.Loop.default_loop()
        auth_mgr = AuthManager(loop, self.cfg)

        # open the auth db
        try:
            auth_mgr.open()
        except:
            logging.error("error while creating an admin",
                    exc_info=True)
            sys.exit(1)

        users = auth_mgr.user_by_type(0)
        for user in users:
            print("%s - %s" % (user['username'], user["key"]))


def run():
    args = docopt(__doc__, version=__version__)
    try:
        s = Server(args)
        s.run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        import traceback
        print(traceback.format_exc())
        print("error: %s" % str(e))
        sys.exit(1)

    sys.exit(0)

if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = pbkdf2
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
    pbkdf2
    ~~~~~~

    This module implements pbkdf2 for Python.  It also has some basic
    tests that ensure that it works.  The implementation is straightforward
    and uses stdlib only stuff and can be easily be copy/pasted into
    your favourite application.

    Use this as replacement for bcrypt that does not need a c implementation
    of a modified blowfish crypto algo.

    Example usage:

    >>> pbkdf2_hex('what i want to hash', 'the random salt')
    'fa7cc8a2b0a932f8e6ea42f9787e9d36e592e0c222ada6a9'

    How to use this:

    1.  Use a constant time string compare function to compare the stored hash
        with the one you're generating::

            def safe_str_cmp(a, b):
                if len(a) != len(b):
                    return False
                rv = 0
                for x, y in zip(a, b):
                    rv |= ord(x) ^ ord(y)
                return rv == 0

    2.  Use `os.urandom` to generate a proper salt of at least 8 byte.
        Use a unique salt per hashed password.

    3.  Store ``algorithm$salt:costfactor$hash`` in the database so that
        you can upgrade later easily to a different algorithm if you need
        one.  For instance ``PBKDF2-256$thesalt:10000$deadbeef...``.


    :copyright: (c) Copyright 2011 by Armin Ronacher.
    :license: BSD, see NOTIC for more details.

    Adapted to Python 3 by Benot Chesneau
"""
import binascii
import hmac
import hashlib
from struct import Struct
from operator import xor
from itertools import starmap

import six
from six.moves import zip

_pack_int = Struct('>I').pack

if six.PY3:
    def _ord(c):
        if isinstance(c, int):
            return c
        return ord(c)
else:
    _ord = ord


def pbkdf2_hex(data, salt, iterations=1000, keylen=24, hashfunc=None):
    """Like :func:`pbkdf2_bin` but returns a hex encoded string."""
    bin = pbkdf2_bin(data, salt, iterations, keylen, hashfunc)
    return binascii.hexlify(bin)

def pbkdf2_bin(data, salt, iterations=1000, keylen=24, hashfunc=None):
    """Returns a binary digest for the PBKDF2 hash algorithm of `data`
    with the given `salt`.  It iterates `iterations` time and produces a
    key of `keylen` bytes.  By default SHA-1 is used as hash function,
    a different hashlib `hashfunc` can be provided.
    """
    hashfunc = hashfunc or hashlib.sha1
    mac = hmac.new(data, None, hashfunc)
    def _pseudorandom(x, mac=mac):
        h = mac.copy()
        h.update(x)
        return [_ord(c) for c in  h.digest()]
    buf = []
    for block in range(1, -(-keylen // mac.digest_size) + 1):
        rv = u = _pseudorandom(salt + _pack_int(block))
        for i in range(iterations - 1):
            if six.PY3:
                u = _pseudorandom(bytes(u))
            else:
                u = _pseudorandom(b''.join([chr(c) for c in u]))
            rv = starmap(xor, zip(rv, u))
        buf.extend(rv)

    if six.PY3:
        return bytes(buf)[:keylen]

    return ''.join(map(chr, buf))[:keylen]

########NEW FILE########
__FILENAME__ = plugins
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import copy
import importlib
import logging
import os
import sys

from tornado import web

class Plugin(object):
    """ basic plugin interfacce """

    name = ""
    version = "?"
    descripton = ""
    mandatory = []

    def app(self, cfg):
        """ return a gaffer application """
        return None


class PluginDir(object):

    def __init__(self, name, rootdir):
        self.name = name
        self.root = rootdir
        self.plugins = []
        self.names = []
        self.mandatory = []

        # load plugins
        self._scan()

        site_path = os.path.join(rootdir, '_site')
        if os.path.isdir(site_path):
            self.site = site_path
        else:
            self.site = None

        if not self.plugins and self.site is None:
            if os.path.isfile(os.path.join(rootdir, 'index.html')):
                self.site = rootdir


    def _scan(self):
        plugins = []
        dirs = []

        # initial pass, read
        for name in os.listdir(self.root):
            if name in (".", "..",):
                continue

            path = os.path.join(self.root, name)
            if (os.path.isdir(path) and
                    os.path.isfile(os.path.join(path, '__init__.py'))):
                # no conflict
                if path not in sys.path:
                    dirs.append((name, path))
                    sys.path.insert(0, os.path.join(self.root, '..',
                        name))
                    sys.path.insert(0, os.path.join(self.root,  name))

        for (name, d) in dirs:
            try:
                for f in os.listdir(d):
                    if f.endswith(".py") and f != "__init__.py":
                        plugins.append(("%s.%s" % (name, f[:-3]), d))
            except OSError:
                sys.stderr.write("error, loading %s" % f)
                sys.stderr.flush()
                continue

        mod = None
        for (name, d) in plugins:
            mod = importlib.import_module(name)
            if hasattr(mod, "__all__"):
                for attr in mod.__all__:
                    plug = getattr(mod, attr)
                    if issubclass(plug, Plugin):
                        self._load_plugin(plug())

    def _load_plugin(self, plug):
        if not plug.name:
            raise RuntimeError("invalid plugin: %s [%s]" % (self.name,
                self.root))

        self.plugins.append(plug)
        self.names.append(plug.name)
        self.mandatory.extend(plug.mandatory or [])


class PluginManager(object):

    def __init__(self, plugin_dir):
        self.plugin_dir = plugin_dir
        self.plugins = {}
        self.installed = []

        self.apps = []
        # scan all plugins
        self.scan()

    def scan(self):
        if not os.path.isdir(self.plugin_dir):
            logging.info("plugging dir %r not found" % self.plugin_dir)
            return

        for name in os.listdir(self.plugin_dir):
            if name in (".", "..",):
                continue
            path = os.path.abspath(os.path.join(self.plugin_dir, name))
            if os.path.isdir(path):
                plug = PluginDir(name, path)
                self.plugins[name] = plug
                self.installed.extend(plug.names)

    def check_mandatory(self):
        sinstalled = set(self.installed)
        for name, plug in self.plugins.items():
            smandatory = set(plug.mandatory)
            diff = smandatory.difference(sinstalled)
            if diff:
                raise RuntimeError("%s requires %s to be used" % (name,
                    diff))

    def get_sites(self):
        handlers = []
        for name, plug in self.plugins.items():
            if plug.site is not None:
                static_path = r"/_plugin/%s/(.*)" % name
                rule = (static_path, web.StaticFileHandler,
                        {"path": plug.site,
                         "default_filename": "index.html"})
                handlers.append(rule)
        return handlers

    def init_apps(self, cfg):
        for name, plugdir in self.plugins.items():
            for plug in plugdir.plugins:
                app = plug.app(cfg)
                if app is not None:
                    self.apps.append((app, plug))
        return self.apps

    ### apps handling

    def start_apps(self, config, loop, manager):
        apps = self.init_apps(config)
        for app, _plug in apps:
            try:
                app.start(loop, manager)
            except Exception:
                # we ignore all exception
                logging.error('Uncaught exception when starting a plugin',
                        exc_info=True)

    def stop_apps(self):
        for app, _plug in self.apps:
            try:
                app.stop()
            except Exception:
                # we ignore all exception
                logging.error('Uncaught exception when stopping a plugin',
                        exc_info=True)

        self.apps = []

    def restart_apps(self, config, loop, manager):
        if not os.path.isdir(config.plugin_dir):
            # the new plugin dir isn't found
            logging.error("config error plugging dir %r not found" %
                    config.plugin_dir)

            if self.plugin_dir != config.plugin_dir:
                logging.info("stop all current plugins")
                self.stop_apps()
            return

        # save all states
        old_plugins = self.plugins.copy()
        old_installed = self.installed
        old_apps = copy.copy(self.apps)
        old_plugin_dir = self.plugin_dir

        # scan the plugin dir
        self.plugin_dir = config.plugin_dir
        self.scan()

        try:
            self.check_mandatory()
        except RuntimeError as e:
            # one dependency is missing, return
            logging.error("Failed to reload plugins: %s" % str(e))

            if self.plugin_dir != old_plugin_dir:
                logging.info("stop all current plugins")
                self.stop_apps()

            # reset values
            self.plugin_dir = old_plugin_dir
            self.plugins = old_plugins
            self.installed = old_installed
            self.apps = old_apps
            return

        # initialize new apps
        apps = self.init_apps(config)

        # stop removed plugins
        for ap in old_apps:
            if ap not in apps:
                app, _ = ap
                try:
                    app.stop()
                except Exception:
                     # we ignore all exception
                    logging.error('Uncaught exception when stopping a plugin',
                        exc_info=True)

        # start or restart plugins
        for app, plug in apps:
            try:
                if (app, plug) in old_apps:
                    app.restart()
                else:
                    app.start(loop, manager)
            except Exception:
                # we ignore all exception
                logging.error('Uncaught exception when (re)starting a plugin',
                        exc_info=True)

########NEW FILE########
__FILENAME__ = users
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import os
import sqlite3
import uuid

from ..util import bytestring, ord_
from .pbkdf2 import pbkdf2_hex
from .util import load_backend


class UserNotFound(Exception):
    """ exception raised when a user doesn't exist"""


class UserConflict(Exception):
    """ exception raised when you try to create an already exisiting user """

class User(object):
    """ instance representing an authenticated user """

    def __init__(self, username, password, user_type=0, key=None, extra=None):
        self.username = username
        self.password = password
        self.user_type = user_type
        self.key = key
        self.extra = extra or {}

    def __str__(self):
        return "USer: %s" % self.username

    def is_authenticated(self):
        return True

    def is_anonymous(self):
        return False

    def is_user(self):
        return self.user_type == 0

    def is_app(self):
        return self.user_type == 1

    @classmethod
    def load(cls, obj):
        return cls(obj['username'], obj['password'], obj.get('user_type', 0),
                obj.get("key"), obj.get('extra'))

    def dump(self):
        return {"username": self.username, "password": self.password,
                "user_type": self.user_type, "key": self.key,
                "extra": self.extra}


class DummyUser(User):

    def __init__(self, *args, **kwargs):
        self.username = "anonymous"
        self.password = None
        self.user_type = 0
        self.key = None
        self.extra = {}

    def is_authenticated(self):
        return False

    def is_anonymous(self):
        return True


class AuthManager(object):

    def __init__(self, loop, cfg):
        self.loop = loop
        self.cfg = cfg

        # initialize the db backend
        if not cfg.auth_backend or cfg.auth_backend == "default":
            self._backend = SqliteAuthHandler(loop, cfg)
        else:
            self._backend = load_backend(cfg.keys_backend)

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        try:
            self.close()
        except:
            pass

    def open(self):
        self._backend.open()

    def close(self):
        self._backend.close()

    def all_users(self, include_user=False):
        return self._backend.all_users(include_user=include_user)

    def create_user(self, username, password, user_type=1, key=None,
            extra=None):

        password = self._hash_password(password)

        # store the user
        self._backend.create_user(username, password, user_type=user_type,
                key=key, extra=extra)

    def authenticate(self, username, password):

        try:
            user = self._backend.get_user(username)
        except UserNotFound:
            return DummyUser()

        _alg, infos, password_hash = user['password'].split("$", 3)
        salt, iterations = infos.split(":")

        password_hash1 =  bytestring(pbkdf2_hex(password.encode('utf-8'),
            salt.encode('utf-8'), iterations=int(iterations)).decode('utf-8'))

        if not self._check_password(password_hash, password_hash1):
            return DummyUser()

        return User.load(user)

    def get_user(self, username):
        return self._backend.get_user(username)

    def set_password(self, username, password):
        password = self._hash_password(password)
        self._backend.set_password(username, password)

    def set_key(self, username, key):
        self._backend.set_key(username, key)

    def update_user(self, username, password, user_type=1, key=None,
            extra=None):
        password = self._hash_password(password)
        self._backend.update_user(username, password, user_type=user_type,
                key=key, extra=extra)

    def delete_user(self, username):
        self._backend.delete_user(username)

    def user_by_key(self, key):
        return self._backend.get_bykey(key)

    def user_by_type(self, user_type):
        return self._backend.get_bytype(user_type)

    def has_user(self, username):
        return self._backend.has_user(username)

    def has_usertype(self, user_type):
        return self._backend.has_usertype(user_type)

    def _check_password(self, a, b):
        # compare password hashes lengths
        if len(a) != len(b):
            return False

        # do a binary comparaison of password hashes
        rv = 0
        for x, y in zip(a, b):
            rv |= ord(x) ^ ord(y)

        return rv == 0

    def _hash_password(self, password):
        # hash the password
        salt = uuid.uuid4().hex
        hashed_password =  bytestring(pbkdf2_hex(password.encode('utf-8'),
                salt.encode('utf-8')).decode('utf-8'))
        return "PBKDF2-256$%s:%s$%s" % (salt, 1000, hashed_password)


class BaseAuthHandler(object):

    def __init__(self, loop, cfg):
        self.loop = loop
        self.cfg = cfg

    def open(self):
        raise NotImplementedError

    def close(self):
        raise NotImplementedError

    def create_user(self, username, password, user_type=0, key=None,
            extra=None):
        raise NotImplementedError

    def get_user(self, username):
        raise NotImplementedError

    def update_user(self, username, password, user_type=0, key=None,
            extra=None):
        raise NotImplementedError

    def set_password(self, username, password):
        raise NotImplementedError

    def set_key(self, username, key):
        raise NotImplementedError

    def delete_user(self, username):
        raise NotImplementedError

    def user_bykey(self, key):
        raise NotImplementedError

    def users_bytype(self, username):
        raise NotImplementedError

    def has_usertype(self, user_type):
        raise NotImplementedError

    def has_user(self, usernamen ):
        raise NotImplementedError

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        try:
            self.close()
        except:
            pass


class SqliteAuthHandler(BaseAuthHandler):
    """ SQLITE AUTH BACKEND FOR THE AUTHENTICATION API in gaffer """

    def __init__(self, loop, cfg):
        super(SqliteAuthHandler, self).__init__(loop, cfg)

        # set dbname
        self.dbname = cfg.auth_dbname or "auth.db"
        if self.dbname != ":memory:":
            self.dbname = os.path.join(cfg.config_dir, self.dbname)

        # intitialize conn
        self.conn = None

    def open(self):
        self.conn = sqlite3.connect(self.dbname)
        with self.conn:
            sql = """CREATE TABLE if not exists auth (user text primary key,
            pwd text, user_type int, key text, extra text)"""
            self.conn.execute(sql)

    def close(self):
        self.conn.commit()
        self.conn.close()

    def all_users(self, include_user=False):
        with self.conn:
            cur = self.conn.cursor()
            if include_user:
                rows = cur.execute("SELECT * FROM auth")
                return [self._make_user(row, False) for row in rows]
            else:
                rows = cur.execute("SELECT user FROM auth")
                return [row[0] for row in rows]

    def create_user(self, username, password, user_type=0, key=None,
            extra=None):
        assert self.conn is not None
        with self.conn:
            try:
                self.conn.execute("INSERT INTO auth VALUES(?, ?, ?, ?, ?)",
                        [username, password, user_type, key,
                         json.dumps(extra or {})])
            except sqlite3.IntegrityError:
                raise UserConflict()

    def get_user(self, username):
        assert self.conn is not None
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("SELECT * FROM auth where user=?", [username])
            row = cur.fetchone()

        if not row:
            raise UserNotFound()

        return self._make_user(row)

    def set_password(self, username, password):
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("UPDATE auth SET pwd=? WHERE user=?", [password,
                username])

    def set_key(self, username, key):
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("UPDATE auth SET key=? WHERE user=?", [key,
                username])

    def update_user(self, username, password, user_type=0, key=None,
            extra=None):
        assert self.conn is not None

        if not self.has_user(username):
            raise UserNotFound()

        with self.conn:
            self.conn.execute("""UPDATE auth SET pwd=?, user_type=?,
            key=?, extra=? WHERE user=?""", [password, user_type, key,
                json.dumps(extra or {}), username])

    def delete_user(self, username):
        assert self.conn is not None
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("DELETE FROM auth WHERE user=?", [username])

    def get_bytype(self, user_type):
        assert self.conn is not None
        with self.conn:
            cur = self.conn.cursor()
            rows = cur.execute("SELECT * from auth WHERE user_type=?",
                    [user_type])

            return [self._make_user(row, False) for row in rows]

    def get_bykey(self, key):
        assert self.conn is not None
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("SELECT * from auth WHERE key=?", [key])
            row = cur.fetchone()

            if not row:
                raise UserNotFound()

            return self._make_user(row)

    def has_user(self, username):
        try:
            self.get_user(username)
        except UserNotFound:
            return False
        return True

    def has_type(self, user_type):
        with self.conn:
            cur = self.conn.cursor()
            cur.execute("SELECT * from auth WHERE user_type=?", [user_type])

            if not cur.fetchone():
                return False
            return True

    def _make_user(self, row, include_password=True):
        user = json.loads(row[4]) or {}
        user.update({"username": row[0], "user_type": row[2], "key": row[3]})

        if include_password:
            user.update({"password": row[1]})
        return user

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

try:
    input = raw_input
except NameError:
    pass

from importlib import import_module
import os
import sys

if os.name == 'nt':
    import ctypes
    import _winreg

    _kernel32 = ctypes.windll.kernel32

    def executablepath():
        '''return full path of hg.exe'''
        size = 600
        buf = ctypes.create_string_buffer(size + 1)
        len = _kernel32.GetModuleFileNameA(None, ctypes.byref(buf), size)
        if len == 0:
            raise ctypes.WinError
        elif len == size:
            raise ctypes.WinError(122)
        return buf.value

    def system_path():
        # look for a system rcpath in the registry
        path = [_winreg.QueryValueEx(_winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,
            'SOFTWARE\\Gaffer'), None)[0].replace('/', '\\')]
        path.append(os.path.dirname(executablepath()))
        return path

    def user_path():
        home = os.path.expanduser('~')
        path = [os.path.join(home, '.gaffer'),]
        userprofile = os.environ.get('USERPROFILE')
        if userprofile:
            path.append(os.path.join(userprofile, '.gaffer'))
        return path

    def is_admin():
        try:
            # only windows users with admin privileges
            # can read the C:\windows\temp
            os.listdir(os.sep.join([os.environ.get('SystemRoot', 'C:\windows'),
                'temp']))
        except:
            return False
        return True

    def default_path():
        userprofile = os.environ.get('USERPROFILE')
        if userprofile:
            return os.path.join(userprofile, '.gaffer')
        return os.path.join(os.path.expanduser('~'), '.gaffer')

    default_user_path = default_path
else:
    def system_path():
        here = os.path.dirname(os.path.dirname(sys.argv[0]))
        return [os.path.join(here, 'etc', 'gaffer'),
                '/etc/gaffer']

    def user_path():
        home = os.path.expanduser('~')
        return [os.path.join(home, '.gaffer'),]

    def is_admin():
        if os.geteuid() == 0:
            return True
        return False

    def default_path():
        if is_admin():
            # if the user is an admin, first test if the program name root has
            # the etc folder. If not fallback to /usr/local/etc/gaffer.
            local_etc = os.path.join(os.path.dirname(os.path.dirname(
                sys.argv[0])), "etc")
            if os.path.isdir(local_etc):
                return os.path.join(local_etc, "gaffer")
            return os.path.join("usr", "local", "etc", "gaffer")

        # if not root, use the user path
        return os.path.join(os.path.expanduser('~'), '.gaffer')

    def default_user_path():
        return os.path.join(os.path.expanduser('~'), '.gaffer')


def load_backend(backend_name):
    """ load pool backend. If this is an external module it should be
    passed as "somelib.backend_mod".

    """
    try:
        if len(backend_name.split(".")) > 1:
            mod = import_module(backend_name)
        elif backend_name == "sqlite":
            mod = import_module("gaffer.gafferd.auth.SqliteAuthHandler")
        return mod
    except ImportError:
        error_msg = "%s isn't a socketpool backend" % backend_name
        raise ImportError(error_msg)

def confirm(prompt, resp=True):
    if resp:
        prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')
    else:
        prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')

    while True:
        ret = input(prompt).lower()
        if not ret:
            return resp

        if ret not in ('y', 'n'):
            print('please enter y or n.')
            continue

        if ret == "y":
            return True

        return False

########NEW FILE########
__FILENAME__ = base
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json

import pyuv
from tornado import httpclient

from ..tornado_pyuv import IOLoop
from .util import make_uri

class GafferNotFound(Exception):
    """ exception raised on HTTP 404 """

class GafferConflict(Exception):
    """ exption raised on HTTP 409 """

class GafferUnauthorized(Exception):
    """ exception raised on HTTP 401 """

class GafferForbidden(Exception):
    """ exception raised on HTTP 403 """


class HTTPClient(object):
    """A blocking HTTP client.

    This interface is provided for convenience and testing; most applications
    that are running an IOLoop will want to use `AsyncHTTPClient` instead.
    Typical usage looks like this::

        http_client = httpclient.HTTPClient()
        try:
            response = http_client.fetch("http://www.friendpaste.com/")
            print response.body
        except httpclient.HTTPError as e:
            print("Error: %s" % e)
    """
    def __init__(self, async_client_class=None, loop=None, **kwargs):
        self.loop = loop
        self._io_loop = IOLoop(_loop=loop)
        if async_client_class is None:
            async_client_class = httpclient.AsyncHTTPClient
        self._async_client = async_client_class(self._io_loop, **kwargs)
        self._response = None
        self._closed = False

    def __del__(self):
        self.close()

    def close(self):
        """Closes the HTTPClient, freeing any resources used."""
        if not self._closed:
            self._async_client.close()
            self._io_loop.close()
            self._closed = True

    def fetch(self, request, **kwargs):
        """Executes a request, returning an `HTTPResponse`.

        The request may be either a string URL or an `HTTPRequest` object.
        If it is a string, we construct an `HTTPRequest` using any additional
        kwargs: ``HTTPRequest(request, **kwargs)``

        If an error occurs during the fetch, we raise an `HTTPError`.
        """
        def callback(response):
            self._response = response
            self._io_loop.stop()
        self._async_client.fetch(request, callback, **kwargs)
        self._io_loop.start()
        response = self._response
        self._response = None
        response.rethrow()
        return response

class BaseClient(object):
    """ base resource object used to abstract request call and response
    retrieving """

    def __init__(self, uri, loop=None, **options):
        self.loop = loop or pyuv.Loop.default_loop()
        self.uri = uri
        self.options = options
        self.client = HTTPClient(loop=loop)

    def request(self, method, path, headers=None, body=None, **params):
        headers = headers or {}
        headers.update({"Accept": "application/json"})
        url = make_uri(self.uri, path, **params)
        method = method.upper()
        if (body is None) and method in ("POST", "PATCH", "PUT"):
            body = ""

        try:
            resp = self.client.fetch(url, method=method, headers=headers,
                    body=body, **self.options)
        except httpclient.HTTPError as e:
            if method != "HEAD":
                # only raise on non head method since we are using head to
                # check status and so on.

                if e.code == 404:
                    raise GafferNotFound(self.json_body(e.response))
                elif e.code == 409:
                    raise GafferConflict(self.json_body(e.response))
                elif e.code == 401:
                    raise GafferUnauthorized(self.json_body(e.response))
                elif e.code == 403:
                    raise GafferForbidden(self.json_body(e.response))
                else:
                    raise
            else:
                if e.response is not None:
                    resp = e.response
                else:
                    raise
        return resp

    def json_body(self, resp):
        respbody = resp.body.decode('utf-8')
        try:
            return json.loads(respbody)
        except ValueError:
            return respbody

########NEW FILE########
__FILENAME__ = job
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json

from ..process import ProcessConfig
from ..util import parse_signal_value

class Job(object):
    """ Job object. Represent a remote job"""

    def __init__(self, server, config=None, sessionid=None):
        self.server = server
        self.sessionid = sessionid or 'default'
        if not isinstance(config, ProcessConfig):
            self.name = config
            self._config = None
        else:
            self.name = config['name']
            self._config = config

    @property
    def config(self):
        if not self._config:
            self._config = self.fetch_config()
        return self._config

    def fetch_config(self):
        resp = self.server.request("get", "/jobs/%s/%s" % (self.sessionid,
             self.name))
        config_dict = self.server.json_body(resp)['config']
        return ProcessConfig.from_dict(config_dict)

    def __str__(self):
        return self.name

    @property
    def active(self):
        """ return True if the process is active """
        resp = self.server.request("get", "/jobs/%s/%s/state" % (
            self.sessionid, self.name))
        if resp.body == b'1':
            return True
        return False

    @property
    def running(self):
        """ return the number of processes running for this template """
        info = self.info()
        return info['running']

    @property
    def running_out(self):
        """ return the number of processes running for this template """
        info = self.info()
        return info['running_out']

    @property
    def numprocesses(self):
        """ return the maximum number of processes that can be launched
        for this template """
        info = self.info()
        return info['max_processes']

    @property
    def pids(self):
        """ return a list of running pids """
        resp = self.server.request("get", "/jobs/%s/%s/pids" % (
            self.sessionid, self.name))
        result = self.server.json_body(resp)
        return result['pids']

    def info(self):
        """ return the process info dict """
        resp = self.server.request("get", "/jobs/%s/%s" % (self.sessionid,
            self.name))
        return self.server.json_body(resp)

    def stats(self):
        """ Return the template stats
        """
        resp = self.server.request("get", "/jobs/%s/%s/stats" %
                (self.sessionid, self.name))
        return self.server.json_body(resp)


    def start(self):
        """ start the process if not started, spawn new processes """
        self.server.request("post", "/jobs/%s/%s/state" % (self.sessionid,
            self.name), body="1")
        return True

    def stop(self):
        """ stop the process """
        self.server.request("post", "/jobs/%s/%s/state" % (self.sessionid,
            self.name), body="0")
        return True

    def restart(self):
        """ restart the process """
        self.server.request("post", "/jobs/%s/%s/state" % (self.sessionid,
            self.name), body="2")
        return True

    def scale(self, num=1):
        body = json.dumps({"scale": num})
        resp = self.server.request("post", "/jobs/%s/%s/numprocesses" % (
            self.sessionid, self.name), body=body)
        result = self.server.json_body(resp)
        return result['numprocesses']


    def commit(self, graceful_timeout=10.0, env=None):
        """ Like ``scale(1) but the process won't be kept alived at the end.
        It is also not handled uring scaling or reaping. """

        env = env or {}
        body = json.dumps({"graceful_timeout": graceful_timeout, "env": env})
        resp = self.server.request("post", "/jobs/%s/%s/commit" % (
            self.sessionid, self.name), body=body)
        result = self.server.json_body(resp)
        return result['pid']

    def kill(self, sig):
        """ send a signal to all processes of this template """

        # we parse the signal at the client level to reduce the time we pass
        # in the server.
        signum =  parse_signal_value(sig)

        body = json.dumps({"signal": signum})
        self.server.request("post", "/jobs/%s/%s/signal" % (self.sessionid,
            self.name), body=body)
        return True

########NEW FILE########
__FILENAME__ = keys
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import uuid

from .base import GafferNotFound

class Keys(object):

    def __init__(self, server):
        self.server = server

    def all_keys(self, include_keys=False):
        path = "/keys"
        if include_keys:
            path += "?include_keys=true"

        resp = self.server.request("get", path)
        return self.server.json_body(resp)["keys"]

    def create_key(self, permissions, key=None, label="", parent=None):
        key = key or uuid.uuid4().hex
        data = {"permissions": permissions}
        if label and label is not None:
            data['label'] = label
        resp = self.set_key(key, data, parent=parent)
        return self.server.json_body(resp)['api_key']

    def set_key(self, key, data, parent=None):
        obj = data.copy()
        obj["key"] = key
        obj["parent"] = parent

        # post the request
        headers = {"Content-Type": "application/json"}
        body = json.dumps(obj)
        return self.server.request("post", "/keys", headers=headers, body=body)


    def get_key(self, key, include_keys=False):
        path = "/keys/%s" % key
        if include_keys:
            path += "?include_keys=true"

        resp = self.server.request("get", path)
        return self.server.json_body(resp)

    def delete_key(self, key):
        self.server.request("delete", "/keys/%s" % key)

    def has_key(self, key):
        try:
            self.server.request("head", "/keys/%s" % key)
        except GafferNotFound:
            return False
        return True

########NEW FILE########
__FILENAME__ = process
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json

from ..util import (is_ssl, parse_ssl_options, parse_signal_value)
from .websocket import IOChannel
from .util import make_uri

class Process(object):
    """ Process Id object. It represent a pid """

    def __init__(self, server, pid):
        self.server = server
        self.pid = pid

        # get info
        resp = server.request("get", "/%s" % pid)
        self.info = self.server.json_body(resp)


    def __str__(self):
        return str(self.pid)

    def __getattr__(self, key):
        if key in self.info:
            return self.info[key]

        return object.__getattribute__(self, key)

    @property
    def active(self):
        """ return True if the process is active """
        resp = self.server.request("head", "/%s" % self.pid)
        if resp.code == 200:
            return True
        return False

    @property
    def stats(self):
        resp = self.server.request("get", "/%s/stats" % self.pid)
        obj = self.server.json_body(resp)
        return obj['stats']

    def stop(self):
        """ stop the process """
        self.server.request("delete", "/%s" % self.pid)
        return True

    def kill(self, sig):
        """ Send a signal to the pid """

        # we parse the signal at the client level to reduce the time we pass
        # in the server.
        signum =  parse_signal_value(sig)

        # make the request
        body = json.dumps({"signal": signum})
        headers = {"Content-Type": "application/json"}
        self.server.request("post", "/%s/signal" % self.pid, body=body,
                headers=headers)
        return True

    def socket(self, mode=3, stream=None, heartbeat=None):
        """ return an IO channel to a PID stream. This channek allows you to
        read and write to a stream if the operation is available

        Args:

          - **mode**: Mask of events that will be detected. The possible events
            are pyuv.UV_READABLE or pyuv.UV_WRITABLE.
          - **stream**: stream name as a string. By default it is using STDIO.
          - **hearbeat**: heartbeat in seconds to maintain the connection alive
            [default 15.0s]
        """
        # build connection url
        if stream is None:
            url = make_uri(self.server.uri, '/%s/channel' % self.pid,
                mode=mode)
        else:
            url = make_uri(self.server.uri, '/%s/channel/%s' % (self.pid,
                stream), mode=mode)
        url = "ws%s" % url.split("http", 1)[1]

        # build connection options
        options = {}
        if heartbeat and heartbeat is not None:
            options['heartbeat'] = heartbeat

        # eventually add sll options
        if is_ssl(url):
            options['ssl_options'] = parse_ssl_options(self.server.options)

        return IOChannel(self.server.loop, url, mode=mode,
                api_key=self.server.api_key, **options)

########NEW FILE########
__FILENAME__ = server
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import base64
import json

from ..process import ProcessConfig
from ..util import is_ssl, parse_ssl_options
from .base import  BaseClient
from .process import Process
from .job import Job
from .util import make_uri
from .websocket import GafferSocket


class Server(BaseClient):
    """ Server, main object to connect to a gaffer node. Most of the
    calls are blocking. (but running in the loop) """

    def __init__(self, uri, loop=None, api_key=None, **options):
        super(Server, self).__init__(uri, loop=loop, **options)
        self.api_key = api_key

    def request(self, method, path, headers=None, body=None, **params):
        headers = headers or {}
        # if we have an api key, pass it to the headers
        if self.api_key is not None:
            headers['X-Api-Key'] = self.api_key

        # continue the request
        return super(Server, self).request(method, path, headers=headers,
                body=body, **params)

    def authenticate(self, username, password):
        """ authenticate against a gafferd node to retrieve an api key """
        # set the basic auth header
        auth_hdr = "%s:%s" % (username, password)
        auth_hdr = b"Basic " + base64.b64encode(auth_hdr.encode("utf-8"))
        headers = {"Authorization": auth_hdr.decode("utf-8")}

        # make the request
        resp = self.request("get", "/auth", headers=headers)

        # set the server api key
        self.api_key = self.json_body(resp)["api_key"]

        # return the api key. useful for clients that store it for later.
        return self.api_key

    @property
    def version(self):
        """ get gaffer version """
        resp = self.request("get", "/")
        return self.json_body(resp)['version']

    def running(self):
        resp = self.request("get", "/pids")
        return self.json_body(resp)['pids']

    pids = running

    def ping(self):
        resp = self.request("get", "/ping")
        return resp.body == b'OK'

    def sessions(self):
        """ get list of current sessions """
        resp = self.request("get", "/sessions")
        obj = self.json_body(resp)
        return obj['sessions']

    def jobs(self, sessionid=None):
        if sessionid is None:
            resp = self.request("get", "/jobs")
        else:
            resp = self.request("get", "/jobs/%s" % sessionid)

        return self.json_body(resp)["jobs"]

    def jobs_walk(self, callback, sessionid=None):
        jobs = self.jobs(sessionid)
        for job in jobs:
            sessionid, name = self._parse_name(job)
            callback(self, Job(self, config=name, sessionid=sessionid))

    def job_exists(self, name):
        sessionid, name = self._parse_name(name)
        resp = self.request("head", "/jobs/%s/%s" % (sessionid, name))
        if resp.code == 200:
            return True
        return False


    def load(self, config, sessionid=None, start=True, force=False):
        """  load a process config object.

        Args:

        - **config**: dict or a ``process.ProcessConfig`` instance
        - **sessionid**: Some processes only make sense in certain contexts.
          this flag instructs gaffer to maintain this process in the sessionid
          context. A context can be for example an application. If no session
          is specified the config will be attached to the ``default`` session.
        """

        sessionid = self._sessionid(sessionid)
        headers = {"Content-Type": "application/json" }

        # build config body
        config_dict = config.to_dict()
        config_dict.update({'start': start})
        body = json.dumps(config_dict)

        name = "%s.%s" % (sessionid, config.name)

        if force:
            if self.job_exists(name):
                self.request("put", "/jobs/%s/%s" % (sessionid, config.name),
                        body=body, headers=headers)
            else:
                self.request("post", "/jobs/%s" % sessionid, body=body,
                        headers=headers)
        else:
            self.request("post", "/jobs/%s" % sessionid, body=body,
                        headers=headers)

        return Job(server=self, config=config, sessionid=sessionid)

    def unload(self, name, sessionid=None):
        sessionid = self._sessionid(sessionid)
        self.request("delete", "/jobs/%s/%s" % (sessionid, name))
        return True

    def reload(self, name, sessionid=None):
        sessionid = self._sessionid(sessionid)
        self.request("post", "/jobs/%s/%s/state" % (sessionid, name),
                body="2")
        return True

    def get_job(self, name):
        sessionid, name = self._parse_name(name)
        resp = self.request("get", "/jobs/%s/%s" % (sessionid, name))
        config_dict = self.json_body(resp)['config']
        return Job(server=self, config=ProcessConfig.from_dict(config_dict),
                sessionid=sessionid)

    def get_process(self, pid):
        return Process(server=self, pid=pid)

    def socket(self, heartbeat=None):
        """ return a direct websocket connection to gaffer """
        url0 =  make_uri(self.uri, '/channel/websocket')
        url = "ws%s" % url0.split("http", 1)[1]

        options = {}
        if heartbeat and heartbeat is not None:
            options['heartbeat'] = heartbeat

        if is_ssl(url):
            options['ssl_options'] = parse_ssl_options(self.options)

        return GafferSocket(self.loop, url, api_key=self.api_key, **options)

    def _parse_name(self, name):
        if "." in name:
            sessionid, name = name.split(".", 1)
        elif "/" in name:
            sessionid, name = name.split("/", 1)
        else:
            sessionid = "default"

        return sessionid, name

    def _sessionid(self, session=None):
        if not session:
            return "default"
        return session

########NEW FILE########
__FILENAME__ = users
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json

from .base import GafferNotFound

class Users(object):

    def __init__(self, server):
        self.server = server

    def all_users(self, include_user=False):
        path = "/users"
        if include_user:
            path += "?include_user=true"

        resp = self.server.request("get", path)
        return self.server.json_body(resp)["users"]

    def create_user(self, username, password, user_type=1, key=None,
            extra=None):

        extra = extra or {}

        # create json obj
        obj = extra.copy()
        obj.update(dict(username=username, password=password,
            user_type=user_type, key=key))

        # post the request
        headers = {"Content-Type": "application/json"}
        body = json.dumps(obj)
        self.server.request("post", "/users", headers=headers, body=body)

    def get_user(self, username):
        resp = self.server.request("get", "/users/%s" % username)
        return self.server.json_body(resp)

    def set_password(self, username, password):
        body = json.dumps({"password": password})
        self.server.request("put", "/users/%s/password" % username, body=body)

    def set_key(self, username, key):
        body = json.dumps({"key": key})
        self.server.request("put", "/users/%s/key" % username, body=body)

    def update_user(self, username, password, user_type=1, key=None,
            extra=None):
        extra = extra or {}
        # create json obj
        obj = extra.copy()
        obj.update(dict(password=password, user_type=user_type, key=key))

        # send the request
        body = json.dumps(obj)
        self.server.request("put", "/users/%s" % username, body=body)

    def delete_user(self, username):
        self.server.request("delete", "/users/%s" % username)

    def has_user(self, username):
        try:
            self.server.request("head", "/users/%s" % username)
        except GafferNotFound:
            return False

        return True

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import six

from ..util import quote, quote_plus

def url_quote(s, charset='utf-8', safe='/:'):
    """URL encode a single string with a given encoding."""
    if isinstance(s, six.text_type):
        s = s.encode(charset)
    elif not isinstance(s, str):
        s = str(s)
    return quote(s, safe=safe)


def url_encode(obj, charset="utf8", encode_keys=False):
    items = []
    if isinstance(obj, dict):
        for k, v in list(obj.items()):
            items.append((k, v))
    else:
        items = list(items)

    tmp = []
    for k, v in items:
        if encode_keys:
            k = encode(k, charset)

        if not isinstance(v, (tuple, list)):
            v = [v]

        for v1 in v:
            if v1 is None:
                v1 = ''
            elif six.callable(v1):
                v1 = encode(v1(), charset)
            else:
                v1 = encode(v1, charset)
            tmp.append('%s=%s' % (quote(k), quote_plus(v1)))
    return '&'.join(tmp)

def encode(v, charset="utf8"):
    if isinstance(v, six.text_type):
        v = v.encode(charset)
    else:
        v = str(v)
    return v


def make_uri(base, *args, **kwargs):
    """Assemble a uri based on a base, any number of path segments,
    and query string parameters.

    """

    # get encoding parameters
    charset = kwargs.pop("charset", "utf-8")
    safe = kwargs.pop("safe", "/:")
    encode_keys = kwargs.pop("encode_keys", True)

    base_trailing_slash = False
    if base and base.endswith("/"):
        base_trailing_slash = True
        base = base[:-1]
    retval = [base]

    # build the path
    _path = []
    trailing_slash = False
    for s in args:
        if s is not None and isinstance(s, six.string_types):
            if len(s) > 1 and s.endswith('/'):
                trailing_slash = True
            else:
                trailing_slash = False
            _path.append(url_quote(s.strip('/'), charset, safe))

    path_str =""
    if _path:
        path_str = "/".join([''] + _path)
        if trailing_slash:
            path_str = path_str + "/"
    elif base_trailing_slash:
        path_str = path_str + "/"

    if path_str:
        retval.append(path_str)

    params_str = url_encode(kwargs, charset, encode_keys)
    if params_str:
        retval.extend(['?', params_str])

    return ''.join(retval)

########NEW FILE########
__FILENAME__ = websocket
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import array
import base64
from collections import deque
import functools
import hashlib
import json
import logging
import os
import re
import socket
import struct
import threading
import time
import uuid

import pyuv
import tornado.escape
from tornado import iostream
from tornado.httputil import HTTPHeaders
from tornado.util import bytes_type

from ..tornado_pyuv import IOLoop, install
install()

from ..error import AlreadyRead
from ..events import EventEmitter
from ..message import (Message, decode_frame, FRAME_ERROR_TYPE,
        FRAME_RESPONSE_TYPE, FRAME_MESSAGE_TYPE)
from ..util import urlparse, ord_

# The initial handshake over HTTP.
WS_INIT = """\
GET %(path)s HTTP/1.1
Host: %(host)s:%(port)s
Upgrade: websocket
Connection: Upgrade
Sec-Websocket-Key: %(key)s
Sec-Websocket-Version: 13
"""

# Magic string defined in the spec for calculating keys.
WS_MAGIC = b'258EAFA5-E914-47DA-95CA-C5AB0DC85B11'

LOGGER = logging.getLogger("gaffer")

def frame(data, opcode=0x01):
    """Encode data in a websocket frame."""
    # [fin, rsv, rsv, rsv] [opcode]
    frame = struct.pack('B', 0x80 | opcode)

    # Our next bit is 1 since we're using a mask.
    length = len(data)
    if length < 126:
        # If length < 126, it fits in the next 7 bits.
        frame += struct.pack('B', 0x80 | length)
    elif length <= 0xFFFF:
        # If length < 0xffff, put 126 in the next 7 bits and write the length
        # in the next 2 bytes.
        frame += struct.pack('!BH', 0x80 | 126, length)
    else:
        # Otherwise put 127 in the next 7 bits and write the length in the next
        # 8 bytes.
        frame += struct.pack('!BQ', 0x80 | 127, length)

    # Clients must apply a 32-bit mask to all data sent.
    mask = [ord_(c) for c in os.urandom(4)]
    frame += struct.pack('!BBBB', *mask)
    # Mask each byte of data using a byte from the mask.
    msg = [ord_(c) ^ mask[i % 4] for i, c in enumerate(data)]
    frame += struct.pack('!' + 'B' * length, *msg)
    return frame


class WebSocket(object):
    """Websocket client for protocol version 13 using the Tornado IO loop."""

    def __init__(self, loop, url, **kwargs):
        ports = {'ws': 80, 'wss': 443}

        self.loop = loop
        self._io_loop = IOLoop(_loop=loop)

        self.url = urlparse(url)
        self.scheme = self.url.scheme
        self.host = self.url.hostname
        self.port = self.url.port or ports[self.url.scheme]
        self.path = self.url.path or '/'

        # support the query argument in the path
        self.path += self.url.query and "?%s" % self.url.query or ""

        self.client_terminated = False
        self.server_terminated = False
        self._final_frame = False
        self._frame_opcode = None
        self._frame_length = None
        self._fragmented_message_buffer = None
        self._fragmented_message_opcode = None
        self._waiting = None
        self._pending_messages = []
        self._started = False

        self.key = base64.b64encode(os.urandom(16))

        # initialize the stream
        if 'ssl_options' in kwargs:
            self.stream = iostream.SSLIOStream(socket.socket(),
                    io_loop=self._io_loop, **kwargs)
        else:
            self.stream = iostream.IOStream(socket.socket(),
                    io_loop=self._io_loop)

        self.graceful_shutdown = kwargs.get('graceful_shutdown', 0)

    def start(self):
        # start the stream
        self.stream.connect((self.host, self.port), self._on_connect)


    def on_open(self):
        pass

    def on_message(self, data):
        pass

    def on_ping(self):
        pass

    def on_pong(self):
        pass

    def on_close(self):
        pass

    def write_message(self, message, binary=False):
        """Sends the given message to the client of this Web Socket."""
        if binary:
            opcode = 0x2
        else:
            opcode = 0x1
        message = tornado.escape.utf8(message)
        assert isinstance(message, bytes_type)

        if not self._started:
            self._pending_messages.append(message)
        else:
            self._write_frame(True, opcode, message)

    def ping(self):
        self._write_frame(True, 0x9, b'')

    def close(self):
        """Closes the WebSocket connection."""
        self._started = False

        if not self.server_terminated:
            if not self.stream.closed():
                self._write_frame(True, 0x8, b'')
            self.server_terminated = True

        if self.graceful_shutdown:
            if self.client_terminated:
                if self._waiting is not None:
                    try:
                        self.stream.io_loop.remove_timeout(self._waiting)
                    except KeyError:
                        pass
                    self._waiting = None
                self._terminate()
            elif self._waiting is None:
                # Give the client a few seconds to complete a clean shutdown,
                # otherwise just close the connection.
                self._waiting = self.stream.io_loop.add_timeout(
                    time.time() + self.graceful_shutdown, self._abort)
        else:
            if self.client_terminated:
                return

            self._terminate()

    def _terminate(self):
        self.client_terminated = True
        self.stream.close()
        self.stream.io_loop.close()

    def _write_frame(self, fin, opcode, data):
        self.stream.write(frame(data, opcode))

    def _on_connect(self):
        req_params = dict(path = self.path, host = self.host,
                key = tornado.escape.native_str(self.key),
                port = self.port)
        request = '\r\n'.join(WS_INIT.splitlines()) % req_params + '\r\n\r\n'
        self.stream.write(request.encode('latin1'))
        self.stream.read_until(b'\r\n\r\n', self._on_headers)

    def _on_headers(self, data):
        first, _, rest = data.partition(b'\r\n')
        headers = HTTPHeaders.parse(tornado.escape.native_str(rest))
        # Expect HTTP 101 response.
        assert re.match('HTTP/[^ ]+ 101',
                tornado.escape.native_str(first))
        # Expect Connection: Upgrade.
        assert headers['Connection'].lower() == 'upgrade'
        # Expect Upgrade: websocket.
        assert headers['Upgrade'].lower() == 'websocket'
        # Sec-WebSocket-Accept should be derived from our key.
        accept = base64.b64encode(hashlib.sha1(self.key + WS_MAGIC).digest())
        assert headers['Sec-WebSocket-Accept'] == tornado.escape.native_str(accept)

        self._started = True
        if self._pending_messages:
            for msg in self._pending_messages:
                self.write_message(msg)
            self._pending_messages = []

        self._async_callback(self.on_open)()
        self._receive_frame()

    def _receive_frame(self):
        self.stream.read_bytes(2, self._on_frame_start)

    def _on_frame_start(self, data):
        header, payloadlen = struct.unpack("BB", data)
        self._final_frame = header & 0x80
        reserved_bits = header & 0x70
        self._frame_opcode = header & 0xf
        self._frame_opcode_is_control = self._frame_opcode & 0x8
        if reserved_bits:
            # client is using as-yet-undefined extensions; abort
            return self._abort()
        if (payloadlen & 0x80):
            # Masked frame -> abort connection
            return self._abort()
        payloadlen = payloadlen & 0x7f
        if self._frame_opcode_is_control and payloadlen >= 126:
            # control frames must have payload < 126
            return self._abort()
        if payloadlen < 126:
            self._frame_length = payloadlen
            self.stream.read_bytes(self._frame_length, self._on_frame_data)
        elif payloadlen == 126:
            self.stream.read_bytes(2, self._on_frame_length_16)
        elif payloadlen == 127:
            self.stream.read_bytes(8, self._on_frame_length_64)

    def _on_frame_length_16(self, data):
        self._frame_length = struct.unpack("!H", data)[0]
        self.stream.read_bytes(self._frame_length, self._on_frame_data)

    def _on_frame_length_64(self, data):
        self._frame_length = struct.unpack("!Q", data)[0]
        self.stream.read_bytes(self._frame_length, self._on_frame_data)

    def _on_frame_data(self, data):
        unmasked = array.array("B", data)

        if self._frame_opcode_is_control:
            # control frames may be interleaved with a series of fragmented
            # data frames, so control frames must not interact with
            # self._fragmented_*
            if not self._final_frame:
                # control frames must not be fragmented
                self._abort()
                return
            opcode = self._frame_opcode
        elif self._frame_opcode == 0:  # continuation frame
            if self._fragmented_message_buffer is None:
                # nothing to continue
                self._abort()
                return
            self._fragmented_message_buffer += unmasked
            if self._final_frame:
                opcode = self._fragmented_message_opcode
                unmasked = self._fragmented_message_buffer
                self._fragmented_message_buffer = None
        else:  # start of new data message
            if self._fragmented_message_buffer is not None:
                # can't start new message until the old one is finished
                self._abort()
                return
            if self._final_frame:
                opcode = self._frame_opcode
            else:
                self._fragmented_message_opcode = self._frame_opcode
                self._fragmented_message_buffer = unmasked

        if self._final_frame:
            self._handle_message(opcode, unmasked.tostring())

        if not self.client_terminated:
            self._receive_frame()

    def _abort(self):
        """Instantly aborts the WebSocket connection by closing the socket"""
        self.client_terminated = True
        self.server_terminated = True
        self._started = False
        self.stream.close()
        self.close()

    def _handle_message(self, opcode, data):
        if self.client_terminated:
            return

        if opcode == 0x1:
            # UTF-8 data
            try:
                decoded = data.decode("utf-8")
            except UnicodeDecodeError:
                self._abort()
                return
            self._async_callback(self.on_message)(decoded)
        elif opcode == 0x2:
            # Binary data
            self._async_callback(self.on_message)(data)
        elif opcode == 0x8:
            # Close
            self.client_terminated = True
            self.close()
        elif opcode == 0x9:
            # Ping
            self._write_frame(True, 0xA, data)
            self._async_callback(self.on_ping)()
        elif opcode == 0xA:
            # Pong
            self._async_callback(self.on_pong)()
        else:
            self._abort()

    def _async_callback(self, callback, *args, **kwargs):
        """Wrap callbacks with this if they are used on asynchronous requests.

        Catches exceptions properly and closes this WebSocket if an exception
        is uncaught.
        """
        if args or kwargs:
            callback = functools.partial(callback, *args, **kwargs)

        def wrapper(*args, **kwargs):
            try:
                return callback(*args, **kwargs)
            except Exception:
                logging.error('Uncaught exception', exc_info=True)
                self._abort()
        return wrapper


class Channel(object):

    def __init__(self, loop, topic):
        self.topic = topic
        self._emitter = EventEmitter(loop)

    def __str__(self):
        return "channel: %s" % self.topic

    def bind(self, event, callback):
        self._emitter.subscribe(event, callback)

    def unbind(self, event, callback):
        self._emitter.unsubscribe(event, callback)

    def bind_all(self, callback):
        self._emitter.subscribe(".", callback)

    def unbind_all(self, callback):
        self._emitter.unsubscribe(".", callback)

    def send(self, event, message):
        self._emitter.publish(event, message)

    def close(self):
        self._emitter.close()


class GafferCommand(object):

    def __init__(self, *args, **kwargs):
        self.identity = uuid.uuid4().hex
        self.name = args[0]
        self.args = args[1:]
        self.kwargs = kwargs

        self._callbacks = []
        self._result = None
        self._error = None
        self._sock = None
        self.active = False

        self._lock = threading.Lock()
        self._condition = threading.Condition()

    def __str__(self):
        return "GafferCommand:%s (%s)" % (self.name, self.identity)

    def done(self):
        """ return True if the command has been completed or returned an error
        """
        return (self._result is not None or self._error is not None)

    def result(self):
        """ Return the value returned by the call. If the call hasn't been
        completed it will return None """
        with self._condition:
            return self._result

    def error(self):
        """ Return the error returned by the call. If the call hasn't been
        completed it will return None """
        with self._condition:
            return self._error

    def add_done_callback(self, callback):
        self._callbacks.append(callback)

    def _start(self, sock):
        self._sock = sock
        self.active = True

    def _set_result(self, result):
        with self._lock:
            self._result = result
            self.active = False

        self._handle_callbacks()

    def _set_error(self, error):
        with self._lock:
            self._error = error
            self.active = False

        self._handle_callbacks()

    def _handle_callbacks(self):
        for cb in self._callbacks:
            try:
                cb(self)
            except Exception:
                logging.error('Uncaught exception', exc_info=True)


class GafferSocket(WebSocket):

    def __init__(self, loop, url, api_key=None, **kwargs):
        loop = loop

        try:
            self.heartbeat_timeout = kwargs.pop('heartbeat')
        except KeyError:
            self.heartbeat_timeout = 15.0

        self.api_key = api_key

        # define status
        self.active = False
        self.closed = False

        # dict to maintain opened channels
        self.channels = dict()

        # dict to maintain commands
        self.commands = dict()

        # emitter for global events
        self._emitter = EventEmitter(loop)
        self._heartbeat = pyuv.Timer(loop)
        super(GafferSocket, self).__init__(loop, url, **kwargs)

        # make sure we authenticate first
        if self.api_key is not None:
            self.write_message("AUTH:%s" % self.api_key)

    def start(self):
        if self.active:
            return

        super(GafferSocket, self).start()
        self.active = True

    def subscribe(self, topic):
        # we already subsribed to this topic
        if topic in self.channels:
            return

        # make sure we started the socket
        assert self.active == True

        # create a new channel.
        # we don't wait the response for it so we make sure we won't send
        # twice the same message until we really want it.
        self.channels[topic] = Channel(self.loop, topic)

        # send subscription message
        msg = {"event": "SUB", "data": {"topic": topic}}
        self.write_message(json.dumps(msg))
        return self.channels[topic]

    def unsubscribe(self, topic):
        # we are not subscribed to this topic
        if topic not in self.channels:
            return

        # make sure we started the socket
        assert self.active == True

        # remove the channels from the list
        channel = self.channels.pop(topic)
        channel.close()

        # send unsubscription message
        msg = {"event": "UNSUB", "data": {"topic": topic}}
        self.write_message(json.dumps(msg))

    def send_command(self, *args, **kwargs):
        # register a new command
        cmd0 = GafferCommand(*args, **kwargs)
        cmd = self.commands[cmd.identity] = cmd0

        # send the new command
        data = {"identity": cmd.identity, "name": cmd.name, "args": cmd.args,
                "kwargs": cmd.kwargs}
        msg = {"event": "CMD", "data": data}
        self.write_message(json.dumps(msg))

        # return the command object
        return cmd

    def bind(self, event, callback):
        """ bind to a global event """
        self._emitter.subscribe(event, callback)

    def unbind(self, event, callback):
        """ unbind to a global event """
        self._emitter.unsubscribe(event, callback)

    def bind_all(self, callback):
        """ bind to all global events """
        self._emitter.subscribe(".", callback)

    def unbind_all(self, callback):
        """ unbind to all global events """
        self._emitter.unsubscribe(".", callback)

    def __getitem__(self, topic):
        try:
            channel = self.channels[topic]
        except KeyError:
            raise KeyError("%s channel isn't subscribed" % topic)
        return channel

    def __delitem__(self, topic):
        self.unsubcribe(topic)

    ### websocket methods

    def on_open(self):
        # start the heartbeat
        self._heartbeat.start(self.on_heartbeat, self.heartbeat_timeout,
                self.heartbeat_timeout)
        self._heartbeat.unref()

    def on_close(self):
        self.active = False
        self.closed = True
        self._heartbeat.stop()

    def on_message(self, raw):
        msg = json.loads(raw)
        assert "event" in msg

        event = msg['event']

        if event == "gaffer:subscription_success":
            self._emitter.publish("subscription_success", msg)
        elif event == "gaffer:subscription_error":
            self._emitter.publish("subscription_error", msg)

            # get topic
            topic = msg['topic']

            # remove the channel from the subscribed list
            if topic in self.channels:
                channel = self.channels.pop(topic)
                channel.close()

        elif event == "gaffer:command_success":
            identity = msg['data']['id']
            result = msg['data']['result']
            if identity in self.commands:
                cmd = self.commands.pop(identity)
                cmd._set_result(result)
                self._emitter.publish("command_success", cmd)
        elif event == "gaffer:command_error":
            identity = msg['data']['id']
            if identity in self.commands:
                cmd = self.commands.pop(identity)
                cmd._set_error(msg['data']['error'])
                self._emitter.publish("command_error", cmd)
        elif event == "gaffer:event":
            # if message type is an event then it should contain a data
            # property
            assert "data" in msg
            data = msg['data']

            topic = data['topic']
            event = data['event']
            if topic in self.channels:
                channel = self.channels[topic]
                channel.send(event, data)

    def on_heartbeat(self, h):
        # on heartbeat send a nop message to the channel
        # it will maintain the connection open
        self.write_message(json.dumps({"event": "NOP"}))


class IOChannel(WebSocket):

    def __init__(self, loop, url, mode=3, api_key=None, **kwargs):
        loop = loop
        self.api_key = api_key

        # initialize the capabilities
        self.mode = mode
        self.readable = False
        self.writable = False
        if mode & pyuv.UV_READABLE:
            self.readable = True

        if mode & pyuv.UV_WRITABLE:
            self.writable = True

        # set heartbeat
        try:
            self.heartbeat_timeout = kwargs.pop('heartbeat')
        except KeyError:
            self.heartbeat_timeout = 15.0
        self._heartbeat = pyuv.Timer(loop)

        # pending messages queue
        self._queue = deque()
        self.pending = {}

        # define status
        self.active = False
        self.closed = False

        # read callback
        self._read_callback = None

        super(IOChannel, self).__init__(loop, url, **kwargs)

        # make sure we authenticate first
        if self.api_key is not None:
            msg = Message("AUTH:%s" % self.api_key)
            self.write_message(msg.encode())

    def start(self):
        if self.active:
            return

        super(IOChannel, self).start()
        self.active = True

    def start_read(self, callback):
        if not self.readable:
            raise IOError("not_readable")

        if self._read_callback is not None:
            raise AlreadyRead()
        self._read_callback = callback

    def stop_read(self):
        self._read_callback = None


    def write(self, data, callback=None):
        if not self.writable:
            raise IOError("not_writable")

        msg = Message(data)
        if callback is not None:
            self.pending[msg.id] = callback

        self.write_message(msg.encode())

    ### websocket methods

    def on_open(self):
        # start the heartbeat
        self._heartbeat.start(self.on_heartbeat, self.heartbeat_timeout,
                self.heartbeat_timeout)
        self._heartbeat.unref()

    def on_close(self):
        self.active = False
        self.closed = True
        self._heartbeat.stop()

    def on_error(self, cb):
        self._on_error_cb = cb

    def on_message(self, raw):
        msg = decode_frame(raw)
        if msg.type in (FRAME_ERROR_TYPE, FRAME_RESPONSE_TYPE):
            # did we received an error?
            error = None
            result = None
            if msg.type == FRAME_ERROR_TYPE:
                error = json.loads(msg.body.decode('utf-8'))
                self.close()
            else:
                result = msg.body

            if msg.id == "gaffer_error":
                if self._on_error_cb is not None:
                    return self._async_callback(self._on_error_cb)(self, error)

            # handle message callback if any
            try:
                callback = self.pending.pop(msg.id)
            except KeyError:
                return

            self._async_callback(callback)(self, result, error)

        elif msg.type == FRAME_MESSAGE_TYPE:
            if self._read_callback is not None:
                self._async_callback(self._read_callback)(self, msg.body)

    def on_heartbeat(self, h):
        # on heartbeat send a nop message to the channel
        # it will maintain the connection open
        self.ping()

########NEW FILE########
__FILENAME__ = client
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import json
import os
import ssl
import sys

import pyuv

from ..events import EventEmitter
from ..httpclient.base import BaseClient
from ..httpclient.util import make_uri
from ..httpclient.websocket import WebSocket
from ..util import is_ssl, parse_ssl_options

class LookupChannel(WebSocket):

    def __init__(self, server, url, **kwargs):
        self.server = server
        loop = server.loop

        try:
            self.heartbeat_timeout = kwargs.pop('heartbeat')
        except KeyError:
            self.heartbeat_timeout = 15.0

        self._heartbeat = pyuv.Timer(loop)
        self._emitter = EventEmitter(loop)

        super(LookupChannel, self).__init__(loop, url, **kwargs)

    def __str__(self):
        return "%s: %s" % (self.__class__.__name__, self.server.url)

    def bind(self, event, callback):
        self._emitter.subscribe(event, callback)

    def unbind(self, event, callback):
        self._emitter.unsubscribe(event, callback)

    def bind_all(self, callback):
        self._emitter.subscribe(".", callback)

    def unbind_all(self, callback):
        self._emitter.unsubscribe(".", callback)

    def close(self):
        self._emitter.close()
        super(LookupChannel, self).close()

    ## websockets methods

    def on_open(self):
        # start the heartbeat
        self._heartbeat.start(self.on_heartbeat, self.heartbeat_timeout,
                self.heartbeat_timeout)
        self._heartbeat.unref()

    def on_close(self):
        self._heartbeat.stop()
        self._emitter.close()

    def on_message(self, message):
        try:
            event = json.loads(message)
        except ValueError:
            return

        if "event" in event:
            self._emitter.publish(event['event'], event)

    def on_heartbeat(self, h):
        self.ping()


class LookupServer(BaseClient):

    @property
    def version(self):
        """ get the lookupd server version """
        resp = self.request("get", "/")
        return self.json_body(resp)['version']

    def ping(self):
        """ ping the lookupd server """
        resp = self.request("get", "/ping")
        return resp.body == b'OK'

    def nodes(self):
        """ get the list of nodes registered to this lookupd server """
        resp = self.request("get", "/nodes")
        return self.json_body(resp)

    def sessions(self, by_node='*'):
        """ get all sessions registered to this lookupd server """
        path = "/sessions"
        if by_node != '*':
            path = "%s/%s" % (path, by_node)
        resp = self.request("get", path)
        return self.json_body(resp)

    def jobs(self):
        """ get all jobs registered to this lookupd server """
        resp = self.request("get", "/jobs")
        return self.json_body(resp)

    def find_job(self, job_name):
        """ find a job on this lookupd server """
        resp = self.request("get", "/findJob", name=job_name)
        return self.json_body(resp)

    def find_session(self, sessionid):
        """ find all jobs for a session on this lookupd server """
        resp = self.request("get", "/findSession", sessionid=sessionid)
        return self.json_body(resp)

    def lookup(self, heartbeat=None):
        """ return a direct websocket connection this node allowing you to
        listen on events.

        Events are:

        - add_node
        - remove_node
        - add_job
        - remove_job
        - add_process
        - remove_process
        """

        url0 = make_uri(self.uri, "/lookup/websocket")
        url = "ws%s" % url0.split("http", 1)[1]
        options = {}
        if heartbeat and heartbeat is not None:
            options['heartbeat'] = heartbeat

        if is_ssl(url):
            options['ssl_options'] = parse_ssl_options(self.options)

        channel = LookupChannel(self, url, **options)
        return channel

########NEW FILE########
__FILENAME__ = http
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import json

from tornado.web import Application
from tornado.httpserver import HTTPServer

from gaffer import __version__
from .. import sockjs
from ..gafferd.http_handlers.util import CorsHandler


from .protocol import LookupWebSocket
from .registry import Registry, JobNotFound


def http_server(io_loop, listener, ssl_options=None, registration_db=None):

    # initialize the registry
    registration_db = registration_db or Registry(loop=io_loop._loop)

    # lookup routes
    user_settings = { "registration_db": registration_db }
    lookup_router = sockjs.SockJSRouter(LookupConnection, "/lookup",
            io_loop=io_loop, user_settings=user_settings)

    # initialize handlers
    handlers = [
            (r'/', WelcomeHandler),
            (r'/ping', PingHandler),
            (r'/version', VersionHandler),
            (r'/nodes', NodesHandler),
            (r'/sessions', SessionsHandler),
            (r'/sessions/([^/]+)', SessionsHandler),
            (r'/jobs', JobsHandler),
            (r'/findJob', FindJobHandler),
            (r'/findSession', FindSessionHandler),
            (r'/ws', LookupWebSocket)] + lookup_router.urls

    # initialize the server
    app = Application(handlers, registration_db=registration_db)
    server = HTTPServer(app, io_loop=io_loop, ssl_options=ssl_options)
    server.add_sockets(listener)
    return server



class WelcomeHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.write({"welcome": "gaffer-lookupd", "version": __version__})


class VersionHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.write({"version": __version__})


class PingHandler(CorsHandler):

    def get(self):
        self.preflight()
        self.set_status(200)
        self.write("OK")


class LookupConnection(sockjs.SockJSConnection):

    def on_open(self, info):
        self.db = self.session.server.settings.get('registration_db')
        self.db.bind_all(self.on_event)

    def on_close(self):
        self.db.unbind_all(self.on_event)

    def on_event(self, event, message):
        if event in ('add_node', 'remove_node', 'identify', ):
            msg = message.infodict()
        else:
            msg = {}
            for k, v in message.items():
                if k == "node":
                    v = v.infodict()
                msg[k] = v
        # add event to the message
        msg['event'] = event
        self.write_message(msg)

    def write_message(self, msg):
        if isinstance(msg, dict):
            self.send(json.dumps(msg))
        else:
            self.send(msg)

class NodesHandler(CorsHandler):

    def get(self, *args, **kwargs):
        self.preflight()
        db = self.settings.get('registration_db')

        registered = db.all_nodes()
        self.write({"nodes": [node.infodict() for node in registered]})


class SessionsHandler(CorsHandler):

    def get(self, *args, **kwargs):
        self.preflight()
        db = self.settings.get('registration_db')

        if len(args) > 0:
            node = args[0]
        else:
            node = '*'

        registered = db.sessions(with_node=node)

        sessions = []
        for sessionid, session_jobs in registered.items():
            all_jobs = {}
            for job_name, jobs in session_jobs.items():
                sources = [{"name": job.node.name,
                            "pids": job.pids,
                            "node_info": job.node.infodict()} for job in jobs]

                all_jobs[job_name] = sources
            sessions.append({"sessionid": sessionid, "jobs": all_jobs})

        self.write({"nb_sessions": len(sessions), "sessions": sessions})


class JobsHandler(CorsHandler):

    def get(self, *args, **kwargs):
        self.preflight()
        db = self.settings.get('registration_db')

        registered = db.jobs()

        all_jobs = []
        for job_name, jobs in registered.items():
            sources = [{"name": job.node.name,
                        "pids": job.pids,
                        "node_info": job.node.infodict()} for job in jobs]
            all_jobs.append({"name": job_name, "sources": sources})

        self.write({"nb_jobs": len(all_jobs), "jobs": all_jobs})


class FindJobHandler(CorsHandler):

    def get(self, *args, **kwargs):
        self.preflight()
        db = self.settings.get('registration_db')

        job_name = self.get_argument("name")
        found = []
        try:
            found = db.find_job(job_name)
        except JobNotFound:
            pass

        jobs = []
        for job in found:
            jobs.append({"name": job.node.name, "pids": job.pids,
                "node_info": job.node.infodict()})

        self.write({"sources": jobs})


class FindSessionHandler(CorsHandler):

    def get(self, *args, **kwargs):
        self.preflight()
        db = self.settings.get('registration_db')

        sid = self.get_argument("sessionid")
        found = []
        try:
            found = db.find_session(sid)
        except JobNotFound:
            pass

        # first order jobs by name
        jobs = {}
        for job in found:
            if job.name not in jobs:
                jobs[job.name] = []

            source = {"name": job.node.name,
                      "pids": job.pids,
                      "node_info": job.node.infodict()}
            jobs[job.name].append(source)

        # create result array
        all_jobs = [{"name": name, "sources": sources} \
                for name, sources in jobs.items()]


        self.write({"nb_jobs": len(all_jobs), "jobs": all_jobs})

########NEW FILE########
__FILENAME__ = main
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
usage: gaffer_lookupd [--version] [-v] [--daemon] [--pidfile=PIDFILE]
                      [--bind=ADDRESS] [--backlog=BACKLOG]
                      [--certfile=CERTFILE] [--keyfile=KEYFILE]
                      [--cacert=CACERT]

Options

    -h --help                   show this help message and exit
    --version                   show version and exit
    -v                          verbose mode
    --daemon                    Start gaffer in daemon mode
    --pidfile=PIDFILE
    --backlog=BACKLOG           default backlog [default: 128]
    --bind=ADDRESS              default HTTP binding [default: 0.0.0.0:5010]
    --certfile=CERTFILE         SSL certificate file
    --keyfile=KEYFILE           SSL key file
    --cacert=CACERT             SSL CA certificate
"""
import os
import sys

# patch tornado IOLoop
from ..tornado_pyuv import IOLoop, install
install()

import pyuv

from .. import __version__
from ..docopt import docopt
from ..pidfile import Pidfile
from ..sig_handler import BaseSigHandler
from ..util import bind_sockets, daemonize, setproctitle_

from .http import http_server


class LookupSigHandler(BaseSigHandler):

    def __init__(self, server):
        self.server = server
        super(LookupSigHandler, self).__init__()

    def handle_quit(self, handle, *args):
        self.server.stop()

    def handle_reload(self, handle, *args):
        # HUP is ignored
        return


class LookupServer(object):

    def __init__(self, args, loop=None):
        self.loop = loop or pyuv.Loop.default_loop()
        self.io_loop = IOLoop(_loop=self.loop)
        self.args = args

        # initialize the signal handler
        self._sig_handler = LookupSigHandler(self)

        # get arguments
        self.addr = args["--bind"]
        if args["--backlog"]:
            try:
                self.backlog = int(args["--backlog"])
            except ValueError:
                raise RuntimeError("backlog should be an integer")
        else:
            self.backlog = 128

         # parse SSL options
        ssl_options = {}
        if self.args["--certfile"] is not None:
            ssl_options['certfile'] = self.args["--certfile"]

        if self.args["--keyfile"] is not None:
            ssl_options["keyfile"] = self.args["--keyfile"]

        if self.args.get("--cacert") is not None:
            ssl_options["ca_certs"] = self.args["--cacert"]

        self.ssl_options = None
        if ssl_options:
            self.ssl_options = ssl_options

        self.started = False

    def start(self):
        if self.started:
            return

        # start the sighandler
        self._sig_handler.start(self.loop)

        # initialize the server
        listener = bind_sockets(self.addr, backlog=self.backlog,
                allows_unix_socket=True)

        # check ssl options now
        if self.ssl_options is not None and isinstance(self.ssl_options, dict):
            # Only certfile is required: it can contain both keys
            if 'certfile' not in self.ssl_options:
                raise RuntimeError('missing key "certfile" in ssl_options')

            if not os.path.exists(self.ssl_options['certfile']):
                raise RuntimeError('certfile "%s" does not exist' %
                        self.ssl_options['certfile'])
            if ('keyfile' in self.ssl_options and
                    not os.path.exists(self.ssl_options['keyfile'])):
                raise RuntimeError('keyfile "%s" does not exist' %
                        self.ssl_options['keyfile'])

        self.hserver = http_server(self.io_loop, listener,
                ssl_options=self.ssl_options)
        self.hserver.start()

        self.started = True

    def run(self):
        if not self.started:
            self.start()

        self.loop.run()

    def stop(self):
        self.hserver.stop()
        self.io_loop.close()
        self.started = False

def main():
    args = docopt(__doc__, version=__version__)
    if args["--daemon"]:
        daemonize()
    setproctitle_("gaffer_lookupd")

    # create pidfile
    pidfile = None
    if args["--pidfile"]:
        pidfile = Pidfile(args["--pidfile"])

        try:
            pidfile.create(os.getpid())
        except RuntimeError as e:
            print(str(e))
            sys.exit(1)

    try:
        s = LookupServer(args)
        s.run()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(str(e))
        sys.exit(1)
    finally:
        if pidfile is not None:
            pidfile.unlink()

    sys.exit(0)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = protocol
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import json

from tornado import websocket

from ..error import ProcessError
from .registry import (NoIdent, JobNotFound, AlreadyIdentified, IdentExists,
        AlreadyRegistered)

class MessageError(ProcessError):

    def __init__(self, reason="invalid_message", msgid=None):
        ProcessError.__init__(self, errno=400, reason=reason)
        self.msgid = msgid


class LookupMessage(object):

    def __init__(self, raw):
        self.raw = raw
        self.type = None
        self.id = None
        self.args = ()

        self.parse(raw)

    def parse(self, raw):
        # message should at least have the msgid and type properties
        try:
            self.id = raw['msgid']
            self.type = raw['type']
        except KeyError:
            raise MessageError(msgid=self.id)

        # validate the message type
        if self.type not in ("REGISTER_JOB", "UNREGISTER_JOB",
                "REGISTER_PROCESS", "UNREGISTER_PROCESS", "PING", "IDENTIFY"):
            raise MessageError("invalid_message_type")

        # validate message arguments
        try:
            if self.type == "IDENTIFY":
                self.args = (raw['name'], raw['origin'], raw['version'],)
            if self.type in ("REGISTER_JOB", "UNREGISTER_JOB"):
                self.args  = (raw['job_name'],)
            elif self.type in ("REGISTER_PROCESS", "UNREGISTER_PROCESS"):
                self.args  = (raw['job_name'], raw['pid'],)
        except KeyError:
            raise MessageError(msgid=self.id)

    def __str__(self):
        return "%s: %s" % (self.__class__.__name__, self.id)


class LookupWebSocket(websocket.WebSocketHandler):

    def open(self):
        db = self.settings.get('registration_db')
        db.add_node(self)
        self.active = True

    def on_close(self):
        db = self.settings.get('registration_db')
        db.remove_node(self)
        self.active = False

    def on_message(self, message):
        db = self.settings.get('registration_db')
        try:
            msg_raw = json.loads(message)
        except ValueError as e:
            self.write_error(400, "invalid_json")

        try:
            msg = LookupMessage(msg_raw)
        except MessageError as e:
            return self.write_error(e.errno, e.reason, e.msgid)

        try:
            if msg.type == "PING":
                db.update(self)
            elif msg.type == "IDENTIFY":
                db.identify(self, *msg.args)
            elif msg.type == "REGISTER_JOB":
                db.add_job(self, *msg.args)
            elif msg.type == "UNREGISTER_JOB":
                db.remove_job(self, *msg.args)
            elif msg.type == "REGISTER_PROCESS":
                db.add_process(self, *msg.args)
            elif msg.type == "UNREGISTER_PROCESS":
                db.remove_process(self, *msg.args)
        except JobNotFound as e:
            return self.write_error(404, str(e), msg.id)
        except AlreadyRegistered as e:
            return self.write_error(409, str(e), msg.id)
        except NoIdent as e:
            return self.write_error(404, str(e), msg.id)
        except AlreadyIdentified as e:
            return self.write_error(409, str(e), msg.id)
        except IdentExists as e:
            return self.write_error(409, str(e), msg.id)

        self.write_message({"ok": True, "msgid": msg.id})

    def write_error(self, errno, reason, msgid=None):
        msg = {"errno": errno, "reason": reason, "msgid": msgid}
        self.write_message(msg)

    def write_message(self, msg):
        if isinstance(msg, dict):
            msg = json.dumps(msg)

        super(LookupWebSocket, self).write_message(msg)

########NEW FILE########
__FILENAME__ = registry
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import OrderedDict
from threading import RLock
import time

import pyuv

from ..events import EventEmitter
from ..util import parse_job_name


class NoIdent(Exception):
    """ exception raised when the not hasn't send its identity """

class JobNotFound(Exception):
    """ exception raised when a job isn't registered for a node """

class AlreadyIdentified(Exception):
    """ exception raised when a job is already identified """

class IdentExists(Exception):
    """ exception raised a node has already been registered with this hostname
    """

class AlreadyRegistered(Exception):
    """ exception raised when a job is alreay registered """


class RemoteJob(object):

    def __init__(self, node, name):
        self.node = node
        self.name = name
        self._pids = set()

    def __str__(self):
        return "%s: %s" % (self.__class__.__name__, self.name)

    @property
    def pids(self):
        return list(self._pids)

    def add(self, pid):
        self._pids.add(pid)

    def remove(self, pid):
        try:
            self._pids.remove(pid)
        except KeyError:
            return


class GafferNode(object):
    """ class to maintain jobs & process / nodes """

    def __init__(self, conn):
        self.conn = conn
        self.sessions = dict()
        self.update()
        self.name = None
        self.origin = None
        self.version = None

    def __str__(self):
        return "node: %s" % self.name

    def identify(self, name, origin, version):
        self.name = name
        self.origin = origin
        self.version = version
        self.update()

    def update(self):
        self.updated = time.time()

    def add_job(self, job_name):
        sessionid, name = parse_job_name(job_name)

        if sessionid in self.sessions:
            session = self.sessions[sessionid]
        else:
            session = self.sessions[sessionid] = {}

        if name in session:
            raise AlreadyRegistered("job %r is already registered" % job_name)

        session[name] = RemoteJob(self, job_name)
        self.update()

    def remove_job(self, job_name):
        """ remove the job registered for this node """
        sessionid, name = parse_job_name(job_name)
        if sessionid not in self.sessions:
            return

        # remove the job if it exists
        session = self.sessions[sessionid]
        try:
            del session[name]
        except KeyError:
            return

        # if session is empty, remove it from the list
        if not session:
            try:
                del self.sessions[sessionid]
            except KeyError:
                return

        self.update()

    def add_process(self, job_name, pid):
        job = self.get_job(job_name)
        job.add(pid)
        self.update()

    def remove_process(self, job_name, pid):
        job = self.get_job(job_name)
        if pid in job.pids:
            job.remove(pid)
            return True
        return False
        self.update()

    def get_job(self, job_name):
        sessionid, name = parse_job_name(job_name)

        try:
            session = self.sessions[sessionid]
        except KeyError:
            raise JobNotFound()

        try:
            job = session[name]
        except KeyError:
            raise JobNotFound()

        return job

    def to_dict(self):
        info = self.infodict()
        sessions = {}
        for sessionid, job in self.sessions.items():
            sessions[sessionid] = { "job_name": job.name, "pids": job.pids }

        info['sessions'] = sessions
        return info

    def infodict(self):
        return dict(name=self.name, origin=self.origin, version=self.version)


class Registry(object):

    def __init__(self, loop=None):
        self.loop = loop or pyuv.Loop.default_loop()
        self.nodes = OrderedDict()
        self._emitter = EventEmitter(self.loop)
        self._lock = RLock()

    def close(self):
        self._emitter.close()

    def bind(self, event, callback):
        self._emitter.subscribe(event, callback)

    def unbind(self, event, callback):
        self._emitter.unsubscribe(event, callback)

    def bind_all(self, callback):
        self._emitter.subscribe(".", callback)

    def unbind_all(self, callback):
        self._emitter.unsubscribe(".", callback)

    def add_node(self, conn):
        """ register a connection. """
        with self._lock:
            node = self.nodes[conn] = GafferNode(conn)
            self._emitter.publish('add_node', node)
            return node

    def remove_node(self, conn):
        """ remove a connection from the registry """
        with self._lock:
            try:
                node = self.nodes.pop(conn)
                node.sessions = {}
                self._emitter.publish('remove_node', node)
            except KeyError:
                pass

    def identify(self, conn, name, origin, version):
        """ identify a node """
        with self._lock:
            # check if we already identified this node
            if self.nodes[conn].name is not None:
                raise AlreadyIdentified()

            # check if we already identified a node with this identity
            for _, node in self.nodes.items():
                if node.name == name and node.origin == origin:
                    raise IdentExists()

            self.nodes[conn].identify(name, origin, version)
            self._emitter.publish('identify', self.nodes[conn])


    def update(self, conn):
        with self._lock:
            # we can update a non identified Node
            if not conn in self.nodes:
                return
            self.nodes[conn].update()

    def all_nodes(self):
        """ get all identified nodes """
        with self._lock:
            nodes = [node for _, node in self.nodes.items() if node is not None]
            return nodes

    def get_node(self, conn):
        """ get a node """
        with self._lock:
            return self._get_node(conn)

    def sessions(self, with_node='*'):
        """ get all sessions from the registry. If ``with_node != '*'`` then
        only the sessions for this node will be returned. """
        if not with_node:
            raise ValueError("with_node should be '*' or a node identity")

        with self._lock:
            sessions = OrderedDict()
            for _, node in self.nodes.items():
                # if the node isn't identified, continue
                if node is None:
                    continue

                # if we filter by node, check if we can add the session
                if with_node != '*' and node.name != with_node:
                    continue

                for sessionid, jobs in node.sessions.items():
                    if not sessionid in sessions:
                        sessions[sessionid] = {}

                    for _, job in jobs.items():
                        if job.name not in sessions[sessionid]:
                            sessions[sessionid][job.name] = []
                        sessions[sessionid][job.name].append(job)

            return sessions

    def find_session(self, sessionid):
        with self._lock:
            all_jobs = []
            for _, node in self.nodes.items():
                # if the node isn't identified, continue
                if node is None:
                    continue
                for session, jobs in node.sessions.items():
                    if sessionid == session:
                        for _, job in jobs.items():
                            all_jobs.append(job)
                        break
            return all_jobs

    def node_by_name(self, name):
        """ get a node by its identity """
        with self._lock:
            nodes = []
            for node in self.nodes:
                if node is None:
                    continue

                if node.name == name:
                    nodes.append(node)
            return nodes

    def find_job(self, job_name):
        """ find a job in the registry, return a list of all remote job
        possible for this ``sessionid.name`` """
        sessionid, name = parse_job_name(job_name)
        with self._lock:
            jobs = []
            for _, node in self.nodes.items():
                # is not identified?
                if node is None:
                    continue

                # does this node support this session?
                if sessionid not in node.sessions:
                    continue

                # finally does this session support this job?
                if name not in node.sessions[sessionid]:
                    continue

                jobs.append(node.sessions[sessionid][name])

            if not jobs:
                raise JobNotFound()
            return jobs

    def jobs(self):
        """ return all remote jobs by their name """
        with self._lock:
            all_jobs = OrderedDict()
            for _, node in self.nodes.items():
                # is not identified?
                if node is None:
                    continue

                for sessionsid, jobs in node.sessions.items():
                    for _, job in jobs.items():
                        if job.name not in all_jobs:
                            all_jobs[job.name] = []

                        all_jobs[job.name].append(job)
            return all_jobs

    def add_job(self, conn, job_name):
        """ add a job to the registry """
        with self._lock:
            node = self._get_node(conn)
            node.add_job(job_name)
            event = {"node": self.nodes[conn], "job_name":  job_name}
            self._emitter.publish('add_job', event)

    def remove_job(self, conn, job_name):
        """ remove a job from the registry """
        with self._lock:
            node = self._get_node(conn)
            node.remove_job(job_name)
            event = {"node": self.nodes[conn], "job_name":  job_name}
            self._emitter.publish('remove_job', event)


    def add_process(self, conn, job_name, pid):
        """ add a process for this job """
        with self._lock:
            node = self._get_node(conn)
            node.add_process(job_name, pid)
            event = {"node": self.nodes[conn], "job_name":  job_name,
                    "pid": pid}
            self._emitter.publish('add_process', event)

    def remove_process(self, conn, job_name, pid):
        """ remove a process for this job """
        with self._lock:
            node = self._get_node(conn)
            # only send an event if we removed the process. It can also means
            # that the process has already been unregistered.
            if node.remove_process(job_name, pid):
                event = {"node": self.nodes[conn], "job_name":  job_name,
                    "pid": pid}
                self._emitter.publish('remove_process', event)

    ### private functions

    def _get_node(self, conn):
        node = self.nodes[conn]
        if node.name is None:
            raise NoIdent("need to send IDENTIFY message first")
        return node

########NEW FILE########
__FILENAME__ = manager
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
The manager module is a core component of gaffer. A Manager is
responsible of maintaining processes and allows you to interract with
them.

Classes
=======

"""
from collections import deque, OrderedDict
from threading import RLock

import pyuv

from .events import EventEmitter
from .error import ProcessError, ProcessConflict, ProcessNotFound
from .pubsub import Topic
from .state import ProcessState, ProcessTracker
from .sync import increment
from .util import parse_signal_value


class Manager(object):
    """ Manager - maintain process alive

    A manager is responsible of maintaining process alive and manage
    actions on them:

    - increase/decrease the number of processes / process template
    - start/stop processes
    - add/remove process templates to manage

    The design is pretty simple. The manager is running on the default
    event loop and listening on events. Events are sent when a process
    exit or from any method call. The control of a manager can be
    extended by adding apps on startup. For example gaffer
    provides an application allowing you to control processes via HTTP.

    Running an application is done like this::

        # initialize the application with the default loop
        loop = pyuv.Loop.default_loop()
        m = Manager(loop=loop)

        # start the application
        m.start(apps=[HttpHandler])

        .... # do smth

        m.stop() # stop the controlller
        m.run() # run the event loop

    .. note::

        The loop can be omitted if the first thing you do is
        launching a manager. The run function is here for convenience. You
        can of course just run `loop.run()` instead

    .. warning::

        The manager should be stopped the last one to prevent any lock
        in your application.


    """
    def __init__(self, loop=None):
        # by default we run on the default loop
        self.loop = loop or pyuv.Loop.default_loop()

        # initialize the emitter
        self.events = EventEmitter(self.loop)

        # initialize the process tracker
        self._tracker = ProcessTracker(self.loop)

        # initialize some values
        self.mapps = []
        self.started = False
        self._stop_ev = None
        self.max_process_id = 0
        self.processes = OrderedDict()
        self.running = OrderedDict()
        self._sessions = OrderedDict()
        self._topics = {}
        self._updates = deque()
        self._signals = []

        self.status = -1
        self.stop_cb = None
        self.restart_cb = None
        self._lock = RLock()

    @property
    def active(self):
        return self.status == 0 and self.started

    def start(self, apps=[]):
        """ start the manager. """
        self.mapps = apps

        self._waker = pyuv.Async(self.loop, self._wakeup)

        # start the process tracker
        self._tracker.start()

        # manage processes
        self.events.subscribe('exit', self._on_exit)

        # start contollers
        for mapp in self.mapps:
            mapp.start(self.loop, self)

        self.started = True
        self.status = 0

    def run(self):
        """ Convenience function to use in place of `loop.run()`
        If the manager is not started it raises a `RuntimeError`.

        Note: if you want to use separately the default loop for this
        thread then just use the start function and run the loop somewhere
        else.
        """
        if not self.started:
            raise RuntimeError("manager hasn't been started")
        self.loop.run()

    def stop(self, callback=None):
        """ stop the manager. This function is threadsafe """

        if not self.started:
            return

        if self.status == 1:
            # someone already requested to stop the manager
            return

        # set the callback
        self.stop_cb = callback

        # update the status to stop and wake up the loop
        self.status = 1
        self._waker.send()

    def restart(self, callback=None):
        """ restart all processes in the manager. This function is
        threadsafe """
        if self.status == 2:
            # a restart is already running
            return

        self.restart_cb = callback
        self.status = 2
        self._waker.send()

    def subscribe(self, topic):
        if topic not in self._topics:
            self._topics[topic] = Topic(topic, self)
            self._topics[topic].start()

        return self._topics[topic].subscribe()

    def unsubscribe(self, topic, channel):
        if topic not in self._topics:
            return
        self._topics[topic].unsubscribe(channel)

    @property
    def sessions(self):
        return list(self._sessions)

    def jobs(self, sessionid=None):
        if not sessionid:
            jobs = []
            for sessionid in self._sessions:
                session = self._sessions[sessionid]
                jobs.extend(["%s.%s" % (sessionid, name) for name in session])
            return jobs
        else:
            try:
                session = self._sessions[sessionid]
            except KeyError:
                raise ProcessNotFound()

            return ["%s.%s" % (sessionid, name) for name in session]


    def jobs_walk(self, callback, sessionid=None):
        with self._lock:
            if not sessionid:
                for sessionid in self._sessions:
                    for name in self._sessions[sessionid]:
                        callback(self, "%s.%s" % (sessionid, name))
            else:
                try:
                    session = self._sessions[sessionid]
                except KeyError:
                    raise ProcessNotFound()

                for name in session:
                    callback(self, "%s.%s" % (sessionid, name))

    # ------------- process functions

    def load(self, config, sessionid=None, env=None, start=True):
        """  load a process config object.

        Args:

        - **config**: a ``process.ProcessConfig`` instance
        - **sessionid**: Some processes only make sense in certain contexts.
          this flag instructs gaffer to maintain this process in the sessionid
          context. A context can be for example an application. If no session
          is specified the config will be attached to the ``default`` session.

        - **env**: dict, None by default, if specified the config env variable will
          be updated with the env values.
        """

        sessionid = self._sessionid(sessionid)

        with self._lock:
            if sessionid in self._sessions:
                # if the process already exists in this context raises a
                # conflict.
                if config.name in self._sessions[sessionid]:
                    raise ProcessConflict()
            else:
                # initialize this session
                self._sessions[sessionid] = OrderedDict()

            # create a new state for this config
            state = ProcessState(config, sessionid, env)
            self._sessions[sessionid][config.name] = state

            pname = "%s.%s" % (sessionid, config.name)
            self._publish("load", name=pname)

        if start:
            self.start_job(pname)

    def unload(self, name_or_process, sessionid=None):
        """ unload a process config. """

        sessionid = self._sessionid(sessionid)
        name = self._get_pname(name_or_process)

        pname = "%s.%s" % (sessionid, name)

        with self._lock:
            if sessionid not in self._sessions:
                raise ProcessNotFound()

            # get the state and remove it from the context
            session = self._sessions[sessionid]
            try:
                state = session.pop(name)
            except KeyError:
                raise ProcessNotFound()

            if not session:
                try:
                    del self._sessions[sessionid]
                except KeyError:
                    pass

            # notify that we unload the process
            self._publish("unload", name=pname)

            # notify that we are stoppping the process
            self._publish("stop", name=pname)
            self._publish("job.%s.stop" % pname, name=pname)

            # stop the process now.
            state.stopped = True
            self._stopall(state)

    def reload(self, name, sessionid=None):
        """ reload a process config. The number of processes is resetted to
        the one in settings and all current processes are killed """

        if not sessionid:
            if hasattr(name, "name"):
                sessionid = 'default'
                name = getattr(name, 'name')
            else:
                sessionid, name = self._parse_name(name)
        else:
            name = self._get_pname(name)

        with self._lock:
            # reset the number of processes
            state = self._get_state(sessionid, name)
            state.reset()

            # kill all the processes and let gaffer manage asynchronously the
            # reload
            self._stopall(state)

            # manage processes
            self._manage_processes(state)

    def update(self, config, sessionid=None, env=None, start=False):
        """ update a process config. All processes are killed """
        sessionid = self._sessionid(sessionid)

        with self._lock:
            state = self._get_state(sessionid, config.name)
            state.update(config, env=env)

            if start:
                # make sure we unstop the process
                state.stop = False

            # kill all the processes and let gaffer manage asynchronously the
            # reload. If the process is not stopped then it will start
            self._stopall(state)

    def get(self, name):
        """ get a job config """
        sessionid, name = self._parse_name(name)
        with self._lock:
            state = self._get_state(sessionid, name)
            return state.config


    def start_job(self, name):
        """ Start a job from which the config have been previously loaded """

        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)

        with self._lock:
            state = self._get_state(sessionid, name)

            # make sure we unstop the process
            state.stopped = False
            # reset the number of processes
            state.reset()

            # notify that we are starting the process
            self._publish("start", name=pname)
            self._publish("job.%s.start" % pname, name=pname)

            # manage processes
            self._manage_processes(state)

    def stop_job(self, name):
        """ stop a jon. All processes of this job are stopped and won't be
        restarted by the manager """

        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)

        with self._lock:
            state = self._get_state(sessionid, name)

            # put the number to 0
            state.numprocesses = 0
            # flag the state to stop
            state.stopped = True

            # notify that we are stoppping the process
            self._publish("stop", name=pname)
            self._publish("job.%s.stop" % pname, name=pname)

            self._stopall(state)

    def commit(self, name, graceful_timeout=0, env=None):
        """ Like ``scale(1) but the process won't be kept alived at the end.
        It is also not handled uring scaling or reaping. """

        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)

        with self._lock:
            state = self._get_state(sessionid, name)

            # commit the job and return the pid
            return self._commit_process(state,
                    graceful_timeout=graceful_timeout, env=env)

    def scale(self, name, n):
        """ Scale the number of processes in for a job. By using this
        function you can increase, decrease or set the number of processes in
        a template. Change is handled once the event loop is idling


        n can be a positive or negative integer. It can also be a string
        containing the opetation to do. For example::

            m.scale("sometemplate", 1) # increase of 1
            m.scale("sometemplate", -1) # decrease of 1
            m.scale("sometemplate", "+1") # increase of 1
            m.scale("sometemplate", "-1") # decrease of 1
            m.scale("sometemplate", "=1") # set the number of processess to 1
        """
        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)

        # find the operation to do
        if isinstance(n, int):
            if n > 0:
                op = "+"
            else:
                op = "-"
            n = abs(n)
        else:
            if n.isdigit():
                op = "+"
                n = int(n)
            else:
                op = n[0]
                if op not in ("=", "+", "-"):
                    raise ValueError("bad_operation")
                n = int(n[1:])

        with self._lock:
            state = self._get_state(sessionid, name)

            # scale
            if op == "=":
                curr = state.numprocesses
                if curr > n:
                    ret = state.decr(curr - n)
                else:
                    ret = state.incr(n - curr)
            elif op == "+":
                ret = state.incr(n)
            else:
                ret = state.decr(n)
            self._publish("update", name=pname)
            self._manage_processes(state)
            return ret

    def info(self, name):
        """ get job' infos """
        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)

        with self._lock:
            state = self._get_state(sessionid, name)

        processes = list(state.running)
        processes.extend(list(state.running_out))

        info = {"name": pname,
                "active":  state.active,
                "running": len(processes),
                "running_out": len(state.running_out),
                "max_processes": state.numprocesses,
                "processes": [p.pid for p in processes]}

        # get config
        config = state.config.to_dict()

        # remove custom channels because they can't be serialized
        config.pop('custom_channels', None)

        # add config to the info
        info['config'] = config
        return info

    def stats(self, name):
        """ return job stats

        """
        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)


        with self._lock:
            state = self._get_state(sessionid, name)
            processes = list(state.running)
            processes.extend(list(state.running_out))

            stats = []
            lmem = []
            lcpu = []
            for p in processes:
                pstats = p.stats
                pstats['pid'] = p.pid
                pstats['os_pid'] = p.os_pid
                stats.append(pstats)
                lmem.append(pstats['mem'])
                lcpu.append(pstats['cpu'])

            if 'N/A' in lmem or not lmem:
                mem, max_mem, min_mem = "N/A"
            else:
                max_mem = max(lmem)
                min_mem = min(lmem)
                mem = sum(lmem)

            if 'N/A' in lcpu or not lcpu:
                cpu, max_cpu, min_cpu = "N/A"
            else:
                max_cpu = max(lcpu)
                min_cpu = min(lcpu)
                cpu = sum(lcpu)

            ret = dict(name=pname, stats=stats, mem=mem,
                    max_mem=max_mem, min_mem=min_mem, cpu=cpu,
                    max_cpu=max_cpu, min_cpu=min_cpu)

            return ret

    def get_process(self, pid):
        """ get an OS process by ID. A process is a ``gaffer.Process`` instance
        attached to a process state that you can use.
        """
        with self._lock:
            return self._get_pid(pid)

    def stop_process(self, pid):
        """ stop a process """
        with self._lock:
            # remove the job from the runnings jobs
            try:
                p = self.running.pop(pid)
            except KeyError:
                raise ProcessNotFound()

            # remove the process from the state
            sessionid, name = self._parse_name(p.name)
            state = self._get_state(sessionid, name)

            # if the process is marked once it means the job has been
            # committed and the process shouldn't be restarted
            if p.once:
                state.running_out.remove(p)
            else:
                state.remove(p)

            # notify we stop this pid
            self._publish("stop_process", pid=p.pid, name=p.name)

            # then stop the process
            p.stop()

            # track this process to make sure it's killed after the
            # graceful time
            graceful_timeout = p.graceful_timeout or state.graceful_timeout
            self._tracker.check(p, graceful_timeout)

    def stopall(self, name):
        """ stop all processes of a job. Processes are just exiting and will
        be restarted by the manager. """

        sessionid, name = self._parse_name(name)
        with self._lock:
            state = self._get_state(sessionid, name)
            # kill all the processes.
            self._stopall(state)


    def kill(self, pid, sig):
        """ send a signal to a process """
        signum = parse_signal_value(sig)
        with self._lock:
            p = self._get_pid(pid)

            # notify we stop this job
            self._publish("proc.%s.kill" % p.pid, pid=p.pid, name=p.name)

            # effectively send the signal
            p.kill(signum)

    def send(self, pid, lines, stream=None):
        """ send some data to the process """
        with self._lock:
            p = self._get_pid(pid)

            # find the stream we need to write to
            if not stream or stream == "stdin":
                target = p
            else:
                if stream in p.streams:
                    target = p.streams[stream]
                else:
                    raise ProcessError(404, "stream_not_found")

            # finally write to the stream
            if isinstance(lines, list):
                target.writelines(lines)
            else:
                target.write(lines)


    def killall(self, name, sig):
        """ send a signal to all processes of a job """
        signum = parse_signal_value(sig)
        sessionid, name = self._parse_name(name)
        pname = "%s.%s" % (sessionid, name)
        with self._lock:
            state = self._get_state(sessionid, name)
            self._publish("job.%s.kill" % pname, name=pname, signum=signum)

            processes = list(state.running)
            processes.extend(list(state.running_out))
            for p in processes:
                # notify we stop this job
                self._publish("proc.%s.kill" % p.pid, pid=p.pid, name=p.name)
                # effectively send the signal
                p.kill(signum)

            self._manage_processes(state)

    def walk(self, callback, name=None):
        with self._lock:
            if not name:
                processes = [p for pid, p in self.running.items()]
            else:
                sessionid, name = self._parse_name(name)
                state = self._get_state(sessionid, name)
                processes = state.running

            for p in processes:
                callback(self, p)

    def list(self, name=None):
        with self._lock:
            if not name:
                processes = [p for pid, p in self.running.items()]
            else:
                sessionid, name = self._parse_name(name)
                state = self._get_state(sessionid, name)
                processes = state.running
            return list(processes)

    def pids(self, name=None):
        return [p.pid for p in self.list(name=name)]

    def manage(self, name):
        sessionid, name = self._parse_name(name)
        with self._lock:
            state = self._get_state(sessionid, name)
            self._manage_processes(state)

    def monitor(self, listener, name):
        """ get stats changes on a process template or id
        """

        sessionid, name = self._parse_name(name)
        with self._lock:
            state = self._get_state(sessionid, name)
            for p in state.running:
                p.monitor(listener)

    def unmonitor(self, listener, name):
        """ get stats changes on a process template or id
        """
        sessionid, name = self._parse_name(name)
        with self._lock:
            state = self._get_state(sessionid, name)
            for p in state.running:
                p.unmonitor(listener)


    # ------------- general purpose utilities

    def wakeup(self):
        self._waker.send()

    def get_process_id(self):
        """ generate a process id """
        self.max_process_id = increment(self.max_process_id)
        return self.max_process_id

    def _get_locked_state(self, name):
        """ utility function to get a state from name generally used for debug
        """
        sessionid, name = self._parse_name(name)
        with self._lock:
            return self._get_state(sessionid, name)


    def _sessionid(self, session=None):
        if not session:
            return "default"
        return session

    def _get_pname(self, name_or_process):
        if hasattr(name_or_process, "name"):
            return name_or_process.name
        else:
            return name_or_process


    def _parse_name(self, name):
        if "." in name:
            sessionid, name = name.split(".", 1)
        elif "/" in name:
            sessionid, name = name.split("/", 1)
        else:
            sessionid = "default"

        return sessionid, name

    def _get_state(self, sessionid, name):
        if sessionid not in self._sessions:
            raise ProcessNotFound()

        session = self._sessions[sessionid]
        if name not in session:
            raise ProcessNotFound()

        return session[name]

    def _get_pid(self, pid):
        try:
            return self.running[pid]
        except KeyError:
            raise ProcessNotFound()



    # ------------- general private functions

    def _shutdown(self):
        with self._lock:

            self._tracker.stop()

            # stop the applications.
            for ctl in self.mapps:
                ctl.stop()

            # we are now stopped
            self.started = False

            # close all handles
            #def walk_cb(h):
            #    if h.active:
            #        h.close()
            #self.loop.walk(walk_cb)

            # if there any stop callback, excute it
            if self.stop_cb is not None:
                self.stop_cb(self)
                self.stop_cb = None

    def _stop(self):
        # stop should be synchronous. We need to first stop the
        # processes and let the applications know about it. It is
        # actually done by setting on startup a timer waiting that all
        # processes have stopped to run. Then applications are stopped.

        self.stopping = True

        # stop all processes
        with self._lock:
            for sid in self._sessions:
                for name, state in self._sessions[sid].items():
                    if not state.stopped:
                        state.stopped = True
                        self._stopall(state)

            self._tracker.on_done(self._shutdown)

    def _restart(self):
        with self._lock:
            # on restart we first restart the applications
            for app in self.mapps:
                app.restart()

            # then we restart the sessions
            for sid in self._sessions:
                session = self._sessions[sid]
                for name in session:
                    self._restart_processes(session[name])

            # if any callback has been set, run it
            if self.restart_cb is not None:
                self.restart_cb(self)
                self.restart_cb = None

            self.status = 0


    # ------------- process type private functions

    def _stop_group(self, state, group):
        while True:
            try:
                p = group.popleft()
            except IndexError:
                break

            if p.pid not in self.running:
                continue

            self.running.pop(p.pid)

            # notify we stop this pid
            self._publish("stop_process", pid=p.pid, name=p.name)

            # stop the process
            p.stop()

            # track this process to make sure it's killed after the
            # graceful time
            graceful_timeout = p.graceful_timeout or state.graceful_timeout
            self._tracker.check(p, graceful_timeout)

    def _stopall(self, state):
        """ stop all processes of a job """

        # stop the flapping detection before killing the process to prevent
        # any race condition
        if state.flapping_timer is not None:
            state.flapping_timer.stop()

        # kill all keepalived processes
        if state.running:
            self._stop_group(state, state.running)

        # kill all others processes (though who have been committed)
        if state.running_out:
            self._stop_group(state, state.running_out)

        # if the job isn't stopped, restart the flapping detection
        if not state.stopped and state.flapping_timer is not None:
            state.flapping_timer.start()

    # ------------- functions that manage the process

    def _commit_process(self, state, graceful_timeout=10.0, env=None):
        """ like spawn but doesn't keep the process associated to the state.
        It should die at the end """
        # get internal process id
        pid = self.get_process_id()

        # start process
        p = state.make_process(self.loop, pid, self._on_process_exit)
        p.spawn(once=True, graceful_timeout=graceful_timeout, env=env)

        # add the pid to external processes in the state
        state.running_out.append(p)

        # we keep a list of all running process by id here
        self.running[pid] = p

        # notify
        self._publish("spawn", name=p.name, pid=pid, os_pid=p.os_pid)

        # on commit we return the pid now, so someone will be able to use it.
        return pid


    def _spawn_process(self, state):
        """ spawn a new process and add it to the state """
        # get internal process id
        pid = self.get_process_id()

        # start process
        p = state.make_process(self.loop, pid, self._on_process_exit)
        p.spawn()

        # add the process to the running state
        state.queue(p)

        # we keep a list of all running process by id here
        self.running[pid] = p

        self._publish("spawn", name=p.name, pid=pid, os_pid=p.os_pid)
        self._publish("job.%s.spawn" % p.name, name=p.name, pid=pid,
            os_pid=p.os_pid)

    def _spawn_processes(self, state):
        """ spawn all processes for a state """
        num_to_start = state.numprocesses - len(state.running)
        for i in range(num_to_start):
            self._spawn_process(state)

    def _reap_processes(self, state):
        if state.stopped:
            return

        diff = len(state.running) - state.numprocesses
        if diff > 0:
            for i in range(diff):
                # remove the process from the running processes
                try:
                    p = state.dequeue()
                except IndexError:
                    return

                # remove the pid from the running processes
                if p.pid in self.running:
                    self.running.pop(p.pid)

                # stop the process
                p.stop()

                # track this process to make sure it's killed after the
                # graceful time
                self._tracker.check(p, state.graceful_timeout)

                # notify others that the process is beeing reaped
                self._publish("reap", name=p.name, pid=p.pid, os_pid=p.os_pid)
                self._publish("job.%s.reap" % p.name, name=p.name, pid=p.pid,
                        os_pid=p.os_pid)
                self._publish("proc.%s.reap" % p.pid,
                        name=p.name, pid=p.pid, os_pid=p.os_pid)

    def _manage_processes(self, state):
        if state.stopped:
            return

        if len(state.running) < state.numprocesses:
            self._spawn_processes(state)
        self._reap_processes(state)

    def _restart_processes(self, state):
        # first launch new processes
        for i in range(state.numprocesses):
            self._spawn_process(state)

        # then reap useless one.
        self._manage_processes(state)

    def _check_flapping(self, state):
        if not state.flapping:
            return True

        check_flapping, can_retry = state.check_flapping()
        if not check_flapping:
            self._publish("flap", name=state.name)

            # stop the processes
            if not state.stopped:
                state.stopped = True
                self._stopall(state)

            if can_retry:
                # if we can retry later then set a callback
                def flapping_cb(handle):
                    # allows respawning
                    state.stopped = False
                    state._flapping_timer = None

                    # restart processes
                    self._restart_processes(state)
                # set a callback
                t = pyuv.Timer(self.loop)
                t.start(flapping_cb, state.flapping.retry_in, 0.0)
                state._flapping_timer = t
            return False
        return True

    def _publish(self, evtype, **ev):
        event = {"event": evtype }
        event.update(ev)
        self.events.publish(evtype, event)


    # ------------- events handler

    def _wakeup(self, handle):
        if self.status == 1:
            handle.close()
            self._stop()

        elif self.status == 2:
            self._restart()

    def _on_exit(self, evtype, msg):
        sessionid, name = self._parse_name(msg['name'])
        once = msg.get('once', False)

        with self._lock:
            try:
                state = self._get_state(sessionid, name)
            except ProcessNotFound:
                # race condition, we already removed this process
                return

            # eventually restart the process
            if not state.stopped and not once:
                # manage the template, eventually restart a new one.
                if self._check_flapping(state):
                    self._manage_processes(state)

    def _on_process_exit(self, process, exit_status, term_signal):
        with self._lock:
            # maybe uncheck this process from the tracker
            self._tracker.uncheck(process)

            # unexpected exit, remove the process from the list of
            # running processes.
            if process.pid in self.running:
                self.running.pop(process.pid)

            sessionid, name = self._parse_name(process.name)
            try:
                state = self._get_state(sessionid, name)
                # remove the process from the state if needed
                if process.once:
                    state.running_out.remove(process)
                else:
                    state.remove(process)
            except (ProcessNotFound, KeyError):
                pass

            # notify other that the process exited
            ev_details = dict(name=process.name, pid=process.pid,
                    exit_status=exit_status, term_signal=term_signal,
                    os_pid=process.os_pid, once=process.once)

            self._publish("exit", **ev_details)
            self._publish("job.%s.exit" % process.name, **ev_details)

########NEW FILE########
__FILENAME__ = message
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import uuid

FRAME_ERROR_TYPE = b'error'
FRAME_RESPONSE_TYPE = b'response'
FRAME_MESSAGE_TYPE = b'message'

MAGIC_V1 = b"V1"

class MessageError(Exception):
    """ exception raised when a message doesn't validate """

class Message(object):
    """ A class to abstract our simple message format used in stream.

    a Message is constructed with a header that starts with the magic bit
    telling us the protocol version, then it add a space followed by the type
    of the message and another space followed by the message id and finally a
    null byte:

        V1 type messageid\0

    The body follow the null byte. Then a full message is:

        V1 type messageid\0body

    This simple messaging protocol allows us to pass any kind of blob as a
    body and eventually a json while still beeing able to know the type of the
    message.
    """

    def __init__(self, body=None, id=None, type=FRAME_MESSAGE_TYPE):
        self.body = body or b""
        if not isinstance(self.body, bytes):
            self.body = self.body.encode('utf-8')

        self.id = id or uuid.uuid4().hex
        if not isinstance(self.id, bytes):
            self.id = self.id.encode('utf-8')

        self.type = type

    def __str__(self):
        return "Message: %s" % self.id.decode("utf-8")

    @classmethod
    def decode_frame(cls, frame):
        if not isinstance(frame, bytes):
            frame = frame.encode('utf-8')

        try:
            # get header and body
            header, body = frame.split(b"\0")

            # parse header
            # since we have only 1 version of the protocol we can ignore for now
            # the magic bit.
            _, type, id = header.split()

            # return the message classe
            return cls(body, id=id, type=type)
        except ValueError:
            raise MessageError("invalid message %r" % frame)

    def encode(self):
        """ encode a message to bytes """
        header = b" ".join([MAGIC_V1, self.type, self.id])
        return b"".join([header, b"\0", self.body])


def decode_frame(frame):
    return Message.decode_frame(frame)

def make_response(body, id=None):
    return Message(body, id=id, type=FRAME_RESPONSE_TYPE)

########NEW FILE########
__FILENAME__ = pidfile
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import errno
import os
import tempfile


class Pidfile(object):
    """
    Manage a PID file. If a specific name is provided
    it and '"%s.oldpid" % name' will be used. Otherwise
    we create a temp file using os.mkstemp.
    """

    def __init__(self, fname):
        self.fname = fname
        self.pid = None

    def create(self, pid):
        oldpid = self.validate()
        if oldpid:
            if oldpid == os.getpid():
                return
            raise RuntimeError("Already running on PID %s " \
                "(or pid file '%s' is stale)" % (os.getpid(), self.fname))

        self.pid = pid

        # Write pidfile
        fdir = os.path.dirname(self.fname)
        if fdir and not os.path.isdir(fdir):
            raise RuntimeError("%s doesn't exist. Can't create pidfile." \
                                    % fdir)
        fd, fname = tempfile.mkstemp(dir=fdir)
        pid_str = "%s\n" % self.pid
        os.write(fd, pid_str.encode('utf-8'))
        if self.fname:
            os.rename(fname, self.fname)
        else:
            self.fname = fname
        os.close(fd)

        # set permissions to -rw-r--r--
        os.chmod(self.fname, 420)

    def rename(self, path):
        self.unlink()
        self.fname = path
        self.create(self.pid)

    def unlink(self):
        """ delete pidfile"""
        try:
            with open(self.fname, "r") as f:
                pid1 = int(f.read() or 0)

            if pid1 == self.pid:
                os.unlink(self.fname)
        except:
            pass

    def validate(self):
        """ Validate pidfile and make it stale if needed"""
        if not self.fname:
            return
        try:
            with open(self.fname, "r") as f:
                wpid = int(f.read() or 0)

                if wpid <= 0:
                    return

                try:
                    os.kill(wpid, 0)
                    return wpid
                except OSError as e:
                    if e.args[0] == errno.ESRCH:
                        return
                    raise
        except IOError as e:
            if e.args[0] == errno.ENOENT:
                return
            raise

########NEW FILE########
__FILENAME__ = process
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
The process module wrap a process and IO redirection

"""


from datetime import timedelta
from functools import partial
import os
import signal
import shlex

import pyuv
import psutil
from psutil.error import AccessDenied
import six

from .events import EventEmitter
from .util import (bytestring, getcwd, check_uid, check_gid,
        bytes2human, substitute_env, IS_WINDOWS)
from .sync import atomic_read, increment, decrement

pyuv.Process.disable_stdio_inheritance()

def get_process_stats(process=None, interval=0):

    """Return information about a process. (can be an pid or a Process object)

    If process is None, will return the information about the current process.
    """
    if process is None:
        process = psutil.Process(os.getpid())

    stats = {}
    try:
        mem_info = process.get_memory_info()
        stats['mem_info1'] = bytes2human(mem_info[0])
        stats['mem_info2'] = bytes2human(mem_info[1])
    except AccessDenied:
        stats['mem_info1'] = stats['mem_info2'] = "N/A"

    try:
        stats['cpu'] = process.get_cpu_percent(interval=interval)
    except AccessDenied:
        stats['cpu'] = "N/A"

    try:
        stats['mem'] = round(process.get_memory_percent(), 1)
    except AccessDenied:
        stats['mem'] = "N/A"

    try:
        cpu_times = process.get_cpu_times()
        ctime = timedelta(seconds=sum(cpu_times))
        ctime = "%s:%s.%s" % (ctime.seconds // 60 % 60,
                        str((ctime.seconds % 60)).zfill(2),
                        str(ctime.microseconds)[:2])
    except AccessDenied:
        ctime = "N/A"

    stats['ctime'] = ctime
    return stats


class RedirectIO(object):

    pipes_count = 2

    def __init__(self, loop, process, stdio=[]):
        self.loop = loop
        self.process = process
        self._emitter = EventEmitter(loop)

        self._stdio = []
        self._channels = []

        # create (channel, stdio) pairs
        for label in stdio[:self.pipes_count]:
            # io registered can any label, so it's easy to redirect
            # stderr to stdout, just use the same label.
            p = pyuv.Pipe(loop)
            io = pyuv.StdIO(stream=p, flags=pyuv.UV_CREATE_PIPE | \
                                            pyuv.UV_READABLE_PIPE | \
                                            pyuv.UV_WRITABLE_PIPE)
            setattr(p, 'label', label)
            self._channels.append(p)
            self._stdio.append(io)

        # create remaining pipes
        for _ in range(self.pipes_count - len(self._stdio)):
            self._stdio.append(pyuv.StdIO(flags=pyuv.UV_IGNORE))

    def start(self):
        # start reading
        for p in self._channels:
            p.start_read(self._on_read)

    @property
    def stdio(self):
        return self._stdio

    def subscribe(self, label, listener):
        self._emitter.subscribe(label, listener)

    def unsubscribe(self, label, listener):
        self._emitter.unsubscribe(label, listener)

    def stop(self, all_events=False):
        for p in self._channels:
            if not p.closed:
                p.close()

        if all_events:
            self._emitter.close()

    def _on_read(self, handle, data, error):
        if not data:
            return

        label = getattr(handle, 'label')
        msg = dict(event=label, name=self.process.name, pid=self.process.pid,
                data=data)
        self._emitter.publish(label, msg)


class RedirectStdin(object):
    """ redirect stdin allows multiple sender to write to same pipe """

    def __init__(self, loop, process):
        self.loop = loop
        self.process = process
        self.channel = pyuv.Pipe(loop)
        self.stdio = pyuv.StdIO(stream=self.channel,
                flags=pyuv.UV_CREATE_PIPE | \
                        pyuv.UV_READABLE_PIPE | \
                        pyuv.UV_WRITABLE_PIPE )
        self._emitter = EventEmitter(loop)

    def start(self):
        self._emitter.subscribe("WRITE", self._on_write)
        self._emitter.subscribe("WRITELINES", self._on_writelines)

    def write(self, data):
        self._emitter.publish("WRITE", data)

    def writelines(self, data):
        self._emitter.publish("WRITELINES", data)

    def stop(self, all_events=False):
        if not self.channel.closed:
            self.channel.close()

        if all_events:
            self._emitter.close()

    def _on_write(self, evtype, data):
        self.channel.write(data)

    def _on_writelines(self, evtype, data):
        self.channel.writelines(data)

    def _on_read(self, handle, data, error):
        if not data:
            return

        label = getattr(handle, 'label')
        msg = dict(event=label, name=self.process.name, pid=self.process.pid,
                data=data)
        self._emitter.publish(label, msg)


class Stream(RedirectStdin):
    """ create custom stdio """

    def __init__(self, loop, process, id):
        super(Stream, self).__init__(loop, process)
        self.id = id

    def start(self):
        super(Stream, self).start()
        self.channel.start_read(self._on_read)

    def subscribe(self, listener):
        self._emitter.subscribe('READ', listener)

    def unsubscribe(self, listener):
        self._emitter.unsubscribe('READ', listener)

    def _on_read(self, handle, data, error):
        if not data:
            return

        msg = dict(event='READ', name=self.process.name, pid=self.process.pid,
                data=data)
        self._emitter.publish('READ', msg)


class ProcessWatcher(object):
    """ object to retrieve process stats """

    def __init__(self, loop, process):
        self.loop = loop
        self.process = process
        self._last_info = None
        self.on_refresh_cb = None
        self._active = 0
        self._refcount = 0
        self._emitter = EventEmitter(loop)


    @property
    def active(self):
        return atomic_read(self._active) > 0

    def subscribe(self, listener):
        self._refcount = increment(self._refcount)
        self._emitter.subscribe("stat", listener)
        self._start()

    def subscribe_once(self, listener):
        self._emitter.subscribe_once("stat", listener)

    def unsubscribe(self, listener):
        self._emitter.unsubscribe(".", listener)
        self._refcount = decrement(self._refcount)
        if not atomic_read(self._refcount):
            self.stop()

    def stop(self, all_events=False):
        if self.active:
            self._active = decrement(self._active)
            self._timer.stop()

        if all_events:
            self._emitter.close()

    def _async_refresh(self, handle):
        try:
            self._last_info = self.refresh()
        except psutil.error.NoSuchProcess:
            self.stop()
            return

        # create the message
        msg = self._last_info.copy()
        msg.update({'pid': self.process.pid, 'os_pid': self.process.os_pid})

        # publish it
        self._emitter.publish("stat", msg)

    def refresh(self, interval=0):
        return get_process_stats(self.process._pprocess, interval=interval)

    def _start(self):
        if not self.active:
            self._timer = pyuv.Timer(self.loop)
            # start the timer to refresh the informations
            # 0.1 is the minimum interval to fetch cpu stats for this
            # process.
            self._timer.start(self._async_refresh, 0.1, 0.1)
            self._active = increment(self._active)


class ProcessConfig(object):
    """ object to maintain a process config """

    DEFAULT_PARAMS = {
            "args": [],
            "env": {},
            "uid": None,
            "gid": None,
            "cwd": None,
            "shell": False,
            "redirect_output": [],
            "redirect_input": False,
            "custom_streams": [],
            "custom_channels": []}

    def __init__(self, name, cmd, **settings):
        """
        Initialize the ProcessConfig object

        Args:

        - **name**: name of the process
        - **cmd**: program command, string)

        Settings:

        - **args**: the arguments for the command to run. Can be a list or
          a string. If **args** is  a string, it's splitted using
          :func:`shlex.split`. Defaults to None.
        - **env**: a mapping containing the environment variables the command
          will run with. Optional
        - **uid**: int or str, user id
        - **gid**: int or st, user group id,
        - **cwd**: working dir
        - **shell**: boolean, run the script in a shell. (UNIX
          only),
        - **os_env**: boolean, pass the os environment to the program
        - **numprocesses**: int the number of OS processes to launch for
          this description
        - **flapping**: a FlappingInfo instance or, if flapping detection
          should be used. flapping parameters are:

          - **attempts**: maximum number of attempts before we stop the
            process and set it to retry later
          - **window**: period in which we are testing the number of
            retry
          - **retry_in**: seconds, the time after we restart the process
            and try to spawn them
          - **max_retry**: maximum number of retry before we give up
            and stop the process.
        - **redirect_output**: list of io to redict (max 2) this is a list of custom
          labels to use for the redirection. Ex: ["a", "b"]will
          redirect stdout & stderr and stdout events will be labeled "a"
        - **redirect_input**: Boolean (False is the default). Set it if
          you want to be able to write to stdin.
        - **graceful_timeout**: graceful time before we send a  SIGKILL
          to the process (which definitely kill it). By default 30s.
          This is a time we let to a process to exit cleanly.

        """
        self.name = name
        self.cmd = cmd
        self.settings = settings

    def __str__(self):
        return "process: %s" % self.name

    def make_process(self, loop, pid, label, env=None, on_exit=None):
        """ create a Process object from the configuration

        Args:

        - **loop**: main pyuv loop instance that will maintain the process
        - **pid**: process id, generally given by the manager
        - **label**: the job label. Usually the process type.
          context. A context can be for example an application.
        - **on_exit**: callback called when the process exited.

        """

        params = {}
        for name, default in self.DEFAULT_PARAMS.items():
            params[name] = self.settings.get(name, default)

        os_env = self.settings.get('os_env', False)
        if os_env:
            env = params.get('env') or {}
            env.update(os.environ)
            params['env'] = env

        if env is not None:
            params['env'].update(env)

        params['on_exit_cb'] = on_exit
        return Process(loop, pid, label, self.cmd, **params)

    def __getitem__(self, key):
        if key == "name":
            return self.name

        if key == "cmd":
            return self.cmd

        return self.settings[key]

    def __setitem__(self, key, value):
        if key in ("name", "cmd"):
            setattr(self, key, value)
        else:
            self.settings[key] = value

    def __contains__(self, key):
        if key in ('name', 'cmd'):
            return True

        if key in self.settings:
            return True

        return False

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default

    def to_dict(self):
        d = dict(name=self.name, cmd=self.cmd)
        d.update(self.settings)
        return d

    @classmethod
    def from_dict(cls, config):
        d = config.copy()
        try:
            name = d.pop('name')
            cmd = d.pop('cmd')
        except KeyError:
            raise ValueError("invalid config dict")

        return cls(name, cmd, **d)

class Process(object):
    """ class wrapping a process

    Args:

    - **loop**: main application loop (a pyuv Loop instance)
    - **name**: name of the process
    - **cmd**: program command, string)
    - **args**: the arguments for the command to run. Can be a list or
      a string. If **args** is  a string, it's splitted using
      :func:`shlex.split`. Defaults to None.
    - **env**: a mapping containing the environment variables the command
      will run with. Optional
    - **uid**: int or str, user id
    - **gid**: int or st, user group id,
    - **cwd**: working dir
    - **detach**: the process is launched but won't be monitored and
      won't exit when the manager is stopped.
    - **shell**: boolean, run the script in a shell. (UNIX
      only)
    - **redirect_output**: list of io to redict (max 2) this is a list of custom
      labels to use for the redirection. Ex: ["a", "b"]will
      redirect stdoutt & stderr and stdout events will be labeled "a"
    - **redirect_input**: Boolean (False is the default). Set it if
      you want to be able to write to stdin.
    - **custom_streams**: list of additional streams that should be created
      and passed to process. This is a list of streams labels. They become
      available through :attr:`streams` attribute.
    - **custom_channels**: list of additional channels that should be passed to
      process.

    """


    def __init__(self, loop, pid, name, cmd, args=None, env=None, uid=None,
            gid=None, cwd=None, detach=False, shell=False,
            redirect_output=[], redirect_input=False, custom_streams=[],
            custom_channels=[], on_exit_cb=None):
        self.loop = loop
        self.pid = pid
        self.name = name
        self.cmd = cmd
        self.env = env or {}

        # set command
        self.cmd = bytestring(cmd)

        # remove args from the command
        args_ = shlex.split(self.cmd)
        if len(args_) == 1:
            self.args = []
        else:
            self.cmd = args_[0]
            self.args = args_[1:]

        # if args have been passed to the options then add them
        if args and args is not None:
            if isinstance(args, six.string_types):
                self.args.extend(shlex.split(bytestring(args)))
            else:
                self.args.extend([bytestring(arg) for arg in args])

        # replace envirnonnement variable in args
        # $PORT for example will become the given env variable.
        self.args = [substitute_env(arg, self.env) for arg in self.args]

        if shell:
            self.args = ['-c', self.cmd] + self.args
            self.cmd = "sh"

        self.uid = uid
        self.gid = gid
        if not IS_WINDOWS:
            if self.uid is not None:
                self.uid = check_uid(uid)

            if self.gid is not None:
                self.gid = check_gid(gid)

        self.cwd = cwd or getcwd()
        self.redirect_output = redirect_output
        self.redirect_input = redirect_input
        self.custom_streams = custom_streams
        self.custom_channels = custom_channels

        self._redirect_io = None
        self._redirect_in = None
        self.streams = {}
        self.detach = detach
        self.on_exit_cb = on_exit_cb
        self._process = None
        self._pprocess = None
        self._process_watcher = None
        self._os_pid = None
        self._info = None
        self.stopped = False
        self.graceful_time = 0
        self.graceful_timeout = None
        self.once = False

        self._setup_stdio()

    def _setup_stdio(self):
        # for now we ignore all stdin
        if not self.redirect_input:
            self._stdio = [pyuv.StdIO(flags=pyuv.UV_IGNORE)]
        else:
            self._redirect_in = RedirectStdin(self.loop, self)
            self._stdio = [self._redirect_in.stdio]
        self._redirect_io = RedirectIO(self.loop, self,
                self.redirect_output)
        self._stdio.extend(self._redirect_io.stdio)
        # create custom streams,
        for label in self.custom_streams:
            stream = self.streams[label] = Stream(self.loop, self,
                len(self._stdio))
            self._stdio.append(stream.stdio)
        # create containers for custom channels.
        for channel in self.custom_channels:
            assert not channel.closed, \
                "Closed channel {0!r} can't be passed to process!" \
                    .format(channel)
            self._stdio.append(pyuv.StdIO(stream=channel,
                flags=pyuv.UV_INHERIT_STREAM))

    def spawn(self, once=False, graceful_timeout=None, env=None):
        """ spawn the process """

        self.once = once
        self.graceful_timeout = graceful_timeout

        if env is not None:
            self.env.update(env)

        kwargs = dict(
                file = self.cmd,
                exit_callback = self._exit_cb,
                args = self.args,
                env = self.env,
                cwd = self.cwd,
                stdio = self._stdio)

        flags = 0
        if self.uid is not None:
            kwargs['uid'] = self.uid
            flags = pyuv.UV_PROCESS_SETUID

        if self.gid is not None:
            kwargs['gid'] = self.gid
            flags = flags | pyuv.UV_PROCESS_SETGID

        if self.detach:
            flags = flags | pyuv.UV_PROCESS_DETACHED

        self.running = True
        self._process = pyuv.Process(self.loop)

        # spawn the process
        self._process.spawn(**kwargs)
        self._running = True
        self._os_pid = self._process.pid
        self._pprocess = psutil.Process(self._process.pid)

        # start to cycle the cpu stats so we can have an accurate number on
        # the first call of ``Process.stats``
        self.loop.queue_work(self._init_cpustats)


        # start redirecting IO
        self._redirect_io.start()

        if self._redirect_in is not None:
            self._redirect_in.start()

        for stream in self.streams.values():
            stream.start()


    @property
    def active(self):
        return self._process.active

    @property
    def closed(self):
        return self._process.closed

    @property
    def os_pid(self):
        """ return the process pid """
        if self._os_pid is None:
            self._os_pid = self._process.pid
        return self._os_pid

    @property
    def info(self):
        """ return the process info. If the process is monitored it
        return the last informations stored asynchronously by the watcher"""

        # info we have on this process
        if self._info is None:
            self._info = dict(pid=self.pid, name=self.name, cmd=self.cmd,
                    args=self.args, env=self.env, uid=self.uid, gid=self.gid,
                    os_pid=None, create_time=None, commited=self.once,
                    redirect_output=self.redirect_output,
                    redirect_input=self.redirect_input,
                    custom_streams=self.custom_streams)

        if (self._info.get('create_time') is None and
                self._pprocess is not None):

            self._info.update({'os_pid': self.os_pid,
                'create_time':self._pprocess.create_time})

        self._info['active'] = self._process.active
        return self._info

    @property
    def stats(self):
        if not self._pprocess:
            return

        return get_process_stats(self._pprocess, 0.0)

    @property
    def status(self):
        """ return the process status """
        if not self._pprocess:
            return
        return self._pprocess.status


    def __lt__(self, other):
        return (self.pid != other.pid and
                self.graceful_time < other.graceful_time)

    __cmp__ = __lt__

    def monitor(self, listener=None):
        """ start to monitor the process

        Listener can be any callable and receive *("stat", process_info)*
        """

        if not self._process_watcher:
            self._process_watcher = ProcessWatcher(self.loop, self)

        self._process_watcher.subscribe(listener)

    def unmonitor(self, listener):
        """ stop monitoring this process.

        listener is the callback passed to the monitor function
        previously.
        """
        if not self._process_watcher:
            return

        self._process_watcher.unsubscribe(listener)

    def monitor_io(self, io_label, listener):
        """ subscribe to registered IO events """
        if not self._redirect_io:
            raise IOError("%s not redirected" % self.name)
        self._redirect_io.subscribe(io_label, listener)

    def unmonitor_io(self, io_label, listener):
        """ unsubscribe to the IO event """
        if not self._redirect_io:
            return
        self._redirect_io.unsubscribe(io_label, listener)

    def write(self, data):
        """ send data to the process via stdin"""
        if not self._redirect_in:
            raise IOError("stdin not redirected")
        self._redirect_in.write(data)

    def writelines(self, data):
        """ send data to the process via stdin"""

        if not self._redirect_in:
            raise IOError("stdin not redirected")
        self._redirect_in.writelines(data)

    def stop(self):
        """ stop the process """
        self.kill(signal.SIGTERM)

    def kill(self, signum):
        """ send a signal to the process """
        if not self.active:
            return

        self._process.kill(signum)

    def close(self):
        self._process.close()

    def _init_cpustats(self):
        try:
            get_process_stats(self._pprocess, 0.1)
        except psutil.error.NoSuchProcess:
            # catch this error. It can can happen when the process is closing
            # very fast
            pass


    def _exit_cb(self, handle, exit_status, term_signal):
        if self._redirect_io is not None:
            self._redirect_io.stop(all_events=True)

        if self._redirect_in is not None:
            self._redirect_in.stop(all_events=True)

        for custom_io in self.streams.values():
            custom_io.stop(all_events=True)

        if self._process_watcher is not None:
            self._process_watcher.stop(all_events=True)

        self._running = False
        handle.close()

        # handle the exit callback
        if self.on_exit_cb is not None:
            self.on_exit_cb(self, exit_status, term_signal)

########NEW FILE########
__FILENAME__ = procfile
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
module to parse and manage a Procfile
"""

try:
    import configparser
except ImportError:
    import ConfigParser as configparser

from collections import OrderedDict
import os
import re
import shlex

RE_LINE = re.compile(r'^([A-Za-z0-9_]+):\s*(.+)$')

class Procfile(object):
    """ Procfile object to parse a procfile and a list of given
    environnment files. """

    def __init__(self, procfile, root=None, envs=None):
        """ main constructor

        Attrs:

        - **procfile**: procfile path
        - **evs**: list of .envs paths, each envs will be added tho the
          global procfile environment"""

        self.procfile = procfile

        if not root:
            self.root = os.path.dirname(procfile) or "."
        else:
            self.root = root

        # set default env
        default_env = os.path.join(self.root, '.env')
        self.envs = [default_env]
        if envs is not None:
            self.envs.extend(envs)

        # initialize value
        self.cfg = self.parse(self.procfile)
        self.env = get_env(self.envs)

        self.redirect_input = []
        self.uid = None
        self.gid = None

        for k, v in self.env.items():
            if k == "GAFFER_UID":
                self.uid = v
            elif k == "GAFFER_GID":
                self.gid = v
            elif k == "GAFFER_REDIRECT_INPUT":
                self.redirect_input = v.split()

        # used to cache the appname
        self._appname = None

    def processes(self):
        """ iterator over the configuration """
        return self.cfg.items()

    def parse(self, procfile):
        """ main function to parse a procfile. It returns a dict """
        cfg = OrderedDict()
        with open(procfile) as f:
            lines = f.readlines()
            for line in lines:
                m = RE_LINE.match(line)
                if m:
                    cfg[m.group(1)] = m.group(2)
        return cfg

    def get_appname(self):
        if not self._appname:
            if self.root == ".":
                path = os.getcwd()
            else:
                path = self.root
            self._appname = os.path.split(path)[1]

        return self._appname

    def as_dict(self, name, concurrency_settings=None):
        """ return a procfile line as a JSON object usable with
        the command ``gafferctl load`` . """

        cmd, args = self.parse_cmd(self.cfg[name])
        concurrency_settings = concurrency_settings or {}

        return OrderedDict([("name", name),("cmd",  cmd),
            ("args", args),("env", self.env),
            ('numprocesses', concurrency_settings.get(name, 1))])

    def as_configparser(self, concurrency_settings=None):
        """ return a ConfigParser object. It can be used to generate a
        gafferd setting file or a configuration file that can be
        included. """

        parser = configparser.ConfigParser()
        concurrency_settings = concurrency_settings or {}

        ln = 0
        dconf = OrderedDict()
        for k, v in self.cfg.items():
            cmd, args = self.parse_cmd(v)
            name = "%s:%s" % (self.get_appname(), k)

            dconf["process:%s" % name] = OrderedDict([("cmd", cmd),
                ("args", " ".join(args)), ("priority", ln),
                ('numprocesses', concurrency_settings.get(k, 1))])
            ln += 1

        parser.read_dict(dconf)
        return parser

    def parse_cmd(self, v):
        args_ = shlex.split(v)
        cmd = args_[0]
        if len(args_) > 1:
            args = args_[1:]
        else:
            args = []
        return cmd, args


def get_env(envs=[]):
    env = {}
    for path in envs:
        if os.path.isfile(path):
            with open(path, 'r') as f:
                lines = f.readlines()
                for line in lines:
                    p = line.split('=', 1)
                    if len(p) == 2:
                        k, v = p
                        # remove double quotes
                        v = v.strip('\n').replace('\"','')
                        env[k] = v
    return env

########NEW FILE########
__FILENAME__ = pubsub
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from functools import partial

from .error import TopicError
from .events import EventEmitter

class EventChannel(object):
    """ An event channel.

    This channel is used to collect global or process events """

    def __init__(self, topic):
        self.topic = topic
        self._emitter = EventEmitter(topic.manager.loop)

    def bind(self, event, callback):
        self._emitter.subscribe(event, callback)

    def unbind(self, event, callback):
        self._emitter.unsubscribe(event, callback)

    def bind_all(self, callback):
        self._emitter.subscribe(".", callback)

    def unbind_all(self, callback):
        self._emitter.unsubscribe(".", callback)

    def close(self):
        self._emitter.close()
        self.topic.unsubscribe(self)

    def dispatch_event(self, evtype, event):
        self._emitter.publish(evtype, event)


class StatChannel(object):
    """ a channel to collect stats.

    This channel is used to collect stats or stream data. """

    def __init__(self, topic):
        self.topic = topic
        self._emitter = EventEmitter(topic.manager.loop)

    def bind(self, callback):
        self._emitter.subscribe("DATA", partial(self._on_message, callback))

    def unbind(self, callback):
        self._emitter.unsubscribe("DATA", partial(self._on_message, callback))

    def close(self):
        self._emitter.close()
        self.topic.unsubscribe(self)

    def dispatch_message(self, message):
        self._emitter.publish("DATA", message)

    def _on_message(self, callback, evtype, msg):
        callback(msg)


class Topic(object):
    """ A subscribed topic.

    A topic receive events and dispatch to all subscribed channels

    Available topics:

    - ``EVENTS``: all the manager events.
    - ``PROCESS:<PID>``: manager events for a pid
    - ``PROCESS:<APPNAME>.<PROCESSNAME>`` manager events for all pids
      associated to ``<APPNAME>.<PROCESSNAME>``
    - ``STATS:<PID>``: collect stats for this pid
    - ``STATS:<APPNAME>.<PROCNAME>``: collect stats for all pids associated to
      ``<APPNAME>.<PROCESSNAME>``
    - ``STREAM:<PID>`` -> get streams for this pid

    ``<PID>``: process ID
    ``<APPNAME>``: name of the app
    ``<PROCNAME>``: template name
    """

    def __init__(self, name, manager):
        self.name = name
        self.manager = manager

        parts = self.name.split(":", 1)
        self.pid = None
        if len(parts) == 1:
            self.source = parts[0].upper()
            self.target = "."
        else:
            self.source, self.target = parts[0].upper(), parts[1].lower()
            if self.target.isdigit():
                self.pid = int(self.target)

        self.channels = set()
        self.active = False

    def start(self):
        if self.active:
            return

        if self.source == "EVENTS":
            self.manager.events.subscribe(self.target, self._dispatch_events)
        elif self.source == "PROCESS":
            self.manager.events.subscribe("proc.%s" % self.target,
                    self._dispatch_process_events)
        elif self.source == "JOB":
            self.manager.events.subscribe("job.%s" % self.target,
                    self._dispatch_job_events)
        elif self.source == "STATS":
            if self.pid is not None:
                proc = self.manager.get_process(self.pid)
                proc.monitor(self._dispatch_data)
            else:
                state = self.manager._get_locked_state(self.target)
                for proc in state.running:
                    proc.monitor(self._dispatch_data)
        elif self.source == "STREAM":
            if not self.pid:
                raise TopicError(400, "invalid topic")

            proc = self.manager.get_process(self.pid)
            proc.monitor_io(self.target, self._dispatch_data)
        else:
            raise TopicError(400, "invalid topic")

        self.active = True

    def stop(self):
        if not self.active:
            return

        if self.source == "EVENTS":
            self.manager.events.unsubscribe(self.target,
                    self._dispatch_events)
        elif self.source == "PROCESS":
            self.manager.events.unsubscribe("proc.%s" % self.target,
                    self._dispatch_process_events)
        elif self.source == "JOB":
            self.manager.events.unsubscribe("job.%s" % self.target,
                    self._dispatch_process_events)
        elif self.source == "STATS":
            if self.pid is not None:
                proc = self.manager.get_process(self.pid)
                proc.unmonitor(self._dispatch_data)
            else:
                state = self.manager._get_locked_state(self.target)
                for proc in state.running:
                    proc.unmonitor(self._dispatch_data)
        elif self.source == "STREAM":
            if self.pid:
                proc = self.manager.get_process(self.pid)
                proc.unmonitor_io(".", self._dispatch_events)

        self.active = False

    def close(self):
        if not self.active:
            return

        self.stop()

        for chan in self.channels:
            try:
                chan.close()
            except:
                pass

    def subscribe(self):
        if not self.active:
            self.start()

        if self.source in ("EVENTS", "PROCESS", "JOB", "STREAM"):
            chan = EventChannel(self)
        else:
            chan = StatChannel(self)

        self.channels.add(chan)
        return chan

    def unsubscribe(self, chan):
        self.channels.remove(chan)
        if not self.channels:
            self.stop()

    def _dispatch_events(self, evtype, event):
        for c in self.channels:
            c.dispatch_event(evtype, event)

    def _dispatch_process_events(self, evtype, event):
        evtype = evtype.split("proc.%s." % self.target, 1)[1]
        self._dispatch_events(evtype, event)

    def _dispatch_job_events(self, evtype, event):

        evtype = evtype.split("job.%s." % self.target, 1)[1]
        self._dispatch_events(evtype, event)

    def _dispatch_data(self, evtype, data):
        for c in self.channels:
            c.dispatch_message(data)

########NEW FILE########
__FILENAME__ = sig_handler
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import signal

import pyuv

class BaseSigHandler(object):
    """ A simple gaffer application to handle signals """

    QUIT_SIGNALS = (signal.SIGQUIT, signal.SIGTERM, signal.SIGINT)

    def __init__(self):
        self._sig_handlers = []

    def start(self, loop):
        self.loop = loop

         # quit signals handling
        for signum in self.QUIT_SIGNALS:
            self._start_signal(self.handle_quit, signum)

        # reload signal
        self._start_signal(self.handle_reload, signal.SIGHUP)

    def _start_signal(self, callback, signum):
        h = pyuv.Signal(self.loop)
        h.start(callback, signum)
        h.unref()
        self._sig_handlers.append(h)

    def stop(self):
        for h in self._sig_handlers:
            try:
                h.stop()
            except:
                pass

    def restart(self):
        # we never restart, just return
        return

    def handle_quit(self, handle, signum):
        raise NotImplementedError

    def handle_reload(self, handle, signum):
        raise NotImplementedError



class SigHandler(BaseSigHandler):
    """ A simple gaffer application to handle signals """

    def start(self, loop, manager):
        self.manager = manager
        super(SigHandler, self).start(loop)

    def handle_quit(self, handle, *args):
        self.manager.stop()

    def handle_reload(self, handle, *args):
        self.manager.restart()

########NEW FILE########
__FILENAME__ = basehandler
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
    sockjs.tornado.basehandler
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    Various base http handlers
"""

import datetime
import socket
import logging

from tornado.web import asynchronous, RequestHandler

CACHE_TIME = 31536000


class BaseHandler(RequestHandler):
    """Base request handler with set of helpers."""
    def initialize(self, server):
        """Initialize request

        `server`
            SockJSRouter instance.
        """
        self.server = server
        self.logged = False

    # Statistics
    def prepare(self):
        """Increment connection count"""
        self.logged = True
        self.server.stats.on_conn_opened()

    def _log_disconnect(self):
        """Decrement connection count"""
        if self.logged:
            self.server.stats.on_conn_closed()
            self.logged = False

    def finish(self):
        """Tornado `finish` handler"""
        self._log_disconnect()

        super(BaseHandler, self).finish()

    def on_connection_close(self):
        """Tornado `on_connection_close` handler"""
        self._log_disconnect()

    # Various helpers
    def enable_cache(self):
        """Enable client-side caching for the current request"""
        self.set_header('Cache-Control', 'max-age=%d, public' % CACHE_TIME)

        d = datetime.datetime.now() + datetime.timedelta(seconds=CACHE_TIME)
        self.set_header('Expires', d.strftime('%a, %d %b %Y %H:%M:%S'))

        self.set_header('access-control-max-age', CACHE_TIME)

    def disable_cache(self):
        """Disable client-side cache for the current request"""
        self.set_header('Cache-Control', 'no-store, no-cache, must-revalidate, max-age=0')

    def handle_session_cookie(self):
        """Handle JSESSIONID cookie logic"""
        # If JSESSIONID support is disabled in the settings, ignore cookie logic
        if not self.server.settings['jsessionid']:
            return

        cookie = self.cookies.get('JSESSIONID')

        if not cookie:
            cv = 'dummy'
        else:
            cv = cookie.value

        self.set_cookie('JSESSIONID', cv)

    def safe_finish(self):
        """Finish session. If it will blow up - connection was set to Keep-Alive and
        client dropped connection, ignore any IOError or socket error."""
        try:
            self.finish()
        except (socket.error, IOError):
            # We don't want to raise IOError exception if finish() call fails.
            # It can happen if connection is set to Keep-Alive, but client
            # closes connection after receiving response.
            logging.debug('Ignoring IOError in safe_finish()')
            pass


class PreflightHandler(BaseHandler):
    """CORS preflight handler"""

    @asynchronous
    def options(self, *args, **kwargs):
        """XHR cross-domain OPTIONS handler"""
        self.enable_cache()
        self.handle_session_cookie()
        self.preflight()

        if self.verify_origin():
            allowed_methods = getattr(self, 'access_methods', 'OPTIONS, POST')
            self.set_header('Access-Control-Allow-Methods', allowed_methods)
            self.set_header('Allow', allowed_methods)

            self.set_status(204)
        else:
            # Set forbidden
            self.set_status(403)

        self.finish()

    def preflight(self):
        """Handles request authentication"""
        origin = self.request.headers.get('Origin', '*')

        # Respond with '*' to 'null' origin
        if origin == 'null':
            origin = '*'

        self.set_header('Access-Control-Allow-Origin', origin)

        headers = self.request.headers.get('Access-Control-Request-Headers')
        if headers:
            self.set_header('Access-Control-Allow-Headers', headers)

        self.set_header('Access-Control-Allow-Credentials', 'true')

    def verify_origin(self):
        """Verify if request can be served"""
        # TODO: Verify origin
        return True

########NEW FILE########
__FILENAME__ = conn
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.conn
    ~~~~~~~~~~~~~~~~~~~

    SockJS connection interface
"""


class SockJSConnection(object):
    def __init__(self, session):
        """Connection constructor.

        `session`
            Associated session
        """
        self.session = session

    # Public API
    def on_open(self, request):
        """Default on_open() handler.

        Override when you need to do some initialization or request validation.
        If you return False, connection will be rejected.

        You can also throw Tornado HTTPError to close connection.

        `request`
            ``ConnectionInfo`` object which contains caller IP address, query string
            parameters and cookies associated with this request (if any).
        """
        pass

    def on_message(self, message):
        """Default on_message handler. Must be overridden in your application"""
        raise NotImplementedError()

    def on_close(self):
        """Default on_close handler."""
        pass

    def send(self, message, binary=False):
        """Send message to the client.

        `message`
            Message to send.
        """
        if not self.is_closed:
            self.session.send_message(message, binary=binary)

    def broadcast(self, clients, message):
        """Broadcast message to the one or more clients.
        Use this method if you want to send same message to lots of clients, as
        it contains several optimizations and will work fast than just having loop
        in your code.

        `clients`
            Clients iterable
        `message`
            Message to send.
        """
        self.session.broadcast(clients, message)

    def close(self):
        self.session.close()

    @property
    def is_closed(self):
        """Check if connection was closed"""
        return self.session.is_closed

########NEW FILE########
__FILENAME__ = migrate
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.migrate
    ~~~~~~~~~~~~~~~~~~~~~~

    `tornado.websocket` to `sockjs.tornado` migration helper.
"""

from . import conn


class WebsocketHandler(conn.SockJSConnection):
    """If you already use Tornado websockets for your application and
    want try sockjs-tornado, change your handlers to derive from this
    WebsocketHandler class. There are some limitations, for example
    only self.request only contains remote_ip, cookies and arguments
    collection"""
    def open(self):
        """open handler"""
        pass

    def on_open(self, info):
        """sockjs-tornado on_open handler"""
        # Store some properties
        self.remote_ip = info.remote_ip

        # Create fake request object
        self.request = info

        # Call open
        self.open()

    def write_message(self, msg):
        self.send(msg)

########NEW FILE########
__FILENAME__ = periodic
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.periodic
    ~~~~~~~~~~~~~~~~~~~~~~~

    This module implements customized PeriodicCallback from tornado with
    support of the sliding window.
"""

import time
import logging


class Callback(object):
    """Custom implementation of the Tornado.Callback with support
    of callback timeout delays.
    """
    def __init__(self, callback, callback_time, io_loop):
        """Constructor.

        `callback`
            Callback function
        `callback_time`
            Callback timeout value (in milliseconds)
        `io_loop`
            io_loop instance
        """
        self.callback = callback
        self.callback_time = callback_time
        self.io_loop = io_loop
        self._running = False

        self.next_run = None

    def calculate_next_run(self):
        """Caltulate next scheduled run"""
        return time.time() + self.callback_time / 1000.0

    def start(self, timeout=None):
        """Start callbacks"""
        self._running = True

        if timeout is None:
            timeout = self.calculate_next_run()

        self.io_loop.add_timeout(timeout, self._run)

    def stop(self):
        """Stop callbacks"""
        self._running = False

    def delay(self):
        """Delay callback"""
        self.next_run = self.calculate_next_run()

    def _run(self):
        if not self._running:
            return

        # Support for shifting callback window
        if self.next_run is not None and time.time() < self.next_run:
            self.start(self.next_run)
            self.next_run = None
            return

        next_call = None
        try:
            next_call = self.callback()
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            logging.error("Error in periodic callback", exc_info=True)

        if self._running:
            self.start(next_call)

########NEW FILE########
__FILENAME__ = proto
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.proto
    ~~~~~~~~~~~~~~~~~~~~

    SockJS protocol related functions
"""
import logging

# TODO: Add support for ujson module once they can accept unicode strings

# Try to find best json encoder available
try:
    # Check for simplejson
    import simplejson

    json_encode = lambda data: simplejson.dumps(data, separators=(',', ':'))
    json_decode = lambda data: simplejson.loads(data)
    JSONDecodeError = ValueError

    logging.debug('sockjs.tornado will use simplejson module')
except ImportError:
    # Use slow json
    import json

    logging.debug('sockjs.tornado will use json module')

    json_encode = lambda data: json.dumps(data, separators=(',', ':'))
    json_decode = lambda data: json.loads(data)
    JSONDecodeError = ValueError

# Protocol handlers
CONNECT = 'o'
DISCONNECT = 'c'
MESSAGE = 'm'
HEARTBEAT = 'h'


# Various protocol helpers
def disconnect(code, reason):
    """Return SockJS packet with code and close reason

    `code`
        Closing code
    `reason`
        Closing reason
    """
    return 'c[%d,"%s"]' % (code, reason)

########NEW FILE########
__FILENAME__ = router
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.router
    ~~~~~~~~~~~~~~~~~~~~~

    SockJS protocol router implementation.
"""
from tornado import version_info

import gaffer.tornado_pyuv as ioloop
from . import transports, session, sessioncontainer, static, stats, proto


DEFAULT_SETTINGS = {
    # Sessions check interval in seconds
    'session_check_interval': 1,
    # Session expiration in seconds
    'disconnect_delay': 5,
    # Heartbeat time in seconds. Do not change this value unless
    # you absolutely sure that new value will work.
    'heartbeat_delay': 25,
    # Enabled protocols
    'disabled_transports': [],
    # SockJS location
    'sockjs_url': 'http://cdn.sockjs.org/sockjs-0.3.min.js',
    # Max response body size
    'response_limit': 128 * 1024,
    # Enable or disable JSESSIONID cookie handling
    'jsessionid': True,
    # Should sockjs-tornado flush messages immediately or queue then and
    # flush on next ioloop tick
    'immediate_flush': True,
    # Enable or disable Nagle for persistent transports
    'disable_nagle': True,
    # Enable IP checks for polling transports. If enabled, all subsequent
    # polling calls should be from the same IP address.
    'verify_ip': True
    }

GLOBAL_HANDLERS = [
    ('xhr_send', transports.XhrSendHandler),
    ('jsonp_send', transports.JSONPSendHandler)
]

TRANSPORTS = {
    'websocket': transports.WebSocketTransport,
    'xhr': transports.XhrPollingTransport,
    'xhr_streaming': transports.XhrStreamingTransport,
    'jsonp': transports.JSONPTransport,
    'eventsource': transports.EventSourceTransport,
    'htmlfile': transports.HtmlFileTransport
}

STATIC_HANDLERS = {
    '/chunking_test': static.ChunkingTestHandler,
    '/info': static.InfoHandler,
    '/iframe[0-9-.a-z_]*.html': static.IFrameHandler,
    '/websocket': transports.RawWebSocketTransport,
    '/?': static.GreetingsHandler
}


class SockJSRouter(object):
    """SockJS protocol router"""
    def __init__(self,
                 connection,
                 prefix='',
                 user_settings=dict(),
                 io_loop=None):
        """Constructor.

        `connection`
            SockJSConnection class
        `prefix`
            Connection prefix
        `user_settings`
            Settings dictionary
        `io_loop`
            Optional IOLoop instance
        """

        # TODO: Version check
        if version_info[0] < 2:
            raise Exception('sockjs-tornado requires Tornado 2.0 or higher.')

        # Store connection class
        self._connection = connection

        # Initialize io_loop
        self.io_loop = io_loop or ioloop.IOLoop.instance()

        # Settings
        self.settings = DEFAULT_SETTINGS.copy()
        if user_settings:
            self.settings.update(user_settings)

        self.websockets_enabled = 'websocket' not in self.settings['disabled_transports']
        self.cookie_needed = self.settings['jsessionid']

        # Sessions
        self._sessions = sessioncontainer.SessionContainer()

        check_interval = self.settings['session_check_interval'] * 1000
        self._sessions_cleanup = ioloop.PeriodicCallback(self._sessions.expire,
                                                         check_interval,
                                                         self.io_loop)
        self._sessions_cleanup.start()
        self._sessions_cleanup._timer.unref()

        # Stats
        self.stats = stats.StatsCollector(self.io_loop)

        # Initialize URLs
        base = prefix + r'/[^/.]+/(?P<session_id>[^/.]+)'

        # Generate global handler URLs
        self._transport_urls = [('%s/%s$' % (base, p[0]), p[1], dict(server=self))
                                for p in GLOBAL_HANDLERS]

        for k, v in TRANSPORTS.items():
            if k in self.settings['disabled_transports']:
                continue

            # Only version 1 is supported
            self._transport_urls.append(
                (r'%s/%s$' % (base, k),
                 v,
                 dict(server=self))
                )

        # Generate static URLs
        self._transport_urls.extend([('%s%s' % (prefix, k), v, dict(server=self)) 
                                     for k, v in STATIC_HANDLERS.items()])

    @property
    def urls(self):
        """List of the URLs to be added to the Tornado application"""
        return self._transport_urls

    def apply_routes(self, routes):
        """Feed list of the URLs to the routes list. Returns list"""
        routes.extend(self._transport_urls)
        return routes

    def create_session(self, session_id, register=True):
        """Creates new session object and returns it.

        `request`
            Request that created the session. Will be used to get query string
            parameters and cookies
        `register`
            Should be session registered in a storage. Websockets don't
            need it.
        """
        # TODO: Possible optimization here for settings.get
        s = session.Session(self._connection,
                            self,
                            session_id,
                            self.settings.get('disconnect_delay')
                            )

        if register:
            self._sessions.add(s)

        return s

    def get_session(self, session_id):
        """Get session by session id

        `session_id`
            Session id
        """
        return self._sessions.get(session_id)

    def get_connection_class(self):
        """Return associated connection class"""
        return self._connection

    # Broadcast helper
    def broadcast(self, clients, msg):
        """Optimized `broadcast` implementation. Depending on type of the session, will json-encode
        message once and will call either `send_message` or `send_jsonifed`.

        `clients`
            Clients iterable
        `msg`
            Message to send
        """
        json_msg = None

        count = 0

        for c in clients:
            sess = c.session
            if not sess.is_closed:
                if sess.send_expects_json:
                    if json_msg is None:
                        json_msg = proto.json_encode(msg)
                    sess.send_jsonified(json_msg, False)
                else:
                    sess.send_message(msg, stats=False)

                count += 1

        self.stats.on_pack_sent(count)

########NEW FILE########
__FILENAME__ = session
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.session
    ~~~~~~~~~~~~~~~~~~~~~~

    SockJS session implementation.
"""

import logging

from . import sessioncontainer, periodic, proto
from .util import bytes_to_str

class ConnectionInfo(object):
    """Connection information object.

    Will be passed to the ``on_open`` handler of your connection class.

    Has few properties:

    `ip`
        Caller IP address
    `cookies`
        Collection of cookies
    `arguments`
        Collection of the query string arguments
    `headers`
        Collection of explicitly exposed headers from the request including:
        origin, referer, x-forward-for (and associated headers)
    `path`
        Request uri path
    """
    _exposed_headers = set(['referer', 'x-client-ip', 'x-forwarded-for',
                            'x-cluster-client-ip', 'via', 'x-real-ip'])
    def __init__(self, ip, cookies, arguments, headers, path):
        self.ip = ip
        self.cookies = cookies
        self.arguments = arguments
        self.headers = {}
        self.path = path

        for header in headers:
            if header.lower() in ConnectionInfo._exposed_headers:
                self.headers[header] = headers[header]

    def get_argument(self, name):
        """Return single argument by name"""
        val = self.arguments.get(name)
        if val:
            return val[0]
        return None

    def get_cookie(self, name):
        """Return single cookie by its name"""
        return self.cookies.get(name)

    def get_header(self, name):
        """Return single header by its name"""
        return self.headers.get(name)


# Session states
CONNECTING = 0
OPEN = 1
CLOSING = 2
CLOSED = 3


class BaseSession(object):
    """Base session implementation class"""
    def __init__(self, conn, server):
        """Base constructor.

        `conn`
            Connection class
        `server`
            SockJSRouter instance
        """
        self.server = server
        self.stats = server.stats

        self.send_expects_json = False

        self.handler = None
        self.state = CONNECTING

        self.conn_info = None

        self.conn = conn(self)

        self.close_reason = None

    def set_handler(self, handler):
        """Set transport handler
        ``handler``
            Handler, should derive from the `sockjs.tornado.transports.base.BaseTransportMixin`.
        """
        if self.handler is not None:
            raise Exception('Attempted to overwrite BaseSession handler')

        self.handler = handler
        self.transport_name = self.handler.name

        if self.conn_info is None:
            self.conn_info = handler.get_conn_info()
            self.stats.on_sess_opened(self.transport_name)

        return True

    def verify_state(self):
        """Verify if session was not yet opened. If it is, open it and call connections `on_open`"""
        if self.state == CONNECTING:
            self.state = OPEN

            self.conn.on_open(self.conn_info)

    def remove_handler(self, handler):
        """Remove active handler from the session

        `handler`
            Handler to remove
        """
        # Attempt to remove another handler
        if self.handler != handler:
            raise Exception('Attempted to remove invalid handler')

        self.handler = None

    def close(self, code=3000, message='Go away!'):
        """Close session or endpoint connection.

        `code`
            Closing code
        `message`
            Close message
        """
        if self.state != CLOSED:
            try:
                self.conn.on_close()
            except:
                logging.debug("Failed to call on_close().", exc_info=True)
            finally:
                self.state = CLOSED
                self.close_reason = (code, message)

            # Bump stats
            self.stats.on_sess_closed(self.transport_name)

            # If we have active handler, notify that session was closed
            if self.handler is not None:
                self.handler.session_closed()

    def delayed_close(self):
        """Delayed close - won't close immediately, but on next ioloop tick."""
        self.state = CLOSING
        self.server.io_loop.add_callback(self.close)

    def get_close_reason(self):
        """Return last close reason tuple.

        For example:

            if self.session.is_closed:
                code, reason = self.session.get_close_reason()

        """
        if self.close_reason:
            return self.close_reason

        return (3000, 'Go away!')

    @property
    def is_closed(self):
        """Check if session was closed."""
        return self.state == CLOSED or self.state == CLOSING

    def send_message(self, msg, stats=True, binary=False):
        """Send or queue outgoing message

        `msg`
            Message to send
        `stats`
            If set to True, will update statistics after operation completes
        """
        raise NotImplemented()

    def send_jsonified(self, msg, stats=True):
        """Send or queue outgoing message which was json-encoded before. Used by the `broadcast`
        method.

        `msg`
            JSON-encoded message to send
        `stats`
            If set to True, will update statistics after operation completes
        """
        raise NotImplemented()

    def broadcast(self, clients, msg):
        """Optimized `broadcast` implementation. Depending on type of the session, will json-encode
        message once and will call either `send_message` or `send_jsonifed`.

        `clients`
            Clients iterable
        `msg`
            Message to send
        """
        self.server.broadcast(clients, msg)


class Session(BaseSession, sessioncontainer.SessionMixin):
    """SockJS session implementation.
    """

    def __init__(self, conn, server, session_id, expiry=None):
        """Session constructor.

        `conn`
            Default connection class
        `server`
            `SockJSRouter` instance
        `session_id`
            Session id
        `expiry`
            Session expiry time
        """
        # Initialize session
        sessioncontainer.SessionMixin.__init__(self, session_id, expiry)
        BaseSession.__init__(self, conn, server)

        self.send_queue = ''
        self.send_expects_json = True

        # Heartbeat related stuff
        self._heartbeat_timer = None
        self._heartbeat_interval = self.server.settings['heartbeat_delay'] * 1000

        self._immediate_flush = self.server.settings['immediate_flush']
        self._pending_flush = False

        self._verify_ip = self.server.settings['verify_ip']

    # Session callbacks
    def on_delete(self, forced):
        """Session expiration callback

        `forced`
            If session item explicitly deleted, forced will be set to True. If
            item expired, will be set to False.
        """
        # Do not remove connection if it was not forced and there's running connection
        if not forced and self.handler is not None and not self.is_closed:
            self.promote()
        else:
            self.close()

    # Add session
    def set_handler(self, handler, start_heartbeat=True):
        """Set active handler for the session

        `handler`
            Associate active Tornado handler with the session
        `start_heartbeat`
            Should session start heartbeat immediately
        """
        # Check if session already has associated handler
        if self.handler is not None:
            handler.send_pack(proto.disconnect(2010, "Another connection still open"))
            return False

        if self._verify_ip and self.conn_info is not None:
            # If IP address doesn't match - refuse connection
            if handler.request.remote_ip != self.conn_info.ip:
                logging.error('Attempted to attach to session %s (%s) from different IP (%s)' % (
                              self.session_id,
                              self.conn_info.ip,
                              handler.request.remote_ip
                              ))

                handler.send_pack(proto.disconnect(2010, "Attempted to connect to session from different IP"))
                return False

        if self.state == CLOSING or self.state == CLOSED:
            handler.send_pack(proto.disconnect(*self.get_close_reason()))
            return False

        # Associate handler and promote session
        super(Session, self).set_handler(handler)

        self.promote()

        if start_heartbeat:
            self.start_heartbeat()

        return True

    def verify_state(self):
        """Verify if session was not yet opened. If it is, open it and call connections `on_open`"""
        # If we're in CONNECTING state - send 'o' message to the client
        if self.state == CONNECTING:
            self.handler.send_pack(proto.CONNECT)

        # Call parent implementation
        super(Session, self).verify_state()

    def remove_handler(self, handler):
        """Detach active handler from the session

        `handler`
            Handler to remove
        """
        super(Session, self).remove_handler(handler)

        self.promote()
        self.stop_heartbeat()

    def send_message(self, msg, stats=True, binary=False):
        """Send or queue outgoing message

        `msg`
            Message to send
        `stats`
            If set to True, will update statistics after operation completes
        """
        self.send_jsonified(proto.json_encode(bytes_to_str(msg)), stats)

    def send_jsonified(self, msg, stats=True):
        """Send JSON-encoded message

        `msg`
            JSON encoded string to send
        `stats`
            If set to True, will update statistics after operation completes
        """
        msg = bytes_to_str(msg)

        if self._immediate_flush:
            if self.handler and self.handler.active and not self.send_queue:
                # Send message right away
                self.handler.send_pack('a[%s]' % msg)
            else:
                if self.send_queue:
                    self.send_queue += ','
                self.send_queue += msg

                self.flush()
        else:
            if self.send_queue:
                self.send_queue += ','
            self.send_queue += msg

            if not self._pending_flush:
                self.server.io_loop.add_callback(self.flush)
                self._pending_flush = True

        if stats:
            self.stats.on_pack_sent(1)

    def flush(self):
        """Flush message queue if there's an active connection running"""
        self._pending_flush = False

        if self.handler is None or not self.handler.active or not self.send_queue:
            return

        self.handler.send_pack('a[%s]' % self.send_queue)
        self.send_queue = ''

    def close(self, code=3000, message='Go away!'):
        """Close session.

        `code`
            Closing code
        `message`
            Closing message
        """
        if self.state != CLOSED:
            # Notify handler
            if self.handler is not None:
                self.handler.send_pack(proto.disconnect(code, message))

        super(Session, self).close(code, message)

    # Heartbeats
    def start_heartbeat(self):
        """Reset hearbeat timer"""
        self.stop_heartbeat()

        self._heartbeat_timer = periodic.Callback(self._heartbeat,
                                                  self._heartbeat_interval,
                                                  self.server.io_loop)
        self._heartbeat_timer.start()

    def stop_heartbeat(self):
        """Stop active heartbeat"""
        if self._heartbeat_timer is not None:
            self._heartbeat_timer.stop()
            self._heartbeat_timer = None

    def delay_heartbeat(self):
        """Delay active heartbeat"""
        if self._heartbeat_timer is not None:
            self._heartbeat_timer.delay()

    def _heartbeat(self):
        """Heartbeat callback"""
        if self.handler is not None:
            self.handler.send_pack(proto.HEARTBEAT)
        else:
            self.stop_heartbeat()

    def on_messages(self, msg_list):
        """Handle incoming messages

        `msg_list`
            Message list to process
        """
        self.stats.on_pack_recv(len(msg_list))

        for msg in msg_list:
            self.conn.on_message(msg)

########NEW FILE########
__FILENAME__ = sessioncontainer
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.sessioncontainer
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Simple heapq-based session implementation with sliding expiration window
    support.
"""

from heapq import heappush, heappop
from time import time
from hashlib import md5
from random import random


def _random_key():
    """Return random session key"""
    i = md5()
    i.update('%s%s' % (random(), time()))
    return i.hexdigest()


class SessionMixin(object):
    """Represents one session object stored in the session container.
    Derive from this object to store additional data.
    """

    def __init__(self, session_id=None, expiry=None):
        """Constructor.

        ``session_id``
            Optional session id. If not provided, will generate
            new session id.
        ``expiry``
            Expiration time. If not provided, will never expire.
        """
        self.session_id = session_id or _random_key()
        self.promoted = None
        self.expiry = expiry

        if self.expiry is not None:
            self.expiry_date = time() + self.expiry

    def is_alive(self):
        """Check if session is still alive"""
        return self.expiry_date > time()

    def promote(self):
        """Mark object as alive, so it won't be collected during next
        run of the garbage collector.
        """
        if self.expiry is not None:
            self.promoted = time() + self.expiry

    def on_delete(self, forced):
        """Triggered when object was expired or deleted."""
        pass

    def __lt__(self, other):
        return self.expiry_date < other.expiry_date
    
    __cmp__ =  __lt__

    def __repr__(self):
        return '%f %s %d' % (getattr(self, 'expiry_date', -1),
                             self.session_id,
                             self.promoted or 0)


class SessionContainer(object):
    """Session container object.

    If we will implement sessions with Tornado timeouts, for polling transports
    it will be nightmare - if load will be high, number of discarded timeouts
    will be huge and will be huge performance hit, as Tornado will have to
    clean them up all the time.
    """
    def __init__(self):
        self._items = dict()
        self._queue = []

    def add(self, session):
        """Add session to the container.

        `session`
            Session object
        """
        self._items[session.session_id] = session

        if session.expiry is not None:
            heappush(self._queue, session)

    def get(self, session_id):
        """Return session object or None if it is not available

        `session_id`
            Session identifier
        """
        return self._items.get(session_id, None)

    def remove(self, session_id):
        """Remove session object from the container

        `session_id`
            Session identifier
        """
        session = self._items.get(session_id, None)

        if session is not None:
            session.promoted = -1
            session.on_delete(True)
            del self._items[session_id]
            return True

        return False

    def expire(self, current_time=None):
        """Expire any old entries

        `current_time`
            Optional time to be used to clean up queue (can be used in unit tests)
        """
        if not self._queue:
            return

        if current_time is None:
            current_time = time()

        while self._queue:
            # Get top most item
            top = self._queue[0]

            # Early exit if item was not promoted and its expiration time
            # is greater than now.
            if top.promoted is None and top.expiry_date > current_time:
                break

            # Pop item from the stack
            top = heappop(self._queue)

            need_reschedule = (top.promoted is not None
                               and top.promoted > current_time)

            # Give chance to reschedule
            if not need_reschedule:
                top.promoted = None
                top.on_delete(False)

                need_reschedule = (top.promoted is not None
                                   and top.promoted > current_time)

            # If item is promoted and expiration time somewhere in future
            # just reschedule it
            if need_reschedule:
                top.expiry_date = top.promoted
                top.promoted = None
                heappush(self._queue, top)
            else:
                del self._items[top.session_id]

########NEW FILE########
__FILENAME__ = static
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

# -*- coding: utf-8 -*-
"""
    sockjs.tornado.static
    ~~~~~~~~~~~~~~~~~~~~~

    Various static handlers required for SockJS to function properly.
"""

import time
import hashlib
import random

from tornado.web import asynchronous

from .basehandler import BaseHandler, PreflightHandler
from .proto import json_encode
from .util import MAXSIZE, str_to_bytes

IFRAME_TEXT = '''<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <script>
    document.domain = document.domain;
    _sockjs_onload = function(){SockJS.bootstrap_iframe();};
  </script>
  <script src="%s"></script>
</head>
<body>
  <h2>Don't panic!</h2>
  <p>This is a SockJS hidden iframe. It's used for cross domain magic.</p>
</body>
</html>'''.strip()


class IFrameHandler(BaseHandler):
    """SockJS IFrame page handler"""
    def get(self):
        data = str_to_bytes(IFRAME_TEXT % self.server.settings['sockjs_url'])

        hsh = hashlib.md5(data).hexdigest()

        value = self.request.headers.get('If-None-Match')
        if value:
            if value.find(hsh) != -1:
                # TODO: Fix me? Right now it is a hack to remove content-type
                # header
                self.clear()
                del self._headers['Content-Type']

                self.set_status(304)
                return

        self.enable_cache()

        self.set_header('Etag', hsh)
        self.write(data)


class GreetingsHandler(BaseHandler):
    """SockJS greetings page handler"""

    def initialize(self, server):
        self.server = server

    def get(self):
        self.enable_cache()

        self.set_header('Content-Type', 'text/plain; charset=UTF-8')
        self.write('Welcome to SockJS!\n')


class ChunkingTestHandler(PreflightHandler):
    """SockJS chunking test handler"""

    # Step timeouts according to sockjs documentation
    steps = [0.005, 0.025, 0.125, 0.625, 3.125]

    def initialize(self, server):
        self.server = server
        self.step = 0
        self.io_loop = server.io_loop

    @asynchronous
    def post(self):
        self.preflight()
        self.set_header('Content-Type', 'application/javascript; charset=UTF-8')

        # Send one 'h' immediately
        self.write('h\n')
        self.flush()

        # Send 2048 spaces followed by 'h'
        self.write(' ' * 2048 + 'h\n')
        self.flush()

        # Send 'h' with different timeouts
        def run_step():
            try:
                self.write('h\n')
                self.flush()

                self.step += 1
                if self.step < len(self.steps):
                    self.io_loop.add_timeout(time.time() + self.steps[self.step],
                                             run_step)
                else:
                    self.finish()
            except IOError:
                pass

        self.io_loop.add_timeout(time.time() + self.steps[0], run_step)


class InfoHandler(PreflightHandler):
    """SockJS 0.2+ /info handler"""
    def initialize(self, server):
        self.server = server
        self.access_methods = 'OPTIONS, GET'

    def get(self):
        self.preflight()
        self.disable_cache()
        self.set_header('Content-Type', 'application/json; charset=UTF-8')

        options = dict(websocket=self.server.websockets_enabled,
                       cookie_needed=self.server.cookie_needed,
                       origins=['*:*'],
                       entropy=random.randint(0, MAXSIZE))

        self.write(json_encode(options))

########NEW FILE########
__FILENAME__ = stats
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import deque

import gaffer.tornado_pyuv as ioloop


class MovingAverage(object):
    """Moving average class implementation"""
    def __init__(self, period=10):
        """Constructor.

        `period`
            Moving window size. Average will be calculated
            from the data in the window.
        """
        self.period = period
        self.stream = deque()
        self.sum = 0
        self.accumulator = 0
        self.last_average = 0

    def add(self, n):
        """Add value to the current accumulator

        `n`
            Value to add
        """
        self.accumulator += n

    def flush(self):
        """Add accumulator to the moving average queue
        and reset it. For example, called by the StatsCollector
        once per second to calculate per-second average.
        """
        n = self.accumulator
        self.accumulator = 0

        stream = self.stream
        stream.append(n)
        self.sum += n

        streamlen = len(stream)

        if streamlen > self.period:
            self.sum -= stream.popleft()
            streamlen -= 1

        if streamlen == 0:
            self.last_average = 0
        else:
            self.last_average = self.sum / float(streamlen)


class StatsCollector(object):
    def __init__(self, io_loop):
        # Sessions
        self.sess_active = 0

        # Avoid circular reference
        self.sess_transports = dict()

        # Connections
        self.conn_active = 0
        self.conn_ps = MovingAverage()

        # Packets
        self.pack_sent_ps = MovingAverage()
        self.pack_recv_ps = MovingAverage()

        self._callback = ioloop.PeriodicCallback(self._update,
                                                 1000,
                                                 io_loop)
        self._callback.start()
        self._callback._timer.unref()

    def _update(self):
        self.conn_ps.flush()

        self.pack_sent_ps.flush()
        self.pack_recv_ps.flush()

    def dump(self):
        """Return dictionary with current statistical information"""
        data = dict(
            # Sessions
            sessions_active=self.sess_active,

            # Connections
            connections_active=self.conn_active,
            connections_ps=self.conn_ps.last_average,

            # Packets
            packets_sent_ps=self.pack_sent_ps.last_average,
            packets_recv_ps=self.pack_recv_ps.last_average
            )

        for k, v in self.sess_transports.iteritems():
            data['transp_' + k] = v

        return data

    # Various event callbacks
    def on_sess_opened(self, transport):
        self.sess_active += 1

        if transport not in self.sess_transports:
            self.sess_transports[transport] = 0

        self.sess_transports[transport] += 1

    def on_sess_closed(self, transport):
        self.sess_active -= 1
        self.sess_transports[transport] -= 1

    def on_conn_opened(self):
        self.conn_active += 1
        self.conn_ps.add(1)

    def on_conn_closed(self):
        self.conn_active -= 1

    def on_pack_sent(self, num):
        self.pack_sent_ps.add(num)

    def on_pack_recv(self, num):
        self.pack_recv_ps.add(num)

########NEW FILE########
__FILENAME__ = base
from .. import session


class BaseTransportMixin(object):
    """Base transport.

    Implements few methods that session expects to see in each transport.
    """

    name = 'override_me_please'

    def get_conn_info(self):
        """Return `ConnectionInfo` object from current transport"""
        return session.ConnectionInfo(self.request.remote_ip,
                                      self.request.cookies,
                                      self.request.arguments,
                                      self.request.headers,
                                      self.request.path)

    def session_closed(self):
        """Called by the session, when it gets closed"""
        pass

########NEW FILE########
__FILENAME__ = eventsource
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.eventsource
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    EventSource transport implementation.
"""

from tornado.web import asynchronous

from . import streamingbase


class EventSourceTransport(streamingbase.StreamingTransportBase):
    name = 'eventsource'

    @asynchronous
    def get(self, session_id):
        # Start response
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()

        self.set_header('Content-Type', 'text/event-stream; charset=UTF-8')
        self.write('\r\n')
        self.flush()

        if not self._attach_session(session_id):
            self.finish()
            return

        if self.session:
            self.session.flush()

    def send_pack(self, message, binary=False):
        if binary:
            raise Exception('binary not supported for EventSourceTransport')

        msg = 'data: %s\r\n\r\n' % message

        self.active = False

        try:
            self.notify_sent(len(msg))

            self.write(msg)
            self.flush(callback=self.send_complete)
        except IOError:
            # If connection dropped, make sure we close offending session instead
            # of propagating error all way up.
            self.session.delayed_close()
            self._detach()

########NEW FILE########
__FILENAME__ = htmlfile
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.htmlfile
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    HtmlFile transport implementation.
"""

from tornado.web import asynchronous

from .. import proto
from . import streamingbase

# HTMLFILE template
HTMLFILE_HEAD = r'''
<!doctype html>
<html><head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
</head><body><h2>Don't panic!</h2>
  <script>
    document.domain = document.domain;
    var c = parent.%s;
    c.start();
    function p(d) {c.message(d);};
    window.onload = function() {c.stop();};
  </script>
'''.strip()
HTMLFILE_HEAD += ' ' * (1024 - len(HTMLFILE_HEAD) + 14)
HTMLFILE_HEAD += '\r\n\r\n'


class HtmlFileTransport(streamingbase.StreamingTransportBase):
    name = 'htmlfile'

    def initialize(self, server):
        super(HtmlFileTransport, self).initialize(server)

    @asynchronous
    def get(self, session_id):
        # Start response
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()
        self.set_header('Content-Type', 'text/html; charset=UTF-8')

        # Grab callback parameter
        callback = self.get_argument('c', None)
        if not callback:
            self.write('"callback" parameter required')
            self.set_status(500)
            self.finish()
            return

        # TODO: Fix me - use parameter
        self.write(HTMLFILE_HEAD % callback)
        self.flush()

        # Now try to attach to session
        if not self._attach_session(session_id):
            self.finish()
            return

        # Flush any pending messages
        if self.session:
            self.session.flush()

    def send_pack(self, message, binary=False):
        if binary:
            raise Exception('binary not supported for HtmlFileTransport')

        # TODO: Just do escaping
        msg = '<script>\np(%s);\n</script>\r\n' % proto.json_encode(message)

        self.active = False

        try:
            self.notify_sent(len(message))

            self.write(msg)
            self.flush(callback=self.send_complete)
        except IOError:
            # If connection dropped, make sure we close offending session instead
            # of propagating error all way up.
            self.session.delayed_close()
            self._detach()

########NEW FILE########
__FILENAME__ = jsonp
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.jsonp
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    JSONP transport implementation.
"""
import logging

from tornado.web import asynchronous

from .. import proto
from . import pollingbase
from ..util import bytes_to_str, unquote_plus

class JSONPTransport(pollingbase.PollingTransportBase):
    name = 'jsonp'

    @asynchronous
    def get(self, session_id):
        # Start response
        self.handle_session_cookie()
        self.disable_cache()

        # Grab callback parameter
        self.callback = self.get_argument('c', None)
        if not self.callback:
            self.write('"callback" parameter required')
            self.set_status(500)
            self.finish()
            return

        # Get or create session without starting heartbeat
        if not self._attach_session(session_id, False):
            return

        # Might get already detached because connection was closed in on_open
        if not self.session:
            return

        if not self.session.send_queue:
            self.session.start_heartbeat()
        else:
            self.session.flush()

    def send_pack(self, message, binary=False):
        if binary:
            raise Exception('binary not supported for JSONPTransport')

        self.active = False

        try:
            # TODO: Just escape
            msg = '%s(%s);\r\n' % (self.callback, proto.json_encode(message))

            self.set_header('Content-Type', 'application/javascript; charset=UTF-8')
            self.set_header('Content-Length', len(msg))

            # TODO: Fix me
            self.set_header('Etag', 'dummy')

            self.write(msg)
            self.flush(callback=self.send_complete)
        except IOError:
            # If connection dropped, make sure we close offending session instead
            # of propagating error all way up.
            self.session.delayed_close()


class JSONPSendHandler(pollingbase.PollingTransportBase):
    def post(self, session_id):
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()

        session = self._get_session(session_id)

        if session is None:
            self.set_status(404)
            return

        #data = self.request.body.decode('utf-8')
        data = bytes_to_str(self.request.body)

        ctype = self.request.headers.get('Content-Type', '').lower()
        if ctype == 'application/x-www-form-urlencoded':
            if not data.startswith('d='):
                logging.exception('jsonp_send: Invalid payload.')

                self.write("Payload expected.")
                self.set_status(500)
                return

            data = unquote_plus(data[2:])

        if not data:
            logging.debug('jsonp_send: Payload expected.')

            self.write("Payload expected.")
            self.set_status(500)
            return

        try:
            messages = proto.json_decode(data)
        except:
            # TODO: Proper error handling
            logging.debug('jsonp_send: Invalid json encoding')

            self.write("Broken JSON encoding.")
            self.set_status(500)
            return

        try:
            session.on_messages(messages)
        except Exception:
            logging.exception('jsonp_send: on_message() failed')

            session.close()

            self.write('Message handler failed.')
            self.set_status(500)
            return

        self.write('ok')
        self.set_header('Content-Type', 'text/plain; charset=UTF-8')
        self.set_status(200)

########NEW FILE########
__FILENAME__ = pollingbase
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.pollingbase
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Polling transports base
"""

from .. import basehandler
from . import base


class PollingTransportBase(basehandler.PreflightHandler, base.BaseTransportMixin):
    """Polling transport handler base class"""
    def initialize(self, server):
        super(PollingTransportBase, self).initialize(server)

        self.session = None
        self.active = True

    def _get_session(self, session_id):
        return self.server.get_session(session_id)

    def _attach_session(self, session_id, start_heartbeat=False):
        session = self._get_session(session_id)

        if session is None:
            session = self.server.create_session(session_id)

        # Try to attach to the session
        if not session.set_handler(self, start_heartbeat):
            return False

        self.session = session

        # Verify if session is properly opened
        session.verify_state()

        return True

    def _detach(self):
        """Detach from the session"""
        if self.session:
            self.session.remove_handler(self)
            self.session = None

    def check_xsrf_cookie(self):
        pass

    def send_message(self, message, binary=False):
        """Called by the session when some data is available"""
        raise NotImplementedError()

    def session_closed(self):
        """Called by the session when it was closed"""
        self._detach()
        self.safe_finish()

    def on_connection_close(self):
        # If connection was dropped by the client, close session.
        # In all other cases, connection will be closed by the server.
        if self.session is not None:
            self.session.close(1002, 'Connection interrupted')

        super(PollingTransportBase, self).on_connection_close()

    def send_complete(self):
        self._detach()
        self.safe_finish()

########NEW FILE########
__FILENAME__ = rawwebsocket
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.rawwebsocket
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Raw websocket transport implementation
"""
import logging
import socket

from .. import websocket, session
from . import base


class RawSession(session.BaseSession):
    """Raw session without any sockjs protocol encoding/decoding. Simply
    works as a proxy between `SockJSConnection` class and `RawWebSocketTransport`."""
    def send_message(self, msg, stats=True, binary=False):
        self.handler.send_pack(msg, binary)

    def on_message(self, msg):
        self.conn.on_message(msg)


class RawWebSocketTransport(websocket.WebSocketHandler, base.BaseTransportMixin):
    """Raw Websocket transport"""
    name = 'rawwebsocket'

    def initialize(self, server):
        self.server = server
        self.session = None
        self.active = True

    def open(self):
        # Stats
        self.server.stats.on_conn_opened()

        # Disable nagle if needed
        if self.server.settings['disable_nagle']:
            self.stream.socket.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)

        # Create and attach to session
        self.session = RawSession(self.server.get_connection_class(), self.server)
        self.session.set_handler(self)
        self.session.verify_state()

    def _detach(self):
        if self.session is not None:
            self.session.remove_handler(self)
            self.session = None

    def on_message(self, message):
        # SockJS requires that empty messages should be ignored
        if not message or not self.session:
            return

        try:
            self.session.on_message(message)
        except Exception:
            logging.exception('RawWebSocket')

            # Close running connection
            self._detach()
            self.abort_connection()

    def on_close(self):
        # Close session if websocket connection was closed
        if self.session is not None:
            # Stats
            self.server.stats.on_conn_closed()

            session = self.session
            self._detach()
            session.close()

    def send_pack(self, message, binary=False):
        # Send message
        try:
            self.write_message(message, binary)
        except IOError:
            self.server.io_loop.add_callback(self.on_close)

    def session_closed(self):
        try:
            self.close()
        except IOError:
            pass
        finally:
            self._detach()

    # Websocket overrides
    def allow_draft76(self):
        return True

########NEW FILE########
__FILENAME__ = streamingbase
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from . import pollingbase


class StreamingTransportBase(pollingbase.PollingTransportBase):
    def initialize(self, server):
        super(StreamingTransportBase, self).initialize(server)

        self.amount_limit = self.server.settings['response_limit']

        # HTTP 1.0 client might send keep-alive
        if hasattr(self.request, 'connection') and not self.request.supports_http_1_1():
            self.request.connection.no_keep_alive = True

    def notify_sent(self, data_len):
        """
            Update amount of data sent
        """
        self.amount_limit -= data_len

    def should_finish(self):
        """
            Check if transport should close long running connection after
            sending X bytes to the client.

            `data_len`
                Amount of data that was sent
        """
        if self.amount_limit <= 0:
            return True

        return False

    def send_complete(self):
        """
            Verify if connection should be closed based on amount of data that was sent.
        """
        self.active = True

        if self.should_finish():
            self._detach()
            self.safe_finish()
        else:
            if self.session:
                self.session.flush()

########NEW FILE########
__FILENAME__ = websocket
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.websocket
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Websocket transport implementation
"""
import logging
import socket

from .. import proto, websocket
from . import base
from ..util import bytes_to_str

class WebSocketTransport(websocket.WebSocketHandler, base.BaseTransportMixin):
    """Websocket transport"""
    name = 'websocket'

    def initialize(self, server):
        self.server = server
        self.session = None
        self.active = True

    def open(self, session_id):
        # Stats
        self.server.stats.on_conn_opened()

        # Disable nagle
        if self.server.settings['disable_nagle']:
            self.stream.socket.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)

        # Handle session
        self.session = self.server.create_session(session_id, register=False)

        if not self.session.set_handler(self):
            self.close()
            return

        self.session.verify_state()

        if self.session:
            self.session.flush()

    def _detach(self):
        if self.session is not None:
            self.session.remove_handler(self)
            self.session = None

    def on_message(self, message):
        # SockJS requires that empty messages should be ignored
        if not message or not self.session:
            return

        try:
            msg = proto.json_decode(bytes_to_str(message))

            if isinstance(msg, list):
                self.session.on_messages(msg)
            else:
                self.session.on_messages((msg,))
        except Exception:
            logging.exception('WebSocket')

            # Close session on exception
            #self.session.close()

            # Close running connection
            self.abort_connection()

    def on_close(self):
        # Close session if websocket connection was closed
        if self.session is not None:
            # Stats
            self.server.stats.on_conn_closed()

            # Detach before closing session
            session = self.session
            self._detach()
            session.close()

    def send_pack(self, message, binary=False):
        # Send message
        try:
            self.write_message(message, binary)
        except IOError:
            self.server.io_loop.add_callback(self.on_close)

    def session_closed(self):
        # If session was closed by the application, terminate websocket
        # connection as well.
        try:
            self.close()
        except IOError:
            pass
        finally:
            self._detach()

    # Websocket overrides
    def allow_draft76(self):
        return True

    def auto_decode(self):
        return False

########NEW FILE########
__FILENAME__ = xhr
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    sockjs.tornado.transports.xhr
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Xhr-Polling transport implementation
"""
import logging

from tornado.web import asynchronous

from .. import proto
from . import pollingbase
from ..util import bytes_to_str

class XhrPollingTransport(pollingbase.PollingTransportBase):
    """xhr-polling transport implementation"""
    name = 'xhr'

    @asynchronous
    def post(self, session_id):
        # Start response
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()

        # Get or create session without starting heartbeat
        if not self._attach_session(session_id, False):
            return

        # Might get already detached because connection was closed in on_open
        if not self.session:
            return

        if not self.session.send_queue:
            self.session.start_heartbeat()
        else:
            self.session.flush()

    def send_pack(self, message, binary=False):
        if binary:
            raise Exception('binary not supported for XhrPollingTransport')

        self.active = False

        try:
            self.set_header('Content-Type', 'application/javascript; charset=UTF-8')
            self.set_header('Content-Length', len(message) + 1)
            self.write(message + '\n')
            self.flush(callback=self.send_complete)
        except IOError:
            # If connection dropped, make sure we close offending session instead
            # of propagating error all way up.
            self.session.delayed_close()


class XhrSendHandler(pollingbase.PollingTransportBase):
    def post(self, session_id):
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()

        session = self._get_session(session_id)

        if session is None:
            self.set_status(404)
            return

        #data = self.request.body.decode('utf-8')
        data = self.request.body
        if not data:
            self.write("Payload expected.")
            self.set_status(500)
            return

        try:
            messages = proto.json_decode(bytes_to_str(data))
        except:
            # TODO: Proper error handling
            self.write("Broken JSON encoding.")
            self.set_status(500)
            return

        try:
            session.on_messages(messages)
        except Exception:
            logging.exception('XHR incoming')
            session.close()

            self.set_status(500)
            return

        self.set_status(204)
        self.set_header('Content-Type', 'text/plain; charset=UTF-8')

########NEW FILE########
__FILENAME__ = xhrstreaming
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
    Xhr-Streaming transport implementation
"""

from tornado.web import asynchronous

from . import streamingbase


class XhrStreamingTransport(streamingbase.StreamingTransportBase):
    name = 'xhr_streaming'

    @asynchronous
    def post(self, session_id):
        # Handle cookie
        self.preflight()
        self.handle_session_cookie()
        self.disable_cache()
        self.set_header('Content-Type', 'application/javascript; charset=UTF-8')

        # Send prelude and flush any pending messages
        self.write('h' * 2048 + '\n')
        self.flush()

        if not self._attach_session(session_id, False):
            self.finish()
            return

        if self.session:
            self.session.flush()

    def send_pack(self, message, binary=False):
        if binary:
            raise Exception('binary not supported for XhrStreamingTransport')

        self.active = False

        try:
            self.notify_sent(len(message))

            self.write(message + '\n')
            self.flush(callback=self.send_complete)
        except IOError:
            # If connection dropped, make sure we close offending session instead
            # of propagating error all way up.
            self.session.delayed_close()
            self._detach()

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import sys

PY3 = sys.version_info[0] == 3

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes
    
    MAXSIZE = sys.maxsize

    def bytes_to_str(b):
        if isinstance(b, text_type):
            return b
        return str(b, 'utf8')

    def str_to_bytes(s):
        if isinstance(s, bytes):
            return s
        return s.encode('utf8')

    import urllib.parse
    unquote_plus = urllib.parse.unquote_plus
else:
    import types
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform == "java":
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):
            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
            del X

    def bytes_to_str(s):
        if isinstance(s, unicode):
            return s.encode('utf-8')
        return s

    def str_to_bytes(s):
        if isinstance(s, unicode):
            return s.encode('utf8')
        return s

    import urllib
    unquote_plus = urllib.unquote_plus

########NEW FILE########
__FILENAME__ = websocket
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

"""
sockjs.tornado.websocket
    ~~~~~~~~~~~~~~~~~~~~~~~~

    This module contains modified version of the WebSocketHandler from
    Tornado with some performance fixes and behavior changes for Hybi10
    protocol.

    Server-side implementation of the WebSocket protocol.

    `WebSockets <http://dev.w3.org/html5/websockets/>`_ allow for bidirectional
    communication between the browser and server.

    .. warning::

       The WebSocket protocol was recently finalized as `RFC 6455
       <http://tools.ietf.org/html/rfc6455>`_ and is not yet supported in
       all browsers.  Refer to http://caniuse.com/websockets for details
       on compatibility.  In addition, during development the protocol
       went through several incompatible versions, and some browsers only
       support older versions.  By default this module only supports the
       latest version of the protocol, but optional support for an older
       version (known as "draft 76" or "hixie-76") can be enabled by
       overriding `WebSocketHandler.allow_draft76` (see that method's
       documentation for caveats).
"""
# Author: Jacob Kristhammar, 2010

import array
import functools
import hashlib
import logging
import struct
import time
import base64
import tornado.escape
import tornado.web

from tornado import stack_context
from tornado.util import bytes_type, b

# Support for 2.5
try:
    make_array = bytearray
    to_string = bytes_type
except NameError:
    make_array = lambda d: array.array("B", d)
    to_string = lambda d: d.tostring()


class WebSocketHandler(tornado.web.RequestHandler):
    """Subclass this class to create a basic WebSocket handler.

    Override on_message to handle incoming messages. You can also override
    open and on_close to handle opened and closed connections.

    See http://dev.w3.org/html5/websockets/ for details on the
    JavaScript interface.  The protocol is specified at
    http://tools.ietf.org/html/rfc6455.

    Here is an example Web Socket handler that echos back all received messages
    back to the client::

      class EchoWebSocket(websocket.WebSocketHandler):
          def open(self):
              print "WebSocket opened"

          def on_message(self, message):
              self.write_message(u"You said: " + message)

          def on_close(self):
              print "WebSocket closed"

    Web Sockets are not standard HTTP connections. The "handshake" is HTTP,
    but after the handshake, the protocol is message-based. Consequently,
    most of the Tornado HTTP facilities are not available in handlers of this
    type. The only communication methods available to you are write_message()
    and close(). Likewise, your request handler class should
    implement open() method rather than get() or post().

    If you map the handler above to "/websocket" in your application, you can
    invoke it in JavaScript with::

      var ws = new WebSocket("ws://localhost:8888/websocket");
      ws.onopen = function() {
         ws.send("Hello, world");
      };
      ws.onmessage = function (evt) {
         alert(evt.data);
      };

    This script pops up an alert box that says "You said: Hello, world".
    """
    def __init__(self, application, request, **kwargs):
        tornado.web.RequestHandler.__init__(self, application, request,
                                            **kwargs)
        self.stream = request.connection.stream
        self.ws_connection = None

    def _execute(self, transforms, *args, **kwargs):
        self.open_args = args
        self.open_kwargs = kwargs

        # Websocket only supports GET method
        if self.request.method != "GET":
            self.stream.write(tornado.escape.utf8(
                "HTTP/1.1 405 Method Not Allowed\r\n"
                "Allow: GET\r\n"
                "Connection: Close\r\n"
                "\r\n"
            ))
            self.stream.close()
            return

        # Upgrade header should be present and should be equal to WebSocket
        if self.request.headers.get("Upgrade", "").lower() != "websocket":
            self.stream.write(tornado.escape.utf8(
                "HTTP/1.1 400 Bad Request\r\n"
                "Connection: Close\r\n"
                "\r\n"
                "Can \"Upgrade\" only to \"WebSocket\"."
            ))
            self.stream.close()
            return

        # Connection header should be upgrade. Some proxy servers/load balancers
        # might mess with it.
        headers = self.request.headers
        connection = map(lambda s: s.strip().lower(), headers.get("Connection", "").split(","))
        if "upgrade" not in connection:
            self.stream.write(tornado.escape.utf8(
                "HTTP/1.1 400 Bad Request\r\n"
                "Connection: Close\r\n"
                "\r\n"
                "\"Connection\" must be \"Upgrade\"."
            ))
            self.stream.close()
            return

        # The difference between version 8 and 13 is that in 8 the
        # client sends a "Sec-Websocket-Origin" header and in 13 it's
        # simply "Origin".
        if self.request.headers.get("Sec-WebSocket-Version") in ("7", "8", "13"):
            self.ws_connection = WebSocketProtocol13(self, self.auto_decode())
            self.ws_connection.accept_connection()
        elif (self.allow_draft76() and
              "Sec-WebSocket-Version" not in self.request.headers):
            self.ws_connection = WebSocketProtocol76(self, self.auto_decode())
            self.ws_connection.accept_connection()
        else:
            self.stream.write(tornado.escape.utf8(
                "HTTP/1.1 426 Upgrade Required\r\n"
                "Sec-WebSocket-Version: 8\r\n\r\n"))
            self.stream.close()

    def abort_connection(self):
        self.ws_connection._abort()

    def write_message(self, message, binary=False):
        """Sends the given message to the client of this Web Socket.

        The message may be either a string or a dict (which will be
        encoded as json).  If the ``binary`` argument is false, the
        message will be sent as utf8; in binary mode any byte string
        is allowed.
        """
        if isinstance(message, dict):
            message = tornado.escape.json_encode(message)
        self.ws_connection.write_message(message, binary=binary)

    def select_subprotocol(self, subprotocols):
        """Invoked when a new WebSocket requests specific subprotocols.

        ``subprotocols`` is a list of strings identifying the
        subprotocols proposed by the client.  This method may be
        overridden to return one of those strings to select it, or
        ``None`` to not select a subprotocol.  Failure to select a
        subprotocol does not automatically abort the connection,
        although clients may close the connection if none of their
        proposed subprotocols was selected.
        """
        return None

    def open(self):
        """Invoked when a new WebSocket is opened.

        The arguments to `open` are extracted from the `tornado.web.URLSpec`
        regular expression, just like the arguments to
        `tornado.web.RequestHandler.get`.
        """
        pass

    def on_message(self, message):
        """Handle incoming messages on the WebSocket

        This method must be overridden.
        """
        raise NotImplementedError

    def on_close(self):
        """Invoked when the WebSocket is closed."""
        pass

    def close(self):
        """Closes this Web Socket.

        Once the close handshake is successful the socket will be closed.
        """
        self.ws_connection.close()

    def allow_draft76(self):
        """Override to enable support for the older "draft76" protocol.

        The draft76 version of the websocket protocol is disabled by
        default due to security concerns, but it can be enabled by
        overriding this method to return True.

        Connections using the draft76 protocol do not support the
        ``binary=True`` flag to `write_message`.

        Support for the draft76 protocol is deprecated and will be
        removed in a future version of Tornado.
        """
        return False

    def auto_decode(self):
        """Override to disable automatic utf-8 decoding of the string messages.

        If you will return False, your on_message handler will receive utf-8
        encoded strings.

        Useful for performance reasons - if your protocol is json based, most
        of the python json decoders work with utf-8 encoded strings and if they
        receive utf-16 as input, then will convert it back to utf-8 and then parse.
        """
        return True

    def get_websocket_scheme(self):
        """Return the url scheme used for this request, either "ws" or "wss".

        This is normally decided by HTTPServer, but applications
        may wish to override this if they are using an SSL proxy
        that does not provide the X-Scheme header as understood
        by HTTPServer.

        Note that this is only used by the draft76 protocol.
        """
        return "wss" if self.request.protocol == "https" else "ws"

    def async_callback(self, callback, *args, **kwargs):
        """Wrap callbacks with this if they are used on asynchronous requests.

        Catches exceptions properly and closes this WebSocket if an exception
        is uncaught.  (Note that this is usually unnecessary thanks to
        `tornado.stack_context`)
        """
        return self.ws_connection.async_callback(callback, *args, **kwargs)

    def _not_supported(self, *args, **kwargs):
        raise Exception("Method not supported for Web Sockets")

    def on_connection_close(self):
        if self.ws_connection:
            self.ws_connection.on_connection_close()
            self.ws_connection = None
            self.on_close()


for method in ["write", "redirect", "set_header", "send_error", "set_cookie",
               "set_status", "flush", "finish"]:
    setattr(WebSocketHandler, method, WebSocketHandler._not_supported)


class WebSocketProtocol(object):
    """Base class for WebSocket protocol versions.
    """
    def __init__(self, handler, auto_decode):
        self.handler = handler
        self.request = handler.request
        self.stream = handler.stream
        self.client_terminated = False
        self.server_terminated = False

        self._auto_decode = auto_decode

    def async_callback(self, callback, *args, **kwargs):
        """Wrap callbacks with this if they are used on asynchronous requests.

        Catches exceptions properly and closes this WebSocket if an exception
        is uncaught.
        """
        if args or kwargs:
            callback = functools.partial(callback, *args, **kwargs)

        def wrapper(*args, **kwargs):
            try:
                return callback(*args, **kwargs)
            except Exception:
                logging.error("Uncaught exception in %s",
                              self.request.path, exc_info=True)
                self._abort()

        return wrapper

    def on_connection_close(self):
        self._abort()

    def _abort(self):
        """Instantly aborts the WebSocket connection by closing the socket"""
        self.client_terminated = True
        self.server_terminated = True
        self.stream.close()  # forcibly tear down the connection
        self.close()  # let the subclass cleanup


class WebSocketProtocol76(WebSocketProtocol):
    """Implementation of the WebSockets protocol, version hixie-76.

    This class provides basic functionality to process WebSockets requests as
    specified in
    http://tools.ietf.org/html/draft-hixie-thewebsocketprotocol-76
    """
    def __init__(self, handler, auto_decode):
        WebSocketProtocol.__init__(self, handler, auto_decode)
        self.challenge = None
        self._waiting = None

    def accept_connection(self):
        try:
            self._handle_websocket_headers()
        except ValueError:
            logging.debug("Malformed WebSocket request received")
            self._abort()
            return

        scheme = self.handler.get_websocket_scheme()

        # draft76 only allows a single subprotocol
        subprotocol_header = ''
        subprotocol = self.request.headers.get("Sec-WebSocket-Protocol", None)
        if subprotocol:
            selected = self.handler.select_subprotocol([subprotocol])
            if selected:
                assert selected == subprotocol
                subprotocol_header = "Sec-WebSocket-Protocol: %s\r\n" % selected

        # Write the initial headers before attempting to read the challenge.
        # This is necessary when using proxies (such as HAProxy), which
        # need to see the Upgrade headers before passing through the
        # non-HTTP traffic that follows.
        self.stream.write(tornado.escape.utf8(
            "HTTP/1.1 101 WebSocket Protocol Handshake\r\n"
            "Upgrade: WebSocket\r\n"
            "Connection: Upgrade\r\n"
            "Server: TornadoServer/%(version)s\r\n"
            "Sec-WebSocket-Origin: %(origin)s\r\n"
            "Sec-WebSocket-Location: %(scheme)s://%(host)s%(uri)s\r\n"
            "%(subprotocol)s"
            "\r\n" % (dict(
                    version=tornado.version,
                    origin=self.request.headers["Origin"],
                    scheme=scheme,
                    host=self.request.host,
                    uri=self.request.uri,
                    subprotocol=subprotocol_header))))
        self.stream.read_bytes(8, self._handle_challenge)

    def challenge_response(self, challenge):
        """Generates the challenge response that's needed in the handshake

        The challenge parameter should be the raw bytes as sent from the
        client.
        """
        key_1 = self.request.headers.get("Sec-Websocket-Key1")
        key_2 = self.request.headers.get("Sec-Websocket-Key2")
        try:
            part_1 = self._calculate_part(key_1)
            part_2 = self._calculate_part(key_2)
        except ValueError:
            raise ValueError("Invalid Keys/Challenge")
        return self._generate_challenge_response(part_1, part_2, challenge)

    def _handle_challenge(self, challenge):
        try:
            challenge_response = self.challenge_response(challenge)
        except ValueError:
            logging.debug("Malformed key data in WebSocket request")
            self._abort()
            return
        self._write_response(challenge_response)

    def _write_response(self, challenge):
        self.stream.write(challenge)
        self.async_callback(self.handler.open)(*self.handler.open_args, **self.handler.open_kwargs)
        self._receive_message()

    def _handle_websocket_headers(self):
        """Verifies all invariant- and required headers

        If a header is missing or have an incorrect value ValueError will be
        raised
        """
        fields = ("Origin", "Host", "Sec-Websocket-Key1",
                  "Sec-Websocket-Key2")
        if not all(map(lambda f: self.request.headers.get(f), fields)):
            raise ValueError("Missing/Invalid WebSocket headers")

    def _calculate_part(self, key):
        """Processes the key headers and calculates their key value.

        Raises ValueError when feed invalid key."""
        number = int(''.join(c for c in key if c.isdigit()))
        spaces = len([s for s in key if s.isspace()])
        try:
            key_number = number // spaces
        except (ValueError, ZeroDivisionError):
            raise ValueError
        return struct.pack(">I", key_number)

    def _generate_challenge_response(self, part_1, part_2, part_3):
        m = hashlib.md5()
        m.update(part_1)
        m.update(part_2)
        m.update(part_3)
        return m.digest()

    def _receive_message(self):
        self.stream.read_bytes(1, self._on_frame_type)

    def _on_frame_type(self, byte):
        frame_type = ord(byte)
        if frame_type == 0x00:
            self.stream.read_until(b("\xff"), self._on_end_delimiter)
        elif frame_type == 0xff:
            self.stream.read_bytes(1, self._on_length_indicator)
        else:
            self._abort()

    def _on_end_delimiter(self, frame):
        if not self.client_terminated:
            msg = frame[:-1]

            if self._auto_decode:
                msg = msg.decode("utf-8", "replace")

            self.async_callback(self.handler.on_message)(msg)
        if not self.client_terminated:
            self._receive_message()

    def _on_length_indicator(self, byte):
        if ord(byte) != 0x00:
            self._abort()
            return
        self.client_terminated = True
        self.close()

    def write_message(self, message, binary=False):
        """Sends the given message to the client of this Web Socket."""
        if binary:
            raise ValueError(
                "Binary messages not supported by this version of websockets")
        if isinstance(message, unicode):
            message = message.encode("utf-8")
        assert isinstance(message, bytes_type)
        self.stream.write(b("\x00") + message + b("\xff"))

    def close(self):
        """Closes the WebSocket connection."""
        if not self.server_terminated:
            if not self.stream.closed():
                self.stream.write("\xff\x00")
            self.server_terminated = True
        if self.client_terminated:
            if self._waiting is not None:
                self.stream.io_loop.remove_timeout(self._waiting)
            self._waiting = None
            self.stream.close()
        elif self._waiting is None:
            self._waiting = self.stream.io_loop.add_timeout(
                time.time() + 5, self._abort)


STRUCT_BB = struct.Struct("BB")
STRUCT_BBH = struct.Struct("!BBH")
STRUCT_BBQ = struct.Struct("!BBQ")
STRUCT_H = struct.Struct("!H")
STRUCT_Q = struct.Struct("!Q")


class WebSocketProtocol13(WebSocketProtocol):
    """Implementation of the WebSocket protocol from RFC 6455.

    This class supports versions 7 and 8 of the protocol in addition to the
    final version 13.
    """
    def __init__(self, handler, auto_decode):
        WebSocketProtocol.__init__(self, handler, auto_decode)
        self._final_frame = False
        self._frame_opcode = None
        self._frame_mask = None
        self._frame_length = None
        self._fragmented_message_buffer = None
        self._fragmented_message_opcode = None
        self._waiting = None

    def accept_connection(self):
        try:
            self._handle_websocket_headers()
            self._accept_connection()
        except ValueError:
            logging.debug("Malformed WebSocket request received")
            self._abort()
            return

    def _handle_websocket_headers(self):
        """Verifies all invariant- and required headers

        If a header is missing or have an incorrect value ValueError will be
        raised
        """
        fields = ("Host", "Sec-Websocket-Key", "Sec-Websocket-Version")
        if not all(map(lambda f: self.request.headers.get(f), fields)):
            raise ValueError("Missing/Invalid WebSocket headers")

    def _challenge_response(self):
        sha1 = hashlib.sha1()
        sha1.update(tornado.escape.utf8(
                self.request.headers.get("Sec-Websocket-Key")))
        sha1.update(b("258EAFA5-E914-47DA-95CA-C5AB0DC85B11")) # Magic value
        return tornado.escape.native_str(base64.b64encode(sha1.digest()))

    def _accept_connection(self):
        subprotocol_header = ''
        subprotocols = self.request.headers.get("Sec-WebSocket-Protocol", '')
        subprotocols = [s.strip() for s in subprotocols.split(',')]
        if subprotocols:
            selected = self.handler.select_subprotocol(subprotocols)
            if selected:
                assert selected in subprotocols
                subprotocol_header = "Sec-WebSocket-Protocol: %s\r\n" % selected

        self.stream.write(tornado.escape.utf8(
            "HTTP/1.1 101 Switching Protocols\r\n"
            "Upgrade: websocket\r\n"
            "Connection: Upgrade\r\n"
            "Sec-WebSocket-Accept: %s\r\n"
            "%s"
            "\r\n" % (self._challenge_response(), subprotocol_header)))

        self.async_callback(self.handler.open)(*self.handler.open_args, **self.handler.open_kwargs)
        self._receive_frame()

    def _write_frame(self, fin, opcode, data):
        if fin:
            finbit = opcode | 0x80
        else:
            finbit = opcode

        l = len(data)
        if l < 126:
            frame = STRUCT_BB.pack(finbit, l)
        elif l <= 0xFFFF:
            frame = STRUCT_BBH.pack(finbit, 126, l)
        else:
            frame = STRUCT_BBQ.pack(finbit, 127, l)
        frame += data
        self.stream.write(frame)

    def write_message(self, message, binary=False):
        """Sends the given message to the client of this Web Socket."""
        if binary:
            opcode = 0x2
        else:
            opcode = 0x1
        message = tornado.escape.utf8(message)
        assert isinstance(message, bytes_type)
        self._write_frame(True, opcode, message)

    def _receive_frame(self):
        self.stream.read_bytes(2, self._on_frame_start)

    def _on_frame_start(self, data):
        header, payloadlen = STRUCT_BB.unpack(data)
        self._final_frame = header & 0x80
        reserved_bits = header & 0x70
        self._frame_opcode = header & 0xf
        self._frame_opcode_is_control = self._frame_opcode & 0x8
        if reserved_bits:
            # client is using as-yet-undefined extensions; abort
            self._abort()
            return
        if not (payloadlen & 0x80):
            # Unmasked frame -> abort connection
            self._abort()
            return
        payloadlen = payloadlen & 0x7f
        if self._frame_opcode_is_control and payloadlen >= 126:
            # control frames must have payload < 126
            self._abort()
            return
        if payloadlen < 126:
            self._frame_length = payloadlen
            self.stream.read_bytes(4, self._on_masking_key)
        elif payloadlen == 126:
            self.stream.read_bytes(2, self._on_frame_length_16)
        elif payloadlen == 127:
            self.stream.read_bytes(8, self._on_frame_length_64)

    def _on_frame_length_16(self, data):
        self._frame_length = STRUCT_H.unpack(data)[0]
        self.stream.read_bytes(4, self._on_masking_key)

    def _on_frame_length_64(self, data):
        self._frame_length = STRUCT_Q.unpack(data)[0]
        self.stream.read_bytes(4, self._on_masking_key)

    def _on_masking_key(self, data):
        self._frame_mask = make_array(data)
        self.stream.read_bytes(self._frame_length, self._on_frame_data)

    def _on_frame_data(self, data):
        unmasked = make_array(data)
        mask = self._frame_mask
        for i in range(len(data)):
            unmasked[i] = unmasked[i] ^ mask[i % 4]

        if self._frame_opcode_is_control:
            # control frames may be interleaved with a series of fragmented
            # data frames, so control frames must not interact with
            # self._fragmented_*
            if not self._final_frame:
                # control frames must not be fragmented
                self._abort()
                return
            opcode = self._frame_opcode
        elif self._frame_opcode == 0:  # continuation frame
            if self._fragmented_message_buffer is None:
                # nothing to continue
                self._abort()
                return
            self._fragmented_message_buffer += unmasked
            if self._final_frame:
                opcode = self._fragmented_message_opcode
                unmasked = self._fragmented_message_buffer
                self._fragmented_message_buffer = None
        else:  # start of new data message
            if self._fragmented_message_buffer is not None:
                # can't start new message until the old one is finished
                self._abort()
                return
            if self._final_frame:
                opcode = self._frame_opcode
            else:
                self._fragmented_message_opcode = self._frame_opcode
                self._fragmented_message_buffer = unmasked

        if self._final_frame:
            self._handle_message(opcode, to_string(unmasked))

        if not self.client_terminated:
            self._receive_frame()

    def _handle_message(self, opcode, data):
        if self.client_terminated:
            return

        if opcode == 0x1:
            # UTF-8 data
            try:
                if self._auto_decode:
                    decoded = data.decode("utf-8")
                else:
                    decoded = data
            except UnicodeDecodeError:
                self._abort()
                return
            self.async_callback(self.handler.on_message)(decoded)
        elif opcode == 0x2:
            # Binary data
            self.async_callback(self.handler.on_message)(data)
        elif opcode == 0x8:
            # Close
            self.client_terminated = True
            self.close()
        elif opcode == 0x9:
            # Ping
            self._write_frame(True, 0xA, data)
        elif opcode == 0xA:
            # Pong
            pass
        else:
            self._abort()

    def close(self):
        """Closes the WebSocket connection."""
        if not self.server_terminated:
            if not self.stream.closed():
                self._write_frame(True, 0x8, b(""))
            self.server_terminated = True
        if self.client_terminated:
            if self._waiting is not None:
                self.stream.io_loop.remove_timeout(self._waiting)
                self._waiting = None
            self.stream.close()
        elif self._waiting is None:
            # Give the client a few seconds to complete a clean shutdown,
            # otherwise just close the connection.
            self._waiting = self.stream.io_loop.add_timeout(
                time.time() + 5, self._abort)

########NEW FILE########
__FILENAME__ = state
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import deque
import heapq
import operator
import signal
from threading import RLock
import time

import pyuv

from .sync import add, sub, increment, atomic_read
from .util import nanotime

class ProcessTracker(object):

    def __init__(self, loop):
        self.processes = []
        self._done_cb = None
        self._check_timer = pyuv.Timer(loop)
        self._lock = RLock()

    def start(self, interval=0.1):
        self._check_timer.start(self._on_check, interval, interval)
        self._check_timer.unref()

    def on_done(self, callback):
        self._done_cb = callback

    def stop(self):
        self._check_timer.stop()
        self.processes = []

    def close(self):
        self.processes = []
        self._done_cb = None
        if not self._check_timer.closed:
            self._check_timer.close()

    def check(self, process, graceful_timeout=10000000000):
        process.graceful_time = graceful_timeout + nanotime()
        heapq.heappush(self.processes, process)

    def uncheck(self, process):
        if process in self.processes:
            del self.processes[operator.indexOf(self.processes, process)]

    def _on_check(self, handle):
        # this function check if a process that need to be stopped after
        # a given graceful time is still in the stopped process. If yes
        # the process is killed. It let the possibility to let the time
        # to some worker to quit.
        #
        # The garbage collector run eveyry 0.1s .
        with self._lock:
            while True:
                if not len(self.processes):
                    # done callback has been set, run it
                    if self._done_cb is not None:
                        self._done_cb()
                        self._donc_cb = None

                    # nothing in the queue, quit
                    break

                # check the diff between the time it is now and the
                # graceful time set when the worker was stopped
                p = heapq.heappop(self.processes)
                now = nanotime()
                delta = p.graceful_time - now
                if delta > 0:
                    # we have anything to do, put the process back in
                    # the heap and return
                    if p.active:
                        heapq.heappush(self.processes, p)
                    break
                else:
                    # a process need to be kill. Send a SIGKILL signal
                    try:
                        p.kill(signal.SIGKILL)
                    except:
                        pass

                    # and close it. (maybe we should just close it)
                    p.close()


class FlappingInfo(object):
    """ object to keep flapping infos """

    def __init__(self, attempts=2, window=1., retry_in=7., max_retry=5):
        self.attempts = attempts
        self.window = window
        self.retry_in = retry_in
        self.max_retry = max_retry

        # exit history
        self.history = deque(maxlen = max_retry)
        self.retries = 0

    def reset(self):
        self.history.clear()
        self.retries = 0

class ProcessState(object):
    """ object used by the manager to maintain the process state for a
    session. """


    def __init__(self, config, sessionid, env=None):
        self.config = config
        self.sessionid = sessionid
        self.env = env

        self.running = deque()
        self.running_out = deque()
        self.stopped = False
        self.setup()

    def setup(self):
        self.name = "%s.%s" % (self.sessionid, self.config.name)
        self.cmd = self.config.cmd
        self._numprocesses = self.config.get('numprocesses', 1)

        # set flapping
        self.flapping = self.config.get('flapping')
        if isinstance(self.flapping, dict):
            try:
                self.flapping = FlappingInfo(**self.flapping)
            except TypeError: # unknown value
                self.flapping = None

        self.flapping_timer = None
        self.stopped = False

    @property
    def active(self):
        return (len(self.running) + len(self.running_out)) > 0

    @property
    def graceful_timeout(self):
        return nanotime(self.config.get('graceful_timeout', 10.0))

    def __str__(self):
        return "state: %s" % self.name

    def make_process(self, loop, id, on_exit):
        """ create an OS process using this template """
        return self.config.make_process(loop, id, self.name, env=self.env,
                on_exit=on_exit)

    def __get_numprocesses(self):
        return atomic_read(self._numprocesses)
    def __set_numprocesses(self, n):
        self._numprocesses = n
    numprocesses = property(__get_numprocesses, __set_numprocesses,
            doc="""return the max numbers of processes that we keep
            alive for this command""")

    @property
    def pids(self):
        pids = [p.pid for p in self.running]
        pids.extend([p.pid for p in self.running_out])
        return pids

    def reset(self):
        """ reset this template to default values """
        self.numprocesses = self.config.get('numprocesses', 1)
        # reset flapping
        if self.flapping and self.flapping is not None:
            self.flapping.reset()

    def update(self, config, env=None):
        """ update a state """
        self.config = config
        self.env = env

        # update the number of preocesses
        self.numprocesses = max(self.config.get('numprocesses', 1),
                self.numprocesses)

    def incr(self, i=1):
        """ increase the maximum number of running processes """

        self._numprocesses = add(self._numprocesses, i)
        return self._numprocesses

    def decr(self, i=1):
        """ decrease the maximum number of running processes """
        self._numprocesses = sub(self._numprocesses, i)
        return self._numprocesses

    def queue(self, process):
        """ put one OS process in the running queue """
        self.running.append(process)

    def dequeue(self):
        """ retrieved one OS process from the queue (FIFO) """
        return self.running.popleft()

    def remove(self, process):
        """ remove an OS process from the running processes """
        try:
            self.running.remove(process)
        except ValueError:
            pass

    def list_processes(self):
        return list(self.running)

    def check_flapping(self):
        """ main function used to check the flapping """
        f = self.flapping

        f.history.append(time.time())
        if len(f.history) >= f.attempts:
            diff = f.history[-1] - f.history[0]
            if diff > f.window:
                f.reset()
                self.flapping = f
            elif f.retries < f.max_retry:
                f.retries = increment(f.retries)
                self.flapping = f
                return False, True
            else:
                f.reset()
                self.flapping = f
                return False, False
        self.flapping = f
        return True, None

########NEW FILE########
__FILENAME__ = tornado_pyuv
# -*- coding: utf-8 -
#
# Copyright (C) 2011 by Sal Ibarra Corretg
#
# This file is part of gaffer. See the NOTICE for more information.

import pyuv

import datetime
import errno
import logging
import os
import time

try:
    import _thread as thread
except ImportError:
    import thread

from collections import deque
import six
from tornado import ioloop, stack_context


class Waker(object):
    def __init__(self, loop):
        self._async = pyuv.Async(loop, lambda x: None)
        self._async.unref()
    def wake(self):
        self._async.send()


class IOLoop(object):
    NONE = ioloop.IOLoop.NONE
    READ = ioloop.IOLoop.READ
    WRITE = ioloop.IOLoop.WRITE
    ERROR = ioloop.IOLoop.ERROR

    _instance_lock = thread.allocate_lock()

    def __init__(self, impl=None, _loop=None):
        if impl is not None:
            raise RuntimeError('When using pyuv the poller implementation cannot be specifiedi')
        self._loop = _loop or pyuv.Loop()
        self._handlers = {}
        self._callbacks = deque()
        self._callback_lock = thread.allocate_lock()
        self._timeouts = set()
        self._running = False
        self._stopped = False
        self._thread_ident = None

        self._cb_handle = pyuv.Prepare(self._loop)
        self._waker = Waker(self._loop)

    @staticmethod
    def instance():
        if not hasattr(IOLoop, "_instance"):
            with IOLoop._instance_lock:
                if not hasattr(IOLoop, "_instance"):
                    # New instance after double check
                    IOLoop._instance = IOLoop()
        return IOLoop._instance

    @staticmethod
    def initialized():
        """Returns true if the singleton instance has been created."""
        return hasattr(IOLoop, "_instance")

    def install(self):
        """Installs this IOLoop object as the singleton instance.

        This is normally not necessary as `instance()` will create
        an IOLoop on demand, but you may want to call `install` to use
        a custom subclass of IOLoop.
        """
        assert not IOLoop.initialized()
        IOLoop._instance = self

    def _close_loop_handles(self):
        def cb(handle):
            if not handle.closed:
                handle.close()
        self._loop.walk(cb)

    def close(self):
        for fd, (poll, callback) in self._handlers.items():
            poll.close()

            try:
                os.close(fd)
            except Exception:
                logging.debug("error closing fd %s", fd, exc_info=True)


        self._handlers = {}
        # Run the loop so the close callbacks are fired and memory is freed
        self._loop.run(pyuv.UV_RUN_NOWAIT)
        self._loop = None

    def add_handler(self, fd, handler, events):
        if fd in self._handlers:
            raise IOError("fd %d already registered" % fd)

        poll = pyuv.Poll(self._loop, fd)
        self._handlers[fd] = (poll, stack_context.wrap(handler))
        poll_events = 0
        if (events & IOLoop.READ):
            poll_events |= pyuv.UV_READABLE
        if (events & IOLoop.WRITE):
            poll_events |= pyuv.UV_WRITABLE
        poll.start(poll_events, self._handle_poll_events)

    def update_handler(self, fd, events):
        poll, _ = self._handlers[fd]
        poll_events = 0
        if (events & IOLoop.READ):
            poll_events |= pyuv.UV_READABLE
        if (events & IOLoop.WRITE):
            poll_events |= pyuv.UV_WRITABLE
        poll.start(poll_events, self._handle_poll_events)

    def remove_handler(self, fd):
        items = self._handlers.pop(fd, None)
        if items is not None:
            items[0].close()

    def set_blocking_signal_threshold(self, seconds, action):
        raise NotImplementedError

    def set_blocking_log_threshold(self, seconds):
        raise NotImplementedError

    def log_stack(self, signal, frame):
        raise NotImplementedError

    def start(self):
        if self._stopped:
            self._stopped = False
            return
        self._thread_ident = thread.get_ident()
        self._running = True

        # This timer will prevent busy-looping in case there is nothing scheduled in the loop
        timer = pyuv.Timer(self._loop)
        timer.start(lambda x: None, 3600, 3600)

        self._loop.run(pyuv.UV_RUN_DEFAULT)

        timer.close()

        # reset the stopped flag so another start/stop pair can be issued
        self._stopped = False

    def stop(self):
        self._running = False
        self._stopped = True
        self._loop.stop()
        self._waker.wake()

    def running(self):
        """Returns true if this IOLoop is currently running."""
        return self._running

    def add_timeout(self, deadline, callback):
        timeout = _Timeout(deadline, stack_context.wrap(callback), io_loop=self)
        self._timeouts.add(timeout)
        return timeout

    def remove_timeout(self, timeout):
        self._timeouts.remove(timeout)
        timer = timeout._timer
        if timer.active:
            timer.stop()

    def add_callback(self, callback):
        with self._callback_lock:
            was_active = self._cb_handle.active
            self._callbacks.append(stack_context.wrap(callback))
            if not was_active:
                self._cb_handle.start(self._prepare_cb)
        if not was_active or thread.get_ident() != self._thread_ident:
            self._waker.wake()

    def handle_callback_exception(self, callback):
        """This method is called whenever a callback run by the IOLoop
        throws an exception.

        By default simply logs the exception as an error.  Subclasses
        may override this method to customize reporting of exceptions.

        The exception itself is not passed explicitly, but is available
        in sys.exc_info.
        """
        logging.error("Exception in callback %r", callback, exc_info=True)

    def _run_callback(self, callback):
        try:
            callback()
        except Exception:
            self.handle_callback_exception(callback)

    def _handle_poll_events(self, handle, poll_events, error):
        events = 0
        if error is not None:
            # Some error was detected, signal readability and writability so that the
            # handler gets and handles the error
            events |= IOLoop.READ
            events |= IOLoop.WRITE
        else:
            if (poll_events & pyuv.UV_READABLE):
                events |= IOLoop.READ
            if (poll_events & pyuv.UV_WRITABLE):
                events |= IOLoop.WRITE
        fd = handle.fileno()
        try:
            self._handlers[fd][1](fd, events)
        except (OSError, IOError) as e:
            if e.args[0] == errno.EPIPE:
                # Happens when the client closes the connection
                pass
            else:
                logging.error("Exception in I/O handler for fd %s", fd, exc_info=True)
        except Exception:
            logging.error("Exception in I/O handler for fd %s", fd, exc_info=True)

    def _prepare_cb(self, handle):
        self._cb_handle.stop()
        with self._callback_lock:
            callbacks = self._callbacks
            self._callbacks = deque()
        while callbacks:
            self._run_callback(callbacks.popleft())


class _Timeout(object):
    """An IOLoop timeout, a UNIX timestamp and a callback"""

    # Reduce memory overhead when there are lots of pending callbacks
    __slots__ = ['deadline', 'callback', 'io_loop', '_timer']

    def __init__(self, deadline, callback, io_loop=None):
        now = time.time()
        if (isinstance(deadline, six.integer_types)
                or isinstance(deadline, float)):
            self.deadline = deadline
        elif isinstance(deadline, datetime.timedelta):
            self.deadline = now + _Timeout.timedelta_to_seconds(deadline)
        else:
            raise TypeError("Unsupported deadline %r" % deadline)
        self.callback = callback
        self.io_loop = io_loop or IOLoop.instance()
        timeout = max(self.deadline - now, 0)
        self._timer = pyuv.Timer(self.io_loop._loop)
        self._timer.start(self._timer_cb, timeout, 0.0)

    def _timer_cb(self, handle):
        self._timer.close()
        self._timer = None
        self.io_loop._timeouts.remove(self)
        self.io_loop._run_callback(self.callback)
        self.io_loop = None

    @staticmethod
    def timedelta_to_seconds(td):
        """Equivalent to td.total_seconds() (introduced in python 2.7)."""
        return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / float(10 ** 6)


class PeriodicCallback(object):
    def __init__(self, callback, callback_time, io_loop=None):
        self.callback = callback
        self.callback_time = callback_time / 1000.0
        self.io_loop = io_loop or IOLoop.instance()
        self._timer = pyuv.Timer(self.io_loop._loop)
        self._running = False

    def _timer_cb(self, timer):
        try:
            self.callback()
        except Exception:
            logging.error("Error in periodic callback", exc_info=True)

    def start(self):
        if self._running:
            return
        self._running = True
        self._timer.start(self._timer_cb, self.callback_time, self.callback_time)
        self._timer.repeat = self.callback_time

    def stop(self):
        if not self._running:
            return
        self._running = False
        self._timer.stop()


def install():
    # Patch Tornado's classes with ours
    ioloop.IOLoop = IOLoop
    ioloop._Timeout = _Timeout
    ioloop.PeriodicCallback = PeriodicCallback

########NEW FILE########
__FILENAME__ = util
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os
import platform
import signal
import socket
import ssl
import string
import sys
import time

IS_WINDOWS = platform.system() == 'Windows'
if not IS_WINDOWS:
    import grp
    import pwd
    import resource
else:
    grp = None
    pwd = None
    resource = None

import six
from tornado import netutil

DEFAULT_CA_CERTS = os.path.dirname(__file__) + '/cacert.pem'

if six.PY3:
    def bytestring(s):
        return s

    def ord_(c):
        return c

    import urllib.parse
    urlparse = urllib.parse.urlparse
    quote = urllib.parse.quote
    quote_plus = urllib.parse.quote_plus
    unquote = urllib.parse.unquote
    urlencode = urllib.parse.urlencode
else:
    def bytestring(s):
        if isinstance(s, unicode):
            return s.encode('utf-8')
        return s

    def ord_(c):
        return ord(c)

    import urlparse
    urlparse = urlparse.urlparse

    import urllib
    quote = urllib.quote
    quote_plus = urllib.quote_plus
    unquote = urllib.unquote
    urlencode = urllib.urlencode


_SYMBOLS = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')

MAXFD = 1024
if hasattr(os, "devnull"):
    REDIRECT_TO = os.devnull
else:
    REDIRECT_TO = "/dev/null"

try:
    from setproctitle import setproctitle
    def setproctitle_(title):
        setproctitle(title)
except ImportError:
    def setproctitle(_title):
        return

def getcwd():
    """Returns current path, try to use PWD env first"""
    try:
        a = os.stat(os.environ['PWD'])
        b = os.stat(os.getcwd())
        if a.ino == b.ino and a.dev == b.dev:
            working_dir = os.environ['PWD']
        else:
            working_dir = os.getcwd()
    except:
        working_dir = os.getcwd()
    return working_dir


def check_uid(val):
    """Return an uid, given a user value.
    If the value is an integer, make sure it's an existing uid.

    If the user value is unknown, raises a ValueError.
    """
    if isinstance(val, six.integer_types):
        try:
            pwd.getpwuid(val)
            return val
        except (KeyError, OverflowError):
            raise ValueError("%r isn't a valid user id" % val)

    if not isinstance(val, str):
        raise TypeError(val)

    try:
        return pwd.getpwnam(val).pw_uid
    except KeyError:
        raise ValueError("%r isn't a valid user val" % val)


def check_gid(val):
    """Return a gid, given a group value

    If the group value is unknown, raises a ValueError.
    """

    if isinstance(val, int):
        try:
            grp.getgrgid(val)
            return val
        except (KeyError, OverflowError):
            raise ValueError("No such group: %r" % val)

    if not isinstance(val, str):
        raise TypeError(val)
    try:
        return grp.getgrnam(val).gr_gid
    except KeyError:
        raise ValueError("No such group: %r" % val)


def bytes2human(n):
    """Translates bytes into a human repr.
    """
    if not isinstance(n, six.integer_types):
        raise TypeError(n)

    prefix = {}
    for i, s in enumerate(_SYMBOLS):
        prefix[s] = 1 << (i + 1) * 10

    for s in reversed(_SYMBOLS):
        if n >= prefix[s]:
            value = int(float(n) / prefix[s])
            return '%s%s' % (value, s)
    return "%sB" % n

def parse_address(netloc, default_port=8000):
    if netloc.startswith("unix:"):
        return netloc.split("unix:")[1]

    # get host
    if '[' in netloc and ']' in netloc:
        host = netloc.split(']')[0][1:].lower()
    elif ':' in netloc:
        host = netloc.split(':')[0].lower()
    elif netloc == "":
        host = "0.0.0.0"
    else:
        host = netloc.lower()

    #get port
    netloc = netloc.split(']')[-1]
    if ":" in netloc:
        port = netloc.split(':', 1)[1]
        if not port.isdigit():
            raise RuntimeError("%r is not a valid port number." % port)
        port = int(port)
    else:
        port = default_port
    return (host, port)

def is_ipv6(addr):
    try:
        socket.inet_pton(socket.AF_INET6, addr)
    except socket.error: # not a valid address
        return False
    return True

def bind_sockets(addr, backlog=128, allows_unix_socket=False):
    # initialize the socket
    addr = parse_address(addr)
    if isinstance(addr, six.string_types):
        if not allows_unix_socket:
            raise RuntimeError("unix addresses aren't supported")

        sock = netutil.bind_unix_socket(addr)
    elif is_ipv6(addr[0]):
        sock = netutil.bind_sockets(addr[1], address=addr[0],
                family=socket.AF_INET6, backlog=backlog)
    else:
        sock = netutil.bind_sockets(addr[1], backlog=backlog)
    return sock

def hostname():
    return socket.getfqdn(socket.gethostname())


try:
    from os import closerange
except ImportError:
    def closerange(fd_low, fd_high):    # NOQA
        # Iterate through and close all file descriptors.
        for fd in range(fd_low, fd_high):
            try:
                os.close(fd)
            except OSError:    # ERROR, fd wasn't open to begin with (ignored)
                pass


# http://www.svbug.com/documentation/comp.unix.programmer-FAQ/faq_2.html#SEC16
def daemonize():
    """Standard daemonization of a process.
    """

    if IS_WINDOWS:
        raise RuntimeError('Daemonizing is not supported on Windows.')

    if os.fork():
        os._exit(0)
    os.setsid()

    if os.fork():
        os._exit(0)

    os.umask(0)
    maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]
    if (maxfd == resource.RLIM_INFINITY):
        maxfd = MAXFD
    closerange(0, maxfd)

    os.open(REDIRECT_TO, os.O_RDWR)
    os.dup2(0, 1)
    os.dup2(0, 2)


def nanotime(s=None):
    """ convert seconds to nanoseconds. If s is None, current time is
    returned """
    if s is not None:
        return int(s) * 1000000000
    return time.time() * 1000000000

def from_nanotime(n):
    """ convert from nanotime to seconds """
    return n / 1.0e9

def substitute_env(s, env):
    return string.Template(s).substitute(env)

def parse_signal_value(sig):
    if sig is None:
        raise ValueError("invalid signal")

    # value passed is a string
    if isinstance(sig, six.string_types):
        if sig.isdigit():
            # if number in the string, try to parse it
            try:
                return int(sig)
            except ValueError:
                raise ValueError("invalid signal")

        # else try to get the signal number from its name
        signame = sig.upper()
        if not signame.startswith('SIG'):
            signame = "SIG%s" % signame
        try:
            signum = getattr(signal, signame)
        except AttributeError:
            raise ValueError("invalid signal name")
        return signum

    # signal is a number, just return it
    return sig

def parse_job_name(name, default='default'):
    if "." in name:
        appname, name = name.split(".", 1)
    elif "/" in name:
        appname, name = name.split("/", 1)
    else:
        appname = default

    return appname, name

def is_ssl(url):
    return url.startswith("https") or url.startswith("wss")

def parse_ssl_options(client_options):
    ssl_options = {}
    if client_options.get('validate_cert'):
        ssl_options["cert_reqs"] = ssl.CERT_REQUIRED
    if client_options.get('ca_certs') is not None:
        ssl_options["ca_certs"] = client_options['ca_certs']
    else:
        ssl_options["ca_certs"] = DEFAULT_CA_CERTS
    if client_options.get('client_key') is not None:
        ssl_options["keyfile"] = client_options['client_key']
    if client_options.get('client_cert') is not None:
        ssl_options["certfile"] = client_options['client_cert']


    # disable SSLv2
    # http://blog.ivanristic.com/2011/09/ssl-survey-protocol-support.html
    if sys.version_info >= (2, 7):
        ssl_options["ciphers"] = "DEFAULT:!SSLv2"
    else:
        # This is really only necessary for pre-1.0 versions
        # of openssl, but python 2.6 doesn't expose version
        # information.
        ssl_options["ssl_version"] = ssl.PROTOCOL_SSLv3
    return ssl_options


########NEW FILE########
__FILENAME__ = webhooks
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
"""
Webhooks allow to register an url to a specific event (or alls) and the
event will be posted on this URL. Each events can triger a post on a
given url.

for example to listen all create events on http://echohttp.com/echo you
can add this line in the webhooks sections of the gaffer setting file::

    [webhooks]
    create = http://echohttp.com/echo you

Or programatically::

    from gaffer.manager import Manager
    from gaffer.webhooks import WebHooks
    hooks = [("create", "http://echohttp.com/echo you ")
    webhooks = WebHooks(hooks=hooks)

    manager = Manager()
    manager.start(apps=[webhooks])


This gaffer application is started like other applications in the
manager. All :doc:`events` are supported.


The :mod:`webhooks` Module
--------------------------

"""
from functools import partial
import json

from tornado.httpclient import HTTPError

from .httpclient import HTTPClient
from .sync import atomic_read, increment, decrement

class WebHooks(object):
    """ webhook app """

    def __init__(self, hooks=[]):
        self.events = {}
        self._refcount = 0
        self.active = False

        # initialize hooks
        for event, url in hooks:
            if not event in self.events:
                self.events[event] = set()
            self.events[event].add(url)
            self.incref()

    def start(self, loop, manager):
        """ start the webhook app """
        self.loop = loop
        self.manager = manager
        self.maybe_start_monitor()

    def stop(self):
        """ stop the webhook app, stop monitoring to events """
        if self.active:
            self._stop_monitor()

    def restart(self):
        self._stop_monitor()
        self._start_monitor()

    def close(self):
        self.stop()
        self.events = []
        self._refcount = 0

    @property
    def refcount(self):
        return atomic_read(self._refcount)

    def register_hook(self, event, url):
        """  associate an url to an event """
        if event not in self.events:
            self.events[event] = set()

        self.events[event].add(url)

        self.incref()
        self.maybe_start_monitor()

    def unregister_hook(self, event, url):
        """ unregister an url for this event """
        if event not in self.events:
            return

        # remove an url from registered hooks
        urls = self.events[event]
        urls.remove(url)
        self.events[event] = urls

        self.decref()
        self.maybe_stop_monitor()

    def maybe_start_monitor(self):
        if self.refcount and not self.active:
            self._start_monitor()

    def maybe_stop_monitor(self):
        if self.refcount > 0 or not self.active:
            return

        self._stop_monitor()

    def incref(self):
        self._refcount = increment(self._refcount)

    def decref(self):
        self._refcount = decrement(self._refcount)

    def _on_event(self, event, msg):
        if not self.active:
            return

        urls = set()
        if event in self.events:
            urls = self.events[event]

        if "." in self.events:
            urls = urls.union(self.events['.'])

        if not urls:
            return

        # queue the hook, it will be executed in a thread asap
        callback = partial(self._post_hook, msg, urls)
        self.loop.queue_work(callback)

    def _post_hook(self, msg, urls):
        body = json.dumps(msg)
        headers = { "Content-Length": str(len(body)),
                    "Content-Type": "application/json" }

        client = HTTPClient()
        for url in urls:
            try:
                client.fetch(url, method="POST", headers=headers,
                        body=body)
            except HTTPError:
                # for now we ignore all http errors.
                pass

    def _start_monitor(self):
        self.manager.events.subscribe(".", self._on_event)
        self.active = True

    def _stop_monitor(self):
        self.manager.events.unsubscribe(".", self._on_event)
        self.active = False

########NEW FILE########
__FILENAME__ = generic
import sys


def resolve_name(name):
    ret = None
    parts = name.split('.')
    cursor = len(parts)
    module_name = parts[:cursor]
    last_exc = None

    while cursor > 0:
        try:
            ret = __import__('.'.join(module_name))
            break
        except ImportError as exc:
            last_exc = exc
            if cursor == 0:
                raise
            cursor -= 1
            module_name = parts[:cursor]

    for part in parts[1:]:
        try:
            ret = getattr(ret, part)
        except AttributeError:
            if last_exc is not None:
                raise last_exc
            raise ImportError(name)

    if ret is None:
        if last_exc is not None:
            raise last_exc
        raise ImportError(name)

    return ret


if __name__ == '__main__':
    cb = resolve_name(sys.argv[1])

    try:
        if len(sys.argv) > 2:
            test_file = sys.argv[2]
            sys.exit(cb(test_file))
        else:
            sys.exit(cb())
    except:
        sys.exit(1)

########NEW FILE########
__FILENAME__ = proc_crash
#!/usr/bin/env python

import sys
import signal
import time

class CrashProcess(object):

    def __init__(self):
        self.alive = True

    def run(self):
        while self.alive:
            time.sleep(0.1)
            break

if __name__ == "__main__":
    c = CrashProcess()
    c.run()
    sys.exit(1)


########NEW FILE########
__FILENAME__ = proc_custom_stream
#!/usr/bin/env python

from __future__ import print_function

import os
import sys

PY3 = sys.version_info[0] == 3

if PY3:
    stream = os.fdopen(3, 'wb+', buffering=0)
else:
    stream =  os.fdopen(3, "w+")

c = stream.readline()
stream.write(c)
stream.flush()

########NEW FILE########
__FILENAME__ = proc_dummy
#!/usr/bin/env python

import signal
import sys
import time

class DummyProcess(object):

    QUIT_SIGNALS = (signal.SIGQUIT, signal.SIGTERM,)

    def __init__(self, testfile):
        self.alive = True
        self.testfile = testfile
        self.queue = []
        signal.signal(signal.SIGHUP, self.handle)
        signal.signal(signal.SIGQUIT, self.handle)
        signal.signal(signal.SIGTERM, self.handle)
        signal.signal(signal.SIGCHLD, self.handle_chld)
        signal.signal(signal.SIGWINCH, self.handle_winch)

    def _write(self, msg):
        with open(self.testfile, 'a+') as f:
            f.write(msg)
            f.flush()

    def handle(self, signum, frame):
        self.queue.append(signum)

    def handle_quit(self):
        self._write('QUIT')
        self.alive = False

    def handle_chld(self, *args):
        self._write('CHLD')

    def handle_winch(self, *args):
        return

    def handle_hup(self):
        self._write('HUP')

    def run(self):
        self._write('START')

        # write to std
        sys.stdout.write("hello out")
        sys.stdout.flush()
        sys.stderr.write("hello err")
        sys.stderr.flush()

        while self.alive:
            sig = None
            try:
                sig = self.queue.pop(0)
            except IndexError:
                pass

            if sig is not None:
                if sig in self.QUIT_SIGNALS:
                    self.handle_quit()
                    break
                elif sig == signal.SIGHUP:
                    self.handle_hup()

            time.sleep(0.001)

        self._write('STOP')

if __name__ == "__main__":
    dummy = DummyProcess(sys.argv[1])
    dummy.run()
    sys.exit(1)



########NEW FILE########
__FILENAME__ = proc_stdin_stdout
#!/usr/bin/env python

from __future__ import print_function

import sys

c = sys.stdin.readline()
print(c)
sys.stdout.flush()

########NEW FILE########
__FILENAME__ = test_channel
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import time

from gaffer.manager import Manager
from gaffer.process import ProcessConfig

from test_manager import dummy_cmd

import pyuv

def test_basic():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, msg):
        emitted.append((ev, msg['name']))

    # subscribe to all events
    chan = m.subscribe("EVENTS")
    chan.bind('.', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    m.scale("dummy", 1)
    m.unload("dummy")

    time.sleep(0.2)
    m.stop()
    m.run()

    assert ('load', 'default.dummy') in emitted
    assert ('start', 'default.dummy') in emitted
    assert ('update', 'default.dummy') in emitted
    assert ('stop', 'default.dummy') in emitted
    assert ('unload', 'default.dummy') in emitted

def test_process_events():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, *args):
        emitted.append(ev)

    # subscribe to all events
    chan = m.subscribe("JOB:default.dummy")
    chan.bind_all(cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    m.unload(config)

    time.sleep(0.2)
    m.stop()
    m.run()

    assert 'start' in emitted
    assert 'spawn' in emitted
    assert 'stop' in emitted
    assert 'exit' in emitted

def test_stats():
    m = Manager()
    monitored = []
    def cb(info):
        monitored.append(info)

    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("a", cmd, args=args, cwd=wdir)
    m.load(config)
    os_pid = m.running[1].os_pid

    chan = m.subscribe("STATS:default.a")
    chan.bind(cb)

    def stop(handle):
        chan.unbind(cb)
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.3, 0.0)

    m.run()
    assert len(monitored) >= 1
    res = monitored[0]
    assert "cpu" in res
    assert res["os_pid"] == os_pid

########NEW FILE########
__FILENAME__ = test_controller
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import signal
import time

import pyuv

from gaffer.error import ProcessError, CommandError, CommandNotFound
from gaffer.controller import Controller, Command
from gaffer.manager import Manager
from gaffer.process import ProcessConfig

from test_manager import dummy_cmd

class TestCommand(Command):

    def __init__(self, name, args=None, kwargs=None):
        super(TestCommand, self).__init__(name, args=args, kwargs=kwargs)
        self.result = None
        self.error = None

    def reply(self, result):
        self.result = result

    def reply_error(self, error):
        self.error = error

def init():
    m = Manager()
    m.start()
    ctl = Controller(m)
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    return m, ctl, config

def init1():
    m = Manager()
    m.start()
    ctl = Controller(m)
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    return m, ctl, testfile, config

def test_basic():
    m, ctl, conf = init()

    config = conf.to_dict()
    cmd = TestCommand("load", [config], {"start": False})
    ctl.process_command(cmd)

    state = m._get_locked_state("dummy")

    assert state.numprocesses == 1
    assert state.name == "default.dummy"
    assert state.cmd == config['cmd']
    assert state.config['args'] == config['args']
    assert state.config['cwd'] == config['cwd']

    cmd1 =  TestCommand("unload", ["dummy"])
    ctl.process_command(cmd1)

    assert m.jobs() == []

    m.stop()
    m.run()

    assert cmd.error == None
    assert cmd.result == {"ok": True}
    assert cmd1.error == None
    assert cmd1.result == {"ok": True}

def test_basic_error():
    m, ctl, config = init()

    cmd = TestCommand("info", ["dummy"])
    ctl.process_command(cmd)

    m.stop()
    m.run()

    assert cmd.result == None
    assert cmd.error == {"errno": 404, "reason": "not_found"}

def test_command_not_found():
    m, ctl, config = init()

    cmd = TestCommand("nocommand")
    ctl.process_command(cmd)

    m.stop()
    m.run()

    assert cmd.result == None
    assert cmd.error == {"errno": 404, "reason": "command_not_found"}

def test_bad_command():
    m, ctl, config = init()

    cmd = TestCommand("load")
    ctl.process_command(cmd)

    m.stop()
    m.run()

    assert cmd.result == None
    assert cmd.error['errno'] == 400

def test_jobs():
    m, ctl, config = init()
    m.load(config)
    cmd = TestCommand("jobs")
    ctl.process_command(cmd)

    cmd1 = TestCommand("jobs", ["default"])
    ctl.process_command(cmd1)

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "jobs" in cmd.result
    assert cmd.result['jobs'] == ["default.dummy"]
    assert isinstance(cmd1.result, dict)
    assert "jobs" in cmd1.result
    assert cmd1.result['jobs'] == ["default.dummy"]

def test_sessions():
    m, ctl, config = init()
    m.load(config)
    cmd = TestCommand("sessions")
    ctl.process_command(cmd)
    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "sessions" in cmd.result
    assert cmd.result['sessions'] == ["default"]

def test_pids():
    m, ctl, config = init()
    m.load(config)
    cmd = TestCommand("pids")
    ctl.process_command(cmd)
    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "pids" in cmd.result
    assert cmd.result['pids'] == [1]

def test_reload():
    m, ctl, config = init()
    m.load(config)

    pids = m.pids()
    cmd = TestCommand("reload", ["dummy"])
    ctl.process_command(cmd)

    pids1 = m.pids()
    jobs = m.jobs()
    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert cmd.result == {"ok": True}
    assert pids != pids1
    assert "default.dummy" in jobs

def test_update():
    m, ctl, config = init()
    m.load(config)

    pids = m.pids()
    config['numprocesses'] = 2
    cmd = TestCommand("update", [config.to_dict()])
    ctl.process_command(cmd)
    m.manage("dummy")
    pids1 = m.pids()
    jobs = m.jobs()
    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert cmd.result == {"ok": True}
    assert pids != pids1
    assert "default.dummy" in jobs
    assert len(pids) == 1
    assert len(pids1) == 2

def test_start_job():
    m, ctl, config = init()
    m.load(config, start=False)
    pids = m.pids()

    cmd = TestCommand("start_job", ["dummy"])
    ctl.process_command(cmd)
    pids1 = m.pids()

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert cmd.result == {"ok": True}
    assert pids != pids1
    assert len(pids) == 0
    assert len(pids1) == 1


def test_stop_job():
    m, ctl, config = init()
    m.load(config)
    pids = m.pids()

    cmd = TestCommand("stop_job", ["dummy"])
    ctl.process_command(cmd)
    pids1 = m.pids()

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert cmd.result == {"ok": True}
    assert pids != pids1
    assert len(pids) == 1
    assert len(pids1) == 0

def test_scale():
    m, ctl, config = init()
    m.load(config)
    pids = m.pids()

    cmd = TestCommand("scale", ["dummy", 1])
    ctl.process_command(cmd)
    time.sleep(0.1)

    pids1 = m.pids()


    cmd = TestCommand("scale", ["dummy", -1])
    ctl.process_command(cmd)
    time.sleep(0.1)

    pids2 = m.pids()


    cmd = TestCommand("scale", ["dummy", "+4"])
    ctl.process_command(cmd)
    time.sleep(0.1)

    pids3 = m.pids()

    cmd = TestCommand("scale", ["dummy", "-1"])
    ctl.process_command(cmd)
    time.sleep(0.1)

    pids4 = m.pids()


    cmd = TestCommand("scale", ["dummy", "=1"])
    ctl.process_command(cmd)
    time.sleep(0.1)

    pids5 = m.pids()


    m.stop()
    m.run()

    assert len(pids) == 1
    assert len(pids1) == 2
    assert len(pids2) == 1
    assert len(pids3) == 5
    assert len(pids4) == 4
    assert len(pids5) == 1


def test_info():
    m, ctl, config = init()
    m.load(config)

    cmd = TestCommand("info", ["dummy"])
    ctl.process_command(cmd)

    m.scale("dummy", 2)
    time.sleep(0.1)

    cmd1 = TestCommand("info", ["dummy"])
    ctl.process_command(cmd1)

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "info" in cmd.result
    info = cmd.result['info']
    assert info['name'] == "default.dummy"
    assert info['active'] == True
    assert info['running'] == 1
    assert info['max_processes'] == 1
    assert "config" in info
    assert info['config'] == config.to_dict()

    assert isinstance(cmd1.result, dict)
    assert "info" in cmd1.result
    info1 = cmd1.result['info']
    assert info1['name'] == "default.dummy"
    assert info1['running'] == 3
    assert info1['config'] == config.to_dict()

def test_stats():
    m, ctl, config = init()
    m.load(config)

    cmd = TestCommand("stats", ["dummy"])
    ctl.process_command(cmd)

    m.scale("dummy", 2)
    time.sleep(0.1)

    cmd1 = TestCommand("stats", ["dummy"])
    ctl.process_command(cmd1)

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "stats" in cmd.result
    stats = cmd.result['stats']
    assert isinstance(stats, dict)
    assert stats['name'] == "default.dummy"
    assert "stats" in stats
    assert len(stats['stats']) == 1
    assert stats['stats'][0]['pid'] == 1

    assert isinstance(cmd1.result, dict)
    stats = cmd1.result['stats']
    assert isinstance(stats, dict)
    assert stats['name'] == "default.dummy"
    assert "stats" in stats
    assert len(stats['stats']) == 3

def test_stopall():
    m, ctl, config = init()
    m.load(config)

    pids = m.pids()

    cmd = TestCommand("stopall", ["dummy"])
    ctl.process_command(cmd)

    jobs = m.jobs()
    pids1 = m.pids()

    m.stop()
    m.run()

    assert len(pids) == 1
    assert jobs == ['default.dummy']
    assert len(pids1) == 0

def test_killall():
    m, ctl, testfile, config = init1()
    m.load(config)

    time.sleep(0.1)
    cmd = TestCommand("killall", ["dummy", signal.SIGHUP])
    ctl.process_command(cmd)
    time.sleep(0.2)

    m.stop_job("dummy")
    time.sleep(0.1)

    m.stop()
    m.run()

    assert cmd.result == {"ok": True}
    with open(testfile, 'r') as f:
        res = f.read()
        assert res == 'STARTHUPQUITSTOP'

def test_process_info():
    m, ctl, config = init()
    m.load(config)

    cmd = TestCommand("process_info", [1])
    ctl.process_command(cmd)

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "info" in cmd.result
    info = cmd.result['info']
    assert info['name'] == "default.dummy"
    assert info['active'] == True
    assert info['pid'] == 1
    assert info['cmd'] == config['cmd']
    assert info['args'] == config['args']

def test_process_stats():
    m, ctl, config = init()
    m.load(config)

    cmd = TestCommand("process_stats", [1])
    ctl.process_command(cmd)

    m.stop()
    m.run()

    assert isinstance(cmd.result, dict)
    assert "stats" in cmd.result
    stats = cmd.result['stats']
    assert "cpu" in stats

def test_stop_process():
    m, ctl, config = init()
    m.load(config)

    pids = m.pids()

    cmd = TestCommand("stop_process", [1])
    ctl.process_command(cmd)

    result = []
    def wait(h):
        result.append((m.jobs(), m.pids()))
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(wait, 0.2, 0.0)

    m.run()

    assert len(result) == 1
    jobs, pids1 = result[0]

    assert len(pids) == 1
    assert jobs == ['default.dummy']
    assert pids != pids1
    assert len(pids1) == 1

def test_kill_process():
    m, ctl, testfile, config = init1()
    m.load(config)

    time.sleep(0.1)
    cmd = TestCommand("kill", [1, signal.SIGHUP])
    ctl.process_command(cmd)
    time.sleep(0.2)

    m.stop_job("dummy")

    t = pyuv.Timer(m.loop)
    t.start(lambda h: m.stop(), 0.2, 0.0)
    m.run()

    assert cmd.result == {"ok": True}
    with open(testfile, 'r') as f:
        res = f.read()
        assert res == 'STARTHUPQUITSTOP'

def test_process_commit():
    m, ctl, config = init()
    config['numprocesses'] = 0
    m.load(config, start=False)
    cmd = TestCommand("commit", ["dummy"])
    ctl.process_command(cmd)
    time.sleep(0.1)

    state = m._get_locked_state("dummy")
    assert len(state.running) == 0
    assert state.numprocesses == 0
    assert len(state.running_out) == 1
    assert m.pids() == [1]

    m.stop()
    m.run()

    assert cmd.result["pid"] == 1

########NEW FILE########
__FILENAME__ = test_docopt
from gaffer.docopt import (docopt, DocoptExit, DocoptLanguageError, Option,
        Argument, Command, AnyOptions, Required, Optional, Either, OneOrMore,
        parse_argv, parse_pattern, #parse_defaults, printable_usage,
        formal_usage, TokenStream, printable_usage)
from pytest import raises


def test_pattern_flat():
    assert Required(OneOrMore(Argument('N')),
                    Option('-a'), Argument('M')).flat() == \
                            [Argument('N'), Option('-a'), Argument('M')]
    assert Required(Optional(AnyOptions()),
                    Optional(Option('-a', None))).flat(AnyOptions) == \
                            [AnyOptions()]


def test_option():
    assert Option.parse('-h') == Option('-h', None)
    assert Option.parse('--help') == Option(None, '--help')
    assert Option.parse('-h --help') == Option('-h', '--help')
    assert Option.parse('-h, --help') == Option('-h', '--help')

    assert Option.parse('-h TOPIC') == Option('-h', None, 1)
    assert Option.parse('--help TOPIC') == Option(None, '--help', 1)
    assert Option.parse('-h TOPIC --help TOPIC') == Option('-h', '--help', 1)
    assert Option.parse('-h TOPIC, --help TOPIC') == Option('-h', '--help', 1)
    assert Option.parse('-h TOPIC, --help=TOPIC') == Option('-h', '--help', 1)

    assert Option.parse('-h  Description...') == Option('-h', None)
    assert Option.parse('-h --help  Description...') == Option('-h', '--help')
    assert Option.parse('-h TOPIC  Description...') == Option('-h', None, 1)

    assert Option.parse('    -h') == Option('-h', None)

    assert Option.parse('-h TOPIC  Descripton... [default: 2]') == \
               Option('-h', None, 1, '2')
    assert Option.parse('-h TOPIC  Descripton... [default: topic-1]') == \
               Option('-h', None, 1, 'topic-1')
    assert Option.parse('--help=TOPIC  ... [default: 3.14]') == \
               Option(None, '--help', 1, '3.14')
    assert Option.parse('-h, --help=DIR  ... [default: ./]') == \
               Option('-h', '--help', 1, "./")
    assert Option.parse('-h TOPIC  Descripton... [dEfAuLt: 2]') == \
               Option('-h', None, 1, '2')


def test_option_name():
    assert Option('-h', None).name == '-h'
    assert Option('-h', '--help').name == '--help'
    assert Option(None, '--help').name == '--help'


def test_commands():
    assert docopt('Usage: prog add', 'add') == {'add': True}
    assert docopt('Usage: prog [add]', '') == {'add': False}
    assert docopt('Usage: prog [add]', 'add') == {'add': True}
    assert docopt('Usage: prog (add|rm)', 'add') == {'add': True, 'rm': False}
    assert docopt('Usage: prog (add|rm)', 'rm') == {'add': False, 'rm': True}
    assert docopt('Usage: prog a b', 'a b') == {'a': True, 'b': True}
    with raises(DocoptExit):
        docopt('Usage: prog a b', 'b a')


def test_printable_and_formal_usage():
    doc = """
    Usage: prog [-hv] ARG
           prog N M

    prog is a program."""
    assert printable_usage(doc) == "Usage: prog [-hv] ARG\n           prog N M"
    assert formal_usage(printable_usage(doc)) == "( [-hv] ARG ) | ( N M )"
    assert printable_usage('uSaGe: prog ARG\n\t \t\n bla') == "uSaGe: prog ARG"


def test_parse_argv():
    o = [Option('-h'), Option('-v', '--verbose'), Option('-f', '--file', 1)]
    TS = lambda s: TokenStream(s, error=DocoptExit)
    assert parse_argv(TS(''), options=o) == []
    assert parse_argv(TS('-h'), options=o) == [Option('-h', None, 0, True)]
    assert parse_argv(TS('-h --verbose'), options=o) == \
            [Option('-h', None, 0, True), Option('-v', '--verbose', 0, True)]
    assert parse_argv(TS('-h --file f.txt'), options=o) == \
            [Option('-h', None, 0, True), Option('-f', '--file', 1, 'f.txt')]
    assert parse_argv(TS('-h --file f.txt arg'), options=o) == \
            [Option('-h', None, 0, True),
             Option('-f', '--file', 1, 'f.txt'),
             Argument(None, 'arg')]
    assert parse_argv(TS('-h --file f.txt arg arg2'), options=o) == \
            [Option('-h', None, 0, True),
             Option('-f', '--file', 1, 'f.txt'),
             Argument(None, 'arg'),
             Argument(None, 'arg2')]
    assert parse_argv(TS('-h arg -- -v'), options=o) == \
            [Option('-h', None, 0, True),
             Argument(None, 'arg'),
             Argument(None, '--'),
             Argument(None, '-v')]


def test_parse_pattern():
    o = [Option('-h'), Option('-v', '--verbose'), Option('-f', '--file', 1)]
    assert parse_pattern('[ -h ]', options=o) == \
               Required(Optional(Option('-h')))
    assert parse_pattern('[ ARG ... ]', options=o) == \
               Required(Optional(OneOrMore(Argument('ARG'))))
    assert parse_pattern('[ -h | -v ]', options=o) == \
               Required(Optional(Either(Option('-h'),
                                Option('-v', '--verbose'))))
    assert parse_pattern('( -h | -v [ --file <f> ] )', options=o) == \
               Required(Required(
                   Either(Option('-h'),
                          Required(Option('-v', '--verbose'),
                               Optional(Option('-f', '--file', 1, None))))))
    assert parse_pattern('(-h|-v[--file=<f>]N...)', options=o) == \
               Required(Required(Either(Option('-h'),
                              Required(Option('-v', '--verbose'),
                                  Optional(Option('-f', '--file', 1, None)),
                                     OneOrMore(Argument('N'))))))
    assert parse_pattern('(N [M | (K | L)] | O P)', options=[]) == \
               Required(Required(Either(
                   Required(Argument('N'),
                            Optional(Either(Argument('M'),
                                            Required(Either(Argument('K'),
                                                            Argument('L')))))),
                   Required(Argument('O'), Argument('P')))))
    assert parse_pattern('[ -h ] [N]', options=o) == \
               Required(Optional(Option('-h')),
                        Optional(Argument('N')))
    assert parse_pattern('[options]', options=o) == \
            Required(Optional(AnyOptions()))
    assert parse_pattern('[options] A', options=o) == \
            Required(Optional(AnyOptions()),
                     Argument('A'))
    assert parse_pattern('-v [options]', options=o) == \
            Required(Option('-v', '--verbose'),
                     Optional(AnyOptions()))
    assert parse_pattern('ADD', options=o) == Required(Argument('ADD'))
    assert parse_pattern('<add>', options=o) == Required(Argument('<add>'))
    assert parse_pattern('add', options=o) == Required(Command('add'))


def test_option_match():
    assert Option('-a').match([Option('-a', value=True)]) == \
            (True, [], [Option('-a', value=True)])
    assert Option('-a').match([Option('-x')]) == (False, [Option('-x')], [])
    assert Option('-a').match([Argument('N')]) == (False, [Argument('N')], [])
    assert Option('-a').match([Option('-x'), Option('-a'), Argument('N')]) == \
            (True, [Option('-x'), Argument('N')], [Option('-a')])
    assert Option('-a').match([Option('-a', value=True), Option('-a')]) == \
            (True, [Option('-a')], [Option('-a', value=True)])


def test_argument_match():
    assert Argument('N').match([Argument(None, 9)]) == \
            (True, [], [Argument('N', 9)])
    assert Argument('N').match([Option('-x')]) == (False, [Option('-x')], [])
    assert Argument('N').match([Option('-x'),
                                Option('-a'),
                                Argument(None, 5)]) == \
            (True, [Option('-x'), Option('-a')], [Argument('N', 5)])
    assert Argument('N').match([Argument(None, 9), Argument(None, 0)]) == \
            (True, [Argument(None, 0)], [Argument('N', 9)])


def test_command_match():
    assert Command('c').match([Argument(None, 'c')]) == \
            (True, [], [Command('c', True)])
    assert Command('c').match([Option('-x')]) == (False, [Option('-x')], [])
    assert Command('c').match([Option('-x'),
                               Option('-a'),
                               Argument(None, 'c')]) == \
            (True, [Option('-x'), Option('-a')], [Command('c', True)])
    assert Either(Command('add', False), Command('rm', False)).match(
            [Argument(None, 'rm')]) == (True, [], [Command('rm', True)])


def test_optional_match():
    assert Optional(Option('-a')).match([Option('-a')]) == \
            (True, [], [Option('-a')])
    assert Optional(Option('-a')).match([]) == (True, [], [])
    assert Optional(Option('-a')).match([Option('-x')]) == \
            (True, [Option('-x')], [])
    assert Optional(Option('-a'), Option('-b')).match([Option('-a')]) == \
            (True, [], [Option('-a')])
    assert Optional(Option('-a'), Option('-b')).match([Option('-b')]) == \
            (True, [], [Option('-b')])
    assert Optional(Option('-a'), Option('-b')).match([Option('-x')]) == \
            (True, [Option('-x')], [])
    assert Optional(Argument('N')).match([Argument(None, 9)]) == \
            (True, [], [Argument('N', 9)])
    assert Optional(Option('-a'), Option('-b')).match(
                [Option('-b'), Option('-x'), Option('-a')]) == \
            (True, [Option('-x')], [Option('-a'), Option('-b')])


def test_required_match():
    assert Required(Option('-a')).match([Option('-a')]) == \
            (True, [], [Option('-a')])
    assert Required(Option('-a')).match([]) == (False, [], [])
    assert Required(Option('-a')).match([Option('-x')]) == \
            (False, [Option('-x')], [])
    assert Required(Option('-a'), Option('-b')).match([Option('-a')]) == \
            (False, [Option('-a')], [])


def test_either_match():
    assert Either(Option('-a'), Option('-b')).match(
            [Option('-a')]) == (True, [], [Option('-a')])
    assert Either(Option('-a'), Option('-b')).match(
            [Option('-a'), Option('-b')]) == \
                    (True, [Option('-b')], [Option('-a')])
    assert Either(Option('-a'), Option('-b')).match(
            [Option('-x')]) == (False, [Option('-x')], [])
    assert Either(Option('-a'), Option('-b'), Option('-c')).match(
            [Option('-x'), Option('-b')]) == \
                    (True, [Option('-x')], [Option('-b')])
    assert Either(Argument('M'),
                  Required(Argument('N'), Argument('M'))).match(
                                   [Argument(None, 1), Argument(None, 2)]) == \
            (True, [], [Argument('N', 1), Argument('M', 2)])


def test_one_or_more_match():
    assert OneOrMore(Argument('N')).match([Argument(None, 9)]) == \
            (True, [], [Argument('N', 9)])
    assert OneOrMore(Argument('N')).match([]) == (False, [], [])
    assert OneOrMore(Argument('N')).match([Option('-x')]) == \
            (False, [Option('-x')], [])
    assert OneOrMore(Argument('N')).match(
            [Argument(None, 9), Argument(None, 8)]) == (
                    True, [], [Argument('N', 9), Argument('N', 8)])
    assert OneOrMore(Argument('N')).match(
            [Argument(None, 9), Option('-x'), Argument(None, 8)]) == (
                    True, [Option('-x')], [Argument('N', 9), Argument('N', 8)])
    assert OneOrMore(Option('-a')).match(
            [Option('-a'), Argument(None, 8), Option('-a')]) == \
                    (True, [Argument(None, 8)], [Option('-a'), Option('-a')])
    assert OneOrMore(Option('-a')).match([Argument(None, 8),
                                          Option('-x')]) == \
                    (False, [Argument(None, 8), Option('-x')], [])
    assert OneOrMore(Required(Option('-a'), Argument('N'))).match(
            [Option('-a'), Argument(None, 1), Option('-x'),
             Option('-a'), Argument(None, 2)]) == \
             (True, [Option('-x')],
              [Option('-a'), Argument('N', 1), Option('-a'), Argument('N', 2)])
    assert OneOrMore(Optional(Argument('N'))).match([Argument(None, 9)]) == \
                    (True, [], [Argument('N', 9)])


def test_list_argument_match():
    assert Required(Argument('N'), Argument('N')).fix().match(
            [Argument(None, '1'), Argument(None, '2')]) == \
                    (True, [], [Argument('N', ['1', '2'])])
    assert OneOrMore(Argument('N')).fix().match(
          [Argument(None, '1'), Argument(None, '2'), Argument(None, '3')]) == \
                    (True, [], [Argument('N', ['1', '2', '3'])])
    assert Required(Argument('N'), OneOrMore(Argument('N'))).fix().match(
          [Argument(None, '1'), Argument(None, '2'), Argument(None, '3')]) == \
                    (True, [], [Argument('N', ['1', '2', '3'])])
    assert Required(Argument('N'), Required(Argument('N'))).fix().match(
            [Argument(None, '1'), Argument(None, '2')]) == \
                    (True, [], [Argument('N', ['1', '2'])])


def test_basic_pattern_matching():
    # ( -a N [ -x Z ] )
    pattern = Required(Option('-a'), Argument('N'),
                       Optional(Option('-x'), Argument('Z')))
    # -a N
    assert pattern.match([Option('-a'), Argument(None, 9)]) == \
            (True, [], [Option('-a'), Argument('N', 9)])
    # -a -x N Z
    assert pattern.match([Option('-a'), Option('-x'),
                          Argument(None, 9), Argument(None, 5)]) == \
            (True, [], [Option('-a'), Argument('N', 9),
                        Option('-x'), Argument('Z', 5)])
    # -x N Z  # BZZ!
    assert pattern.match([Option('-x'),
                          Argument(None, 9),
                          Argument(None, 5)]) == \
            (False, [Option('-x'), Argument(None, 9), Argument(None, 5)], [])


def test_pattern_either():
    assert Option('-a').either == Either(Required(Option('-a')))
    assert Argument('A').either == Either(Required(Argument('A')))
    assert Required(Either(Option('-a'), Option('-b')),
                    Option('-c')).either == \
            Either(Required(Option('-a'), Option('-c')),
                   Required(Option('-b'), Option('-c')))
    assert Optional(Option('-a'),
                    Either(Option('-b'),
                    Option('-c'))).either == \
            Either(Required(Option('-b'), Option('-a')),
                   Required(Option('-c'), Option('-a')))
    assert Either(Option('-x'), Either(Option('-y'), Option('-z'))).either == \
            Either(Required(Option('-x')),
                   Required(Option('-y')),
                   Required(Option('-z')))
    assert OneOrMore(Argument('N'), Argument('M')).either == \
            Either(Required(Argument('N'), Argument('M'),
                            Argument('N'), Argument('M')))


def test_pattern_fix_repeating_arguments():
    assert Option('-a').fix_repeating_arguments() == Option('-a')
    assert Argument('N', None).fix_repeating_arguments() == Argument('N', None)
    assert Required(Argument('N'),
                    Argument('N')).fix_repeating_arguments() == \
            Required(Argument('N', []), Argument('N', []))
    assert Either(Argument('N'),
                        OneOrMore(Argument('N'))).fix() == \
            Either(Argument('N', []), OneOrMore(Argument('N', [])))


def test_set():
    assert Argument('N') == Argument('N')
    assert set([Argument('N'), Argument('N')]) == set([Argument('N')])


def test_pattern_fix_identities_1():
    pattern = Required(Argument('N'), Argument('N'))
    assert pattern.children[0] == pattern.children[1]
    assert pattern.children[0] is not pattern.children[1]
    pattern.fix_identities()
    assert pattern.children[0] is pattern.children[1]


def test_pattern_fix_identities_2():
    pattern = Required(Optional(Argument('X'), Argument('N')), Argument('N'))
    assert pattern.children[0].children[1] == pattern.children[1]
    assert pattern.children[0].children[1] is not pattern.children[1]
    pattern.fix_identities()
    assert pattern.children[0].children[1] is pattern.children[1]


def test_long_options_error_handling():
#    with raises(DocoptLanguageError):
#        docopt('Usage: prog --non-existent', '--non-existent')
#    with raises(DocoptLanguageError):
#        docopt('Usage: prog --non-existent')
    with raises(DocoptExit):
        docopt('Usage: prog', '--non-existent')
    with raises(DocoptExit):
        docopt('''Usage: prog [--version --verbose]\n\n
                  --version\n--verbose''', '--ver')
    with raises(DocoptLanguageError):
        docopt('Usage: prog --long\n\n--long ARG')
    with raises(DocoptExit):
        docopt('Usage: prog --long ARG\n\n--long ARG', '--long')
    with raises(DocoptLanguageError):
        docopt('Usage: prog --long=ARG\n\n--long')
    with raises(DocoptExit):
        docopt('Usage: prog --long\n\n--long', '--long=ARG')


def test_short_options_error_handling():
    with raises(DocoptLanguageError):
        docopt('Usage: prog -x\n\n-x  this\n-x  that')

#    with raises(DocoptLanguageError):
#        docopt('Usage: prog -x')
    with raises(DocoptExit):
        docopt('Usage: prog', '-x')

    with raises(DocoptLanguageError):
        docopt('Usage: prog -o\n\n-o ARG')
    with raises(DocoptExit):
        docopt('Usage: prog -o ARG\n\n-o ARG', '-o')


def test_matching_paren():
    with raises(DocoptLanguageError):
        docopt('Usage: prog [a [b]')
    with raises(DocoptLanguageError):
        docopt('Usage: prog [a [b] ] c )')


def test_allow_double_dash():
    assert docopt('usage: prog [-o] [--] <arg>\n\n-o',
                  '-- -o') == {'-o': False, '<arg>': '-o', '--': True}
    assert docopt('usage: prog [-o] [--] <arg>\n\n-o',
                  '-o 1') == {'-o': True, '<arg>': '1', '--': False}
    with raises(DocoptExit):
        docopt('usage: prog [-o] <arg>\n\n-o', '-- -o')  # '--' not allowed


def test_docopt():
    doc = '''Usage: prog [-v] A

    -v  Be verbose.'''
    assert docopt(doc, 'arg') == {'-v': False, 'A': 'arg'}
    assert docopt(doc, '-v arg') == {'-v': True, 'A': 'arg'}

    doc = """Usage: prog [-vqr] [FILE]
              prog INPUT OUTPUT
              prog --help

    Options:
      -v  print status messages
      -q  report only file names
      -r  show all occurrences of the same error
      --help

    """
    a = docopt(doc, '-v file.py')
    assert a == {'-v': True, '-q': False, '-r': False, '--help': False,
                 'FILE': 'file.py', 'INPUT': None, 'OUTPUT': None}

    a = docopt(doc, '-v')
    assert a == {'-v': True, '-q': False, '-r': False, '--help': False,
                 'FILE': None, 'INPUT': None, 'OUTPUT': None}

    with raises(DocoptExit):  # does not match
        docopt(doc, '-v input.py output.py')

    with raises(DocoptExit):
        docopt(doc, '--fake')

    with raises(SystemExit):
        docopt(doc, '--hel')

    #with raises(SystemExit):
    #    docopt(doc, 'help')  XXX Maybe help command?


def test_language_errors():
    with raises(DocoptLanguageError):
        docopt('no usage with colon here')
    with raises(DocoptLanguageError):
        docopt('usage: here \n\n and again usage: here')


def test_issue_40():
    with raises(SystemExit):  # i.e. shows help
        docopt('usage: prog --help-commands | --help', '--help')
    assert docopt('usage: prog --aabb | --aa', '--aa') == {'--aabb': False,
                                                           '--aa': True}


def test_issue34_unicode_strings():
    try:
        assert docopt(eval("u'usage: prog [-o <a>]'"), '') == \
                {'-o': False, '<a>': None}
    except SyntaxError:
        pass  # Python 3


def test_count_multiple_flags():
    assert docopt('usage: prog [-v]', '-v') == {'-v': True}
    assert docopt('usage: prog [-vv]', '') == {'-v': 0}
    assert docopt('usage: prog [-vv]', '-v') == {'-v': 1}
    assert docopt('usage: prog [-vv]', '-vv') == {'-v': 2}
    with raises(DocoptExit):
        docopt('usage: prog [-vv]', '-vvv')
    assert docopt('usage: prog [-v | -vv | -vvv]', '-vvv') == {'-v': 3}
    assert docopt('usage: prog -v...', '-vvvvvv') == {'-v': 6}
    assert docopt('usage: prog [--ver --ver]', '--ver --ver') == {'--ver': 2}


def test_count_multiple_commands():
    assert docopt('usage: prog [go]', 'go') == {'go': True}
    assert docopt('usage: prog [go go]', '') == {'go': 0}
    assert docopt('usage: prog [go go]', 'go') == {'go': 1}
    assert docopt('usage: prog [go go]', 'go go') == {'go': 2}
    with raises(DocoptExit):
        docopt('usage: prog [go go]', 'go go go')
    assert docopt('usage: prog go...', 'go go go go go') == {'go': 5}


def test_any_options_parameter():
    with raises(DocoptExit):
        docopt('usage: prog [options]', '-foo --bar --spam=eggs')
#    assert docopt('usage: prog [options]', '-foo --bar --spam=eggs',
#                  any_options=True) == {'-f': True, '-o': 2,
#                                         '--bar': True, '--spam': 'eggs'}
    with raises(DocoptExit):
        docopt('usage: prog [options]', '--foo --bar --bar')
#    assert docopt('usage: prog [options]', '--foo --bar --bar',
#                  any_options=True) == {'--foo': True, '--bar': 2}
    with raises(DocoptExit):
        docopt('usage: prog [options]', '--bar --bar --bar -ffff')
#    assert docopt('usage: prog [options]', '--bar --bar --bar -ffff',
#                  any_options=True) == {'--bar': 3, '-f': 4}
    with raises(DocoptExit):
        docopt('usage: prog [options]', '--long=arg --long=another')
#    assert docopt('usage: prog [options]', '--long=arg --long=another',
#                  any_options=True) == {'--long': ['arg', 'another']}


#def test_options_shortcut_multiple_commands():
#    # any_options is disabled
#    assert docopt('usage: prog c1 [options] prog c2 [options]',
#        'c2 -o', any_options=True) == {'-o': True, 'c1': False, 'c2': True}
#    assert docopt('usage: prog c1 [options] prog c2 [options]',
#        'c1 -o', any_options=True) == {'-o': True, 'c1': True, 'c2': False}


def test_options_shortcut_does_not_add_options_to_patter_second_time():
    assert docopt('usage: prog [options] [-a]\n\n-a -b', '-a') == \
            {'-a': True, '-b': False}
    with raises(DocoptExit):
        docopt('usage: prog [options] [-a]\n\n-a -b', '-aa')


def test_default_value_for_positional_arguments():
    # disabled right now
    assert docopt('usage: prog [<p>]\n\n<p>  [default: x]', '') == \
            {'<p>': None}
    #       {'<p>': 'x'}
    assert docopt('usage: prog [<p>]...\n\n<p>  [default: x y]', '') == \
            {'<p>': []}
    #       {'<p>': ['x', 'y']}
    assert docopt('usage: prog [<p>]...\n\n<p>  [default: x y]', 'this') == \
            {'<p>': ['this']}
    #       {'<p>': ['this']}


#def test_parse_defaults():
#    assert parse_defaults("""usage: prog
#
#                          -o, --option <o>
#                          --another <a>  description
#                                         [default: x]
#                          <a>
#                          <another>  description [default: y]""") == \
#           ([Option('-o', '--option', 1, None),
#             Option(None, '--another', 1, 'x')],
#            [Argument('<a>', None),
#             Argument('<another>', 'y')])
#
#    doc = '''
#    -h, --help  Print help message.
#    -o FILE     Output file.
#    --verbose   Verbose mode.'''
#    assert parse_defaults(doc)[0] == [Option('-h', '--help'),
#                                      Option('-o', None, 1),
#                                      Option(None, '--verbose')]


def test_issue_59():
    assert docopt('usage: prog --long=<a>', '--long=') == {'--long': ''}
    assert docopt('usage: prog -l <a>\n\n-l <a>', ['-l', '']) == {'-l': ''}


def test_options_first():
    assert docopt('usage: prog [--opt] [<args>...]',
                  '--opt this that') == {'--opt': True,
                                         '<args>': ['this', 'that']}
    assert docopt('usage: prog [--opt] [<args>...]',
                  'this that --opt') == {'--opt': True,
                                         '<args>': ['this', 'that']}
    assert docopt('usage: prog [--opt] [<args>...]',
                  'this that --opt',
                  options_first=True) == {'--opt': False,
                                          '<args>': ['this', 'that', '--opt']}


def test_issue_68_options_shortcut_does_not_include_options_in_usage_patter():
    args = docopt('usage: prog [-ab] [options]\n\n-x\n-y', '-ax')
    # Need to use `is` (not `==`) since we want to make sure
    # that they are not 1/0, but strictly True/False:
    assert args['-a'] is True
    assert args['-b'] is False
    assert args['-x'] is True
    assert args['-y'] is False


def test_issue_65_evaluate_argv_when_called_not_when_imported():
    import sys
    sys.argv = 'prog -a'.split()
    assert docopt('usage: prog [-ab]') == {'-a': True, '-b': False}
    sys.argv = 'prog -b'.split()
    assert docopt('usage: prog [-ab]') == {'-a': False, '-b': True}


def test_issue_85_any_option_multiple_subcommands():
    docopt('usage:\n  fs good [options]\n  fs fail [options]\n\nOptions:\n  --loglevel=<loglevel>\n',
                  'fail --loglevel 5') ==  {'--loglevel': '5',
                                            'fail': True,
                                            'good': False}

########NEW FILE########
__FILENAME__ = test_events
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import pyuv

from gaffer.events import EventEmitter


def test_basic():
    events = set()
    emitted = []
    loop = pyuv.Loop.default_loop()

    def cb(ev):
        emitted.append(True)

    emitter = EventEmitter(loop)
    emitter.subscribe("test", cb)
    events = emitter._events.copy()
    emitter.publish("test")
    loop.run()

    assert "test" in events
    assert events["test"] == set([(False, cb)])
    assert emitted == [True]
    assert "test" in emitter._events
    assert emitter._events["test"] == set([(False, cb)])


def test_publish_value():
    emitted = []
    loop = pyuv.Loop.default_loop()
    def cb(ev, val):
        emitted.append(val)

    emitter = EventEmitter(loop)
    emitter.subscribe("test", cb)
    emitter.publish("test", 1)
    emitter.publish("test", 2)
    loop.run()

    assert emitted == [1, 2]

def test_publish_once():
    emitted = []
    loop = pyuv.Loop.default_loop()
    def cb(ev, val):
        emitted.append(val)

    emitter = EventEmitter(loop)
    emitter.subscribe_once("test", cb)
    emitter.publish("test", 1)
    loop.run()

    assert emitted == [1]
    assert emitter._events["test"] == set()


def test_multiple_listener():
    emitted = []

    loop = pyuv.Loop.default_loop()
    def cb1(ev, val):
        emitted.append((1, val))

    def cb2(ev, val):
        emitted.append((2, val))

    emitter = EventEmitter(loop)
    emitter.subscribe("test", cb1)
    emitter.subscribe("test", cb2)
    emitter.publish("test", 1)
    loop.run()

    assert (1, 1) in emitted
    assert (2, 1) in emitted


def test_multipart():
    emitted = []
    emitted2 = []
    loop = pyuv.Loop.default_loop()

    def cb1(ev, val):
        emitted.append(val)

    def cb2(ev, val):
        emitted2.append(val)

    emitter = EventEmitter(loop)
    emitter.subscribe("a.b", cb1)
    emitter.subscribe("a", cb2)
    emitter.publish("a.b", 1)
    emitter.publish("a", 2)
    loop.run()

    assert emitted == [1]
    assert 1 in emitted2
    assert 2 in emitted2


def test_multipart2():
    emitted = []
    loop = pyuv.Loop.default_loop()

    def cb(ev, val):
        emitted.append(ev)

    emitter = EventEmitter(loop)
    emitter.subscribe("a.b", cb)
    emitter.publish("a.b.c", 2)
    loop.run()

    assert emitted == ['a.b.c']

def test_wildcard():
    loop = pyuv.Loop.default_loop()
    emitted = []
    emitted2 = []
    emitted3 = []

    def cb(ev, val):
        emitted.append(val)

    def cb2(ev, val):
        emitted2.append(val)

    def cb3(ev, val):
        emitted3.append(val)


    emitter = EventEmitter(loop)
    emitter.subscribe(".", cb)
    emitter.subscribe("a.b", cb2)
    emitter.subscribe("a.b.", cb3)

    assert emitter._wildcards == set([(False, cb)])

    emitter.publish("a.b", 1)
    loop.run()

    assert emitted == [1]
    assert emitted2 == [1]
    assert emitted3 == [1]

def test_unsubscribe():
    emitted = []
    loop = pyuv.Loop.default_loop()

    def cb(ev, v):
        emitted.append(v)

    emitter = EventEmitter(loop)
    emitter.subscribe("test", cb)
    emitter.publish("test", "a")

    def unsubscribe(handle):
        emitter.unsubscribe("test", cb)
        emitter.publish("test", "b")

    t = pyuv.Timer(loop)
    t.start(unsubscribe, 0.2, 0.0)
    loop.run()

    assert emitted == ["a"]

########NEW FILE########
__FILENAME__ = test_http
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os
import time

import pytest
import pyuv

from gaffer import __version__
from gaffer.manager import Manager
from gaffer.gafferd.http import HttpHandler
from gaffer.httpclient import (Server, Job, Process,
        GafferNotFound, GafferConflict)
from gaffer.process import ProcessConfig

from test_manager import dummy_cmd

TEST_HOST = '127.0.0.1'
TEST_PORT = (os.getpid() % 31000) + 1024

TEST_URI = "%s:%s" % (TEST_HOST, TEST_PORT)

class MockConfig(object):
    def __init__(self, **kwargs):
        self.set_defaults()

        for attrname, attrval in kwargs.items():
            setattr(self, attrname, attrval)

    def set_defaults(self):
        self.webhooks = []
        self.processes = []
        self.ssl_options = None
        self.client_ssl_options = {}
        self.bind = "0.0.0.0:5000"
        self.lookupd_addresses = []
        self.broadcast_address = None
        self.backlog = 128
        self.daemonize = False
        self.pidfile = None
        self.logfile = None
        self.loglevel = "info"

        # auth(z) API
        self.require_key = False
        self.auth_backend = "default"
        self.keys_backend = "default"
        self.auth_dbname = None
        self.keys_dbname = None

def start_manager():
    http_handler = HttpHandler(MockConfig(bind=TEST_URI))
    m = Manager()
    m.start(apps=[http_handler])
    time.sleep(0.2)
    return m

def get_server(loop):
    return Server("http://%s:%s" % (TEST_HOST, TEST_PORT), loop=loop)

def init():
    m = start_manager()
    s = get_server(m.loop)
    return (m, s)

def test_basic():
    m = start_manager()
    s = get_server(m.loop)
    assert s.version == __version__

    m.stop()
    m.run()

def test_simple_job():
    m, s = init()

    assert s.jobs() == []

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config, start=False)
    time.sleep(0.2)
    assert len(m.jobs()) == 1
    assert len(s.jobs()) == 1
    assert s.jobs()[0] == "default.dummy"

    m.stop()
    m.run()

def test_job_create():
    m, s = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)

    s.load(config, start=False)
    time.sleep(0.2)
    assert len(m.jobs()) == 1
    assert len(s.jobs()) == 1
    assert s.jobs()[0] == "default.dummy"
    assert "default.dummy" in m.jobs()
    assert len(m.running) == 0

    with pytest.raises(GafferConflict):
        s.load(config, start=False)

    job = s.get_job("dummy")
    assert isinstance(job, Job)

    m.stop()
    m.run()

def test_remove_job():
    m, s = init()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    s.load(config, start=False)
    assert s.jobs()[0] == "default.dummy"
    s.unload("dummy")
    assert len(s.jobs()) == 0
    assert len(m.jobs()) == 0
    assert len(m.sessions) == 0
    m.stop()
    m.run()

def test_notfound():
    m, s = init()

    with pytest.raises(GafferNotFound):
        s.get_job("dummy")

    m.stop()
    m.run()

def test_process_start_stop():
    m, s = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    job = s.load(config, start=False)
    assert isinstance(job, Job)

    job.start()
    time.sleep(0.2)

    assert len(m.running) == 1
    info = job.info()
    assert info['running'] == 1
    assert info['active'] == True
    assert info['max_processes'] == 1

    job.stop()
    time.sleep(0.2)
    assert len(m.running) == 0
    assert job.active == False

    s.unload("dummy")
    assert len(s.jobs()) == 0

    job = s.load(config, start=True)
    time.sleep(0.2)
    assert len(m.running) == 1
    assert job.active == True

    job.restart()
    time.sleep(0.4)
    assert len(m.running) == 1
    assert job.active == True

    m.stop()
    m.run()

def test_job_scale():
    m, s = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    job = s.load(config)
    time.sleep(0.2)
    assert isinstance(job, Job)
    assert job.active == True
    assert job.numprocesses == 1


    job.scale(3)
    time.sleep(0.2)
    assert job.numprocesses == 4
    assert job.running == 4

    job.scale(-3)
    time.sleep(0.2)
    assert job.numprocesses == 1
    assert job.running == 1

    m.stop()
    m.run()

def test_running():
    m, s = init()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    job = s.load(config)
    time.sleep(0.2)

    assert len(m.running) == 1
    assert len(s.running()) == 1

    assert 1 in m.running
    assert s.running()[0] == 1

    m.stop()
    m.run()


def test_pids():
    m, s = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    s.load(config)
    time.sleep(0.2)

    job = s.get_job("dummy")
    assert isinstance(job, Job) == True

    pid = s.get_process(1)
    assert isinstance(pid, Process) == True
    assert pid.pid == 1
    assert pid.info.get('name') == "default.dummy"
    assert pid.name == "default.dummy"
    assert pid.os_pid == pid.info.get('os_pid')
    assert job.pids == [1]

    pid.stop()
    assert 1 not in m.running

    time.sleep(0.2)
    assert job.pids == [2]
    m.stop()
    m.run()


def test_stats():
    m, s = init()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    s.load(config)
    time.sleep(0.2)

    pid = s.get_process(1)
    assert isinstance(pid, Process) == True
    assert pid.pid == 1

    stats = pid.stats
    assert isinstance(stats, dict) == True
    assert "cpu" in stats
    assert "mem_info1" in stats

    pid.stop()
    m.stop()
    m.run()

def test_sessions():
    m, s = init()
    started = []
    stopped = []
    def cb(evtype, info):
        if evtype == "start":
            started.append(info['name'])
        elif evtype == "stop":
            stopped.append(info['name'])

    m.events.subscribe('start', cb)
    m.events.subscribe('stop', cb)
    testfile, cmd, args, wdir = dummy_cmd()
    a = ProcessConfig("a", cmd, args=args, cwd=wdir)
    b = ProcessConfig("b", cmd, args=args, cwd=wdir)


    # load process config in different sessions
    m.load(a, sessionid="ga", start=False)
    m.load(b, sessionid="ga", start=False)
    m.load(a, sessionid="gb", start=False)

    sessions = s.sessions()

    ga1 = s.jobs('ga')
    gb1 = s.jobs('gb')

    start_app = lambda mgr, job: job.start()
    stop_app = lambda mgr, job: job.stop()

    s.jobs_walk(start_app, "ga")
    s.jobs_walk(start_app, "gb")

    ga2 = []
    def rem_cb(h):
        s.unload("a", sessionid="ga")
        [ga2.append(name) for name in s.jobs('ga')]

    t0 = pyuv.Timer(m.loop)
    t0.start(rem_cb, 0.2, 0.0)
    s.jobs_walk(stop_app, "gb")

    def stop(handle):
        m.events.unsubscribe("start", cb)
        m.events.unsubscribe("stop", cb)
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.6, 0.0)
    m.run()

    assert len(sessions) == 2
    assert sessions == ['ga', 'gb']
    assert ga1 == ['ga.a', 'ga.b']
    assert gb1 == ['gb.a']
    assert started == ['ga.a', 'ga.b', 'gb.a']
    assert stopped == ['gb.a', 'ga.a']
    assert ga2 == ['ga.b']

def test_job_notfound():
    m, s = init()

    with pytest.raises(GafferNotFound):
        s.jobs("unknown_sessionid")

    m.stop()
    m.run()


def test_job_commit():
    m, s = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=0)
    job = s.load(config, start=False)
    time.sleep(0.2)
    assert isinstance(job, Job)
    assert job.active == False
    assert job.numprocesses == 0
    assert job.running == 0
    assert job.running_out == 0


    pid = job.commit()

    assert pid == 1
    assert s.pids() == [1]
    assert job.active == True
    assert job.numprocesses == 0
    assert job.running == 1
    assert job.running_out == 1


    state = m._get_locked_state("dummy")
    assert len(state.running) == 0
    assert state.numprocesses == 0
    assert len(state.running_out) == 1
    assert m.pids() == [1]


    job.scale(1)
    assert s.pids() == [1, 2]
    assert job.active == True
    assert job.numprocesses == 1
    assert job.running == 2
    assert job.running_out == 1

    m.stop()
    m.run()

if __name__ == "__main__":
    test_simple_job()

########NEW FILE########
__FILENAME__ = test_keys
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import pyuv
import pytest

from gaffer.gafferd.keys import (KeyNotFound, KeyConflict, InvalidKey,
        UnknownPermission, Key, DummyKey, KeyManager, SqliteKeyBackend)

from test_http import MockConfig

def test_config():
    return MockConfig(auth_dbname=":memory:", keys_dbname=":memory:")

def test_sqlite_backend():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with SqliteKeyBackend(loop, conf) as h:
        h.set_key("test", {"permission": {}})

        with pytest.raises(KeyConflict):
            h.set_key("test", {"permission": {}})

        assert h.has_key("test") == True

        key = h.get_key("test")
        assert key == {"key": "test", "permission": {}}

        key = h.delete_key("test")
        with pytest.raises(KeyNotFound):
            key = h.get_key("test")

        h.set_key("test", {"permission": {}})
        h.set_key("test1", {"permission": {}}, "test")

        assert h.has_key("test1") == True
        assert h.all_subkeys("test") == [{"key": "test1", "permission": {}}]
        assert len(h.all_keys()) == 2
        assert h.all_keys() == ["test", "test1"]
        assert h.all_keys(include_key=True) == [{"key": "test",
            "permission": {}}, {"key": "test1", "permission": {}}]

def test_key_manager():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.set_key("test", {"permission": {}})

        with pytest.raises(KeyConflict):
            h.set_key("test", {"permission": {}})

        assert h.has_key("test") == True

        key = h.get_key("test")
        assert key == {"key": "test", "permission": {}}
        assert len(h._cache) == 1
        assert "test" in h._cache
        assert len(h._entries) == 1
        assert list(h._entries) == ["test"]


        key = h.delete_key("test")
        with pytest.raises(KeyNotFound):
            key = h.get_key("test")

        h.set_key("test", {"permission": {}})
        h.set_key("test1", {"permission": {}}, "test")

        assert h.has_key("test1") == True
        assert h.all_subkeys("test") == [{"key": "test1", "permission": {}}]
        assert len(h.all_keys()) == 2
        assert h.all_keys() == ["test", "test1"]
        assert h.all_keys(include_key=True) == [{"key": "test",
            "permission": {}}, {"key": "test1", "permission": {}}]

        h.get_key("test")
        h.get_key("test1")
        assert len(h._cache) == 2
        assert "test" in h._cache
        assert "test1" in h._cache
        assert len(h._entries) == 2
        assert list(h._entries) == ["test", "test1"]

        # make sure keys are deleted from the cache
        h.delete_key("test")
        assert len(h._cache) == 0
        assert len(h._entries) == 0

        with pytest.raises(KeyNotFound):
            key = h.get_key("test")

        with pytest.raises(KeyNotFound):
            key = h.get_key("test1")


def test_create_key():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"admin": True}, label="some key", key="test")
        key = h.get_key("test")

        assert key == {"key": "test", "label": "some key",
                "permissions": {"admin": True}}


def test_admin_permissions():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"admin": True}, key="test", label="admin")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.label == "admin"
        assert key.permissions == {"admin": True}
        assert key.is_admin() == True
        assert key.can_create_key() == True
        assert key.can_create_user() == True
        assert key.can_manage_all() == True
        assert key.can_read_all() == True
        assert key.can_write_all() == True
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == True
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == True
        assert key.can_manage("test1.test") == True

def test_manage_all_permission():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["*"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.permissions == {"manage": ["*"]}
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == True
        assert key.can_read_all() == True
        assert key.can_write_all() == True
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == True
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == True
        assert key.can_manage("test1.test") == True

def test_read_all_permission():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["*"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.permissions == {"read": ["*"]}
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == True
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_write_all_permission():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["*"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.permissions == {"write": ["*"]}
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == True
        assert key.can_write_all() == True
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False


def test_manage_session():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["test"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == True
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_manage_session2():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["test", "test1"]}, key="test1")
        key = Key.load(h.get_key("test1"))

        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == True
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == True
        assert key.can_manage("test1.test") == True


def test_manage_job():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["test.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_manage_job2():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["test.test", "test1"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == True
        assert key.can_manage("test1.test") == True

def test_manage_job3():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"manage": ["test.test", "test1.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == True
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == True



def test_read_session():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_read_session1():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test", "test1"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_read_job():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_read_job2():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test.test", "test1"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_read_job3():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test.test", "test1.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_write_session():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["test"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_write_session1():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["test", "test1"]}, key="test")
        key = Key.load(h.get_key("test"))
        assert key.api_key == "test"
        assert key.is_admin() == False
        assert key.can_create_key() == False
        assert key.can_create_user() == False
        assert key.can_manage_all() == False
        assert key.can_read_all() == False
        assert key.can_write_all() == False
        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == True
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False


def test_write_job():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["test.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == False
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == False
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_write_job2():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["test.test", "test1"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == True
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == True
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False

def test_write_job3():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"write": ["test.test", "test1.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))
        assert key.can_read("test") == False
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == True
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False


def test_mix():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with KeyManager(loop, conf) as h:
        h.create_key({"read": ["test"], "write": ["test1.test"]}, key="test1")
        key = Key.load(h.get_key("test1"))

        assert key.can_read("test") == True
        assert key.can_read("test.test") == True
        assert key.can_read("test1") == False
        assert key.can_read("test1.test") == True
        assert key.can_write("test") == False
        assert key.can_write("test.test") == False
        assert key.can_write("test1") == False
        assert key.can_write("test1.test") == True
        assert key.can_manage("test") == False
        assert key.can_manage("test.test") == False
        assert key.can_manage("test1") == False
        assert key.can_manage("test1.test") == False


########NEW FILE########
__FILENAME__ = test_lookup
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import copy
import os
import time

import pytest
import pyuv

# patch tornado IOLoop
from gaffer.tornado_pyuv import IOLoop, install
install()


from gaffer.gafferd.http import HttpHandler
from gaffer.gafferd.lookup import LookupClient
from gaffer.httpclient import GafferNotFound
from gaffer.lookupd.client import LookupServer
from gaffer.lookupd.http import http_server
from gaffer.lookupd.registry import (RemoteJob, GafferNode, Registry,
        NoIdent, JobNotFound, AlreadyIdentified, IdentExists,
        AlreadyRegistered)
from gaffer.manager import Manager
from gaffer.process import ProcessConfig
from gaffer.util import bind_sockets, hostname

from test_manager import dummy_cmd
from test_http import MockConfig

TEST_GAFFERD_HOST = '127.0.0.1'
TEST_GAFFERD_PORT = (os.getpid() % 31000) + 1024
GAFFERD_ADDR = "%s:%s" % (TEST_GAFFERD_HOST, TEST_GAFFERD_PORT)

TEST_LOOKUPD_HOST = '127.0.0.1'
TEST_LOOKUPD_PORT = (os.getpid() % 31000) + 1023
LOOKUPD_ADDR = "%s:%s" % (TEST_LOOKUPD_HOST, TEST_LOOKUPD_PORT)

def test_registry_basic():
    r = Registry()
    # fake connections
    c1 = object()
    c2 = object()
    r.add_node(c1)
    r.add_node(c2)

    assert c1 in r.nodes
    assert c2 in r.nodes

    assert isinstance(r.nodes[c1], GafferNode)
    assert isinstance(r.nodes[c2], GafferNode)
    assert r.nodes[c1] is not r.nodes[c2]

    r.remove_node(c2)
    assert c2 not in r.nodes

def test_registry_identify():
    r = Registry()
    c1 = object()
    c2 = object()
    c3 = object()
    r.add_node(c1)
    r.add_node(c2)
    r.add_node(c3)

    with pytest.raises(NoIdent):
        r.get_node(c1)

    updated = r.nodes[c1].updated
    r.identify(c1, "c1", "broadcast", 1.0)

    n1 = r.get_node(c1)
    assert updated != n1.updated

    assert n1.name == "c1"
    assert n1.origin == "broadcast"
    assert n1.version == 1.0

    with pytest.raises(AlreadyIdentified):
        r.identify(c1, "c1", "broadcast", 1.0)

    with pytest.raises(IdentExists):
        r.identify(c2, "c1", "broadcast", 1.0)

    r.identify(c2, "c1", "broadcast:8001", 1.0)
    n2 = r.get_node(c2)

    assert n2.name == "c1"
    assert n2.origin == "broadcast:8001"

    r.identify(c3, "c3", "broadcast2", 1.0)
    n3 = r.get_node(c3)

    assert n3.name == "c3"
    assert n3.origin == "broadcast2"

def test_registry_add_job():
    r = Registry()
    c1 = object()
    c2 = object()
    c3 = object()
    c4 = object()
    r.add_node(c1)
    r.identify(c1, "c1", "broadcast", 1.0)
    r.add_node(c2)
    r.identify(c2, "c2", "broadcast2", 1.0)
    r.add_node(c3)
    r.identify(c3, "c3", "broadcast3", 1.0)
    r.add_node(c4)
    r.identify(c4, "c4", "broadcast4", 1.0)

    n1 = r.get_node(c1)
    n2 = r.get_node(c2)
    n3 = r.get_node(c3)
    n4 = r.get_node(c4)

    assert r.sessions() == {}
    assert len(r.jobs()) == 0

    r.add_job(c1, 'a.job1')
    sessions = r.sessions()
    assert 'a' in sessions
    assert 'a.job1' in sessions['a']
    assert len(sessions['a']['a.job1']) == 1
    job = sessions['a']['a.job1'][0]
    assert isinstance(job, RemoteJob)
    assert job.name == 'a.job1'
    assert job.node is n1
    assert len(r.jobs()) == 1
    assert job == r.find_job('a.job1')[0]

    jobs = r.jobs()
    assert list(jobs) == ['a.job1']
    assert jobs['a.job1'][0] == job
    assert len(n1.sessions) == 1
    session = r.find_session('a')
    assert session[0].name == "a.job1"

    with pytest.raises(AlreadyRegistered):
        r.add_job(c1, 'a.job1')

    r.add_job(c2, 'a.job1')
    sessions = r.sessions()
    assert len(sessions['a']['a.job1']) == 2
    job = sessions['a']['a.job1'][1]
    assert isinstance(job, RemoteJob)
    assert job.name == 'a.job1'
    assert job.node is n2
    assert len(r.jobs()) == 1
    jobs = r.jobs()
    assert list(jobs) == ['a.job1']
    assert jobs['a.job1'][1] == job

    r.add_job(c3, 'a.job2')
    sessions = r.sessions()
    assert len(sessions['a']['a.job1']) == 2
    assert len(sessions['a']['a.job2']) == 1
    job = sessions['a']['a.job2'][0]
    assert isinstance(job, RemoteJob)
    assert job.name == 'a.job2'
    assert job.node is n3
    assert len(r.jobs()) == 2
    jobs = r.jobs()
    assert list(jobs) == ['a.job1', 'a.job2']
    assert jobs['a.job2'][0] == job

    r.add_job(c4, 'b.job1')
    sessions = r.sessions()
    assert len(sessions) == 2
    assert len(sessions['b']['b.job1']) == 1
    job = sessions['b']['b.job1'][0]
    assert isinstance(job, RemoteJob)
    assert job.name == 'b.job1'
    assert job.node is n4
    assert len(r.jobs()) == 3
    jobs = r.jobs()
    assert list(jobs) == ['a.job1', 'a.job2', 'b.job1']

    assert jobs['a.job1'][0].node == n1
    assert jobs['a.job1'][1].node == n2
    assert jobs['a.job2'][0].node == n3
    assert jobs['b.job1'][0].node == n4
    return r, c1, c2, c3, c3, n1, n2, n3, n4

def test_registry_remove_job():
    r, c1, c2, c3, c3, n1, n2, n3, n4 = test_registry_add_job()

    # remove a job
    assert len(r.sessions()['a']['a.job1']) == 2
    assert len(n2.sessions['a']) == 1
    r.remove_job(c2, 'a.job1')
    assert 'a' not in n2.sessions
    assert len(r.sessions()['a']['a.job1']) == 1
    assert list(r.jobs()) == ['a.job1', 'a.job2', 'b.job1']


    r.remove_job(c1, 'a.job1')
    assert 'a.job1' not in r.sessions()['a']
    assert list(r.jobs()) == ['a.job2', 'b.job1']

    with pytest.raises(JobNotFound):
        r.find_job('a.job1')

def test_registry_add_process():
    r = Registry()
    c1 = object()
    r.add_node(c1)

    with pytest.raises(NoIdent):
        r.add_process(c1, "a.job1", 1)

    r.identify(c1, "c1", "broadcast", 1.0)

    with pytest.raises(JobNotFound):
        r.add_process(c1, "a.job1", 1)

    r.add_job(c1, "a.job1")
    r.add_process(c1, "a.job1", 1)
    job = r.find_job("a.job1")[0]
    assert job.pids == [1]
    assert job.node == r.get_node(c1)

    c2 = object()
    r.add_node(c2)
    r.identify(c2, "c2", "broadcast2", 1.0)
    r.add_job(c2, "a.job1")
    r.add_process(c2, "a.job1", 1)

    jobs = r.find_job("a.job1")
    assert len(jobs) == 2

    job2 = r.find_job("a.job1")[1]
    assert job2.pids == [1]
    assert job2.node == r.get_node(c2)
    assert job2 != job

    r.add_process(c1, "a.job1", 2)
    job = r.find_job("a.job1")[0]
    assert job.pids == [1, 2]
    jobs = r.find_job("a.job1")
    assert len(jobs) == 2

    return r, c1, c2

def test_registry_remove_process():
    r, c1, c2 = test_registry_add_process()

    r.remove_process(c1, "a.job1", 1)
    job = r.find_job("a.job1")[0]
    assert job.pids == [2]

    # just to confirm we don't raise anything
    r.remove_process(c1, "a.job1", 1)

    r.remove_process(c1, "a.job1", 2)
    assert job.pids == []

    assert len(r.find_job("a.job1")) == 2
    job2 = r.find_job("a.job1")[1]
    assert job2.pids == [1]


def test_registry_events():
    loop = pyuv.Loop.default_loop()
    async = pyuv.Async(loop, lambda h: h.stop())

    r = Registry(loop)
    emitted = []
    def cb(event, message):
        emitted.append((event, copy.deepcopy(message)))

    r.bind_all(cb)
    c1 = object()
    r.add_node(c1)

    r.identify(c1, "c1", "broadcast", 1.0)
    r.update(c1)
    r.add_job(c1, "a.job1")
    r.add_process(c1, "a.job1", 1)
    r.remove_process(c1, "a.job1", 1)
    r.remove_job(c1, "a.job1")
    r.remove_node(c1)

    t = pyuv.Timer(loop)
    t.start(lambda h: async.close(), 0.2, 0.0)
    loop.run()

    assert len(emitted) == 7
    actions = [line[0] for line in emitted]
    assert list(actions) == ['add_node', 'identify', 'add_job', 'add_process',
            'remove_process', 'remove_job', 'remove_node']

    assert isinstance(emitted[0][1], GafferNode)
    assert isinstance(emitted[1][1], GafferNode)
    assert isinstance(emitted[2][1], dict)
    assert "job_name" in emitted[2][1]
    assert emitted[2][1]['job_name'] == "a.job1"
    assert isinstance(emitted[3][1], dict)
    assert "job_name" in emitted[3][1]
    assert emitted[3][1]['job_name'] == "a.job1"
    assert "pid" in emitted[3][1]
    assert emitted[3][1]['pid'] == 1
    assert isinstance(emitted[4][1], dict)
    assert "job_name" in emitted[4][1]
    assert emitted[4][1]['job_name'] == "a.job1"
    assert "pid" in emitted[4][1]
    assert emitted[4][1]['pid'] == 1
    assert isinstance(emitted[5][1], dict)
    assert emitted[5][1]['job_name'] == "a.job1"
    assert isinstance(emitted[6][1], GafferNode)
    assert emitted[6][1].sessions == {}

def test_lookup_service():
    loop = pyuv.Loop.default_loop()
    r = Registry(loop)
    sock = bind_sockets(LOOKUPD_ADDR)
    io_loop = IOLoop(_loop=loop)
    server = http_server(io_loop, sock, registration_db=r)
    server.start()

    emitted = []
    def cb(event, message):
        emitted.append((event, message))

    r.bind_all(cb)

    client = LookupClient(loop, "ws://%s/ws" % LOOKUPD_ADDR)
    client.start()

    messages = []
    messages.append(client.identify("c1", "broadcast", 1.0))
    messages.append(client.ping())
    messages.append(client.add_job("a.job1"))
    messages.append(client.add_process("a.job1", 1))
    messages.append(client.remove_process("a.job1", 1))
    messages.append(client.remove_job("a.job1"))

    t0 = pyuv.Timer(loop)
    t0.start(lambda h: client.close(), 0.4, 0.0)

    def stop(h):
        h.close()
        server.stop()
        io_loop.close()

    t = pyuv.Timer(loop)
    t.start(stop, 0.6, 0.0)
    loop.run()

    assert len(messages) == 6
    results = ["ok" in msg.result() for msg in messages]
    assert results == [True, True, True, True, True, True]

    assert len(emitted) == 7
    actions = [line[0] for line in emitted]
    assert list(actions) == ['add_node', 'identify', 'add_job', 'add_process',
            'remove_process', 'remove_job', 'remove_node']

    assert isinstance(emitted[0][1], GafferNode)
    assert isinstance(emitted[1][1], GafferNode)
    assert isinstance(emitted[2][1], dict)
    assert "job_name" in emitted[2][1]
    assert emitted[2][1]['job_name'] == "a.job1"
    assert isinstance(emitted[3][1], dict)
    assert "job_name" in emitted[3][1]
    assert emitted[3][1]['job_name'] == "a.job1"
    assert "pid" in emitted[3][1]
    assert emitted[3][1]['pid'] == 1
    assert isinstance(emitted[4][1], dict)
    assert "job_name" in emitted[4][1]
    assert emitted[4][1]['job_name'] == "a.job1"
    assert "pid" in emitted[4][1]
    assert emitted[4][1]['pid'] == 1
    assert isinstance(emitted[5][1], dict)
    assert emitted[5][1]['job_name'] == "a.job1"
    assert isinstance(emitted[6][1], GafferNode)
    assert emitted[6][1].sessions == {}

def test_lookup_manager():

    # intiallize the lookupd server
    loop = pyuv.Loop.default_loop()
    r = Registry(loop)
    sock = bind_sockets(LOOKUPD_ADDR)
    io_loop = IOLoop(_loop=loop)
    server = http_server(io_loop, sock, registration_db=r)
    server.start()

    # subscribe to events
    emitted = []
    def cb(event, message):
        emitted.append((event, message))
    r.bind_all(cb)


    # start the manager with the HTTP API
    http_handler = HttpHandler(MockConfig(bind=GAFFERD_ADDR,
            lookupd_addresses=["http://%s" % LOOKUPD_ADDR]))
    m = Manager(loop=loop)
    m.start(apps=[http_handler])

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    m.stop_process(1)
    m.unload("dummy")

    t = pyuv.Timer(loop)

    def do_stop(h):
        server.stop()
        io_loop.close(True)

    def stop_server(m):
        t.start(do_stop, 0.4, 0.0)

    m.stop(stop_server)
    loop.run()

    assert len(emitted) == 7
    actions = [line[0] for line in emitted]
    assert list(actions) == ['add_node', 'identify', 'add_job', 'add_process',
            'remove_process', 'remove_job', 'remove_node']


    assert isinstance(emitted[0][1], GafferNode)
    assert isinstance(emitted[1][1], GafferNode)
    assert isinstance(emitted[2][1], dict)
    assert "job_name" in emitted[2][1]
    assert emitted[2][1]['job_name'] == "default.dummy"
    assert isinstance(emitted[3][1], dict)
    assert "job_name" in emitted[3][1]
    assert emitted[3][1]['job_name'] == "default.dummy"
    assert "pid" in emitted[3][1]
    assert emitted[3][1]['pid'] == 1
    assert isinstance(emitted[4][1], dict)
    assert "job_name" in emitted[4][1]
    assert emitted[4][1]['job_name'] == "default.dummy"
    assert "pid" in emitted[4][1]
    assert emitted[4][1]['pid'] == 1
    assert isinstance(emitted[5][1], dict)
    assert emitted[5][1]['job_name'] == "default.dummy"
    assert isinstance(emitted[6][1], GafferNode)
    assert emitted[6][1].sessions == {}

def test_lookup_client():
    # intiallize the lookupd server
    loop = pyuv.Loop.default_loop()
    r = Registry(loop)
    sock = bind_sockets(LOOKUPD_ADDR)
    io_loop = IOLoop(_loop=loop)
    server = http_server(io_loop, sock, registration_db=r)
    server.start()

    lookup_address = "http://%s" % LOOKUPD_ADDR
    client = LookupServer(lookup_address, loop)


    # start the manager with the HTTP API
    http_handler = HttpHandler(MockConfig(bind=GAFFERD_ADDR,
            lookupd_addresses=[lookup_address]))
    m = Manager(loop=loop)
    m.start(apps=[http_handler])
    time.sleep(0.1)
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)

    t = pyuv.Timer(loop)

    def do_stop(h):
        server.stop()
        io_loop.close(True)

    def stop_server(m):
        t.start(do_stop, 0.1, 0.0)


    results = []
    def collect_info2(h):
        results.append(client.sessions())
        results.append(client.jobs())
        m.stop(stop_server)

    def collect_info(h):
        results.append(client.nodes())
        results.append(client.sessions())
        results.append(client.jobs())
        results.append(client.find_job("default.dummy"))
        m.unload("dummy")
        h.start(collect_info2, 0.3, 0.0)

    t0 = pyuv.Timer(loop)
    t0.start(collect_info, 0.3, 0.0)

    loop.run()
    assert len(results) == 6

    host = hostname()

    assert "nodes" in results[0]
    nodes = results[0]['nodes']
    assert len(nodes) == 1

    assert "name" in nodes[0]
    assert nodes[0]['name'] == host

    assert "sessions" in results[1]
    assert "nb_sessions" in results[1]
    assert results[1]["nb_sessions"] == 1
    sessions = results[1]["sessions"]
    assert isinstance(sessions, list)
    assert len(sessions) == 1
    session = sessions[0]
    assert session['sessionid'] == "default"
    assert "jobs" in session
    jobs = session["jobs"]
    assert isinstance(jobs, dict)
    assert "default.dummy" in jobs
    job = jobs["default.dummy"]
    assert isinstance(job, list)
    assert job[0]["name"] == host
    assert job[0]["pids"] == [1]
    node_info = job[0]["node_info"]
    assert node_info["name"] == host

    assert "jobs" in results[2]
    assert "nb_jobs" in results[2]
    assert results[2]["nb_jobs"] == 1
    job1 = results[2]["jobs"][0]
    job1["name"] == "default.dummy"
    assert len(job1["sources"]) == 1
    assert job1["sources"] == job

    assert "sources" in results[3]
    assert len(results[3]["sources"]) == 1
    assert results[3]["sources"] == job

    assert results[4]["sessions"] == []
    assert results[4]["nb_sessions"] == 0

    assert results[5]["jobs"] == []
    assert results[5]["nb_jobs"] == 0


def test_lookup_client_events():
    # intiallize the lookupd server
    loop = pyuv.Loop.default_loop()
    r = Registry(loop)
    sock = bind_sockets(LOOKUPD_ADDR)
    io_loop = IOLoop(_loop=loop)
    server = http_server(io_loop, sock, registration_db=r)
    server.start()

    lookup_address = "http://%s" % LOOKUPD_ADDR
    client = LookupServer(lookup_address, loop)
    channel = client.lookup()

    emitted = []
    def cb(event, msg):
        emitted.append((event, msg))
    channel.bind_all(cb)
    channel.start()

    http_handler = HttpHandler(MockConfig(bind=GAFFERD_ADDR,
        lookupd_addresses=[lookup_address]))
    m = Manager(loop=loop)
    t = pyuv.Timer(loop)
    t0 = pyuv.Timer(loop)

    def do_stop(h):
        channel.close()
        server.stop()
        io_loop.close(True)

    def stop_server(m):
        t.start(do_stop, 0.4, 0.0)

    def wait(h):
        m.stop(stop_server)

    def on_manager(h):
        # start the manager with the HTTP API
        m.start(apps=[http_handler])
        testfile, cmd, args, wdir = dummy_cmd()
        config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
        m.load(config)
        m.stop_process(1)
        m.unload("dummy")
        h.start(wait, 0.3, 0.0)

    t0.start(on_manager, 0.3, 0.0)
    loop.run()
    actions = [line[0] for line in emitted]
    assert len(emitted) == 7
    assert list(actions) == ['add_node', 'identify', 'add_job', 'add_process',
            'remove_process', 'remove_job', 'remove_node']

if __name__ == "__main__":
    test_lookup_client_events()

########NEW FILE########
__FILENAME__ = test_manager
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from collections import deque
from functools import partial
import os
import signal
import sys
import time
from tempfile import mkstemp

import pyuv
import pytest

from gaffer.error import ProcessError, ProcessNotFound
from gaffer.manager import Manager
from gaffer.process import ProcessConfig, Process
from gaffer.state import FlappingInfo

def tmpfile():
     fd, testfile = mkstemp()
     os.close(fd)
     return testfile

def dummy_cmd():
    fd, testfile = mkstemp()
    os.close(fd)
    if sys.platform == 'win32':
        cmd = "cmd.exe"
        args = args=["/c", "proc_dummy.py", testfile],
    else:
        cmd = sys.executable
        args = ["-u", "./proc_dummy.py", testfile]
    wdir = os.path.dirname(__file__)
    return (testfile, cmd, args, wdir)


def crash_cmd():
    fd, testfile = mkstemp()
    os.close(fd)
    if sys.platform == 'win32':
        cmd = "cmd.exe"
        args = args=["/c", "proc_crash.py"],
    else:
        cmd = sys.executable
        args = ["-u", "./proc_crash.py"]
    wdir = os.path.dirname(__file__)
    return (cmd, args, wdir)

def test_simple():
    m = Manager()
    m.start()
    assert m.started == True

    def on_stop(manager):
        assert manager.started == False

    m.stop(on_stop)
    m.run()

def test_simple_job():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()

    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)

    m.load(config, start=False)
    state = m._get_locked_state("dummy")

    assert state.numprocesses == 1
    assert state.name == "default.dummy"
    assert state.cmd == cmd
    assert state.config['args'] == args
    assert state.config['cwd'] == wdir

    m.unload("dummy")

    with pytest.raises(ProcessError):
        m._get_locked_state("dummy")

    m.stop()
    m.run()

def test_start_stop_job():
    res = []
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    state = m._get_locked_state("dummy")
    res.append(len(state.running))
    m.stop_job("dummy")
    res.append(len(state.running))
    m.stop()
    m.run()

    assert res == [1, 0]


def test_start_multiple():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=2)
    m.load(config)
    state = m._get_locked_state("dummy")
    assert len(state.running) == 2
    m.stop()
    m.run()

def test_scalein():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=1)
    m.load(config)
    state = m._get_locked_state("dummy")

    assert len(state.running) == 1
    ret = m.scale("dummy", 1)
    assert ret == 2

    time.sleep(0.2)
    assert len(state.running) == 2

    ret = m.scale("dummy", 1)
    assert ret == 3

    time.sleep(0.2)
    assert len(state.running) == 3


    ret = m.scale("dummy", 3)
    assert ret == 6

    time.sleep(0.2)
    assert len(state.running) == 6

    m.stop()
    m.run()

def test_scaleout():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    state = m._get_locked_state("dummy")


    assert len(state.running) == 4
    ret = m.scale("dummy", -1)
    assert ret == 3

    time.sleep(0.2)
    assert len(state.running) == 3

    ret = m.scale("dummy", -2)
    assert ret == 1

    time.sleep(0.2)
    assert len(state.running) == 1
    m.stop()
    m.run()

def test_numprocesses():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    state = m._get_locked_state("dummy")

    assert len(state.running) == 4
    state.numprocesses = 0
    assert state.numprocesses == 0

    m.manage("dummy")
    time.sleep(0.2)
    assert len(state.running) == 0
    m.stop()
    m.run()

def test_process_id():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    state = m._get_locked_state("dummy")

    res = []
    def cb(m, p):
        res.append(p.pid)

    res2 = [p.pid for p in m.list("dummy")]
    m.walk(cb, "dummy")
    m.stop()
    m.run()

    assert res == [1, 2, 3, 4]
    assert res == res2

def test_restart_process():
    results = []
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)

    results.append(m.pids("dummy"))
    m.reload("dummy")

    def cb(handle):
        results.append(m.pids("dummy"))
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(cb, 0.4, 0.0)
    m.run()

    assert results[0] != results[1]

def test_restart_manager():
    results = []
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    results.append(m.pids("dummy"))

    def cb(manager):
        results.append(m.pids("dummy"))
        m.stop()

    m.restart()
    t = pyuv.Timer(m.loop)
    t.start(cb, 0.4, 0.0)
    m.run()

    assert results[0] != results[1]

def test_send_signal():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    time.sleep(0.2)

    m.killall("dummy", signal.SIGHUP)
    time.sleep(0.2)


    with pytest.raises(ProcessError) as e:
        m.killall("dummy1", signal.SIGHUP)
        assert e.errno == 404

    m.stop_job("dummy")

    def stop(handle):
        handle.stop()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.8, 0.0)
    m.run()

    with open(testfile, 'r') as f:
        res = f.read()
        assert res == 'STARTHUPQUITSTOP'

def test_flapping():
    m = Manager()
    m.start()
    states = []
    cmd, args, wdir = crash_cmd()

    # create flapping info object
    flapping = FlappingInfo(attempts=1., window=1, retry_in=0.1,
            max_retry=1)

    # load conf
    conf1 = ProcessConfig("crashing", cmd, args=args, cwd=wdir,
            flapping=flapping)
    conf2 = ProcessConfig("crashing2", cmd, args=args, cwd=wdir)
    m.load(conf1)
    m.load(conf2)

    time.sleep(0.2)

    def cb(handle):
        state = m._get_locked_state("crashing")
        states.append(state.stopped)
        state2 = m._get_locked_state("crashing2")
        states.append(state2.stopped)
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(cb, 0.8, 0.0)
    m.run()

    assert states == [True, False]

def test_events():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, msg):
        emitted.append((ev, msg['name']))

    # subscribe to all events
    m.events.subscribe('.', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=4)
    m.load(config)
    m.scale("dummy", 1)
    m.unload("dummy")

    time.sleep(0.2)
    m.stop()
    m.run()

    assert ('load', 'default.dummy') in emitted
    assert ('start', 'default.dummy') in emitted
    assert ('update', 'default.dummy') in emitted
    assert ('stop', 'default.dummy') in emitted
    assert ('unload', 'default.dummy') in emitted

def test_process_events():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, *args):
        emitted.append(ev)

    # subscribe to all events
    m.events.subscribe('job.default.dummy', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    m.unload("dummy")

    time.sleep(0.2)
    m.stop()
    m.run()

    assert 'job.default.dummy.start' in emitted
    assert 'job.default.dummy.spawn' in emitted
    assert 'job.default.dummy.stop' in emitted
    assert 'job.default.dummy.exit' in emitted

def test_process_exit_event():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, msg):
        emitted.append(msg)

    # subscribe to all events
    m.events.subscribe('job.default.dummy.exit', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    m.unload("dummy")

    time.sleep(0.2)
    m.stop()
    m.run()

    assert len(emitted) == 1
    assert len(emitted[0]) == 7

    msg = emitted[0]
    assert "exit_status" in msg
    assert msg['once'] == False

def test_process_stats():
    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)
    m.load(config)
    time.sleep(0.2)
    info = m.stats("dummy")
    info_by_id = m.get_process(1).info
    os_pid = m.running[1].os_pid
    m.stop()
    m.run()

    assert isinstance(info, dict)
    assert isinstance(info_by_id, dict)
    assert "os_pid" in info_by_id
    assert info_by_id["os_pid"] == os_pid
    assert info['name'] == "default.dummy"
    assert len(info['stats']) == 1
    assert info['stats'][0]['os_pid'] == info_by_id['os_pid']

def test_processes_stats():

    def collect_cb(inf, m, name):
        inf.append(m.stats(name))

    m = Manager()
    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    testfile1, cmd1, args1, wdir1 = dummy_cmd()
    configa = ProcessConfig("a", cmd, args=args, cwd=wdir)
    m.load(configa)

    time.sleep(0.2)
    infos = []
    infos2 = []
    m.jobs_walk(partial(collect_cb, infos))
    configb = ProcessConfig("b", cmd, args=args, cwd=wdir)
    m.load(configb)
    m.jobs_walk(partial(collect_cb, infos2))
    m.stop()
    m.run()

    assert len(infos) == 1
    assert len(infos2) == 2
    assert infos[0]['name'] == "default.a"
    assert infos2[0]['name'] == "default.a"
    assert infos2[1]['name'] == "default.b"

def test_monitor():
    m = Manager()
    monitored = []
    def cb(evtype, info):
        monitored.append((evtype, info))

    m.start()
    testfile, cmd, args, wdir = dummy_cmd()
    configa = ProcessConfig("a", cmd, args=args, cwd=wdir)
    m.load(configa)
    time.sleep(0.2)
    os_pid = m.running[1].os_pid
    m.monitor(cb, "a")

    def stop(handle):
        m.unmonitor(cb, "a")
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.3, 0.0)

    m.run()
    assert len(monitored) >= 1
    res = monitored[0]
    assert res[0] == "stat"
    assert "cpu" in res[1]
    assert res[1]["os_pid"] == os_pid

def test_priority():
    m = Manager()
    started = []
    def cb(evtype, info):
        started.append(info['name'])

    m.start()
    m.events.subscribe('start', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    a = ProcessConfig("a", cmd, args=args, cwd=wdir)
    b = ProcessConfig("b", cmd, args=args, cwd=wdir)
    c = ProcessConfig("c", cmd, args=args, cwd=wdir)


    m.load(a, start=False)
    m.load(b, start=False)
    m.load(c, start=False)

    # start all processes
    m.jobs_walk(lambda mgr, label: mgr.start_job(label))

    def stop(handle):
        m.events.unsubscribe("start", cb)
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.4, 0.0)
    m.run()

    assert started == ["default.a", "default.b", "default.c"]

def test_sessions():
    m = Manager()
    started = []
    stopped = []
    def cb(evtype, info):
        if evtype == "start":
            started.append(info['name'])
        elif evtype == "stop":
            stopped.append(info['name'])

    start_job = lambda mgr, label: mgr.start_job(label)
    stop_job = lambda mgr, label: mgr.stop_job(label)

    m.start()
    m.events.subscribe('start', cb)
    m.events.subscribe('stop', cb)
    testfile, cmd, args, wdir = dummy_cmd()
    a = ProcessConfig("a", cmd, args=args, cwd=wdir)
    b = ProcessConfig("b", cmd, args=args, cwd=wdir)


    # load process config in different sessions
    m.load(a, sessionid="ga", start=False)
    m.load(b, sessionid="ga", start=False)
    m.load(a, sessionid="gb", start=False)


    sessions = m.sessions

    ga1 = m.jobs('ga')
    gb1 = m.jobs('gb')

    m.jobs_walk(start_job, "ga")
    m.jobs_walk(start_job, "gb")

    ga2 = []
    def rem_cb(h):
        m.unload("a", sessionid="ga")
        [ga2.append(name) for name in m.jobs('ga')]


    t0 = pyuv.Timer(m.loop)
    t0.start(rem_cb, 0.2, 0.0)
    m.jobs_walk(stop_job, "gb")

    def stop(handle):
        m.events.unsubscribe("start", cb)
        m.events.unsubscribe("stop", cb)
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.6, 0.0)
    m.run()

    assert len(sessions) == 2
    assert sessions == ['ga', 'gb']
    assert ga1 == ['ga.a', 'ga.b']
    assert gb1 == ['gb.a']
    assert started == ['ga.a', 'ga.b', 'gb.a']
    assert stopped == ['gb.a', 'ga.a']
    assert ga2 == ['ga.b']


def test_process_commit():
    emitted = []
    m = Manager()
    m.start()

    def cb(ev, msg):
        emitted.append(msg)

    # subscribe to all events
    m.events.subscribe('job.default.dummy.exit', cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=0)
    m.load(config)

    pids0 = m.pids()
    pid = m.commit("dummy", env={"BLAH": "test"})
    process = m.get_process(pid)
    pids1 = m.pids()

    state = m._get_locked_state("dummy")

    assert len(state.running) == 0
    assert state.numprocesses == 0
    assert len(state.running_out) == 1


    m.unload("dummy")

    time.sleep(0.2)
    m.stop()
    m.run()

    assert pids0 == []
    assert pid == 1
    assert process.name == "default.dummy"
    assert process.pid == 1
    assert "BLAH" in process.env
    assert pids1 == [1]

    assert len(emitted) == 1
    assert len(emitted[0]) == 7

    msg = emitted[0]
    assert "exit_status" in msg
    assert msg['once'] == True

if __name__ == "__main__":
    test_sessions()

########NEW FILE########
__FILENAME__ = test_message
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from gaffer.message import (Message, decode_frame, make_response,
        FRAME_ERROR_TYPE, FRAME_RESPONSE_TYPE, FRAME_MESSAGE_TYPE, MAGIC_V1)

def test_encode():
    m = Message(b"test", id=b"someid")

    # validate info
    assert m.body == b"test"
    assert m.id == b"someid"
    assert m.type == FRAME_MESSAGE_TYPE
    assert str(m) == "Message: someid"
    assert m.encode() == b"V1 message someid\0test"

def test_decode():
    m = decode_frame(b"V1 message someid\0test")

    assert isinstance(m, Message)
    assert m.id == b"someid"
    assert m.type == FRAME_MESSAGE_TYPE
    assert m.body == b"test"

def test_response():
    m = make_response("test", id="someid")

    assert isinstance(m, Message)
    assert m.id == b"someid"
    assert m.type == FRAME_RESPONSE_TYPE
    assert m.body == b"test"
    assert m.encode() == b"V1 response someid\0test"

########NEW FILE########
__FILENAME__ = test_pbkdf2
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from gaffer.gafferd.pbkdf2 import pbkdf2_hex


def test_pbkdf2():

    def check(data, salt, iterations, keylen, expected):
        rv = pbkdf2_hex(data, salt, iterations, keylen)
        err_msg = """Test Failed:
        Expected:   %(expected)s
        Got:        %(rv)s
        Parameters:'
            data=%(data)s
            salt=%(salt)s
            iterations=%(iterations)s""" % {"expected": expected,
                    "rv": rv, "data": data, "salt": salt,
                    "iterations": iterations}

        assert rv == expected, err_msg

    # From RFC 6070
    check(b'password', b'salt', 1, 20,
          b'0c60c80f961f0e71f3a9b524af6012062fe037a6')
    check(b'password', b'salt', 2, 20,
          b'ea6c014dc72d6f8ccd1ed92ace1d41f0d8de8957')
    check(b'password', b'salt', 4096, 20,
          b'4b007901b765489abead49d926f721d065a429c1')
    check(b'passwordPASSWORDpassword', b'saltSALTsaltSALTsaltSALTsaltSALTsalt',
          4096, 25, b'3d2eec4fe41c849b80c8d83662c0e44a8b291a964cf2f07038')
    check(b'pass\x00word', b'sa\x00lt', 4096, 16,
          b'56fa6aa75548099dcc37d7f03425e0c3')
    # This one is from the RFC but it just takes for ages
    ##check('password', 'salt', 16777216, 20,
    ##      'eefe3d61cd4da4e4e9945b3d6ba2158c2634e984')

    # From Crypt-PBKDF2
    check(b'password', b'ATHENA.MIT.EDUraeburn', 1, 16,
          b'cdedb5281bb2f801565a1122b2563515')
    check(b'password', b'ATHENA.MIT.EDUraeburn', 1, 32,
          b'cdedb5281bb2f801565a1122b25635150ad1f7a04bb9f3a333ecc0e2e1f70837')
    check(b'password', b'ATHENA.MIT.EDUraeburn', 2, 16,
          b'01dbee7f4a9e243e988b62c73cda935d')
    check(b'password', b'ATHENA.MIT.EDUraeburn', 2, 32,
          b'01dbee7f4a9e243e988b62c73cda935da05378b93244ec8f48a99e61ad799d86')
    check(b'password', b'ATHENA.MIT.EDUraeburn', 1200, 32,
          b'5c08eb61fdf71e4e4ec3cf6ba1f5512ba7e52ddbc5e5142f708a31e2e62b1e13')
    check(b'X' * 64, b'pass phrase equals block size', 1200, 32,
          b'139c30c0966bc32ba55fdbf212530ac9c5ec59f1a452f5cc9ad940fea0598ed1')
    check(b'X' * 65, b'pass phrase exceeds block size', 1200, 32,
          b'9ccad6d468770cd51b10e6a68721be611a8b4d282601db3b36be9246915ec82a')

########NEW FILE########
__FILENAME__ = test_pid_channel
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os
import sys
import time

import pytest
import pyuv

from gaffer.gafferd.http import HttpHandler
from gaffer.httpclient import Server, Process
from gaffer.httpclient.websocket import IOChannel
from gaffer.manager import Manager
from gaffer.process import ProcessConfig

from test_manager import dummy_cmd
from test_http import MockConfig

TEST_HOST = '127.0.0.1'
TEST_PORT = (os.getpid() % 31000) + 1024
TEST_URI = "%s:%s" % (TEST_HOST, TEST_PORT)


if sys.version_info >= (3, 0):
    linesep = os.linesep.encode()
else:
    linesep = os.linesep

def start_manager():
    http_handler = HttpHandler(MockConfig(bind=TEST_URI))
    m = Manager()
    m.start(apps=[http_handler])
    time.sleep(0.2)
    return m

def get_server(loop):
    return Server("http://%s:%s" % (TEST_HOST, TEST_PORT), loop=loop)

def init():
    m = start_manager()
    s = get_server(m.loop)
    return (m, s)


def test_stdio():
    m, s = init()

    emitted = []
    def cb(ch, data):
        emitted.append(data)

    responses = []
    def cb2(ch, result, error):
        responses.append((result, error))

    if sys.platform == 'win32':
        config =  ProcessConfig("echo",  "cmd.exe",
                args=["/c", "proc_stdin_stdout.py"],
                redirect_output=["stdout"], redirect_input=True)

    else:
        config = ProcessConfig("echo", "./proc_stdin_stdout.py",
            cwd=os.path.dirname(__file__), redirect_output=["stdout"],
            redirect_input=True)

    # load the process
    m.load(config)
    time.sleep(0.2)

    # start a channel
    p = s.get_process(1)
    channel = p.socket()
    channel.start()

    # subscribe to remote events
    channel.start_read(cb)

    # write to STDIN
    channel.write(b"ECHO" + linesep, cb2)

    def stop(handle):
        channel.stop_read()
        channel.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.3, 0.0)
    m.run()

    assert len(emitted) == 1
    assert emitted == [b'ECHO\n\n']
    assert responses == [(b"OK", None)]

def test_mode_readable():
    m, s = init()

    if sys.platform == 'win32':
        config =  ProcessConfig("echo",  "cmd.exe",
                args=["/c", "proc_stdin_stdout.py"],
                redirect_output=["stdout"], redirect_input=True)

    else:
        config = ProcessConfig("echo", "./proc_stdin_stdout.py",
            cwd=os.path.dirname(__file__), redirect_output=["stdout"],
            redirect_input=True)

    # load the process
    m.load(config)
    time.sleep(0.2)

    # start a channel
    p1 = s.get_process(1)
    channel = p1.socket(mode=pyuv.UV_READABLE)
    channel.start()

    # subscribe to remote events
    channel.start_read(lambda ch, d: None)

    with pytest.raises(IOError):
        channel.write(b"ECHO" + linesep)

    def stop(handle):
        channel.stop_read()
        channel.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.3, 0.0)
    m.run()

def test_mode_writable():
    m, s = init()

    responses = []
    def cb2(ch, result, error):
        responses.append((result, error))

    if sys.platform == 'win32':
        config =  ProcessConfig("echo",  "cmd.exe",
                args=["/c", "proc_stdin_stdout.py"],
                redirect_output=["stdout"], redirect_input=True)

    else:
        config = ProcessConfig("echo", "./proc_stdin_stdout.py",
            cwd=os.path.dirname(__file__), redirect_output=["stdout"],
            redirect_input=True)

    # load the process
    m.load(config)
    time.sleep(0.2)

    # start a channel
    p1 = s.get_process(1)
    channel = p1.socket(mode=pyuv.UV_WRITABLE)
    channel.start()

    # subscribe to remote events
    with pytest.raises(IOError):
        channel.start_read(lambda ch, d: None)


    channel.write(b"ECHO" + linesep, cb2)

    def stop(handle):
        channel.stop_read()
        channel.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.3, 0.0)
    m.run()

    assert responses == [(b"OK", None)]

def test_custom_stream():
    m, s = init()

    emitted = []
    def cb(ch, data):
        emitted.append(data)

    responses = []
    def cb2(ch, result, error):
        responses.append((result, error))

    if sys.platform == 'win32':
        config =  ProcessConfig("echo",  "cmd.exe",
                args=["/c", "proc_custom_stream.py"],
                custom_streams=["ctrl"])
    else:
        config = ProcessConfig("echo", "./proc_custom_stream.py",
            cwd=os.path.dirname(__file__), custom_streams=["ctrl"])

    m.load(config)

    # start a channel
    p = s.get_process(1)
    channel = p.socket(stream="ctrl")
    channel.start()

    # subscribe to remote events
    channel.start_read(cb)

    # write to STDIN
    channel.write(b"ECHO" + linesep, cb2)

    def stop(handle):
        channel.stop_read()
        channel.close()
        m.stop()


    t = pyuv.Timer(m.loop)
    t.start(stop, 0.4, 0.0)
    m.loop.run()

    assert len(emitted) == 1
    assert emitted == [b'ECHO\n']
    assert responses == [(b"OK", None)]

if __name__ == "__main__":
    test_custom_stream()

########NEW FILE########
__FILENAME__ = test_process
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import os
import signal
import sys
import time
import socket

import pyuv
from gaffer.process import Process

from test_manager import dummy_cmd

if sys.version_info >= (3, 0):
    linesep = os.linesep.encode()
else:
    linesep = os.linesep

def test_simple():
    exit_res = []
    def exit_cb(process, return_code, term_signal):
        exit_res.append(process)


    loop = pyuv.Loop.default_loop()
    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd, on_exit_cb=exit_cb)

    assert p.pid == "someid"
    assert p.name == "dummy"
    assert p.cmd == cmd
    assert p.args == args
    assert cwd == cwd

    p.spawn()
    assert p.active == True

    time.sleep(0.2)
    p.stop()
    loop.run()
    assert p.active == False
    with open(testfile, 'r') as f:
        res = f.read()
        assert res == 'STARTQUITSTOP'

    assert len(exit_res) == 1
    assert exit_res[0].name == "dummy"
    assert exit_res[0].active == False


def test_signal():
    loop = pyuv.Loop.default_loop()
    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd)
    p.spawn()
    time.sleep(0.2)
    p.kill(signal.SIGHUP)
    time.sleep(0.2)
    p.stop()
    loop.run()
    with open(testfile, 'r') as f:
        res = f.read()
        assert res == 'STARTHUPQUITSTOP'


def test_info():
    loop = pyuv.Loop.default_loop()
    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd)
    p.spawn()
    time.sleep(0.2)
    info = p.info
    os_pid = p.os_pid
    p.stop()
    loop.run()

    assert info['os_pid'] == os_pid
    assert info['name'] == "dummy"
    assert info['pid'] == "someid"



def test_stats():
    loop = pyuv.Loop.default_loop()
    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd)
    p.spawn()
    time.sleep(0.2)
    stats = p.stats
    os_pid = p.os_pid
    p.stop()
    loop.run()

    assert "cpu" in stats
    assert "mem_info1" in stats


def test_stat_events():
    loop = pyuv.Loop.default_loop()
    monitored = []
    def cb(evtype, info):
        monitored.append((evtype, info))

    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd)
    p.spawn()
    time.sleep(0.2)
    os_pid = p.os_pid
    p.monitor(cb)

    def stop(handle):
        p.unmonitor(cb)
        assert p._process_watcher.active == False
        p.stop()

    t = pyuv.Timer(loop)
    t.start(stop, 0.3, 0.0)
    loop.run()

    assert len(monitored) >= 1
    res = monitored[0]
    assert res[0] == "stat"
    assert "cpu" in res[1]
    assert "mem_info1" in res[1]
    assert res[1]['os_pid'] == os_pid


def test_stat_events_refcount():
    loop = pyuv.Loop.default_loop()
    monitored = []
    def cb(evtype, info):
        monitored.append((evtype, info))

    def cb2(evtype, info):
        monitored.append((evtype, info))

    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd)
    p.spawn()
    time.sleep(0.2)
    os_pid = p.os_pid
    p.monitor(cb)
    p.monitor(cb2)
    def stop(handle):
        p.unmonitor(cb)
        assert p._process_watcher.active == True
        assert p._process_watcher._refcount == 1
        p.unmonitor(cb2)
        assert p._process_watcher.active == False
        p.stop()

    t = pyuv.Timer(loop)
    t.start(stop, 0.3, 0.0)
    loop.run()

    assert len(monitored) >= 2
    res = monitored[0]
    assert res[0] == "stat"
    assert "cpu" in res[1]


def test_redirect_output():
    loop = pyuv.Loop.default_loop()
    monitored1 = []
    monitored2 = []
    def cb(evtype, info):
        monitored1.append((evtype, info))

    def cb2(evtype, info):
        monitored2.append((evtype, info))

    testfile, cmd, args, cwd = dummy_cmd()
    p = Process(loop, "someid", "dummy", cmd, args=args,
        cwd=cwd, redirect_output=["stdout", "stderr"])
    p.spawn()
    time.sleep(0.2)
    os_pid = p.os_pid

    p.monitor_io("stdout", cb)
    p.monitor_io("stderr", cb2)

    p.stop()
    loop.run()

    assert len(monitored1) == 1
    assert len(monitored2) == 1

    ev1 = monitored1[0]
    ev2 = monitored2[0]

    assert ev1[0] == 'stdout'
    assert ev1[1] == {'data': b'hello out', 'pid': "someid", 'name': 'dummy',
            'event': 'stdout'}

    assert ev2[0] == 'stderr'
    assert ev2[1] == {'data': b'hello err', 'pid': "someid", 'name': 'dummy',
            'event': 'stderr'}

def test_redirect_input():
    loop = pyuv.Loop.default_loop()
    monitored = []
    def cb(evtype, info):
        monitored.append(info['data'])

    if sys.platform == 'win32':
        p = Process(loop, "someid", "echo", "cmd.exe",
                args=["/c", "proc_stdin_stdout.py"],
                redirect_output=["stdout"], redirect_input=True)

    else:
        p = Process(loop, "someid", "echo", "./proc_stdin_stdout.py",
            cwd=os.path.dirname(__file__),
            redirect_output=["stdout"], redirect_input=True)
    p.spawn()
    time.sleep(0.2)
    p.monitor_io("stdout", cb)
    p.write(b"ECHO" + linesep)

    def stop(handle):
        p.unmonitor_io("stdout", cb)
        p.stop()

    t = pyuv.Timer(loop)
    t.start(stop, 0.3, 0.0)
    loop.run()

    assert len(monitored) == 1
    assert monitored == [b'ECHO\n\n']

def test_custom_stream():
    loop = pyuv.Loop.default_loop()
    monitored = []
    def cb(evtype, info):
        monitored.append(info['data'])

    if sys.platform == 'win32':
        p = Process(loop, "someid", "echo", "cmd.exe",
                args=["/c", "proc_custom_stream.py"],
                custom_streams=['ctrl'])

    else:
        p = Process(loop, "someid", "echo", "./proc_custom_stream.py",
                cwd=os.path.dirname(__file__),
                custom_streams=['ctrl'])
    p.spawn()
    time.sleep(0.2)
    stream = p.streams['ctrl']
    assert stream.id == 3
    stream.subscribe(cb)
    stream.write(b"ECHO" + linesep)

    def stop(handle):
        stream.unsubscribe(cb)
        p.stop()

    t = pyuv.Timer(loop)
    t.start(stop, 0.3, 0.0)
    loop.run()

    assert len(monitored) == 1
    assert monitored == [b'ECHO\n']

def test_custom_channel():
    if sys.platform == 'win32':
        return

    loop = pyuv.Loop.default_loop()
    sockets = socket.socketpair(socket.AF_UNIX)
    pipes = []
    for sock in sockets:
        pipe = pyuv.Pipe(loop)
        pipe.open(sock.fileno())
        pipes.append(pipe)
    channel = pipes[0]
    monitored = []
    def cb(handle, data, error):
        if not data:
            return
        monitored.append(data)

    p = Process(loop, "someid", "echo", "./proc_custom_stream.py",
            cwd=os.path.dirname(__file__),
            custom_channels=[pipes[1]])

    p.spawn()
    channel.start_read(cb)
    time.sleep(0.2)
    channel.write(b"ECHO" + linesep)

    def stop(handle):
        channel.stop_read()
        p.stop()

    t = pyuv.Timer(loop)
    t.start(stop, 0.3, 0.0)
    loop.run()

    assert len(monitored) == 1
    assert monitored == [b'ECHO\n']

def test_substitue_env():
    loop = pyuv.Loop.default_loop()

    cmd = 'echo "test" > $NULL_PATH'
    env = {"NULL_PATH": "/dev/null"}
    p = Process(loop, "someid", "null", cmd, env=env,
            cwd=os.path.dirname(__file__),
            redirect_output=["stdout"], redirect_input=True)

    cmd2 = "echo"
    args = ["test", ">", "$NULL_PATH"]
    p2 = Process(loop, "someid", "null", cmd2, args=args,
            env=env, cwd=os.path.dirname(__file__),
            redirect_output=["stdout"], redirect_input=True)


    assert "/dev/null" in p.args
    assert "$NULL_PATH" not in p.args
    assert "/dev/null" in p2.args
    assert "$NULL_PATH" not in p2.args

########NEW FILE########
__FILENAME__ = test_sync
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

from gaffer.sync import *

def test_increment():
    i = 0
    i = increment(i)
    assert i == 1

    i = increment(i)
    assert i == 2


def test_decrement():
    i = 0
    i = decrement(i)
    assert i == -1

    i = decrement(i)
    assert i == -2

def test_combine():
    i = 1
    i = decrement(i)
    assert i == 0
    i = increment(i)
    assert i == 1

def test_add():
    i = 1
    i = add(i, 2)
    assert i == 3

def test_sub():
    i = 3
    i = sub(i, 2)
    assert i == 1

def test_read():
    i = 10
    v = atomic_read(i)
    assert i == v

def test_compare_and_swap():
    a, b = 1, 2
    a = compare_and_swap(a, b)

    assert a == b

    a, b = 1, 1
    a = compare_and_swap(a, b)
    assert a == 1

########NEW FILE########
__FILENAME__ = test_user
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.

import pyuv
import pytest

from gaffer.gafferd.users import (AuthManager, User, DummyUser,
        SqliteAuthHandler, UserConflict, UserNotFound)

from test_http import MockConfig

def test_config():
    return MockConfig(auth_dbname=":memory:", keys_dbname=":memory:")

def test_sqlite_backend():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with SqliteAuthHandler(loop, conf) as h:
        h.create_user("test", "test")

        with pytest.raises(UserConflict):
            h.create_user("test", "test")

        assert h.has_user("test") == True
        assert h.has_user("test1") == False

        h.create_user("test1", "test")
        assert h.has_user("test1") == True

        user = h.get_user("test")
        assert user == {"username": "test", "password": "test", "user_type": 0,
                "key": None}

        h.set_password("test", "test1")
        user = h.get_user("test")
        assert user['password'] == "test1"

        h.set_key("test", "test_key")
        user = h.get_user("test")
        assert user['key'] == "test_key"

        h.update_user("test", "test")
        user = h.get_user("test")
        assert user == {"username": "test", "password": "test", "user_type": 0,
                "key": None}

        h.delete_user("test")
        with pytest.raises(UserNotFound):
            h.get_user("test")

def test_auth_backend():
    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with AuthManager(loop, conf) as auth:
        auth.create_user("test", "test")

        with pytest.raises(UserConflict):
            auth.create_user("test", "test")

        assert auth.has_user("test") == True
        assert auth.has_user("test1") == False

        auth.create_user("test1", "test")
        assert auth.has_user("test1") == True

        user = auth.get_user("test")
        assert user["password"] != "test"

        auth.set_password("test", "test1")
        user1 = auth.get_user("test")
        assert user1['password'] != user["password"]

        auth.set_key("test", "test_key")
        user = auth.get_user("test")
        assert user['key'] == "test_key"

        auth.update_user("test", "test")
        user3 = auth.get_user("test")
        assert user3 != user

        auth.delete_user("test")
        with pytest.raises(UserNotFound):
            auth.get_user("test")

def test_authenticate():

    conf = test_config()
    loop = pyuv.Loop.default_loop()

    with AuthManager(loop, conf) as auth:
        auth.create_user("test", "test")
        user = auth.authenticate("test", "test")
        assert isinstance(user, User)
        assert user.is_authenticated() == True
        assert user.is_anonymous() == False
        user1 = auth.authenticate("test", "test1")
        assert isinstance(user1, DummyUser)
        assert user1.is_authenticated() == False
        assert user1.is_anonymous() == True

########NEW FILE########
__FILENAME__ = test_webchannel
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import json
import os
import time

import pyuv

from gaffer import __version__
from gaffer.gafferd.http import HttpHandler
from gaffer.httpclient import (Server, Job, Process,
        GafferNotFound, GafferConflict, WebSocket)
from gaffer.manager import Manager
from gaffer.process import ProcessConfig

from test_manager import dummy_cmd
from test_http import MockConfig

TEST_HOST = '127.0.0.1'
TEST_PORT = (os.getpid() % 31000) + 1024
TEST_URL = "ws://%s:%s/channel/websocket" % (TEST_HOST, str(TEST_PORT))


def start_manager():
    http_handler = HttpHandler(MockConfig(bind="%s:%s" % (TEST_HOST,
        TEST_PORT)))
    m = Manager(loop=pyuv.Loop.default_loop())
    m.start(apps=[http_handler])
    return m

def get_server(loop):
    return Server("http://%s:%s" % (TEST_HOST, TEST_PORT), loop=loop)

def init():
    m = start_manager()
    s = get_server(m.loop)

    # get a gaffer socket
    socket = s.socket()
    socket.start()

    return (m, s, socket)

def test_basic_socket():
    m = start_manager()
    s = get_server(m.loop)
    t = pyuv.Timer(m.loop)

    # get a gaffer socket
    socket = s.socket()
    socket.start()

    messages =[]
    def cb(event, data):
        print("got %s" % event)
        messages.append(event)

    # bind to all events
    socket.subscribe('EVENTS')
    socket['EVENTS'].bind_all(cb)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=1)


    def stop_all(handle):
        m.stop()
        socket.close()

    def load_process(ev, msg):
        m.load(config)
        m.scale("dummy", 1)
        m.unload("dummy")
        t.start(stop_all, 0.6, 0.0)

    socket.bind("subscription_success", load_process)
    m.run()

    assert 'load' in messages
    assert 'start' in messages
    assert 'update' in messages
    assert 'stop' in messages
    assert 'unload' in messages


def test_stats():
    m = start_manager()
    s = get_server(m.loop)

    # get a gaffer socket
    socket = s.socket()
    socket.start()

    monitored = []
    def cb(event, info):
        monitored.append(info)

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("a", cmd, args=args, cwd=wdir)

    # bind to all events
    socket.subscribe("STATS:default.a")
    socket["STATS:default.a"].bind_all(cb)


    m.load(config)
    os_pid = m.running[1].os_pid


    def stop(handle):
        socket.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.4, 0.0)

    m.run()

    assert len(monitored) >= 1
    res = monitored[0]
    assert "cpu" in res
    assert res["os_pid"] == os_pid


def test_simple_job():
    m, s, socket = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)

    # send a command
    cmd0 = socket.send_command("load", config.to_dict(), start=False)
    cmd1 = socket.send_command("jobs")

    results = []
    def do_events(h):
        results.append((len(m.jobs()), len(s.jobs()), s.jobs()[0]))

    def stop(h):
        h.close()
        socket.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(do_events, 0.4, 0.0)
    t1 = pyuv.Timer(m.loop)
    t1.start(stop, 0.8, 0.0)
    m.run()

    assert cmd0.error() == None
    assert cmd1.error() == None
    assert results[0] == (1, 1, "default.dummy")
    assert cmd0.result() == {"ok": True}
    assert cmd1.result()["jobs"][0] == "default.dummy"


def test_remove_job():
    m, s, socket = init()
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir)

    results = []
    def cb(event, cmd):
        jobs = m.jobs()
        results.append((cmd, jobs))

    socket.bind("command_success", cb)
    socket.send_command("load", config.to_dict(), start=False)
    socket.send_command("unload", "dummy")

    def stop(h):
        h.close()
        socket.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.4, 0.0)
    m.run()

    assert len(results) == 2
    cmd0, jobs0 = results[0]
    cmd1, jobs1 = results[1]

    assert cmd0.result()["ok"] == True
    assert cmd1.result()["ok"] == True
    assert len(jobs0) == 1
    assert jobs0[0] == "default.dummy"
    assert len(jobs1) == 0


def test_notfound():
    m, s, socket = init()
    cmd = socket.send_command("info", "dummy")

    def stop(h):
        h.close()
        socket.close()
        m.stop()

    t = pyuv.Timer(m.loop)
    t.start(stop, 0.2, 0.0)
    m.run()

    assert cmd is not None
    assert cmd.error() is not None
    assert cmd.error()["errno"] == 404
    assert cmd.error()["reason"] == "not_found"


def test_commit():
    m, s, socket = init()

    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir,
            numprocesses=0)

    # send a command
    cmd0 = socket.send_command("load", config.to_dict(), start=False)
    cmd1 = socket.send_command("commit", "dummy")

    def stop(c):
        socket.close()
        m.stop()


    cmd1.add_done_callback(stop)
    m.run()

    assert cmd0.error() == None
    assert cmd1.error() == None
    assert cmd0.result() == {"ok": True}
    assert cmd1.result()["pid"] == 1

if __name__ == "__main__":
    test_basic_socket()

########NEW FILE########
__FILENAME__ = test_webhooks
# -*- coding: utf-8 -
#
# This file is part of gaffer. See the NOTICE for more information.
import os
import json
import time

# patch tornado IOLoop
from gaffer.tornado_pyuv import IOLoop, install
install()

import pyuv
from tornado.web import Application, RequestHandler
from tornado.httpserver import HTTPServer
from tornado import netutil
from gaffer.manager import Manager
from gaffer.process import ProcessConfig
from gaffer.webhooks import WebHooks

from test_manager import dummy_cmd

TEST_HOST = '127.0.0.1'
TEST_PORT = (os.getpid() % 31000) + 1024

class TestHandler(RequestHandler):
    def post(self, *args):
        obj = json.loads(self.request.body.decode('utf-8'))
        received = self.settings.get('received')
        received.append((obj['event'], obj['name']))
        self.write("ok")

def get_server(loop, received):
    io_loop = IOLoop(_loop=loop)

    test_handlers = [
            (r'/([^/]+)', TestHandler)
    ]

    app = Application(test_handlers, received=received)
    server = HTTPServer(app, io_loop=io_loop)

    [sock] = netutil.bind_sockets(TEST_PORT, address=TEST_HOST)
    server.add_socket(sock)
    return server

def make_uri(path):
    return "http://%s:%s%s" % (TEST_HOST, TEST_PORT, path)

def create_hooks(events):
    test_hooks = []
    for ev in events:
        test_hooks.append((ev, make_uri('/%s' % ev)))
    return test_hooks

def test_manager_hooks():
    hooks = create_hooks(['load', 'unload', 'start', 'update', 'stop',
        'job.default.dummy.start', 'job.default.dummy.spawn',
        'job.default.dummy.stop', 'job.default.dummy.exit'])
    emitted = []
    loop = pyuv.Loop.default_loop()
    s = get_server(loop, emitted)
    s.start()
    m = Manager(loop=loop)
    m.start(apps=[WebHooks(hooks)])
    testfile, cmd, args, wdir = dummy_cmd()
    config = ProcessConfig("dummy", cmd, args=args, cwd=wdir, numprocesses=1)
    m.load(config)
    m.manage("dummy")
    m.scale("dummy", 1)
    m.unload("dummy")

    t = pyuv.Timer(loop)

    def on_stop(manager):
        t.start(lambda h: s.stop(), 0.4, 0.0)

    m.stop(on_stop)

    m.run()
    assert ('load', 'default.dummy') in emitted
    assert ('start', 'default.dummy') in emitted
    assert ('update', 'default.dummy') in emitted
    assert ('stop', 'default.dummy') in emitted
    assert ('unload', 'default.dummy') in emitted
    assert ('job.default.dummy.start', 'default.dummy') in emitted
    assert ('job.default.dummy.spawn', 'default.dummy') in emitted
    assert ('job.default.dummy.stop', 'default.dummy') in emitted
    assert ('job.default.dummy.exit', 'default.dummy') in emitted

########NEW FILE########
__FILENAME__ = test_websocket
# -*- coding: utf-8 -

import os
import time

import pyuv
import tornado.web
from tornado.httpserver import HTTPServer
from tornado import netutil
from gaffer import sockjs
from gaffer.tornado_pyuv import IOLoop
from gaffer.httpclient.websocket import WebSocket

TEST_PORT = (os.getpid() % 31000) + 1024
TEST_HOST = "127.0.0.1"
TEST_URL = "ws://%s:%s/echo/websocket" % (TEST_HOST, str(TEST_PORT))

h_opened = []
h_messages = []

c_opened = []
c_messages = []


class HelloConnection(sockjs.SockJSConnection):

    def on_open(self, info):
        h_opened.append(True)

    def on_message(self, message):
        h_messages.append(message)
        self.send(message)


class HelloClient(WebSocket):

    def on_open(self):
        c_opened.append(True)

    def on_message(self, message):
        c_messages.append(message)


def test_basic():
    loop = pyuv.Loop.default_loop()
    io_loop = IOLoop(_loop=loop)

    HelloRouter = sockjs.SockJSRouter(HelloConnection, '/echo',
            io_loop=io_loop)

    ws = HelloClient(loop, TEST_URL)
    app = tornado.web.Application(HelloRouter.urls)
    server = HTTPServer(app, io_loop=io_loop)

    [sock] = netutil.bind_sockets(TEST_PORT, address=TEST_HOST)
    server.add_socket(sock)

    server.start()
    ws.start()

    t = pyuv.Timer(loop)
    t1 = pyuv.Timer(loop)

    start = time.time()
    def do_stop(handle):
        handle.close()
        server.stop()
        ws.close()
        io_loop.close()

    def init(handle):
        handle.close()
        ws.write_message("hello")
        t1.start(do_stop, 0.2, 0.0)

    t.start(init, 0.2, 0.0)
    loop.run()

    assert h_opened == [True]
    assert c_opened == [True]
    assert h_messages == ["hello"]
    assert c_messages == ["hello"]

if __name__ == "__main__":
    import logging
    logging.getLogger().setLevel(logging.DEBUG)
    test_basic()

########NEW FILE########

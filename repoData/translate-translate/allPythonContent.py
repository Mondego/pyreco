__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Translate Toolkit documentation build configuration file, created by
# sphinx-quickstart on Mon Mar 26 23:48:04 2012.
#
# This file is execfile()d with the current directory set to its containing
# dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

sys.path.insert(0, os.path.abspath('_ext'))
sys.path.insert(0, os.path.abspath('.'))
sys.path.insert(0, os.path.abspath('..'))

# -- General configuration ----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions # coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'translate_docs',
    'sphinx.ext.autodoc',
    'sphinx.ext.coverage',
    'sphinx.ext.extlinks',
    'sphinx.ext.intersphinx',
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Translate Toolkit'
copyright = u'2002-2013, Translate'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.11.0'
# The full version, including alpha/beta/rc tags.
release = '1.11.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build', '_themes/README.rst', 'releases/README.rst']

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# -- Missing modules --------------------------------------------------
import sys


class Mock(object):
    VERSION = None

    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return Mock()

    @classmethod
    def __getattr__(cls, name):
        if name in ('__file__', '__path__'):
            return '/dev/null'
        elif name[0] == name[0].upper():
            mockType = type(name, (), {'fixtag': None})
            mockType.__module__ = __name__
            return mockType
        else:
            return Mock()

MOCK_MODULES = [
    'aeidon',
    'aeidon.encodings',
    'aeidon.util',
    'aeidon.files',
    'BeautifulSoup',
    'glib',
    'gobject',
    'gtk',
    'iniparse',
    'lucene',
    'PyLucene',
    'vobject',
    'xapian',
    'xml',
    'xml.dom',
    'xml.etree',
    'xml.parsers',
]
for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = Mock()

# Needed for _get_pylucene_version() used by translate.search.indexing.__init__
sys.modules['lucene'].VERSION = "2.3.0"
sys.modules['PyLucene'].VERSION = "2.2.0"

# -- Options for HTML output --------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'sphinx-bootstrap'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
    'nosidebar': True,
}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'TranslateToolkitdoc'


# -- Options for LaTeX output -------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual])
latex_documents = [
  ('index', 'TranslateToolkit.tex', u'Translate Toolkit Documentation',
   u'Translate.org.za', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'translatetoolkit', u'Translate Toolkit Documentation',
     [u'Translate.org.za'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output -----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'TranslateToolkit', u'Translate Toolkit Documentation',
   u'Translate.org.za', 'TranslateToolkit', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Coverage checker options -------------------------------------------------

coverage_ignore_modules = []

coverage_ignore_functions = ['main']

coverage_ignore_classes = []

coverage_write_headline = False

# -- Options for Intersphinx -------------------------------------------------

intersphinx_mapping = {
    'python': ('http://docs.python.org/2.7', None),
    'pytest': ('http://pytest.org/latest/', None),
    'django': ('http://django.readthedocs.org/en/latest/', None),
    'pootle': ('http://docs.translatehouse.org/projects/pootle/en/latest/', None),
    'virtaal': ('http://docs.translatehouse.org/projects/virtaal/en/latest/', None),
    'guide': ('http://docs.translatehouse.org/projects/localization-guide/en/latest/', None),
}


# -- Options for Exernal links -------------------------------------------------

extlinks = {
    # :role: (URL, prefix)
    'bug': ('http://bugs.locamotion.org/show_bug.cgi?id=%s',
            'bug '),
    'man': ('http://linux.die.net/man/1/%s', ''),
    'wiki': ('http://translate.sourceforge.net/wiki/%s', ''),
    'wp': ('http://en.wikipedia.org/wiki/%s', ''),
}

# -- Options for Linkcheck -------------------------------------------------

# Add regex's here for links that should be ignored.
linkcheck_ignore = [
    'http://your_server.com/filename.html',  # Example URL
    '.*localhost.*',
]

########NEW FILE########
__FILENAME__ = translate_docs
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2012 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Sphinx extension with custom stuff for Translate Toolkit docs."""

import docutils


def setup(app):
    # :opt: to mark options -P --pot and options values --progress=dots
    app.add_generic_role(
        name="opt",
        nodeclass=docutils.nodes.literal
    )

########NEW FILE########
__FILENAME__ = test_odf_xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import difflib
import os
import os.path as path
import sys
import zipfile

from lxml import etree

# get directory of this test
dir = os.path.dirname(os.path.abspath(__file__))
# get top-level directory (moral equivalent of ../..)
dir = os.path.dirname(os.path.dirname(dir))
# load python modules from top-level
sys.path.insert(0, dir)

from translate.convert import odf2xliff, xliff2odf
from translate.storage import factory, xliff


def setup_module(module):
    os.chdir(path.dirname(__file__))


def args(src, tgt, **kwargs):
    arg_list = []
    arg_list.extend([u'--errorlevel=traceback', src, tgt])
    for flag, value in kwargs.iteritems():
        value = unicode(value)
        if len(flag) == 1:
            arg_list.append(u'-%s' % flag)
        else:
            arg_list.append(u'--%s' % flag)
        if value is not None:
            arg_list.append(value)
    return arg_list


def xliff___eq__(self, other):
    return self.units == other.units

xliff.xlifffile.__eq__ = xliff___eq__


def print_diff(store1, store2):
    for line in difflib.unified_diff(str(store1).split('\n'), str(store2).split('\n')):
        print(line)

SOURCE_ODF = u'test_2.odt'
REFERENCE_XLF = u'test_2-test_odf2xliff-reference.xlf'
GENERATED_XLF_ITOOLS = u'test_2-test_odf2xliff-itools.xlf'
GENERATED_XLF_TOOLKIT = u'test_2-test_odf2xliff-toolkit.xlf'

TARGET_XLF = u'test_2-test_roundtrip.xlf'
REFERENCE_ODF = u'test_2.odt'
GENERATED_ODF = u'test_2-test_roundtrip-generated.odt'


def test_odf2xliff():
    reference_xlf = factory.getobject(REFERENCE_XLF)

    odf2xliff.main(args(SOURCE_ODF, GENERATED_XLF_TOOLKIT))
    generated_xlf_toolkit = factory.getobject(GENERATED_XLF_TOOLKIT)
    print_diff(reference_xlf, generated_xlf_toolkit)
    assert reference_xlf == generated_xlf_toolkit

    odf2xliff.main(args(SOURCE_ODF, GENERATED_XLF_ITOOLS))
    generated_xlf_itools = factory.getobject(GENERATED_XLF_ITOOLS)
    print_diff(reference_xlf, generated_xlf_itools)
    assert reference_xlf == generated_xlf_itools


def is_content_file(filename):
    return filename in (u'content.xml', u'meta.xml', u'styles.xml')


class ODF(object):

    def __init__(self, filename):
        self.odf = zipfile.ZipFile(filename)

    def _get_data(self, filename):
        return self.odf.read(filename)

    def _get_doc_root(self, filename):
        return etree.tostring(etree.fromstring(self._get_data(filename)), pretty_print=True)

    def __eq__(self, other):
        if other is None:
            return False
        l1 = sorted(zi.filename for zi in self.odf.infolist())
        l2 = sorted(zi.filename for zi in other.odf.infolist())
        if l1 != l2:
            print("File lists don't match:")
            print(l1)
            print(l2)
            return False
        for filename in l1:
            if is_content_file(filename):
                l = self._get_doc_root(filename)
                r = other._get_doc_root(filename)
                if l != r:
                    print("difference for file named", filename)
                    return False
            else:
                if self._get_data(filename) != other._get_data(filename):
                    print("difference for file named", filename)
                    return False
        return True

    def __str__(self):
        return self._get_doc_root('content.xml')


def test_roundtrip():

    odf2xliff.main(args(SOURCE_ODF, TARGET_XLF))
    xliff2odf.main(args(TARGET_XLF, GENERATED_ODF, t=SOURCE_ODF))

    reference_odf = ODF(REFERENCE_ODF)
    generated_odf = ODF(GENERATED_ODF)

    print_diff(reference_odf, generated_odf)
    assert reference_odf == generated_odf


def remove(filename):
    """Removes the file if it exists."""
    if os.path.exists(filename):
        os.unlink(filename)


def teardown_module(module):
    remove(GENERATED_XLF_TOOLKIT)
    remove(GENERATED_ODF)
    remove(GENERATED_XLF_ITOOLS)
    remove(TARGET_XLF)


if __name__ == '__main__':
    setup_module(None)
    test_roundtrip()
    teardown_module(None)

########NEW FILE########
__FILENAME__ = test_xliff_conformance
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os
import os.path as path
import sys
from subprocess import call

from lxml import etree


# get directory of this test
dir = os.path.dirname(os.path.abspath(__file__))
# get top-level directory (moral equivalent of ../..)
dir = os.path.dirname(os.path.dirname(dir))
# load python modules from top-level
sys.path.insert(0, dir)
# add top-level to PYTHONPATH for subprocesses
os.environ["PYTHONPATH"] = os.pathsep.join(sys.path)
# add {top-level}/translate/convert to PATH for [po]o2xliff
os.environ["PATH"] = os.pathsep.join([os.path.join(dir,
                                                   "translate", "convert"),
                                      os.environ["PATH"]])

schema = None


def xmllint(fullpath):
    return schema.validate(etree.parse(fullpath))


def setup_module(module):
    global schema
    os.chdir(path.dirname(__file__))
    schema = etree.XMLSchema(etree.parse('xliff-core-1.1.xsd'))


def find_files(base, check_ext):
    for dirpath, _dirnames, filenames in os.walk(base):
        for filename in filenames:
            fullpath = path.join(dirpath, filename)
            _namepath, ext = path.splitext(fullpath)
            if check_ext == ext:
                yield fullpath


def test_open_office_to_xliff():
    assert call(['oo2xliff', 'en-US.sdf', '-l', 'fr', 'fr']) == 0
    for filepath in find_files('fr', '.xlf'):
        assert xmllint(filepath)
    cleardir('fr')


def test_po_to_xliff():
    OUTPUT = 'af-pootle.xlf'
    assert call(['po2xliff', 'af-pootle.po', OUTPUT]) == 0
    assert xmllint(OUTPUT)


def teardown_module(module):
    pass


def cleardir(testdir):
    """removes the test directory"""
    if os.path.exists(testdir):
        for dirpath, subdirs, filenames in os.walk(testdir, topdown=False):
            for name in filenames:
                os.remove(os.path.join(dirpath, name))
            for name in subdirs:
                os.rmdir(os.path.join(dirpath, name))
    if os.path.exists(testdir):
        os.rmdir(testdir)
    assert not os.path.exists(testdir)

########NEW FILE########
__FILENAME__ = buildxpi
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Create a XPI language pack from Mozilla sources and translated l10n files.
This script has only been tested with Firefox 3.1 beta sources.

(Basically the process described at
https://developer.mozilla.org/en/Creating_a_Language_Pack)

Example usage::

    buildxpi.py -L /path/to/l10n -s /path/to/mozilla-central -o /path/to/xpi_output af ar

- "/path/to/l10n" is the path to a the parent directory of the "af" and "ar"
  directories containing the Afrikaans and Arabic translated files.
- "/path/to/mozilla-central" is the path to the Firefox sources checked out
  from Mercurial. Note that --mozproduct is not specified, because the default
  is "browser". For Thunderbird (>=3.0) it should be "/path/to/comm-central"
  and "--mozproduct mail" should be specified, although this is not yet
  working.
- "/path/to/xpi_output" is the path to the output directory.
- "af ar" are the languages (Afrikaans and Arabic in this case) to build
  language packs for.

NOTE: The .mozconfig in Firefox source directory gets backed up,
overwritten and replaced.
"""

import logging
import os
import re
from glob import glob
from shutil import move, rmtree
from subprocess import PIPE, CalledProcessError, Popen
from tempfile import mkdtemp


logger = logging.getLogger(__name__)


class RunProcessError(CalledProcessError):
    """Subclass of CalledProcessError exception with custom message strings
    """
    _default_message = "Command '%s' returned exit status %d"

    def __init__(self, message=None, **kwargs):
        """Use and strip string message='' from kwargs"""
        self._message = message or self._default_message
        super(RunProcessError, self).__init__(**kwargs)

    def __str__(self):
        """Format exception message string (avoiding TypeErrors)"""
        output = ''
        message = self._message
        if message.count('%') != 2:
            output += message + '\n'
            message = self._default_message

        output += message % (self.cmd, self.returncode)
        return output


def run(cmd, expected_status=0, fail_msg=None, stdout=-1, stderr=-1):
    """Run a command
    """
    # Default is to capture (and log) std{out,error} unless run as script
    if __name__ == '__main__' or logger.getEffectiveLevel() == logging.DEBUG:
        std = None
    else:
        std = PIPE

    if stdout == -1:
        stdout = std
    if stderr == -1:
        stderr = std

    cmdstring = isinstance(str, basestring) and cmd or ' '.join(cmd)
    logger.debug('>>> %s $ %s', os.getcwd(), cmdstring)
    p = Popen(cmd, stdout=stdout, stderr=stderr)
    (output, error) = p.communicate()
    cmd_status = p.wait()

    if stdout == PIPE:
        if cmd_status != expected_status:
            logger.info('%s', output)
    elif stderr == PIPE:
        logger.warning('%s', error)

    if cmd_status != expected_status:
        raise RunProcessError(returncode=cmd_status, cmd=cmdstring,
                              message=fail_msg)
    return cmd_status


def build_xpi(l10nbase, srcdir, outputdir, langs, product, delete_dest=False,
              soft_max_version=False):
    MOZCONFIG = os.path.join(srcdir, '.mozconfig')
    # Backup existing .mozconfig if it exists
    backup_name = ''
    if os.path.exists(MOZCONFIG):
        backup_name = MOZCONFIG + '.tmp'
        os.rename(MOZCONFIG, backup_name)

    # Create a temporary directory for building
    builddir = mkdtemp('', 'buildxpi')

    try:
        # Create new .mozconfig
        content = """
ac_add_options --disable-compile-environment
ac_add_options --disable-gstreamer
ac_add_options --disable-ogg
ac_add_options --disable-opus
ac_add_options --disable-webrtc
ac_add_options --disable-wave
ac_add_options --disable-webm
ac_add_options --disable-alsa
ac_add_options --disable-pulseaudio
ac_add_options --disable-libjpeg-turbo
mk_add_options MOZ_OBJDIR=%(builddir)s
ac_add_options --with-l10n-base=%(l10nbase)s
ac_add_options --enable-application=%(product)s
""" % \
            {
                'builddir': builddir,
                'l10nbase': l10nbase,
                'product': product
            }

        mozconf = open(MOZCONFIG, 'w').write(content)

        # Try to make sure that "environment shell" is defined
        # (python/mach/mach/mixin/process.py)
        if not any (var in os.environ
                    for var in ('SHELL', 'MOZILLABUILD', 'COMSPEC')):
            os.environ['SHELL'] = '/bin/sh'

        # Start building process.
        # See https://developer.mozilla.org/en/Creating_a_Language_Pack for
        # more details.
        olddir = os.getcwd()
        os.chdir(srcdir)
        run(['make', '-f', 'client.mk', 'configure'],
            fail_msg="Build environment error - "
                     "check logs, fix errors, and try again")

        os.chdir(builddir)
        run(['make', '-C', 'config'],
            fail_msg="Unable to successfully configure build for XPI!")

        moz_app_version = []
        if soft_max_version:
            version = open(os.path.join(srcdir, product, 'config', 'version.txt')).read().strip()
            version = re.sub(r'(^[0-9]*\.[0-9]*).*', r'\1.*', version)
            moz_app_version = ['MOZ_APP_MAXVERSION=%s' % version]
        run(['make', '-C', os.path.join(product, 'locales')] +
            ['langpack-%s' % lang for lang in langs] + moz_app_version,
            fail_msg="Unable to successfully build XPI!")

        destfiles = []
        for lang in langs:
            xpiglob = glob(
                os.path.join(
                    builddir,
                    product == 'mail' and 'mozilla' or '',
                    'dist',
                    '*',
                    'xpi',
                    '*.%s.langpack.xpi' % lang
                )
            )[0]
            filename = os.path.split(xpiglob)[1]
            destfile = os.path.join(outputdir, filename)
            destfiles.append(destfile)
            if delete_dest:
                if os.path.isfile(destfile):
                    os.unlink(destfile)
            move(xpiglob, outputdir)

    finally:
        os.chdir(olddir)
        # Clean-up
        rmtree(builddir)
        if backup_name:
            os.remove(MOZCONFIG)
            os.rename(backup_name, MOZCONFIG)

    return destfiles


def create_option_parser():
    from argparse import ArgumentParser
    usage = 'Usage: buildxpi.py [<options>] <lang> [<lang2> ...]'
    p = ArgumentParser(usage=usage)

    p.add_argument(
        '-L', '--l10n-base',
        type=str,
        dest='l10nbase',
        default='l10n',
        help='The directory containing the <lang> subdirectory.'
    )
    p.add_argument(
        '-o', '--output-dir',
        type=str,
        dest='outputdir',
        default='.',
        help='The directory to copy the built XPI to (default: current directory).'
    )
    p.add_argument(
        '-p', '--mozproduct',
        type=str,
        dest='mozproduct',
        default='browser',
        help='The Mozilla product name (default: "browser").'
    )
    p.add_argument(
        '-s', '--src',
        type=str,
        dest='srcdir',
        default='mozilla',
        help='The directory containing the Mozilla l10n sources.'
    )
    p.add_argument(
        '-d', '--delete-dest',
        dest='delete_dest',
        action='store_true',
        default=False,
        help='Delete output XPI if it already exists.'
    )

    p.add_argument(
        '-v', '--verbose',
        dest='verbose',
        action='store_true',
        default=False,
        help='Be more noisy'
    )

    p.add_argument(
        '--soft-max-version',
        dest='soft_max_version',
        action='store_true',
        default=False,
        help='Override a fixed max version with one to cover the whole cycle '
             'e.g. 24.0a1 becomes 24.0.*'
    )

    p.add_argument(
        "langs",
        nargs="+"
    )

    return p

if __name__ == '__main__':
    args = create_option_parser().parse_args()

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)

    build_xpi(
        l10nbase=os.path.abspath(args.l10nbase),
        srcdir=os.path.abspath(args.srcdir),
        outputdir=os.path.abspath(args.outputdir),
        langs=args.langs,
        product=args.mozproduct,
        delete_dest=args.delete_dest,
        soft_max_version=args.soft_max_version
    )

########NEW FILE########
__FILENAME__ = get_moz_enUS
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of Virtaal.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os
import shutil
from ConfigParser import ConfigParser, NoSectionError


srccheckout = "mozilla"
l10ncheckout = "l10n"
product = "browser"
verbose = False


def path_neutral(path):
    """Convert a path specified using Unix path seperator into a
    platform path"""
    newpath = ""
    for seg in path.split("/"):
        if not seg:
            newpath = os.sep
        newpath = os.path.join(newpath, seg)
    return newpath


def process_l10n_ini(inifile):
    """Read a Mozilla l10n.ini file and process it to find the localisation
    files needed by a project"""

    l10n = ConfigParser()
    l10n.readfp(open(path_neutral(inifile)))
    l10n_ini_path = os.path.dirname(inifile)

    for dir in l10n.get('compare', 'dirs').split():
        frompath = os.path.join(l10n_ini_path, l10n.get('general', 'depth'),
                                dir, 'locales', 'en-US')
        topath = os.path.join(l10ncheckout, 'en-US', dir)
        if not os.path.exists(frompath):
            if verbose:
                print("[Missing source]: %s" % frompath)
            continue
        if os.path.exists(topath):
            if verbose:
                print("[Existing target]: %s" % topath)
            continue
        if verbose:
            print('%s -> %s' % (frompath, topath))
        try:
            shutil.copytree(frompath, topath)
        except OSError as e:
            print(e)

    try:
        for include in l10n.options('includes'):
            include_ini = os.path.join(
                l10n_ini_path, l10n.get('general', 'depth'),
                l10n.get('includes', include)
            )
            if os.path.isfile(include_ini):
                process_l10n_ini(include_ini)
    except TypeError:
        pass
    except NoSectionError:
        pass


def create_option_parser():
    from argparse import ArgumentParser
    p = ArgumentParser()

    p.add_argument(
        '-s', '--src',
        type=str,
        dest='srcdir',
        default='mozilla',
        help='The directory containing the Mozilla l10n sources.'
    )
    p.add_argument(
        '-d', '--dest',
        type=str,
        dest='destdir',
        default='l10n',
        help='The destination directory to copy the en-US locale files to.'
    )
    p.add_argument(
        '-p', '--mozproduct',
        type=str,
        dest='mozproduct',
        default='browser',
        help='The Mozilla product name.'
    )
    p.add_argument(
        '--delete-dest',
        dest='deletedest',
        default=False,
        action='store_true',
        help='Delete the destination directory (if it exists).'
    )

    p.add_argument(
        '-v', '--verbose',
        dest='verbose',
        action='store_true',
        default=False,
        help='Be more noisy'
    )

    return p

if __name__ == '__main__':
    args = create_option_parser().parse_args()
    srccheckout = args.srcdir
    l10ncheckout = args.destdir
    product = args.mozproduct

    enUS_dir = os.path.join(l10ncheckout, 'en-US')
    if args.deletedest and os.path.exists(enUS_dir):
        shutil.rmtree(enUS_dir)
    if not os.path.exists(enUS_dir):
        os.makedirs(enUS_dir)

    if args.verbose:
        print("%s -s %s -d %s -p %s -v %s" %
              (__file__, srccheckout, l10ncheckout, product,
               args.deletedest and '--delete-dest' or ''))
    product_ini = os.path.join(srccheckout, product, 'locales', 'l10n.ini')
    if not os.path.isfile(product_ini):
        # Done for Fennec
        product_ini = os.path.join(srccheckout, 'locales', 'l10n.ini')
    process_l10n_ini(product_ini)

########NEW FILE########
__FILENAME__ = mozcronbuild
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os

from tools.mozilla import moz_l10n_builder


MOZDIR = os.path.join(os.path.expanduser('~'), 'mozbuild')


def build_langs(langs, verbose):
    olddir = os.getcwd()
    os.chdir(MOZDIR)

    moz_l10n_builder.main(
        langs=langs,
        mozcheckout=True,
        recover=True,
        potpack=True,
        potincl=['README.mozilla-pot'],
        popack=True,
        update_trans=True,
        diff=False,
        langpack=True,
        verbose=verbose
    )

    os.chdir(olddir)


def check_potpacks():
    """Copy new and check available POT-packs."""
    pass


def update_rss():
    """Update the RSS feed with the available POT-packs."""
    pass


USAGE = '%prog [<options>]'


def create_option_parser():
    """Creates and returns cmd-line option parser."""

    from argparse import ArgumentParser

    parser = ArgumentParser(usage=USAGE)
    parser.add_argument(
        '-q', '--quiet',
        dest='verbose',
        action='store_false',
        default=True,
        help='Print as little as possible output.'
    )

    return parser


def main(langs, verbose):
    if not langs:
        langs = ['ALL']

    if not os.path.isdir(MOZDIR):
        os.makedirs(MOZDIR)

    build_langs(langs, verbose)
    check_potpacks()
    update_rss()


def main_cmd_line():
    """Processes command-line arguments and send them to main()."""
    args, langs = create_option_parser().parse_args()

    main(langs, args.verbose)

if __name__ == '__main__':
    main_cmd_line()

########NEW FILE########
__FILENAME__ = moz_l10n_builder
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#
# moz-l10n-builder - takes a set of PO files, migrates them to a Mozilla build
# and creates XPIs and Windows .exe files.

"""Contains a Python-port of the moz-l10n-builder bash script."""

# NOTE: Because this script is adapted from a Bash script, some things in here
#       might be a little less Pythonic. See os.system() calls for more
#       details.

import glob
import os
import shutil
import StringIO
import tempfile
import time
from subprocess import Popen, PIPE, STDOUT

join = os.path.join

try:
    # Make sure that all convertion tools are available
    from translate.convert import moz2po
    from translate.convert import po2moz
    from translate.convert import po2prop
    from translate.convert import txt2po
except ImportError:
    raise Exception('Could not find the Translate Toolkit convertion tools. Please check your installation.')

DEFAULT_TARGET_APP = 'browser'
langpack_release = '1'
targetapp = 'browser'
mozversion = '3'
l10ndir = 'l10n'
mozilladir = "mozilla"
podir = "po"
podir_recover = podir + '-recover'
podir_updated = podir + '-updated'
potpacks = "potpacks"
popacks = 'popacks'
#: Mapping of possible "targetapp"s to product names.
products = {'browser': 'firefox'}

devnull = open(os.devnull, 'wb')
#: Global program options
options = {'verbose': True}
USAGE = 'Usage: %prog [options] <langs...|ALL>'


class CommandError(StandardError):
    """Exception raised if a command does not return its expected value."""

    def __init__(self, cmd, status):
        self.cmd = cmd
        self.status = status

    def __str__(self):
        return '"%s" return unexptected status %d' % (self.cmd, self.status)


##### Utility Functions #####


def delfiles(pattern, path, files):
    """Delete files with names in C{files} matching glob-pattern C{glob} in the
        directory specified by C{path}.

        This function is meant to be used with C{os.walk}
        """
    path = os.path.abspath(path)
    match_files = glob.glob(join(path, pattern))
    for f in files:
        if join(path, f) in match_files:
            os.unlink(join(path, f))


def run(cmd, expected_status=0, stdout=None, stderr=None, shell=False):
    global options
    if options['verbose']:
        print('>>> %s $ %s' % (os.getcwd(), ' '.join(cmd)))
    p = Popen(cmd, stdout=stdout, stderr=stderr, shell=shell)
    cmd_status = p.wait()

    if stdout == PIPE:
        print(p.stdout.read())
    elif stderr == PIPE:
        print(p.stderr.read())

    if cmd_status != expected_status:
        print('!!! "%s" returned unexpected status %d' % (' '.join(cmd),
                                                          cmd_status))
        #raise CommandError(cmd, cmd_status)


def get_langs(lang_args):
    """Returns the languages to handle based on the languages specified on the
        command-line.

        If "ALL" was specified, the languages are read from the Mozilla
        product's C{shipped-locales} file. If "ZA" was specified, all South
        African languages are selected.
        """

    langs = []

    if isinstance(lang_args, str):
        if lang_args == 'ALL':
            lang_args = ['ALL']
        elif lang_args == 'ZA':
            lang_args = ['ZA']
        else:
            lang_args = []

    if not lang_args:
        print(USAGE)
        exit(1)

    for lang in lang_args:
        if lang == 'ALL':
            # Get all available languages from the locales file
            locales_filename = join(mozilladir, targetapp,
                                    'locales', 'shipped-locales')
            for line in open(locales_filename).readlines():
                langcode = line.split()[0]
                if langcode != 'en-US':
                    langs.append(langcode)

        elif lang == 'ZA':
            # South African languages
            langs = langs + ["af", "en_ZA", "nr", "nso", "ss",
                             "st", "tn", "ts", "ve", "xh", "zu"]
        elif lang != 'en-US':
            langs.append(lang)

    langs = list(set(langs))  # Remove duplicates from langs

    print('Selected languages: %s' % (' '.join(langs)))

    return langs
#############################


def checkout(cvstag, langs):
    """Check-out needed files from Mozilla's CVS."""

    olddir = os.getcwd()
    if cvstag != '-A':
        cvstag = "-r %s" % (cvstag)

    if not os.path.exists(mozilladir):
        run(['cvs', '-d:pserver:anonymous@cvs-mirror.mozilla.org:/cvsroot',
             'co', cvstag, join(mozilladir, 'client.mk')])
        run(['cvs', '-d:pserver:anonymous@cvs-mirror.mozilla.org:/cvsroot',
             'co', join(mozilladir, 'tools', 'l10n')])

    os.chdir(mozilladir)
    run(['cvs', 'up', cvstag, 'client.mk'])
    run(['make', '-f', 'client.mk', 'l10n-checkout',
         'MOZ_CO_PROJECT=%s' % (targetapp)])
    os.chdir(olddir)

    if not os.path.exists(l10ndir):
        run(['cvs', '-d:pserver:anonymous@cvs-mirror.mozilla.org:/l10n',
             'co', '-d', l10ndir, '-l', 'l10n'])

    os.chdir(l10ndir)
    for lang in langs:
        print('    %s' % (lang))
        buildlang = lang.replace('_', '-')
        if os.path.isdir(buildlang):
            run(['cvs', 'up', buildlang])
        else:
            run(['cvs', '-d:pserver:anonymous@cvs-mirror.mozilla.org:/l10n',
                 'co', '-d', buildlang, join('l10n', buildlang)])
    os.chdir(olddir)

    # Make latest POT file
    for rmdir in ('en-US', 'pot'):
        try:
            shutil.rmtree(join(l10ndir, rmdir))
        except OSError as oe:
            # "No such file or directory" errors are fine.
            # The rest we raise again.
            if oe.errno != 2:
                raise oe

    os.chdir(mozilladir)
    run(['cvs', 'up', join('tools', 'l10n')])
    run(['python', 'tools/l10n/l10n.py',
         '--dest=' + join(os.pardir, l10ndir),
         '--app=' + targetapp,
         'en-US'])
    os.chdir(olddir)

    os.chdir(l10ndir)
    run(['moz2po', '--progress=none', '-P', '--duplicates=msgctxt',
         'en-US', 'pot'])

    # Delete the help-related POT-files, seeing as Firefox help is now on-line.
    try:
        shutil.rmtree(join('pot', 'browser', 'chrome', 'help'))
    except OSError as oe:
        # "No such file or directory" errors are fine. The rest we raise again.
        if oe.errno != 2:
            raise oe

    if mozversion < '3':
        for f in ['en-US/browser/README.txt pot/browser/README.txt.pot',
                  'en-US/browser/os2/README.txt pot/browser/os2/README.txt.pot',
                  'en-US/mail/README.txt pot/mail/README.txt.pot',
                  'en-US/mail/os2/README.txt pot/mail/os2/README.txt.pot',]:
            run(['txt2po', '--progress=none', '-P', f])
    os.chdir(olddir)


def recover_lang(lang, buildlang):
    print('    %s' % (lang))
    if not os.path.isdir(join(podir_recover, buildlang)):
        os.makedirs(join(podir_recover, buildlang))

    run(['moz2po', '--progress=none', '--errorlevel=traceback',
         '--duplicates=msgctxt', '--exclude=".#*"',
         '-t', join(l10ndir, 'en-US'),
         join(l10ndir, buildlang),
         join(podir_recover, buildlang)])


def pack_pot(includes):
    timestamp = time.strftime('%Y%m%d')

    inc = []
    for fn in includes:
        if not os.path.exists(fn):
            print('!!! Warning: Path "%s" does not exist. Skipped.' % (fn))
        else:
            inc.append(fn)

    try:
        os.makedirs(potpacks)
    except OSError:
        pass

    packname = join(potpacks, '%s-%s-%s' % (products[targetapp],
                                            mozversion, timestamp))
    run(['tar', 'cjf', packname + '.tar.bz2',
         join(l10ndir, 'en-US'), join(l10ndir, 'pot')] + inc)
    run(['zip', '-qr9', packname + '.zip',
         join(l10ndir, 'en-US'), join(l10ndir, 'pot')] + inc)


def pack_po(lang, buildlang):
    timestamp = time.strftime('%Y%m%d')

    try:
        os.makedirs(popacks)
    except OSError:
        pass

    print('    %s' % (lang))
    packname = join(popacks, '%s-%s-%s-%s' % (products[targetapp], mozversion,
                                              buildlang, timestamp))
    run(['tar', 'cjf', packname + '.tar.bz2', '--exclude', '.svn',
         join(l10ndir, buildlang), join(podir, buildlang)])
    run(['zip', '-qr9', packname + '.zip', join(l10ndir, buildlang),
         join(podir, buildlang)], '-x', '*.svn*')


def pre_po2moz_hacks(lang, buildlang, debug):
    """Hacks that should be run before running C{po2moz}."""

    # Protect the real original PO dir
    temp_po = tempfile.mkdtemp()
    shutil.copytree(join(podir, buildlang), join(temp_po, buildlang))

    # Fix for languages that have no Windows codepage
    if lang == 've':
        srcs = glob.glob(join(podir, 'en_ZA',
                              'browser', 'installer', '*.properties'))
        dest = join(temp_po, buildlang, 'browser', 'installer')

        for src in srcs:
            shutil.copy2(src, dest)

    old = join(temp_po, buildlang)
    new = join(podir_updated, buildlang)
    templates = join(l10ndir, 'pot')
    run(['pomigrate2', '--use-compendium', '--quiet', '--pot2po',
         old, new, templates])

    for dirpath, dirnames, filenames in os.walk(join(podir_updated, buildlang)):
        delfiles("*.html.po", dirpath, dirnames + filenames)
        delfiles("*.xhtml.po", dirpath, dirnames + filenames)

    if debug:
        olddir = os.getcwd()
        os.chdir(join("%s" % (podir_updated), buildlang))
        run(['podebug', '--progress=none', '--errorlevel=traceback',
             '--ignore=mozilla',
             '.', '.'])
        os.chdir(olddir)

    # Create l10n related files
    if os.path.isdir(join(l10ndir, buildlang)):
        for dirpath, dirnames, filenames in os.walk(join(l10ndir, buildlang)):
            delfiles("*.dtd", dirpath, dirnames + filenames)
            delfiles("*.properties", dirpath, dirnames + filenames)

    shutil.rmtree(temp_po)


def post_po2moz_hacks(lang, buildlang):
    """Hacks that should be run after running C{po2moz}."""

    # Hack to fix creating Thunderber installer
    inst_inc_po = join(podir_updated, lang,
                       'mail', 'installer', 'installer.inc.po')
    if os.path.isfile(inst_inc_po):
        tempdir = tempfile.mkdtemp()
        tmp_po = join(tempdir, 'installer.%s.properties.po' % (lang))
        shutil.copy2(inst_po, tmp_po)

        inst_inc = join(l10ndir, 'en-US', 'mail', 'installer', 'installer.inc')
        tmp_properties = join(tempdir, 'installer.properties')
        shutil.copy2(inst_inc, tmp_properties)

        run(['po2prop', '--progress=none', '--errorlevel=traceback',
             '-t', tmp_properties,  # -t /tmp/installer.properties
             tmp_po,                # /tmp/installer.$lang.properties.po
             tmp_po[:-3]])          # /tmp/installer.$lang.properties

        # mv /tmp/installer.$lang.properties \
        #    $l10ndir/$buildlang/mail/installer/installer.inc
        shutil.move(
            tmp_po[:-3],
            join(l10ndir, buildlang, 'mail', 'installer', 'installer.inc')
        )

        shutil.rmtree(tempdir)

    def copyfile(filename, language):
        enUS = join(l10ndir, 'en-US')
        dir, filename = os.path.split(filename)

        if dir.startswith(enUS):
            dir = dir[len(enUS)+1:]

        if os.path.isfile(join(enUS, dir, filename)):
            try:
                os.makedirs(join(l10ndir, language, dir))
            except OSError:
                pass  # Don't worry if the directory already exists
            shutil.copy2(
                join(enUS, dir, filename),
                join(l10ndir, language, dir)
            )

    def copyfiletype(filetype, language):
        for dirpath, dirnames, filenames in os.walk(join(l10ndir, "en-US")):
            for f in dirnames + filenames:
                if f.endswith(filetype):
                    copyfile(join(dirpath, f), language)

    # Copy and update non-translatable files
    for ft in ('.xhtml', '.html', '.rdf'):
        copyfiletype(ft, buildlang)

    for f in (join('browser', 'extra-jar.mn'),
              join('browser', 'firefox-l10n.js'),
              join('browser', 'README.txt'),
              join('browser', 'microsummary-generators', 'list.txt'),
              join('browser', 'profile', 'chrome', 'userChrome-example.css'),
              join('browser', 'profile', 'chrome', 'userContent-example.css'),
              join('browser', 'searchplugins', 'list.txt'),
              join('extensions', 'reporter', 'chrome',
                   'reporterOverlay.properties'),
              join('mail', 'all-l10n.js'),
              join('toolkit', 'chrome', 'global', 'intl.css'),
              join('toolkit', 'installer', 'windows', 'charset.mk')):
        copyfile(f, buildlang)


def migrate_lang(lang, buildlang, recover, update_transl, debug):
    print('    %s' % (lang))

    if recover and not os.path.isdir(join(podir, buildlang)):
        # If we recovered the .po files for lang, but there is no other po
        # directory, we use the recovered .po files
        try:
            os.mkdir(podir)
        except OSError as oe:
            # "File exists" errors are fine. The rest we raise again.
            if oe.errno != 17:
                raise oe

        shutil.copytree(join(podir_recover, buildlang), join(podir, buildlang))

    if update_transl:
        olddir = os.getcwd()
        os.chdir(podir)
        run(['svn', 'up', buildlang])
        os.chdir(olddir)

        os.chdir(l10ndir)
        run(['cvs', 'up', buildlang])
        os.chdir(olddir)

    # Migrate language from current PO to latest POT
    if os.path.isdir(join(podir, '.svn')):
        shutil.rmtree(join(podir_updated, '.svn'))
        shutil.copytree(join(podir, '.svn'), podir_updated)
    if os.path.isdir(join(podir_updated, buildlang)):
        shutil.rmtree(join(podir_updated, buildlang))
    shutil.copytree(join(podir, buildlang), join(podir_updated, buildlang))

    for dirpath, dirnames, filenames in os.walk(join(podir_updated, buildlang)):
        delfiles("*.po", dirpath, dirnames + filenames)

    pre_po2moz_hacks(lang, buildlang, debug)

    ###################################################
    args = [
        '--progress=none',
        '--errorlevel=traceback',
        '--exclude=".svn"',
        '-t', join(l10ndir, 'en-US'),
        '-i', join(podir_updated, buildlang),
        '-o', join(l10ndir, buildlang)
    ]

    if debug:
        args.append('--fuzzy')

    run(['po2moz'] + args)
    ###################################################

    post_po2moz_hacks(lang, buildlang)

    # Clean up where we made real tabs \t
    if mozversion < '3':
        run(['sed', '-i', '"/^USAGE_MSG/s/\\\t/\t/g"',
             join(l10ndir, buildlang,
                  'toolkit', 'installer', 'unix', 'install.it')])
        run(['sed', '-i', '"/^#define MSG_USAGE/s/\\\t/\t/g"',
             join(l10ndir, buildlang,
                  'browser', 'installer', 'installer.inc')])

    # Fix bookmark file to point to the locale
    # FIXME - need some way to preserve this file if its been translated
    # already
    run(['sed', '-i', 's/en-US/%s/g' % (buildlang),
         join(l10ndir, buildlang, 'browser', 'profile', 'bookmarks.html')])


def create_diff(lang, buildlang):
    """Create CVS-diffs for all languages."""

    if not os.path.isdir('diff'):
        os.mkdir('diff')

    print('    %s' % (lang))
    olddir = os.getcwd()

    os.chdir(l10ndir)
    outfile = join(os.pardir, 'diff', buildlang + '-l10n.diff')
    run(['cvs', 'diff', '--newfile', buildlang], stdout=open(outfile, 'w'))
    os.chdir(olddir)

    os.chdir(join(podir_updated, buildlang))
    outfile = join(os.pardir, os.pardir, 'diff', buildlang + '-po.diff')
    run(['svn', 'diff',
         '--diff-cmd',
         'diff -x "-u --ignore-matching-lines=^\"POT\|^\"X-Gene"'],
         stdout=open(outfile, 'w'))
    os.chdir(olddir)


def create_langpack(lang, buildlang):
    """Builds a XPI and installers for languages."""

    print('    %s' % (lang))

    olddir = os.getcwd()

    os.chdir(mozilladir)
    run(['./configure', '--disable-compile-environment', '--disable-xft',
         '--enable-application=%s' % (targetapp)])
    os.chdir(olddir)

    os.chdir(join(mozilladir, targetapp, 'locales'))
    langpack_name = 'langpack-' + buildlang
    moz_brand_dir = join('other-licenses', 'branding', 'firefox')
    langpack_file = join("'$(_ABS_DIST)'", 'install',
                         "Firefox-Languagepack-'$(MOZ_APP_VERSION)'-%s.'$(AB_CD)'.xpi" % langpack_release)
    run(['make', langpack_name, 'MOZ_BRANDING_DIRECTORY=' + moz_brand_dir,
         'LANGPACK_FILE=' + langpack_file])
    # The commented out (and very long) line below was found commented
    # out in the source script as well.
    #( cd $mozilladir/$targetapp/locales; make repackage-win32-installer-af MOZ_BRANDING_DIRECTORY=other-licenses/branding/firefox WIN32_INSTALLER_IN=../../../Firefox-Setup-2.0.exe WIN32_INSTALLER_OUT='$(_ABS_DIST)'"/install/sea/Firefox-Setup-"'$(MOZ_APP_VERSION).$(AB_CD)'".exe" )
    os.chdir(olddir)


def create_option_parser():
    """Creates and returns cmd-line option parser."""

    from argparse import ArgumentParser

    parser = ArgumentParser(usage=USAGE)

    parser.add_argument(
        '-q', '--quiet',
        dest='verbose',
        action='store_false',
        default=True,
        help='Print as little as possible output.'
    )
    parser.add_argument(
        '--mozilla-product',
        dest='mozproduct',
        default=DEFAULT_TARGET_APP,
        help='Which product to build'
    )
    parser.add_argument(
        '--mozilla-checkout',
        dest='mozcheckout',
        action='store_true',
        default=False,
        help="Update of the Mozilla l10n files and POT files"
    )
    parser.add_argument(
        '--recover',
        dest='recover',
        action='store_true',
        default=False,
        help="build PO files from Mozilla's l10n files"
    )
    parser.add_argument(
        '--mozilla-tag',
        dest='moztag',
        default='-A',
        help='The tag to check out of CVS (implies --mozilla-checkout)'
    )
    parser.add_argument(
        '--update-translations',
        dest='update_translations',
        action='store_true',
        default=False,
        help="Update translations"
    )
    parser.add_argument(
        '--diff',
        dest='diff',
        action='store_true',
        default=False,
        help='Create diffs for migrated translations and localized Mozilla files'
    )
    parser.add_argument(
        '--potpack',
        dest='potpack',
        action='store_true',
        default=False,
        help="Create packages of the en-US and POT directories with today's timestamp"
    )
    parser.add_argument(
        '--pot-include',
        dest='potincl',
        action='append',
        default=[],
        help='Files to include in the POT pack (only used with --potpack)'
    )
    parser.add_argument(
        '--nomigrate',
        dest='migrate',
        action='store_false',
        default=True,
        help="Don't migrate"
    )
    parser.add_argument(
        '--popack',
        dest='popack',
        action='store_true',
        default=False,
        help="Create packages of all specified languages' PO-files with today's timestamp"
    )
    parser.add_argument(
        '--langpack',
        dest='langpack',
        action='store_true',
        default=False,
        help="Build a langpack"
    )
    parser.add_argument(
        '--debug',
        dest='debug',
        action='store_true',
        default=False,
        help="Add podebug debug markers"
    )

    return parser


def main(langs=['ALL'], mozproduct='browser', mozcheckout=False, moztag='-A',
         recover=False, potpack=False, potincl=[], migrate=True, popack=False,
         update_trans=False, debug=False, diff=False, langpack=False,
         verbose=True):
    global options
    options['verbose'] = verbose
    targetapp = mozproduct
    langs = get_langs(langs)

    if mozcheckout:
        print('Checking out')
        checkout(moztag, langs)

    if potpack:
        print('Packing POT files')
        pack_pot(potincl)

    for lang in langs:
        buildlang = lang.replace('_', '-')

        if recover:
            print('Recovering')
            recover_lang(lang, buildlang)

        if migrate:
            print('Migrating')
            migrate_lang(lang, buildlang, recover, update_trans, debug)

        if popack:
            print('Creating PO-packs')
            pack_po(lang, buildlang)

        if diff:
            print('Creating diffs')
            create_diff(lang, buildlang)

        if langpack:
            print('Creating langpacks')
            create_langpack(lang, buildlang)

    print('FIN')
    devnull.close()


def main_cmd_line():
    options, langs = create_option_parser().parse_known_args()

    main(
        langs=langs,
        mozproduct=targetapp,
        mozcheckout=args.mozcheckout,
        moztag=args.moztag,
        recover=args.recover,
        potpack=args.potpack,
        potincl=args.potincl,
        migrate=args.migrate,
        popack=args.popack,
        update_trans=args.update_translations,
        debug=args.debug,
        diff=args.diff,
        langpack=args.langpack,
        verbose=args.verbose
    )


if __name__ == '__main__':
    main_cmd_line()

########NEW FILE########
__FILENAME__ = accesskey
# -*- coding: utf-8 -*-
#
# Copyright 2002-2009,2011 Zuza Software Foundation
#
# This file is part of The Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""functions used to manipulate access keys in strings"""

from translate.storage.placeables.general import XMLEntityPlaceable


DEFAULT_ACCESSKEY_MARKER = u"&"


class UnitMixer(object):
    """Helper to mix separately defined labels and accesskeys into one unit."""

    def __init__(self, labelsuffixes, accesskeysuffixes):
        self.labelsuffixes = labelsuffixes
        self.accesskeysuffixes = accesskeysuffixes

    def match_entities(self, index):
        """Populates mixedentities from the index."""
        #: Entities which have a .label/.title and .accesskey combined
        mixedentities = {}
        for entity in index:
            for labelsuffix in self.labelsuffixes:
                if entity.endswith(labelsuffix):
                    entitybase = entity[:entity.rfind(labelsuffix)]
                    # see if there is a matching accesskey in this line,
                    # making this a mixed entity
                    for akeytype in self.accesskeysuffixes:
                        if (entitybase + akeytype) in index:
                            # add both versions to the list of mixed entities
                            mixedentities[entity] = {}
                            mixedentities[entitybase+akeytype] = {}
                    # check if this could be a mixed entity (labelsuffix and
                    # ".accesskey")
        return mixedentities

    def mix_units(self, label_unit, accesskey_unit, target_unit):
        """Mix the given units into the given target_unit if possible.

        Might return None if no match is possible.
        """
        target_unit.addlocations(label_unit.getlocations())
        target_unit.addlocations(accesskey_unit.getlocations())
        target_unit.msgidcomment = target_unit._extract_msgidcomments() + \
                             label_unit._extract_msgidcomments()
        target_unit.msgidcomment = target_unit._extract_msgidcomments() + \
                             accesskey_unit._extract_msgidcomments()
        target_unit.addnote(label_unit.getnotes("developer"), "developer")
        target_unit.addnote(accesskey_unit.getnotes("developer"), "developer")
        target_unit.addnote(label_unit.getnotes("translator"), "translator")
        target_unit.addnote(accesskey_unit.getnotes("translator"), "translator")
        label = label_unit.source
        accesskey = accesskey_unit.source
        label = combine(label, accesskey)
        if label is None:
            return None
        target_unit.source = label
        target_unit.target = ""
        return target_unit

    def find_mixed_pair(self, mixedentities, store, unit):
        entity = unit.getid()
        if entity not in mixedentities:
            return None, None

        # depending on what we come across first, work out the label
        # and the accesskey
        labelentity, accesskeyentity = None, None
        for labelsuffix in self.labelsuffixes:
            if entity.endswith(labelsuffix):
                entitybase = entity[:entity.rfind(labelsuffix)]
                for akeytype in self.accesskeysuffixes:
                    if (entitybase + akeytype) in store.id_index:
                        labelentity = entity
                        accesskeyentity = labelentity[:labelentity.rfind(labelsuffix)] + akeytype
                        break
        else:
            for akeytype in self.accesskeysuffixes:
                if entity.endswith(akeytype):
                    accesskeyentity = entity
                    for labelsuffix in self.labelsuffixes:
                        labelentity = accesskeyentity[:accesskeyentity.rfind(akeytype)] + labelsuffix
                        if labelentity in store.id_index:
                            break
                    else:
                        labelentity = None
                        accesskeyentity = None
        return (labelentity, accesskeyentity)


def extract(string, accesskey_marker=DEFAULT_ACCESSKEY_MARKER):
    """Extract the label and accesskey from a label+accesskey string

    The function will also try to ignore &entities; which would obviously not
    contain accesskeys.

    :type string: Unicode
    :param string: A string that might contain a label with accesskey marker
    :type accesskey_marker: Char
    :param accesskey_marker: The character that is used to prefix an access key
    """
    assert isinstance(string, unicode)
    assert isinstance(accesskey_marker, unicode)
    assert len(accesskey_marker) == 1
    if string == u"":
        return u"", u""
    accesskey = u""
    label = string
    marker_pos = 0
    while marker_pos >= 0:
        marker_pos = string.find(accesskey_marker, marker_pos)
        if marker_pos != -1:
            marker_pos += 1
            if marker_pos == len(string):
                break
            if (accesskey_marker == '&' and
                XMLEntityPlaceable.regex.match(string[marker_pos-1:])):
                continue
            label = string[:marker_pos-1] + string[marker_pos:]
            if string[marker_pos] != " ":  # FIXME weak filtering
                accesskey = string[marker_pos]
    return label, accesskey


def combine(label, accesskey,
            accesskey_marker=DEFAULT_ACCESSKEY_MARKER):
    """Combine a label and and accesskey to form a label+accesskey string

    We place an accesskey marker before the accesskey in the label and this
    creates a string with the two combined e.g. "File" + "F" = "&File"

    The case of the accesskey is preferred unless no match is found, in which
    case the alternate case is used.

    :type label: unicode
    :param label: a label
    :type accesskey: unicode char
    :param accesskey: The accesskey
    :rtype: unicode or None
    :return: label+accesskey string or None if uncombineable
    """
    assert isinstance(label, unicode)
    assert isinstance(accesskey, unicode)

    if len(accesskey) == 0:
        return None

    searchpos = 0
    accesskeypos = -1
    in_entity = False
    accesskeyaltcasepos = -1

    if accesskey.isupper():
        accesskey_alt_case = accesskey.lower()
    else:
        accesskey_alt_case = accesskey.upper()

    while (accesskeypos < 0) and searchpos < len(label):
        searchchar = label[searchpos]
        if searchchar == '&':
            in_entity = True
        elif searchchar == ';' or searchchar == " ":
            in_entity = False
        if not in_entity:
            if searchchar == accesskey:  # Prefer supplied case
                accesskeypos = searchpos
            elif searchchar == accesskey_alt_case:  # Other case otherwise
                if accesskeyaltcasepos == -1:
                    # only want to remember first altcasepos
                    accesskeyaltcasepos = searchpos
                    # note: we keep on looping through in hope
                    # of exact match
        searchpos += 1

    # if we didn't find an exact case match, use an alternate one if available
    if accesskeypos == -1:
        accesskeypos = accesskeyaltcasepos

    # now we want to handle whatever we found...
    if accesskeypos >= 0:
        return label[:accesskeypos] + accesskey_marker + label[accesskeypos:]
    # can't currently mix accesskey if it's not in label
    return None

########NEW FILE########
__FILENAME__ = convert
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Handles converting of files between formats (used by
:mod:`translate.convert` tools)."""

import os.path
from cStringIO import StringIO

from translate.misc import optrecurse


# Don't import optparse ourselves, get the version from optrecurse.
optparse = optrecurse.optparse


class ConvertOptionParser(optrecurse.RecursiveOptionParser, object):
    """A specialized Option Parser for convertor tools..."""

    def __init__(self, formats, usetemplates=False, usepots=False,
                 allowmissingtemplate=False, description=None):
        """construct the specialized Option Parser"""
        optrecurse.RecursiveOptionParser.__init__(self, formats, usetemplates,
                                                  allowmissingtemplate=allowmissingtemplate,
                                                  description=description)
        self.usepots = usepots
        self.settimestampoption()
        self.setpotoption()
        self.set_usage()

    def add_fuzzy_option(self, default=False):
        """Adds an option to include / exclude fuzzy translations."""
        fuzzyhelp = "use translations marked fuzzy"
        nofuzzyhelp = "don't use translations marked fuzzy"
        if default:
            fuzzyhelp += " (default)"
        else:
            nofuzzyhelp += " (default)"
        self.add_option("", "--fuzzy", dest="includefuzzy",
                        action="store_true", default=default, help=fuzzyhelp)
        self.add_option("", "--nofuzzy", dest="includefuzzy",
                        action="store_false", default=default, help=nofuzzyhelp)
        self.passthrough.append("includefuzzy")

    def add_threshold_option(self, default=None):
        """Adds an option to output only stores where translation percentage
        exceeds the threshold.
        """
        self.add_option("", "--threshold", dest="outputthreshold", default=default,
                        metavar="PERCENT", type="int",
                        help="only convert files where the translation completion is above PERCENT")
        self.passthrough.append("outputthreshold")

    def add_duplicates_option(self, default="msgctxt"):
        """Adds an option to say what to do with duplicate strings."""
        self.add_option(
            "", "--duplicates", dest="duplicatestyle", default=default,
            type="choice", choices=["msgctxt", "merge"],
            help="what to do with duplicate strings (identical source text): merge, msgctxt (default: '%s')" %
                 default,
            metavar="DUPLICATESTYLE"
        )
        self.passthrough.append("duplicatestyle")

    def add_multifile_option(self, default="single"):
        """Adds an option to say how to split the po/pot files."""
        self.add_option(
            "", "--multifile",
            dest="multifilestyle", default=default,
            type="choice", choices=["single", "toplevel", "onefile"],
            help="how to split po/pot files (single, toplevel or onefile)",
            metavar="MULTIFILESTYLE"
        )
        self.passthrough.append("multifilestyle")

    def potifyformat(self, fileformat):
        """Converts a .po to a .pot where required."""
        if fileformat is None:
            return fileformat
        elif fileformat == "po":
            return "pot"
        elif fileformat.endswith(os.extsep + "po"):
            return fileformat + "t"
        else:
            return fileformat

    def getformathelp(self, formats):
        """Make a nice help string for describing formats..."""
        # include implicit pot options...
        helpformats = []
        for fileformat in formats:
            helpformats.append(fileformat)
            potformat = self.potifyformat(fileformat)
            if potformat != fileformat:
                helpformats.append(potformat)
        return super(ConvertOptionParser, self).getformathelp(helpformats)

    def filterinputformats(self, options):
        """Filters input formats, processing relevant switches in options."""
        if self.usepots and options.pot:
            return [self.potifyformat(inputformat) for inputformat in self.inputformats]
        else:
            return self.inputformats

    def filteroutputoptions(self, options):
        """Filters output options, processing relevant switches in options."""
        if self.usepots and options.pot:
            outputoptions = {}
            for (inputformat, templateformat), (outputformat, convertor) in self.outputoptions.iteritems():
                inputformat = self.potifyformat(inputformat)
                templateformat = self.potifyformat(templateformat)
                outputformat = self.potifyformat(outputformat)
                outputoptions[(inputformat, templateformat)] = (outputformat, convertor)
            return outputoptions
        else:
            return self.outputoptions

    def setpotoption(self):
        """Sets the ``-P``/``--pot`` option depending on input/output
        formats etc."""
        if self.usepots:
            potoption = optparse.Option(
                "-P", "--pot",
                action="store_true", dest="pot", default=False,
                help="output PO Templates (.pot) rather than PO files (.po)"
            )
            self.define_option(potoption)

    def settimestampoption(self):
        """Sets ``-S``/``--timestamp`` option."""
        timestampopt = optparse.Option(
            "-S", "--timestamp",
            action="store_true", dest="timestamp", default=False,
            help="skip conversion if the output file has newer timestamp"
        )
        self.define_option(timestampopt)

    def verifyoptions(self, options):
        """Verifies that the options are valid (required options are
        present, etc)."""
        pass

    def run(self, argv=None):
        """Parses the command line options and runs the conversion."""
        (options, args) = self.parse_args(argv)
        options.inputformats = self.filterinputformats(options)
        options.outputoptions = self.filteroutputoptions(options)
        try:
            self.verifyoptions(options)
        except Exception as e:
            self.error(str(e))
        self.recursiveprocess(options)

    def processfile(self, fileprocessor, options, fullinputpath,
                    fulloutputpath, fulltemplatepath):
        if options.timestamp and _output_is_newer(fullinputpath, fulloutputpath):
            return False

        return super(ConvertOptionParser,
                    self).processfile(fileprocessor, options,
                                      fullinputpath, fulloutputpath,
                                      fulltemplatepath)


def copyinput(inputfile, outputfile, templatefile, **kwargs):
    """Copies the input file to the output file."""
    outputfile.write(inputfile.read())
    return True


def copytemplate(inputfile, outputfile, templatefile, **kwargs):
    """Copies the template file to the output file."""
    outputfile.write(templatefile.read())
    return True


class Replacer:
    """An object that knows how to replace strings in files."""

    def __init__(self, searchstring, replacestring):
        self.searchstring = searchstring
        self.replacestring = replacestring

    def doreplace(self, text):
        """actually replace the text"""
        if self.searchstring is not None and self.replacestring is not None:
            return text.replace(self.searchstring, self.replacestring)
        else:
            return text

    def searchreplaceinput(self, inputfile, outputfile,
                           templatefile, **kwargs):
        """copies the input file to the output file, searching and replacing"""
        outputfile.write(self.doreplace(inputfile.read()))
        return True

    def searchreplacetemplate(self, inputfile, outputfile,
                              templatefile, **kwargs):
        """Copies the template file to the output file, searching and
        replacing."""
        outputfile.write(self.doreplace(templatefile.read()))
        return True

# archive files need to know how to:
# - openarchive: creates an archive object for the archivefilename
#   * requires a constructor that takes the filename
# - iterarchivefile: iterate through the names in the archivefile
#   * requires the default iterator to do this
# - archivefileexists: check if a given pathname exists inside the archivefile
#   * uses the in operator - requires __contains__ (or will use __iter__
#     by default)
# - openarchiveinputfile: returns an open input file from the archive, given
#   the path
#   * requires an archivefile.openinputfile method that takes the pathname
# - openarchiveoutputfile: returns an open output file from the archive, given
#   the path
#   * requires an archivefile.openoutputfile method that takes the pathname


class ArchiveConvertOptionParser(ConvertOptionParser):
    """ConvertOptionParser that can handle recursing into single archive files.

    ``archiveformats`` maps extension to class. If the extension doesn't
    matter, it can be None.

    If the extension is only valid for input/output/template, it can be
    given as ``(extension, filepurpose)``."""

    def __init__(self, formats, usetemplates=False, usepots=False,
                 description=None, archiveformats=None):
        if archiveformats is None:
            self.archiveformats = {}
        else:
            self.archiveformats = archiveformats
        self.archiveoptions = {}
        ConvertOptionParser.__init__(self, formats, usetemplates, usepots,
                                     description=description)

    def setarchiveoptions(self, **kwargs):
        """Allows setting options that will always be passed to openarchive."""
        self.archiveoptions = kwargs

    def isrecursive(self, fileoption, filepurpose='input'):
        """Checks if **fileoption** is a recursive file."""
        if self.isarchive(fileoption, filepurpose):
            return True
        return super(ArchiveConvertOptionParser, self).isrecursive(fileoption,
                                                                   filepurpose)

    def isarchive(self, fileoption, filepurpose='input'):
        """Returns whether the file option is an archive file."""
        if not isinstance(fileoption, (str, unicode)):
            return False
        mustexist = (filepurpose != 'output')
        if mustexist and not os.path.isfile(fileoption):
            return False
        fileext = self.splitext(fileoption)[1]
        # if None is in the archive formats, then treat all non-directory
        # inputs as archives
        return self.getarchiveclass(fileext, filepurpose, os.path.isdir(fileoption)) is not None

    def getarchiveclass(self, fileext, filepurpose, isdir=False):
        """Returns the archiveclass for the given fileext and filepurpose"""
        archiveclass = self.archiveformats.get(fileext, None)
        if archiveclass is not None:
            return archiveclass
        archiveclass = self.archiveformats.get((fileext, filepurpose), None)
        if archiveclass is not None:
            return archiveclass
        if not isdir:
            archiveclass = self.archiveformats.get(None, None)
            if archiveclass is not None:
                return archiveclass
            archiveclass = self.archiveformats.get((None, filepurpose), None)
            if archiveclass is not None:
                return archiveclass
        return None

    def openarchive(self, archivefilename, filepurpose, **kwargs):
        """Creates an archive object for the given file."""
        archiveext = self.splitext(archivefilename)[1]
        archiveclass = self.getarchiveclass(archiveext, filepurpose,
                                            os.path.isdir(archivefilename))
        archiveoptions = self.archiveoptions.copy()
        archiveoptions.update(kwargs)
        return archiveclass(archivefilename, **archiveoptions)

    def recurseinputfiles(self, options):
        """Recurse through archive file / directories and return files
        to be converted."""
        if self.isarchive(options.input, 'input'):
            options.inputarchive = self.openarchive(options.input, 'input')
            return self.recursearchivefiles(options)
        else:
            return super(ArchiveConvertOptionParser, self).recurseinputfiles(options)

    def recursearchivefiles(self, options):
        """Recurse through archive files and convert files."""
        inputfiles = []
        for inputpath in options.inputarchive:
            if self.isexcluded(options, inputpath):
                continue
            top, name = os.path.split(inputpath)
            if not self.isvalidinputname(options, name):
                continue
            inputfiles.append(inputpath)
        return inputfiles

    def openinputfile(self, options, fullinputpath):
        """Opens the input file."""
        if self.isarchive(options.input, 'input'):
            return options.inputarchive.openinputfile(fullinputpath)
        else:
            return super(ArchiveConvertOptionParser, self).openinputfile(options, fullinputpath)

    def getfullinputpath(self, options, inputpath):
        """Gets the absolute path to an input file."""
        if self.isarchive(options.input, 'input'):
            return inputpath
        else:
            return os.path.join(options.input, inputpath)

    def opentemplatefile(self, options, fulltemplatepath):
        """Opens the template file (if required)."""
        if fulltemplatepath is not None:
            if (options.recursivetemplate and
                self.isarchive(options.template, 'template')):
                # TODO: deal with different names in input/template archives
                if fulltemplatepath in options.templatearchive:
                    return options.templatearchive.openinputfile(fulltemplatepath)
                else:
                    self.warning("missing template file %s" % fulltemplatepath)
        return super(ArchiveConvertOptionParser, self).opentemplatefile(options, fulltemplatepath)

    def getfulltemplatepath(self, options, templatepath):
        """Gets the absolute path to a template file."""
        if templatepath is not None and self.usetemplates and options.template:
            if self.isarchive(options.template, 'template'):
                return templatepath
            elif not options.recursivetemplate:
                return templatepath
            else:
                return os.path.join(options.template, templatepath)
        else:
            return None

    def templateexists(self, options, templatepath):
        """Returns whether the given template exists..."""
        if templatepath is not None:
            if self.isarchive(options.template, 'template'):
                # TODO: deal with different names in input/template archives
                return templatepath in options.templatearchive
        return super(ArchiveConvertOptionParser, self).templateexists(options, templatepath)

    def getfulloutputpath(self, options, outputpath):
        """Gets the absolute path to an output file."""
        if self.isarchive(options.output, 'output'):
            return outputpath
        elif options.recursiveoutput and options.output:
            return os.path.join(options.output, outputpath)
        else:
            return outputpath

    def checkoutputsubdir(self, options, subdir):
        """Checks to see if subdir under ``options.output`` needs to be
        created, creates if neccessary."""
        if not self.isarchive(options.output, 'output'):
            super(ArchiveConvertOptionParser, self).checkoutputsubdir(options, subdir)

    def openoutputfile(self, options, fulloutputpath):
        """Opens the output file."""
        if self.isarchive(options.output, 'output'):
            outputstream = options.outputarchive.openoutputfile(fulloutputpath)
            if outputstream is None:
                self.warning("Could not find where to put %s in output "
                             "archive; writing to tmp" % fulloutputpath)
                return StringIO()
            return outputstream
        else:
            return super(ArchiveConvertOptionParser, self).openoutputfile(options, fulloutputpath)

    def inittemplatearchive(self, options):
        """Opens the ``templatearchive`` if not already open."""
        if not self.usetemplates:
            return
        if (options.template and
            self.isarchive(options.template, 'template') and
            not hasattr(options, "templatearchive")):
            options.templatearchive = self.openarchive(options.template,
                                                       'template')

    def initoutputarchive(self, options):
        """Creates an outputarchive if required."""
        if options.output and self.isarchive(options.output, 'output'):
            options.outputarchive = self.openarchive(options.output, 'output',
                                                     mode="w")

    def recursiveprocess(self, options):
        """Recurse through directories and convert files."""
        if hasattr(options, "multifilestyle"):
            self.setarchiveoptions(multifilestyle=options.multifilestyle)
            for filetype in ("input", "output", "template"):
                allowoption = "allowrecursive%s" % filetype
                if (options.multifilestyle == "onefile" and
                    getattr(options, allowoption, True)):
                    setattr(options, allowoption, False)
        self.inittemplatearchive(options)
        self.initoutputarchive(options)
        return super(ArchiveConvertOptionParser, self).recursiveprocess(options)

    def processfile(self, fileprocessor, options, fullinputpath,
                    fulloutputpath, fulltemplatepath):
        """Run an invidividual conversion."""
        if options.timestamp and _output_is_newer(fullinputpath, fulloutputpath):
            return False

        if self.isarchive(options.output, 'output'):
            inputfile = self.openinputfile(options, fullinputpath)
            # TODO: handle writing back to same archive as input/template
            templatefile = self.opentemplatefile(options, fulltemplatepath)
            outputfile = self.openoutputfile(options, fulloutputpath)
            passthroughoptions = self.getpassthroughoptions(options)
            if fileprocessor(inputfile, outputfile, templatefile,
                             **passthroughoptions):
                if not outputfile.isatty():
                    outputfile.close()
                return True
            else:
                if fulloutputpath and os.path.isfile(fulloutputpath):
                    outputfile.close()
                    os.unlink(fulloutputpath)
                return False
        else:
            return super(ArchiveConvertOptionParser,
                        self).processfile(fileprocessor, options,
                                          fullinputpath, fulloutputpath,
                                          fulltemplatepath)


def _output_is_newer(input_path, output_path):
    """Check if input_path was not modified since output_path was generated,
    used to avoid needless regeneration of output.
    """
    if not input_path or not output_path:
        return False

    if not os.path.exists(input_path) or not os.path.exists(output_path):
        return False

    input_mtime = os.path.getmtime(input_path)
    output_mtime = os.path.getmtime(output_path)

    return output_mtime > input_mtime


def should_output_store(store, threshold):
    """Check if the percent of translated source words more than or equal to
    the given threshold.
    """

    if not threshold:
        return True

    from translate.storage import statsdb

    units = filter(lambda unit: unit.istranslatable(), store.units)
    translated = filter(lambda unit: unit.istranslated(), units)
    wordcounts = dict(map(lambda unit: (unit, statsdb.wordsinunit(unit)), units))
    sourcewords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][0], elementlist))
    tranlated_count = sourcewords(translated)
    total_count = sourcewords(units)
    percent = tranlated_count * 100 / total_count

    return percent >= threshold


def main(argv=None):
    parser = ArchiveConvertOptionParser({}, description=__doc__)
    parser.run(argv)

########NEW FILE########
__FILENAME__ = csv2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2003-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Comma-Separated Value (.csv) files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/csv2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import csvl10n, po


logger = logging.getLogger(__name__)


def replacestrings(source, *pairs):
    """Use ``pairs`` of ``(original, replacement)`` to replace text found in
    ``source``.

    :param source: String to on which ``pairs`` of strings are to be replaced
    :type source: String
    :param \*pairs: Strings to be matched and replaced
    :type \*pairs: One or more tuples of (original, replacement)
    :return: String with ``*pairs`` of strings replaced
    """
    for orig, new in pairs:
        source = source.replace(orig, new)
    return source


def quotecsvstr(source):
    return '"' + \
           replacestrings(source,
                          ('\\"', '"'), ('"', '\\"'),
                          ("\\\\'", "\\'"), ('\\\\n', '\\n')) + \
           '"'


def simplify(string):
    return filter(type(string).isalnum, string)


class csv2po:
    """a class that takes translations from a .csv file and puts them in a
    .po file"""

    def __init__(self, templatepo=None, charset=None, duplicatestyle="keep"):
        """construct the converter..."""
        self.pofile = templatepo
        self.charset = charset
        self.duplicatestyle = duplicatestyle
        self.commentindex = {}
        self.sourceindex = {}
        self.simpleindex = {}
        self.csvfile = None
        self.duplicatecomments = []
        if self.pofile is not None:
            self.unmatched = 0
            self.makeindex()

    def makeindex(self):
        """makes indexes required for searching..."""
        for pounit in self.pofile.units:
            joinedcomment = " ".join(pounit.getlocations())
            source = pounit.source
            # the definitive way to match is by source comment (joinedcomment)
            if joinedcomment in self.commentindex:
                # unless more than one thing matches...
                self.duplicatecomments.append(joinedcomment)
            else:
                self.commentindex[joinedcomment] = pounit
            # do simpler matching in case things have been mangled...
            simpleid = simplify(source)
            # but check for duplicates
            if (simpleid in self.simpleindex and
                not (source in self.sourceindex)):
                # keep a list of them...
                self.simpleindex[simpleid].append(pounit)
            else:
                self.simpleindex[simpleid] = [pounit]
            # also match by standard msgid
            self.sourceindex[source] = pounit
        for comment in self.duplicatecomments:
            if comment in self.commentindex:
                del self.commentindex[comment]

    def convertunit(self, csvunit):
        """converts csv unit to po unit"""
        pounit = po.pounit(encoding="UTF-8")
        if csvunit.location:
            pounit.addlocation(csvunit.location)
        pounit.source = csvunit.source
        pounit.target = csvunit.target
        return pounit

    def handlecsvunit(self, csvunit):
        """handles reintegrating a csv unit into the .po file"""
        if (len(csvunit.location.strip()) > 0 and
            csvunit.location in self.commentindex):
            pounit = self.commentindex[csvunit.location]
        elif csvunit.source in self.sourceindex:
            pounit = self.sourceindex[csvunit.source]
        elif simplify(csvunit.source) in self.simpleindex:
            thepolist = self.simpleindex[simplify(csvunit.source)]
            if len(thepolist) > 1:
                csvfilename = getattr(self.csvfile, "filename", "(unknown)")
                matches = "\n  ".join(["possible match: " +
                                       pounit.source for pounit in thepolist])
                logger.warning("%s - csv entry not unique in pofile, "
                               "multiple matches found:\n"
                               "  location\t%s\n"
                               "  original\t%s\n"
                               "  translation\t%s\n"
                               "  %s",
                               csvfilename, csvunit.location,
                               csvunit.source, csvunit.target, matches)
                self.unmatched += 1
                return
            pounit = thepolist[0]
        else:
            csvfilename = getattr(self.csvfile, "filename", "(unknown)")
            logger.warning("%s - csv entry not found in pofile:\n"
                           "  location\t%s\n"
                           "  original\t%s\n"
                           "  translation\t%s",
                           csvfilename, csvunit.location,
                           csvunit.source, csvunit.target)
            self.unmatched += 1
            return
        if pounit.hasplural():
            # we need to work out whether we matched the singular or the plural
            singularid = pounit.source.strings[0]
            pluralid = pounit.source.strings[1]
            if csvunit.source == singularid:
                pounit.msgstr[0] = csvunit.target
            elif csvunit.source == pluralid:
                pounit.msgstr[1] = csvunit.target
            elif simplify(csvunit.source) == simplify(singularid):
                pounit.msgstr[0] = csvunit.target
            elif simplify(csvunit.source) == simplify(pluralid):
                pounit.msgstr[1] = csvunit.target
            else:
                logger.warning("couldn't work out singular/plural: %r, %r, %r",
                               csvunit.source, singularid, pluralid)
                self.unmatched += 1
                return
        else:
            pounit.target = csvunit.target

    def convertstore(self, thecsvfile):
        """converts a csvfile to a pofile, and returns it. uses templatepo if
        given at construction"""
        self.csvfile = thecsvfile
        if self.pofile is None:
            self.pofile = po.pofile()
            mergemode = False
        else:
            mergemode = True
        if self.pofile.units and self.pofile.units[0].isheader():
            targetheader = self.pofile.units[0]
            self.pofile.updateheader(content_type="text/plain; charset=UTF-8",
                                     content_transfer_encoding="8bit")
        else:
            targetheader = self.pofile.makeheader(charset="UTF-8",
                                                  encoding="8bit")
        targetheader.addnote("extracted from %s" % self.csvfile.filename,
                             "developer")
        mightbeheader = True
        for csvunit in self.csvfile.units:
            #if self.charset is not None:
            #    csvunit.source = csvunit.source.decode(self.charset)
            #    csvunit.target = csvunit.target.decode(self.charset)
            if mightbeheader:
                # ignore typical header strings...
                mightbeheader = False
                if csvunit.match_header():
                    continue
                if (len(csvunit.location.strip()) == 0 and
                    csvunit.source.find("Content-Type:") != -1):
                    continue
            if mergemode:
                self.handlecsvunit(csvunit)
            else:
                pounit = self.convertunit(csvunit)
                self.pofile.addunit(pounit)
        self.pofile.removeduplicates(self.duplicatestyle)
        return self.pofile


def convertcsv(inputfile, outputfile, templatefile, charset=None,
               columnorder=None, duplicatestyle="msgctxt"):
    """reads in inputfile using csvl10n, converts using csv2po, writes to
    outputfile"""
    inputstore = csvl10n.csvfile(inputfile, fieldnames=columnorder)
    if templatefile is None:
        convertor = csv2po(charset=charset, duplicatestyle=duplicatestyle)
    else:
        templatestore = po.pofile(templatefile)
        convertor = csv2po(templatestore, charset=charset,
                           duplicatestyle=duplicatestyle)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
        ("csv", "po"): ("po", convertcsv),
        ("csv", "pot"): ("po", convertcsv),
        ("csv", None): ("po", convertcsv),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True,
                                         description=__doc__)
    parser.add_option("", "--charset", dest="charset", default=None,
        help="set charset to decode from csv files", metavar="CHARSET")
    parser.add_option("", "--columnorder", dest="columnorder", default=None,
        help="specify the order and position of columns (location,source,target)")
    parser.add_duplicates_option()
    parser.passthrough.append("charset")
    parser.passthrough.append("columnorder")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = csv2tbx
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Comma-Separated Value (.csv) files to a TermBase eXchange (.tbx)
glossary file

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/csv2tbx.html
for examples and usage instructions
"""

from translate.storage import csvl10n, tbx


class csv2tbx:
    """a class that takes translations from a .csv file and puts them in a
    .tbx file"""

    def __init__(self, charset=None):
        """construct the converter..."""
        self.charset = charset

    def convertfile(self, csvfile):
        """converts a csvfile to a tbxfile, and returns it. uses templatepo
        if given at construction"""
        mightbeheader = True
        self.tbxfile = tbx.tbxfile()
        for csvunit in csvfile.units:
            if mightbeheader:
                # ignore typical header strings...
                mightbeheader = False
                if csvunit.match_header():
                    continue
                if (len(csvunit.location.strip()) == 0 and
                    csvunit.source.find("Content-Type:") != -1):
                    continue
            term = tbx.tbxunit.buildfromunit(csvunit)
            # TODO: we might want to get the location or other information
            # from CSV
            self.tbxfile.addunit(term)
        return self.tbxfile


def convertcsv(inputfile, outputfile, templatefile, charset=None,
               columnorder=None):
    """reads in inputfile using csvl10n, converts using csv2tbx, writes to
    outputfile"""
    inputstore = csvl10n.csvfile(inputfile, fieldnames=columnorder)
    convertor = csv2tbx(charset=charset)
    outputstore = convertor.convertfile(inputstore)
    if len(outputstore.units) == 0:
        return 0
    outputfile.write(str(outputstore))
    return 1


def main():
    from translate.convert import convert
    formats = {
        ("csv", "tbx"): ("tbx", convertcsv),
        ("csv", None): ("tbx", convertcsv),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=False,
                                         description=__doc__)
    parser.add_option("", "--charset", dest="charset", default=None,
        help="set charset to decode from csv files", metavar="CHARSET")
    parser.add_option("", "--columnorder", dest="columnorder", default=None,
        help="specify the order and position of columns (comment,source,target)")
    parser.passthrough.append("charset")
    parser.passthrough.append("columnorder")
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = dtd2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert a Mozilla .dtd UTF-8 localization format to a
Gettext PO localization file.

Uses the po and dtd modules, and the
dtd2po convertor class which is in this module
You can convert back to .dtd using po2dtd.py.
"""

from translate.convert.accesskey import UnitMixer
from translate.misc import quote
from translate.storage import dtd, po


def is_css_entity(entity):
    """Says if the given entity is likely to contain CSS that should not be
    translated."""
    if '.' in entity:
        prefix, suffix = entity.rsplit('.', 1)
        if (suffix in ["height", "width", "unixWidth", "macWidth", "size"] or
            suffix.startswith("style")):
            return True
    return False


class dtd2po:

    def __init__(self, blankmsgstr=False, duplicatestyle="msgctxt"):
        self.currentgroup = None
        self.blankmsgstr = blankmsgstr
        self.duplicatestyle = duplicatestyle
        self.mixedentities = {}
        self.mixer = UnitMixer(dtd.labelsuffixes, dtd.accesskeysuffixes)

    def convertcomments(self, dtd_unit, po_unit):
        entity = dtd_unit.getid()
        if len(entity) > 0:
            po_unit.addlocation(entity)
        for commenttype, comment in dtd_unit.comments:
            # handle groups
            if (commenttype == "locgroupstart"):
                groupcomment = comment.replace('BEGIN', 'GROUP')
                self.currentgroup = groupcomment
            elif (commenttype == "locgroupend"):
                groupcomment = comment.replace('END', 'GROUP')
                self.currentgroup = None
            # handle automatic comment
            if commenttype == "automaticcomment":
                po_unit.addnote(comment, origin="developer")
            # handle normal comments
            else:
                po_unit.addnote(quote.stripcomment(comment), origin="developer")
        # handle group stuff
        if self.currentgroup is not None:
            po_unit.addnote(quote.stripcomment(self.currentgroup),
                          origin="translator")
        if is_css_entity(entity):
            po_unit.addnote("Do not translate this.  Only change the numeric values if you need this dialogue box to appear bigger",
                          origin="developer")

    def convertstrings(self, dtd_unit, po_unit):
        # extract the string, get rid of quoting
        unquoted = dtd_unit.source.replace("\r", "")
        # escape backslashes... but not if they're for a newline
        # unquoted = unquoted.replace("\\", "\\\\").replace("\\\\n", "\\n")
        # now split the string into lines and quote them
        lines = unquoted.split('\n')
        while lines and not lines[0].strip():
            del lines[0]
        while lines and not lines[-1].strip():
            del lines[-1]
        # quotes have been escaped already by escapeforpo, so just add the
        # start and end quotes
        if len(lines) > 1:
            po_unit.source = "\n".join([lines[0].rstrip() + ' '] +
                    [line.strip() + ' ' for line in lines[1:-1]] +
                    [lines[-1].lstrip()])
        elif lines:
            po_unit.source = lines[0]
        else:
            po_unit.source = ""
        po_unit.target = ""

    def convertunit(self, dtd_unit):
        """Converts a simple (non-mixed) dtd unit into a po unit.

        Returns None if empty or not for translation.
        """
        if dtd_unit is None:
            return None
        po_unit = po.pounit(encoding="UTF-8")
        # remove unwanted stuff
        for commentnum in range(len(dtd_unit.comments)):
            commenttype, locnote = dtd_unit.comments[commentnum]
            # if this is a localization note
            if commenttype == 'locnote':
                # parse the locnote into the entity and the actual note
                typeend = quote.findend(locnote, 'LOCALIZATION NOTE')
                # parse the id
                idstart = locnote.find('(', typeend)
                if idstart == -1:
                    continue
                idend = locnote.find(')', (idstart + 1))
                entity = locnote[idstart+1:idend].strip()
                # parse the actual note
                actualnotestart = locnote.find(':', (idend + 1))
                actualnoteend = locnote.find('-->', idend)
                actualnote = locnote[actualnotestart+1:actualnoteend].strip()
                # if it's for this entity, process it
                if dtd_unit.getid() == entity:
                    # if it says don't translate (and nothing more),
                    if actualnote.startswith("DONT_TRANSLATE"):
                        # take out the entity,definition and the
                        # DONT_TRANSLATE comment
                        dtd_unit.setid("")
                        dtd_unit.source = ""
                        del dtd_unit.comments[commentnum]
                        # finished this for loop
                        break
                    else:
                        # convert it into an automatic comment, to be
                        # processed by convertcomments
                        dtd_unit.comments[commentnum] = ("automaticcomment",
                                                       actualnote)
        # do a standard translation
        self.convertcomments(dtd_unit, po_unit)
        self.convertstrings(dtd_unit, po_unit)
        if po_unit.isblank() and not po_unit.getlocations():
            return None
        else:
            return po_unit

    def convertmixedunit(self, labeldtd, accesskeydtd):
        label_unit = self.convertunit(labeldtd)
        accesskey_unit = self.convertunit(accesskeydtd)
        if label_unit is None:
            return accesskey_unit
        if accesskey_unit is None:
            return label_unit
        target_unit = po.pounit(encoding="UTF-8")
        return self.mixer.mix_units(label_unit, accesskey_unit, target_unit)

    def convertdtdunit(self, store, unit, mixbucket="dtd"):
        """Converts a unit from store to a po unit, keeping track of mixed
        entities along the way.

        ``mixbucket`` can be specified to indicate if the given unit is part of
        the template or the translated file.
        """
        # keep track of whether accesskey and label were combined
        entity = unit.getid()
        if entity not in self.mixedentities:
            return self.convertunit(unit)

        # use special convertmixed unit which produces one pounit with
        # both combined for the label and None for the accesskey
        alreadymixed = self.mixedentities[entity].get(mixbucket, None)
        if alreadymixed:
            # we are successfully throwing this away...
            return None
        elif alreadymixed is False:
            # The mix failed before
            return self.convertunit(unit)

        #assert alreadymixed is None
        labelentity, accesskeyentity = self.mixer.find_mixed_pair(self.mixedentities, store, unit)
        labeldtd = store.id_index.get(labelentity, None)
        accesskeydtd = store.id_index.get(accesskeyentity, None)
        po_unit = self.convertmixedunit(labeldtd, accesskeydtd)
        if po_unit is not None:
            if accesskeyentity is not None:
                self.mixedentities[accesskeyentity][mixbucket] = True
            if labelentity is not None:
                self.mixedentities[labelentity][mixbucket] = True
            return po_unit
        else:
            # otherwise the mix failed. add each one separately and
            # remember they weren't mixed
            if accesskeyentity is not None:
                self.mixedentities[accesskeyentity][mixbucket] = False
            if labelentity is not None:
                self.mixedentities[labelentity][mixbucket] = False

        return self.convertunit(unit)

    def convertstore(self, dtd_store):
        target_store = po.pofile()
        targetheader = target_store.init_headers(
                x_accelerator_marker="&",
                x_merge_on="location",
        )
        targetheader.addnote("extracted from %s" % dtd_store.filename,
                             "developer")

        dtd_store.makeindex()
        self.mixedentities = self.mixer.match_entities(dtd_store.id_index)
        # go through the dtd and convert each unit
        for dtd_unit in dtd_store.units:
            if not dtd_unit.istranslatable():
                continue
            po_unit = self.convertdtdunit(dtd_store, dtd_unit)
            if po_unit is not None:
                target_store.addunit(po_unit)
        target_store.removeduplicates(self.duplicatestyle)
        return target_store

    def mergestore(self, origdtdfile, translateddtdfile):
        target_store = po.pofile()
        targetheader = target_store.init_headers(
                x_accelerator_marker="&",
                x_merge_on="location",
        )
        targetheader.addnote("extracted from %s, %s" %
                             (origdtdfile.filename,
                              translateddtdfile.filename),
                             "developer")

        origdtdfile.makeindex()
        #TODO: self.mixedentities is overwritten below, so this is useless:
        self.mixedentities = self.mixer.match_entities(origdtdfile.id_index)
        translateddtdfile.makeindex()
        self.mixedentities = self.mixer.match_entities(translateddtdfile.id_index)
        # go through the dtd files and convert each unit
        for origdtd in origdtdfile.units:
            if not origdtd.istranslatable():
                continue
            origpo = self.convertdtdunit(origdtdfile, origdtd,
                                         mixbucket="orig")
            orig_entity = origdtd.getid()
            if orig_entity in self.mixedentities:
                mixedentitydict = self.mixedentities[orig_entity]
                if "orig" not in mixedentitydict:
                    # this means that the entity is mixed in the translation,
                    # but not the original - treat as unmixed
                    mixbucket = "orig"
                    del self.mixedentities[orig_entity]
                elif mixedentitydict["orig"]:
                    # the original entity is already mixed successfully
                    mixbucket = "translate"
                else:
                    # ??
                    mixbucket = "orig"
            else:
                mixbucket = "translate"
            if origpo is None:
                # this means its a mixed entity (with accesskey) that's
                # already been dealt with)
                continue
            if orig_entity in translateddtdfile.id_index:
                translateddtd = translateddtdfile.id_index[orig_entity]
                translatedpo = self.convertdtdunit(translateddtdfile,
                                                   translateddtd,
                                                   mixbucket=mixbucket)
            else:
                translatedpo = None
            if origpo is not None:
                if translatedpo is not None and not self.blankmsgstr:
                    origpo.target = translatedpo.source
                target_store.addunit(origpo)
        target_store.removeduplicates(self.duplicatestyle)
        return target_store


def convertdtd(inputfile, outputfile, templatefile, pot=False,
               duplicatestyle="msgctxt"):
    """reads in inputfile and templatefile using dtd, converts using dtd2po,
    writes to outputfile"""
    android_dtd = False
    if hasattr(inputfile, "name"):
        # Check if it is an Android DTD file.
        if ("embedding/android" in inputfile.name or
            "mobile/android/base" in inputfile.name):
            android_dtd = True
    inputstore = dtd.dtdfile(inputfile, android=android_dtd)
    convertor = dtd2po(blankmsgstr=pot, duplicatestyle=duplicatestyle)
    if templatefile is None:
        outputstore = convertor.convertstore(inputstore)
    else:
        templatestore = dtd.dtdfile(templatefile, android=android_dtd)
        outputstore = convertor.mergestore(templatestore, inputstore)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
        "dtd": ("po", convertdtd),
        ("dtd", "dtd"): ("po", convertdtd),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = factory
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Factory methods to convert supported input files to supported translatable files."""

import os


#from translate.convert import prop2po, po2prop, odf2xliff, xliff2odf


__all__ = ['converters', 'UnknownExtensionError', 'UnsupportedConversionError']

# Turn into property to support lazy loading of things?
converters = {}
#for module in (prop2po, po2prop, odf2xliff, xliff2odf):
#    if not hasattr(module, 'formats'):
#        continue
#    for extension in module.formats:
#        if extension not in converters:
#            converters[extension] = []
#        converters[extension].append(module.formats[extension])


class UnknownExtensionError(Exception):

    def __init__(self, afile):
        self.file = afile

    def __str__(self):
        return 'Unable to find extension for file: %s' % (self.file)

    def __unicode__(self):
        return unicode(str(self))


class UnsupportedConversionError(Exception):

    def __init__(self, in_ext=None, out_ext=None, templ_ext=None):
        self.in_ext = in_ext
        self.out_ext = out_ext
        self.templ_ext = templ_ext

    def __str__(self):
        msg = "Unsupported conversion from %s to %s" % (self.in_ext, self.out_ext)
        if self.templ_ext:
            msg += ' with template %s' % (self.templ_ext)
        return msg

    def __unicode__(self):
        return unicode(str(self))


def get_extension(filename):
    path, fname = os.path.split(filename)
    ext = fname.split(os.extsep)[-1]
    if ext == fname:
        return None
    return ext


def get_converter(in_ext, out_ext=None, templ_ext=None):
    convert_candidates = None
    if templ_ext:
        if (in_ext, templ_ext) in converters:
            convert_candidates = converters[(in_ext, templ_ext)]
        else:
            raise UnsupportedConversionError(in_ext, out_ext, templ_ext)
    else:
        if in_ext in converters:
            convert_candidates = converters[in_ext]
        elif (in_ext,) in converters:
            convert_candidates = converters[(in_ext,)]
        else:
            raise UnsupportedConversionError(in_ext, out_ext)

    convert_fn = None
    if not out_ext:
        out_ext, convert_fn = convert_candidates[0]
    else:
        for ext, func in convert_candidates:
            if ext == out_ext:
                convert_fn = func
                break

    if not convert_fn:
        raise UnsupportedConversionError(in_ext, out_ext, templ_ext)
    return convert_fn


def get_output_extensions(ext):
    """Compiles a list of possible output extensions for the given input extension."""
    out_exts = []
    for key in converters:
        in_ext = key
        if isinstance(key, tuple):
            in_ext = key[0]
        if in_ext == ext:
            for out_ext, convert_fn in converters[key]:
                out_exts.append(out_ext)
    return out_exts


def convert(inputfile, template=None, options=None, convert_options=None):
    """Convert the given input file to an appropriate output format, optionally
        using the given template file and further options.

        If the output extension (format) cannot be inferred the first converter
        that can handle the input file (and the format/extension it gives as
        output) is used.

        :type  inputfile: file
        :param inputfile: The input file to be converted
        :type  template: file
        :param template: Template file to use during conversion
        :type  options: dict (default: None)
        :param options: Valid options are:
            - in_ext: The extension (format) of the input file.
            - out_ext: The extension (format) to use for the output file.
            - templ_ext: The extension (format) of the template file.
            - in_fname: File name of the input file; used only to determine
              the input file extension (format).
            - templ_fname: File name of the template file; used only to
              determine the template file extension (format).
        :returns: a 2-tuple: The new output file (in a temporary directory) and
                  the extension (format) of the output file. The caller is
                  responsible for deleting the (temporary) output file."""
    in_ext, out_ext, templ_ext = None, None, None

    # Get extensions from options
    if options is None:
        options = {}
    else:
        if 'in_ext' in options:
            in_ext = options['in_ext']
        if 'out_ext' in options:
            out_ext = options['out_ext']
        if template and 'templ_ext' in options:
            templ_ext = options['templ_ext']

        # If we still do not have extensions, try and get it from the *_fname options
        if not in_ext and 'in_fname' in options:
            in_ext = get_extension(options['in_fname'])
        if template and not templ_ext and 'templ_fname' in options:
            templ_fname = get_extension(options['templ_fname'])

    # If we still do not have extensions, get it from the file names
    if not in_ext and hasattr(inputfile, 'name'):
        in_ext = get_extension(inputfile.name)
    if template and not templ_ext and hasattr(template, 'name'):
        templ_ext = get_extension(template.name)

    if not in_ext:
        raise UnknownExtensionError(inputfile)
    if template and not templ_ext:
        raise UnknownExtensionError(template)

    out_ext_candidates = get_output_extensions(in_ext)
    if not out_ext_candidates:
        # No converters registered for the in_ext we have
        raise UnsupportedConversionError(in_ext=in_ext, templ_ext=templ_ext)
    if out_ext and out_ext not in out_ext_candidates:
        # If out_ext has a value at this point, it was given in options, so
        # we just take a second to make sure that the conversion is supported.
        raise UnsupportedConversionError(in_ext, out_ext, templ_ext)

    if not out_ext and templ_ext in out_ext_candidates:
        # If we're using a template, chances are (pretty damn) good that the
        # output file will be of the same type
        out_ext = templ_ext
    else:
        # As a last resort, we'll just use the first possible output format
        out_ext = out_ext_candidates[0]

    # XXX: We are abusing tempfile.mkstemp() below: we are only using it to
    #      obtain a temporary file name to use the normal open() with. This is
    #      done because a tempfile.NamedTemporaryFile simply gave too many
    #      issues when being closed (and deleted) by the rest of the toolkit
    #      (eg. TranslationStore.savefile()). Therefore none of mkstemp()'s
    #      security features are being utilised.
    import tempfile
    tempfd, tempfname = tempfile.mkstemp(prefix='ttk_convert', suffix=os.extsep + out_ext)
    os.close(tempfd)
    outputfile = open(tempfname, 'w')

    if convert_options is None:
        convert_options = {}
    get_converter(in_ext, out_ext, templ_ext)(inputfile, outputfile, template, **convert_options)
    if hasattr(outputfile, 'closed') and hasattr(outputfile, 'close') and not outputfile.closed:
        outputfile.close()
    return outputfile, out_ext

########NEW FILE########
__FILENAME__ = html2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert HTML files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/html2po.html
for examples and usage instructions.
"""

from translate.storage import html, po


class html2po:

    def convertfile(self, inputfile, filename, includeuntagged=False,
                    duplicatestyle="msgctxt", keepcomments=False):
        """converts a html file to .po format"""
        thetargetfile = po.pofile()
        htmlparser = html.htmlfile(includeuntaggeddata=includeuntagged,
                                   inputfile=inputfile)
        for htmlunit in htmlparser.units:
            thepo = thetargetfile.addsourceunit(htmlunit.source)
            thepo.addlocations(htmlunit.getlocations())
            if keepcomments:
                thepo.addnote(htmlunit.getnotes(), "developer")
        thetargetfile.removeduplicates(duplicatestyle)
        return thetargetfile


def converthtml(inputfile, outputfile, templates, includeuntagged=False,
                pot=False, duplicatestyle="msgctxt", keepcomments=False):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    convertor = html2po()
    outputfilepos = outputfile.tell()
    outputstore = convertor.convertfile(inputfile, getattr(inputfile, "name",
                                                           "unknown"),
                                        includeuntagged,
                                        duplicatestyle=duplicatestyle,
                                        keepcomments=keepcomments)
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    formats = {
        "html": ("po", converthtml),
        "htm": ("po", converthtml),
        "xhtml": ("po", converthtml),
        None: ("po", converthtml),
    }
    parser = convert.ConvertOptionParser(formats, usepots=True,
                                         description=__doc__)
    parser.add_option("-u", "--untagged", dest="includeuntagged",
                      default=False, action="store_true",
                      help="include untagged sections")
    parser.passthrough.append("includeuntagged")
    parser.add_option("--keepcomments", dest="keepcomments", default=False,
                      action="store_true",
                      help="preserve html comments as translation notes in the output")
    parser.passthrough.append("keepcomments")
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ical2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert iCal files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ical2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import ical, po


logger = logging.getLogger(__name__)


class ical2po:
    """convert a iCal file to a .po file for handling the translation..."""

    def convert_store(self, input_store, duplicatestyle="msgctxt"):
        """converts a iCal file to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.header()
        output_header.addnote("extracted from %s" % input_store.filename, "developer")
        for input_unit in input_store.units:
            output_unit = self.convert_unit(input_unit, "developer")
            if output_unit is not None:
                output_store.addunit(output_unit)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def merge_store(self, template_store, input_store, blankmsgstr=False, duplicatestyle="msgctxt"):
        """converts two iCal files to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.header()
        output_header.addnote("extracted from %s, %s" % (template_store.filename, input_store.filename), "developer")

        input_store.makeindex()
        for template_unit in template_store.units:
            origpo = self.convert_unit(template_unit, "developer")
            # try and find a translation of the same name...
            template_unit_name = "".join(template_unit.getlocations())
            if template_unit_name in input_store.locationindex:
                translatedini = input_store.locationindex[template_unit_name]
                translatedpo = self.convert_unit(translatedini, "translator")
            else:
                translatedpo = None
            # if we have a valid po unit, get the translation and add it...
            if origpo is not None:
                if translatedpo is not None and not blankmsgstr:
                    origpo.target = translatedpo.source
                output_store.addunit(origpo)
            elif translatedpo is not None:
                logger.error("error converting original iCal definition %s",
                             origpo.name)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def convert_unit(self, input_unit, commenttype):
        """Converts a .ini unit to a .po unit. Returns None if empty
        or not for translation."""
        if input_unit is None:
            return None
        # escape unicode
        output_unit = po.pounit(encoding="UTF-8")
        output_unit.addlocation("".join(input_unit.getlocations()))
        output_unit.addnote(input_unit.getnotes("developer"), "developer")
        output_unit.source = input_unit.source
        output_unit.target = ""
        return output_unit


def convertical(input_file, output_file, template_file, pot=False, duplicatestyle="msgctxt"):
    """Reads in *input_file* using iCal, converts using :class:`ical2po`,
    writes to *output_file*."""
    input_store = ical.icalfile(input_file)
    convertor = ical2po()
    if template_file is None:
        output_store = convertor.convert_store(input_store, duplicatestyle=duplicatestyle)
    else:
        template_store = ical.icalfile(template_file)
        output_store = convertor.merge_store(template_store, input_store, blankmsgstr=pot, duplicatestyle=duplicatestyle)
    if output_store.isempty():
        return 0
    output_file.write(str(output_store))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"ics": ("po", convertical), ("ics", "ics"): ("po", convertical)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ini2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert .ini files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ini2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import po


logger = logging.getLogger(__name__)


class ini2po:
    """Convert a .ini file to a .po file for handling the translation..."""

    def convert_store(self, input_store, duplicatestyle="msgctxt"):
        """Convert a .ini file to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.header()
        output_header.addnote("extracted from %s" % input_store.filename,
                              "developer")

        for input_unit in input_store.units:
            output_unit = self.convert_unit(input_unit, "developer")
            if output_unit is not None:
                output_store.addunit(output_unit)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def merge_store(self, template_store, input_store, blankmsgstr=False,
                    duplicatestyle="msgctxt"):
        """Convert two .ini files to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.header()
        note = "extracted from %s, %s" % (template_store.filename,
                                          input_store.filename)
        output_header.addnote(note, "developer")

        input_store.makeindex()
        for template_unit in template_store.units:
            origpo = self.convert_unit(template_unit, "developer")
            # Try and find a translation of the same name...
            template_unit_name = "".join(template_unit.getlocations())
            if template_unit_name in input_store.locationindex:
                translatedini = input_store.locationindex[template_unit_name]
                translatedpo = self.convert_unit(translatedini, "translator")
            else:
                translatedpo = None
            # If we have a valid po unit, get the translation and add it...
            if origpo is not None:
                if translatedpo is not None and not blankmsgstr:
                    origpo.target = translatedpo.source
                output_store.addunit(origpo)
            elif translatedpo is not None:
                logger.error("error converting original ini definition %s",
                             origpo.name)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def convert_unit(self, input_unit, commenttype):
        """Convert a .ini unit to a .po unit. Returns None if empty or not for
        translation.
        """
        if input_unit is None:
            return None
        # Escape unicode.
        output_unit = po.pounit(encoding="UTF-8")
        output_unit.addlocation("".join(input_unit.getlocations()))
        output_unit.source = input_unit.source
        output_unit.target = ""
        return output_unit


def convertini(input_file, output_file, template_file, pot=False,
               duplicatestyle="msgctxt", dialect="default"):
    """Read in *input_file* using ini, converts using :class:`ini2po`, writes
    to *output_file*.
    """
    from translate.storage import ini
    input_store = ini.inifile(input_file, dialect=dialect)
    convertor = ini2po()
    if template_file is None:
        output_store = convertor.convert_store(input_store,
                                               duplicatestyle=duplicatestyle)
    else:
        template_store = ini.inifile(template_file, dialect=dialect)
        output_store = convertor.merge_store(template_store, input_store,
                                             blankmsgstr=pot,
                                             duplicatestyle=duplicatestyle)
    if output_store.isempty():
        return 0
    output_file.write(str(output_store))
    return 1


def convertisl(input_file, output_file, template_file, pot=False,
               duplicatestyle="msgctxt", dialect="inno"):
    return convertini(input_file, output_file, template_file, pot=False,
                      duplicatestyle="msgctxt", dialect=dialect)


def main(argv=None):
    from translate.convert import convert
    formats = {
        "ini": ("po", convertini), ("ini", "ini"): ("po", convertini),
        "isl": ("po", convertisl), ("isl", "isl"): ("po", convertisl),
        "iss": ("po", convertisl), ("iss", "iss"): ("po", convertisl),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = json2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007, 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert JSON files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/json2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import po


logger = logging.getLogger(__name__)


class json2po:
    """Convert a JSON file to a PO file"""

    def convert_store(self, input_store, duplicatestyle="msgctxt"):
        """Converts a JSON file to a PO file"""
        output_store = po.pofile()
        output_header = output_store.header()
        output_header.addnote("extracted from %s" % input_store.filename,
                              "developer")
        for input_unit in input_store.units:
            output_unit = self.convert_unit(input_unit, "developer")
            if output_unit is not None:
                output_store.addunit(output_unit)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def merge_store(self, template_store, input_store, blankmsgstr=False,
                    duplicatestyle="msgctxt"):
        """Converts two JSON files to a PO file"""
        output_store = po.pofile()
        output_header = output_store.header()
        output_header.addnote("extracted from %s, %s" % (template_store.filename,
                                                         input_store.filename),
                              "developer")

        input_store.makeindex()
        for template_unit in template_store.units:
            origpo = self.convert_unit(template_unit, "developer")
            # try and find a translation of the same name...
            template_unit_name = "".join(template_unit.getlocations())
            if template_unit_name in input_store.locationindex:
                translatedjson = input_store.locationindex[template_unit_name]
                translatedpo = self.convert_unit(translatedjson, "translator")
            else:
                translatedpo = None
            # if we have a valid po unit, get the translation and add it...
            if origpo is not None:
                if translatedpo is not None and not blankmsgstr:
                    origpo.target = translatedpo.source
                output_store.addunit(origpo)
            elif translatedpo is not None:
                logger.error("error converting original JSON definition %s",
                             origpo.name)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def convert_unit(self, input_unit, commenttype):
        """Converts a JSON unit to a PO unit

        :return: None if empty or not for translation
        """
        if input_unit is None:
            return None
        # escape unicode
        output_unit = po.pounit(encoding="UTF-8")
        output_unit.addlocation(input_unit.getid())
        output_unit.source = input_unit.source
        output_unit.target = ""
        return output_unit


def convertjson(input_file, output_file, template_file, pot=False,
                duplicatestyle="msgctxt", dialect="default", filter=None):
    """Reads in *input_file* using jsonl10n, converts using :class:`json2po`,
    writes to *output_file*."""
    from translate.storage import jsonl10n
    if filter is not None:
        filter = filter.split(',')
    input_store = jsonl10n.JsonFile(input_file, filter=filter)
    convertor = json2po()
    if template_file is None:
        output_store = convertor.convert_store(input_store,
                                               duplicatestyle=duplicatestyle)
    else:
        template_store = jsonl10n.JsonFile(template_file)
        output_store = convertor.merge_store(template_store, input_store,
                                             blankmsgstr=pot,
                                             duplicatestyle=duplicatestyle)
    if output_store.isempty():
        return 0
    output_file.write(str(output_store))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
               "json": ("po", convertjson),
               ("json", "json"): ("po", convertjson),
              }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True, description=__doc__)
    parser.add_option("", "--filter", dest="filter", default=None,
            help="leaves to extract e.g. 'name,desc': (default: extract everything)",
            metavar="FILTER")
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.passthrough.append("filter")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = moz2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Mozilla .dtd and .properties files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/moz2po.html
for examples and usage instructions.
"""

from translate.convert import (convert, dtd2po, mozfunny2prop, mozlang2po,
                               prop2po)


def main(argv=None):
    formats = {
        (None, "*"): ("*", convert.copytemplate),
        ("*", "*"): ("*", convert.copyinput),
        "*": ("*", convert.copyinput),
    }
    # handle formats that convert to .po files
    converters = [
        ("dtd", dtd2po.convertdtd),
        ("properties", prop2po.convertmozillaprop),
        ("it", mozfunny2prop.it2po),
        ("ini", mozfunny2prop.ini2po),
        ("inc", mozfunny2prop.inc2po),
        ("lang", mozlang2po.convertlang),
    ]
    for format, converter in converters:
        formats[(format, format)] = (format + ".po", converter)
        formats[format] = (format + ".po", converter)
    # handle search and replace
    replacer = convert.Replacer("en-US", "${locale}")
    for replaceformat in ("js", "rdf", "manifest"):
        formats[(None, replaceformat)] = (replaceformat,
                                          replacer.searchreplacetemplate)
        formats[(replaceformat, replaceformat)] = (replaceformat,
                                                   replacer.searchreplaceinput)
        formats[replaceformat] = (replaceformat, replacer.searchreplaceinput)
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True,
                                         description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = mozfunny2prop
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Converts additional Mozilla files to properties files.
"""

import string

from translate.convert import prop2po
from translate.misc.wStringIO import StringIO


def inc2prop(lines):
    """convert a .inc file with #defines in it to a properties file"""
    yield "# converted from #defines file\n"
    for line in lines:
        line = line.decode("utf-8")
        if line.startswith("# "):
            commented = True
            line = line.replace("# ", "", 1)
        else:
            commented = False
        if not line.strip():
            yield line
        elif line.startswith("#define"):
            parts = string.split(line.replace("#define", "", 1).strip(), maxsplit=1)
            if not parts:
                continue
            if len(parts) == 1:
                key, value = parts[0], ""
            else:
                key, value = parts
            # special case: uncomment MOZ_LANGPACK_CONTRIBUTORS
            if key == "MOZ_LANGPACK_CONTRIBUTORS":
                commented = False
            if commented:
                yield "# "
            yield "%s = %s\n" % (key, value)
        else:
            if commented:
                yield "# "
            yield line


def it2prop(lines, encoding="cp1252"):
    """convert a pseudo-properties .it file to a conventional properties file"""
    yield "# converted from pseudo-properties .it file\n"
    # differences: ; instead of # for comments
    #              [section] titles that we replace with # section: comments
    for line in lines:
        line = line.decode(encoding)
        if not line.strip():
            yield line
        elif line.lstrip().startswith(";"):
            yield line.replace(";", "#", 1)
        elif line.lstrip().startswith("[") and line.rstrip().endswith("]"):
            yield "# section: " + line
        else:
            yield line


def funny2prop(lines, itencoding="cp1252"):
    hashstarts = len([line for line in lines if line.startswith("#")])
    if hashstarts:
        for line in inc2prop(lines):
            yield line
    else:
        for line in it2prop(lines, encoding=itencoding):
            yield line


def inc2po(inputfile, outputfile, templatefile, encoding=None, pot=False, duplicatestyle="msgctxt"):
    """wraps prop2po but converts input/template files to properties first"""
    inputlines = inputfile.readlines()
    inputproplines = [line for line in inc2prop(inputlines)]
    inputpropfile = StringIO("".join(inputproplines))
    if templatefile is not None:
        templatelines = templatefile.readlines()
        templateproplines = [line for line in inc2prop(templatelines)]
        templatepropfile = StringIO("".join(templateproplines))
    else:
        templatepropfile = None
    return prop2po.convertprop(inputpropfile, outputfile, templatepropfile, personality="mozilla", pot=pot, duplicatestyle=duplicatestyle)


def it2po(inputfile, outputfile, templatefile, encoding="cp1252", pot=False, duplicatestyle="msgctxt"):
    """wraps prop2po but converts input/template files to properties first"""
    inputlines = inputfile.readlines()
    inputproplines = [line for line in it2prop(inputlines, encoding=encoding)]
    inputpropfile = StringIO("".join(inputproplines))
    if templatefile is not None:
        templatelines = templatefile.readlines()
        templateproplines = [line for line in it2prop(templatelines, encoding=encoding)]
        templatepropfile = StringIO("".join(templateproplines))
    else:
        templatepropfile = None
    return prop2po.convertprop(inputpropfile, outputfile, templatepropfile, personality="mozilla", pot=pot, duplicatestyle=duplicatestyle)


def ini2po(inputfile, outputfile, templatefile, encoding="UTF-8", pot=False, duplicatestyle="msgctxt"):
    return it2po(inputfile=inputfile, outputfile=outputfile, templatefile=templatefile, encoding=encoding, pot=pot, duplicatestyle=duplicatestyle)


def main(argv=None):
    import sys
    lines = sys.stdin.readlines()
    for line in funny2prop(lines):
        sys.stdout.write(line)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = mozlang2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008, 2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

# Original Author: Dan Schafer <dschafer@mozilla.com>
# Date: 10 Jun 2008

"""Convert Mozilla .lang files to Gettext PO localization files.
"""

from translate.storage import mozilla_lang as lang, po


class lang2po:

    def __init__(self, duplicatestyle="msgctxt"):
        self.duplicatestyle = duplicatestyle

    def convertstore(self, thelangfile):
        """converts a file to .po format"""
        thetargetfile = po.pofile()

        # Set up the header
        targetheader = thetargetfile.header()
        targetheader.addnote("extracted from %s" %
                             thelangfile.filename, "developer")

        # For each lang unit, make the new po unit accordingly
        for langunit in thelangfile.units:
            newunit = thetargetfile.addsourceunit(langunit.source)
            newunit.settarget(langunit.target)
            newunit.addlocations(langunit.getlocations())
            newunit.addnote(langunit.getnotes(), 'developer')

        # Remove duplicates, because we can
        thetargetfile.removeduplicates(self.duplicatestyle)
        return thetargetfile


def convertlang(inputfile, outputfile, templates, pot=False,
                duplicatestyle="msgctxt", encoding="utf-8"):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    inputstore = lang.LangStore(inputfile, encoding=encoding)
    convertor = lang2po(duplicatestyle=duplicatestyle)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


formats = {
    "lang": ("po", convertlang)
}


def main(argv=None):
    from translate.convert import convert
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    parser = convert.ConvertOptionParser(formats, usepots=True,
                                           description=__doc__)
    parser.add_option("", "--encoding", dest="encoding", default='utf-8',
    help="The encoding of the input file (default: UTF-8)")
    parser.passthrough.append("encoding")
    parser.add_duplicates_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = odf2xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert OpenDocument (ODF) files to XLIFF localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/odf2xliff.html
for examples and usage instructions.
"""

from contextlib import contextmanager
from cStringIO import StringIO

from translate.storage import factory, odf_io


def convertodf(inputfile, outputfile, templates, engine='toolkit'):
    """reads in stdin using fromfileclass, converts using convertorclass,
       writes to stdout
    """

    def translate_toolkit_implementation(store):
        from translate.storage.xml_extract import extract
        from translate.storage import odf_shared

        contents = odf_io.open_odf(inputfile)
        for data in contents.values():
            parse_state = extract.ParseState(odf_shared.no_translate_content_elements,
                                             odf_shared.inline_elements)
            extract.build_store(StringIO(data), store, parse_state)

    def itools_implementation(store):
        from itools.handlers import get_handler
        from itools.gettext.po import encode_source
        import itools.odf

        filename = getattr(inputfile, 'name', 'unkown')
        handler = get_handler(filename)

        try:
            get_units = handler.get_units
        except AttributeError:
            raise AttributeError('error: the file "%s" could not be processed' % filename)

        # Make the XLIFF file
        for source, context, line in get_units():
            source = encode_source(source)
            unit = store.UnitClass(source)
            store.addunit(unit)

    @contextmanager
    def store_context():
        store = factory.getobject(outputfile)
        try:
            store.setfilename(store.getfilenode('NoName'), inputfile.name)
        except:
            print("couldn't set origin filename")
        yield store
        store.save()

    # Since the convertoptionsparser will give us an open file, we risk that
    # it could have been opened in non-binary mode on Windows, and then we'll
    # have problems, so let's make sure we have what we want.
    inputfile.close()
    inputfile = file(inputfile.name, mode='rb')

    with store_context() as store:
        if engine == "toolkit":
            translate_toolkit_implementation(store)
        else:
            itools_implementation(store)

    return True


# For formats see OpenDocument 1.2 draft 7 Appendix C
formats = {
    "sxw": ("xlf", convertodf),
    "odt": ("xlf", convertodf),  # Text
    "ods": ("xlf", convertodf),  # Spreadsheet
    "odp": ("xlf", convertodf),  # Presentation
    "odg": ("xlf", convertodf),  # Drawing
    "odc": ("xlf", convertodf),  # Chart
    "odf": ("xlf", convertodf),  # Formula
    "odi": ("xlf", convertodf),  # Image
    "odm": ("xlf", convertodf),  # Master Document
    "ott": ("xlf", convertodf),  # Text template
    "ots": ("xlf", convertodf),  # Spreadsheet template
    "otp": ("xlf", convertodf),  # Presentation template
    "otg": ("xlf", convertodf),  # Drawing template
    "otc": ("xlf", convertodf),  # Chart template
    "otf": ("xlf", convertodf),  # Formula template
    "oti": ("xlf", convertodf),  # Image template
    "oth": ("xlf", convertodf),  # Web page template
}


def main(argv=None):

    def add_options(parser):
        parser.add_option("", "--engine", dest="engine", default="toolkit",
                          type="choice", choices=["toolkit", "itools"],
                          help="""Choose whether itools (--engine=itools) or the translate toolkit (--engine=toolkit)
                          should be used as the engine to convert an ODF file to an XLIFF file.""")
        parser.passthrough = ['engine']
        return parser

    from translate.convert import convert
    parser = convert.ConvertOptionParser(formats, description=__doc__)
    add_options(parser)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = oo2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2003-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert an OpenOffice.org (SDF) localization file to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/oo2po.html
for examples and usage instructions.
"""

import logging
from urllib import urlencode

from translate.storage import oo, po


# TODO: support using one GSI file as template, another as input (for when English is in one and translation in another)

logger = logging.getLogger(__name__)


class oo2po:

    def __init__(self, sourcelanguage, targetlanguage, blankmsgstr=False, long_keys=False):
        """construct an oo2po converter for the specified languages"""
        self.sourcelanguage = sourcelanguage
        self.targetlanguage = targetlanguage
        self.blankmsgstr = blankmsgstr
        self.long_keys = long_keys

    def maketargetunit(self, part1, part2, translators_comment, key, subkey):
        """makes a base unit (.po or XLIFF) out of a subkey of two parts"""
        #TODO: Do better
        text1 = getattr(part1, subkey)
        if text1 == "":
            return None
        text2 = getattr(part2, subkey)

        unit = po.pounit(text1.decode('utf-8'), encoding="UTF-8")
        unit.target = text2.decode('utf-8')
        unit.addlocation(key + "." + subkey)
        if getattr(translators_comment, subkey).strip() != "":
            unit.addnote(getattr(translators_comment, subkey), origin="developer")
        return unit

    def convertelement(self, theoo):
        """convert an oo element into a list of base units (.po or XLIFF)"""
        if self.sourcelanguage in theoo.languages:
            part1 = theoo.languages[self.sourcelanguage]
        else:
            logger.error("/".join(theoo.lines[0].getkey()) +
                         "language not found: %s", self.sourcelanguage)
            return []
        if self.blankmsgstr:
            # use a blank part2
            part2 = oo.ooline()
        else:
            if self.targetlanguage in theoo.languages:
                part2 = theoo.languages[self.targetlanguage]
            else:
                # if the language doesn't exist, the translation is missing ... so make it blank
                part2 = oo.ooline()
        if "x-comment" in theoo.languages:
            translators_comment = theoo.languages["x-comment"]
        else:
            translators_comment = oo.ooline()
        key = oo.makekey(part1.getkey(), self.long_keys)
        unitlist = []
        for subkey in ("text", "quickhelptext", "title"):
            unit = self.maketargetunit(part1, part2, translators_comment,
                                       key, subkey)
            if unit is not None:
                unitlist.append(unit)
        return unitlist

    def convertstore(self, theoofile, duplicatestyle="msgctxt"):
        """converts an entire oo file to a base class format (.po or XLIFF)"""
        thetargetfile = po.pofile()
        # create a header for the file
        bug_url = 'http://qa.openoffice.org/issues/enter_bug.cgi?%s' % \
                  urlencode({
                      "subcomponent": "ui",
                      "comment": "",
                      "short_desc": "Localization issue in file: %s" %
                                    theoofile.filename,
                      "component": "l10n",
                      "form_name": "enter_issue",
                  })
        targetheader = thetargetfile.init_headers(
                              x_accelerator_marker="~",
                              x_merge_on="location",
                              report_msgid_bugs_to=bug_url,
        )
        targetheader.addnote("extracted from %s" % theoofile.filename,
                             "developer")
        thetargetfile.setsourcelanguage(self.sourcelanguage)
        thetargetfile.settargetlanguage(self.targetlanguage)
        # go through the oo and convert each element
        for theoo in theoofile.units:
            unitlist = self.convertelement(theoo)
            for unit in unitlist:
                thetargetfile.addunit(unit)
        thetargetfile.removeduplicates(duplicatestyle)
        return thetargetfile


def verifyoptions(options):
    """verifies the commandline options"""
    if not options.pot and not options.targetlanguage:
        raise ValueError("You must specify the target language unless generating POT files (-P)")


def convertoo(inputfile, outputfile, templates, pot=False, sourcelanguage=None, targetlanguage=None, duplicatestyle="msgid_comment", multifilestyle="single"):
    """reads in stdin using inputstore class, converts using convertorclass, writes to stdout"""
    inputstore = oo.oofile()
    if hasattr(inputfile, "filename"):
        inputfilename = inputfile.filename
    else:
        inputfilename = "(input file name not known)"
    inputstore.filename = inputfilename
    inputstore.parse(inputfile.read())
    if not sourcelanguage:
        testlangtype = targetlanguage or (inputstore and inputstore.languages[0]) or ""
        if testlangtype.isdigit():
            sourcelanguage = "01"
        else:
            sourcelanguage = "en-US"
    if not sourcelanguage in inputstore.languages:
        logger.warning("sourcelanguage '%s' not found in inputfile '%s' "
                       "(contains %s)",
                       sourcelanguage, inputfilename,
                       ", ".join(inputstore.languages))
    if targetlanguage and targetlanguage not in inputstore.languages:
        logger.warning("targetlanguage '%s' not found in inputfile '%s' "
                       "(contains %s)",
                       targetlanguage, inputfilename,
                       ", ".join(inputstore.languages))
    convertor = oo2po(sourcelanguage, targetlanguage, blankmsgstr=pot, long_keys=multifilestyle != "single")
    outputstore = convertor.convertstore(inputstore, duplicatestyle)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
                "oo": ("po", convertoo),
                "sdf": ("po", convertoo),
              }
    # always treat the input as an archive unless it is a directory
    archiveformats = {(None, "input"): oo.oomultifile}
    parser = convert.ArchiveConvertOptionParser(formats, usepots=True,
                                                description=__doc__,
                                                archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
                      help="set target language to extract from oo file (e.g. af-ZA)",
                      metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage", default=None,
            help="set source language code (default en-US)", metavar="LANG")
    parser.add_option("", "--nonrecursiveinput", dest="allowrecursiveinput",
                      default=True, action="store_false",
                      help="don't treat the input oo as a recursive store")
    parser.add_duplicates_option()
    parser.add_multifile_option()
    parser.passthrough.append("pot")
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.verifyoptions = verifyoptions
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = oo2xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2003-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert an OpenOffice.org (SDF) localization file to XLIFF localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/oo2po.html
for examples and usage instructions.
"""

import logging
from urllib import urlencode

from translate.storage import oo, xliff


# TODO: support using one GSI file as template, another as input (for when English is in one and translation in another)

logger = logging.getLogger(__name__)


class oo2xliff:

    def __init__(self, sourcelanguage, targetlanguage, blankmsgstr=False, long_keys=False):
        """construct an oo2xliff converter for the specified languages"""
        self.sourcelanguage = sourcelanguage
        self.targetlanguage = targetlanguage
        self.blankmsgstr = blankmsgstr
        self.long_keys = long_keys

    def maketargetunit(self, part1, part2, translators_comment, key, subkey):
        """makes a base unit (.po or XLIFF) out of a subkey of two parts"""
        #TODO: Do better
        text1 = getattr(part1, subkey)
        if text1 == "":
            return None
        text2 = getattr(part2, subkey)

        unit = xliff.xliffunit(text1)
        unit.target = text2
        if unit.target:
            unit.markfuzzy(False)
        else:
            unit.markfuzzy(True)
        unit.addlocation(key + "." + subkey)
        if getattr(translators_comment, subkey).strip() != "":
            unit.addnote(getattr(translators_comment, subkey), origin="developer")
        return unit

    def convertelement(self, theoo):
        """convert an oo element into a list of base units (.po or XLIFF)"""
        if self.sourcelanguage in theoo.languages:
            part1 = theoo.languages[self.sourcelanguage]
        else:
            logging.error("/".join(theoo.lines[0].getkey()) +
                          "language not found: %s", self.sourcelanguage)
            return []
        if self.blankmsgstr:
            # use a blank part2
            part2 = oo.ooline()
        else:
            if self.targetlanguage in theoo.languages:
                part2 = theoo.languages[self.targetlanguage]
            else:
                # if the language doesn't exist, the translation is missing ... so make it blank
                part2 = oo.ooline()
        if "x-comment" in theoo.languages:
            translators_comment = theoo.languages["x-comment"]
        else:
            translators_comment = oo.ooline()
        key = oo.makekey(part1.getkey(), self.long_keys)
        unitlist = []
        for subkey in ("text", "quickhelptext", "title"):
            unit = self.maketargetunit(part1, part2, translators_comment, key, subkey)
            if unit is not None:
                unitlist.append(unit)
        return unitlist

    def convertstore(self, theoofile, duplicatestyle="msgctxt"):
        """converts an entire oo file to a base class format (.po or XLIFF)"""
        thetargetfile = xliff.xlifffile()
        thetargetfile.setsourcelanguage(self.sourcelanguage)
        thetargetfile.settargetlanguage(self.targetlanguage)
        # create a header for the file
        bug_url = 'http://qa.openoffice.org/issues/enter_bug.cgi?%s' % \
                  urlencode({
                      "subcomponent": "ui",
                      "comment": "",
                      "short_desc": "Localization issue in file: %s" %
                                    theoofile.filename,
                      "component": "l10n",
                      "form_name": "enter_issue",
                  })
        # go through the oo and convert each element
        for theoo in theoofile.units:
            unitlist = self.convertelement(theoo)
            for unit in unitlist:
                thetargetfile.addunit(unit)
        return thetargetfile


def verifyoptions(options):
    """verifies the commandline options"""
    if not options.targetlanguage:
        raise ValueError("You must specify the target language.")


def convertoo(inputfile, outputfile, templates, pot=False, sourcelanguage=None, targetlanguage=None, duplicatestyle="msgctxt", multifilestyle="single"):
    """reads in stdin using inputstore class, converts using convertorclass, writes to stdout"""
    inputstore = oo.oofile()
    if hasattr(inputfile, "filename"):
        inputfilename = inputfile.filename
    else:
        inputfilename = "(input file name not known)"
    inputstore.filename = inputfilename
    inputstore.parse(inputfile.read())
    if not sourcelanguage:
        testlangtype = targetlanguage or (inputstore and inputstore.languages[0]) or ""
        if testlangtype.isdigit():
            sourcelanguage = "01"
        else:
            sourcelanguage = "en-US"
    if not sourcelanguage in inputstore.languages:
        logger.warning("sourcelanguage '%s' not found in inputfile '%s' "
                       "(contains %s)",
                       sourcelanguage, inputfilename,
                       ", ".join(inputstore.languages))
    if not pot and (targetlanguage and
                    targetlanguage not in inputstore.languages):
        logger.warning("targetlanguage '%s' not found in inputfile '%s' "
                       "(contains %s)",
                       targetlanguage, inputfilename,
                       ", ".join(inputstore.languages))
    convertor = oo2xliff(sourcelanguage, targetlanguage, blankmsgstr=pot,
                         long_keys=(multifilestyle != "single"))
    outputstore = convertor.convertstore(inputstore, duplicatestyle)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
                "oo": ("xlf", convertoo),
                "sdf": ("xlf", convertoo),
              }
    # always treat the input as an archive unless it is a directory
    archiveformats = {(None, "input"): oo.oomultifile}
    parser = convert.ArchiveConvertOptionParser(formats, usepots=False,
                                                description=__doc__,
                                                archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
                      help="set target language to extract from oo file (e.g. af-ZA)",
                      metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage", default=None,
            help="set source language code (default en-US)", metavar="LANG")
    parser.add_option("", "--nonrecursiveinput", dest="allowrecursiveinput",
                      default=True, action="store_false",
                      help="don't treat the input oo as a recursive store")
    parser.add_duplicates_option()
    parser.add_multifile_option()
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.verifyoptions = verifyoptions
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = php2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert PHP localization files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/php2po.html
for examples and usage instructions.
"""

import logging

from translate.convert import convert
from translate.storage import php, po


logger = logging.getLogger(__name__)


class php2po:
    """Convert a .php file to a .po file for handling the translation."""

    def convertstore(self, inputstore, duplicatestyle="msgctxt"):
        """Convert a .php file to a .po file."""
        outputstore = po.pofile()
        outputheader = outputstore.header()
        outputheader.addnote("extracted from %s" % inputstore.filename,
                             "developer")

        for inputunit in inputstore.units:
            outputunit = self.convertunit(inputunit, "developer")
            if outputunit is not None:
                outputstore.addunit(outputunit)
        outputstore.removeduplicates(duplicatestyle)
        return outputstore

    def mergestore(self, templatestore, inputstore, blankmsgstr=False,
                   duplicatestyle="msgctxt"):
        """Convert two .php files to a .po file."""
        outputstore = po.pofile()
        outputheader = outputstore.header()
        outputheader.addnote("extracted from %s, %s" % (templatestore.filename,
                                                        inputstore.filename),
                             "developer")

        inputstore.makeindex()
        # Loop through the original file, looking at units one by one.
        for templateunit in templatestore.units:
            outputunit = self.convertunit(templateunit, "developer")
            # Try and find a translation of the same name.
            if templateunit.name in inputstore.locationindex:
                translatedinputunit = inputstore.locationindex[templateunit.name]
                # Need to check that this comment is not a copy of the
                # developer comments.
                translatedoutputunit = self.convertunit(translatedinputunit,
                                                        "translator")
            else:
                translatedoutputunit = None
            # If we have a valid po unit, get the translation and add it.
            if outputunit is not None:
                if translatedoutputunit is not None and not blankmsgstr:
                    outputunit.target = translatedoutputunit.source
                outputstore.addunit(outputunit)
            elif translatedoutputunit is not None:
                logger("error converting original properties definition %s",
                       templateunit.name)
        outputstore.removeduplicates(duplicatestyle)
        return outputstore

    def convertunit(self, inputunit, origin):
        """Convert a .php unit to a .po unit."""
        outputunit = po.pounit(encoding="UTF-8")
        outputunit.addnote(inputunit.getnotes(origin), origin)
        outputunit.addlocation("".join(inputunit.getlocations()))
        outputunit.source = inputunit.source
        outputunit.target = ""
        return outputunit


def convertphp(inputfile, outputfile, templatefile, pot=False,
               duplicatestyle="msgctxt"):
    """Read inputfile using php, convert using php2po, write to outputfile."""
    inputstore = php.phpfile(inputfile)
    convertor = php2po()
    if templatefile is None:
        outputstore = convertor.convertstore(inputstore,
                                             duplicatestyle=duplicatestyle)
    else:
        templatestore = php.phpfile(templatefile)
        outputstore = convertor.mergestore(templatestore, inputstore,
                                           blankmsgstr=pot,
                                           duplicatestyle=duplicatestyle)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    formats = {
            "php": ("po", convertphp), ("php", "php"): ("po", convertphp),
            "html": ("po", convertphp), ("html", "html"): ("po", convertphp),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2csv
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2003-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to Comma-Separated Value (.csv) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/csv2po.html
for examples and usage instructions.
"""

from translate.storage import csvl10n, po


class po2csv:

    def convertcomments(self, inputunit):
        return " ".join(inputunit.getlocations())

    def convertunit(self, inputunit):
        csvunit = csvl10n.csvunit()
        if inputunit.isheader():
            return None
            #csvunit.location = "location"
            #csvunit.source = "source"
            #csvunit.target = "target"
        elif inputunit.isblank():
            return None
        else:
            csvunit.location = self.convertcomments(inputunit)
            csvunit.source = inputunit.source
            csvunit.target = inputunit.target
        return csvunit

    def convertplurals(self, inputunit):
        """Convert PO plural units

        We only convert the first plural form.  So languages with multiple
        plurals are not handled.  For single plural languages we simply
        skip this plural extraction.
        """
        if len(inputunit.target.strings) == 1:  # No plural forms
            return None
        csvunit = csvl10n.csvunit()
        csvunit.location = self.convertcomments(inputunit)
        csvunit.source = inputunit.source.strings[1]
        csvunit.target = inputunit.target.strings[1]
        return csvunit

    def convertstore(self, inputstore, columnorder=None):
        if columnorder is None:
            columnorder = ['location', 'source', 'target']
        outputstore = csvl10n.csvfile(fieldnames=columnorder)
        for inputunit in inputstore.units:
            outputunit = self.convertunit(inputunit)
            if outputunit is not None:
                outputstore.addunit(outputunit)
            if inputunit.hasplural():
                outputunit = self.convertplurals(inputunit)
                if outputunit is not None:
                    outputstore.addunit(outputunit)
        return outputstore


def convertcsv(inputfile, outputfile, templatefile, columnorder=None):
    """reads in inputfile using po, converts using po2csv, writes to outputfile"""
    # note that templatefile is not used, but it is required by the converter...
    inputstore = po.pofile(inputfile)
    if inputstore.isempty():
        return 0
    convertor = po2csv()
    outputstore = convertor.convertstore(inputstore, columnorder)
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"po": ("csv", convertcsv)}
    parser = convert.ConvertOptionParser(formats, description=__doc__)
    parser.add_option("", "--columnorder", dest="columnorder", default=None,
        help="specify the order and position of columns (location,source,target)")
    parser.passthrough.append("columnorder")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2dtd
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2009,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Converts a Gettext PO file to a UTF-8 encoded Mozilla .dtd file.

.. note: Conversion is either done using a template plus PO file or just
   using the .po file.
"""

import warnings

from translate.convert import accesskey, convert
from translate.misc import quote
from translate.storage import dtd, po


def dtdwarning(message, category, filename, lineno, line=None):
    return "Warning: %s\n" % message
warnings.formatwarning = dtdwarning


def applytranslation(entity, dtdunit, inputunit, mixedentities):
    """applies the translation for entity in the po unit to the dtd unit"""
    # this converts the po-style string to a dtd-style string
    unquotedstr = inputunit.target
    # check there aren't missing entities...
    if len(unquotedstr.strip()) == 0:
        return
    # handle mixed entities
    for labelsuffix in dtd.labelsuffixes:
        if entity.endswith(labelsuffix):
            if entity in mixedentities:
                unquotedstr, akey = accesskey.extract(unquotedstr)
                break
    else:
        for akeytype in dtd.accesskeysuffixes:
            if entity.endswith(akeytype):
                if entity in mixedentities:
                    label, unquotedstr = accesskey.extract(unquotedstr)
                    if not unquotedstr:
                        warnings.warn("Could not find accesskey for %s" % entity)
                        # Use the source language accesskey
                        label, unquotedstr = accesskey.extract(inputunit.source)
                    else:
                        original = dtdunit.source
                        # For the sake of diffs we keep the case of the
                        # accesskey the same if we know the translation didn't
                        # change. Casing matters in XUL.
                        if unquotedstr == dtdunit.source and original.lower() == unquotedstr.lower():
                            if original.isupper():
                                unquotedstr = unquotedstr.upper()
                            elif original.islower():
                                unquotedstr = unquotedstr.lower()
    dtdunit.source = unquotedstr


class redtd:
    """this is a convertor class that creates a new dtd based on a template using translations in a po"""

    def __init__(self, dtdfile, android=False, remove_untranslated=False):
        self.dtdfile = dtdfile
        self.mixer = accesskey.UnitMixer(dtd.labelsuffixes, dtd.accesskeysuffixes)
        self.android = False
        self.remove_untranslated = remove_untranslated

    def convertstore(self, inputstore, includefuzzy=False):
        for inunit in inputstore.units:
            self.handleinunit(inunit, includefuzzy)
        return self.dtdfile

    def handleinunit(self, inunit, includefuzzy):
        entities = inunit.getlocations()
        mixedentities = self.mixer.match_entities(entities)
        for entity in entities:
            if entity in self.dtdfile.id_index:
                # now we need to replace the definition of entity with msgstr
                dtdunit = self.dtdfile.id_index[entity]  # find the dtd
                if inunit.istranslated() or not bool(inunit.source):
                    applytranslation(entity, dtdunit, inunit, mixedentities)
                elif self.remove_untranslated and not (includefuzzy and inunit.isfuzzy()):
                    dtdunit.entity = None
                else:
                    applytranslation(entity, dtdunit, inunit, mixedentities)


class po2dtd:
    """this is a convertor class that creates a new dtd file based on a po file without a template"""

    def __init__(self, android=False, remove_untranslated=False):
        self.android = android
        self.remove_untranslated = remove_untranslated

    def convertcomments(self, inputunit, dtdunit):
        entities = inputunit.getlocations()
        if len(entities) > 1:
            # don't yet handle multiple entities
            dtdunit.comments.append(("conversionnote", '<!-- CONVERSION NOTE - multiple entities -->\n'))
            dtdunit.entity = entities[0]
        elif len(entities) == 1:
            dtdunit.entity = entities[0]
        else:
            # this produces a blank entity, which doesn't write anything out
            dtdunit.entity = ""

        if inputunit.isfuzzy():
            dtdunit.comments.append(("potype", "fuzzy\n"))
        for note in inputunit.getnotes("translator").split("\n"):
            if not note:
                continue
            note = quote.unstripcomment(note)
            if (note.find('LOCALIZATION NOTE') == -1) or (note.find('GROUP') == -1):
                dtdunit.comments.append(("comment", note))
        # msgidcomments are special - they're actually localization notes
        msgidcomment = inputunit._extract_msgidcomments()
        if msgidcomment:
            locnote = quote.unstripcomment("LOCALIZATION NOTE (" + dtdunit.entity + "): " + msgidcomment)
            dtdunit.comments.append(("locnote", locnote))

    def convertstrings(self, inputunit, dtdunit):
        if inputunit.istranslated() or not bool(inputunit.source):
            unquoted = inputunit.target
        elif self.remove_untranslated:
            unquoted = None
        else:
            unquoted = inputunit.source
        dtdunit.source = dtd.removeinvalidamps(dtdunit.entity, unquoted)

    def convertunit(self, inputunit):
        dtdunit = dtd.dtdunit()
        self.convertcomments(inputunit, dtdunit)
        self.convertstrings(inputunit, dtdunit)
        return dtdunit

    def convertstore(self, inputstore, includefuzzy=False):
        outputstore = dtd.dtdfile(android=self.android)
        self.currentgroups = []
        for inputunit in inputstore.units:
            if ((includefuzzy or not inputunit.isfuzzy()) and
                (inputunit.istranslated() or not self.remove_untranslated)):
                dtdunit = self.convertunit(inputunit)
                if dtdunit is not None:
                    outputstore.addunit(dtdunit)
        return outputstore


def convertdtd(inputfile, outputfile, templatefile, includefuzzy=False,
               remove_untranslated=False, outputthreshold=None):
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    # Some of the DTD files used for Firefox Mobile are actually completely
    # different with different escaping and quoting rules. The best way to
    # identify them seems to be on their file path in the tree (based on code
    # in compare-locales).
    android_dtd = False
    header_comment = u""
    input_header = inputstore.header()
    if input_header:
        header_comment = input_header.getnotes("developer")
        if "embedding/android" in header_comment or "mobile/android/base" in header_comment:
            android_dtd = True

    if templatefile is None:
        convertor = po2dtd(android=android_dtd,
                           remove_untranslated=remove_untranslated)
    else:
        templatestore = dtd.dtdfile(templatefile, android=android_dtd)
        convertor = redtd(templatestore, android=android_dtd,
                          remove_untranslated=remove_untranslated)
    outputstore = convertor.convertstore(inputstore, includefuzzy)
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    # handle command line options
    formats = {"po": ("dtd", convertdtd), ("po", "dtd"): ("dtd", convertdtd)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_option("", "--removeuntranslated", dest="remove_untranslated",
            default=False, action="store_true",
            help="remove untranslated strings from output")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.passthrough.append("remove_untranslated")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2html
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to HTML files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/html2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import html, po


class po2html:
    """po2html can take a po file and generate html. best to give it a
    template file otherwise will just concat msgstrs"""

    def lookup(self, string):
        unit = self.inputstore.sourceindex.get(string, None)
        if unit is None:
            return string
        unit = unit[0]
        if unit.istranslated():
            return unit.target
        if self.includefuzzy and unit.isfuzzy():
            return unit.target
        return unit.source

    def mergestore(self, inputstore, templatetext, includefuzzy):
        """converts a file to .po format"""
        self.inputstore = inputstore
        self.inputstore.makeindex()
        self.includefuzzy = includefuzzy
        output_store = html.htmlfile(inputfile=templatetext, callback=self.lookup)
        return output_store.filesrc


def converthtml(inputfile, outputfile, templatefile, includefuzzy=False,
                outputthreshold=None):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    convertor = po2html()
    if templatefile is None:
        raise ValueError("must have template file for HTML files")
    else:
        outputstring = convertor.mergestore(inputstore, templatefile,
                                            includefuzzy)
    outputfilepos = outputfile.tell()
    outputfile.write(outputstring.encode('utf-8'))
    return 1


def main(argv=None):
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    formats = {
        ("po", "htm"): ("htm", converthtml),
        ("po", "html"): ("html", converthtml),
        ("po", "xhtml"): ("xhtml", converthtml),
        ("po"): ("html", converthtml),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2ical
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to iCal files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ical2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import factory, ical


class reical:

    def __init__(self, templatefile, inputstore):
        self.templatefile = templatefile
        self.templatestore = ical.icalfile(templatefile)
        self.inputstore = inputstore

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inputstore.makeindex()
        for unit in self.templatestore.units:
            for location in unit.getlocations():
                if location in self.inputstore.locationindex:
                    inputunit = self.inputstore.locationindex[location]
                    if inputunit.isfuzzy() and not self.includefuzzy:
                        unit.target = unit.source
                    else:
                        unit.target = inputunit.target
                else:
                    unit.target = unit.source
        return str(self.templatestore)


def convertical(inputfile, outputfile, templatefile, includefuzzy=False,
                outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("must have template file for iCal files")
    else:
        convertor = reical(templatefile, inputstore)
    outputstring = convertor.convertstore(includefuzzy)
    outputfile.write(outputstring)
    return 1


def main(argv=None):
    # handle command line options
    formats = {("po", "ics"): ("ics", convertical)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2ini
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to .ini files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ini2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import factory


class reini:

    def __init__(self, templatefile, inputstore, dialect="default"):
        from translate.storage import ini
        self.templatefile = templatefile
        self.templatestore = ini.inifile(templatefile, dialect=dialect)
        self.inputstore = inputstore

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inputstore.makeindex()
        for unit in self.templatestore.units:
            for location in unit.getlocations():
                if location in self.inputstore.locationindex:
                    inputunit = self.inputstore.locationindex[location]
                    if inputunit.isfuzzy() and not self.includefuzzy:
                        unit.target = unit.source
                    else:
                        unit.target = inputunit.target
                else:
                    unit.target = unit.source
        return str(self.templatestore)


def convertini(inputfile, outputfile, templatefile, includefuzzy=False, dialect="default",
               outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("must have template file for ini files")
    else:
        convertor = reini(templatefile, inputstore, dialect)
    outputstring = convertor.convertstore(includefuzzy)
    outputfile.write(outputstring)
    return 1


def convertisl(inputfile, outputfile, templatefile, includefuzzy=False, dialect="inno",
               outputthreshold=None):
    convertini(inputfile, outputfile, templatefile, includefuzzy, dialect,
               outputthreshold=outputthreshold)


def main(argv=None):
    # handle command line options
    formats = {
               ("po", "ini"): ("ini", convertini),
               ("po", "isl"): ("isl", convertisl),
              }
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2json
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to JSON files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/json2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import factory


class rejson:

    def __init__(self, templatefile, inputstore):
        from translate.storage import jsonl10n
        self.templatefile = templatefile
        self.templatestore = jsonl10n.JsonFile(templatefile)
        self.inputstore = inputstore

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inputstore.makeindex()
        for unit in self.templatestore.units:
            inputunit = self.inputstore.locationindex.get(unit.getid())
            if inputunit is not None:
                if inputunit.isfuzzy() and not self.includefuzzy:
                    unit.target = unit.source
                else:
                    unit.target = inputunit.target
            else:
                unit.target = unit.source
        return str(self.templatestore)


def convertjson(inputfile, outputfile, templatefile, includefuzzy=False,
                outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("Must have template file for JSON files")
    else:
        convertor = rejson(templatefile, inputstore)
    outputstring = convertor.convertstore(includefuzzy)
    outputfile.write(outputstring)
    return 1


def main(argv=None):
    # handle command line options
    formats = {
               ("po", "json"): ("json", convertjson),
              }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2moz
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to Mozilla .dtd and .properties files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/moz2po.html
for examples and usage instructions.
"""

import os.path

from translate.convert import (convert, po2dtd, po2mozlang, po2prop,
                               prop2mozfunny)


class MozConvertOptionParser(convert.ConvertOptionParser):

    def __init__(self, formats, usetemplates=False, usepots=False,
                 description=None):
        convert.ConvertOptionParser.__init__(self, formats, usetemplates, usepots,
                                             description=description)

    def splitinputext(self, inputpath):
        """splits a inputpath into name and extension"""
        # TODO: not sure if this should be here, was in po2moz
        d, n = os.path.dirname(inputpath), os.path.basename(inputpath)
        s = n.find(".")
        if s == -1:
            return (inputpath, "")
        root = os.path.join(d, n[:s])
        ext = n[s+1:]
        return (root, ext)

    def recursiveprocess(self, options):
        """recurse through directories and convert files"""
        self.replacer.replacestring = options.locale
        result = super(MozConvertOptionParser, self).recursiveprocess(options)
        return result


def main(argv=None):
    # handle command line options
    formats = {("dtd.po", "dtd"): ("dtd", po2dtd.convertdtd),
               ("properties.po", "properties"): ("properties",
                                                 po2prop.convertmozillaprop),
               ("it.po", "it"): ("it", prop2mozfunny.po2it),
               ("ini.po", "ini"): ("ini", prop2mozfunny.po2ini),
               ("inc.po", "inc"): ("inc", prop2mozfunny.po2inc),
               ("lang.po", "lang"): ("lang", po2mozlang.convertlang),
               # (None, "*"): ("*", convert.copytemplate),
               ("*", "*"): ("*", convert.copyinput),
               "*": ("*", convert.copyinput)}
    # handle search and replace
    replacer = convert.Replacer("${locale}", None)
    for replaceformat in ("js", "rdf", "manifest"):
        formats[(None, replaceformat)] = (replaceformat,
                                          replacer.searchreplacetemplate)
        formats[(replaceformat, replaceformat)] = (replaceformat,
                                                   replacer.searchreplaceinput)
        formats[replaceformat] = (replaceformat, replacer.searchreplaceinput)
    parser = MozConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_option("-l", "--locale", dest="locale", default=None,
        help="set output locale (required as this sets the directory names)",
        metavar="LOCALE")
    parser.add_option("", "--removeuntranslated", dest="remove_untranslated",
            default=False, action="store_true",
            help="remove untranslated strings from output")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.passthrough.append("remove_untranslated")
    parser.replacer = replacer
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2mozlang
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008,2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

# Original Author: Dan Schafer <dschafer@mozilla.com>
# Date: 10 Jun 2008

"""Convert Gettext PO localization files to Mozilla .lang files.
"""

from translate.convert import convert
from translate.storage import mozilla_lang as lang, po


class po2lang:

    def __init__(self, duplicatestyle="msgctxt", mark_active=True):
        self.duplicatestyle = duplicatestyle
        self.mark_active = mark_active

    def convertstore(self, inputstore, includefuzzy=False):
        """converts a file to .lang format"""
        thetargetfile = lang.LangStore(mark_active=self.mark_active)

        # Run over the po units
        for pounit in inputstore.units:
            if pounit.isheader() or not pounit.istranslatable():
                continue
            newunit = thetargetfile.addsourceunit(pounit.source)
            if includefuzzy or not pounit.isfuzzy():
                newunit.settarget(pounit.target)
            else:
                newunit.settarget("")
            if pounit.getnotes('developer'):
                newunit.addnote(pounit.getnotes('developer'), 'developer')
        return thetargetfile


def convertlang(inputfile, outputfile, templates, includefuzzy=False, mark_active=True,
                outputthreshold=None, remove_untranslated=None):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if inputstore.isempty():
        return 0
    convertor = po2lang(mark_active=mark_active)
    outputstore = convertor.convertstore(inputstore, includefuzzy)
    outputfile.write(str(outputstore))
    return 1


formats = {
    "po": ("lang", convertlang),
    ("po", "lang"): ("lang", convertlang),
}


def main(argv=None):
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                           description=__doc__)
    parser.add_option("", "--mark-active", dest="mark_active", default=False,
            action="store_true", help="mark the file as active")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.passthrough.append("mark_active")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2oo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2010-2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to an OpenOffice.org (SDF) localization file.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/oo2po.html
for examples and usage instructions.
"""

import logging
import os
import time

from translate.convert import convert
from translate.filters import autocorrect, checks, pofilter
from translate.storage import factory, oo


logger = logging.getLogger(__name__)


class reoo:

    def __init__(self, templatefile, languages=None, timestamp=None, includefuzzy=False, long_keys=False, filteraction="exclude"):
        """construct a reoo converter for the specified languages (timestamp=0 means leave unchanged)"""
        # languages is a pair of language ids
        self.long_keys = long_keys
        self.readoo(templatefile)
        self.languages = languages
        self.filteraction = filteraction
        if timestamp is None:
            self.timestamp = time.strptime("2002-02-02 02:02:02", "%Y-%m-%d %H:%M:%S")
        else:
            self.timestamp = timestamp
        if self.timestamp:
            self.timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", self.timestamp)
        else:
            self.timestamp_str = None
        self.includefuzzy = includefuzzy

    def makeindex(self):
        """makes an index of the oo keys that are used in the source file"""
        self.index = {}
        for ookey, theoo in self.o.ookeys.iteritems():
            sourcekey = oo.makekey(ookey, self.long_keys)
            self.index[sourcekey] = theoo

    def readoo(self, of):
        """read in the oo from the file"""
        oosrc = of.read()
        self.o = oo.oofile()
        self.o.parse(oosrc)
        self.makeindex()

    def handleunit(self, unit):
        # TODO: make this work for multiple columns in oo...
        locations = unit.getlocations()
        # technically our formats should just have one location for each entry...
        # but we handle multiple ones just to be safe...
        for location in locations:
            subkeypos = location.rfind('.')
            subkey = location[subkeypos+1:]
            key = location[:subkeypos]
            # this is just to handle our old system of using %s/%s:%s instead of %s/%s#%s
            key = key.replace(':', '#')
            # this is to handle using / instead of \ in the sourcefile...
            key = key.replace('\\', '/')
            key = oo.normalizefilename(key)
            if key in self.index:
                # now we need to replace the definition of entity with msgstr
                theoo = self.index[key]  # find the oo
                self.applytranslation(key, subkey, theoo, unit)
            else:
                logger.warning("couldn't find key %s from po in %d keys",
                               key, len(self.index))
                try:
                    sourceunitlines = str(unit)
                    if isinstance(sourceunitlines, unicode):
                        sourceunitlines = sourceunitlines.encode("utf-8")
                    logger.warning(sourceunitlines)
                except:
                    logger.warning("error outputting source unit %r", str(unit))

    def applytranslation(self, key, subkey, theoo, unit):
        """applies the translation from the source unit to the oo unit"""
        if not self.includefuzzy and unit.isfuzzy():
            return
        makecopy = False
        if self.languages is None:
            part1 = theoo.lines[0]
            if len(theoo.lines) > 1:
                part2 = theoo.lines[1]
            else:
                makecopy = True
        else:
            part1 = theoo.languages[self.languages[0]]
            if self.languages[1] in theoo.languages:
                part2 = theoo.languages[self.languages[1]]
            else:
                makecopy = True
        if makecopy:
            part2 = oo.ooline(part1.getparts())
        unquotedid = unit.source
        unquotedstr = unit.target
        # If there is no translation, we don't want to add a line
        if len(unquotedstr) == 0:
            return
        if isinstance(unquotedstr, unicode):
            unquotedstr = unquotedstr.encode("UTF-8")
        # finally set the new definition in the oo, but not if its empty
        if len(unquotedstr) > 0:
            setattr(part2, subkey, unquotedstr)
        # set the modified time
        if self.timestamp_str:
            part2.timestamp = self.timestamp_str
        if self.languages:
            part2.languageid = self.languages[1]
        if makecopy:
            theoo.addline(part2)

    def convertstore(self, sourcestore):
        # translate the strings
        for unit in sourcestore.units:
            # there may be more than one element due to msguniq merge
            if filter.validelement(unit, sourcestore.filename, self.filteraction):
                self.handleunit(unit)
        # return the modified oo file object
        return self.o


def getmtime(filename):
    import stat
    return time.localtime(os.stat(filename)[stat.ST_MTIME])


class oocheckfilter(pofilter.pocheckfilter):

    def validelement(self, unit, filename, filteraction):
        """Returns whether or not to use unit in conversion. (filename is just for error reporting)"""
        if filteraction == "none":
            return True
        filterresult = self.filterunit(unit)
        if filterresult:
            if filterresult != autocorrect:
                for filtername, filtermessage in filterresult.iteritems():
                    location = unit.getlocations()[0].encode('utf-8')
                    if filtername in self.options.error:
                        logger.error("Error at %s::%s: %s",
                                     filename, location, filtermessage)
                        return not filteraction in ["exclude-all", "exclude-serious"]
                    if filtername in self.options.warning or self.options.alwayswarn:
                        logger.warning("Warning at %s::%s: %s",
                                       filename, location, filtermessage)
                        return not filteraction in ["exclude-all"]
        return True


class oofilteroptions:
    error = ['variables', 'xmltags', 'escapes']
    warning = ['blank']
    #To only issue warnings for tests listed in warning, change the following to False:
    alwayswarn = True
    limitfilters = error + warning
    #To use all available tests, uncomment the following:
    #limitfilters = []
    #To exclude certain tests, list them in here:
    excludefilters = {}
    includefuzzy = False
    includereview = False
    autocorrect = False

options = oofilteroptions()
filter = oocheckfilter(options, [checks.OpenOfficeChecker, checks.StandardUnitChecker], checks.openofficeconfig)


def convertoo(inputfile, outputfile, templatefile, sourcelanguage=None,
              targetlanguage=None, timestamp=None, includefuzzy=False,
              multifilestyle="single", skip_source=False, filteraction=None,
              outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    inputstore.filename = getattr(inputfile, 'name', '')
    if not targetlanguage:
        raise ValueError("You must specify the target language")
    if not sourcelanguage:
        if targetlanguage.isdigit():
            sourcelanguage = "01"
        else:
            sourcelanguage = "en-US"
    languages = (sourcelanguage, targetlanguage)
    if templatefile is None:
        raise ValueError("must have template file for oo files")
    else:
        convertor = reoo(templatefile, languages=languages,
                         timestamp=timestamp, includefuzzy=includefuzzy,
                         long_keys=multifilestyle != "single",
                         filteraction=filteraction)
    outputstore = convertor.convertstore(inputstore)
    # TODO: check if we need to manually delete missing items
    outputfile.write(outputstore.__str__(skip_source, targetlanguage))
    return True


def main(argv=None):
    formats = {
                ("po", "oo"): ("oo", convertoo),
                ("xlf", "oo"): ("oo", convertoo),
                ("po", "sdf"): ("sdf", convertoo),
              }
    # always treat the input as an archive unless it is a directory
    archiveformats = {(None, "output"): oo.oomultifile, (None, "template"): oo.oomultifile}
    parser = convert.ArchiveConvertOptionParser(formats, usetemplates=True, description=__doc__, archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
                      help="set target language code (e.g. af-ZA) [required]",
                      metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage",
                      default=None,
                      help="set source language code (default en-US)",
                      metavar="LANG")
    parser.add_option("-T", "--keeptimestamp", dest="timestamp", default=None,
                      action="store_const", const=0,
            help="don't change the timestamps of the strings")
    parser.add_option("", "--nonrecursiveoutput", dest="allowrecursiveoutput",
                      default=True, action="store_false",
                      help="don't treat the output oo as a recursive store")
    parser.add_option("", "--nonrecursivetemplate",
                      dest="allowrecursivetemplate", default=True,
                      action="store_false",
                      help="don't treat the template oo as a recursive store")
    parser.add_option("", "--skipsource", dest="skip_source", default=False,
                      action="store_true",
                      help="don't output the source language, but fallback to it where needed")
    parser.add_option("", "--filteraction", dest="filteraction", default="none", metavar="ACTION",
                      help="action on pofilter failure: none (default), warn, exclude-serious, exclude-all")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.add_multifile_option()
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.passthrough.append("timestamp")
    parser.passthrough.append("skip_source")
    parser.passthrough.append("filteraction")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2php
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to PHP localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/php2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.misc import quote
from translate.storage import php, po


eol = "\n"


class rephp:

    def __init__(self, templatefile, inputstore):
        self.templatefile = templatefile
        self.inputstore = inputstore
        self.inmultilinemsgid = False
        self.inecho = False
        self.inarray = False
        self.equaldel = "="
        self.enddel = ";"
        self.prename = ""
        self.quotechar = ""

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inputstore.makeindex()
        outputlines = []
        for line in self.templatefile.readlines():
            outputstr = self.convertline(line)
            outputlines.append(outputstr)
        return outputlines

    def convertline(self, line):
        line = unicode(line, 'utf-8')
        returnline = ""
        # handle multiline msgid if we're in one
        if self.inmultilinemsgid:
            # see if there's more
            endpos = line.rfind("%s%s" % (self.quotechar, self.enddel))
            # if there was no '; or the quote is escaped, we have to continue
            if endpos >= 0 and line[endpos-1] != '\\':
                self.inmultilinemsgid = False
            # if we're echoing...
            if self.inecho:
                returnline = line
        # otherwise, this could be a comment
        elif line.strip()[:2] == '//' or line.strip()[:2] == '/*':
            returnline = quote.rstripeol(line) + eol
        elif line.find('array(') != -1:
            self.inarray = True
            self.prename = line[:line.find('=')].strip() + "->"
            self.equaldel = "=>"
            self.enddel = ","
            returnline = quote.rstripeol(line) + eol
        elif self.inarray and line.find(');') != -1:
            self.inarray = False
            self.equaldel = "="
            self.enddel = ";"
            self.prename = ""
            returnline = quote.rstripeol(line) + eol
        else:
            line = quote.rstripeol(line)
            equalspos = line.find(self.equaldel)
            hashpos = line.find("#")
            # if no equals, just repeat it
            if equalspos == -1:
                returnline = quote.rstripeol(line) + eol
            elif 0 <= hashpos < equalspos:
                # Assume that this is a '#' comment line
                returnline = quote.rstripeol(line) + eol
            # otherwise, this is a definition
            else:
                # now deal with the current string...
                key = line[:equalspos].rstrip()
                lookupkey = self.prename + key.lstrip()
                # Calculate space around the equal sign
                prespace = line[len(line[:equalspos].rstrip()):equalspos]
                postspacestart = len(line[equalspos+len(self.equaldel):])
                postspaceend = len(line[equalspos+len(self.equaldel):].lstrip())
                postspace = line[equalspos+len(self.equaldel):equalspos+(postspacestart-postspaceend)+len(self.equaldel)]
                self.quotechar = line[equalspos+(postspacestart-postspaceend)+len(self.equaldel)]
                inlinecomment_pos = line.rfind("%s%s" % (self.quotechar,
                                                         self.enddel))
                if inlinecomment_pos > -1:
                    inlinecomment = line[inlinecomment_pos+2:]
                else:
                    inlinecomment = ""
                if lookupkey in self.inputstore.locationindex:
                    unit = self.inputstore.locationindex[lookupkey]
                    if (unit.isfuzzy() and not self.includefuzzy) or len(unit.target) == 0:
                        value = unit.source
                    else:
                        value = unit.target
                    value = php.phpencode(value, self.quotechar)
                    self.inecho = False
                    if isinstance(value, str):
                        value = value.decode('utf8')
                    returnline = "%(key)s%(pre)s%(del)s%(post)s%(quote)s%(value)s%(quote)s%(enddel)s%(comment)s%(eol)s" % {
                                     "key": key,
                                     "pre": prespace, "del": self.equaldel,
                                     "post": postspace,
                                     "quote": self.quotechar, "value": value,
                                     "enddel": self.enddel,
                                     "comment": inlinecomment, "eol": eol,
                                  }
                else:
                    self.inecho = True
                    returnline = line + eol
                # no string termination means carry string on to next line
                endpos = line.rfind("%s%s" % (self.quotechar, self.enddel))
                # if there was no '; or the quote is escaped, we have to
                # continue
                if endpos == -1 or line[endpos-1] == '\\':
                    self.inmultilinemsgid = True
        if isinstance(returnline, unicode):
            returnline = returnline.encode('utf-8')
        return returnline


def convertphp(inputfile, outputfile, templatefile, includefuzzy=False,
               outputthreshold=None):
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("must have template file for php files")
        # convertor = po2php()
    else:
        convertor = rephp(templatefile, inputstore)
    outputphplines = convertor.convertstore(includefuzzy)
    outputfile.writelines(outputphplines)
    return 1


def main(argv=None):
    # handle command line options
    formats = {
            ("po", "php"): ("php", convertphp),
            ("po", "html"): ("html", convertphp),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2prop
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to Java/Mozilla .properties files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/prop2po.html
for examples and usage instructions.
"""

import warnings

from translate.convert import accesskey, convert
from translate.misc import quote
from translate.storage import po, properties


eol = u"\n"


def applytranslation(key, propunit, inunit, mixedkeys):
    """applies the translation for key in the po unit to the prop unit"""
    # this converts the po-style string to a prop-style string
    value = inunit.target
    # handle mixed keys
    for labelsuffix in properties.labelsuffixes:
        if key.endswith(labelsuffix):
            if key in mixedkeys:
                value, akey = accesskey.extract(value)
                break
    else:
        for akeysuffix in properties.accesskeysuffixes:
            if key.endswith(akeysuffix):
                if key in mixedkeys:
                    label, value = accesskey.extract(value)
                    if not value:
                        warnings.warn("Could not find accesskey for %s" % key)
                        # Use the source language accesskey
                        label, value = accesskey.extract(inunit.source)
                    else:
                        original = propunit.source
                        # For the sake of diffs we keep the case of the
                        # accesskey the same if we know the translation didn't
                        # change. Casing matters in XUL.
                        if value == propunit.source and original.lower() == value.lower():
                            if original.isupper():
                                value = value.upper()
                            elif original.islower():
                                value = value.lower()
    return value


class reprop:

    def __init__(self, templatefile, inputstore, personality, encoding=None,
                 remove_untranslated=False):
        self.templatefile = templatefile
        self.inputstore = inputstore
        self.personality = properties.get_dialect(personality)
        self.encoding = encoding
        if self.encoding is None:
            self.encoding = self.personality.default_encoding
        self.remove_untranslated = remove_untranslated
        self.mixer = accesskey.UnitMixer(properties.labelsuffixes,
                                         properties.accesskeysuffixes)

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inmultilinemsgid = False
        self.inecho = False
        self.inputstore.makeindex()
        if self.personality.name == "gaia":
            self._explode_gaia_plurals()
        outputlines = []
        # Readlines doesn't work for UTF-16, we read() and splitlines(keepends) instead
        content = self.templatefile.read().decode(self.encoding)
        for line in content.splitlines(True):
            outputstr = self.convertline(line)
            outputlines.append(outputstr)
        return u"".join(outputlines).encode(self.encoding)


    def _handle_accesskeys(self, inunit, currkey):
        value = inunit.target
        if self.personality.name == "mozilla":
            keys = inunit.getlocations()
            mixedkeys = self.mixer.match_entities(keys)
            for key in keys:
                if key == currkey and key in self.inputstore.locationindex:
                    propunit = self.inputstore.locationindex[key]  # find the prop
                    value = applytranslation(key, propunit, inunit, mixedkeys)
                    break

        return value

    def _explode_gaia_plurals(self):
        """Explode the gaia plurals."""
        from translate.lang import data
        for unit in self.inputstore.units:
            if not unit.hasplural():
                continue
            if unit.isfuzzy() and not self.includefuzzy or not unit.istranslated():
                continue

            names = data.cldr_plural_categories
            location = unit.getlocations()[0]
            for category, text in zip(names, unit.target.strings):
                # TODO: for now we assume all forms are present. We need to
                # fill in the rest after mapping things to the proper CLDR names.
                if category == 'zero':
                    # [zero] cases are translated as separate units
                    continue
                new_unit = self.inputstore.addsourceunit(u"fish")  # not used
                new_location = '%s[%s]' % (location, category)
                new_unit.addlocation(new_location)
                new_unit.target = text
                self.inputstore.locationindex[new_location] = new_unit

            # We don't want the plural marker to be translated:
            del self.inputstore.locationindex[location]

    def convertline(self, line):
        returnline = u""
        # handle multiline msgid if we're in one
        if self.inmultilinemsgid:
            msgid = quote.rstripeol(line).strip()
            # see if there's more
            self.inmultilinemsgid = (msgid[-1:] == '\\')
            # if we're echoing...
            if self.inecho:
                returnline = line
        # otherwise, this could be a comment
        elif line.strip()[:1] == '#':
            returnline = quote.rstripeol(line) + eol
        else:
            line = quote.rstripeol(line)
            delimiter_char, delimiter_pos = self.personality.find_delimiter(line)
            if quote.rstripeol(line)[-1:] == '\\':
                self.inmultilinemsgid = True
            if delimiter_pos == -1:
                key = self.personality.key_strip(line)
                delimiter = " %s " % self.personality.delimiters[0]
            else:
                key = self.personality.key_strip(line[:delimiter_pos])
                # Calculate space around the equal sign
                prespace = line[line.find(' ', len(key)):delimiter_pos]
                postspacestart = len(line[delimiter_pos+1:])
                postspaceend = len(line[delimiter_pos+1:].lstrip())
                postspace = line[delimiter_pos+1:delimiter_pos+(postspacestart-postspaceend)+1]
                delimiter = prespace + delimiter_char + postspace
            if key in self.inputstore.locationindex:
                unit = self.inputstore.locationindex[key]
                if not unit.istranslated() and bool(unit.source) and self.remove_untranslated:
                    returnline = u""
                else:
                    if unit.isfuzzy() and not self.includefuzzy or len(unit.target) == 0:
                        value = unit.source
                    else:
                        value = self._handle_accesskeys(unit, key)
                    self.inecho = False
                    assert isinstance(value, unicode)
                    returnline = "%(key)s%(del)s%(value)s%(term)s%(eol)s" % {
                        "key": "%s%s%s" % (self.personality.key_wrap_char,
                                           key,
                                           self.personality.key_wrap_char),
                        "del": delimiter,
                        "value": "%s%s%s" % (self.personality.value_wrap_char,
                                             self.personality.encode(value),
                                             self.personality.value_wrap_char),
                        "term": self.personality.pair_terminator,
                        "eol": eol,
                    }
            else:
                self.inecho = True
                returnline = line + eol
        assert isinstance(returnline, unicode)
        return returnline


def convertstrings(inputfile, outputfile, templatefile, personality="strings",
                   includefuzzy=False, encoding=None, outputthreshold=None,
                   remove_untranslated=False):
    """.strings specific convertor function"""
    return convertprop(inputfile, outputfile, templatefile,
                       personality="strings", includefuzzy=includefuzzy,
                       encoding=encoding, outputthreshold=outputthreshold,
                       remove_untranslated=remove_untranslated)


def convertmozillaprop(inputfile, outputfile, templatefile,
                       includefuzzy=False, remove_untranslated=False,
                       outputthreshold=None):
    """Mozilla specific convertor function"""
    return convertprop(inputfile, outputfile, templatefile,
                       personality="mozilla", includefuzzy=includefuzzy,
                       remove_untranslated=remove_untranslated,
                       outputthreshold=outputthreshold)


def convertprop(inputfile, outputfile, templatefile, personality="java",
                includefuzzy=False, encoding=None, remove_untranslated=False,
                outputthreshold=None):
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("must have template file for properties files")
        # convertor = po2prop()
    else:
        convertor = reprop(templatefile, inputstore, personality, encoding,
                           remove_untranslated)
    outputprop = convertor.convertstore(includefuzzy)
    outputfile.write(outputprop)
    return 1

formats = {
    ("po", "properties"): ("properties", convertprop),
    ("po", "lang"): ("lang", convertprop),
    ("po", "strings"): ("strings", convertstrings),
}


def main(argv=None):
    # handle command line options
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_option("", "--personality", dest="personality",
            default=properties.default_dialect, type="choice",
            choices=properties.dialects.keys(),
            help="override the input file format: %s (for .properties files, default: %s)" %
                 (", ".join(properties.dialects.iterkeys()),
                  properties.default_dialect),
            metavar="TYPE")
    parser.add_option("", "--encoding", dest="encoding", default=None,
            help="override the encoding set by the personality",
            metavar="ENCODING")
    parser.add_option("", "--removeuntranslated", dest="remove_untranslated",
            default=False, action="store_true",
            help="remove key value from output if it is untranslated")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.passthrough.append("personality")
    parser.passthrough.append("encoding")
    parser.passthrough.append("remove_untranslated")
    parser.run(argv)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2rc
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006,2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files back to Windows Resource (.rc) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/rc2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import po, rc


class rerc:

    def __init__(self, templatefile, charset="utf-8", lang=None, sublang=None):
        self.templatefile = templatefile
        self.templatestore = rc.rcfile(templatefile, encoding=charset)
        self.inputdict = {}
        self.charset = charset
        self.lang = lang
        self.sublang = sublang

    def convertstore(self, inputstore, includefuzzy=False):
        self.makestoredict(inputstore, includefuzzy)
        outputblocks = []
        for block in self.templatestore.blocks:
            outputblocks.append(self.convertblock(block))
        if self.charset == "utf-8":
            outputblocks.insert(0, "#pragma code_page(65001)\n")
            outputblocks.append("#pragma code_page(default)")
        return outputblocks

    def makestoredict(self, store, includefuzzy=False):
        """ make a dictionary of the translations"""
        for unit in store.units:
            if includefuzzy or not unit.isfuzzy():
                for location in unit.getlocations():
                    rcstring = unit.target
                    if len(rcstring.strip()) == 0:
                        rcstring = unit.source
                    self.inputdict[location] = rc.escape_to_rc(rcstring).encode(self.charset)

    def convertblock(self, block):
        newblock = block
        if isinstance(newblock, unicode):
            newblock = newblock.encode('utf-8')
        if newblock.startswith("LANGUAGE"):
            return "LANGUAGE %s, %s" % (self.lang, self.sublang)
        for unit in self.templatestore.units:
            location = unit.getlocations()[0]
            if location in self.inputdict:
                if self.inputdict[location] != unit.match.groupdict()['value']:
                    newmatch = unit.match.group().replace(unit.match.groupdict()['value'],
                                                          self.inputdict[location])
                    newblock = newblock.replace(unit.match.group(), newmatch)
        if isinstance(newblock, unicode):
            newblock = newblock.encode(self.charset)
        return newblock


def convertrc(inputfile, outputfile, templatefile, includefuzzy=False,
              charset=None, lang=None, sublang=None, outputthreshold=None):
    inputstore = po.pofile(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if not lang:
        raise ValueError("must specify a target language")
    if templatefile is None:
        raise ValueError("must have template file for rc files")
        # convertor = po2rc()
    else:
        convertor = rerc(templatefile, charset, lang, sublang)
    outputrclines = convertor.convertstore(inputstore, includefuzzy)
    outputfile.writelines(outputrclines)
    return 1


def main(argv=None):
    # handle command line options
    formats = {("po", "rc"): ("rc", convertrc)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    defaultcharset = "utf-8"
    parser.add_option("", "--charset", dest="charset", default=defaultcharset,
        help="charset to use to decode the RC files (default: %s)" % defaultcharset,
        metavar="CHARSET")
    parser.add_option("-l", "--lang", dest="lang", default=None,
        help="LANG entry", metavar="LANG")
    defaultsublang = "SUBLANG_DEFAULT"
    parser.add_option("", "--sublang", dest="sublang", default=defaultsublang,
        help="SUBLANG entry (default: %s)" % defaultsublang, metavar="SUBLANG")
    parser.passthrough.append("charset")
    parser.passthrough.append("lang")
    parser.passthrough.append("sublang")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2sub
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to subtitle files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/sub2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import factory


class resub:

    def __init__(self, templatefile, inputstore):
        from translate.storage import subtitles
        self.templatefile = templatefile
        self.templatestore = subtitles.SubtitleFile(templatefile)
        self.inputstore = inputstore

    def convertstore(self, includefuzzy=False):
        self.includefuzzy = includefuzzy
        self.inputstore.makeindex()
        for unit in self.templatestore.units:
            for location in unit.getlocations():
                if location in self.inputstore.locationindex:
                    inputunit = self.inputstore.locationindex[location]
                    if inputunit.isfuzzy() and not self.includefuzzy:
                        unit.target = unit.source
                    else:
                        unit.target = inputunit.target
                else:
                    unit.target = unit.source
        return str(self.templatestore)


def convertsub(inputfile, outputfile, templatefile, includefuzzy=False,
               outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    if templatefile is None:
        raise ValueError("must have template file for subtitle files")
    else:
        convertor = resub(templatefile, inputstore)
    outputstring = convertor.convertstore(includefuzzy)
    outputfile.write(outputstring)
    return 1


def main(argv=None):
    # handle command line options
    formats = {
         ("po", "srt"): ("srt", convertsub),
         ("po", "sub"): ("sub", convertsub),
         ("po", "ssa"): ("ssa", convertsub),
         ("po", "ass"): ("ass", convertsub),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2symb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to Symbian translation files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/symb2po.html
for examples and usage instructions.
"""

from translate.storage import factory
from translate.storage.pypo import po_escape_map
from translate.storage.symbian import *


def escape(text):
    for key, val in po_escape_map.iteritems():
        text = text.replace(key, val)
    return '"%s"' % text


def replace_header_items(ps, replacments):
    match = read_while(ps, header_item_or_end_re.match, lambda match: match is None)
    while not ps.current_line.startswith('*/'):
        match = header_item_re.match(ps.current_line)
        if match is not None:
            key = match.groupdict()['key']
            if key in replacments:
                ps.current_line = match.expand('\g<key>\g<space>%s\n' % replacments[key])
        ps.read_line()


def parse(ps, header_replacements, body_replacements):
    replace_header_items(ps, header_replacements)
    try:
        while True:
            eat_whitespace(ps)
            skip_no_translate(ps)
            match = string_entry_re.match(ps.current_line)
            if match is not None:
                key = match.groupdict()['id']
                if key in body_replacements:
                    value = body_replacements[key].target or body_replacements[key].source
                    ps.current_line = match.expand(u'\g<start>\g<id>\g<space>%s\n' % escape(value))
            ps.read_line()
    except StopIteration:
        pass


def line_saver(charset):
    result = []

    def save_line(line):
        result.append(line.encode(charset))
    return result, save_line


def write_symbian(f, header_replacements, body_replacements):
    lines = list(f)
    charset = read_charset(lines)
    result, save_line = line_saver(charset)
    parse(ParseState(iter(lines), charset, save_line), header_replacements, body_replacements)
    return result


def build_location_index(store):
    po_header = store.parseheader()
    index = {}
    for unit in store.units:
        for location in unit.getlocations():
            index[location] = unit
    index['r_string_languagegroup_name'] = store.UnitClass(po_header['Language-Team'])
    return index


def build_header_index(store):
    po_header = store.parseheader()
    return {'Author': po_header['Last-Translator']}


def convert_symbian(input_file, output_file, template_file, pot=False, duplicatestyle="msgctxt"):
    store = factory.getobject(input_file)
    location_index = build_location_index(store)
    header_index = build_header_index(store)
    output = write_symbian(template_file, header_index, location_index)
    for line in output:
        output_file.write(line)
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"po": ("r0", convert_symbian)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2tiki
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Mozilla Corporation, Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO files to TikiWiki's language.php files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/tiki2po.html
for examples and usage instructions.
"""

import sys

from translate.storage import po, tiki


class po2tiki:

    def convertstore(self, thepofile):
        """Converts a given (parsed) po file to a tiki file.

        :param thepofile: a pofile pre-loaded with input data
        """
        thetargetfile = tiki.TikiStore()
        for unit in thepofile.units:
            if not (unit.isblank() or unit.isheader()):
                newunit = tiki.TikiUnit(unit.source)
                newunit.settarget(unit.target)
                locations = unit.getlocations()
                if locations:
                    newunit.addlocations(locations)
                # If a word is "untranslated" but the target isn't empty and isn't the same as the source
                # it's been translated and we switch it. This is an assumption but should remain true as long
                # as these scripts are used.
                if newunit.getlocations() == ["untranslated"] and unit.source != unit.target and unit.target != "":
                    newunit.location = []
                    newunit.addlocation("translated")

                thetargetfile.addunit(newunit)
        return thetargetfile


def convertpo(inputfile, outputfile, template=None):
    """Converts from po file format to tiki.

    :param inputfile: file handle of the source
    :param outputfile: file handle to write to
    :param template: unused
    """
    inputstore = po.pofile(inputfile)
    if inputstore.isempty():
        return False
    convertor = po2tiki()
    outputstore = convertor.convertstore(inputstore)
    outputfile.write(str(outputstore))
    return True


def main(argv=None):
    """Will convert from .po to tiki style .php"""
    from translate.convert import convert
    from translate.misc import stdiotell
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)

    formats = {"po": ("tiki", convertpo)}

    parser = convert.ConvertOptionParser(formats, description=__doc__)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2tmx
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to a TMX (Translation Memory eXchange) file.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/po2tmx.html
for examples and usage instructions.
"""

import os

from translate.convert import convert
from translate.misc import wStringIO
from translate.storage import po, tmx


class po2tmx:

    def cleancomments(self, comments, comment_type=None):
        """Removes the comment marks from the PO strings."""
        # FIXME this is a bit hacky, needs some fixes in the PO classes
        for index, comment in enumerate(comments):
            if comment.startswith("#"):
                if comment_type is None:
                    comments[index] = comment[1:].rstrip()
                else:
                    comments[index] = comment[2:].strip()

        return ''.join(comments)

    def convertfiles(self, inputfile, tmxfile, sourcelanguage='en',
                     targetlanguage=None, comment=None):
        """converts a .po file (possibly many) to TMX file"""
        inputstore = po.pofile(inputfile)
        for inunit in inputstore.units:
            if inunit.isheader() or inunit.isblank() or not inunit.istranslated() or inunit.isfuzzy():
                continue
            source = inunit.source
            translation = inunit.target

            commenttext = {
                'source': self.cleancomments(inunit.sourcecomments, "source"),
                'type': self.cleancomments(inunit.typecomments, "type"),
                'others': self.cleancomments(inunit.othercomments),
            }.get(comment, None)

            tmxfile.addtranslation(source, sourcelanguage, translation,
                                   targetlanguage, commenttext)


def convertpo(inputfile, outputfile, templatefile, sourcelanguage='en',
              targetlanguage=None, comment=None):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    convertor = po2tmx()
    convertor.convertfiles(inputfile, outputfile.tmxfile, sourcelanguage,
                           targetlanguage, comment)
    return 1


class tmxmultifile:

    def __init__(self, filename, mode=None):
        """initialises tmxmultifile from a seekable inputfile or writable outputfile"""
        self.filename = filename
        if mode is None:
            if os.path.exists(filename):
                mode = 'r'
            else:
                mode = 'w'
        self.mode = mode
#        self.multifilestyle = multifilestyle
        self.multifilename = os.path.splitext(filename)[0]
#        self.multifile = open(filename, mode)
        self.tmxfile = tmx.tmxfile()

    def openoutputfile(self, subfile):
        """returns a pseudo-file object for the given subfile"""

        def onclose(contents):
            pass
        outputfile = wStringIO.CatchStringOutput(onclose)
        outputfile.filename = subfile
        outputfile.tmxfile = self.tmxfile
        return outputfile


class TmxOptionParser(convert.ArchiveConvertOptionParser):

    def recursiveprocess(self, options):
        if not options.targetlanguage:
            raise ValueError("You must specify the target language")
        super(TmxOptionParser, self).recursiveprocess(options)
        self.output = open(options.output, 'w')
        options.outputarchive.tmxfile.setsourcelanguage(options.sourcelanguage)
        self.output.write(str(options.outputarchive.tmxfile))
        self.output.close()


def main(argv=None):
    formats = {"po": ("tmx", convertpo), ("po", "tmx"): ("tmx", convertpo)}
    archiveformats = {(None, "output"): tmxmultifile, (None, "template"): tmxmultifile}
    parser = TmxOptionParser(formats, usepots=False, usetemplates=False, description=__doc__, archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
            help="set target language code (e.g. af-ZA) [required]", metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage", default='en',
            help="set source language code (default: en)", metavar="LANG")
    comments = ['source', 'type', 'others', 'none']
    comments_help = ("set default comment import: none, source, type or "
                     "others (default: none)")
    parser.add_option("", "--comments", dest="comment", default="none",
                      type="choice", choices=comments, help=comments_help)
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.passthrough.append("comment")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2ts
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to Qt Linguist (.ts) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ts2po.html
for examples and usage instructions.
"""

from translate.storage import po, ts


class po2ts:

    def convertstore(self, inputstore, templatefile=None, context=None):
        """converts a .po file to .ts format (using a template .ts file if given)"""
        if templatefile is None:
            tsfile = ts.QtTsParser()
        else:
            tsfile = ts.QtTsParser(templatefile)
        for inputunit in inputstore.units:
            if inputunit.isheader() or inputunit.isblank():
                continue
            source = inputunit.source
            translation = inputunit.target
            comment = inputunit.getnotes("translator")
            transtype = None
            if not inputunit.istranslated():
                transtype = "unfinished"
            elif inputunit.getnotes("developer") == "(obsolete)":
                transtype = "obsolete"
            if isinstance(source, str):
                source = source.decode("utf-8")
            if isinstance(translation, str):
                translation = translation.decode("utf-8")
            for sourcelocation in inputunit.getlocations():
                if context is None:
                    if "#" in sourcelocation:
                        contextname = sourcelocation[:sourcelocation.find("#")]
                    else:
                        contextname = sourcelocation
                else:
                    contextname = context
                tsfile.addtranslation(contextname, source, translation, comment, transtype, createifmissing=True)
        return tsfile.getxml()


def convertpo(inputfile, outputfile, templatefile, context):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    inputstore = po.pofile(inputfile)
    if inputstore.isempty():
        return 0
    convertor = po2ts()
    outputstring = convertor.convertstore(inputstore, templatefile, context)
    outputfile.write(outputstring)
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"po": ("ts", convertpo), ("po", "ts"): ("ts", convertpo)}
    parser = convert.ConvertOptionParser(formats, usepots=False, usetemplates=True, description=__doc__)
    parser.add_option("-c", "--context", dest="context", default=None,
                      help="use supplied context instead of the one in the .po file comment")
    parser.passthrough.append("context")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2txt
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to plain text (.txt) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/txt2po.html
for examples and usage instructions.
"""

import textwrap

from translate.convert import convert
from translate.storage import factory


class po2txt:
    """po2txt can take a po file and generate txt. best to give it a template file otherwise will just concat msgstrs"""

    def __init__(self, wrap=None):
        self.wrap = wrap

    def wrapmessage(self, message):
        """rewraps text as required"""
        if self.wrap is None:
            return message
        return "\n".join([textwrap.fill(line, self.wrap, replace_whitespace=False) for line in message.split("\n")])

    def convertstore(self, inputstore, includefuzzy):
        """converts a file to txt format"""
        txtresult = ""
        for unit in inputstore.units:
            if unit.isheader():
                continue
            if unit.istranslated() or (includefuzzy and unit.isfuzzy()):
                txtresult += self.wrapmessage(unit.target) + "\n" + "\n"
            else:
                txtresult += self.wrapmessage(unit.source) + "\n" + "\n"
        return txtresult.rstrip()

    def mergestore(self, inputstore, templatetext, includefuzzy):
        """converts a file to txt format"""
        txtresult = templatetext
        # TODO: make a list of blocks of text and translate them individually
        # rather than using replace
        for unit in inputstore.units:
            if unit.isheader():
                continue
            if not unit.isfuzzy() or includefuzzy:
                txtsource = unit.source
                txttarget = self.wrapmessage(unit.target)
                if unit.istranslated():
                    txtresult = txtresult.replace(txtsource, txttarget)
        return txtresult


def converttxt(inputfile, outputfile, templatefile, wrap=None, includefuzzy=False, encoding='utf-8',
               outputthreshold=None):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    convertor = po2txt(wrap=wrap)
    if templatefile is None:
        outputstring = convertor.convertstore(inputstore, includefuzzy)
    else:
        templatestring = templatefile.read().decode(encoding)
        outputstring = convertor.mergestore(inputstore, templatestring, includefuzzy)
    outputfile.write(outputstring.encode('utf-8'))
    return 1


def main(argv=None):
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    formats = {("po", "txt"): ("txt", converttxt), ("po"): ("txt", converttxt), ("xlf", "txt"): ("txt", converttxt), ("xlf"): ("txt", converttxt)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_option("", "--encoding", dest="encoding", default='utf-8', type="string",
            help="The encoding of the template file (default: UTF-8)")
    parser.passthrough.append("encoding")
    parser.add_option("-w", "--wrap", dest="wrap", default=None, type="int",
            help="set number of columns to wrap text at", metavar="WRAP")
    parser.passthrough.append("wrap")
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2web2py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert GNU/gettext PO files to web2py translation dictionaries (.py).

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/web2py2po.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import factory


class po2pydict:

    def __init__(self):
        return

    def convertstore(self, inputstore, includefuzzy):
        from cStringIO import StringIO
        str_obj = StringIO()

        mydict = dict()
        for unit in inputstore.units:
            if unit.isheader():
                continue
            if unit.istranslated() or (includefuzzy and unit.isfuzzy()):
                mydict[unit.source] = unit.target
            else:
                mydict[unit.source] = unit.source
                # The older convention is to prefix with "*** ":
                #mydict[unit.source] = '*** ' + unit.source

        str_obj.write('{\n')
        for source_str in mydict:
            str_obj.write("%s:%s,\n" % (repr(str(source_str)), repr(str(mydict[source_str]))))
        str_obj.write('}\n')
        str_obj.seek(0)
        return str_obj


def convertpy(inputfile, outputfile, templatefile=None, includefuzzy=False,
              outputthreshold=None):
    inputstore = factory.getobject(inputfile)

    if not convert.should_output_store(inputstore, outputthreshold):
        return False

    convertor = po2pydict()
    outputstring = convertor.convertstore(inputstore, includefuzzy)
    outputfile.write(outputstring.read())
    return 1


def main(argv=None):
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    formats = {("po", "py"): ("py", convertpy), ("po"): ("py", convertpy)}
    parser = convert.ConvertOptionParser(formats, usetemplates=False, description=__doc__)
    parser.add_threshold_option()
    parser.add_fuzzy_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2wordfast
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to a Wordfast translation memory file.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/po2wordfast.html
for examples and usage instructions.
"""

import os

from translate.convert import convert
from translate.misc import wStringIO
from translate.storage import po, wordfast


class po2wordfast:

    def convertfiles(self, inputfile, wffile, sourcelanguage='en', targetlanguage=None):
        """converts a .po file (possibly many) to a Wordfast TM file"""
        inputstore = po.pofile(inputfile)
        for inunit in inputstore.units:
            if inunit.isheader() or inunit.isblank() or not inunit.istranslated():
                continue
            source = inunit.source
            target = inunit.target
            newunit = wffile.addsourceunit(source)
            newunit.target = target
            newunit.targetlang = targetlanguage


def convertpo(inputfile, outputfile, templatefile, sourcelanguage='en', targetlanguage=None):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    convertor = po2wordfast()
    outputfile.wffile.header.targetlang = targetlanguage
    convertor.convertfiles(inputfile, outputfile.wffile, sourcelanguage, targetlanguage)
    return 1


class wfmultifile:

    def __init__(self, filename, mode=None):
        """initialises wfmultifile from a seekable inputfile or writable outputfile"""
        self.filename = filename
        if mode is None:
            if os.path.exists(filename):
                mode = 'r'
            else:
                mode = 'w'
        self.mode = mode
        self.multifilename = os.path.splitext(filename)[0]
        self.wffile = wordfast.WordfastTMFile()

    def openoutputfile(self, subfile):
        """returns a pseudo-file object for the given subfile"""

        def onclose(contents):
            pass
        outputfile = wStringIO.CatchStringOutput(onclose)
        outputfile.filename = subfile
        outputfile.wffile = self.wffile
        return outputfile


class WfOptionParser(convert.ArchiveConvertOptionParser):

    def recursiveprocess(self, options):
        if not options.targetlanguage:
            raise ValueError("You must specify the target language")
        super(WfOptionParser, self).recursiveprocess(options)
        self.output = open(options.output, 'w')
        #options.outputarchive.wffile.setsourcelanguage(options.sourcelanguage)
        self.output.write(str(options.outputarchive.wffile))


def main(argv=None):
    formats = {"po": ("txt", convertpo), ("po", "txt"): ("txt", convertpo)}
    archiveformats = {(None, "output"): wfmultifile, (None, "template"): wfmultifile}
    parser = WfOptionParser(formats, usepots=False, usetemplates=False, description=__doc__, archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
                      help="set target language code (e.g. af-ZA) [required]", metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage", default='en',
                      help="set source language code (default: en)", metavar="LANG")
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = po2xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Gettext PO localization files to XLIFF localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/xliff2po.html
for examples and usage instructions.
"""

from translate.storage import po, poxliff


class po2xliff:

    def convertunit(self, outputstore, inputunit, filename):
        """creates a transunit node"""
        source = inputunit.source
        target = inputunit.target
        if inputunit.isheader():
            unit = outputstore.addheaderunit(target, filename)
        else:
            unit = outputstore.addsourceunit(source, filename, True)
            unit.target = target
            #Explicitly marking the fuzzy state will ensure that normal (translated)
            #units in the PO file end up as approved in the XLIFF file.
            if target:
                unit.markfuzzy(inputunit.isfuzzy())
            else:
                unit.markapproved(False)

            #Handle #: location comments
            for location in inputunit.getlocations():
                unit.createcontextgroup("po-reference", self.contextlist(location), purpose="location")

            #Handle #. automatic comments
            comment = inputunit.getnotes("developer")
            if comment:
                unit.createcontextgroup("po-entry", [("x-po-autocomment", comment)], purpose="information")
                unit.addnote(comment, origin="developer")

            #TODO: x-format, etc.

        #Handle # other comments
        comment = inputunit.getnotes("translator")
        if comment:
            unit.createcontextgroup("po-entry", [("x-po-trancomment", comment)], purpose="information")
            unit.addnote(comment, origin="po-translator")

        return unit

    def contextlist(self, location):
        contexts = []
        if ":" in location:
            sourcefile, linenumber = location.split(":", 1)
        else:
            sourcefile, linenumber = location, None
        contexts.append(("sourcefile", sourcefile))
        if linenumber:
            contexts.append(("linenumber", linenumber))
        return contexts

    def convertstore(self, inputstore, templatefile=None, **kwargs):
        """converts a .po file to .xlf format"""
        if templatefile is None:
            outputstore = poxliff.PoXliffFile(**kwargs)
        else:
            outputstore = poxliff.PoXliffFile(templatefile, **kwargs)
        filename = inputstore.filename
        for inputunit in inputstore.units:
            if inputunit.isblank():
                continue
            transunitnode = self.convertunit(outputstore, inputunit, filename)
        return str(outputstore)


def convertpo(inputfile, outputfile, templatefile):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    inputstore = po.pofile(inputfile)
    if inputstore.isempty():
        return 0
    convertor = po2xliff()
    outputstring = convertor.convertstore(inputstore, templatefile)
    outputfile.write(outputstring)
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
        "po": ("xlf", convertpo),
        ("po", "xlf"): ("xlf", convertpo),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = poreplace
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Simple script to do replacements on translated strings inside po files.
"""

# this is used as the basis for other scripts, it currently replaces nothing

from translate.storage import po


class poreplace:

    def convertstring(self, postr):
        """does the conversion required on the given string (nothing in this case)"""
        return postr

    def convertfile(self, thepofile):
        """goes through a po file and converts each element"""
        for thepo in thepofile.units:
            thepo.msgstr = [self.convertstring(postr) for postr in thepo.msgstr]
        return thepofile

    def convertpo(self, inputfile, outputfile, templatefile):
        """reads in inputfile using po, converts using poreplace, writes to outputfile"""
        # note that templatefile is not used, but it is required by the converter...
        inputstore = po.pofile(inputfile)
        if inputstore.isempty():
            return 0
        outputstore = self.convertfile(inputstore)
        if outputstore.isempty():
            return 0
        outputfile.write(str(outputstore))
        return 1


def main(converterclass, argv=None):
    # handle command line options
    from translate.convert import convert
    replacer = converterclass()
    formats = {"po": ("po", replacer.convertpo), "pot": ("pot", replacer.convertpo)}
    parser = convert.ConvertOptionParser(formats, usepots=True)
    parser.run(argv)


if __name__ == '__main__':
    main(poreplace)

########NEW FILE########
__FILENAME__ = pot2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert template files (like .pot or template .xlf files) to translation
files, preserving existing translations.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pot2po.html
for examples and usage instructions.
"""

from translate.misc.multistring import multistring
from translate.search import match
from translate.storage import catkeys, factory, poheader
from translate.tools import pretranslate


def convertpot(input_file, output_file, template_file, tm=None,
        min_similarity=75, fuzzymatching=True, classes=None,
        classes_str=factory.classes_str, **kwargs):
    """Main conversion function."""
    input_store = factory.getobject(input_file, classes=classes,
                                    classes_str=classes_str)
    try:
        temp_store = factory.getobject(input_file, classes_str=classes_str)
    except:
        # StringIO and other file like objects will be closed after parsing
        temp_store = None

    template_store = None
    if template_file is not None:
        template_store = factory.getobject(template_file,
                                           classes_str=classes_str)

    output_store = convert_stores(input_store, template_store, temp_store, tm,
                                  min_similarity, fuzzymatching, **kwargs)
    output_file.write(str(output_store))

    return 1


def convert_stores(input_store, template_store, temp_store=None, tm=None,
        min_similarity=75, fuzzymatching=True, **kwargs):
    """Actual conversion function, works on stores not files, returns
    a properly initialized pretranslated output store, with structure
    based on input_store, metadata based on template_store, migrates
    old translations from template_store and pretranslating from TM.
    """
    if temp_store is None:
        temp_store = input_store

    # Create fuzzy matchers to be used by pretranslate.pretranslate_unit
    matchers = []

    _prepare_merge(input_store, temp_store, template_store)
    if fuzzymatching:
        if template_store:
            matcher = match.matcher(template_store, max_candidates=1,
                                    min_similarity=min_similarity,
                                    max_length=3000, usefuzzy=True)
            matcher.addpercentage = False
            matchers.append(matcher)
        if tm:
            matcher = pretranslate.memory(tm, max_candidates=1,
                                          min_similarity=min_similarity,
                                          max_length=1000)
            matcher.addpercentage = False
            matchers.append(matcher)

    #initialize store
    _store_pre_merge(input_store, temp_store, template_store)

    # Do matching
    for input_unit in temp_store.units:
        if input_unit.istranslatable():
            input_unit = pretranslate \
                    .pretranslate_unit(input_unit, template_store, matchers,
                                       mark_reused=True,
                                       merge_on=input_store.merge_on)
            _unit_post_merge(input_unit, input_store, temp_store, template_store)

    #finalize store
    _store_post_merge(input_store, temp_store, template_store)

    return temp_store


##dispatchers
def _prepare_merge(input_store, output_store, template_store, **kwargs):
    """Prepare stores & TM matchers before merging."""
    # Dispatch to format specific functions
    prepare_merge_hook = "_prepare_merge_%s" % input_store.__class__.__name__
    if prepare_merge_hook in globals():
        globals()[prepare_merge_hook](input_store, output_store,
                                      template_store, **kwargs)

    # Generate an index so we can search by source string and location later on
    input_store.makeindex()
    if template_store:
        template_store.makeindex()


def _store_pre_merge(input_store, output_store, template_store, **kwargs):
    """Initialize the new file with things like headers and metadata."""
    #formats that implement poheader interface are a special case
    if isinstance(input_store, poheader.poheader):
        _do_poheaders(input_store, output_store, template_store)
    elif isinstance(input_store, catkeys.CatkeysFile):
        #FIXME: shouldn't we be merging template_store.header instead?
        #FIXME: also this should be a format specific hook
        output_store.header = input_store.header

    # Dispatch to format specific functions
    store_pre_merge_hook = "_store_pre_merge_%s" % input_store.__class__.__name__
    if store_pre_merge_hook in globals():
        globals()[store_pre_merge_hook](input_store, output_store,
                                        template_store, **kwargs)


def _store_post_merge(input_store, output_store, template_store, **kwargs):
    """Close file after merging all translations, used for adding
    statistics, obsolete messages and similar wrapup tasks."""
    # Dispatch to format specific functions
    store_post_merge_hook = "_store_post_merge_%s" % input_store.__class__.__name__
    if store_post_merge_hook in globals():
        globals()[store_post_merge_hook](input_store, output_store,
                                         template_store, **kwargs)


def _unit_post_merge(input_unit, input_store, output_store, template_store,
        **kwargs):
    """Handle any unit level cleanup and situations not handled by the merge()
    function.
    """
    #dispatch to format specific functions
    unit_post_merge_hook = "_unit_post_merge_%s" % input_unit.__class__.__name__
    if unit_post_merge_hook in globals():
        globals()[unit_post_merge_hook](input_unit, input_store, output_store,
                                        template_store, **kwargs)


## Format specific functions
def _unit_post_merge_pounit(input_unit, input_store, output_store,
        template_store):
    """PO format specific plural string initializtion logic."""
    #FIXME: do we want to do that for poxliff also?
    if input_unit.hasplural() and len(input_unit.target) == 0:
        # untranslated plural unit; Let's ensure that we have the correct
        # number of plural forms:
        nplurals, plural = output_store.getheaderplural()
        if nplurals and nplurals.isdigit() and nplurals != '2':
            input_unit.target = multistring([""] * int(nplurals))


def _store_post_merge_pofile(input_store, output_store, template_store):
    """PO format specific: adds newly obsoleted messages to end of store."""
    # Let's take care of obsoleted messages
    if template_store:
        newlyobsoleted = []
        for unit in template_store.units:
            if unit.isheader() or unit.isblank():
                continue
            if (unit.target and not (input_store.findid(unit.getid()) or
                hasattr(unit, "reused"))):
                # Not in .pot, make it obsolete
                unit.makeobsolete()
                newlyobsoleted.append(unit)
        for unit in newlyobsoleted:
            output_store.addunit(unit)


def _do_poheaders(input_store, output_store, template_store):
    """Adds initialized PO headers to output store."""
    # header values
    charset = "UTF-8"
    encoding = "8bit"
    project_id_version = None
    pot_creation_date = None
    po_revision_date = None
    last_translator = None
    language_team = None
    mime_version = None
    plural_forms = None
    kwargs = {}

    if template_store is not None and isinstance(template_store, poheader.poheader):
        templateheadervalues = template_store.parseheader()
        for key, value in templateheadervalues.iteritems():
            if key == "Project-Id-Version":
                project_id_version = value
            elif key == "Last-Translator":
                last_translator = value
            elif key == "Language-Team":
                language_team = value
            elif key == "PO-Revision-Date":
                po_revision_date = value
            elif key in ("POT-Creation-Date", "MIME-Version"):
                # don't know how to handle these keys, or ignoring them
                pass
            elif key == "Content-Type":
                kwargs[key] = value
            elif key == "Content-Transfer-Encoding":
                encoding = value
            elif key == "Plural-Forms":
                plural_forms = value
            else:
                kwargs[key] = value

    inputheadervalues = input_store.parseheader()
    for key, value in inputheadervalues.iteritems():
        if key in ("Project-Id-Version", "Last-Translator", "Language-Team",
                   "PO-Revision-Date", "Content-Type",
                   "Content-Transfer-Encoding", "Plural-Forms"):
            # want to carry these from the template so we ignore them
            pass
        elif key == "POT-Creation-Date":
            pot_creation_date = value
        elif key == "MIME-Version":
            mime_version = value
        else:
            kwargs[key] = value

    output_header = output_store \
            .init_headers(charset=charset, encoding=encoding,
                          project_id_version=project_id_version,
                          pot_creation_date=pot_creation_date,
                          po_revision_date=po_revision_date,
                          last_translator=last_translator,
                          language_team=language_team,
                          mime_version=mime_version,
                          plural_forms=plural_forms, **kwargs)

    # Get the header comments and fuzziness state
    # override some values from input file
    if template_store is not None:
        template_header = template_store.header()
        if template_header is not None:
            if template_header.getnotes("translator"):
                output_header.addnote(template_header.getnotes("translator"),
                                      "translator", position="replace")
            output_header.markfuzzy(template_header.isfuzzy())


def main(argv=None):
    from translate.convert import convert
    formats = {
        "pot": ("po", convertpot),
        ("pot", "po"): ("po", convertpot),
        "xlf": ("xlf", convertpot),
        ("xlf", "xlf"): ("xlf", convertpot),
        "ts": ("ts", convertpot),
        "lang": ("lang", convertpot),
        ("lang", "lang"): ("lang", convertpot),
        ("ts", "ts"): ("ts", convertpot),
        "catkeys": ("catkeys", convertpot),
        ("catkeys", "catkeys"): ("catkeys", convertpot),
    }
    parser = convert.ConvertOptionParser(formats, usepots=True,
            usetemplates=True, allowmissingtemplate=True, description=__doc__)

    parser.add_option("", "--tm", dest="tm", default=None,
            help="The file to use as translation memory when fuzzy matching")
    parser.passthrough.append("tm")

    defaultsimilarity = 75
    parser.add_option("-s", "--similarity", dest="min_similarity",
            default=defaultsimilarity, type="float",
            help="The minimum similarity for inclusion (default: %d%%)" %
                    defaultsimilarity)
    parser.passthrough.append("min_similarity")

    parser.add_option("--nofuzzymatching", dest="fuzzymatching",
            action="store_false", default=True, help="Disable fuzzy matching")
    parser.passthrough.append("fuzzymatching")

    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = prop2mozfunny
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Converts properties files to additional Mozilla format files.
"""

from translate.convert import mozfunny2prop, po2prop
from translate.misc.wStringIO import StringIO
from translate.storage import properties


def prop2inc(pf):
    """convert a properties file back to a .inc file with #defines in it"""
    # any leftover blanks will not be included at the end
    pendingblanks = []
    for unit in pf.units:
        for comment in unit.comments:
            if comment.startswith("# converted from") and "#defines" in comment:
                pass
            else:
                for blank in pendingblanks:
                    yield blank
                # TODO: could convert commented # x=y back to # #define x y
                yield comment + "\n"
        if unit.isblank():
            pendingblanks.append("\n")
        else:
            definition = "#define %s %s\n" % (unit.name, unit.value.replace("\n", "\\n"))
            if isinstance(definition, unicode):
                definition = definition.encode("UTF-8")
            for blank in pendingblanks:
                yield blank
            yield definition


def prop2it(pf):
    """convert a properties file back to a pseudo-properties .it file"""
    for unit in pf.units:
        for comment in unit.comments:
            if comment.startswith("# converted from") and "pseudo-properties" in comment:
                pass
            elif comment.startswith("# section: "):
                yield comment.replace("# section: ", "", 1) + "\n"
            else:
                yield comment.replace("#", ";", 1) + "\n"
        if unit.isblank():
            yield ""
        else:
            definition = "%s=%s\n" % (unit.name, unit.value)
            if isinstance(definition, unicode):
                definition = definition.encode("UTF-8")
            yield definition


def prop2funny(src, itencoding="cp1252"):
    lines = src.split("\n")
    header = lines[0]
    if not header.startswith("# converted from "):
        waspseudoprops = len([line for line in lines if line.startswith("# section:")])
        wasdefines = len([line for line in lines if line.startswith("#filter") or line.startswith("#unfilter")])
    else:
        waspseudoprops = "pseudo-properties" in header
        wasdefines = "#defines" in header
        lines = lines[1:]
    if not (waspseudoprops ^ wasdefines):
        raise ValueError("could not determine file type as pseudo-properties or defines file")
    pf = properties.propfile(personality="mozilla")
    pf.parse("\n".join(lines))
    if wasdefines:
        for line in prop2inc(pf):
            yield line + "\n"
    elif waspseudoprops:
        for line in prop2it(pf):
            yield line.decode("utf-8").encode(itencoding) + "\n"


def po2inc(inputfile, outputfile, templatefile, encoding=None, includefuzzy=False,
           remove_untranslated=False, outputthreshold=None):
    """wraps po2prop but converts outputfile to properties first"""
    outputpropfile = StringIO()
    if templatefile is not None:
        templatelines = templatefile.readlines()
        templateproplines = [line for line in mozfunny2prop.inc2prop(templatelines)]
        templatepropfile = StringIO("".join(templateproplines))
    else:
        templatepropfile = None
    result = po2prop.convertmozillaprop(inputfile, outputpropfile,
                                        templatepropfile,
                                        includefuzzy=includefuzzy,
                                        remove_untranslated=remove_untranslated,
                                        outputthreshold=outputthreshold)
    if result:
        outputpropfile.seek(0)
        pf = properties.propfile(outputpropfile, personality="mozilla")
        outputlines = prop2inc(pf)
        outputfile.writelines(outputlines)
    return result


def po2it(inputfile, outputfile, templatefile, encoding="cp1252", includefuzzy=False,
          remove_untranslated=False, outputthreshold=None):
    """wraps po2prop but converts outputfile to properties first"""
    outputpropfile = StringIO()
    if templatefile is not None:
        templatelines = templatefile.readlines()
        templateproplines = [line for line in mozfunny2prop.it2prop(templatelines, encoding=encoding)]
        templatepropfile = StringIO("".join(templateproplines))
    else:
        templatepropfile = None
    result = po2prop.convertmozillaprop(inputfile, outputpropfile,
                                        templatepropfile,
                                        includefuzzy=includefuzzy,
                                        remove_untranslated=remove_untranslated,
                                        outputthreshold=outputthreshold)
    if result:
        outputpropfile.seek(0)
        pf = properties.propfile(outputpropfile, personality="mozilla")
        outputlines = prop2it(pf)
        for line in outputlines:
            line = line.decode("utf-8").encode(encoding)
            outputfile.write(line)
    return result


def po2ini(inputfile, outputfile, templatefile, encoding="UTF-8", includefuzzy=False,
           remove_untranslated=False, outputthreshold=None):
    """wraps po2prop but converts outputfile to properties first using UTF-8 encoding"""
    return po2it(inputfile=inputfile, outputfile=outputfile,
                 templatefile=templatefile, encoding=encoding,
                 includefuzzy=includefuzzy,
                 remove_untranslated=remove_untranslated,
                 outputthreshold=outputthreshold)


def main(argv=None):
    import sys
    # TODO: get encoding from charset.mk, using parameter
    src = sys.stdin.read()
    for line in prop2funny(src):
        sys.stdout.write(line)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = prop2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2014 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Java/Mozilla .properties files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/prop2po.html
for examples and usage instructions.
"""

import logging

from translate.convert.accesskey import UnitMixer
from translate.storage import po, properties


logger = logging.getLogger(__name__)


class prop2po:
    """convert a .properties file to a .po file for handling the
    translation."""

    def __init__(self, personality="java", blankmsgstr=False,
                 duplicatestyle="msgctxt"):
        self.personality = personality
        self.blankmsgstr = blankmsgstr
        self.duplicatestyle = duplicatestyle
        self.mixedkeys = {}
        self.mixer = UnitMixer(properties.labelsuffixes,
                               properties.accesskeysuffixes)

    def convertstore(self, thepropfile):
        """converts a .properties file to a .po file..."""
        thetargetfile = po.pofile()
        if self.personality in ("mozilla", "skype"):
            targetheader = thetargetfile.init_headers(
                    x_accelerator_marker="&",
                    x_merge_on="location",
            )
        else:
            targetheader = thetargetfile.header()
        targetheader.addnote("extracted from %s" % thepropfile.filename,
                             "developer")

        thepropfile.makeindex()
        self.mixedkeys = self.mixer.match_entities(thepropfile.id_index)
        # we try and merge the header po with any comments at the start of the
        # properties file
        appendedheader = False
        waitingcomments = []
        for propunit in thepropfile.units:
            pounit = self.convertpropunit(thepropfile, propunit, "developer")
            if pounit is None:
                waitingcomments.extend(propunit.comments)
            # FIXME the storage class should not be creating blank units
            if pounit is "discard":
                continue
            if not appendedheader:
                if propunit.isblank():
                    targetheader.addnote("\n".join(waitingcomments).rstrip(),
                                         "developer", position="prepend")
                    waitingcomments = []
                    pounit = None
                appendedheader = True
            if pounit is not None:
                pounit.addnote("\n".join(waitingcomments).rstrip(),
                               "developer", position="prepend")
                waitingcomments = []
                thetargetfile.addunit(pounit)
        if self.personality == "gaia":
            thetargetfile = self.fold_gaia_plurals(thetargetfile)
        thetargetfile.removeduplicates(self.duplicatestyle)
        return thetargetfile

    def mergestore(self, origpropfile, translatedpropfile):
        """converts two .properties files to a .po file..."""
        thetargetfile = po.pofile()
        if self.personality in ("mozilla", "skype"):
            targetheader = thetargetfile.init_headers(
                    x_accelerator_marker="&",
                    x_merge_on="location",
            )
        else:
            targetheader = thetargetfile.header()
        targetheader.addnote("extracted from %s, %s" % (origpropfile.filename, translatedpropfile.filename),
                             "developer")
        origpropfile.makeindex()
        #TODO: self.mixedkeys is overwritten below, so this is useless:
        self.mixedkeys = self.mixer.match_entities(origpropfile.id_index)
        translatedpropfile.makeindex()
        self.mixedkeys = self.mixer.match_entities(translatedpropfile.id_index)
        # we try and merge the header po with any comments at the start of
        # the properties file
        appendedheader = False
        waitingcomments = []
        # loop through the original file, looking at units one by one
        for origprop in origpropfile.units:
            origpo = self.convertpropunit(origpropfile, origprop, "developer")
            if origpo is None:
                waitingcomments.extend(origprop.comments)
            # FIXME the storage class should not be creating blank units
            if origpo is "discard":
                continue
            # handle the header case specially...
            if not appendedheader:
                if origprop.isblank():
                    targetheader.addnote(u"".join(waitingcomments).rstrip(),
                                         "developer", position="prepend")
                    waitingcomments = []
                    origpo = None
                appendedheader = True
            # try and find a translation of the same name...
            if origprop.name in translatedpropfile.locationindex:
                translatedprop = translatedpropfile.locationindex[origprop.name]
                # Need to check that this comment is not a copy of the
                # developer comments
                translatedpo = self.convertpropunit(translatedpropfile,
                                                    translatedprop,
                                                    "translator")
                if translatedpo is "discard":
                    continue
            else:
                translatedpo = None
            # if we have a valid po unit, get the translation and add it...
            if origpo is not None:
                if translatedpo is not None and not self.blankmsgstr:
                    origpo.target = translatedpo.source
                origpo.addnote(u"".join(waitingcomments).rstrip(),
                               "developer", position="prepend")
                waitingcomments = []
                thetargetfile.addunit(origpo)
            elif translatedpo is not None:
                logger.error("didn't convert original property definition '%s'",
                             origprop.name)
        if self.personality == "gaia":
            thetargetfile = self.fold_gaia_plurals(thetargetfile)
        thetargetfile.removeduplicates(self.duplicatestyle)
        return thetargetfile

    def fold_gaia_plurals(self, postore):
        """Fold the multiple plural units of a gaia file into a gettext plural."""

        def _append_plural_unit(store, plurals, plural):
            units = plurals[plural]
            sources = [u.source for u in units]
            targets = [u.target for u in units]
            # TODO: only consider the right ones for sources and targets
            plural_unit = store.addsourceunit(sources)
            plural_unit.target = targets
            plural_unit.addlocation(plural)
            del plurals[plural]

        new_store = type(postore)()
        plurals = {}
        current_plural = u""
        for unit in postore.units:
            if not unit.istranslatable():
                #TODO: reconsider: we could lose header comments here
                continue
            if u"plural(n)" in unit.source:
                if current_plural:
                    # End of a set of plural units
                    _append_plural_unit(new_store, plurals, current_plural)
                    current_plural = u""
                # start of a set of plural units
                location = unit.getlocations()[0]
                current_plural = location
                plurals[location] = []
                # We ignore the first one, since it doesn't contain translatable
                # text, only a marker.
            else:
                location = unit.getlocations()[0]
                if current_plural and location.startswith(current_plural):
                    plurals[current_plural].append(unit)
                    if not '[zero]' in location:
                        # We want to keep [zero] cases separately translatable
                        continue
                elif current_plural:
                    # End of a set of plural units
                    _append_plural_unit(new_store, plurals, current_plural)
                    current_plural = u""

                new_store.addunit(unit)

        if current_plural:
            # The file ended with a set of plural units
            _append_plural_unit(new_store, plurals, current_plural)
            current_plural = u""

        # if everything went well, there should be nothing left in plurals
        if len(plurals) != 0:
            logger.warning("Not all plural units converted correctly:" +
                           "\n".join(plurals.keys()))
        return new_store

    def convertunit(self, propunit, commenttype):
        """Converts a .properties unit to a .po unit. Returns None if empty
        or not for translation."""
        if propunit is None:
            return None
        # escape unicode
        pounit = po.pounit(encoding="UTF-8")
        if hasattr(propunit, "comments"):
            for comment in propunit.comments:
                if "DONT_TRANSLATE" in comment:
                    return "discard"
            pounit.addnote(u"".join(propunit.getnotes()).rstrip(), commenttype)
        # TODO: handle multiline msgid
        if propunit.isblank():
            return None
        pounit.addlocation(propunit.name)
        pounit.source = propunit.source
        pounit.target = u""
        return pounit

    def convertmixedunit(self, labelprop, accesskeyprop, commenttype):
        label_unit = self.convertunit(labelprop, commenttype)
        accesskey_unit = self.convertunit(accesskeyprop, commenttype)
        if label_unit is None:
            return accesskey_unit
        if accesskey_unit is None:
            return label_unit
        target_unit = po.pounit(encoding="UTF-8")
        return self.mixer.mix_units(label_unit, accesskey_unit, target_unit)

    def convertpropunit(self, store, unit, commenttype, mixbucket="dtd"):
        """Converts a unit from store to a po unit, keeping track of mixed
        names along the way.

        ``mixbucket`` can be specified to indicate if the given unit is part of
        the template or the translated file.
        """
        if self.personality != "mozilla":
            # XXX should we enable unit mixing for other personalities?
            return self.convertunit(unit, commenttype)

        # keep track of whether accesskey and label were combined
        key = unit.getid()
        if key not in self.mixedkeys:
            return self.convertunit(unit, commenttype)

        # use special convertmixed unit which produces one pounit with
        # both combined for the label and None for the accesskey
        alreadymixed = self.mixedkeys[key].get(mixbucket, None)
        if alreadymixed:
            # we are successfully throwing this away...
            return None
        elif alreadymixed is False:
            # The mix failed before
            return self.convertunit(unit, commenttype)

        #assert alreadymixed is None
        labelkey, accesskeykey = self.mixer.find_mixed_pair(self.mixedkeys, store, unit)
        labelprop = store.id_index.get(labelkey, None)
        accesskeyprop = store.id_index.get(accesskeykey, None)
        po_unit = self.convertmixedunit(labelprop, accesskeyprop, commenttype)
        if po_unit is not None:
            if accesskeykey is not None:
                self.mixedkeys[accesskeykey][mixbucket] = True
            if labelkey is not None:
                self.mixedkeys[labelkey][mixbucket] = True
            return po_unit
        else:
            # otherwise the mix failed. add each one separately and
            # remember they weren't mixed
            if accesskeykey is not None:
                self.mixedkeys[accesskeykey][mixbucket] = False
            if labelkey is not None:
                self.mixedkeys[labelkey][mixbucket] = False

        return self.convertunit(unit, commenttype)


def convertstrings(inputfile, outputfile, templatefile, personality="strings",
                   pot=False, duplicatestyle="msgctxt", encoding=None):
    """.strings specific convertor function"""
    return convertprop(inputfile, outputfile, templatefile,
                       personality="strings", pot=pot,
                       duplicatestyle=duplicatestyle, encoding=encoding)


def convertmozillaprop(inputfile, outputfile, templatefile, pot=False,
                       duplicatestyle="msgctxt"):
    """Mozilla specific convertor function"""
    return convertprop(inputfile, outputfile, templatefile,
                       personality="mozilla", pot=pot,
                       duplicatestyle=duplicatestyle)


def convertprop(inputfile, outputfile, templatefile, personality="java",
                pot=False, duplicatestyle="msgctxt", encoding=None):
    """reads in inputfile using properties, converts using prop2po, writes
    to outputfile"""
    inputstore = properties.propfile(inputfile, personality, encoding)
    convertor = prop2po(personality=personality, blankmsgstr=pot,
                        duplicatestyle=duplicatestyle)
    if templatefile is None:
        outputstore = convertor.convertstore(inputstore)
    else:
        templatestore = properties.propfile(templatefile, personality, encoding)
        outputstore = convertor.mergestore(templatestore, inputstore)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


formats = {
    "properties": ("po", convertprop),
    ("properties", "properties"): ("po", convertprop),
    "lang": ("po", convertprop),
    ("lang", "lang"): ("po", convertprop),
    "strings": ("po", convertstrings),
    ("strings", "strings"): ("po", convertstrings),
}


def main(argv=None):
    from translate.convert import convert
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True,
                                         description=__doc__)
    parser.add_option("", "--personality", dest="personality",
            default=properties.default_dialect,
            type="choice",
            choices=properties.dialects.keys(),
            help="override the input file format: %s (for .properties files, default: %s)" %
                 (", ".join(properties.dialects.iterkeys()),
                  properties.default_dialect),
            metavar="TYPE")
    parser.add_option("", "--encoding", dest="encoding", default=None,
            help="override the encoding set by the personality",
            metavar="ENCODING")
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.passthrough.append("personality")
    parser.passthrough.append("encoding")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rc2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Windows RC files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/rc2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import po, rc


logger = logging.getLogger(__name__)


class rc2po:
    """Convert a .rc file to a .po file for handling the translation."""

    def convert_store(self, input_store, duplicatestyle="msgctxt"):
        """converts a .rc file to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.init_headers(
                x_accelerator_marker="&",
                x_merge_on="location",
        )
        output_header.addnote("extracted from %s" % input_store.filename, "developer")
        for input_unit in input_store.units:
            output_unit = self.convert_unit(input_unit, "developer")
            if output_unit is not None:
                output_store.addunit(output_unit)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def merge_store(self, template_store, input_store, blankmsgstr=False, duplicatestyle="msgctxt"):
        """converts two .rc files to a .po file..."""
        output_store = po.pofile()
        output_header = output_store.init_headers(
                x_accelerator_marker="&",
                x_merge_on="location",
        )
        output_header.addnote("extracted from %s, %s" % (template_store.filename, input_store.filename), "developer")
        input_store.makeindex()
        for template_unit in template_store.units:
            origpo = self.convert_unit(template_unit, "developer")
            # try and find a translation of the same name...
            template_unit_name = "".join(template_unit.getlocations())
            if template_unit_name in input_store.locationindex:
                translatedrc = input_store.locationindex[template_unit_name]
                translatedpo = self.convert_unit(translatedrc, "translator")
            else:
                translatedpo = None
            # if we have a valid po unit, get the translation and add it...
            if origpo is not None:
                if translatedpo is not None and not blankmsgstr:
                    origpo.target = translatedpo.source
                output_store.addunit(origpo)
            elif translatedpo is not None:
                logging.error("error converting original rc definition %s",
                              template_unit.name)
        output_store.removeduplicates(duplicatestyle)
        return output_store

    def convert_unit(self, input_unit, commenttype):
        """Converts a .rc unit to a .po unit. Returns None if empty
        or not for translation."""
        if input_unit is None:
            return None
        # escape unicode
        output_unit = po.pounit(encoding="UTF-8")
        output_unit.addlocation("".join(input_unit.getlocations()))
        output_unit.source = input_unit.source
        output_unit.target = ""
        return output_unit


def convertrc(input_file, output_file, template_file, pot=False, duplicatestyle="msgctxt", charset=None, lang=None, sublang=None):
    """reads in input_file using rc, converts using rc2po, writes to output_file"""
    input_store = rc.rcfile(input_file, lang, sublang, encoding=charset)
    convertor = rc2po()
    if template_file is None:
        output_store = convertor.convert_store(input_store, duplicatestyle=duplicatestyle)
    else:
        template_store = rc.rcfile(template_file, lang, sublang, encoding=charset)
        output_store = convertor.merge_store(template_store, input_store, blankmsgstr=pot, duplicatestyle=duplicatestyle)
    if output_store.isempty():
        return 0
    output_file.write(str(output_store))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"rc": ("po", convertrc), ("rc", "rc"): ("po", convertrc),
               "nls": ("po", convertrc), ("nls", "nls"): ("po", convertrc)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, usepots=True, description=__doc__)
    DEFAULTCHARSET = "cp1252"
    parser.add_option("", "--charset", dest="charset", default=DEFAULTCHARSET,
        help="charset to use to decode the RC files (default: %s)" % DEFAULTCHARSET, metavar="CHARSET")
    DEFAULTLANG = "LANG_ENGLISH"
    parser.add_option("-l", "--lang", dest="lang", default=DEFAULTLANG,
        help="LANG entry (default: %s)" % DEFAULTLANG, metavar="LANG")
    DEFAULTSUBLANG = "SUBLANG_DEFAULT"
    parser.add_option("", "--sublang", dest="sublang", default=DEFAULTSUBLANG,
        help="SUBLANG entry (default: %s)" % DEFAULTSUBLANG, metavar="SUBLANG")
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.passthrough.append("charset")
    parser.passthrough.append("lang")
    parser.passthrough.append("sublang")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = sub2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert subtitle files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/sub2po.html
for examples and usage instructions.
"""

import logging

from translate.storage import po


logger = logging.getLogger(__name__)


def convert_store(input_store, duplicatestyle="msgctxt"):
    """converts a subtitle file to a .po file..."""
    output_store = po.pofile()
    output_header = output_store.header()
    output_header.addnote("extracted from %s" % input_store.filename,
                          "developer")

    for input_unit in input_store.units:
        output_unit = convert_unit(input_unit, "developer")
        if output_unit is not None:
            output_store.addunit(output_unit)
    output_store.removeduplicates(duplicatestyle)
    return output_store


def merge_store(template_store, input_store, blankmsgstr=False,
                duplicatestyle="msgctxt"):
    """converts two subtitle files to a .po file..."""
    output_store = po.pofile()
    output_header = output_store.headers()
    output_header.addnote("extracted from %s, %s" %
                          (template_store.filename, input_store.filename),
                          "developer")

    input_store.makeindex()
    for template_unit in template_store.units:
        origpo = convert_unit(template_unit, "developer")
        # try and find a translation of the same name...
        template_unit_name = "".join(template_unit.getlocations())
        if template_unit_name in input_store.locationindex:
            translatedini = input_store.locationindex[template_unit_name]
            translatedpo = convert_unit(translatedini, "translator")
        else:
            translatedpo = None
        # if we have a valid po unit, get the translation and add it...
        if origpo is not None:
            if translatedpo is not None and not blankmsgstr:
                origpo.target = translatedpo.source
            output_store.addunit(origpo)
        elif translatedpo is not None:
            logger.error("error converting original subtitle definition %s",
                         origini.name)
    output_store.removeduplicates(duplicatestyle)
    return output_store


def convert_unit(input_unit, commenttype):
    """Converts a subtitle unit to a .po unit. Returns None if empty
    or not for translation."""
    if input_unit is None:
        return None
    # escape unicode
    output_unit = po.pounit(encoding="UTF-8")
    output_unit.addlocation("".join(input_unit.getlocations()))
    output_unit.addnote(input_unit.getnotes("developer"), "developer")
    output_unit.source = input_unit.source
    output_unit.target = ""
    return output_unit


def convertsub(input_file, output_file, template_file=None, pot=False,
               duplicatestyle="msgctxt"):
    """Reads in *input_file* using translate.subtitles, converts using
    :class:`sub2po`, writes to *output_file*."""
    from translate.storage import subtitles
    input_store = subtitles.SubtitleFile(input_file)
    if template_file is None:
        output_store = convert_store(input_store,
                                     duplicatestyle=duplicatestyle)
    else:
        template_store = subtitles.SubtitleFile(template_file)
        output_store = merge_store(template_store, input_store,
                                   blankmsgstr=pot,
                                   duplicatestyle=duplicatestyle)
    if output_store.isempty():
        return 0
    output_file.write(str(output_store))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
          "srt": ("po", convertsub), ("srt", "srt"): ("po", convertsub),
          "sub": ("po", convertsub), ("sub", "sub"): ("po", convertsub),
          "ssa": ("po", convertsub), ("ssa", "ssa"): ("po", convertsub),
          "ass": ("po", convertsub), ("ass", "ass"): ("po", convertsub),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = symb2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Symbian localisation files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/symb2po.html
for examples and usage instructions.
"""

from translate.storage import factory
from translate.storage.pypo import unescape
from translate.storage.symbian import *


def read_header_items(ps):
    match = read_while(ps, header_item_or_end_re.match, lambda match: match is None)
    if match.groupdict()['end_comment'] is not None:
        return {}

    results = {}
    while match:
        match_chunks = match.groupdict()
        ps.read_line()
        results[match_chunks['key']] = match_chunks['value']
        match = header_item_re.match(ps.current_line)

    match = read_while(ps, identity, lambda line: not line.startswith('*/'))
    ps.read_line()
    return results


def parse(ps):
    header = read_header_items(ps)
    units = []
    try:
        while True:
            eat_whitespace(ps)
            skip_no_translate(ps)
            match = string_entry_re.match(ps.current_line)
            if match is not None:
                units.append((match.groupdict()['id'], unescape(match.groupdict()['str'][1:-1])))
            ps.read_line()
    except StopIteration:
        pass
    return header, units


def read_symbian(f):
    lines = list(f)
    charset = read_charset(lines)
    return parse(ParseState(iter(lines), charset))


def get_template_dict(template_file):
    if template_file is not None:
        template_header, template_units = read_symbian(template_file)
        return template_header, dict(template_units)
    else:
        return {}, {}


def build_output(units, template_header, template_dict):
    output_store = factory.classes['po']()
    ignore = set(['r_string_languagegroup_name'])
    header_entries = {
        'Last-Translator': template_header.get('Author', ''),
        'Language-Team': template_dict.get('r_string_languagegroup_name', ''),
        'Content-Transfer-Encoding': '8bit',
        'Content-Type': 'text/plain; charset=UTF-8',
    }
    output_store.updateheader(add=True, **header_entries)
    for id, source in units:
        if id in ignore:
            continue
        unit = output_store.UnitClass(source)
        unit.target = template_dict.get(id, '')
        unit.addlocation(id)
        output_store.addunit(unit)
    return output_store


def convert_symbian(input_file, output_file, template_file, pot=False, duplicatestyle="msgctxt"):
    header, units = read_symbian(input_file)
    template_header, template_dict = get_template_dict(template_file)
    output_store = build_output(units, template_header, template_dict)

    if output_store.isempty():
        return 0
    else:
        output_file.write(str(output_store))
        return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"r01": ("po", convert_symbian)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_accesskey
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of The Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Test the various functions for combining and extracting accesskeys and
labels"""

from translate.convert import accesskey


def test_get_label_and_accesskey():
    """test that we can extract the label and accesskey components from an
    accesskey+label string"""
    assert accesskey.extract(u"") == (u"", u"")
    assert accesskey.extract(u"File") == (u"File", u"")
    assert accesskey.extract(u"&File") == (u"File", u"F")
    assert accesskey.extract(u"~File", u"~") == (u"File", u"F")
    assert accesskey.extract(u"_File", u"_") == (u"File", u"F")


def test_ignore_entities():
    """test that we don't get confused with entities and a & access key
    marker"""
    assert accesskey.extract(u"Set &browserName; as &Default") != (u"Set &browserName; as &Default", u"b")
    assert accesskey.extract(u"Set &browserName; as &Default") == (u"Set &browserName; as Default", u"D")


def test_alternate_accesskey_marker():
    """check that we can identify the accesskey if the marker is different"""
    assert accesskey.extract(u"~File", u"~") == (u"File", u"F")
    assert accesskey.extract(u"&File", u"~") == (u"&File", u"")


def test_unicode():
    """test that we can do the same with unicode strings"""
    assert accesskey.extract(u"Ei") == (u"Ei", u"")
    assert accesskey.extract(u"E&i") == (u"Ei", u"")
    assert accesskey.extract(u"E_i", u"_") == (u"Ei", u"")
    label, akey = accesskey.extract(u"E&i")
    assert label, akey == (u"Ei", u"")
    assert isinstance(label, unicode) and isinstance(akey, unicode)
    assert accesskey.combine(u"Ei", u"") == (u"E&i")


def test_numeric():
    """test combining and extracting numeric markers"""
    assert accesskey.extract(u"&100%") == (u"100%", u"1")
    assert accesskey.combine(u"100%", u"1") == u"&100%"


def test_empty_string():
    """test that we can handle and empty label+accesskey string"""
    assert accesskey.extract(u"") == (u"", u"")
    assert accesskey.extract(u"", u"~") == (u"", u"")


def test_end_of_string():
    """test that we can handle an accesskey at the end of the string"""
    assert accesskey.extract(u"Hlola&") == (u"Hlola&", u"")


def test_combine_label_accesskey():
    """test that we can combine accesskey and label to create a label+accesskey
    string"""
    assert accesskey.combine(u"File", u"F") == u"&File"
    assert accesskey.combine(u"File", u"F", u"~") == u"~File"


def test_combine_label_accesskey_different_capitals():
    """test that we can combine accesskey and label to create a label+accesskey
    string when we have more then one case or case is wrong."""
    # Prefer the correct case, even when an alternate case occurs first
    assert accesskey.combine(u"Close Other Tabs", u"o") == u"Cl&ose Other Tabs"
    assert accesskey.combine(u"Other Closed Tab", u"o") == u"Other Cl&osed Tab"
    assert accesskey.combine(u"Close Other Tabs", u"O") == u"Close &Other Tabs"
    # Correct case is missing from string, so use alternate case
    assert accesskey.combine(u"Close Tabs", u"O") == u"Cl&ose Tabs"
    assert accesskey.combine(u"Other Tabs", u"o") == u"&Other Tabs"


def test_uncombinable():
    """test our behaviour when we cannot combine label and accesskey"""
    assert accesskey.combine(u"File", u"D") is None
    assert accesskey.combine(u"File", u"") is None
    assert accesskey.combine(u"", u"") is None


def test_accesskey_already_in_text():
    """test that we can combine if the accesskey is already in the text"""
    assert accesskey.combine(u"Mail & Newsgroups", u"N") == u"Mail & &Newsgroups"
    assert accesskey.extract(u"Mail & &Newsgroups") == (u"Mail & Newsgroups", u"N")

########NEW FILE########
__FILENAME__ = test_convert
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys

import pytest

from translate.convert import convert


class TestConvertCommand:
    """Tests running actual commands on files"""
    convertmodule = convert
    defaultoptions = {"progress": "none"}

    def setup_method(self, method):
        """creates a clean test directory for the given method"""
        self.testdir = "%s_%s" % (self.__class__.__name__, method.__name__)
        self.cleardir()
        os.mkdir(self.testdir)
        self.rundir = os.path.abspath(os.getcwd())

    def teardown_method(self, method):
        """removes the test directory for the given method"""
        os.chdir(self.rundir)
        self.cleardir()

    def cleardir(self):
        """removes the test directory"""
        if os.path.exists(self.testdir):
            for dirpath, subdirs, filenames in os.walk(self.testdir, topdown=False):
                for name in filenames:
                    os.remove(os.path.join(dirpath, name))
                for name in subdirs:
                    os.rmdir(os.path.join(dirpath, name))
        if os.path.exists(self.testdir):
            os.rmdir(self.testdir)
        assert not os.path.exists(self.testdir)

    def run_command(self, *argv, **kwargs):
        """runs the command via the main function, passing self.defaultoptions and keyword arguments as --long options and argv arguments straight"""
        os.chdir(self.testdir)
        argv = list(argv)
        kwoptions = getattr(self, "defaultoptions", {}).copy()
        kwoptions.update(kwargs)
        for key, value in kwoptions.iteritems():
            if value is True:
                argv.append("--%s" % key)
            else:
                argv.append("--%s=%s" % (key, value))
        try:
            self.convertmodule.main(argv)
        finally:
            os.chdir(self.rundir)

    def get_testfilename(self, filename):
        """gets the path to the test file"""
        return os.path.join(self.testdir, filename)

    def open_testfile(self, filename, mode="r"):
        """opens the given filename in the testdirectory in the given mode"""
        filename = self.get_testfilename(filename)
        if not mode.startswith("r"):
            subdir = os.path.dirname(filename)
            currentpath = ""
            if not os.path.isdir(subdir):
                for part in subdir.split(os.sep):
                    currentpath = os.path.join(currentpath, part)
                    if not os.path.isdir(currentpath):
                        os.mkdir(currentpath)
        return open(filename, mode)

    def create_testfile(self, filename, contents):
        """creates the given file in the testdirectory with the given contents"""
        testfile = self.open_testfile(filename, "w")
        testfile.write(contents)
        testfile.close()

    def read_testfile(self, filename):
        """reads the given file in the testdirectory and returns the contents"""
        testfile = open(self.get_testfilename(filename))
        content = testfile.read()
        testfile.close()
        return content

    def help_check(self, options, option, last=False):
        """check that a help string occurs and remove it"""
        assert option in options
        newoptions = []
        for line in options.splitlines():
            if option in line or not line.lstrip().startswith("-"):
                continue
            newoptions.append(line)
        if last:
            assert newoptions == []
        return "\n".join(newoptions)

    def test_help(self):
        """tests getting help (returning the help_string so further tests can be done)"""
        stdout = sys.stdout
        helpfile = self.open_testfile("help.txt", "w")
        sys.stdout = helpfile
        try:
            pytest.raises(SystemExit, self.run_command, help=True)
        finally:
            sys.stdout = stdout
        helpfile.close()
        help_string = self.read_testfile("help.txt")
        print(help_string)
        convertsummary = self.convertmodule.__doc__.split("\n")[0]
        # the convertsummary might be wrapped. this will probably unwrap it
        assert convertsummary in help_string.replace("\n", " ")
        usageline = help_string[:help_string.find("\n")]
        # Different versions of optparse might contain either upper or
        # lowercase versions of 'Usage:' and 'Options:', so we need to take
        # that into account
        assert (usageline.startswith("Usage: ") or usageline.startswith("usage: ")) \
            and "[--version] [-h|--help]" in usageline
        options = help_string[help_string.find("ptions:\n"):]
        options = options[options.find("\n")+1:]
        options = self.help_check(options, "--progress=PROGRESS")
        options = self.help_check(options, "--version")
        options = self.help_check(options, "-h, --help")
        options = self.help_check(options, "--manpage")
        options = self.help_check(options, "--errorlevel=ERRORLEVEL")
        options = self.help_check(options, "-i INPUT, --input=INPUT")
        options = self.help_check(options, "-x EXCLUDE, --exclude=EXCLUDE")
        options = self.help_check(options, "-o OUTPUT, --output=OUTPUT")
        options = self.help_check(options, "-S, --timestamp")
        return options

########NEW FILE########
__FILENAME__ = test_csv2po
#!/usr/bin/env python

from translate.convert import csv2po, test_convert
from translate.misc import wStringIO
from translate.storage import csvl10n, po
from translate.storage.test_base import first_translatable, headerless_len


def test_replacestrings():
    """Test the _replacestring function"""
    assert csv2po.replacestrings("Test one two three",
                                 ("one", "een"),
                                 ("two", "twee")) == "Test een twee three"


class TestCSV2PO:

    def csv2po(self, csvsource, template=None):
        """helper that converts csv source to po source without requiring files"""
        inputfile = wStringIO.StringIO(csvsource)
        inputcsv = csvl10n.csvfile(inputfile)
        if template:
            templatefile = wStringIO.StringIO(template)
            inputpot = po.pofile(templatefile)
        else:
            inputpot = None
        convertor = csv2po.csv2po(templatepo=inputpot)
        outputpo = convertor.convertstore(inputcsv)
        return outputpo

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        print(str(storage))
        assert headerless_len(storage.units) == 1
        return first_translatable(storage)

    def test_simpleentity(self):
        """checks that a simple csv entry definition converts properly to a po entry"""
        csvheader = 'location,source,target\n'
        csvsource = 'intl.charset.default,ISO-8859-1,UTF-16'
        # Headerless
        pofile = self.csv2po(csvsource)
        pounit = self.singleelement(pofile)
        # With header
        pofile = self.csv2po(csvheader + csvsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ['intl.charset.default']
        assert pounit.source == "ISO-8859-1"
        assert pounit.target == "UTF-16"

    def test_simpleentity_with_template(self):
        """checks that a simple csv entry definition converts properly to a po entry"""
        csvsource = '''source,original,translation
intl.charset.default,ISO-8859-1,UTF-16'''
        potsource = '''#: intl.charset.default
msgid "ISO-8859-1"
msgstr ""
'''
        pofile = self.csv2po(csvsource, potsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ['intl.charset.default']
        assert pounit.source == "ISO-8859-1"
        assert pounit.target == "UTF-16"

    def test_newlines(self):
        """tests multiline po entries"""
        minicsv = r'''"Random comment
with continuation","Original text","Langdradige teks
wat lank aanhou"
'''
        pofile = self.csv2po(minicsv)
        unit = self.singleelement(pofile)
        assert unit.getlocations() == ['Random comment\nwith continuation']
        assert unit.source == "Original text"
        print(unit.target)
        assert unit.target == "Langdradige teks\nwat lank aanhou"

    def test_tabs(self):
        """Test the escaping of tabs"""
        minicsv = ',"First column\tSecond column","Twee kolomme gesky met \t"'
        pofile = self.csv2po(minicsv)
        unit = self.singleelement(pofile)
        print(unit.source)
        assert unit.source == "First column\tSecond column"
        assert not pofile.findunit("First column\tSecond column").target == "Twee kolomme gesky met \\t"

    def test_quotes(self):
        """Test the escaping of quotes (and slash)"""
        minicsv = r''',"Hello ""Everyone""","Good day ""All"""
,"Use \"".","Gebruik \""."'''
        print(minicsv)
        csvfile = csvl10n.csvfile(wStringIO.StringIO(minicsv))
        print(str(csvfile))
        pofile = self.csv2po(minicsv)
        unit = first_translatable(pofile)
        assert unit.source == 'Hello "Everyone"'
        assert pofile.findunit('Hello "Everyone"').target == 'Good day "All"'
        print(str(pofile))
        for unit in pofile.units:
            print(unit.source)
            print(unit.target)
            print()
#        assert pofile.findunit('Use \\".').target == 'Gebruik \\".'

    def test_empties(self):
        """Tests that things keep working with empty entries"""
        minicsv = ',SomeSource,'
        pofile = self.csv2po(minicsv)
        assert pofile.findunit("SomeSource") is not None
        assert pofile.findunit("SomeSource").target == ""
        assert headerless_len(pofile.units) == 1

    def test_kdecomment(self):
        """checks that we can merge into KDE comment entries"""
        csvsource = '''location,source,target
simple.c,Source,Target'''
        potsource = r'''#: simple.c
msgid "_: KDE comment\n"
"Source"
msgstr ""
'''
        pofile = self.csv2po(csvsource, potsource)
        pounit = self.singleelement(pofile)
        assert pounit._extract_msgidcomments() == 'KDE comment'
        assert pounit.source == "Source"
        assert pounit.target == "Target"


class TestCSV2POCommand(test_convert.TestConvertCommand, TestCSV2PO):
    """Tests running actual csv2po commands on files"""
    convertmodule = csv2po

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--charset=CHARSET")
        options = self.help_check(options, "--columnorder=COLUMNORDER")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE", last=True)

########NEW FILE########
__FILENAME__ = test_dtd2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.convert import dtd2po, test_convert
from translate.misc import wStringIO
from translate.storage import dtd, po


class TestDTD2PO:

    def dtd2po(self, dtdsource, dtdtemplate=None):
        """helper that converts dtd source to po source without requiring files"""
        inputfile = wStringIO.StringIO(dtdsource)
        inputdtd = dtd.dtdfile(inputfile)
        convertor = dtd2po.dtd2po()
        if dtdtemplate is None:
            outputpo = convertor.convertstore(inputdtd)
        else:
            templatefile = wStringIO.StringIO(dtdtemplate)
            templatedtd = dtd.dtdfile(templatefile)
            outputpo = convertor.mergestore(templatedtd, inputdtd)
        return outputpo

    def convertdtd(self, dtdsource):
        """call the convertdtd, return the outputfile"""
        inputfile = wStringIO.StringIO(dtdsource)
        outputfile = wStringIO.StringIO()
        templatefile = None
        assert dtd2po.convertdtd(inputfile, outputfile, templatefile)
        return outputfile.getvalue()

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile.units[1])
        return pofile.units[1]

    def countelements(self, pofile):
        """returns the number of non-header items"""
        if pofile.units[0].isheader():
            return len(pofile.units) - 1
        else:
            return len(pofile.units)

    def test_simpleentity(self):
        """checks that a simple dtd entity definition converts properly to a po entry"""
        dtdsource = '<!ENTITY test.me "bananas for sale">\n'
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "bananas for sale"
        assert pounit.target == ""
        # Now with a template language
        dtdtemplate = '<!ENTITY test.me "bananas for sale">\n'
        dtdtranslated = '<!ENTITY test.me "piesangs te koop">\n'
        pofile = self.dtd2po(dtdtranslated, dtdtemplate)
        pounit = self.singleelement(pofile)
        assert pounit.source == "bananas for sale"
        assert pounit.target == "piesangs te koop"

    def test_convertdtd(self):
        """checks that the convertdtd function is working"""
        dtdsource = '<!ENTITY saveas.label "Save As...">\n'
        posource = self.convertdtd(dtdsource)
        pofile = po.pofile(wStringIO.StringIO(posource))
        unit = self.singleelement(pofile)
        assert unit.source == "Save As..."
        assert unit.target == ""

    def test_apos(self):
        """apostrophe should not break a single-quoted entity definition, bug 69"""
        dtdsource = "<!ENTITY test.me 'bananas &apos; for sale'>\n"
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "bananas ' for sale"

    def test_quotes(self):
        """quotes should be handled in a single-quoted entity definition"""
        dtdsource = """<!ENTITY test.metoo '"Bananas" for sale'>\n"""
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        print(str(pounit))
        assert pounit.source == '"Bananas" for sale'

    def test_emptyentity(self):
        """checks that empty entity definitions survive into po file, bug 15"""
        dtdsource = '<!ENTITY credit.translation "">\n'
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        assert "credit.translation" in str(pounit)
        assert 'msgctxt "credit.translation"' in str(pounit)

    def test_two_empty_entities(self):
        """checks that two empty entitu definitions have correct context (bug 2190)."""
        dtdsource = '<!ENTITY community.exp.start "">\n<!ENTITY contribute.end "">\n'
        pofile = self.dtd2po(dtdsource)
        assert pofile.units[-2].getcontext() == u"community.exp.start"
        assert pofile.units[-1].getcontext() == u"contribute.end"

    def test_emptyentity_translated(self):
        """checks that if we translate an empty entity it makes it into the PO, bug 101"""
        dtdtemplate = '<!ENTITY credit.translation "">\n'
        dtdsource = '<!ENTITY credit.translation "Translators Names">\n'
        pofile = self.dtd2po(dtdsource, dtdtemplate)
        unit = self.singleelement(pofile)
        print(unit)
        assert "credit.translation" in str(unit)
        # We don't want this to simply be seen as a header:
        assert len(unit.getid()) != 0
        assert unit.target == "Translators Names"

    def test_localisaton_note_simple(self):
        """test the simple localisation more becomes a #. comment"""
        dtdsource = '''<!-- LOCALIZATION NOTE (alwaysCheckDefault.height):
  There's some sort of bug which makes wrapping checkboxes not properly reflow,
  causing the bottom border of the groupbox to be cut off; set this
  appropriately if your localization causes this checkbox to wrap.
-->
<!ENTITY alwaysCheckDefault.height  "3em">
'''
        pofile = self.dtd2po(dtdsource)
        posource = str(pofile)
        print(posource)
        assert posource.count('#.') == 5  # 1 Header extracted from, 3 comment lines, 1 autoinserted comment

    def test_localisation_note_merge(self):
        """test that LOCALIZATION NOTES are added properly as #. comments and disambiguated with msgctxt entries"""
        dtdtemplate = '<!--LOCALIZATION NOTE (%s): Some note -->\n' + \
            '<!ENTITY %s "Source text">\n'
        dtdsource = dtdtemplate % ("note1.label", "note1.label") + dtdtemplate % ("note2.label", "note2.label")
        pofile = self.dtd2po(dtdsource)
        posource = str(pofile.units[1]) + str(pofile.units[2])
        print(posource)
        assert posource.count('#.') == 2
        assert posource.count('msgctxt') == 2

    def test_donttranslate_simple(self):
        """check that we handle DONT_TRANSLATE messages properly"""
        dtdsource = '''<!-- LOCALIZATION NOTE (region.Altitude): DONT_TRANSLATE -->
<!ENTITY region.Altitude "Very High">'''
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 0
        dtdsource = '''<!-- LOCALIZATION NOTE (exampleOpenTag.label): DONT_TRANSLATE: they are text for HTML tagnames: "<i>" and "</i>" -->
<!ENTITY exampleOpenTag.label "&lt;i&gt;">'''
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 0
        dtdsource = '''<!-- LOCALIZATION NOTE (imapAdvanced.label): Do not translate "IMAP" -->
<!ENTITY imapAdvanced.label "Advanced IMAP Server Settings">'''
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 1

    def test_donttranslate_label(self):
        """test strangeness when label entity is marked DONT_TRANSLATE and accesskey is not, bug 30"""
        dtdsource = '<!--LOCALIZATION NOTE (editorCheck.label): DONT_TRANSLATE -->\n' + \
            '<!ENTITY editorCheck.label "Composer">\n<!ENTITY editorCheck.accesskey "c">\n'
        pofile = self.dtd2po(dtdsource)
        posource = str(pofile)
        # we need to decided what we're going to do here - see the comments in bug 30
        # this tests the current implementation which is that the DONT_TRANSLATE string is removed, but the other remains
        assert 'editorCheck.label' not in posource
        assert 'editorCheck.accesskey' in posource

    def test_donttranslate_onlyentity(self):
        """if the entity is itself just another entity then it shouldn't appear in the output PO file"""
        dtdsource = '''<!-- LOCALIZATION NOTE (mainWindow.title): DONT_TRANSLATE -->
<!ENTITY mainWindow.title "&brandFullName;">'''
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 0

    def test_donttranslate_commentedout(self):
        """check that we don't process messages in <!-- comments -->: bug 102"""
        dtdsource = '''<!-- commenting out until bug 38906 is fixed
<!ENTITY messagesHeader.label         "Messages"> -->'''
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 0

    def test_spaces_at_start_of_dtd_lines(self):
        """test that pretty print spaces at the start of subsequent DTD element lines are removed from the PO file, bug 79"""
        # Space at the end of the line
        dtdsource = '<!ENTITY  noupdatesfound.intro "First line then \n' + \
          '                                          next lines.">\n'
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        # We still need to decide how we handle line line breaks in the DTD entities.  It seems that we should actually
        # drop the line break but this has not been implemented yet.
        assert pounit.source == "First line then \nnext lines."
        # No space at the end of the line
        dtdsource = '<!ENTITY  noupdatesfound.intro "First line then\n' + \
          '                                          next lines.">\n'
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "First line then \nnext lines."

    def test_accesskeys_folding(self):
        """test that we fold accesskeys into message strings"""
        dtdsource_template = '<!ENTITY  fileSaveAs.%s "Save As...">\n<!ENTITY  fileSaveAs.%s "S">\n'
        lang_template = '<!ENTITY  fileSaveAs.%s "Gcina ka...">\n<!ENTITY  fileSaveAs.%s "G">\n'
        for label in ("label", "title"):
            for accesskey in ("accesskey", "accessKey", "akey"):
                pofile = self.dtd2po(dtdsource_template % (label, accesskey))
                pounit = self.singleelement(pofile)
                assert pounit.source == "&Save As..."
                # Test with template (bug 155)
                pofile = self.dtd2po(lang_template % (label, accesskey), dtdsource_template % (label, accesskey))
                pounit = self.singleelement(pofile)
                assert pounit.source == "&Save As..."
                assert pounit.target == "&Gcina ka..."

    def test_accesskeys_mismatch(self):
        """check that we can handle accesskeys that don't match and thus can't be folded into the .label entry"""
        dtdsource = '<!ENTITY  fileSave.label "Save">\n' + \
           '<!ENTITY  fileSave.accesskey "z">\n'
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 2

    def test_carriage_return_in_multiline_dtd(self):
        """test that we create nice PO files when we find a \r\n in a multiline DTD element"""
        dtdsource = '<!ENTITY  noupdatesfound.intro "First line then \r\n' + \
          '                                          next lines.">\n'
        pofile = self.dtd2po(dtdsource)
        unit = self.singleelement(pofile)
        assert unit.source == "First line then \nnext lines."

    def test_multiline_with_blankline(self):
        """test that we can process a multiline entity that has a blank line in it, bug 331"""
        dtdsource = '''
<!ENTITY multiline.text "
Some text

Some other text
">'''
        pofile = self.dtd2po(dtdsource)
        unit = self.singleelement(pofile)
        assert unit.source == "Some text \n \nSome other text"

    def test_mulitline_closing_quotes(self):
        """test that we support various styles and spaces after closing quotes on multiline entities"""
        dtdsource = '''
<!ENTITY pref.plural '<span>opsies</span><span
                      class="noWin">preferences</span>' >
'''
        pofile = self.dtd2po(dtdsource)
        unit = self.singleelement(pofile)
        assert unit.source == '<span>opsies</span><span \nclass="noWin">preferences</span>'

    def test_preserving_spaces(self):
        """test that we preserve space that appear at the start of the first line of a DTD entity"""
        # Space before first character
        dtdsource = '<!ENTITY mainWindow.titlemodifiermenuseparator " - ">'
        pofile = self.dtd2po(dtdsource)
        unit = self.singleelement(pofile)
        assert unit.source == " - "
        # Double line and spaces
        dtdsource = '<!ENTITY mainWindow.titlemodifiermenuseparator " - with a newline\n    and more text">'
        pofile = self.dtd2po(dtdsource)
        unit = self.singleelement(pofile)
        print(repr(unit.source))
        assert unit.source == " - with a newline \nand more text"

    def test_escaping_newline_tabs(self):
        """test that we handle all kinds of newline permutations"""
        dtdsource = '<!ENTITY  noupdatesfound.intro "A hard coded newline.\\nAnd tab\\t and a \\r carriage return.">\n'
        converter = dtd2po.dtd2po()
        thedtd = dtd.dtdunit()
        thedtd.parse(dtdsource)
        thepo = po.pounit()
        converter.convertstrings(thedtd, thepo)
        print(thedtd)
        print(thepo.source)
        # \n in a dtd should also appear as \n in the PO file
        assert thepo.source == r"A hard coded newline.\nAnd tab\t and a \r carriage return."

    def test_abandoned_accelerator(self):
        """test that when a language DTD has an accelerator but the template DTD does not that we abandon the accelerator"""
        dtdtemplate = '<!ENTITY test.label "Test">\n'
        dtdlanguage = '<!ENTITY test.label "Toets">\n<!ENTITY test.accesskey "T">\n'
        pofile = self.dtd2po(dtdlanguage, dtdtemplate)
        unit = self.singleelement(pofile)
        assert unit.source == "Test"
        assert unit.target == "Toets"

    def test_unassociable_accelerator(self):
        """test to see that we can handle accelerator keys that cannot be associated correctly"""
        dtdsource = '<!ENTITY  managecerts.button "Manage Certificates...">\n<!ENTITY  managecerts.accesskey "M">'
        pofile = self.dtd2po(dtdsource)
        assert pofile.units[1].source == "Manage Certificates..."
        assert pofile.units[2].source == "M"
        pofile = self.dtd2po(dtdsource, dtdsource)
        assert pofile.units[1].target == "Manage Certificates..."
        assert pofile.units[2].target == "M"

    def test_changed_labels_and_accelerators(self):
        """test to ensure that when the template changes an entity name we can still manage the accelerators"""
        dtdtemplate = '''<!ENTITY  managecerts.caption      "Manage Certificates">
<!ENTITY  managecerts.text         "Use the Certificate Manager to manage your personal certificates, as well as those of other people and certificate authorities.">
<!ENTITY  managecerts.button       "Manage Certificates...">
<!ENTITY  managecerts.accesskey    "M">'''
        dtdlanguage = '''<!ENTITY managecerts.label " ">
<!ENTITY managecerts.text "            .">
<!ENTITY managecerts.button " ...">
<!ENTITY managecerts.accesskey "">'''
        pofile = self.dtd2po(dtdlanguage, dtdtemplate)
        print(pofile)
        assert pofile.units[3].source == "Manage Certificates..."
        assert pofile.units[3].target == u" ..."
        assert pofile.units[4].source == "M"
        assert pofile.units[4].target == u""

    @mark.xfail(reason="Not Implemented")
    def test_accelerator_keys_not_in_sentence(self):
        """tests to ensure that we can manage accelerator keys that are not part of the transated sentence eg in Chinese"""
        dtdtemplate = '''<!ENTITY useAutoScroll.label             "Use autoscrolling">
<!ENTITY useAutoScroll.accesskey         "a">'''
        dtdlanguage = '''<!ENTITY useAutoScroll.label             "(Autoscrolling)">
<!ENTITY useAutoScroll.accesskey         "a">'''
        pofile = self.dtd2po(dtdlanguage, dtdtemplate)
        print(pofile)
        assert pofile.units[1].target == "(&Autoscrolling)"
        # We assume that accesskeys with no associated key should be done as follows "XXXX (&A)"
        # TODO - check that we can unfold this from PO -> DTD
        dtdlanguage = '''<!ENTITY useAutoScroll.label             "">
<!ENTITY useAutoScroll.accesskey         "a">'''
        pofile = self.dtd2po(dtdlanguage, dtdtemplate)
        print(pofile)
        assert pofile.units[1].target == " (&A)"

    def test_exclude_entity_includes(self):
        """test that we don't turn an include into a translatable string"""
        dtdsource = '<!ENTITY % brandDTD SYSTEM "chrome://branding/locale/brand.dtd">'
        pofile = self.dtd2po(dtdsource)
        assert self.countelements(pofile) == 0

    def test_linewraps(self):
        """check that redundant line wraps are removed from the po file"""
        dtdsource = '''<!ENTITY generic.longDesc "
<p>Test me.</p>
">'''
        pofile = self.dtd2po(dtdsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "<p>Test me.</p>"

    def test_merging_with_new_untranslated(self):
        """test that when we merge in new untranslated strings with existing translations we manage the encodings properly"""
        # This should probably be in test_po.py but was easier to do here
        dtdtemplate = '''<!ENTITY unreadFolders.label "Unread">\n<!ENTITY viewPickerUnread.label "Unread">\n<!ENTITY unreadColumn.label "Unread">'''
        dtdlanguage = '''<!ENTITY viewPickerUnread.label "">\n<!ENTITY unreadFolders.label "">'''
        pofile = self.dtd2po(dtdlanguage, dtdtemplate)
        print(pofile)
        assert pofile.units[1].source == "Unread"

    def test_merge_without_template(self):
        """test that we we manage the case where we merge and their is no template file"""
        # If we supply a template file we should fail if the template file does not exist or is blank.  We should
        # not put the translation in as the source.
        # TODO: this test fails, since line 16 checks for "not dtdtemplate"
        #   instead of checking for "dtdtemplate is None". What is correct?
        dtdtemplate = ''
        dtdsource = '<!ENTITY no.template "Target">'
        pofile = self.dtd2po(dtdsource, dtdtemplate)
        print(pofile)
        assert self.countelements(pofile) == 0


class TestDTD2POCommand(test_convert.TestConvertCommand, TestDTD2PO):
    """Tests running actual dtd2po commands on files"""
    convertmodule = dtd2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE", last=True)

########NEW FILE########
__FILENAME__ = test_html2po
#!/usr/bin/env python

from pytest import mark

from translate.convert import html2po, po2html, test_convert
from translate.misc import wStringIO


class TestHTML2PO:

    def html2po(self, markup, includeuntagged=False, duplicatestyle="msgctxt", keepcomments=False):
        """Helper to convert html to po without a file."""
        inputfile = wStringIO.StringIO(markup)
        convertor = html2po.html2po()
        outputpo = convertor.convertfile(inputfile, "test", includeuntagged, duplicatestyle, keepcomments)
        return outputpo

    def po2html(self, posource, htmltemplate):
        """Helper to convert po to html without a file."""
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(htmltemplate)
        assert po2html.converthtml(inputfile, outputfile, templatefile)
        return outputfile.getvalue()

    def countunits(self, pofile, expected):
        """helper to check that we got the expected number of messages"""
        actual = len(pofile.units)
        if actual > 0:
            if pofile.units[0].isheader():
                actual = actual - 1
        print(pofile)
        assert actual == expected

    def compareunit(self, pofile, unitnumber, expected):
        """helper to validate a PO message"""
        if not pofile.units[0].isheader():
            unitnumber = unitnumber - 1
        print('unit source: ' + pofile.units[unitnumber].source.encode('utf-8') + '|')
        print('expected: ' + expected.encode('utf-8') + '|')
        assert unicode(pofile.units[unitnumber].source) == unicode(expected)

    def check_single(self, markup, itemtext):
        """checks that converting this markup produces a single element with value itemtext"""
        pofile = self.html2po(markup)
        self.countunits(pofile, 1)
        self.compareunit(pofile, 1, itemtext)

    def check_null(self, markup):
        """checks that converting this markup produces no elements"""
        pofile = self.html2po(markup)
        self.countunits(pofile, 0)

    def check_phpsnippet(self, php):
        """Given a snippet of php, put it into an HTML shell and see
        if the results are as expected"""
        self.check_single('<html><head></head><body><p><a href="' + php + '/site.html">Body text</a></p></body></html>', "Body text")
        self.check_single('<html><head></head><body><p>More things in <a href="' + php + '/site.html">Body text</a></p></body></html>', 'More things in <a href="' + php + '/site.html">Body text</a>')
        self.check_null('<html><head></head><body><p>' + php + '</p></body></html>')

    def test_htmllang(self):
        """test to ensure that we no longer use the lang attribure"""
        markup = '''<html lang="en"><head><title>My title</title></head><body></body></html>'''
        pofile = self.html2po(markup)
        self.countunits(pofile, 1)
        # Check that the first item is the <title> not <head>
        self.compareunit(pofile, 1, "My title")

    def test_title(self):
        """test that we can extract the <title> tag"""
        self.check_single("<html><head><title>My title</title></head><body></body></html>", "My title")

    def test_title_with_linebreak(self):
        """Test a linebreak in the <title> tag"""
        htmltext = '''<html>
<head>
  <title>My
title</title>
</head>
<body>
</body>
</html>
'''
        self.check_single(htmltext, "My title")

    def test_meta(self):
        """Test that we can extract certain <meta> info from <head>."""
        self.check_single('''<html><head><meta name="keywords" content="these are keywords"></head><body></body></html>''', "these are keywords")

    def test_tag_p(self):
        """test that we can extract the <p> tag"""
        self.check_single("<html><head></head><body><p>A paragraph.</p></body></html>", "A paragraph.")
        markup = "<p>First line.<br>Second line.</p>"
        pofile = self.html2po(markup)
        self.compareunit(pofile, 1, "First line.<br>Second line.")

    def test_tag_p_with_linebreak(self):
        """Test newlines within the <p> tag."""
        htmltext = '''<html>
<head>
</head>
<body>
<p>
A paragraph is a section in a piece of writing, usually highlighting a
particular point or topic. It always begins on a new line and usually
with indentation, and it consists of at least one sentence.
</p>
</body>
</html>
'''
        self.check_single(htmltext, "A paragraph is a section in a piece of writing, usually highlighting a particular point or topic. It always begins on a new line and usually with indentation, and it consists of at least one sentence.")
        markup = "<p>First\nline.<br>Second\nline.</p>"
        pofile = self.html2po(markup)
        self.compareunit(pofile, 1, "First line.<br>Second line.")

    def test_tag_div(self):
        """test that we can extract the <div> tag"""
        self.check_single("<html><head></head><body><div>A paragraph.</div></body></html>", "A paragraph.")
        markup = "<div>First line.<br>Second line.</div>"
        pofile = self.html2po(markup)
        self.compareunit(pofile, 1, "First line.<br>Second line.")

    def test_tag_div_with_linebreaks(self):
        """Test linebreaks within a <div> tag."""
        htmltext = '''<html>
<head>
</head>
<body>
<div>
A paragraph is a section in a piece of writing, usually highlighting a
particular point or topic. It always begins on a new line and usually
with indentation, and it consists of at least one sentence.
</div>
</body>
</html>
'''
        self.check_single(htmltext, "A paragraph is a section in a piece of writing, usually highlighting a particular point or topic. It always begins on a new line and usually with indentation, and it consists of at least one sentence.")
        markup = "<div>First\nline.<br>Second\nline.</div>"
        pofile = self.html2po(markup)
        self.compareunit(pofile, 1, "First line.<br>Second line.")

    def test_tag_a(self):
        """test that we can extract the <a> tag"""
        self.check_single('<html><head></head><body><p>A paragraph with <a href="http://translate.org.za/">hyperlink</a>.</p></body></html>', 'A paragraph with <a href="http://translate.org.za/">hyperlink</a>.')

    def test_tag_a_with_linebreak(self):
        """Test that we can extract the <a> tag with newlines in it."""
        htmltext = '''<html>
<head>
</head>
<body>
<p>A
paragraph
with <a
href="http://translate.org.za/">hyperlink</a>
and
newlines.</p></body></html>
'''
        self.check_single(htmltext, 'A paragraph with <a href="http://translate.org.za/">hyperlink</a> and newlines.')

    def test_tag_img(self):
        """Test that we can extract the alt attribute from the <img> tag."""
        self.check_single('''<html><head></head><body><img src="picture.png" alt="A picture"></body></html>''', "A picture")

    def test_img_empty(self):
        """Test that we can extract the alt attribute from the <img> tag."""
        htmlsource = '''<html><head></head><body><img src="images/topbar.jpg" width="750" height="80"></body></html>'''
        self.check_null(htmlsource)

    def test_tag_table_summary(self):
        """Test that we can extract the summary attribute."""
        self.check_single('''<html><head></head><body><table summary="Table summary"></table></body></html>''', "Table summary")

    def test_table_simple(self):
        """Test that we can fully extract a simple table."""
        markup = '''<html><head></head><body><table><tr><th>Heading One</th><th>Heading Two</th></tr><tr><td>One</td><td>Two</td></tr></table></body></html>'''
        pofile = self.html2po(markup)
        self.countunits(pofile, 4)
        self.compareunit(pofile, 1, "Heading One")
        self.compareunit(pofile, 2, "Heading Two")
        self.compareunit(pofile, 3, "One")
        self.compareunit(pofile, 4, "Two")

    def test_table_complex(self):
        markup = '''<table summary="This is the summary"><caption>A caption</caption><thead><tr><th abbr="Head 1">Heading One</th><th>Heading Two</th></tr></thead><tfoot><tr><td>Foot One</td><td>Foot Two</td></tr></tfoot><tbody><tr><td>One</td><td>Two</td></tr></tbody></table>'''
        pofile = self.html2po(markup)
        self.countunits(pofile, 9)
        self.compareunit(pofile, 1, "This is the summary")
        self.compareunit(pofile, 2, "A caption")
        self.compareunit(pofile, 3, "Head 1")
        self.compareunit(pofile, 4, "Heading One")
        self.compareunit(pofile, 5, "Heading Two")
        self.compareunit(pofile, 6, "Foot One")
        self.compareunit(pofile, 7, "Foot Two")
        self.compareunit(pofile, 8, "One")
        self.compareunit(pofile, 9, "Two")

    def test_table_empty(self):
        """Test that we ignore tables that are empty.

        A table is deemed empty if it has no translatable content.
        """

        self.check_null('''<html><head></head><body><table><tr><td><img src="bob.png"></td></tr></table></body></html>''')
        self.check_null('''<html><head></head><body><table><tr><td>&nbsp;</td></tr></table></body></html>''')
        self.check_null('''<html><head></head><body><table><tr><td><strong></strong></td></tr></table></body></html>''')

    def test_address(self):
        """Test to see if the address element is extracted"""
        self.check_single("<body><address>My address</address></body>", "My address")

    def test_headings(self):
        """Test to see if the h* elements are extracted"""
        markup = "<html><head></head><body><h1>Heading One</h1><h2>Heading Two</h2><h3>Heading Three</h3><h4>Heading Four</h4><h5>Heading Five</h5><h6>Heading Six</h6></body></html>"
        pofile = self.html2po(markup)
        self.countunits(pofile, 6)
        self.compareunit(pofile, 1, "Heading One")
        self.compareunit(pofile, 2, "Heading Two")
        self.compareunit(pofile, 3, "Heading Three")
        self.compareunit(pofile, 4, "Heading Four")
        self.compareunit(pofile, 5, "Heading Five")
        self.compareunit(pofile, 6, "Heading Six")

    def test_headings_with_linebreaks(self):
        """Test to see if h* elements with newlines can be extracted"""
        markup = "<html><head></head><body><h1>Heading\nOne</h1><h2>Heading\nTwo</h2><h3>Heading\nThree</h3><h4>Heading\nFour</h4><h5>Heading\nFive</h5><h6>Heading\nSix</h6></body></html>"
        pofile = self.html2po(markup)
        self.countunits(pofile, 6)
        self.compareunit(pofile, 1, "Heading One")
        self.compareunit(pofile, 2, "Heading Two")
        self.compareunit(pofile, 3, "Heading Three")
        self.compareunit(pofile, 4, "Heading Four")
        self.compareunit(pofile, 5, "Heading Five")
        self.compareunit(pofile, 6, "Heading Six")

    def test_dt(self):
        """Test to see if the definition list title (dt) element is extracted"""
        self.check_single("<html><head></head><body><dl><dt>Definition List Item Title</dt></dl></body></html>", "Definition List Item Title")

    def test_dd(self):
        """Test to see if the definition list description (dd) element is extracted"""
        self.check_single("<html><head></head><body><dl><dd>Definition List Item Description</dd></dl></body></html>", "Definition List Item Description")

    def test_span(self):
        """test to check that we don't double extract a span item"""
        self.check_single("<html><head></head><body><p>You are a <span>Spanish</span> sentence.</p></body></html>", "You are a <span>Spanish</span> sentence.")

    def test_ul(self):
        """Test to see if the list item <li> is exracted"""
        markup = "<html><head></head><body><ul><li>Unordered One</li><li>Unordered Two</li></ul><ol><li>Ordered One</li><li>Ordered Two</li></ol></body></html>"
        pofile = self.html2po(markup)
        self.countunits(pofile, 4)
        self.compareunit(pofile, 1, "Unordered One")
        self.compareunit(pofile, 2, "Unordered Two")
        self.compareunit(pofile, 3, "Ordered One")
        self.compareunit(pofile, 4, "Ordered Two")

    def test_duplicates(self):
        """check that we use the default style of msgctxt to disambiguate duplicate messages"""
        markup = "<html><head></head><body><p>Duplicate</p><p>Duplicate</p></body></html>"
        pofile = self.html2po(markup)
        self.countunits(pofile, 2)
        # FIXME change this so that we check that the msgctxt is correctly added
        self.compareunit(pofile, 1, "Duplicate")
        self.compareunit(pofile, 2, "Duplicate")

    def test_multiline_reflow(self):
        """check that we reflow multiline content to make it more readable for translators"""
        self.check_single('''<td valign="middle" width="96%"><font class="headingwhite">South
                  Africa</font></td>''', '''South Africa''')

    @mark.xfail(reason="Not Implemented")
    def test_nested_tags(self):
        """check that we can extract items within nested tags"""
        markup = "<div><p>Extract this</p>And this</div>"
        pofile = self.html2po(markup)
        self.countunits(pofile, 2)
        self.compareunit(pofile, 1, "Extract this")
        self.compareunit(pofile, 2, "And this")

    def test_carriage_return(self):
        """Remove carriage returns from files in dos format."""
        htmlsource = '''<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">\r
<html><!-- InstanceBegin template="/Templates/masterpage.dwt" codeOutsideHTMLIsLocked="false" -->\r
<head>\r
<!-- InstanceBeginEditable name="doctitle" -->\r
<link href="fmfi.css" rel="stylesheet" type="text/css">\r
</head>\r
\r
<body>\r
<p>The rapid expansion of telecommunications infrastructure in recent\r
years has helped to bridge the digital divide to a limited extent.</p> \r
</body>\r
<!-- InstanceEnd --></html>\r
'''

        self.check_single(htmlsource, 'The rapid expansion of telecommunications infrastructure in recent years has helped to bridge the digital divide to a limited extent.')

    def test_encoding_latin1(self):
        """Convert HTML input in iso-8859-1 correctly to unicode."""
        htmlsource = '''<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><!-- InstanceBegin template="/Templates/masterpage.dwt" codeOutsideHTMLIsLocked="false" -->
<head>
<!-- InstanceBeginEditable name="doctitle" -->
<title>FMFI - South Africa - CSIR Openphone - Overview</title>
<!-- InstanceEndEditable -->
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="keywords" content="fmfi, first mile, first inch, wireless, rural development, access devices, mobile devices, wifi, connectivity, rural connectivty, ict, low cost, cheap, digital divide, csir, idrc, community">

<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable -->
<link href="../../../fmfi.css" rel="stylesheet" type="text/css">
</head>

<body>
<p>We aim to please \x96 will you aim too, please?</p>
<p>South Africa\x92s language diversity can be challenging.</p>
</body>
</html>
'''
        pofile = self.html2po(htmlsource)

        self.countunits(pofile, 4)
        self.compareunit(pofile, 3, u'We aim to please \x96 will you aim too, please?')
        self.compareunit(pofile, 4, u'South Africa\x92s language diversity can be challenging.')

    def test_strip_html(self):
        """Ensure that unnecessary html is stripped from the resulting unit."""

        htmlsource = '''<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title>FMFI - Contact</title>
</head>
<body>
<table width="100%"  border="0" cellpadding="0" cellspacing="0">
  <tr align="left" valign="top">
    <td width="150" height="556">
      <table width="157" height="100%" border="0" cellspacing="0" id="leftmenubg-color">
      <tr>
          <td align="left" valign="top" height="555">
            <table width="100%" border="0" cellspacing="0" cellpadding="2">
              <tr align="left" valign="top" bgcolor="#660000">
                <td width="4%"><strong></strong></td>
                <td width="96%"><strong><font class="headingwhite">Projects</font></strong></td>
              </tr>
              <tr align="left" valign="top">
                <td valign="middle" width="4%"><img src="images/arrow.gif" width="8" height="8"></td>
                <td width="96%"><a href="index.html">Home Page</a></td>
              </tr>
            </table>
          </td>
      </tr>
      </table>
    </td>
  </tr>
</table>
</body>
</html>
'''
        pofile = self.html2po(htmlsource)
        self.countunits(pofile, 3)
        self.compareunit(pofile, 2, u'Projects')
        self.compareunit(pofile, 3, u'Home Page')

        # Translate and convert back:
        pofile.units[2].target = 'Projekte'
        pofile.units[3].target = 'Tuisblad'
        htmlresult = self.po2html(str(pofile), htmlsource).replace('\n', ' ').replace('= "', '="').replace('> <', '><')
        snippet = '<td width="96%"><strong><font class="headingwhite">Projekte</font></strong></td>'
        assert snippet in htmlresult
        snippet = '<td width="96%"><a href="index.html">Tuisblad</a></td>'
        assert snippet in htmlresult

    @mark.xfail(reason="Performing major HTML surgery")
    def test_php(self):
        """Test that PHP snippets don't interfere"""

        # A simple string
        self.check_phpsnippet('''<?=$phpvariable?>''')

        # Contains HTML tag charcters (< and >)
        self.check_phpsnippet('''<?=($a < $b ? $foo : ($b > c ? $bar : $cat))?>''')

        # Make sure basically any symbol can be handled
        self.check_phpsnippet(''' <? asdfghjkl qwertyuiop 1234567890!@#$%^&*()-=_+[]\{}|;':",./<>? ?> ''')

    def test_multiple_php(self):
        """Test multiple PHP snippets in a string to make sure they get restored properly"""
        php1 = '''<?=$phpvariable?>'''
        php2 = '''<?=($a < $b ? $foo : ($b > c ? $bar : $cat))?>'''
        php3 = '''<? asdfghjklqwertyuiop1234567890!@#$%^&*()-=_+[]\{}|;':",./<>? ?>'''

        # Put 3 different strings into an html string
        innertext = '<a href="' + php1 + '/site.html">Body text</a> and some ' + php2 + ' more text ' + php2 + php3
        htmlsource = '<html><head></head><body><p>' + innertext + '</p></body></html>'
        self.check_single(htmlsource, innertext)

    def test_php_multiline(self):

        # A multi-line php string to test
        php1 = '''<? abc
def
ghi ?>'''

        # Scatter the php strings throughout the file, and show what the translation should be
        innertext = '<a href="' + php1 + '/site.html">Body text</a> and some ' + php1 + ' more text ' + php1 + php1
        innertrans = '<a href="' + php1 + '/site.html">Texte de corps</a> et encore de ' + php1 + ' plus de texte ' + php1 + php1

        htmlsource = '<html><head></head><body><p>' + innertext + '</p></body></html>'  # Current html file
        transsource = '<html><head></head><body><p>' + innertrans + '</p></body></html>'  # Expected translation

        pofile = self.html2po(htmlsource)
        pofile.units[1].target = innertrans  # Register the translation in the PO file
        htmlresult = self.po2html(pofile, htmlsource)
        assert htmlresult == transsource

    def test_comments(self):
        """Test that HTML comments are converted to translator notes in output"""
        pofile = self.html2po('<!-- comment outside block --><p><!-- a comment -->A paragraph<!-- with another comment -->.</p>', keepcomments=True)
        self.compareunit(pofile, 1, 'A paragraph.')
        notes = pofile.getunits()[-1].getnotes()
        assert unicode(notes) == ' a comment \n with another comment '


class TestHTML2POCommand(test_convert.TestConvertCommand, TestHTML2PO):
    """Tests running actual html2po commands on files"""
    convertmodule = html2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")
        options = self.help_check(options, "--keepcomments")
        options = self.help_check(options, "-u, --untagged", last=True)

########NEW FILE########
__FILENAME__ = test_json2po
#!/usr/bin/env python

from translate.convert import json2po, test_convert
from translate.misc import wStringIO
from translate.storage import jsonl10n


class TestJson2PO:

    def json2po(self, jsonsource, template=None, filter=None):
        """helper that converts json source to po source without requiring files"""
        inputfile = wStringIO.StringIO(jsonsource)
        inputjson = jsonl10n.JsonFile(inputfile, filter=filter)
        convertor = json2po.json2po()
        outputpo = convertor.convert_store(inputjson)
        return outputpo

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        print(str(storage))
        assert len(storage.units) == 1
        return storage.units[0]

    def test_simple(self):
        """test the most basic json conversion"""
        jsonsource = '''{ "text": "A simple string"}'''
        poexpected = '''#: .text
msgid "A simple string"
msgstr ""
'''
        poresult = self.json2po(jsonsource)
        assert str(poresult.units[1]) == poexpected

    def test_filter(self):
        """test basic json conversion with filter option"""
        jsonsource = '''{ "text": "A simple string", "number": 42 }'''
        poexpected = '''#: .text
msgid "A simple string"
msgstr ""
'''
        poresult = self.json2po(jsonsource, filter=["text"])
        assert str(poresult.units[1]) == poexpected

    def test_miltiple_units(self):
        """test that we can handle json with multiple units"""
        jsonsource = '''
{
     "name": "John",
     "surname": "Smith",
     "address":
     {
         "streetAddress": "Koeistraat 21",
         "city": "Pretoria",
         "country": "South Africa",
         "postalCode": "10021"
     },
     "phoneNumber":
     [
         {
           "type": "home",
           "number": "012 345-6789"
         },
         {
           "type": "fax",
           "number": "012 345-6788"
         }
     ]
 }
 '''

        poresult = self.json2po(jsonsource)
        assert poresult.units[0].isheader()
        print(len(poresult.units))
        assert len(poresult.units) == 11


class TestJson2POCommand(test_convert.TestConvertCommand, TestJson2PO):
    """Tests running actual json2po commands on files"""
    convertmodule = json2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--duplicates")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--filter", last=True)

########NEW FILE########
__FILENAME__ = test_moz2po
#!/usr/bin/env python

from translate.convert import moz2po, test_convert


class TestMoz2PO:
    pass


class TestMoz2POCommand(test_convert.TestConvertCommand, TestMoz2PO):
    """Tests running actual moz2po commands on files"""
    convertmodule = moz2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE", last=True)

########NEW FILE########
__FILENAME__ = test_mozfunny2prop
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import mozfunny2prop
from translate.misc import wStringIO
from translate.storage import po


class TestInc2PO:

    def inc2po(self, incsource, inctemplate=None):
        """helper that converts .inc source to po source without requiring files"""
        inputfile = wStringIO.StringIO(incsource)
        if inctemplate:
            templatefile = wStringIO.StringIO(inctemplate)
        else:
            templatefile = None
        outputfile = wStringIO.StringIO()
        result = mozfunny2prop.inc2po(inputfile, outputfile, templatefile)
        outputpo = outputfile.getvalue()
        outputpofile = po.pofile(wStringIO.StringIO(outputpo))
        return outputpofile

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile)
        return pofile.units[1]

    def countelements(self, pofile):
        """counts the number of non-header entries"""
        assert pofile.units[0].isheader()
        print(pofile)
        return len(pofile.units) - 1

    def test_simpleentry(self):
        """checks that a simple inc entry converts properly to a po entry"""
        incsource = '#define MOZ_LANGPACK_CREATOR mozilla.org\n'
        pofile = self.inc2po(incsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["MOZ_LANGPACK_CREATOR"]
        assert pounit.source == "mozilla.org"
        assert pounit.target == ""

    def test_uncomment_contributors(self):
        """checks that the contributors entry is automatically uncommented"""
        incsource = '''# If non-English locales wish to credit multiple contributors, uncomment this
# variable definition and use the format specified.
# #define MOZ_LANGPACK_CONTRIBUTORS <em:contributor>Joe Solon</em:contributor> <em:contributor>Suzy Solon</em:contributor>'''
        pofile = self.inc2po(incsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["MOZ_LANGPACK_CONTRIBUTORS"]
        assert "Joe Solon" in pounit.source

########NEW FILE########
__FILENAME__ = test_mozlang2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import mozlang2po, test_convert
from translate.misc import wStringIO
from translate.storage import mozilla_lang as lang


class TestLang2PO:

    def lang2po(self, source):
        """helper that converts .lang source to po source without requiring files"""
        inputfile = wStringIO.StringIO(source)
        inputlang = lang.LangStore(inputfile)
        convertor = mozlang2po.lang2po()
        outputpo = convertor.convertstore(inputlang)
        return outputpo

    def convertlng(self, source):
        """call the convertlng, return the outputfile"""
        inputfile = wStringIO.StringIO(source)
        outputfile = wStringIO.StringIO()
        templatefile = None
        assert lang2po.convertlang(inputfile, outputfile, templatefile)
        return outputfile.getvalue()

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile)
        return pofile.units[1]

    def countelements(self, pofile):
        """counts the number of non-header entries"""
        assert pofile.units[0].isheader()
        print(pofile)
        return len(pofile.units) - 1

    def test_simpleentry(self):
        """checks that a simple lang entry converts properly to a po entry"""
        source = ';One\nEen\n'
        pofile = self.lang2po(source)
        pounit = self.singleelement(pofile)
        assert pounit.source == "One"
        assert pounit.target == "Een"

    def test_simpleentry(self):
        """Handle simple comments"""
        source = '# Comment\n;One\nEen\n'
        pofile = self.lang2po(source)
        pounit = self.singleelement(pofile)
        assert pounit.source == "One"
        assert pounit.target == "Een"
        assert pounit.getnotes() == "Comment"


class TestLang2POCommand(test_convert.TestConvertCommand, TestLang2PO):
    """Tests running actual lang2po commands on files"""
    convertmodule = mozlang2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--encoding=ENCODING")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE", last=True)

########NEW FILE########
__FILENAME__ = test_oo2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import urlparse
from urlparse import parse_qs

from translate.convert import oo2po, po2oo, test_convert
from translate.misc import wStringIO
from translate.storage import oo, po
from translate.storage.poheader import poheader


class TestOO2PO:
    target_filetype = po.pofile
    conversion_module = oo2po
    conversion_class = oo2po.oo2po

    def convert(self, oosource, sourcelanguage='en-US', targetlanguage='af-ZA'):
        """helper that converts oo source to po source without requiring files"""
        inputoo = oo.oofile(oosource)
        convertor = self.conversion_class(sourcelanguage, targetlanguage)
        outputpo = convertor.convertstore(inputoo)
        return outputpo

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        if isinstance(pofile, poheader):
            assert len(pofile.units) == 2
            assert pofile.units[0].isheader()
            return pofile.units[1]
        else:
            assert len(pofile.units) == 1
            return pofile.units[0]

    def roundtripstring(self, filename, entitystring):
        """Convert the supplied string as part of an OpenOffice.org GSI file to po and back.

        Return the string once it has been through all the conversions."""

        ootemplate = r'helpcontent2	%s	0	help	par_id3150670 35				0	en-US	%s				2002-02-02 02:02:02'

        oosource = ootemplate % (filename, entitystring)
        ooinputfile = wStringIO.StringIO(oosource)
        ootemplatefile = wStringIO.StringIO(oosource)
        pooutputfile = wStringIO.StringIO()

        self.conversion_module.convertoo(ooinputfile, pooutputfile, ootemplatefile, targetlanguage='en-US')
        posource = pooutputfile.getvalue()

        poinputfile = wStringIO.StringIO(posource)
        ootemplatefile = wStringIO.StringIO(oosource)
        oooutputfile = wStringIO.StringIO()
        po2oo.convertoo(poinputfile, oooutputfile, ootemplatefile, targetlanguage="en-US")
        ooresult = oooutputfile.getvalue()
        print("original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult)
        return ooresult.split('\t')[10]

    def check_roundtrip(self, filename, text):
        """Checks that the text converted to po and back is the same as the original."""
        assert self.roundtripstring(filename, text) == text

    def test_simpleentity(self):
        """checks that a simple oo entry converts properly to a po entry"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        pofile = self.convert(oosource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "Character"
        assert pounit.target == ""

    def test_escapes(self):
        """checks that a simple oo entry converts escapes properly to a po entry"""
        oosource = r"wizards	source\formwizard\dbwizres.src	0	string	RID_DB_FORM_WIZARD_START + 19				0	en-US	Newline \n Newline Tab \t Tab CR \r CR				20050924 09:13:58"
        pofile = self.convert(oosource)
        pounit = self.singleelement(pofile)
        poelementsrc = str(pounit)
        print(poelementsrc)
        assert "Newline \n Newline" in pounit.source
        assert "Tab \t Tab" in pounit.source
        assert "CR \r CR" in pounit.source

    def test_roundtrip_escape(self):
        self.check_roundtrip('strings.src', r'The given command is not a SELECT statement.\nOnly queries are allowed.')
        self.check_roundtrip('source\\ui\\dlg\\AutoControls_tmpl.hrc', r';\t59\t,\t44\t:\t58\t{Tab}\t9\t{Space}\t32')
        self.check_roundtrip('inc_openoffice\\windows\\msi_languages\\Nsis.ulf', r'The installation files must be unpacked and copied to your hard disk in preparation for the installation. After that, the %PRODUCTNAME installation will start automatically.\r\n\r\nClick \'Next\' to continue.')
        self.check_roundtrip('file.xhp', r'\<ahelp\>')
        self.check_roundtrip('file.xhp', r'\<ahelp prop=\"value\"\>')
        self.check_roundtrip('file.xhp', r'\<ahelp prop=\"value\"\>marked up text\</ahelp\>')
        self.check_roundtrip('file.xhp', r'\<ahelp prop=\"value>>\"\>')
        self.check_roundtrip('file.xhp', r'''\<ahelp prop=\"value>>\"\>'Next'>> or "<<Previous"\</ahelp\>''')
        self.check_roundtrip('address_auto.xhp', r'''example, \<item type=\"literal\"\>'Harry\\'s Bar'.\</item\>''')

    def test_roundtrip_whitespaceonly(self):
        """check items that are only special instances of whitespce"""
        self.check_roundtrip('choose_chart_type.xhp', r' ')
        self.check_roundtrip('choose_chart_type.xhp', '\xc2\xa0')

    def test_double_escapes(self):
        oosource = r"helpcontent2	source\text\shared\01\02100001.xhp	0	help	par_id3150670 35				0	en-US	\\<				2002-02-02 02:02:02"
        pofile = self.convert(oosource)
        pounit = self.singleelement(pofile)
        poelementsrc = str(pounit)
        print(poelementsrc)
        assert pounit.source == r"\<"

    def test_escapes_helpcontent2(self):
        """checks that a helpcontent2 entry converts escapes properly to a po entry"""
        oosource = r"helpcontent2	source\text\smath\guide\parentheses.xhp	0	help	par_id3150344	4			0	en-US	size *2 \\langle x \\rangle				2002-02-02 02:02:02"
        pofile = self.convert(oosource)
        pounit = self.singleelement(pofile)
        poelementsrc = str(pounit)
        print(poelementsrc)
        assert pounit.source == r'size *2 \langle x \rangle'

    def test_msgid_bug_error_address(self):
        """tests the we have the correct url for reporting msgid bugs"""
        oosource = r"wizards	source\formwizard\dbwizres.src	0	string	RID_DB_FORM_WIZARD_START + 19				0	en-US	Newline \n Newline Tab \t Tab CR \r CR				20050924 09:13:58"
        pofile = self.convert(oosource)
        assert pofile.units[0].isheader()
        assert pofile.parseheader()["Report-Msgid-Bugs-To"]
        bug_url = urlparse.urlparse(pofile.parseheader()["Report-Msgid-Bugs-To"])
        print(bug_url)
        assert bug_url[:3] == ("http", "qa.openoffice.org", "/issues/enter_bug.cgi")
        assert parse_qs(bug_url[4], True) == {u'comment': [u''],
                                              u'component': [u'l10n'],
                                              u'form_name': [u'enter_issue'],
                                              u'short_desc': [u'Localization issue in file: '],
                                              u'subcomponent': [u'ui'],}

    def test_x_comment_inclusion(self):
        """test that we can merge x-comment language entries into comment sections of the PO file"""
        en_USsource = r"wizards	source\formwizard\dbwizres.src	0	string	RID_DB_FORM_WIZARD_START + 19				0	en-US	Text		Quickhelp	Title	20050924 09:13:58"
        xcommentsource = r"wizards	source\formwizard\dbwizres.src	0	string	RID_DB_FORM_WIZARD_START + 19				0	x-comment	%s		%s	%s	20050924 09:13:58"
        # Real comment
        comment = "Comment"
        commentsource = en_USsource + '\n' + xcommentsource % (comment, comment, comment)
        pofile = self.convert(commentsource)
        if isinstance(pofile, poheader):
            units = pofile.units[1:]
        else:
            units = pofile.units
        textunit = units[0]
        assert textunit.source == "Text"
        assert comment in textunit.getnotes("developer")
        quickhelpunit = units[1]
        assert quickhelpunit.source == "Quickhelp"
        assert comment in quickhelpunit.getnotes("developer")
        titleunit = units[2]
        assert titleunit.source == "Title"
        assert comment in titleunit.getnotes("developer")
        # Whitespace and blank
        for comment in ("   ", ""):
            commentsource = en_USsource + '\n' + xcommentsource % (comment, comment, comment)
            pofile = self.convert(commentsource)
            if isinstance(pofile, poheader):
                units = pofile.units[1:]
            else:
                units = pofile.units
            textunit = units[0]
            assert textunit.source == "Text"
            assert textunit.getnotes("developer") == ""
            quickhelpunit = units[1]
            assert quickhelpunit.source == "Quickhelp"
            assert quickhelpunit.getnotes("developer") == ""
            titleunit = units[2]
            assert titleunit.source == "Title"
            assert titleunit.getnotes("developer") == ""


class TestOO2POCommand(test_convert.TestConvertCommand, TestOO2PO):
    """Tests running actual oo2po commands on files"""
    convertmodule = oo2po

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--source-language=LANG")
        options = self.help_check(options, "--language=LANG")
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")
        options = self.help_check(options, "--multifile=MULTIFILESTYLE")
        options = self.help_check(options, "--nonrecursiveinput", last=True)

    def test_preserve_filename(self):
        """Ensures that the filename is preserved."""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("snippet.sdf", oosource)
        oofile = oo.oofile(self.open_testfile("snippet.sdf"))
        assert oofile.filename.endswith("snippet.sdf")
        oofile.parse(oosource)
        assert oofile.filename.endswith("snippet.sdf")

    def test_simple_pot(self):
        """tests the simplest possible conversion to a pot file"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource)
        self.run_command("simple.oo", "simple.pot", pot=True, nonrecursiveinput=True)
        pofile = self.target_filetype(self.open_testfile("simple.pot"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "Character"
        assert poelement.target == ""

    def test_simple_po(self):
        """tests the simplest possible conversion to a po file"""
        oosource1 = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        oosource2 = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	ku	Karakter				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource1 + "\n" + oosource2)
        self.run_command("simple.oo", "simple.po", lang="ku", nonrecursiveinput=True)
        pofile = self.target_filetype(self.open_testfile("simple.po"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "Character"
        assert poelement.target == "Karakter"

    def test_onefile_nonrecursive(self):
        """tests the --multifile=onefile option and make sure it doesn't produce a directory"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource)
        self.run_command("simple.oo", "simple.pot", pot=True, multifile="onefile")
        assert os.path.isfile(self.get_testfilename("simple.pot"))

    def test_remove_duplicates(self):
        """test that removing of duplicates works correctly (bug 171)"""
        oosource = r'''
sd	source\ui\animations\SlideTransitionPane.src	0	checkbox	DLG_SLIDE_TRANSITION_PANE	CB_AUTO_PREVIEW	HID_SD_SLIDETRANSITIONPANE_CB_AUTO_PREVIEW		1	en-US	Automatic preview				20060725 03:26:42
sd	source\ui\animations\AnimationSchemesPane.src	0	checkbox	DLG_ANIMATION_SCHEMES_PANE	CB_AUTO_PREVIEW	HID_SD_ANIMATIONSCHEMESPANE_CB_AUTO_PREVIEW		1	en-US	Automatic preview				20060725 03:26:42
sd	source\ui\animations\CustomAnimationCreateDialog.src	0	checkbox	RID_TP_CUSTOMANIMATION_ENTRANCE	CBX_PREVIEW			143	en-US	Automatic preview				20060725 03:26:42
sd	source\ui\animations\CustomAnimationCreateDialog.src	0	checkbox	RID_TP_CUSTOMANIMATION_ENTRANCE	CBX_PREVIEW			143	fr	Aperu automatique				20060725 03:26:42
sd	source\ui\animations\CustomAnimationSchemesPane.src	0	checkbox	DLG_CUSTOMANIMATION_SCHEMES_PANE	4			0	en-US	Automatic preview				20060725 03:26:42
sd	source\ui\animations\CustomAnimationSchemesPane.src	0	checkbox	DLG_CUSTOMANIMATION_SCHEMES_PANE	4			0	fr	Aperu automatique				20060725 03:26:42
'''
        self.create_testfile("simple.oo", oosource)
        self.run_command("simple.oo", "simple.po", language="fr", multifile="onefile", error="traceback", duplicates="merge")
        pofile = self.target_filetype(self.open_testfile("simple.po"))
        assert len(pofile.units) == 2
        assert pofile.units[1].target == u"Aperu automatique"

########NEW FILE########
__FILENAME__ = test_oo2xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os

from translate.convert import oo2xliff, test_convert, test_oo2po
from translate.storage import oo, xliff


class TestOO2XLIFF(test_oo2po.TestOO2PO):
    target_filetype = xliff.xlifffile
    conversion_module = oo2xliff
    conversion_class = oo2xliff.oo2xliff

    def test_msgid_bug_error_address(self):
        pass


class TestOO2POCommand(test_convert.TestConvertCommand, TestOO2XLIFF):
    """Tests running actual oo2xliff commands on files"""
    convertmodule = oo2xliff

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--source-language=LANG")
        options = self.help_check(options, "--language=LANG")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")
        options = self.help_check(options, "--multifile=MULTIFILESTYLE")
        options = self.help_check(options, "--nonrecursiveinput", last=True)

    def test_preserve_filename(self):
        """Ensures that the filename is preserved."""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("snippet.sdf", oosource)
        oofile = oo.oofile(self.open_testfile("snippet.sdf"))
        assert oofile.filename.endswith("snippet.sdf")
        oofile.parse(oosource)
        assert oofile.filename.endswith("snippet.sdf")

    def test_simple_xlf(self):
        """tests the simplest possible conversion to a xlf file"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource)
        self.run_command("simple.oo", "simple.xlf", lang="ku", nonrecursiveinput=True)
        pofile = self.target_filetype(self.open_testfile("simple.xlf"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "Character"
        assert poelement.target == ""

    def test_simple_po(self):
        """tests the simplest possible conversion to a po file"""
        oosource1 = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        oosource2 = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	ku	Karakter				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource1 + "\n" + oosource2)
        self.run_command("simple.oo", "simple.po", lang="ku", nonrecursiveinput=True)
        pofile = self.target_filetype(self.open_testfile("simple.po"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "Character"
        assert poelement.target == "Karakter"

    def test_onefile_nonrecursive(self):
        """tests the --multifile=onefile option and make sure it doesn't produce a directory"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        self.create_testfile("simple.oo", oosource)
        self.run_command("simple.oo", "simple.xlf", lang="ku", multifile="onefile")
        assert os.path.isfile(self.get_testfilename("simple.xlf"))

########NEW FILE########
__FILENAME__ = test_php2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import php2po, test_convert
from translate.misc import wStringIO
from translate.storage import php, po


class TestPhp2PO:

    def php2po(self, phpsource, phptemplate=None):
        """helper that converts .php source to po source without requiring files"""
        inputfile = wStringIO.StringIO(phpsource)
        inputphp = php.phpfile(inputfile)
        convertor = php2po.php2po()
        if phptemplate:
            templatefile = wStringIO.StringIO(phptemplate)
            templatephp = php.phpfile(templatefile)
            outputpo = convertor.mergestore(templatephp, inputphp)
        else:
            outputpo = convertor.convertstore(inputphp)
        return outputpo

    def convertphp(self, phpsource):
        """call the convertphp, return the outputfile"""
        inputfile = wStringIO.StringIO(phpsource)
        outputfile = wStringIO.StringIO()
        templatefile = None
        assert php2po.convertphp(inputfile, outputfile, templatefile)
        return outputfile.getvalue()

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile)
        return pofile.units[1]

    def countelements(self, pofile):
        """counts the number of non-header entries"""
        assert pofile.units[0].isheader()
        print(pofile)
        return len(pofile.units) - 1

    def test_simpleentry(self):
        """checks that a simple php entry converts properly to a po entry"""
        phpsource = """$_LANG['simple'] = 'entry';"""
        pofile = self.php2po(phpsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "entry"
        assert pounit.target == ""

    def test_convertphp(self):
        """checks that the convertphp function is working"""
        phpsource = """$_LANG['simple'] = 'entry';"""
        posource = self.convertphp(phpsource)
        pofile = po.pofile(wStringIO.StringIO(posource))
        pounit = self.singleelement(pofile)
        assert pounit.source == "entry"
        assert pounit.target == ""

    def test_unicode(self):
        """checks that unicode entries convert properly"""
        unistring = u'Norsk bokm\u00E5l'
        phpsource = """$lang['nb'] = '%s';""" % unistring
        pofile = self.php2po(phpsource)
        pounit = self.singleelement(pofile)
        print(repr(pofile.units[0].target))
        print(repr(pounit.source))
        assert pounit.source == u'Norsk bokm\u00E5l'

    def test_multiline(self):
        """checks that multiline enties can be parsed"""
        phpsource = r"""$lang['5093'] = 'Unable to connect to your IMAP server. You may have exceeded the maximum number
of connections to this server. If so, use the Advanced IMAP Server Settings dialog to
reduce the number of cached connections.';"""
        pofile = self.php2po(phpsource)
        print(repr(pofile.units[1].target))
        assert self.countelements(pofile) == 1

    def test_comments_before(self):
        """test to ensure that we take comments from .php and place them in .po"""
        phpsource = '''/* Comment */
$lang['prefPanel-smime'] = 'Security';'''
        pofile = self.php2po(phpsource)
        pounit = self.singleelement(pofile)
        assert pounit.getnotes("developer") == "/* Comment */"
        # TODO write test for inline comments and check for // comments that precede an entry

    def test_emptyentry(self):
        """checks that empty definitions survives into po file"""
        phpsource = '''/* comment */\n$lang['credit'] = '';'''
        pofile = self.php2po(phpsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["$lang['credit']"]
        assert pounit.getcontext() == "$lang['credit']"
        assert "#. /* comment" in str(pofile)
        assert pounit.source == ""

    def test_hash_comment_with_equals(self):
        """Check that a # comment with = in it doesn't confuse us. Bug 1298."""
        phpsource = '''# inside alt= stuffies\n$variable = 'stringy';'''
        pofile = self.php2po(phpsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["$variable"]
        assert "#. # inside alt= stuffies" in str(pofile)
        assert pounit.source == "stringy"

    def test_emptyentry_translated(self):
        """checks that if we translate an empty definition it makes it into the PO"""
        phptemplate = '''$lang['credit'] = '';'''
        phpsource = '''$lang['credit'] = 'Translators Names';'''
        pofile = self.php2po(phpsource, phptemplate)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["$lang['credit']"]
        assert pounit.source == ""
        assert pounit.target == "Translators Names"

    def test_newlines_in_value(self):
        """check that we can carry newlines that appear in the entry value into the PO"""
        # Single quotes - \n is not a newline
        phpsource = r'''$lang['name'] = 'value1\nvalue2';'''
        pofile = self.php2po(phpsource)
        unit = self.singleelement(pofile)
        assert unit.source == r"value1\nvalue2"
        # Double quotes - \n is a newline
        phpsource = r'''$lang['name'] = "value1\nvalue2";'''
        pofile = self.php2po(phpsource)
        unit = self.singleelement(pofile)
        assert unit.source == "value1\nvalue2"

    def test_spaces_in_name(self):
        """checks that if we have spaces in the name we create a good PO with no spaces"""
        phptemplate = '''$lang[ 'credit' ] = 'Something';'''
        phpsource = '''$lang[ 'credit' ] = ''n Ding';'''
        pofile = self.php2po(phpsource, phptemplate)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations() == ["$lang[ 'credit' ]"]


class TestPhp2POCommand(test_convert.TestConvertCommand, TestPhp2PO):
    """Tests running actual php2po commands on files"""
    convertmodule = php2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE", last=True)

########NEW FILE########
__FILENAME__ = test_po2csv
#!/usr/bin/env python

from translate.convert import csv2po, po2csv, test_convert
from translate.misc import wStringIO
from translate.storage import csvl10n, po
from translate.storage.test_base import first_translatable, headerless_len


class TestPO2CSV:

    def po2csv(self, posource):
        """helper that converts po source to csv source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2csv.po2csv()
        outputcsv = convertor.convertstore(inputpo)
        return outputcsv

    def csv2po(self, csvsource, template=None):
        """helper that converts csv source to po source without requiring files"""
        inputfile = wStringIO.StringIO(csvsource)
        inputcsv = csvl10n.csvfile(inputfile)
        if template:
            templatefile = wStringIO.StringIO(template)
            inputpot = po.pofile(templatefile)
        else:
            inputpot = None
        convertor = csv2po.csv2po(templatepo=inputpot)
        outputpo = convertor.convertstore(inputcsv)
        return outputpo

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert headerless_len(storage.units) == 1
        return first_translatable(storage)

    def test_simpleentity(self):
        """checks that a simple csv entry definition converts properly to a po entry"""
        minipo = r'''#: term.cpp
msgid "Term"
msgstr "asdf"'''
        csvfile = self.po2csv(minipo)
        unit = self.singleelement(csvfile)
        assert unit.location == "term.cpp"
        assert unit.source == "Term"
        assert unit.target == "asdf"

    def test_multiline(self):
        """tests multiline po entries"""
        minipo = r'''msgid "First part "
"and extra"
msgstr "Eerste deel "
"en ekstra"'''
        csvfile = self.po2csv(minipo)
        unit = self.singleelement(csvfile)
        assert unit.source == "First part and extra"
        assert unit.target == "Eerste deel en ekstra"

    def test_escapednewlines(self):
        """Test the escaping of newlines"""
        minipo = r'''msgid "First line\nSecond line"
msgstr "Eerste lyn\nTweede lyn"
'''
        csvfile = self.po2csv(minipo)
        unit = self.singleelement(csvfile)
        assert unit.source == "First line\nSecond line"
        assert unit.target == "Eerste lyn\nTweede lyn"
        pofile = self.csv2po(str(csvfile))
        unit = self.singleelement(pofile)
        assert unit.source == "First line\nSecond line"
        assert unit.target == "Eerste lyn\nTweede lyn"

    def test_escapedtabs(self):
        """Test the escaping of tabs"""
        minipo = r'''msgid "First column\tSecond column"
msgstr "Eerste kolom\tTweede kolom"
'''
        csvfile = self.po2csv(minipo)
        unit = self.singleelement(csvfile)
        assert unit.source == "First column\tSecond column"
        assert unit.target == "Eerste kolom\tTweede kolom"
        assert csvfile.findunit("First column\tSecond column").target == "Eerste kolom\tTweede kolom"

    def test_escapedquotes(self):
        """Test the escaping of quotes (and slash)"""
        minipo = r'''msgid "Hello \"Everyone\""
msgstr "Good day \"All\""

msgid "Use \\\"."
msgstr "Gebruik \\\"."
'''
        csvfile = self.po2csv(minipo)
        assert csvfile.findunit('Hello "Everyone"').target == 'Good day "All"'
        assert csvfile.findunit('Use \\".').target == 'Gebruik \\".'

    def test_escapedescape(self):
        """Test the escaping of pure escapes is unaffected"""
        minipo = r'''msgid "Find\\Options"
msgstr "Vind\\Opsies"
'''
        csvfile = self.po2csv(minipo)
        print(minipo)
        print(csvfile)
        assert csvfile.findunit(r'Find\Options').target == r'Vind\Opsies'

    def test_singlequotes(self):
        """Tests that single quotes are preserved correctly"""
        minipo = '''msgid "source 'source'"\nmsgstr "target 'target'"\n'''
        csvfile = self.po2csv(minipo)
        print(str(csvfile))
        assert csvfile.findunit("source 'source'").target == "target 'target'"
        # Make sure we don't mess with start quotes until writing
        minipo = '''msgid "'source'"\nmsgstr "'target'"\n'''
        csvfile = self.po2csv(minipo)
        print(str(csvfile))
        assert csvfile.findunit(r"'source'").target == r"'target'"
        # TODO check that we escape on writing not in the internal representation

    def test_empties(self):
        """Tests that things keep working with empty entries"""
        minipo = 'msgid "Source"\nmsgstr ""\n\nmsgid ""\nmsgstr ""'
        csvfile = self.po2csv(minipo)
        assert csvfile.findunit("Source") is not None
        assert csvfile.findunit("Source").target == ""
        assert headerless_len(csvfile.units) == 1

    def test_kdecomments(self):
        """test that we don't carry KDE comments to CSV"""
        minipo = '#: simple.c\nmsgid "_: KDE comment\\n"\n"Same"\nmsgstr "Same"\n'
        csvfile = self.po2csv(minipo)
        unit = self.singleelement(csvfile)
        assert unit.source == "Same"
        assert unit.target == "Same"


class TestPO2CSVCommand(test_convert.TestConvertCommand, TestPO2CSV):
    """Tests running actual po2csv commands on files"""
    convertmodule = po2csv

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--columnorder=COLUMNORDER", last=True)

########NEW FILE########
__FILENAME__ = test_po2dtd
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import warnings

import pytest

from translate.convert import dtd2po, po2dtd, test_convert
from translate.misc import wStringIO
from translate.storage import dtd, po


class TestPO2DTD:

    def setup_method(self, method):
        warnings.resetwarnings()

    def teardown_method(self, method):
        warnings.resetwarnings()

    def po2dtd(self, posource, remove_untranslated=False):
        """helper that converts po source to dtd source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2dtd.po2dtd(remove_untranslated=remove_untranslated)
        outputdtd = convertor.convertstore(inputpo)
        return outputdtd

    def merge2dtd(self, dtdsource, posource):
        """helper that merges po translations to dtd source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(dtdsource)
        templatedtd = dtd.dtdfile(templatefile)
        convertor = po2dtd.redtd(templatedtd)
        outputdtd = convertor.convertstore(inputpo)
        return outputdtd

    def convertdtd(self, posource, dtdtemplate, remove_untranslated=False):
        """helper to exercise the command line function"""
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(dtdtemplate)
        assert po2dtd.convertdtd(inputfile, outputfile, templatefile,
                                 remove_untranslated=remove_untranslated)
        return outputfile.getvalue()

    def roundtripsource(self, dtdsource):
        """converts dtd source to po and back again, returning the resulting source"""
        dtdinputfile = wStringIO.StringIO(dtdsource)
        dtdinputfile2 = wStringIO.StringIO(dtdsource)
        pooutputfile = wStringIO.StringIO()
        dtd2po.convertdtd(dtdinputfile, pooutputfile, dtdinputfile2)
        posource = pooutputfile.getvalue()
        poinputfile = wStringIO.StringIO(posource)
        dtdtemplatefile = wStringIO.StringIO(dtdsource)
        dtdoutputfile = wStringIO.StringIO()
        po2dtd.convertdtd(poinputfile, dtdoutputfile, dtdtemplatefile)
        dtdresult = dtdoutputfile.getvalue()
        print_string = "Original DTD:\n%s\n\nPO version:\n%s\n\n"
        print_string = print_string + "Output DTD:\n%s\n################"
        print(print_string % (dtdsource, posource, dtdresult))
        return dtdresult

    def roundtripstring(self, entitystring):
        """Just takes the contents of a ENTITY definition (with quotes) and does a roundtrip on that"""
        dtdintro, dtdoutro = '<!ENTITY Test.RoundTrip ', '>\n'
        dtdsource = dtdintro + entitystring + dtdoutro
        dtdresult = self.roundtripsource(dtdsource)
        assert dtdresult.startswith(dtdintro) and dtdresult.endswith(dtdoutro)
        return dtdresult[len(dtdintro):-len(dtdoutro)]

    def check_roundtrip(self, dtdsource, dtdcompare=None):
        """Checks that the round-tripped string is the same as dtdcompare.

        If no dtdcompare string is provided then the round-tripped string is
        compared with the original string.

        The reason why sometimes another string is provided to compare with the
        resulting string from the roundtrip is that if the original string
        contains some characters, like " character, or escapes like &quot;,
        then when the roundtrip is performed those characters or escapes are
        escaped, rendering a round-tripped string which differs from the
        original one.
        """
        if not dtdcompare:
            dtdcompare = dtdsource
        assert self.roundtripstring(dtdsource) == dtdcompare

    def test_joinlines(self):
        """tests that po lines are joined seamlessly (bug 16)"""
        multilinepo = '''#: pref.menuPath\nmsgid ""\n"<span>Tools &gt; Options</"\n"span>"\nmsgstr ""\n'''
        dtdfile = self.po2dtd(multilinepo)
        dtdsource = str(dtdfile)
        assert "</span>" in dtdsource

    def test_escapedstr(self):
        """tests that \n in msgstr is escaped correctly in dtd"""
        multilinepo = '''#: pref.menuPath\nmsgid "Hello\\nEveryone"\nmsgstr "Good day\\nAll"\n'''
        dtdfile = self.po2dtd(multilinepo)
        dtdsource = str(dtdfile)
        assert "Good day\nAll" in dtdsource

    def test_missingaccesskey(self):
        """tests that proper warnings are given if access key is missing"""
        simplepo = '''#: simple.label
#: simple.accesskey
msgid "Simple &String"
msgstr "Dimpled Ring"
'''
        simpledtd = '''<!ENTITY simple.label "Simple String">
<!ENTITY simple.accesskey "S">'''
        warnings.simplefilter("error")
        assert pytest.raises(Warning, self.merge2dtd, simpledtd, simplepo)

    def test_accesskeycase(self):
        """tests that access keys come out with the same case as the original, regardless"""
        simplepo_template = '''#: simple.label\n#: simple.accesskey\nmsgid "%s"\nmsgstr "%s"\n'''
        simpledtd_template = '''<!ENTITY simple.label "Simple %s">\n<!ENTITY simple.accesskey "%s">'''
        possibilities = [
                #(en label, en akey, en po, af po, af label, expected af akey)
                ("Sis", "S", "&Sis", "&Sies", "Sies", "S"),
                ("Sis", "s", "Si&s", "&Sies", "Sies", "S"),
                ("Sis", "S", "&Sis", "Sie&s", "Sies", "s"),
                ("Sis", "s", "Si&s", "Sie&s", "Sies", "s"),
                # untranslated strings should have the casing of the source
                ("Sis", "S", "&Sis", "", "Sis", "S"),
                ("Sis", "s", "Si&s", "", "Sis", "s"),
                ("Suck", "S", "&Suck", "", "Suck", "S"),
                ("Suck", "s", "&Suck", "", "Suck", "s"),
                ]
        for (en_label, en_akey, po_source, po_target, target_label, target_akey) in possibilities:
            simplepo = simplepo_template % (po_source, po_target)
            simpledtd = simpledtd_template % (en_label, en_akey)
            dtdfile = self.merge2dtd(simpledtd, simplepo)
            dtdfile.makeindex()
            accel = dtd.unquotefromdtd(dtdfile.id_index["simple.accesskey"].definition)
            assert accel == target_akey

    def test_accesskey_types(self):
        """tests that we can detect the various styles of accesskey"""
        simplepo_template = '''#: simple.%s\n#: simple.%s\nmsgid "&File"\nmsgstr "F&aele"\n'''
        simpledtd_template = '''<!ENTITY simple.%s "File">\n<!ENTITY simple.%s "a">'''
        for label in ("label", "title"):
            for accesskey in ("accesskey", "accessKey", "akey"):
                simplepo = simplepo_template % (label, accesskey)
                simpledtd = simpledtd_template % (label, accesskey)
                dtdfile = self.merge2dtd(simpledtd, simplepo)
                dtdfile.makeindex()
                assert dtd.unquotefromdtd(dtdfile.id_index["simple.%s" % accesskey].definition) == "a"

    def test_ampersandfix(self):
        """tests that invalid ampersands are fixed in the dtd"""
        simplestring = '''#: simple.string\nmsgid "Simple String"\nmsgstr "Dimpled &Ring"\n'''
        dtdfile = self.po2dtd(simplestring)
        dtdsource = str(dtdfile)
        assert "Dimpled Ring" in dtdsource

        po_snippet = r'''#: searchIntegration.label
#: searchIntegration.accesskey
msgid "Allow &searchIntegration.engineName; to &search messages"
msgstr "&searchIntegration.engineName; &ileti aramasna izin ver"
'''
        dtd_snippet = r'''<!ENTITY searchIntegration.accesskey      "s">
<!ENTITY searchIntegration.label       "Allow &searchIntegration.engineName; to search messages">'''
        dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
        dtdsource = str(dtdfile)
        print(dtdsource)
        assert '"&searchIntegration.engineName; ileti aramasna izin ver"' in dtdsource

    def test_accesskey_missing(self):
        """tests that missing ampersands use the source accesskey"""
        po_snippet = r'''#: key.label
#: key.accesskey
msgid "&Search"
msgstr "Ileti"
'''
        dtd_snippet = r'''<!ENTITY key.accesskey      "S">
<!ENTITY key.label       "Ileti">'''
        dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
        dtdsource = str(dtdfile)
        print(dtdsource)
        assert '"Ileti"' in dtdsource
        assert '""' not in dtdsource
        assert '"S"' in dtdsource

    def test_accesskey_and_amp_case_no_accesskey(self):
        """tests that accesskey and &amp; can work together

        If missing we use the source accesskey"""
        po_snippet = r'''#: key.label
#: key.accesskey
msgid "Colour & &Light"
msgstr "Lig en Kleur"
'''
        dtd_snippet = r'''<!ENTITY key.accesskey      "L">
<!ENTITY key.label       "Colour &amp; Light">'''
        dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
        dtdsource = str(dtdfile)
        print(dtdsource)
        assert '"Lig en Kleur"' in dtdsource
        assert '"L"' in dtdsource

    def test_accesskey_and_amp_case_no_amp(self):
        """tests that accesskey and &amp; can work together

        If present we use the target accesskey"""
        po_snippet = r'''#: key.label
#: key.accesskey
msgid "Colour & &Light"
msgstr "Lig en &Kleur"
'''
        dtd_snippet = r'''<!ENTITY key.accesskey      "L">
<!ENTITY key.label       "Colour &amp; Light">'''
        dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
        dtdsource = str(dtdfile)
        print(dtdsource)
        assert '"Lig en Kleur"' in dtdsource
        assert '"K"' in dtdsource

    def test_accesskey_and_amp_case_both_amp_and_accesskey(self):
        """tests that accesskey and &amp; can work together

        If present both & (and) and a marker then we use the correct source
        accesskey"""
        po_snippet = r'''#: key.label
#: key.accesskey
msgid "Colour & &Light"
msgstr "Lig & &Kleur"
'''
        dtd_snippet = r'''<!ENTITY key.accesskey      "L">
<!ENTITY key.label       "Colour &amp; Light">'''
        dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
        dtdsource = str(dtdfile)
        print(dtdsource)
        assert '"Lig &amp; Kleur"' in dtdsource
        assert '"K"' in dtdsource

    def test_entities_two(self):
        """test the error ouput when we find two entities"""
        simplestring = '''#: simple.string second.string\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        dtdfile = self.po2dtd(simplestring)
        dtdsource = str(dtdfile)
        assert "CONVERSION NOTE - multiple entities" in dtdsource

    def test_entities(self):
        """tests that entities are correctly idnetified in the dtd"""
        simplestring = '''#: simple.string\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        dtdfile = self.po2dtd(simplestring)
        dtdsource = str(dtdfile)
        assert dtdsource.startswith("<!ENTITY simple.string")

    def test_comments_translator(self):
        """tests for translator comments"""
        simplestring = '''# Comment1\n# Comment2\n#: simple.string\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        dtdfile = self.po2dtd(simplestring)
        dtdsource = str(dtdfile)
        assert dtdsource.startswith("<!-- Comment1 -->")

    def test_retains_hashprefix(self):
        """tests that hash prefixes in the dtd are retained"""
        hashpo = '''#: lang.version\nmsgid "__MOZILLA_LOCALE_VERSION__"\nmsgstr "__MOZILLA_LOCALE_VERSION__"\n'''
        hashdtd = '#expand <!ENTITY lang.version "__MOZILLA_LOCALE_VERSION__">\n'
        dtdfile = self.merge2dtd(hashdtd, hashpo)
        regendtd = str(dtdfile)
        assert regendtd == hashdtd

    def test_convertdtd(self):
        """checks that the convertdtd function is working"""
        posource = '''#: simple.label\n#: simple.accesskey\nmsgid "Simple &String"\nmsgstr "Dimpled &Ring"\n'''
        dtdtemplate = '''<!ENTITY simple.label "Simple String">\n<!ENTITY simple.accesskey "S">\n'''
        dtdexpected = '''<!ENTITY simple.label "Dimpled Ring">\n<!ENTITY simple.accesskey "R">\n'''
        newdtd = self.convertdtd(posource, dtdtemplate)
        print(newdtd)
        assert newdtd == dtdexpected

    def test_untranslated_with_template(self):
        """test removing of untranslated entries in redtd"""
        posource = '''#: simple.label
msgid "Simple string"
msgstr "Dimpled ring"

#: simple.label2
msgid "Simple string 2"
msgstr ""

#: simple.label3
msgid "Simple string 3"
msgstr "Simple string 3"

#: simple.label4
#, fuzzy
msgid "Simple string 4"
msgstr "simple string four"
'''
        dtdtemplate = '''<!ENTITY simple.label "Simple string">
<!ENTITY simple.label2 "Simple string 2">
<!ENTITY simple.label3 "Simple string 3">
<!ENTITY simple.label4 "Simple string 4">
'''
        dtdexpected = '''<!ENTITY simple.label "Dimpled ring">

<!ENTITY simple.label3 "Simple string 3">

'''
        newdtd = self.convertdtd(posource, dtdtemplate, remove_untranslated=True)
        print(newdtd)
        assert newdtd == dtdexpected

    def test_untranslated_without_template(self):
        """test removing of untranslated entries in po2dtd"""
        posource = '''#: simple.label
msgid "Simple string"
msgstr "Dimpled ring"

#: simple.label2
msgid "Simple string 2"
msgstr ""

#: simple.label3
msgid "Simple string 3"
msgstr "Simple string 3"

#: simple.label4
#, fuzzy
msgid "Simple string 4"
msgstr "simple string four"
'''
        dtdexpected = '''<!ENTITY simple.label "Dimpled ring">
<!ENTITY simple.label3 "Simple string 3">
'''
        newdtd = self.po2dtd(posource, remove_untranslated=True)
        print(newdtd)
        assert str(newdtd) == dtdexpected

    def test_blank_source(self):
        """test removing of untranslated entries where source is blank"""
        posource = '''#: simple.label
msgid "Simple string"
msgstr "Dimpled ring"

#: simple.label2
msgid ""
msgstr ""

#: simple.label3
msgid "Simple string 3"
msgstr "Simple string 3"
'''
        dtdtemplate = '''<!ENTITY simple.label "Simple string">
<!ENTITY simple.label2 "">
<!ENTITY simple.label3 "Simple string 3">
'''
        dtdexpected_with_template = '''<!ENTITY simple.label "Dimpled ring">
<!ENTITY simple.label2 "">
<!ENTITY simple.label3 "Simple string 3">
'''

        dtdexpected_no_template = '''<!ENTITY simple.label "Dimpled ring">
<!ENTITY simple.label3 "Simple string 3">
'''
        newdtd_with_template = self.convertdtd(posource, dtdtemplate, remove_untranslated=True)
        print(newdtd_with_template)
        assert newdtd_with_template == dtdexpected_with_template
        newdtd_no_template = self.po2dtd(posource, remove_untranslated=True)
        print(newdtd_no_template)
        assert str(newdtd_no_template) == dtdexpected_no_template

    def test_newlines_escapes(self):
        """check that we can handle a \n in the PO file"""
        posource = '''#: simple.label\n#: simple.accesskey\nmsgid "A hard coded newline.\\n"\nmsgstr "Hart gekoeerde nuwe lyne\\n"\n'''
        dtdtemplate = '<!ENTITY  simple.label "A hard coded newline.\n">\n'
        dtdexpected = '''<!ENTITY  simple.label "Hart gekoeerde nuwe lyne\n">\n'''
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected

    def test_roundtrip_simple(self):
        """checks that simple strings make it through a dtd->po->dtd roundtrip"""
        self.check_roundtrip('"Hello"')
        self.check_roundtrip('"Hello Everybody"')

    def test_roundtrip_escape(self):
        """checks that escapes in strings make it through a dtd->po->dtd roundtrip"""
        self.check_roundtrip(r'"Simple Escape \ \n \\ \: \t \r "')
        self.check_roundtrip(r'"End Line Escape \"')

    def test_roundtrip_quotes(self):
        """Checks that quotes make it through a DTD->PO->DTD roundtrip.

        Quotes may be escaped or not.
        """
        # NOTE: during the roundtrip, if " quote mark is present, then it is
        # converted to &quot; and the resulting string is always enclosed
        # between " characters independently of which quotation marks the
        # original string is enclosed between. Thus the string cannot be
        # compared with itself and therefore other string should be provided to
        # compare with the result.
        #
        # Thus the string cannot be compared with itself and therefore another
        # string should be provided to compare with the roundtrip result.
        self.check_roundtrip(r"""'Quote Escape "" '""",
                             r'''"Quote Escape &quot;&quot; "''')
        self.check_roundtrip(r'''"Double-Quote Escape &quot;&quot; "''')
        self.check_roundtrip(r'''"Single-Quote ' "''')
        self.check_roundtrip(r'''"Single-Quote Escape \' "''')
        # NOTE: during the roundtrip, if " quote mark is present, then ' is
        # converted to &apos; and " is converted to &quot; Also the resulting
        # string is always enclosed between " characters independently of which
        # quotation marks the original string is enclosed between. Thus the
        # string cannot be compared with itself and therefore another string
        # should be provided to compare with the result.
        #
        # Thus the string cannot be compared with itself and therefore another
        # string should be provided to compare with the roundtrip result.
        self.check_roundtrip(r"""'Both Quotes "" &apos;&apos; '""",
                             r'''"Both Quotes &quot;&quot; &apos;&apos; "''')
        self.check_roundtrip(r'''"Both Quotes &quot;&quot; &apos;&apos; "''')
        # NOTE: during the roundtrip, if &quot; is present, then ' is converted
        # to &apos; Also the resulting string is always enclosed between "
        # characters independently of which quotation marks the original string
        # is enclosed between.
        #
        # Thus the string cannot be compared with itself and therefore another
        # string should be provided to compare with the roundtrip result.
        self.check_roundtrip(r'''"Both Quotes &quot;&quot; '' "''',
                             r'''"Both Quotes &quot;&quot; &apos;&apos; "''')

    def test_roundtrip_amp(self):
        """Checks that quotes make it through a DTD->PO->DTD roundtrip.

        Quotes may be escaped or not.
        """
        self.check_roundtrip('"Colour &amp; Light"')

    def test_merging_entries_with_spaces_removed(self):
        """dtd2po removes pretty printed spaces, this tests that we can merge this back into the pretty printed dtd"""
        posource = '''#: simple.label\nmsgid "First line then "\n"next lines."\nmsgstr "Eerste lyne en dan volgende lyne."\n'''
        dtdtemplate = '<!ENTITY simple.label "First line then\n' + \
          '                                          next lines.">\n'
        dtdexpected = '<!ENTITY simple.label "Eerste lyne en dan volgende lyne.">\n'
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected

    def test_preserving_spaces(self):
        """ensure that we preseve spaces between entity and value. Bug 1662"""
        posource = '''#: simple.label\nmsgid "One"\nmsgstr "Een"\n'''
        dtdtemplate = '<!ENTITY     simple.label         "One">\n'
        dtdexpected = '<!ENTITY     simple.label         "Een">\n'
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected

    def test_preserving_spaces(self):
        """Preseve spaces after value. Bug 1662"""
        # Space between value and >
        posource = '''#: simple.label\nmsgid "One"\nmsgstr "Een"\n'''
        dtdtemplate = '<!ENTITY simple.label "One" >\n'
        dtdexpected = '<!ENTITY simple.label "Een" >\n'
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected
        # Space after >
        dtdtemplate = '<!ENTITY simple.label "One"> \n'
        dtdexpected = '<!ENTITY simple.label "Een"> \n'
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected

    def test_comments(self):
        """test that we preserve comments, bug 351"""
        posource = '''#: name\nmsgid "Text"\nmsgstr "Teks"'''
        dtdtemplate = '''<!ENTITY name "%s">\n<!-- \n\nexample -->\n'''
        dtdfile = self.merge2dtd(dtdtemplate % "Text", posource)
        print(dtdfile)
        assert str(dtdfile) == dtdtemplate % "Teks"

    def test_duplicates(self):
        """test that we convert duplicates back correctly to their respective entries."""
        posource = r'''#: bookmarksMenu.label bookmarksMenu.accesskey
msgctxt "bookmarksMenu.label bookmarksMenu.accesskey"
msgid "&Bookmarks"
msgstr "Dipu&kutshwayo1"

#: bookmarksItem.title
msgctxt "bookmarksItem.title
msgid "Bookmarks"
msgstr "Dipukutshwayo2"

#: bookmarksButton.label
msgctxt "bookmarksButton.label"
msgid "Bookmarks"
msgstr "Dipukutshwayo3"
'''
        dtdtemplate = r'''<!ENTITY bookmarksMenu.label "Bookmarks">
<!ENTITY bookmarksMenu.accesskey "B">
<!ENTITY bookmarksItem.title "Bookmarks">
<!ENTITY bookmarksButton.label "Bookmarks">
'''
        dtdexpected = r'''<!ENTITY bookmarksMenu.label "Dipukutshwayo1">
<!ENTITY bookmarksMenu.accesskey "k">
<!ENTITY bookmarksItem.title "Dipukutshwayo2">
<!ENTITY bookmarksButton.label "Dipukutshwayo3">
'''
        dtdfile = self.merge2dtd(dtdtemplate, posource)
        print(dtdfile)
        assert str(dtdfile) == dtdexpected


class TestPO2DTDCommand(test_convert.TestConvertCommand, TestPO2DTD):
    """Tests running actual po2dtd commands on files"""
    convertmodule = po2dtd
    defaultoptions = {"progress": "none"}
    # TODO: because of having 2 base classes, we need to call all their setup and teardown methods
    # (otherwise we won't reset the warnings etc)

    def setup_method(self, method):
        """call both base classes setup_methods"""
        test_convert.TestConvertCommand.setup_method(self, method)
        TestPO2DTD.setup_method(self, method)

    def teardown_method(self, method):
        """call both base classes teardown_methods"""
        test_convert.TestConvertCommand.teardown_method(self, method)
        TestPO2DTD.teardown_method(self, method)

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--removeuntranslated")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2html
#!/usr/bin/env python

from pytest import mark

from translate.convert import po2html, test_convert
from translate.misc import wStringIO


class TestPO2Html:

    def converthtml(self, posource, htmltemplate, includefuzzy=False):
        """helper to exercise the command line function"""
        inputfile = wStringIO.StringIO(posource)
        print(inputfile.getvalue())
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(htmltemplate)
        assert po2html.converthtml(inputfile, outputfile, templatefile, includefuzzy)
        print(outputfile.getvalue())
        return outputfile.getvalue()

    def test_simple(self):
        """simple po to html test"""
        htmlsource = '<p>A sentence.</p>'
        posource = '''#: html:3\nmsgid "A sentence."\nmsgstr "'n Sin."\n'''
        htmlexpected = '''<p>'n Sin.</p>'''
        assert htmlexpected in self.converthtml(posource, htmlsource)

    def test_linebreaks(self):
        """Test that a po file can be merged into a template with linebreaks in it."""
        htmlsource = '''<html>
<head>
</head>
<body>
<div>
A paragraph is a section in a piece of writing, usually highlighting a
particular point or topic. It always begins on a new line and usually
with indentation, and it consists of at least one sentence.
</div>
</body>
</html>
'''
        posource = '''#: None:1
msgid ""
"A paragraph is a section in a piece of writing, usually highlighting a "
"particular point or topic. It always begins on a new line and usually with "
"indentation, and it consists of at least one sentence."
msgstr ""
"'n Paragraaf is 'n afdeling in 'n geskrewe stuk wat gewoonlik 'n spesifieke "
"punt uitlig. Dit begin altyd op 'n nuwe lyn (gewoonlik met indentasie) en "
"dit bestaan uit ten minste een sin."
'''
        htmlexpected = '''<body>
<div>
'n Paragraaf is 'n afdeling in 'n geskrewe stuk wat gewoonlik
'n spesifieke punt uitlig. Dit begin altyd op 'n nuwe lyn
(gewoonlik met indentasie) en dit bestaan uit ten minste een
sin.
</div>
</body>'''
        assert htmlexpected.replace("\n", " ") in self.converthtml(posource, htmlsource).replace("\n", " ")

    @mark.xfail(reason="Not Implemented")
    def test_entities(self):
        """Tests that entities are handled correctly"""
        htmlsource = '<p>5 less than 6</p>'
        posource = '#:html:3\nmsgid "5 less than 6"\nmsgstr "5 < 6"\n'
        htmlexpected = '<p>5 &lt; 6</p>'
        assert htmlexpected in self.converthtml(posource, htmlsource)

        htmlsource = '<p>Fish &amp; chips</p>'
        posource = '#: html:3\nmsgid "Fish & chips"\nmsgstr "Vis & skyfies"\n'
        htmlexpected = '<p>Vis &amp; skyfies</p>'
        assert htmlexpected in self.converthtml(posource, htmlsource)

    @mark.xfail(reason="Not Implemented")
    def test_escapes(self):
        """Tests that PO escapes are correctly handled"""
        htmlsource = '<div>Row 1<br />Row 2</div>'
        posource = '#: html:3\nmsgid "Row 1\\n"\n"Row 2"\nmsgstr "Ry 1\\n"\n"Ry 2"\n'
        htmlexpected = '<div>Ry 1<br />Ry 2</div>'
        assert htmlexpected in self.converthtml(posource, htmlsource)

        htmlsource = '<p>"leverage"</p>'
        posource = '#: html3\nmsgid "\\"leverage\\""\nmsgstr "\\"ek is dom\\""\n'
        htmlexpected = '<p>"ek is dom"</p>'
        assert htmlexpected in self.converthtml(posource, htmlsource)

    def test_states_translated(self):
        """Test that we use target when translated"""
        htmlsource = '<div>aaa</div>'
        posource = 'msgid "aaa"\nmsgstr "bbb"\n'
        htmltarget = '<div>bbb</div>'
        assert htmltarget in self.converthtml(posource, htmlsource)
        assert htmlsource not in self.converthtml(posource, htmlsource)

    def test_states_untranslated(self):
        """Test that we use source when a string is untranslated"""
        htmlsource = '<div>aaa</div>'
        posource = 'msgid "aaa"\nmsgstr ""\n'
        htmltarget = htmlsource
        assert htmltarget in self.converthtml(posource, htmlsource)

    def test_states_fuzzy(self):
        """Test that we use source when a string is fuzzy

        This fixes :bug:`3145`
        """
        htmlsource = '<div>aaa</div>'
        posource = '#: html:3\n#, fuzzy\nmsgid "aaa"\nmsgstr "bbb"\n'
        htmltarget = '<div>bbb</div>'
        # Don't use fuzzies
        assert htmltarget not in self.converthtml(posource, htmlsource, includefuzzy=False)
        assert htmlsource in self.converthtml(posource, htmlsource, includefuzzy=False)
        # Use fuzzies
        assert htmltarget in self.converthtml(posource, htmlsource, includefuzzy=True)
        assert htmlsource not in self.converthtml(posource, htmlsource, includefuzzy=True)

    def test_untranslated_attributes(self):
        """Verify that untranslated attributes are output as source, not dropped."""
        htmlsource = '<meta name="keywords" content="life, the universe, everything" />'
        posource = '#: test.html+:-1\nmsgid "life, the universe, everything"\nmsgstr ""'
        expected = '<meta name="keywords" content="life, the universe, everything" />'
        assert expected in self.converthtml(posource, htmlsource)


class TestPO2HtmlCommand(test_convert.TestConvertCommand, TestPO2Html):
    """Tests running actual po2oo commands on files"""
    convertmodule = po2html

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2ical
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pytest
pytest.importorskip("vobject")

from translate.convert import po2ical, test_convert
from translate.misc import wStringIO
from translate.storage import po


icalboiler = '''BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//hacksw/handcal//NONSGML v1.0//EN
BEGIN:VEVENT
UID:uid1@example.com
DTSTART:19970714T170000Z
DTEND:19970715T035959Z
DTSTAMP:19970714T170000Z
ORGANIZER;CN=John Doe:MAILTO:john.doe@example.com
SUMMARY:%s
END:VEVENT
END:VCALENDAR
'''.replace("\n", "\r\n")


class TestPO2Ical:

    def po2ical(self, posource):
        """helper that converts po source to .ics source without requiring
        files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2ical.reical()
        outputical = convertor.convertstore(inputpo)
        return outputical

    def merge2ical(self, propsource, posource):
        """helper that merges po translations to .ics source without requiring
        files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(propsource)
        #templateprop = properties.propfile(templatefile)
        convertor = po2ical.reical(templatefile, inputpo)
        outputical = convertor.convertstore()
        print(outputical)
        return outputical

    def test_simple_summary(self):
        """test that we output correctly for Inno files."""
        posource = ur'''#: [uid1@example.com]SUMMARY
msgid "Value"
msgstr "Waarde"
'''
        icaltemplate = icalboiler % "Value"
        icalexpected = icalboiler % "Waarde"
        icalfile = self.merge2ical(icaltemplate, posource)
        print(icalexpected)
        assert icalfile == icalexpected

    # FIXME we should also test for DESCRIPTION, LOCATION and COMMENT
    # The library handle any special encoding issues, we might want to test
    # those


class TestPO2IcalCommand(test_convert.TestConvertCommand, TestPO2Ical):
    """Tests running actual po2ical commands on files"""
    convertmodule = po2ical
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2ini
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import importorskip

from translate.convert import po2ini, test_convert
from translate.misc import wStringIO
from translate.storage import po


importorskip("iniparse")


class TestPO2Ini:

    def po2ini(self, posource):
        """helper that converts po source to .ini source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2ini.reini()
        outputini = convertor.convertstore(inputpo)
        return outputini

    def merge2ini(self, inisource, posource, dialect="default"):
        """helper that merges po translations to .ini source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(inisource)
        convertor = po2ini.reini(templatefile, inputpo, dialect=dialect)
        outputini = convertor.convertstore()
        print(outputini)
        return outputini

    def test_merging_simple(self):
        """check the simplest case of merging a translation"""
        posource = '''#: [section]prop\nmsgid "value"\nmsgstr "waarde"\n'''
        initemplate = '''[section]\nprop=value\n'''
        iniexpected = '''[section]\nprop=waarde\n'''
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_space_preservation(self):
        """check that we preserve any spacing in ini files when merging"""
        posource = '''#: [section]prop\nmsgid "value"\nmsgstr "waarde"\n'''
        initemplate = '''[section]\nprop  =  value\n'''
        iniexpected = '''[section]\nprop  =  waarde\n'''
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_merging_blank_entries(self):
        """check that we can correctly merge entries that are blank in the template"""
        posource = r'''#: [section]accesskey-accept
msgid ""
"_: accesskey-accept\n"
""
msgstr ""'''
        initemplate = '[section]\naccesskey-accept=\n'
        iniexpected = '[section]\naccesskey-accept=\n'
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_merging_fuzzy(self):
        """check merging a fuzzy translation"""
        posource = '''#: [section]prop\n#, fuzzy\nmsgid "value"\nmsgstr "waarde"\n'''
        initemplate = '''[section]\nprop=value\n'''
        iniexpected = '''[section]\nprop=value\n'''
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_merging_propertyless_template(self):
        """check that when merging with a template with no ini values that we copy the template"""
        posource = ""
        initemplate = "# A comment\n"
        iniexpected = initemplate
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_empty_value(self):
        """test that we handle an value in translation that is missing in the template"""
        posource = '''#: [section]key
msgctxt "key"
msgid ""
msgstr "translated"
'''
        initemplate = '''[section]\nkey =\n'''
        iniexpected = '''[section]\nkey =translated\n'''
        inifile = self.merge2ini(initemplate, posource)
        print(inifile)
        assert inifile == iniexpected

    def test_dialects_inno(self):
        """test that we output correctly for Inno files."""
        posource = ur'''#: [section]prop
msgid "value\tvalue2\n"
msgstr "\t2\n"
'''
        initemplate = '''[section]\nprop  =  value%tvalue%n\n'''
        iniexpected = '''[section]\nprop  =  %t2%n\n'''
        inifile = self.merge2ini(initemplate, posource, "inno")
        print(inifile)
        assert inifile == iniexpected


class TestPO2IniCommand(test_convert.TestConvertCommand, TestPO2Ini):
    """Tests running actual po2ini commands on files"""
    convertmodule = po2ini
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2moz
#!/usr/bin/env python

from translate.convert import po2moz, test_convert


class TestPO2Moz:
    pass


class TestPO2MozCommand(test_convert.TestConvertCommand, TestPO2Moz):
    """Tests running actual po2moz commands on files"""
    convertmodule = po2moz
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "-l LOCALE, --locale=LOCALE")
        options = self.help_check(options, "--removeuntranslated")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2mozlang
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import po2mozlang, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPO2Lang:

    def po2lang(self, posource):
        """helper that converts po source to .lang source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2mozlang.po2lang(mark_active=False)
        outputlang = convertor.convertstore(inputpo)
        return outputlang

    def test_simple(self):
        """check the simplest case of merging a translation"""
        posource = '''#: prop\nmsgid "Source"\nmsgstr "Target"\n'''
        propexpected = ''';Source\nTarget\n'''
        langfile = self.po2lang(posource)
        print(langfile)
        assert str(langfile) == propexpected

    def test_comment(self):
        """Simple # comments"""
        posource = '''#. Comment\n#: prop\nmsgid "Source"\nmsgstr "Target"\n'''
        propexpected = '''# Comment\n;Source\nTarget\n'''
        langfile = self.po2lang(posource)
        print(langfile)
        assert str(langfile) == propexpected

    def test_fuzzy(self):
        """What happens with a fuzzy string"""
        posource = '''#. Comment\n#: prop\n#, fuzzy\nmsgid "Source"\nmsgstr "Target"\n'''
        propexpected = '''# Comment\n;Source\nSource\n'''
        langfile = self.po2lang(posource)
        print(langfile)
        assert str(langfile) == propexpected

    def test_ok_marker(self):
        """The {ok} marker"""
        posource = '''#: prop\nmsgid "Same"\nmsgstr "Same"\n'''
        propexpected = ''';Same\nSame {ok}\n'''
        langfile = self.po2lang(posource)
        print(langfile)
        assert str(langfile) == propexpected


class TestPO2LangCommand(test_convert.TestConvertCommand, TestPO2Lang):
    """Tests running actual po2prop commands on files"""
    convertmodule = po2mozlang
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--mark-active")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2oo
#!/usr/bin/env python

import os
import warnings

from pytest import mark

from translate.convert import oo2po, po2oo, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPO2OO:

    def setup_method(self, method):
        warnings.resetwarnings()

    def teardown_method(self, method):
        warnings.resetwarnings()

    def convertoo(self, posource, ootemplate, language="en-US"):
        """helper to exercise the command line function"""
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(ootemplate)
        assert po2oo.convertoo(inputfile, outputfile, templatefile, targetlanguage=language, timestamp=0)
        return outputfile.getvalue()

    def roundtripstring(self, entitystring):
        oointro, oooutro = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	', '				2002-02-02 02:02:02' + '\r\n'
        oosource = oointro + entitystring + oooutro
        ooinputfile = wStringIO.StringIO(oosource)
        ooinputfile2 = wStringIO.StringIO(oosource)
        pooutputfile = wStringIO.StringIO()
        oo2po.convertoo(ooinputfile, pooutputfile, ooinputfile2, targetlanguage='en-US')
        posource = pooutputfile.getvalue()
        poinputfile = wStringIO.StringIO(posource)
        ootemplatefile = wStringIO.StringIO(oosource)
        oooutputfile = wStringIO.StringIO()
        po2oo.convertoo(poinputfile, oooutputfile, ootemplatefile, targetlanguage="en-US")
        ooresult = oooutputfile.getvalue()
        print("original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult)
        assert ooresult.startswith(oointro) and ooresult.endswith(oooutro)
        return ooresult[len(oointro):-len(oooutro)]

    def check_roundtrip(self, oosource):
        """Checks that the round-tripped string is the same as the original"""
        assert self.roundtripstring(oosource) == oosource

    def test_convertoo(self):
        """checks that the convertoo function is working"""
        oobase = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	%s	%s				20050924 09:13:58' + '\r\n'
        posource = '''#: numpages.src#RID_SVXPAGE_NUM_OPTIONS.STR_BULLET.string.text\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        ootemplate = oobase % ('en-US', 'Simple String')
        ooexpected = oobase % ('zu', 'Dimpled Ring')
        newoo = self.convertoo(posource, ootemplate, language="zu")
        assert newoo == ootemplate + ooexpected

    def test_pofilter(self):
        """Tests integration with pofilter"""
        #Some bad po with a few errors:
        posource = '#: sourcefile.bla#ID_NUMBER.txet.gnirts\nmsgid "<tag cow=\\"3\\">Mistake."\nmsgstr "  <etiket koei=\\"3\\">(fout) "'
        filter = po2oo.filter
        pofile = po.pofile()
        pofile.parse(posource)
        assert not filter.validelement(pofile.units[0], "dummyname.po", "exclude-all")

    def test_roundtrip_simple(self):
        """checks that simple strings make it through a oo->po->oo roundtrip"""
        self.check_roundtrip('Hello')
        self.check_roundtrip('"Hello"')
        self.check_roundtrip('"Hello Everybody"')

    def test_roundtrip_escape(self):
        """checks that escapes in strings make it through a oo->po->oo roundtrip"""
        self.check_roundtrip(r'"Simple Escape \ \n \\ \: \t \r "')
        self.check_roundtrip(r'"More escapes \\n \\t \\r \\: "')
        self.check_roundtrip(r'"More escapes \\\n \\\t \\\r \\\: "')
        self.check_roundtrip(r'"More escapes \\\\n \\\\t \\\\r \\\\: "')
        self.check_roundtrip(r'"End Line Escape \"')
        self.check_roundtrip(r'"\\rangle \\langle')
        self.check_roundtrip(r'\\\\<')
        self.check_roundtrip(r'\\\<')
        self.check_roundtrip(r'\\<')
        self.check_roundtrip(r'\<')

    def test_roundtrip_quotes(self):
        """checks that (escaped) quotes in strings make it through a oo->po->oo roundtrip"""
        self.check_roundtrip(r"""'Quote Escape "" '""")
        self.check_roundtrip(r'''"Single-Quote ' "''')
        self.check_roundtrip(r'''"Single-Quote Escape \' "''')
        self.check_roundtrip(r"""'Both Quotes "" '' '""")

    @mark.xfail(reason="this test fails because the resultant PO file returns as po.isempty since...")
    def test_roundtrip_spaces(self):
        # FIXME: this test fails because the resultant PO file returns as po.isempty since .isblank returns true
        # which is caused by isblankmsgtr returning True.  Its a complete mess which would mean unravelling lots
        # of yuch in pypo.  Until we have time to do that unravelling we're diabling this test.  You can reenable
        # once we've fixed that.
        """checks that (escaped) quotes in strings make it through a oo->po->oo roundtrip"""
        self.check_roundtrip(" ")
        self.check_roundtrip(u"\u00a0")

    def test_default_timestamp(self):
        """test to ensure that we revert to the default timestamp"""
        oointro, oooutro = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Text				', '\r\n'
        posource = '''#: numpages.src#RID_SVXPAGE_NUM_OPTIONS.STR_BULLET.string.text\nmsgid "Text"\nmsgstr "Text"\n'''
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(oointro + '20050924 09:13:58' + oooutro)
        assert po2oo.convertoo(inputfile, outputfile, templatefile, targetlanguage="en-US")
        assert outputfile.getvalue() == oointro + '2002-02-02 02:02:02' + oooutro

    def test_escape_conversion(self):
        """test to ensure that we convert escapes correctly"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Column1\tColumn2\r\n				2002-02-02 02:02:02' + '\r\n'
        posource = '''#: numpages.src#RID_SVXPAGE_NUM_OPTIONS.STR_BULLET.string.text\nmsgid "Column1\\tColumn2\\r\\n"\nmsgstr "Kolom1\\tKolom2\\r\\n"\n'''
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(oosource)
        assert po2oo.convertoo(inputfile, outputfile, templatefile, targetlanguage="af-ZA")
        assert "\tKolom1\\tKolom2\\r\\n\t" in outputfile.getvalue()

    def test_helpcontent_escapes(self):
        """test to ensure that we convert helpcontent escapes correctly"""
        # Note how this test specifically uses incorrect spacing in the
        # translation. The extra space before 'hid' and an extra space before
        # the closing tag should not confuse us.
        oosource = r'helpcontent2	source\text\shared\3dsettings_toolbar.xhp	0	help	par_idN1056A				0	en-US	\<ahelp hid=\".\"\>The 3D-Settings toolbar controls properties of selected 3D objects.\</ahelp\>				2002-02-02 02:02:02' + '\r\n'
        posource = r'''#: 3dsettings_toolbar.xhp#par_idN1056A.help.text
msgid ""
"<ahelp hid=\".\">The 3D-Settings toolbar controls properties of selected 3D "
"ob jects.</ahelp>"
msgstr ""
"<ahelp  hid=\".\" >Zeee 3DDDD-Settings toolbar controls properties of selected 3D "
"objects.</ahelp>"
'''
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(oosource)
        assert po2oo.convertoo(inputfile, outputfile, templatefile, targetlanguage="af-ZA")
        assert r"\<ahelp  hid=\".\" \>Zeee 3DDDD-Settings toolbar controls properties of selected 3D objects.\</ahelp\>" in outputfile.getvalue()

    def test_helpcontent_escapes2(self):
        """test to ensure that we convert helpcontent escapes correctly"""
        oosource = r'helpcontent2	source\text\scalc\05\empty_cells.xhp	0	help	par_id2629474				0	en-US	A1: <empty>				2002-02-02 02:02:02' + '\r\n'
        posource = r'''#: empty_cells.xhp#par_id2629474.help.text
msgid "A1: <empty>"
msgstr "Aa1: <empty>"
'''
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        templatefile = wStringIO.StringIO(oosource)
        assert po2oo.convertoo(inputfile, outputfile, templatefile, targetlanguage="af-ZA")
        assert r"Aa1: <empty>" in outputfile.getvalue()


class TestPO2OOCommand(test_convert.TestConvertCommand, TestPO2OO):
    """Tests running actual po2oo commands on files"""
    convertmodule = po2oo

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--source-language=LANG")
        options = self.help_check(options, "--language=LANG")
        options = self.help_check(options, "-T, --keeptimestamp")
        options = self.help_check(options, "--nonrecursiveoutput")
        options = self.help_check(options, "--nonrecursivetemplate")
        options = self.help_check(options, "--filteraction")
        options = self.help_check(options, "--skipsource")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--multifile=MULTIFILESTYLE", last=True)

    def merge2oo(self, oosource, posource):
        """helper that merges po translations to oo source through files"""
        outputoo = convertor.convertstore(inputpo)
        return outputoo

    def convertoo(self, posource, ootemplate, language="en-US"):
        """helper to exercise the command line function"""
        self.create_testfile(os.path.join("input", "svx", "source", "dialog.po"), posource)
        self.create_testfile("input.oo", ootemplate)
        self.run_command("input", "output.oo", template="input.oo", language=language, keeptimestamp=True)
        return self.read_testfile("output.oo")

########NEW FILE########
__FILENAME__ = test_po2php
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.convert import po2php, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPO2Php:

    def po2php(self, posource):
        """helper that converts po source to .php source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2php.po2php()
        outputphp = convertor.convertstore(inputpo)
        return outputphp

    def merge2php(self, phpsource, posource):
        """helper that merges po translations to .php source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(phpsource)
        #templatephp = php.phpfile(templatefile)
        convertor = po2php.rephp(templatefile, inputpo)
        outputphp = convertor.convertstore()
        print(outputphp)
        return outputphp

    def test_merging_simple(self):
        """check the simplest case of merging a translation"""
        posource = '''#: $lang['name']\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang['name'] = 'value';\n'''
        phpexpected = '''$lang['name'] = 'waarde';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_space_preservation(self):
        """check that we preserve any spacing in php files when merging"""
        posource = '''#: $lang['name']\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang['name']  =  'value';\n'''
        phpexpected = '''$lang['name']  =  'waarde';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_merging_blank_entries(self):
        """check that we can correctly merge entries that are blank in the template"""
        posource = r'''#: accesskey-accept
msgid ""
"_: accesskey-accept\n"
""
msgstr ""'''
        phptemplate = '''$lang['accesskey-accept'] = '';\n'''
        phpexpected = '''$lang['accesskey-accept'] = '';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_merging_fuzzy(self):
        """check merging a fuzzy translation"""
        posource = '''#: %24lang%5B+%27name%27+%5D\n#, fuzzy\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang['name']  =  'value';\n'''
        phpexpected = '''$lang['name']  =  'value';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_locations_with_spaces(self):
        """check that a location with spaces in php but spaces removed in PO is used correctly"""
        posource = '''#: %24lang%5B+%27name%27+%5D\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang[ 'name' ]  =  'value';\n'''
        phpexpected = '''$lang[ 'name' ]  =  'waarde';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_inline_comments(self):
        """check that we include inline comments from the template.  Bug 590"""
        posource = '''#: %24lang%5B+%27name%27+%5D\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang[ 'name' ]  =  'value'; //inline comment\n'''
        phpexpected = '''$lang[ 'name' ]  =  'waarde'; //inline comment\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_named_variables(self):
        """check that we convert correctly if using named variables."""
        posource = '''#: $dictYear
msgid "Year"
msgstr "Jaar"
'''
        phptemplate = '''$dictYear = 'Year';\n'''
        phpexpected = '''$dictYear = 'Jaar';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert phpfile == [phpexpected]

    def test_multiline(self):
        """Check that we convert multiline strings correctly.

        Bug 1296."""
        posource = r'''#: $string['upgradesure']
msgid ""
"Your Moodle files have been changed, and you are\n"
"about to automatically upgrade your server to this version:\n"
"<p><b>$a</b></p>\n"
"<p>Once you do this you can not go back again.</p>\n"
"<p>Are you sure you want to upgrade this server to this version?</p>"
msgstr ""
'''
        phptemplate = '''$string['upgradesure'] = 'Your Moodle files have been changed, and you are
about to automatically upgrade your server to this version:
<p><b>$a</b></p>
<p>Once you do this you can not go back again.</p>
<p>Are you sure you want to upgrade this server to this version?</p>';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile[0])
        assert phpfile[0] == phptemplate

    def test_hash_comment(self):
        """check that we convert # comments correctly."""
        posource = '''#: $variable
msgid "stringy"
msgstr "stringetjie"
'''
        phptemplate = '''# inside alt= stuffies\n$variable = 'stringy';\n'''
        phpexpected = '''# inside alt= stuffies\n$variable = 'stringetjie';\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert "".join(phpfile) == phpexpected

    def test_arrays(self):
        """check that we can handle arrays"""
        posource = '''#: $lang->'name'\nmsgid "value"\nmsgstr "waarde"\n'''
        phptemplate = '''$lang = array(\n    'name' => 'value',\n);\n'''
        phpexpected = '''$lang = array(\n    'name' => 'waarde',\n);\n'''
        phpfile = self.merge2php(phptemplate, posource)
        print(phpfile)
        assert "".join(phpfile) == phpexpected

    @mark.xfail(reason="Need to review if we want this behaviour")
    def test_merging_propertyless_template(self):
        """check that when merging with a template with no property values that we copy the template"""
        posource = ""
        proptemplate = "# A comment\n"
        propexpected = proptemplate
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == [propexpected]


class TestPO2PhpCommand(test_convert.TestConvertCommand, TestPO2Php):
    """Tests running actual po2php commands on files"""
    convertmodule = po2php
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2prop
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import po2prop, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPO2Prop:

    def po2prop(self, posource):
        """helper that converts po source to .properties source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2prop.po2prop()
        outputprop = convertor.convertstore(inputpo)
        return outputprop

    def merge2prop(self, propsource, posource, personality="java", remove_untranslated=False):
        """helper that merges po translations to .properties source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(propsource)
        #templateprop = properties.propfile(templatefile)
        convertor = po2prop.reprop(templatefile, inputpo, personality=personality, remove_untranslated=remove_untranslated)
        outputprop = convertor.convertstore()
        print(outputprop)
        return outputprop

    def test_merging_simple(self):
        """check the simplest case of merging a translation"""
        posource = '''#: prop\nmsgid "value"\nmsgstr "waarde"\n'''
        proptemplate = '''prop=value\n'''
        propexpected = '''prop=waarde\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_merging_untranslated(self):
        """check the simplest case of merging an untranslated unit"""
        posource = '''#: prop\nmsgid "value"\nmsgstr ""\n'''
        proptemplate = '''prop=value\n'''
        propexpected = proptemplate
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_hard_newlines_preserved(self):
        """check that we preserver hard coded newlines at the start and end of sentence"""
        posource = '''#: prop\nmsgid "\\nvalue\\n\\n"\nmsgstr "\\nwaarde\\n\\n"\n'''
        proptemplate = '''prop=\\nvalue\\n\\n\n'''
        propexpected = '''prop=\\nwaarde\\n\\n\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_space_preservation(self):
        """check that we preserve any spacing in properties files when merging"""
        posource = '''#: prop\nmsgid "value"\nmsgstr "waarde"\n'''
        proptemplate = '''prop  =  value\n'''
        propexpected = '''prop  =  waarde\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_merging_blank_entries(self):
        """check that we can correctly merge entries that are blank in the template"""
        posource = r'''#: accesskey-accept
msgid ""
"_: accesskey-accept\n"
""
msgstr ""'''
        proptemplate = 'accesskey-accept=\n'
        propexpected = 'accesskey-accept=\n'
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_merging_fuzzy(self):
        """check merging a fuzzy translation"""
        posource = '''#: prop\n#, fuzzy\nmsgid "value"\nmsgstr "waarde"\n'''
        proptemplate = '''prop=value\n'''
        propexpected = '''prop=value\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_mozilla_accesskeys(self):
        """check merging Mozilla accesskeys"""
        posource = '''#: prop.label prop.accesskey
msgid "&Value"
msgstr "&Waarde"

#: key.label key.accesskey
msgid "&Key"
msgstr "&Sleutel"
'''
        proptemplate = '''prop.label=Value
prop.accesskey=V
key.label=Key
key.accesskey=K
'''
        propexpected = '''prop.label=Waarde
prop.accesskey=W
key.label=Sleutel
key.accesskey=S
'''
        propfile = self.merge2prop(proptemplate, posource, personality="mozilla")
        print(propfile)
        assert propfile == propexpected

    def test_mozilla_accesskeys_missing_accesskey(self):
        """check merging Mozilla accesskeys"""
        posource = '''#: prop.label prop.accesskey
# No accesskey because we forgot or language doesn't do accesskeys
msgid "&Value"
msgstr "Waarde"
'''
        proptemplate = '''prop.label=Value
prop.accesskey=V
'''
        propexpected = '''prop.label=Waarde
prop.accesskey=V
'''
        propfile = self.merge2prop(proptemplate, posource, personality="mozilla")
        print(propfile)
        assert propfile == propexpected

    def test_mozilla_margin_whitespace(self):
        """Check handling of Mozilla leading and trailing spaces"""
        posource = '''#: sepAnd
msgid " and "
msgstr "  "

#: sepComma
msgid ", "
msgstr " "
'''
        proptemplate = r'''sepAnd = \u0020and\u0020
sepComma = ,\u20
'''
        propexpected = r'''sepAnd = \u0020\u0020
sepComma = \u0020
'''
        propfile = self.merge2prop(proptemplate, posource, personality="mozilla")
        print(propfile)
        assert propfile == propexpected

    def test_mozilla_all_whitespace(self):
        """Check for all white-space Mozilla hack, remove when the
        corresponding code is removed."""
        posource = '''#: accesskey-accept
msgctxt "accesskey-accept"
msgid ""
msgstr " "

#: accesskey-help
msgid "H"
msgstr ""
'''
        proptemplate = '''accesskey-accept=
accesskey-help=H
'''
        propexpected = '''accesskey-accept=
accesskey-help=
'''
        propfile = self.merge2prop(proptemplate, posource, personality="mozilla")
        print(propfile)
        assert propfile == propexpected

    def test_merging_propertyless_template(self):
        """check that when merging with a template with no property values that we copy the template"""
        posource = ""
        proptemplate = "# A comment\n"
        propexpected = proptemplate
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_delimiters(self):
        """test that we handle different delimiters."""
        posource = '''#: prop\nmsgid "value"\nmsgstr "translated"\n'''
        proptemplate = '''prop %s value\n'''
        propexpected = '''prop %s translated\n'''
        for delim in ['=', ':', '']:
            print("testing '%s' as delimiter" % delim)
            propfile = self.merge2prop(proptemplate % delim, posource)
            print(propfile)
            assert propfile == propexpected % delim

    def test_empty_value(self):
        """test that we handle an value in the template"""
        posource = '''#: key
msgctxt "key"
msgid ""
msgstr "translated"
'''
        proptemplate = '''key\n'''
        propexpected = '''key = translated\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected

    def test_personalities(self):
        """test that we output correctly for Java and Mozilla style property files.  Mozilla uses Unicode, while Java uses escaped Unicode"""
        posource = u'''#: prop\nmsgid "value"\nmsgstr ""\n'''
        proptemplate = u'''prop  =  value\n'''
        propexpectedjava = u'''prop  =  \\u1E7D\\u1E01\\u1E3D\\u1E7B\\u1E1D\n'''
        propfile = self.merge2prop(proptemplate, posource)
        assert propfile == propexpectedjava

        propexpectedmozilla = u'''prop  =  \n'''.encode('utf-8')
        propfile = self.merge2prop(proptemplate, posource, personality="mozilla")
        assert propfile == propexpectedmozilla

        proptemplate = u'''prop  =  value\n'''.encode('utf-16')
        propexpectedskype = u'''prop  =  \n'''.encode('utf-16')
        propfile = self.merge2prop(proptemplate, posource, personality="skype")
        assert propfile == propexpectedskype

        proptemplate = u'''"prop" = "value";\n'''.encode('utf-16')
        propexpectedstrings = u'''"prop" = "";\n'''.encode('utf-16')
        propfile = self.merge2prop(proptemplate, posource, personality="strings")
        assert propfile == propexpectedstrings

    def test_merging_untranslated_simple(self):
        """check merging untranslated entries in two 1) use English 2) drop key, value pair"""
        posource = '''#: prop\nmsgid "value"\nmsgstr ""\n'''
        proptemplate = '''prop = value\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == proptemplate  # We use the existing values
        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=True)
        print(propfile)
        assert propfile == ''  # We drop the key

    def test_merging_untranslated_multiline(self):
        """check merging untranslated entries with multiline values"""
        posource = '''#: prop\nmsgid "value1 value2"\nmsgstr ""\n'''
        proptemplate = '''prop = value1 \
    value2
'''
        propexpected = '''prop = value1 value2\n'''
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected  # We use the existing values
        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=True)
        print(propfile)
        assert propfile == ''  # We drop the key

    def test_merging_untranslated_comments(self):
        """check merging untranslated entries with comments"""
        posource = '''#: prop\nmsgid "value"\nmsgstr ""\n'''
        proptemplate = '''# A comment\nprop = value\n'''
        propexpected = '# A comment\nprop = value\n'
        propfile = self.merge2prop(proptemplate, posource)
        print(propfile)
        assert propfile == propexpected  # We use the existing values
        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=True)
        print(propfile)
        # FIXME ideally we should drop the comment as well as the unit
        assert propfile == '# A comment\n'  # We drop the key

    def test_merging_untranslated_unchanged(self):
        """check removing untranslated entries but keeping unchanged ones"""
        posource = '''#: prop
msgid "value"
msgstr ""

#: prop2
msgid "value2"
msgstr "value2"
'''
        proptemplate = '''prop=value
prop2=value2
'''

        propexpected = '''prop2=value2\n'''
        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=True)
        print(propfile)
        assert propfile == propexpected

    def test_merging_blank(self):
        """We always merge in a blank translation for a blank source"""
        posource = '''#: prop
msgctxt "prop"
msgid ""
msgstr "value"

#: prop2
msgctxt "prop2"
msgid ""
msgstr ""
'''
        proptemplate = '''prop=
prop2=
'''

        propexpected = '''prop=value
prop2=
'''

        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=False)
        print(propfile)
        assert propfile == propexpected
        propfile = self.merge2prop(proptemplate, posource, remove_untranslated=True)
        print(propfile)
        assert propfile == propexpected

    def test_gaia_plurals(self):
        """Test back conversion of gaia plural units."""
        proptemplate = '''
message-multiedit-header={[ plural(n) ]}
message-multiedit-header[zero]=Edit
message-multiedit-header[one]={{ n }} selected
message-multiedit-header[two]={{ n }} selected
message-multiedit-header[few]={{ n }} selected
message-multiedit-header[many]={{ n }} selected
message-multiedit-header[other]={{ n }} selected
'''
        posource = r'''#: message-multiedit-header[zero]
msgctxt "message-multiedit-header[zero]"
msgid "Edit"
msgstr "Redigeer"

#: message-multiedit-header
msgctxt "message-multiedit-header"
msgid "Edit"
msgid_plural "{{ n }} selected"
msgstr[0] "xxxRedigeerxxx"
msgstr[1] "{{ n }} gekies"
msgstr[2] "{{ n }} gekies"
msgstr[3] "{{ n }} gekies"
msgstr[4] "{{ n }} gekies"
msgstr[5] "{{ n }} gekies"
'''
        propexpected = '''
message-multiedit-header={[ plural(n) ]}
message-multiedit-header[zero]=Redigeer
message-multiedit-header[one]={{ n }} gekies
message-multiedit-header[two]={{ n }} gekies
message-multiedit-header[few]={{ n }} gekies
message-multiedit-header[many]={{ n }} gekies
message-multiedit-header[other]={{ n }} gekies
'''
        propfile = self.merge2prop(proptemplate, posource, personality="gaia")
        assert propfile == propexpected


class TestPO2PropCommand(test_convert.TestConvertCommand, TestPO2Prop):
    """Tests running actual po2prop commands on files"""
    convertmodule = po2prop
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--personality=TYPE")
        options = self.help_check(options, "--encoding=ENCODING")
        options = self.help_check(options, "--removeuntranslated")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2sub
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import importorskip

from translate.convert import po2sub, test_convert
from translate.misc import wStringIO
from translate.storage import po


# Technically subtitles can also use an older gaupol
importorskip("aeidon")


class TestPO2Sub:

    def po2sub(self, posource):
        """helper that converts po source to subtitle source without requiring
        files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2sub.resub()
        outputsub = convertor.convertstore(inputpo)
        return outputsub

    def merge2sub(self, subsource, posource):
        """helper that merges po translations to subtitle source without
        requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        templatefile = wStringIO.StringIO(subsource)
        convertor = po2sub.resub(templatefile, inputpo)
        outputsub = convertor.convertstore()
        print(outputsub)
        return outputsub

    def test_subrip(self):
        """test SubRip or .srt files."""
        posource = u'''#: 00:00:20.000-->00:00:24.400
msgid "Altocumulus clouds occur between six thousand"
msgstr "Blah blah blah blah"

#: 00:00:24.600-->00:00:27.800
msgid "and twenty thousand feet above ground level."
msgstr "Koei koei koei koei"
'''
        subtemplate = '''1
00:00:20,000 --> 00:00:24,400
Altocumulus clouds occur between six thousand

2
00:00:24,600 --> 00:00:27,800
and twenty thousand feet above ground level.
'''
        subexpected = '''1
00:00:20,000 --> 00:00:24,400
Blah blah blah blah

2
00:00:24,600 --> 00:00:27,800
Koei koei koei koei
'''
        subfile = self.merge2sub(subtemplate, posource)
        print(subexpected)
        assert subfile == subexpected


class TestPO2SubCommand(test_convert.TestConvertCommand, TestPO2Sub):
    """Tests running actual po2sub commands on files"""
    convertmodule = po2sub
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy", last=True)

########NEW FILE########
__FILENAME__ = test_po2tiki
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# po2tiki unit tests
# Author: Wil Clouser <wclouser@mozilla.com>
# Date: 2008-12-01

from translate.convert import po2tiki, test_convert
from translate.misc import wStringIO


class TestPo2Tiki:

    def test_convertpo(self):
        inputfile = """
#: translated
msgid "zero_source"
msgstr "zero_target"

#: unused
msgid "one_source"
msgstr "one_target"
        """
        outputfile = wStringIO.StringIO()
        po2tiki.convertpo(inputfile, outputfile)

        output = outputfile.getvalue()

        assert '"one_source" => "one_target",' in output
        assert '"zero_source" => "zero_target",' in output


class TestPo2TikiCommand(test_convert.TestConvertCommand, TestPo2Tiki):
    """Tests running actual po2tiki commands on files"""
    convertmodule = po2tiki
    defaultoptions = {}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)

########NEW FILE########
__FILENAME__ = test_po2tmx
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import po2tmx, test_convert
from translate.misc import wStringIO
from translate.misc.xml_helpers import XML_NS
from translate.storage import tmx


class TestPO2TMX:

    def po2tmx(self, posource, sourcelanguage='en', targetlanguage='af',
               comment=None):
        """helper that converts po source to tmx source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        outputfile = wStringIO.StringIO()
        outputfile.tmxfile = tmx.tmxfile(inputfile=None, sourcelanguage=sourcelanguage)
        po2tmx.convertpo(inputfile, outputfile, templatefile=None,
                         sourcelanguage=sourcelanguage,
                         targetlanguage=targetlanguage, comment=comment)
        return outputfile.tmxfile

    def test_basic(self):
        minipo = r"""# Afrikaans translation of program ABC
#
msgid ""
msgstr ""
"Project-Id-Version: program 2.1-branch\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2006-01-09 07:15+0100\n"
"PO-Revision-Date: 2004-03-30 17:02+0200\n"
"Last-Translator: Zuza Software Foundation <xxx@translate.org.za>\n"
"Language-Team: Afrikaans <translate-discuss-xxx@lists.sourceforge.net>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

# Please remember to do something
#: ../dir/file.xml.in.h:1 ../dir/file2.xml.in.h:4
msgid "Applications"
msgstr "Toepassings"
"""
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert tmx.translate("Applications") == "Toepassings"
        assert tmx.translate("bla") is None
        xmltext = str(tmx)
        assert xmltext.index('creationtool="Translate Toolkit - po2tmx"')
        assert xmltext.index('adminlang')
        assert xmltext.index('creationtoolversion')
        assert xmltext.index('datatype')
        assert xmltext.index('o-tmf')
        assert xmltext.index('segtype')
        assert xmltext.index('srclang')

    def test_sourcelanguage(self):
        minipo = 'msgid "String"\nmsgstr "String"\n'
        tmx = self.po2tmx(minipo, sourcelanguage="xh")
        print("The generated xml:")
        print(str(tmx))
        header = tmx.document.find("header")
        assert header.get("srclang") == "xh"

    def test_targetlanguage(self):
        minipo = 'msgid "String"\nmsgstr "String"\n'
        tmx = self.po2tmx(minipo, targetlanguage="xh")
        print("The generated xml:")
        print(str(tmx))
        tuv = tmx.document.findall(".//%s" % tmx.namespaced("tuv"))[1]
        #tag[0] will be the source, we want the target tuv
        assert tuv.get("{%s}lang" % XML_NS) == "xh"

    def test_multiline(self):
        """Test multiline po entry"""
        minipo = r'''msgid "First part "
"and extra"
msgstr "Eerste deel "
"en ekstra"'''
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert tmx.translate('First part and extra') == 'Eerste deel en ekstra'

    def test_escapednewlines(self):
        """Test the escaping of newlines"""
        minipo = r'''msgid "First line\nSecond line"
msgstr "Eerste lyn\nTweede lyn"
'''
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert tmx.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"

    def test_escapedtabs(self):
        """Test the escaping of tabs"""
        minipo = r'''msgid "First column\tSecond column"
msgstr "Eerste kolom\tTweede kolom"
'''
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert tmx.translate("First column\tSecond column") == "Eerste kolom\tTweede kolom"

    def test_escapedquotes(self):
        """Test the escaping of quotes (and slash)"""
        minipo = r'''msgid "Hello \"Everyone\""
msgstr "Good day \"All\""

msgid "Use \\\"."
msgstr "Gebruik \\\"."
'''
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert tmx.translate('Hello "Everyone"') == 'Good day "All"'
        assert tmx.translate(r'Use \".') == r'Gebruik \".'

    def test_exclusions(self):
        """Test that empty and fuzzy messages are excluded"""
        minipo = r'''#, fuzzy
msgid "One"
msgstr "Een"

msgid "Two"
msgstr ""

msgid ""
msgstr "Drie"
'''
        tmx = self.po2tmx(minipo)
        print("The generated xml:")
        print(str(tmx))
        assert "<tu" not in str(tmx)
        assert len(tmx.units) == 0

    def test_nonascii(self):
        """Tests that non-ascii conversion works."""
        minipo = r'''msgid "Bzier curve"
msgstr "Bzier-kurwe"
'''
        tmx = self.po2tmx(minipo)
        print(str(tmx))
        assert tmx.translate(u"Bzier curve") == u"Bzier-kurwe"

    def test_nonecomments(self):
        """Tests that none comments are imported."""
        minipo = r'''#My comment rules
msgid "Bzier curve"
msgstr "Bzier-kurwe"
'''
        tmx = self.po2tmx(minipo)
        print(str(tmx))
        unit = tmx.findunits(u"Bzier curve")
        assert len(unit[0].getnotes()) == 0

    def test_otherscomments(self):
        """Tests that others comments are imported."""
        minipo = r'''#My comment rules
msgid "Bzier curve"
msgstr "Bzier-kurwe"
'''
        tmx = self.po2tmx(minipo, comment='others')
        print(str(tmx))
        unit = tmx.findunits(u"Bzier curve")
        assert unit[0].getnotes() == u"My comment rules"

    def test_sourcecomments(self):
        """Tests that source comments are imported."""
        minipo = r'''#: ../PuzzleFourSided.h:45
msgid "Bzier curve"
msgstr "Bzier-kurwe"
'''
        tmx = self.po2tmx(minipo, comment='source')
        print(str(tmx))
        unit = tmx.findunits(u"Bzier curve")
        assert unit[0].getnotes() == u"../PuzzleFourSided.h:45"

    def test_typecomments(self):
        """Tests that others comments are imported."""
        minipo = r'''#, csharp-format
msgid "Bzier curve"
msgstr "Bzier-kurwe"
'''
        tmx = self.po2tmx(minipo, comment='type')
        print(str(tmx))
        unit = tmx.findunits(u"Bzier curve")
        assert unit[0].getnotes() == u"csharp-format"


class TestPO2TMXCommand(test_convert.TestConvertCommand, TestPO2TMX):
    """Tests running actual po2tmx commands on files"""
    convertmodule = po2tmx

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-l LANG, --language=LANG")
        options = self.help_check(options, "--source-language=LANG")
        options = self.help_check(options, "--comments", last=True)

########NEW FILE########
__FILENAME__ = test_po2ts
#!/usr/bin/env python

from translate.convert import po2ts, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPO2TS:

    def po2ts(self, posource):
        """helper that converts po source to ts source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        inputpo = po.pofile(inputfile)
        convertor = po2ts.po2ts()
        outputts = convertor.convertstore(inputpo)
        return outputts

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(storage.units) == 1
        return storage.units[0]

    def test_simpleunit(self):
        """checks that a simple po entry definition converts properly to a ts entry"""
        minipo = r'''#: term.cpp
msgid "Term"
msgstr "asdf"'''
        tsfile = self.po2ts(minipo)
        print(tsfile)
        assert "<name>term.cpp</name>" in tsfile
        assert "<source>Term</source>" in tsfile
        assert "<translation>asdf</translation>" in tsfile
        assert "<comment>" not in tsfile

    def test_fullunit(self):
        """check that an entry with various settings is converted correctly"""
        posource = '''# Translator comment
#. Automatic comment
#: location.cpp:100
msgid "Source"
msgstr "Target"
'''
        tsfile = self.po2ts(posource)
        print(tsfile)
        # The other section are a duplicate of test_simplentry
        # FIXME need to think about auto vs trans comments maybe in TS v1.1
        assert "<comment>Translator comment</comment>" in tsfile

    def test_fuzzyunit(self):
        """check that we handle fuzzy units correctly"""
        posource = '''#: term.cpp
#, fuzzy
msgid "Source"
msgstr "Target"'''
        tsfile = self.po2ts(posource)
        print(tsfile)
        assert '''<translation type="unfinished">Target</translation>''' in tsfile

    def test_obsolete(self):
        """test that we can take back obsolete messages"""
        posource = '''#. (obsolete)
#: term.cpp
msgid "Source"
msgstr "Target"'''
        tsfile = self.po2ts(posource)
        print(tsfile)
        assert '''<translation type="obsolete">Target</translation>''' in tsfile

    def test_duplicates(self):
        """test that we can handle duplicates in the same context block"""
        posource = '''#: @@@#1
msgid "English"
msgstr "a"

#: @@@#3
msgid "English"
msgstr "b"
'''
        tsfile = self.po2ts(posource)
        print(tsfile)
        assert tsfile.find("English") != tsfile.rfind("English")


class TestPO2TSCommand(test_convert.TestConvertCommand, TestPO2TS):
    """Tests running actual po2ts commands on files"""
    convertmodule = po2ts

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-c CONTEXT, --context=CONTEXT")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE", last=True)

########NEW FILE########
__FILENAME__ = test_po2txt
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import po2txt, test_convert
from translate.misc import wStringIO


class TestPO2Txt:

    def po2txt(self, posource, txttemplate=None):
        """helper that converts po source to txt source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        print(inputfile.getvalue())
        outputfile = wStringIO.StringIO()
        if txttemplate:
            templatefile = wStringIO.StringIO(txttemplate)
        else:
            templatefile = None
        assert po2txt.converttxt(inputfile, outputfile, templatefile)
        print(outputfile.getvalue())
        return outputfile.getvalue()

    def test_basic(self):
        """test basic conversion"""
        txttemplate = "Heading\n\nBody text"
        posource = 'msgid "Heading"\nmsgstr "Opskrif"\n\nmsgid "Body text"\nmsgstr "Lyfteks"\n'
        assert self.po2txt(posource, txttemplate) == "Opskrif\n\nLyfteks"

    def test_nonascii(self):
        """test conversion with non-ascii text"""
        txttemplate = "Heading\n\nFile content"
        posource = 'msgid "Heading"\nmsgstr "Opskrif"\n\nmsgid "File content"\nmsgstr "Lerinhoud"\n'
        assert self.po2txt(posource, txttemplate) == "Opskrif\n\nLerinhoud"

    def test_blank_handling(self):
        """check that we discard blank messages"""
        txttemplate = "Heading\n\nBody text"
        posource = 'msgid "Heading"\nmsgstr "Opskrif"\n\nmsgid "Body text"\nmsgstr ""\n'
        assert self.po2txt(posource) == "Opskrif\n\nBody text"
        assert self.po2txt(posource, txttemplate) == "Opskrif\n\nBody text"

    def test_fuzzy_handling(self):
        """check that we handle fuzzy message correctly"""
        txttemplate = "Heading\n\nBody text"
        posource = '#, fuzzy\nmsgid "Heading"\nmsgstr "Opskrif"\n\nmsgid "Body text"\nmsgstr "Lyfteks"\n'
        assert self.po2txt(posource) == "Heading\n\nLyfteks"
        assert self.po2txt(posource, txttemplate) == "Heading\n\nLyfteks"


class TestPO2TxtCommand(test_convert.TestConvertCommand, TestPO2Txt):
    """Tests running actual po2txt commands on files"""
    convertmodule = po2txt
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--threshold=PERCENT")
        options = self.help_check(options, "--fuzzy")
        options = self.help_check(options, "--nofuzzy")
        options = self.help_check(options, "--encoding")
        options = self.help_check(options, "-w WRAP, --wrap=WRAP", last=True)

########NEW FILE########
__FILENAME__ = test_po2xliff
#!/usr/bin/env python

from translate.convert import po2xliff
from translate.misc.xml_helpers import XML_NS, getText
from translate.storage import po, poxliff


class TestPO2XLIFF:

    def po2xliff(self, posource, sourcelanguage='en', targetlanguage=None):
        """helper that converts po source to xliff source without requiring files"""
        postore = po.pofile(posource)
        convertor = po2xliff.po2xliff()
        outputxliff = convertor.convertstore(postore, None, sourcelanguage=sourcelanguage, targetlanguage=targetlanguage)
        return poxliff.PoXliffFile(outputxliff)

    def getnode(self, xliff):
        """Retrieves the trans-unit node from the dom"""
        assert len(xliff.units) == 1
        unit = xliff.units[0]
        return unit

    def test_minimal(self):
        minipo = '''msgid "red"\nmsgstr "rooi"\n'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        print(str(xliff))
        assert len(xliff.units) == 1
        assert xliff.translate("red") == "rooi"
        assert xliff.translate("bla") is None

    def test_basic(self):
        minipo = r"""# Afrikaans translation of program ABC
#
msgid ""
msgstr ""
"Project-Id-Version: program 2.1-branch\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2006-01-09 07:15+0100\n"
"PO-Revision-Date: 2004-03-30 17:02+0200\n"
"Last-Translator: Zuza Software Foundation <xxx@translate.org.za>\n"
"Language-Team: Afrikaans <translate-discuss-xxx@lists.sourceforge.net>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

# Please remember to do something
#: ../dir/file.xml.in.h:1 ../dir/file2.xml.in.h:4
msgid "Applications"
msgstr "Toepassings"
"""
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        print(str(xliff))
        assert xliff.translate("Applications") == "Toepassings"
        assert xliff.translate("bla") is None
        xmltext = str(xliff)
        assert xmltext.index('<xliff ') >= 0
        assert xmltext.index(' version="1.1"') >= 0
        assert xmltext.index('<file')
        assert xmltext.index('source-language')
        assert xmltext.index('datatype')

    def test_multiline(self):
        """Test multiline po entry"""
        minipo = r'''msgid "First part "
"and extra"
msgstr "Eerste deel "
"en ekstra"'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        print(str(xliff))
        assert xliff.translate('First part and extra') == 'Eerste deel en ekstra'

    def test_escapednewlines(self):
        """Test the escaping of newlines"""
        minipo = r'''msgid "First line\nSecond line"
msgstr "Eerste lyn\nTweede lyn"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"
        assert xliff.translate("First line\\nSecond line") is None
        assert xmltext.find("line\\nSecond") == -1
        assert xmltext.find("lyn\\nTweede") == -1
        assert xmltext.find("line\nSecond") > 0
        assert xmltext.find("lyn\nTweede") > 0

    def test_escapedtabs(self):
        """Test the escaping of tabs"""
        minipo = r'''msgid "First column\tSecond column"
msgstr "Eerste kolom\tTweede kolom"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate("First column\tSecond column") == "Eerste kolom\tTweede kolom"
        assert xliff.translate("First column\\tSecond column") is None
        assert xmltext.find("column\\tSecond") == -1
        assert xmltext.find("kolom\\tTweede") == -1
        assert xmltext.find("column\tSecond") > 0
        assert xmltext.find("kolom\tTweede") > 0

    def test_escapedquotes(self):
        """Test the escaping of quotes (and slash)"""
        minipo = r'''msgid "Hello \"Everyone\""
msgstr "Good day \"All\""

msgid "Use \\\"."
msgstr "Gebruik \\\"."
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate('Hello "Everyone"') == 'Good day "All"'
        assert xliff.translate(r'Use \".') == r'Gebruik \".'
        assert xmltext.find(r'\&quot;') > 0 or xmltext.find(r'\"') > 0
        assert xmltext.find(r"\\") == -1

    def getcontexttuples(self, node, namespace):
        """Returns all the information in the context nodes as a list of tuples
        of (type, text)"""
        contexts = node.findall(".//{%s}context" % namespace)
        return [(context.get("context-type"), getText(context)) for context in contexts]

    def test_locationcomments(self):
        minipo = r'''#: file.c:123 asdf.c
msgid "one"
msgstr "kunye"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate("one") == "kunye"
        assert len(xliff.units) == 1
        node = xliff.units[0].xmlelement
        contextgroups = node.findall(".//%s" % xliff.namespaced("context-group"))
        assert len(contextgroups) == 2
        for group in contextgroups:
            assert group.get("name") == "po-reference"
            assert group.get("purpose") == "location"
        tuples = self.getcontexttuples(node, xliff.namespace)
        assert tuples == [("sourcefile", "file.c"), ("linenumber", "123"), ("sourcefile", "asdf.c")]

    def test_othercomments(self):
        minipo = r'''# Translate?
# How?
msgid "one"
msgstr "kunye"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate("one") == "kunye"
        assert len(xliff.units) == 1
        node = xliff.units[0].xmlelement
        contextgroups = node.findall(".//%s" % xliff.namespaced("context-group"))
        assert len(contextgroups) == 1
        for group in contextgroups:
            assert group.get("name") == "po-entry"
            assert group.get("purpose") == "information"
        tuples = self.getcontexttuples(node, xliff.namespace)
        assert tuples == [("x-po-trancomment", "Translate?\nHow?")]

        assert xliff.units[0].getnotes("translator") == "Translate?\nHow?"

    def test_automaticcomments(self):
        minipo = r'''#. Don't translate.
#. Please
msgid "one"
msgstr "kunye"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert xliff.translate("one") == "kunye"
        assert len(xliff.units) == 1
        node = xliff.units[0].xmlelement
        contextgroups = node.findall(".//%s" % xliff.namespaced("context-group"))
        assert len(contextgroups) == 1
        for group in contextgroups:
            assert group.get("name") == "po-entry"
            assert group.get("purpose") == "information"
        tuples = self.getcontexttuples(node, xliff.namespace)
        assert tuples == [("x-po-autocomment", "Don't translate.\nPlease")]

    def test_header(self):
        minipo = r'''# Pulana  Translation for bla
# Hallo Ma!
#, fuzzy
msgid ""
msgstr ""
"Content-Type: text/plain; charset=UTF-8\n"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert len(xliff.units) == 1
        unit = xliff.units[0]
        assert unit.source == unit.target == "Content-Type: text/plain; charset=UTF-8\n"
        assert unit.xmlelement.get("restype") == "x-gettext-domain-header"
        assert unit.xmlelement.get("approved") != "yes"
        assert unit.xmlelement.get("{%s}space" % XML_NS) == "preserve"
        assert unit.getnotes("po-translator") == "Pulana  Translation for bla\nHallo Ma!"

    def test_fuzzy(self):
        minipo = r'''#, fuzzy
msgid "two"
msgstr "pedi"

msgid "three"
msgstr "raro"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert len(xliff.units) == 2
        assert xliff.units[0].isfuzzy()
        assert not xliff.units[1].isfuzzy()

    def test_germanic_plurals(self):
        minipo = r'''msgid "cow"
msgid_plural "cows"
msgstr[0] "inkomo"
msgstr[1] "iinkomo"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert len(xliff.units) == 1
        assert xliff.translate("cow") == "inkomo"

    def test_funny_plurals(self):
        minipo = r'''msgid "cow"
msgid_plural "cows"
msgstr[0] "inkomo"
msgstr[1] "iinkomo"
msgstr[2] "iiinkomo"
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert len(xliff.units) == 1
        assert xliff.translate("cow") == "inkomo"

    def test_language_tags(self):
        minipo = r'''msgid "Een"
msgstr "Uno"
'''
        xliff = self.po2xliff(minipo, "af", "es")
        assert xliff.sourcelanguage == "af"
        assert xliff.targetlanguage == "es"

    def test_variables(self):
        minipo = r'''msgid "%s%s%s%s has made %s his or her buddy%s%s"
msgstr "%s%s%s%s het %s sy/haar vriend/vriendin gemaak%s%s"'''
        xliff = self.po2xliff(minipo)
        print(xliff.units[0].source)
        assert xliff.units[0].source == "%s%s%s%s has made %s his or her buddy%s%s"

    def test_approved(self):
        minipo = r'''#, fuzzy
msgid "two"
msgstr "pedi"

msgid "three"
msgstr "raro"

msgid "four"
msgstr ""
'''
        xliff = self.po2xliff(minipo)
        print("The generated xml:")
        xmltext = str(xliff)
        print(xmltext)
        assert len(xliff.units) == 3
        assert xliff.units[0].xmlelement.get("approved") != "yes"
        assert not xliff.units[0].isapproved()
        assert xliff.units[1].xmlelement.get("approved") == "yes"
        assert xliff.units[1].isapproved()
        assert xliff.units[2].xmlelement.get("approved") != "yes"
        assert not xliff.units[2].isapproved()

########NEW FILE########
__FILENAME__ = test_pot2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import warnings

from pytest import mark

from translate.convert import pot2po, test_convert
from translate.misc import wStringIO
from translate.storage import po


class TestPOT2PO:

    def setup_method(self, method):
        warnings.resetwarnings()

    def teardown_method(self, method):
        warnings.resetwarnings()

    def convertpot(self, potsource, posource=None):
        """helper that converts pot source to po source without requiring files"""
        potfile = wStringIO.StringIO(potsource)
        if posource:
            pofile = wStringIO.StringIO(posource)
        else:
            pofile = None
        pooutfile = wStringIO.StringIO()
        pot2po.convertpot(potfile, pooutfile, pofile)
        pooutfile.seek(0)
        return po.pofile(pooutfile.read())

    def singleunit(self, pofile):
        """checks that the pofile contains a single non-header unit, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile.units[1])
        return pofile.units[1]

    def test_convertpot_blank(self):
        """checks that the convertpot function is working for a simple file initialisation"""
        potsource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        newpo = self.convertpot(potsource)
        assert str(self.singleunit(newpo)) == potsource

    def test_convertpot_blank_plurals(self):
        """checks that the convertpot function is working for initialising plurals correctly"""
        potsource = r'''msgid ""
msgstr""

msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] ""
msgstr[1] ""
'''
        posource = r'''msgid ""
msgstr""
"Plural-Forms: nplurals=1; plural=0;\n"
'''

        poexpected = r'''msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] ""
'''
        newpo = self.convertpot(potsource, posource)
        assert str(self.singleunit(newpo)) == poexpected

    def test_merging_simple(self):
        """checks that the convertpot function is working for a simple merge"""
        potsource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        posource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.convertpot(potsource, posource)
        assert str(self.singleunit(newpo)) == posource

    def test_merging_messages_marked_fuzzy(self):
        """test that when we merge PO files with a fuzzy message that it remains fuzzy"""
        potsource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        posource = '''#: simple.label%ssimple.accesskey\n#, fuzzy\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.convertpot(potsource, posource)
        assert str(self.singleunit(newpo)) == posource

    def test_merging_plurals_with_fuzzy_matching(self):
        """test that when we merge PO files with a fuzzy message that it remains fuzzy"""
        potsource = r'''#: file.cpp:2
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] ""
msgstr[1] ""
'''
        posource = r'''#: file.cpp:3
#, fuzzy
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] "%d handleiding."
msgstr[1] "%d handleidings."
'''
        # The #: comment and msgid's are different between the pot and the po
        poexpected = r'''#: file.cpp:2
#, fuzzy
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] "%d handleiding."
msgstr[1] "%d handleidings."
'''
        newpo = self.convertpot(potsource, posource)
        assert str(self.singleunit(newpo)) == poexpected

    @mark.xfail(reason="Not implemented - review if this is even correct")
    def test_merging_msgid_change(self):
        """tests that if the msgid changes but the location stays the same that we merge"""
        potsource = '''#: simple.label\n#: simple.accesskey\nmsgid "Its &hard coding a newline.\\n"\nmsgstr ""\n'''
        posource = '''#: simple.label\n#: simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
        poexpected = '''#: simple.label\n#: simple.accesskey\n#, fuzzy\nmsgid "Its &hard coding a newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert str(self.singleunit(newpo)) == poexpected

    def test_merging_location_change(self):
        """tests that if the location changes but the msgid stays the same that we merge"""
        potsource = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        posource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        poexpected = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert str(self.singleunit(newpo)) == poexpected

    def test_merging_location_and_whitespace_change(self):
        """test that even if the location changes that if the msgid
        only has whitespace changes we can still merge"""

        potsource = '''#: singlespace.label%ssinglespace.accesskey\nmsgid "&We have spaces"\nmsgstr ""\n''' % po.lsep
        posource = '''#: doublespace.label%sdoublespace.accesskey\nmsgid "&We  have  spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
        poexpected = '''#: singlespace.label%ssinglespace.accesskey\n#, fuzzy\nmsgid "&We have spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert str(self.singleunit(newpo)) == poexpected

    def test_merging_location_ambiguous_with_disambiguous(self):
        """test that when we have a PO in ambiguous (Gettext form) and merge with disamabiguous (KDE comment form)
        that we don't duplicate the location #: comments"""
        potsource = '''#: location.c:1\nmsgid ""\n"_: location.c:1\\n"\n"Source"\nmsgstr ""\n\n''' + \
                    '''#: location.c:10\nmsgid ""\n"_: location.c:10\\n"\n"Source"\nmsgstr ""\n'''
        posource = '''#: location.c:1\n#: location.c:10\nmsgid "Source"\nmsgstr "Target"\n\n'''
        poexpected1 = '''#: location.c:1\n#, fuzzy\nmsgid ""\n"_: location.c:1\\n"\n"Source"\nmsgstr "Target"\n'''
        poexpected2 = '''#: location.c:10\n#, fuzzy\nmsgid ""\n"_: location.c:10\\n"\n"Source"\nmsgstr "Target"\n'''
        newpo = self.convertpot(potsource, posource)
        print("Expected:\n", poexpected1, "Actual:\n", newpo.units[1])
        assert str(newpo.units[1]) == poexpected1
        assert str(newpo.units[2]) == poexpected2

    @mark.xfail(reason="Not Implemented - needs review")
    def test_merging_accelerator_changes(self):
        """test that a change in the accelerator localtion still allows merging"""
        potsource = '''#: someline.c\nmsgid "A&bout"\nmsgstr ""\n'''
        posource = '''#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        poexpected = '''#: someline.c\nmsgid "A&bout"\nmsgstr "&Info"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert str(self.singleunit(newpo)) == poexpected

    @mark.xfail(reason="Not Implemented - review if this is even correct")
    def test_lines_cut_differently(self):
        """Checks that the correct formatting is preserved when pot an po lines differ."""
        potsource = '''#: simple.label\nmsgid "Line split "\n"differently"\nmsgstr ""\n'''
        posource = '''#: simple.label\nmsgid "Line"\n" split differently"\nmsgstr "Lyne verskillend gesny"\n'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == posource

    def test_merging_automatic_comments_dont_duplicate(self):
        """ensure that we can merge #. comments correctly"""
        potsource = '''#. Row 35\nmsgid "&About"\nmsgstr ""\n'''
        posource = '''#. Row 35\nmsgid "&About"\nmsgstr "&Info"\n'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == posource

    def test_merging_automatic_comments_new_overides_old(self):
        """ensure that new #. comments override the old comments"""
        potsource = '''#. new comment\n#: someline.c\nmsgid "&About"\nmsgstr ""\n'''
        posource = '''#. old comment\n#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        poexpected = '''#. new comment\n#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_merging_comments_with_blank_comment_lines(self):
        """test that when we merge a comment that has a blank line we keep the blank line"""
        potsource = '''#: someline.c\nmsgid "About"\nmsgstr ""\n'''
        posource = '''# comment1\n#\n# comment2\n#: someline.c\nmsgid "About"\nmsgstr "Omtrent"\n'''
        poexpected = posource
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_empty_commentlines(self):
        potsource = '''#: paneSecurity.title
msgid "Security"
msgstr ""
'''
        posource = '''# - Contributor(s):
# -
# - Alternatively, the
# -
#: paneSecurity.title
msgid "Security"
msgstr "Sekuriteit"
'''
        poexpected = posource
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        print("expected")
        print(poexpected)
        print("got:")
        print(str(newpounit))
        assert str(newpounit) == poexpected

    def test_merging_msgidcomments(self):
        """ensure that we can merge msgidcomments messages"""
        potsource = r'''#: window.width
msgid ""
"_: Do not translate this.\n"
"36em"
msgstr ""
'''
        posource = r'''#: window.width
msgid ""
"_: Do not translate this.\n"
"36em"
msgstr "36em"
'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == posource

    def test_merging_msgid_with_msgidcomment(self):
        """test that we can merge an otherwise identical string that has a different msgid"""
        potsource = r'''#: pref.certs.title
msgid ""
"_: pref.certs.title\n"
"Certificates"
msgstr ""

#: certs.label
msgid ""
"_: certs.label\n"
"Certificates"
msgstr ""
'''
        posource = r'''#: pref.certs.title
msgid ""
"_: pref.certs.title\n"
"Certificates"
msgstr ""

#: certs.label
msgid ""
"_: certs.label\n"
"Certificates"
msgstr "Sertifikate"
'''
        expected = r'''#: pref.certs.title
#, fuzzy
msgid ""
"_: pref.certs.title\n"
"Certificates"
msgstr "Sertifikate"
'''
        newpo = self.convertpot(potsource, posource)
        newpounit = newpo.units[1]
        assert str(newpounit) == expected

    def test_merging_plurals(self):
        """ensure that we can merge plural messages"""
        potsource = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] ""\nmsgstr[1] ""\n'''
        posource = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] "Een"\nmsgstr[1] "Twee"\nmsgstr[2] "Drie"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == posource

    def test_merging_obsoleting_messages(self):
        """check that we obsolete messages no longer present in the new file"""
        #add emtpy msgid line to help factory identify format
        potsource = 'msgid ""\nmsgstr ""\n'
        posource = '# Some comment\n#. Extracted comment\n#: obsoleteme:10\nmsgid "One"\nmsgstr "Een"\n'
        expected = '# Some comment\n#~ msgid "One"\n#~ msgstr "Een"\n'
        newpo = self.convertpot(potsource, posource)
        print(str(newpo))
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == expected

    def test_not_obsoleting_empty_messages(self):
        """check that we don't obsolete (and keep) untranslated messages"""
        #add emtpy msgid line to help factory identify format
        potsource = 'msgid ""\nmsgstr ""\n'
        posource = '#: obsoleteme:10\nmsgid "One"\nmsgstr ""\n'
        newpo = self.convertpot(potsource, posource)
        print(str(newpo))
        # We should only have the header
        assert len(newpo.units) == 1

    def test_merging_new_before_obsolete(self):
        """test to check that we place new blank message before obsolete messages"""
        potsource = '''#: newline.c\nmsgid "&About"\nmsgstr ""\n'''
        posource = '''#~ msgid "Old"\n#~ msgstr "Oud"\n'''
        newpo = self.convertpot(potsource, posource)
        assert len(newpo.units) == 3
        assert newpo.units[0].isheader()
        assert newpo.units[2].isobsolete()
        assert str(newpo.units[1]) == potsource
        assert str(newpo.units[2]) == posource

        # Now test with real units present in posource
        posource2 = '''msgid "Old"\nmsgstr "Oud"\n'''
        newpo = self.convertpot(potsource, posource)
        assert len(newpo.units) == 3
        assert newpo.units[0].isheader()
        assert newpo.units[2].isobsolete()
        assert str(newpo.units[1]) == potsource
        assert str(newpo.units[2]) == posource

    def test_merging_resurect_obsolete_messages(self):
        """check that we can reuse old obsolete messages if the message comes back"""
        potsource = '''#: resurect.c\nmsgid "&About"\nmsgstr ""\n'''
        posource = '''#~ msgid "&About"\n#~ msgstr "&Omtrent"\n'''
        expected = '''#: resurect.c\nmsgid "&About"\nmsgstr "&Omtrent"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert len(newpo.units) == 2
        assert newpo.units[0].isheader()
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == expected

    def test_merging_resurect_obsolete_messages_into_msgidcomment(self):
        """check that we can reuse old obsolete messages even if the recipient has a msgidcomment"""
        potsource = '''#: resurect1.c\nmsgid "About"\nmsgstr ""\n\n''' + \
                    '''#: resurect2.c\nmsgid ""\n"_: resurect2.c\\n"\n"About"\nmsgstr ""\n'''
        posource = '''#~ msgid "About"\n#~ msgstr "Omtrent"\n'''
        expected1 = '''#: resurect1.c\nmsgid "About"\nmsgstr "Omtrent"\n'''
        expected2 = '''#: resurect2.c\n#, fuzzy\nmsgid ""\n"_: resurect2.c\\n"\n"About"\nmsgstr "Omtrent"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert len(newpo.units) == 3
        assert newpo.units[0].isheader()
        assert str(newpo.units[1]) == expected1
        assert str(newpo.units[2]) == expected2

    def test_header_initialisation(self):
        """test to check that we initialise the header correctly"""
        potsource = r'''#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;\n"
"X-Generator: Translate Toolkit 0.10rc2\n"
'''
        posource = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: old@example.com\n"
"POT-Creation-Date: 2006-01-01 01:01+0100\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.9\n"
'''
        expected = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.10rc2\n"
'''
        newpo = self.convertpot(potsource, posource)
        print('Output Header:\n%s' % newpo)
        print('Expected Header:\n%s' % expected)
        assert str(newpo) == expected

    def test_merging_comments(self):
        """Test that we can merge comments correctly"""
        potsource = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr ""\n'''
        posource = '''#. Don't do it!\n#: file.py:2\nmsgid "One"\nmsgstr "Een"\n'''
        poexpected = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr "Een"\n'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_merging_typecomments(self):
        """Test that we can merge with typecomments"""
        potsource = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr ""\n'''
        posource = '''#: file.c:2\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
        poexpected = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        print(newpounit)
        assert str(newpounit) == poexpected

        potsource = '''#: file.c:1\n#, c-format\nmsgid "%d computers"\nmsgstr ""\n'''
        posource = '''#: file.c:2\n#, c-format\nmsgid "%s computers "\nmsgstr "%s-rekenaars"\n'''
        poexpected = '''#: file.c:1\n#, fuzzy, c-format\nmsgid "%d computers"\nmsgstr "%s-rekenaars"\n'''
        newpo = self.convertpot(potsource, posource)
        newpounit = self.singleunit(newpo)
        assert newpounit.isfuzzy()
        assert newpounit.hastypecomment("c-format")

    def test_msgctxt(self):
        """Test that msgctxt is migrated correctly"""
        potsource = """
#: something.h:5
msgctxt "context1"
msgid "text"
msgstr ""

#: something.h:6
msgctxt "context2"
msgid "text"
msgstr ""
"""
        posource = """
#: something.h:3
msgctxt "context0"
msgid "text"
msgstr "teks"

#: something.h:4
msgctxt "context1"
msgid "text"
msgstr "sms"
"""
        poexpected = """
#: something.h:5
msgctxt "context1"
msgid "text"
msgstr "sms"

#: something.h:6
#, fuzzy
msgctxt "context2"
msgid "text"
msgstr "teks"
"""
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert poexpected in str(newpo)

    def test_msgctxt_multiline(self):
        """Test multiline msgctxt fields."""
        pot_source = r'''#. |1MV
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_21\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""
'''

        po_source = r'''#. |1MV
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_21\n"
"LngText.text"
msgid "Computing space requirements"
msgstr "A szksges lemezterlet kiszmtsa"
'''

        new_po = self.convertpot(pot_source, po_source)
        assert new_po.units[0].isheader()
        unit = new_po.units[1]
        assert not unit.isfuzzy()
        assert po_source == str(unit)

    def test_msgid_merge_on_location(self):
        """Tests that unit merges rely on location-based matching."""
        pot_source = r'''
msgid ""
msgstr ""
"X-Merge-On: location\n"

#. $:am
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_11\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""

#. NjJ3
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_12\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""
'''

        po_source = r'''
#. $:am
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_11\n"
"LngText.text"
msgid "Computing space requirements"
msgstr "A szksges lemezterlet kiszmtsa"

#. NjJ3
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_12\n"
"LngText.text"
msgid "Computing space requirements"
msgstr "A szksges lemezterlet kiszmtsa"
'''

        new_po = self.convertpot(pot_source, po_source)

        assert len(new_po.units) == 3
        assert new_po.units[0].isheader()

        assert not new_po.units[1].isfuzzy()
        assert new_po.units[2].isfuzzy()

    def test_msgid_merge_on_id(self):
        """Tests that unit merges rely on location-based matching."""
        pot_source = r'''
msgid ""
msgstr ""
"X-Merge-On: id\n"

#. $:am
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_11\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""

#. NjJ3
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_12\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""
'''
        pot_source_noheader = r'''
#. $:am
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_11\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""

#. NjJ3
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_12\n"
"LngText.text"
msgid "Computing space requirements"
msgstr ""
'''

        po_source = r'''
#. $:am
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_11\n"
"LngText.text"
msgid "Computing space requirements"
msgstr "A szksges lemezterlet kiszmtsa"

#. NjJ3
#: ActionTe.ulf
msgctxt ""
"ActionTe.ulf\n"
"OOO_ACTIONTEXT_12\n"
"LngText.text"
msgid "Computing space requirements"
msgstr "A szksges lemezterlet kiszmtsa"
'''

        for pot in (pot_source, pot_source_noheader):
            new_po = self.convertpot(pot, po_source)

            assert len(new_po.units) == 3
            assert new_po.units[0].isheader()

            assert not new_po.units[1].isfuzzy()
            assert not new_po.units[2].isfuzzy()

    def test_empty_msgid(self):
        """Test that we handle empty msgids correctly."""
        #TODO: this test will fail if we don't have the gettext location
        # comment in the pot file
        potsource = '#: file:1\nmsgctxt "bla"\nmsgid ""\nmsgstr ""\n'
        posource = r"""
msgid ""
"Project-Id-Version: Pootle 0.10\n"
msgstr ""

msgctxt "bla"
msgid ""
msgstr "trans"
"""
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert len(newpo.units) == 2
        assert newpo.units[0].isheader()
        unit = newpo.units[1]
        assert unit.source == u""
        assert unit.getid() == u"bla\04"
        assert unit.target == "trans"
        assert not unit.isfuzzy()

    def test_migrate_msgidcomment_to_msgctxt(self):
        """Test that we migrate correctly from msgidcomments to msgctxt.

        This is needed for our move away from using msgidcomments for mozilla."""
        potsource = r'''
msgid ""
msgstr ""
"X-Merge-On: location\n"
"X-Accelerator-Marker: &\n"


#: bla
msgctxt "bla"
msgid ""
msgstr ""
'''
        posource = r'''
msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"X-Accelerator-Marker: &\n"


#: bla
msgid ""
"_: bla\n"
msgstr "trans"
'''
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert len(newpo.units) == 2
        assert newpo.units[0].isheader()
        unit = newpo.units[1]
        assert unit.source == u""
        assert unit.getid() == u"bla\04"
        assert unit.target == "trans"
        assert not unit.isfuzzy()

    def test_obsolete_msgctxt(self):
        """Test that obsolete units' msgctxt is preserved."""
        potsource = 'msgctxt "newContext"\nmsgid "First unit"\nmsgstr ""'
        posource = """
msgctxt "newContext"
msgid "First unit"
msgstr "Eerste eenheid"

#~ msgctxt "context"
#~ msgid "Old unit"
#~ msgstr "Ou eenheid1"

#~ msgctxt "context2"
#~ msgid "Old unit"
#~ msgstr "Ou eenheid2"

#~ msgid "Old unit"
#~ msgstr "Ou eenheid3"
"""
        newpo = self.convertpot(potsource, posource)
        print(newpo)
        assert len(newpo.units) == 5
        assert newpo.units[1].getcontext() == 'newContext'
        # Search in unit string, because obsolete units can't return a context
        assert 'msgctxt "context"' in str(newpo.units[2])
        assert 'msgctxt "context2"' in str(newpo.units[3])

    def test_small_strings(self):
        """Test that units with small source strings are not incorrectly
        populated by means of fuzzy matching."""
        potsource = r'''#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;\n"
"X-Generator: Translate Toolkit 0.10rc2\n"
"X-Merge-On: location\n"
"X-Accelerator-Marker: &\n"

#: new_disassociated_mozilla_accesskey
msgid "R"
msgstr ""
'''
        posource = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: old@example.com\n"
"POT-Creation-Date: 2006-01-01 01:01+0100\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.9\n"
"X-Accelerator-Marker: &\n"

#: old_disassociated_mozilla_accesskey
msgid "R"
msgstr "S"
'''
        expected = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.10rc2\n"
"X-Merge-On: location\n"
"X-Accelerator-Marker: &\n"

#: new_disassociated_mozilla_accesskey
msgid "R"
msgstr ""
'''
        newpo = self.convertpot(potsource, posource)
        print('Output:\n%s' % newpo)
        print('Expected:\n%s' % expected)
        assert str(newpo) == expected


class TestPOT2POCommand(test_convert.TestConvertCommand, TestPOT2PO):
    """Tests running actual pot2po commands on files"""
    convertmodule = pot2po

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--tm")
        options = self.help_check(options, "-s MIN_SIMILARITY, --similarity=MIN_SIMILARITY")
        options = self.help_check(options, "--nofuzzymatching", last=True)

########NEW FILE########
__FILENAME__ = test_prop2mozfunny
#!/usr/bin/env python

from translate.convert import prop2mozfunny
from translate.misc import wStringIO


class TestPO2Prop:

    def merge2inc(self, incsource, posource):
        """helper that merges po translations to .inc source without requiring files"""
        inputfile = wStringIO.StringIO(posource)
        templatefile = wStringIO.StringIO(incsource)
        outputfile = wStringIO.StringIO()
        result = prop2mozfunny.po2inc(inputfile, outputfile, templatefile)
        outputinc = outputfile.getvalue()
        print(outputinc)
        assert result
        return outputinc

    def test_no_endlines_added(self):
        """check that we don't add newlines at the end of file"""
        posource = '''# converted from #defines file\n#: MOZ_LANG_TITLE\nmsgid "English (US)"\nmsgstr "Deutsch (DE)"\n\n'''
        inctemplate = '''#define MOZ_LANG_TITLE Deutsch (DE)\n'''
        incexpected = inctemplate
        incfile = self.merge2inc(inctemplate, posource)
        print(incfile)
        assert incfile == incexpected

    def test_uncomment_contributors(self):
        """check that we handle uncommenting contributors properly"""
        posource = '''# converted from #defines file
#: MOZ_LANGPACK_CONTRIBUTORS
msgid "<em:contributor>Joe Solon</em:contributor>"
msgstr "<em:contributor>Mr Fury</em:contributor>"
'''
        inctemplate = '''# #define MOZ_LANGPACK_CONTRIBUTORS <em:contributor>Joe Solon</em:contributor>\n'''
        incexpected = '''#define MOZ_LANGPACK_CONTRIBUTORS <em:contributor>Mr Fury</em:contributor>\n'''
        incfile = self.merge2inc(inctemplate, posource)
        print(incfile)
        assert incfile == incexpected

    def test_multiline_comment_newlines(self):
        """Ensure that we preserve newlines in multiline comments"""
        inctemplate = '''# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#filter emptyLines
'''
        incexpected = inctemplate
        incfile = self.merge2inc(inctemplate, None)
        print(incfile)
        assert incfile == incexpected

########NEW FILE########
__FILENAME__ = test_prop2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.convert import prop2po, test_convert
from translate.misc import wStringIO
from translate.storage import po, properties


class TestProp2PO:

    def prop2po(self, propsource, proptemplate=None, personality="java"):
        """helper that converts .properties source to po source without requiring files"""
        inputfile = wStringIO.StringIO(propsource)
        inputprop = properties.propfile(inputfile, personality=personality)
        convertor = prop2po.prop2po(personality=personality)
        if proptemplate:
            templatefile = wStringIO.StringIO(proptemplate)
            templateprop = properties.propfile(templatefile)
            outputpo = convertor.mergestore(templateprop, inputprop)
        else:
            outputpo = convertor.convertstore(inputprop)
        return outputpo

    def convertprop(self, propsource):
        """call the convertprop, return the outputfile"""
        inputfile = wStringIO.StringIO(propsource)
        outputfile = wStringIO.StringIO()
        templatefile = None
        assert prop2po.convertprop(inputfile, outputfile, templatefile)
        return outputfile.getvalue()

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        assert len(pofile.units) == 2
        assert pofile.units[0].isheader()
        print(pofile)
        return pofile.units[1]

    def countelements(self, pofile):
        """counts the number of non-header entries"""
        assert pofile.units[0].isheader()
        print(pofile)
        return len(pofile.units) - 1

    def test_simpleentry(self):
        """checks that a simple properties entry converts properly to a po entry"""
        propsource = 'SAVEENTRY=Save file\n'
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "Save file"
        assert pounit.target == ""

    def test_convertprop(self):
        """checks that the convertprop function is working"""
        propsource = 'SAVEENTRY=Save file\n'
        posource = self.convertprop(propsource)
        pofile = po.pofile(wStringIO.StringIO(posource))
        pounit = self.singleelement(pofile)
        assert pounit.source == "Save file"
        assert pounit.target == ""

    def test_tab_at_end_of_string(self):
        """check that we preserve tabs at the end of a string"""
        propsource = r"TAB_AT_END=This setence has a tab at the end.\t"
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "This setence has a tab at the end.\t"

        propsource = r"SPACE_THEN_TAB_AT_END=This setence has a space then tab at the end. \t"
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "This setence has a space then tab at the end. \t"

        propsource = r"SPACE_AT_END=This setence will keep its 4 spaces at the end.    "
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "This setence will keep its 4 spaces at the end.    "

        propsource = r"SPACE_AT_END_NO_TRIM=This setence will keep its 4 spaces at the end.\    "
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "This setence will keep its 4 spaces at the end.    "

        propsource = r"SPACE_AT_END_NO_TRIM2=This setence will keep its 4 spaces at the end.\\    "
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.source == "This setence will keep its 4 spaces at the end.\    "

    def test_tab_at_start_of_value(self):
        """check that tabs in a property are ignored where appropriate"""
        propsource = r"property	=	value"
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.getlocations()[0] == "property"
        assert pounit.source == "value"

    def test_unicode(self):
        """checks that unicode entries convert properly"""
        unistring = r'Norsk bokm\u00E5l'
        propsource = 'nb = %s\n' % unistring
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        print(repr(pofile.units[0].target))
        print(repr(pounit.source))
        assert pounit.source == u'Norsk bokm\u00E5l'

    def test_multiline_escaping(self):
        """checks that multiline enties can be parsed"""
        propsource = r"""5093=Unable to connect to your IMAP server. You may have exceeded the maximum number \
of connections to this server. If so, use the Advanced IMAP Server Settings dialog to \
reduce the number of cached connections."""
        pofile = self.prop2po(propsource)
        print(repr(pofile.units[1].target))
        assert self.countelements(pofile) == 1

    def test_comments(self):
        """test to ensure that we take comments from .properties and place them in .po"""
        propsource = '''# Comment
prefPanel-smime=Security'''
        pofile = self.prop2po(propsource)
        pounit = self.singleelement(pofile)
        assert pounit.getnotes("developer") == "# Comment"

    def test_multiline_comments(self):
        """test to ensure that we handle multiline comments well"""
        propsource = '''# Comment
# commenty 2

## @name GENERIC_ERROR
## @loc none
prefPanel-smime=
'''
        pofile = self.prop2po(propsource)
        print(str(pofile))
        #header comments:
        assert "#. # Comment\n#. # commenty 2" in str(pofile)
        pounit = self.singleelement(pofile)
        assert pounit.getnotes("developer") == "## @name GENERIC_ERROR\n## @loc none"

    def test_folding_accesskeys(self):
        """check that we can fold various accesskeys into their associated label (bug #115)"""
        propsource = r'''cmd_addEngine.label = Add Engines...
cmd_addEngine.accesskey = A'''
        pofile = self.prop2po(propsource, personality="mozilla")
        pounit = self.singleelement(pofile)
        assert pounit.source == "&Add Engines..."

    def test_dont_translate(self):
        """check that we know how to ignore don't translate instructions in properties files (bug #116)"""
        propsource = '''# LOCALIZATION NOTE (dont): DONT_TRANSLATE.
dont=don't translate me
do=translate me
'''
        pofile = self.prop2po(propsource)
        assert self.countelements(pofile) == 1

    def test_emptyproperty(self):
        """checks that empty property definitions survive into po file, bug 15"""
        for delimiter in ["=", ""]:
            propsource = '# comment\ncredit%s' % delimiter
            pofile = self.prop2po(propsource)
            pounit = self.singleelement(pofile)
            assert pounit.getlocations() == ["credit"]
            assert pounit.getcontext() == "credit"
            assert 'msgctxt "credit"' in str(pounit)
            assert "#. # comment" in str(pofile)
            assert pounit.source == ""

    def test_emptyproperty_translated(self):
        """checks that if we translate an empty property it makes it into the PO"""
        for delimiter in ["=", ""]:
            proptemplate = 'credit%s' % delimiter
            propsource = 'credit=Translators Names'
            pofile = self.prop2po(propsource, proptemplate)
            pounit = self.singleelement(pofile)
            assert pounit.getlocations() == ["credit"]
            # FIXME we don't seem to get a _: comment but we should
            #assert pounit.getcontext() == "credit"
            assert pounit.source == ""
            assert pounit.target == "Translators Names"

    def test_newlines_in_value(self):
        """check that we can carry newlines that appear in the property value into the PO"""
        propsource = '''prop=\\nvalue\\n\n'''
        pofile = self.prop2po(propsource)
        unit = self.singleelement(pofile)
        assert unit.source == "\nvalue\n"

    def test_unassociated_comments(self):
        """check that we can handle comments not directly associated with a property"""
        propsource = '''# Header comment\n\n# Comment\n\nprop=value\n'''
        pofile = self.prop2po(propsource)
        unit = self.singleelement(pofile)
        assert unit.source == "value"
        assert unit.getnotes("developer") == "# Comment"

    def test_unassociated_comment_order(self):
        """check that we can handle the order of unassociated comments"""
        propsource = '''# Header comment\n\n# 1st Unassociated comment\n\n# 2nd Connected comment\nprop=value\n'''
        pofile = self.prop2po(propsource)
        unit = self.singleelement(pofile)
        assert unit.source == "value"
        assert unit.getnotes("developer") == "# 1st Unassociated comment\n# 2nd Connected comment"

    def test_x_header(self):
        """Test that we correctly create the custom header entries
        (accelerators, merge criterion).
        """
        propsource = '''prop=value\n'''

        outputpo = self.prop2po(propsource, personality="mozilla")
        assert "X-Accelerator-Marker" in str(outputpo)
        assert "X-Merge-On" in str(outputpo)

        # Even though the gaia flavour inherrits from mozilla, it should not
        # get the header
        outputpo = self.prop2po(propsource, personality="gaia")
        assert "X-Accelerator-Marker" not in str(outputpo)
        assert "X-Merge-On" not in str(outputpo)

    def test_gaia_plurals(self):
        """Test conversion of gaia plural units."""
        propsource = '''
message-multiedit-header={[ plural(n) ]}
message-multiedit-header[zero]=Edit
message-multiedit-header[one]={{ n }} selected
message-multiedit-header[two]={{ n }} selected
message-multiedit-header[few]={{ n }} selected
message-multiedit-header[many]={{ n }} selected
message-multiedit-header[other]={{ n }} selected
'''
        outputpo = self.prop2po(propsource, personality="gaia")
        pounit = outputpo.units[-1]
        assert pounit.hasplural()
        assert pounit.getlocations() == [u'message-multiedit-header']

        print(outputpo)
        zero_unit = outputpo.units[-2]
        assert not zero_unit.hasplural()
        assert zero_unit.source == u"Edit"

    def test_successive_gaia_plurals(self):
        """Test conversion of two successive gaia plural units."""
        propsource = '''
message-multiedit-header={[ plural(n) ]}
message-multiedit-header[zero]=Edit
message-multiedit-header[one]={{ n }} selected
message-multiedit-header[two]={{ n }} selected
message-multiedit-header[few]={{ n }} selected
message-multiedit-header[many]={{ n }} selected
message-multiedit-header[other]={{ n }} selected

message-multiedit-header2={[ plural(n) ]}
message-multiedit-header2[zero]=Edit 2
message-multiedit-header2[one]={{ n }} selected 2
message-multiedit-header2[two]={{ n }} selected 2
message-multiedit-header2[few]={{ n }} selected 2
message-multiedit-header2[many]={{ n }} selected 2
message-multiedit-header2[other]={{ n }} selected 2
'''
        outputpo = self.prop2po(propsource, personality="gaia")
        pounit = outputpo.units[-1]
        assert pounit.hasplural()
        assert pounit.getlocations() == [u'message-multiedit-header2']

        pounit = outputpo.units[-3]
        assert pounit.hasplural()
        assert pounit.getlocations() == [u'message-multiedit-header']

        print(outputpo)
        zero_unit = outputpo.units[-2]
        assert not zero_unit.hasplural()
        assert zero_unit.source == u"Edit 2"

        zero_unit = outputpo.units[-4]
        assert not zero_unit.hasplural()
        assert zero_unit.source == u"Edit"


class TestProp2POCommand(test_convert.TestConvertCommand, TestProp2PO):
    """Tests running actual prop2po commands on files"""
    convertmodule = prop2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--personality=TYPE")
        options = self.help_check(options, "--encoding=ENCODING")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE", last=True)

########NEW FILE########
__FILENAME__ = test_tiki2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# tiki2po unit tests
# Author: Wil Clouser <wclouser@mozilla.com>
# Date: 2008-12-01

from translate.convert import test_convert, tiki2po
from translate.misc import wStringIO


class TestTiki2Po:

    def test_converttiki_defaults(self):
        inputfile = """
"zero_source" => "zero_target",
// ### Start of unused words
"one_source" => "one_target",
// ### end of unused words
        """
        outputfile = wStringIO.StringIO()
        tiki2po.converttiki(inputfile, outputfile)

        output = outputfile.getvalue()

        assert '#: translated' in output
        assert 'msgid "zero_source"' in output
        assert "one_source" not in output

    def test_converttiki_includeunused(self):
        inputfile = """
"zero_source" => "zero_target",
// ### Start of unused words
"one_source" => "one_target",
// ### end of unused words
        """
        outputfile = wStringIO.StringIO()
        tiki2po.converttiki(inputfile, outputfile, includeunused=True)

        output = outputfile.getvalue()

        assert '#: translated' in output
        assert 'msgid "zero_source"' in output
        assert '#: unused' in output
        assert 'msgid "one_source"' in output


class TestTiki2PoCommand(test_convert.TestConvertCommand, TestTiki2Po):
    """Tests running actual tiki2po commands on files"""
    convertmodule = tiki2po
    defaultoptions = {}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--include-unused")

########NEW FILE########
__FILENAME__ = test_ts2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.convert import test_convert, ts2po
from translate.misc import wStringIO


class TestTS2PO:

    def ts2po(self, tssource):
        converter = ts2po.ts2po()
        tsfile = wStringIO.StringIO(tssource)
        outputpo = converter.convertfile(tsfile)
        print("The generated po:")
        print(str(outputpo))
        return outputpo

    def test_blank(self):
        """tests blank conversion"""
        tssource = '''<!DOCTYPE TS><TS>
<context>
    <name>MainWindowBase</name>
    <message>
        <source>Project:</source>
        <translation type="unfinished"></translation>
    </message>
</context>
</TS>
'''
        pofile = self.ts2po(tssource)
        assert len(pofile.units) == 2
        assert pofile.units[1].source == "Project:"
        assert pofile.units[1].target == ""
        assert pofile.units[1].getlocations()[0].startswith("MainWindowBase")
        assert not pofile.units[1].isfuzzy()

    def test_basic(self):
        """tests basic conversion"""
        tssource = '''<!DOCTYPE TS><TS>
<context>
    <name>AboutDialog</name>
    <message>
        <source>&amp;About</source>
        <translation>&amp;Gii thiu</translation>
    </message>
</context>
</TS>
'''
        pofile = self.ts2po(tssource)
        assert len(pofile.units) == 2
        assert pofile.units[1].source == "&About"
        assert pofile.units[1].target == u"&Gii thiu"
        assert pofile.units[1].getlocations()[0].startswith("AboutDialog")

    def test_unfinished(self):
        """tests unfinished conversion"""
        tssource = '''<!DOCTYPE TS><TS>
<context>
    <name>MainWindowBase</name>
    <message>
        <source>Project:</source>
        <translation type="unfinished">Projek vergardering</translation>
    </message>
</context>
</TS>
'''
        pofile = self.ts2po(tssource)
        assert len(pofile.units) == 2
        assert pofile.units[1].source == "Project:"
        assert pofile.units[1].target == "Projek vergardering"
        assert pofile.units[1].getlocations()[0].startswith("MainWindowBase")
        assert pofile.units[1].isfuzzy()

    def test_multiline(self):
        """tests multiline message conversion"""
        tssource = '''<!DOCTYPE TS><TS>
<context>
    <name>@default</name>
    <message>
        <source>Source with
new line</source>
        <translation>Test with
new line</translation>
    </message>
</context>
</TS>
'''
        pofile = self.ts2po(tssource)
        assert len(pofile.units) == 2
        assert pofile.units[1].source == "Source with\nnew line"
        assert pofile.units[1].target == "Test with\nnew line"
        assert pofile.units[1].getlocations()[0].startswith("@default")

    def test_obsolete(self):
        """test the handling of obsolete TS entries"""
        tssource = '''<!DOCTYPE TS><TS>
<context>
    <name>Obsoleted</name>
    <message>
        <source>Failed</source>
        <translation type="obsolete">Mislukt</translation>
    </message>
</context>
</TS>
'''
        pofile = self.ts2po(tssource)
        assert pofile.units[1].getnotes("developer") == "(obsolete)"
        # Test that we aren't following the old style
        assert "_ OBSOLETE" not in pofile.units[1].getnotes()


class TestTS2POCommand(test_convert.TestConvertCommand, TestTS2PO):
    """Tests running actual ts2po commands on files"""
    convertmodule = ts2po

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")
        options = self.help_check(options, "-P, --pot", last=True)

########NEW FILE########
__FILENAME__ = test_txt2po
#!/usr/bin/env python

from translate.convert import test_convert, txt2po
from translate.misc import wStringIO
from translate.storage import txt


class TestTxt2PO:

    def txt2po(self, txtsource, template=None):
        """helper that converts txt source to po source without requiring files"""
        inputfile = wStringIO.StringIO(txtsource)
        inputtxt = txt.TxtFile(inputfile)
        convertor = txt2po.txt2po()
        outputpo = convertor.convertstore(inputtxt)
        return outputpo

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        print(str(storage))
        assert len(storage.units) == 1
        return storage.units[0]

    def test_simple(self):
        """test the most basic txt conversion"""
        txtsource = "A simple string"
        poexpected = '''#: :1
msgid "A simple string"
msgstr ""
'''
        poresult = self.txt2po(txtsource)
        assert str(poresult.units[1]) == poexpected

    def test_miltiple_units(self):
        """test that we can handle txt with multiple units"""
        txtsource = """First unit
Still part of first unit

Second unit is a heading
------------------------

Third unit with blank after but no more units.

"""
        poresult = self.txt2po(txtsource)
        assert poresult.units[0].isheader()
        assert len(poresult.units) == 4

    def test_carriage_return(self):
        """Remove carriage returns from files in dos format."""
        txtsource = '''The rapid expansion of telecommunications infrastructure in recent years has\r
helped to bridge the digital divide to a limited extent.\r
'''

        txtexpected = '''The rapid expansion of telecommunications infrastructure in recent years has
helped to bridge the digital divide to a limited extent.'''

        poresult = self.txt2po(txtsource)
        pounit = poresult.units[1]
        assert str(pounit.getsource()) == txtexpected


class TestDoku2po:

    def doku2po(self, txtsource, template=None):
        """helper that converts dokuwiki source to po source without requiring files."""
        inputfile = wStringIO.StringIO(txtsource)
        inputtxt = txt.TxtFile(inputfile, flavour="dokuwiki")
        convertor = txt2po.txt2po()
        outputpo = convertor.convertstore(inputtxt)
        return outputpo

    def singleelement(self, storage):
        """checks that the pofile contains a single non-header element, and returns it"""
        print(str(storage))
        assert len(storage.units) == 1
        return storage.units[0]

    def test_basic(self):
        """Tests that we can convert some basic things."""
        dokusource = """=====Heading=====

This is a wiki page.
"""
        poresult = self.doku2po(dokusource)
        assert poresult.units[0].isheader()
        assert len(poresult.units) == 3
        assert poresult.units[1].source == "Heading"
        assert poresult.units[2].source == "This is a wiki page."

    def test_bullets(self):
        """Tests that we can convert some basic things."""
        dokusource = """  * This is a fact.
  * This is a fact.
"""
        poresult = self.doku2po(dokusource)
        assert poresult.units[0].isheader()
        assert len(poresult.units) == 3
        assert poresult.units[1].source == "This is a fact."
        assert poresult.units[2].source == "This is a fact."

    def test_numbers(self):
        """Tests that we can convert some basic things."""
        dokusource = """  - This is an item.
  - This is an item.
"""
        poresult = self.doku2po(dokusource)
        assert poresult.units[0].isheader()
        assert len(poresult.units) == 3
        assert poresult.units[1].source == "This is an item."
        assert poresult.units[2].source == "This is an item."

    def test_spacing(self):
        """Tests that we can convert some basic things."""
        dokusource = """ =====         Heading  =====
  * This is an item.
    * This is a subitem.
        * This is a tabbed item.
"""
        poresult = self.doku2po(dokusource)
        assert poresult.units[0].isheader()
        assert len(poresult.units) == 5
        assert poresult.units[1].source == "Heading"
        assert poresult.units[2].source == "This is an item."
        assert poresult.units[3].source == "This is a subitem."
        assert poresult.units[4].source == "This is a tabbed item."


class TestTxt2POCommand(test_convert.TestConvertCommand, TestTxt2PO):
    """Tests running actual txt2po commands on files"""
    convertmodule = txt2po
    defaultoptions = {"progress": "none"}

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--duplicates")
        options = self.help_check(options, "--encoding")
        options = self.help_check(options, "--flavour", last=True)

########NEW FILE########
__FILENAME__ = test_xliff2po
#!/usr/bin/env python

from translate.convert import test_convert, xliff2po
from translate.misc import wStringIO
from translate.storage import po, xliff
from translate.storage.poheader import poheader
from translate.storage.test_base import first_translatable, headerless_len


class TestXLIFF2PO:
    target_filetype = po.pofile
    xliffskeleton = '''<?xml version="1.0" ?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
  <file original="filename.po" source-language="en-US" datatype="po">
    <body>
        %s
    </body>
  </file>
</xliff>'''

    def xliff2po(self, xliffsource):
        """helper that converts xliff source to po source without requiring files"""
        inputfile = wStringIO.StringIO(xliffsource)
        convertor = xliff2po.xliff2po()
        outputpo = convertor.convertstore(inputfile)
        print("The generated po:")
        print(type(outputpo))
        print(str(outputpo))
        return outputpo

    def test_minimal(self):
        minixlf = self.xliffskeleton % '''<trans-unit>
        <source>red</source>
        <target>rooi</target>
      </trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert headerless_len(pofile.units) == 1
        assert pofile.translate("red") == "rooi"
        assert pofile.translate("bla") is None

    def test_basic(self):
        headertext = '''Project-Id-Version: program 2.1-branch
Report-Msgid-Bugs-To:
POT-Creation-Date: 2006-01-09 07:15+0100
PO-Revision-Date: 2004-03-30 17:02+0200
Last-Translator: Zuza Software Foundation &lt;xxx@translate.org.za>
Language-Team: Afrikaans &lt;translate-discuss-xxx@lists.sourceforge.net>
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit'''

        minixlf = (self.xliffskeleton % '''<trans-unit id="1" restype="x-gettext-domain-header" approved="no" xml:space="preserve">
  <source>%s</source>
  <target>%s</target>
  <note from="po-translator">Zulu translation of program ABC</note>
  </trans-unit>
  <trans-unit>
    <source>gras</source>
    <target>utshani</target>
  </trans-unit>''') % (headertext, headertext)

        print(minixlf)
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("gras") == "utshani"
        assert pofile.translate("bla") is None
        potext = str(pofile)
        assert potext.index('# Zulu translation of program ABC') == 0
        assert potext.index('msgid "gras"\n')
        assert potext.index('msgstr "utshani"\n')
        assert potext.index('MIME-Version: 1.0\\n')

    def test_translatorcomments(self):
        """Tests translator comments"""
        minixlf = self.xliffskeleton % '''<trans-unit>
        <source>nonsense</source>
        <target>matlhapolosa</target>
        <context-group name="po-entry" purpose="information">
            <context context-type="x-po-trancomment">Couldn't do
it</context>
        </context-group>
        <note from="po-translator">Couldn't do
it</note>
</trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("bla") is None
        unit = first_translatable(pofile)
        assert unit.getnotes("translator") == "Couldn't do it"
        potext = str(pofile)
        assert potext.index("# Couldn't do it\n") >= 0

        minixlf = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
        <context-group name="po-entry" purpose="information">
            <context context-type="x-po-trancomment">Couldn't do
it</context>
        </context-group>
        <note from="po-translator">Couldn't do
it</note>
</trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("bla") is None
        unit = first_translatable(pofile)
        assert unit.getnotes("translator") == "Couldn't do\nit"
        potext = str(pofile)
        assert potext.index("# Couldn't do\n# it\n") >= 0

    def test_autocomment(self):
        """Tests automatic comments"""
        minixlf = self.xliffskeleton % '''<trans-unit>
        <source>nonsense</source>
        <target>matlhapolosa</target>
        <context-group name="po-entry" purpose="information">
            <context context-type="x-po-autocomment">Note that this is
garbage</context>
        </context-group>
        <note from="developer">Note that this is
garbage</note>
</trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("bla") is None
        unit = first_translatable(pofile)
        assert unit.getnotes("developer") == "Note that this is garbage"
        potext = str(pofile)
        assert potext.index("#. Note that this is garbage\n") >= 0

        minixlf = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
        <context-group name="po-entry" purpose="information">
            <context context-type="x-po-autocomment">Note that this is
garbage</context>
        </context-group>
        <note from="developer">Note that this is
garbage</note>
</trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("bla") is None
        unit = first_translatable(pofile)
        assert unit.getnotes("developer") == "Note that this is\ngarbage"
        potext = str(pofile)
        assert potext.index("#. Note that this is\n#. garbage\n") >= 0

    def test_locations(self):
        """Tests location comments (#:)"""
        minixlf = self.xliffskeleton % '''<trans-unit id="1">
        <source>nonsense</source>
        <target>matlhapolosa</target>
        <context-group name="po-reference" purpose="location">
            <context context-type="sourcefile">example.c</context>
            <context context-type="linenumber">123</context>
            </context-group>
        <context-group name="po-reference" purpose="location">
            <context context-type="sourcefile">place.py</context>
        </context-group>
</trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("bla") is None
        unit = first_translatable(pofile)
        locations = unit.getlocations()
        assert len(locations) == 2
        assert "example.c:123" in locations
        assert "place.py" in locations

    def test_fuzzy(self):
        """Tests fuzzyness"""
        minixlf = self.xliffskeleton % '''<trans-unit approved="no">
            <source>book</source>
        </trans-unit>
        <trans-unit id="2" approved="yes">
            <source>nonsense</source>
            <target>matlhapolosa</target>
        </trans-unit>
        <trans-unit id="2" approved="no">
            <source>verb</source>
            <target state="needs-review-translation">lediri</target>
        </trans-unit>'''
        pofile = self.xliff2po(minixlf)
        assert pofile.translate("nonsense") == "matlhapolosa"
        assert pofile.translate("verb") == "lediri"
        assert pofile.translate("book") is None
        assert pofile.translate("bla") is None
        assert headerless_len(pofile.units) == 3
        #TODO: decide if this one should be fuzzy:
        #assert pofile.units[0].isfuzzy()
        assert not pofile.units[2].isfuzzy()
        assert pofile.units[3].isfuzzy()

    def test_plurals(self):
        """Tests fuzzyness"""
        minixlf = self.xliffskeleton % '''<group id="1" restype="x-gettext-plurals">
        <trans-unit id="1[0]" xml:space="preserve">
            <source>cow</source>
            <target>inkomo</target>
        </trans-unit>
        <trans-unit id="1[1]" xml:space="preserve">
            <source>cows</source>
            <target>iinkomo</target>
        </trans-unit>
</group>'''
        pofile = self.xliff2po(minixlf)
        print(str(pofile))
        potext = str(pofile)
        assert headerless_len(pofile.units) == 1
        assert potext.index('msgid_plural "cows"')
        assert potext.index('msgstr[0] "inkomo"')
        assert potext.index('msgstr[1] "iinkomo"')


class TestBasicXLIFF2PO(test_convert.TestConvertCommand, TestXLIFF2PO):
    """This tests a basic XLIFF file without xmlns attribute"""
    convertmodule = xliff2po

    xliffskeleton = '''<?xml version="1.0" ?>
<xliff version="1.1">
  <file original="filename.po" source-language="en-US" datatype="po">
    <body>
        %s
    </body>
  </file>
</xliff>'''

    def test_simple_convert(self):
        self.create_testfile("simple_convert.xlf", self.xliffskeleton % """
                             <trans-unit xml:space="preserve" id="1" approved="yes">
                               <source>One</source>
                               <target state="translated">Een</target>
                             </trans-unit>
                             """)
        self.run_command(i="simple_convert.xlf", o="simple_convert.po")
        assert 'msgstr "Een"' in self.read_testfile("simple_convert.po")


class TestXLIFF2POCommand(test_convert.TestConvertCommand, TestXLIFF2PO):
    """Tests running actual xliff2po commands on files"""
    convertmodule = xliff2po

    def singleelement(self, pofile):
        """checks that the pofile contains a single non-header element, and returns it"""
        if isinstance(pofile, poheader):
            assert len(pofile.units) == 2
            assert pofile.units[0].isheader()
            return pofile.units[1]
        else:
            assert len(pofile.units) == 1
            return pofile.units[0]

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-P, --pot")
        options = self.help_check(options, "--duplicates=DUPLICATESTYLE")

    def test_preserve_filename(self):
        """Ensures that the filename is preserved."""
        xliffsource = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
</trans-unit>'''
        self.create_testfile("snippet.xlf", xliffsource)
        xlifffile = xliff.xlifffile(self.open_testfile("snippet.xlf"))
        assert xlifffile.filename.endswith("snippet.xlf")
        xlifffile.parse(xliffsource)
        assert xlifffile.filename.endswith("snippet.xlf")

    def test_simple_pot(self):
        """tests the simplest possible conversion to a pot file"""
        xliffsource = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target></target>
</trans-unit>'''
        self.create_testfile("simple.xlf", xliffsource)
        self.run_command("simple.xlf", "simple.pot", pot=True)
        pofile = po.pofile(self.open_testfile("simple.pot"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "nonsense"
        assert poelement.target == ""

    def test_simple_po(self):
        """tests the simplest possible conversion to a po file"""
        xliffsource = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
</trans-unit>'''
        self.create_testfile("simple.xlf", xliffsource)
        self.run_command("simple.xlf", "simple.po")
        pofile = po.pofile(self.open_testfile("simple.po"))
        poelement = self.singleelement(pofile)
        assert poelement.source == "nonsense"
        assert poelement.target == "matlhapolosa"

    def test_remove_duplicates(self):
        """test that removing of duplicates works correctly"""
        xliffsource = self.xliffskeleton % '''<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
</trans-unit>
<trans-unit xml:space="preserve">
        <source>nonsense</source>
        <target>matlhapolosa</target>
</trans-unit>'''
        self.create_testfile("simple.xlf", xliffsource)
        self.run_command("simple.xlf", "simple.po", error="traceback", duplicates="merge")
        pofile = self.target_filetype(self.open_testfile("simple.po"))
        assert len(pofile.units) == 2
        assert pofile.units[1].target == u"matlhapolosa"

########NEW FILE########
__FILENAME__ = tiki2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Mozilla Corporation, Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert TikiWiki's language.php files to GetText PO files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/tiki2po.html
for examples and usage instructions.
"""

import sys

from translate.storage import po, tiki


class tiki2po:

    def __init__(self, includeunused=False):
        """
        :param includeunused: On conversion, should the "unused" section be
                              preserved?  Default: False
        """
        self.includeunused = includeunused

    def convertstore(self, thetikifile):
        """Converts a given (parsed) tiki file to a po file.

        :param thetikifile: a tikifile pre-loaded with input data
        """
        thetargetfile = po.pofile()

        # For each lang unit, make the new po unit accordingly
        for unit in thetikifile.units:
            if not self.includeunused and "unused" in unit.getlocations():
                continue
            newunit = po.pounit()
            newunit.source = unit.source
            newunit.settarget(unit.target)
            locations = unit.getlocations()
            if locations:
                newunit.addlocations(locations)
            thetargetfile.addunit(newunit)
        return thetargetfile


def converttiki(inputfile, outputfile, template=None, includeunused=False):
    """Converts from tiki file format to po.

    :param inputfile: file handle of the source
    :param outputfile: file handle to write to
    :param template: unused
    :param includeunused: Include the "usused" section of the tiki
                          file? Default: False
    """
    convertor = tiki2po(includeunused=includeunused)
    inputstore = tiki.TikiStore(inputfile)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return False
    outputfile.write(str(outputstore))
    return True


def main(argv=None):
    """Converts tiki .php files to .po."""
    from translate.convert import convert
    from translate.misc import stdiotell
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)

    formats = {"php": ("po", converttiki)}

    parser = convert.ConvertOptionParser(formats, description=__doc__)
    parser.add_option("", "--include-unused", dest="includeunused",
                      action="store_true", default=False,
                      help="Include strings in the unused section")
    parser.passthrough.append("includeunused")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ts2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Qt Linguist (.ts) files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/ts2po.html
for examples and usage instructions.
"""


from translate.storage import po, ts


class ts2po:

    def __init__(self, duplicatestyle="msgctxt", pot=False):
        self.duplicatestyle = duplicatestyle
        self.pot = pot

    def convertmessage(self, contextname, messagenum, source, target, msgcomments, transtype):
        """makes a pounit from the given message"""
        thepo = po.pounit(encoding="UTF-8")
        thepo.addlocation("%s#%d" % (contextname, messagenum))
        thepo.source = source
        if not self.pot:
            thepo.target = target
        if len(msgcomments) > 0:
            thepo.addnote(msgcomments)
        if transtype == "unfinished" and thepo.istranslated():
            thepo.markfuzzy()
        if transtype == "obsolete":
            # This should use the Gettext obsolete method but it would require quite a bit of work
            thepo.addnote("(obsolete)", origin="developer")
            # using the fact that -- quote -- "(this is nonsense)"
        return thepo

    def convertfile(self, inputfile):
        """converts a .ts file to .po format"""
        tsfile = ts.QtTsParser(inputfile)
        thetargetfile = po.pofile()

        for contextname, messages in tsfile.iteritems():
            messagenum = 0
            for message in messages:
                messagenum += 1
                source = tsfile.getmessagesource(message)
                translation = tsfile.getmessagetranslation(message)
                comment = tsfile.getmessagecomment(message)
                transtype = tsfile.getmessagetype(message)
                thepo = self.convertmessage(contextname, messagenum, source, translation, comment, transtype)
                thetargetfile.addunit(thepo)
        thetargetfile.removeduplicates(self.duplicatestyle)
        return thetargetfile


def convertts(inputfile, outputfile, templates, pot=False, duplicatestyle="msgctxt"):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    convertor = ts2po(duplicatestyle=duplicatestyle, pot=pot)
    outputstore = convertor.convertfile(inputfile)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"ts": ("po", convertts)}
    parser = convert.ConvertOptionParser(formats, usepots=True, description=__doc__)
    parser.add_duplicates_option()
    parser.passthrough.append("pot")
    parser.run(argv)

########NEW FILE########
__FILENAME__ = txt2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert plain text (.txt) files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/txt2po.html
for examples and usage instructions.
"""

from translate.storage import po, txt


class txt2po:

    def __init__(self, duplicatestyle="msgctxt"):
        self.duplicatestyle = duplicatestyle

    def convertstore(self, thetxtfile):
        """converts a file to .po format"""
        thetargetfile = po.pofile()
        targetheader = thetargetfile.header()
        targetheader.addnote("extracted from %s" % thetxtfile.filename,
                             "developer")

        for txtunit in thetxtfile.units:
            newunit = thetargetfile.addsourceunit(txtunit.source)
            newunit.addlocations(txtunit.getlocations())
        thetargetfile.removeduplicates(self.duplicatestyle)
        return thetargetfile


def converttxt(inputfile, outputfile, templates, duplicatestyle="msgctxt",
               encoding="utf-8", flavour=None):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    inputstore = txt.TxtFile(inputfile, encoding=encoding, flavour=flavour)
    convertor = txt2po(duplicatestyle=duplicatestyle)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    from translate.misc import stdiotell
    import sys
    sys.stdout = stdiotell.StdIOWrapper(sys.stdout)
    formats = {"txt": ("po", converttxt), "*": ("po", converttxt)}
    parser = convert.ConvertOptionParser(formats, usepots=True,
                                         description=__doc__)
    parser.add_option("", "--encoding", dest="encoding", default='utf-8',
                      type="string",
                      help="The encoding of the input file (default: UTF-8)")
    parser.passthrough.append("encoding")
    parser.add_option("", "--flavour", dest="flavour", default="plain",
                      type="choice",
                      choices=["plain", "dokuwiki", "mediawiki"],
                      help="The flavour of text file: plain (default), dokuwiki, mediawiki",
                      metavar="FLAVOUR")
    parser.passthrough.append("flavour")
    parser.add_duplicates_option()
    parser.run(argv)

########NEW FILE########
__FILENAME__ = web2py2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert web2py translation dictionaries (.py) to GNU/gettext PO files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/web2py2po.html
for examples and usage instructions.
"""

from translate.storage import po


class web2py2po:

    def __init__(self, pofile=None):
        self.mypofile = pofile

    def convertunit(self, source_str, target_str):
        pounit = po.pounit(encoding="UTF-8")
        pounit.setsource(source_str)
        if target_str:
            pounit.settarget(target_str)
        return pounit

    def convertstore(self, mydict):

        targetheader = self.mypofile.header()
        targetheader.addnote("extracted from web2py", "developer")

        for source_str in mydict.keys():
            target_str = mydict[source_str]
            if target_str == source_str:
                # a convention with new (untranslated) web2py files
                target_str = u''
            elif target_str.startswith(u'*** '):
                # an older convention
                target_str = u''
            pounit = self.convertunit(source_str, target_str)
            self.mypofile.addunit(pounit)

        return self.mypofile


def convertpy(inputfile, outputfile, encoding="UTF-8"):

    new_pofile = po.pofile()
    convertor = web2py2po(new_pofile)

    mydict = eval(inputfile.read())
    if not isinstance(mydict, dict):
        return 0

    outputstore = convertor.convertstore(mydict)

    if outputstore.isempty():
        return 0

    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {
        ("py", "po"): ("po", convertpy),
        ("py", None): ("po", convertpy)
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=False,
                                         usepots=True,
                                         description=__doc__)
    parser.add_duplicates_option()
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = xliff2odf
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Convert XLIFF translation files to OpenDocument (ODF) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/odf2xliff.html
for examples and usage instructions.
"""
import zipfile
from cStringIO import StringIO

import lxml.etree as etree

from translate.storage import factory, odf_io, odf_shared
from translate.storage.xml_extract import extract, generate, unit_tree


def first_child(unit_node):
    return unit_node.children.values()[0]


def translate_odf(template, input_file):

    def load_dom_trees(template):
        odf_data = odf_io.open_odf(template)
        return dict((filename, etree.parse(StringIO(data))) for filename, data in odf_data.iteritems())

    def load_unit_tree(input_file, dom_trees):
        store = factory.getobject(input_file)
        tree = unit_tree.build_unit_tree(store)

        def extract_unit_tree(filename, root_dom_element_name):
            """Find the subtree in 'tree' which corresponds to the data in XML file 'filename'"""

            def get_tree():
                try:
                    return tree.children['office:%s' % root_dom_element_name, 0]
                except KeyError:
                    return unit_tree.XPathTree()
            return (filename, get_tree())

        return dict([extract_unit_tree('content.xml', 'document-content'),
                     extract_unit_tree('meta.xml', 'document-meta'),
                     extract_unit_tree('styles.xml', 'document-styles')])

    def translate_dom_trees(unit_trees, dom_trees):
        make_parse_state = lambda: extract.ParseState(odf_shared.no_translate_content_elements, odf_shared.inline_elements)
        for filename, dom_tree in dom_trees.iteritems():
            file_unit_tree = unit_trees[filename]
            generate.apply_translations(dom_tree.getroot(), file_unit_tree, generate.replace_dom_text(make_parse_state))
        return dom_trees

    # Since the convertoptionsparser will give us an open file, we risk that
    # it could have been opened in non-binary mode on Windows, and then we'll
    # have problems, so let's make sure we have what we want.
    template.close()
    template = file(template.name, mode='rb')
    dom_trees = load_dom_trees(template)
    unit_trees = load_unit_tree(input_file, dom_trees)
    return translate_dom_trees(unit_trees, dom_trees)


def write_odf(xlf_data, template, output_file, dom_trees):

    def write_content_to_odf(output_zip, dom_trees):
        for filename, dom_tree in dom_trees.iteritems():
            output_zip.writestr(filename, etree.tostring(dom_tree, encoding='UTF-8', xml_declaration=True))

    # Since the convertoptionsparser will give us an open file, we risk that
    # it could have been opened in non-binary mode on Windows, and then we'll
    # have problems, so let's make sure we have what we want.
    template.close()
    template = file(template.name, mode='rb')
    template_zip = zipfile.ZipFile(template, 'r')
    output_file.close()
    output_file = file(output_file.name, mode='wb')
    output_zip = zipfile.ZipFile(output_file, 'w', compression=zipfile.ZIP_DEFLATED)
    # Let's keep the XLIFF file out of the generated ODF for now. Note the
    # weird handling of the manifest since it can only be written to the ZIP
    # file once.
#    output_zip = odf_io.copy_odf(template_zip, output_zip, dom_trees.keys() + ['META-INF/manifest.xml'])
#    output_zip = odf_io.add_file(output_zip, template_zip.read('META-INF/manifest.xml'), 'translation.xlf', xlf_data)
    output_zip = odf_io.copy_odf(template_zip, output_zip, dom_trees.keys())
    write_content_to_odf(output_zip, dom_trees)


def convertxliff(input_file, output_file, template):
    """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
    xlf_data = input_file.read()
    dom_trees = translate_odf(template, StringIO(xlf_data))
    write_odf(xlf_data, template, output_file, dom_trees)
    output_file.close()
    return True

formats = {
    ('xlf', 'odt'): ("odt", convertxliff),  # Text
    ('xlf', 'ods'): ("ods", convertxliff),  # Spreadsheet
    ('xlf', 'odp'): ("odp", convertxliff),  # Presentation
    ('xlf', 'odg'): ("odg", convertxliff),  # Drawing
    ('xlf', 'odc'): ("odc", convertxliff),  # Chart
    ('xlf', 'odf'): ("odf", convertxliff),  # Formula
    ('xlf', 'odi'): ("odi", convertxliff),  # Image
    ('xlf', 'odm'): ("odm", convertxliff),  # Master Document
    ('xlf', 'ott'): ("ott", convertxliff),  # Text template
    ('xlf', 'ots'): ("ots", convertxliff),  # Spreadsheet template
    ('xlf', 'otp'): ("otp", convertxliff),  # Presentation template
    ('xlf', 'otg'): ("otg", convertxliff),  # Drawing template
    ('xlf', 'otc'): ("otc", convertxliff),  # Chart template
    ('xlf', 'otf'): ("otf", convertxliff),  # Formula template
    ('xlf', 'oti'): ("oti", convertxliff),  # Image template
    ('xlf', 'oth'): ("oth", convertxliff),  # Web page template
}


def main(argv=None):
    from translate.convert import convert

    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = xliff2oo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2010-2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert XLIFF localization files to an OpenOffice.org (SDF) localization file.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/oo2po.html
for examples and usage instructions.
"""

import logging
import os
import time

from translate.filters import autocorrect, checks, pofilter
from translate.storage import factory, oo


logger = logging.getLogger(__name__)


class reoo:

    def __init__(self, templatefile, languages=None, timestamp=None, includefuzzy=False, long_keys=False, filteraction="exclude"):
        """construct a reoo converter for the specified languages (timestamp=0 means leave unchanged)"""
        # languages is a pair of language ids
        self.long_keys = long_keys
        self.readoo(templatefile)
        self.languages = languages
        self.filteraction = filteraction
        if timestamp is None:
            self.timestamp = time.strptime("2002-02-02 02:02:02", "%Y-%m-%d %H:%M:%S")
        else:
            self.timestamp = timestamp
        if self.timestamp:
            self.timestamp_str = time.strftime("%Y-%m-%d %H:%M:%S", self.timestamp)
        else:
            self.timestamp_str = None
        self.includefuzzy = includefuzzy

    def makeindex(self):
        """makes an index of the oo keys that are used in the source file"""
        self.index = {}
        for ookey, theoo in self.o.ookeys.iteritems():
            sourcekey = oo.makekey(ookey, self.long_keys)
            self.index[sourcekey] = theoo

    def readoo(self, of):
        """read in the oo from the file"""
        oosrc = of.read()
        self.o = oo.oofile()
        self.o.parse(oosrc)
        self.makeindex()

    def handleunit(self, unit):
        # TODO: make this work for multiple columns in oo...
        locations = unit.getlocations()
        # technically our formats should just have one location for each entry...
        # but we handle multiple ones just to be safe...
        for location in locations:
            subkeypos = location.rfind('.')
            subkey = location[subkeypos+1:]
            key = location[:subkeypos]
            # this is just to handle our old system of using %s/%s:%s instead of %s/%s#%s
            key = key.replace(':', '#')
            # this is to handle using / instead of \ in the sourcefile...
            key = key.replace('\\', '/')
            key = oo.normalizefilename(key)
            if key in self.index:
                # now we need to replace the definition of entity with msgstr
                theoo = self.index[key]  # find the oo
                self.applytranslation(key, subkey, theoo, unit)
            else:
                logger.warning("couldn't find key %s from po in %d keys",
                               key, len(self.index))
                try:
                    sourceunitlines = str(unit)
                    if isinstance(sourceunitlines, unicode):
                        sourceunitlines = sourceunitlines.encode("utf-8")
                    logger.warning(sourceunitlines)
                except:
                    logger.error("error outputting source unit %r", str(unit))

    def applytranslation(self, key, subkey, theoo, unit):
        """applies the translation from the source unit to the oo unit"""
        if not self.includefuzzy and unit.isfuzzy():
            return
        makecopy = False
        if self.languages is None:
            part1 = theoo.lines[0]
            if len(theoo.lines) > 1:
                part2 = theoo.lines[1]
            else:
                makecopy = True
        else:
            part1 = theoo.languages[self.languages[0]]
            if self.languages[1] in theoo.languages:
                part2 = theoo.languages[self.languages[1]]
            else:
                makecopy = True
        if makecopy:
            part2 = oo.ooline(part1.getparts())
        unquotedid = unit.source
        unquotedstr = unit.target
        # If there is no translation, we don't want to add a line
        if len(unquotedstr.strip()) == 0:
            return
        if isinstance(unquotedstr, unicode):
            unquotedstr = unquotedstr.encode("UTF-8")
        # finally set the new definition in the oo, but not if its empty
        if len(unquotedstr) > 0:
            subkey = subkey.strip()
            setattr(part2, subkey, unquotedstr)
        # set the modified time
        if self.timestamp_str:
            part2.timestamp = self.timestamp_str
        if self.languages:
            part2.languageid = self.languages[1]
        if makecopy:
            theoo.addline(part2)

    def convertstore(self, sourcestore):
        self.p = sourcestore
        # translate the strings
        for unit in self.p.units:
            # there may be more than one element due to msguniq merge
            if filter.validelement(unit, self.p.filename, self.filteraction):
                self.handleunit(unit)
        # return the modified oo file object
        return self.o


def getmtime(filename):
    import stat
    return time.localtime(os.stat(filename)[stat.ST_MTIME])


class oocheckfilter(pofilter.pocheckfilter):

    def validelement(self, unit, filename, filteraction):
        """Returns whether or not to use unit in conversion. (filename is just for error reporting)"""
        if filteraction == "none":
            return True
        filterresult = self.filterunit(unit)
        if filterresult:
            if filterresult != autocorrect:
                for filtername, filtermessage in filterresult.iteritems():
                    location = unit.getlocations()[0]
                    if filtername in self.options.error:
                        logger.error("Error at %s::%s: %s",
                                     filename, location, filtermessage)
                        return not filteraction in ["exclude-all", "exclude-serious"]
                    if filtername in self.options.warning or self.options.alwayswarn:
                        logger.warning("Warning at %s::%s: %s",
                                       filename, location, filtermessage)
                        return not filteraction in ["exclude-all"]
        return True


class oofilteroptions:
    error = ['variables', 'xmltags', 'escapes']
    warning = ['blank']
    #To only issue warnings for tests listed in warning, change the following to False:
    alwayswarn = True
    limitfilters = error + warning
    #To use all available tests, uncomment the following:
    #limitfilters = []
    #To exclude certain tests, list them in here:
    excludefilters = {}
    includefuzzy = False
    includereview = False
    autocorrect = False

options = oofilteroptions()
filter = oocheckfilter(options, [checks.OpenOfficeChecker, checks.StandardUnitChecker], checks.openofficeconfig)


def convertoo(inputfile, outputfile, templatefile, sourcelanguage=None,
              targetlanguage=None, timestamp=None, includefuzzy=False,
              multifilestyle="single", skip_source=False, filteraction=None):
    inputstore = factory.getobject(inputfile)
    inputstore.filename = getattr(inputfile, 'name', '')
    if not targetlanguage:
        raise ValueError("You must specify the target language")
    if not sourcelanguage:
        if targetlanguage.isdigit():
            sourcelanguage = "01"
        else:
            sourcelanguage = "en-US"
    languages = (sourcelanguage, targetlanguage)
    if templatefile is None:
        raise ValueError("must have template file for oo files")
    else:
        convertor = reoo(templatefile, languages=languages,
                         timestamp=timestamp, includefuzzy=includefuzzy,
                         long_keys=multifilestyle != "single",
                         filteraction=filteraction)
    outputstore = convertor.convertstore(inputstore)
    # TODO: check if we need to manually delete missing items
    outputfile.write(outputstore.__str__(skip_source, targetlanguage))
    return True


def main(argv=None):
    from translate.convert import convert
    formats = {
                ("po", "oo"): ("oo", convertoo),
                ("xlf", "oo"): ("oo", convertoo),
                ("xlf", "sdf"): ("sdf", convertoo),
              }
    # always treat the input as an archive unless it is a directory
    archiveformats = {(None, "output"): oo.oomultifile, (None, "template"): oo.oomultifile}
    parser = convert.ArchiveConvertOptionParser(formats, usetemplates=True, description=__doc__, archiveformats=archiveformats)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
                      help="set target language code (e.g. af-ZA) [required]",
                      metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage",
                      default=None,
                      help="set source language code (default en-US)",
                      metavar="LANG")
    parser.add_option("-T", "--keeptimestamp", dest="timestamp", default=None,
                      action="store_const", const=0,
            help="don't change the timestamps of the strings")
    parser.add_option("", "--nonrecursiveoutput", dest="allowrecursiveoutput",
                      default=True, action="store_false",
                      help="don't treat the output oo as a recursive store")
    parser.add_option("", "--nonrecursivetemplate",
                      dest="allowrecursivetemplate", default=True,
                      action="store_false",
                      help="don't treat the template oo as a recursive store")
    parser.add_option("", "--skipsource", dest="skip_source", default=False,
                      action="store_true",
                      help="don't output the source language, but fallback to it where needed")
    parser.add_option("", "--filteraction", dest="filteraction", default="none", metavar="ACTION",
                      help="action on pofilter failure: none (default), warn, exclude-serious, exclude-all")
    parser.add_fuzzy_option()
    parser.add_multifile_option()
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.passthrough.append("timestamp")
    parser.passthrough.append("skip_source")
    parser.passthrough.append("filteraction")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = xliff2po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert XLIFF localization files to Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/xliff2po.html
for examples and usage instructions.
"""

from translate.misc import wStringIO
from translate.storage import po, xliff


class xliff2po:

    def converttransunit(self, transunit):
        """makes a pounit from the given transunit"""
        thepo = po.pounit()

        # Header
        if transunit.getrestype() == "x-gettext-domain-header":
            thepo.source = ""
        else:
            thepo.source = transunit.source
        thepo.target = transunit.target

        # Location comments
        locations = transunit.getlocations()
        if locations:
            thepo.addlocations(locations)

        # NOTE: Supporting both <context> and <note> tags in xliff files
        # for comments
        # Translator comments
        trancomments = transunit.getnotes("translator")
        if trancomments:
            thepo.addnote(trancomments, origin="translator")

        # Automatic and Developer comments
        autocomments = transunit.getnotes("developer")
        if autocomments:
            thepo.addnote(autocomments, origin="developer")

        # See 5.6.1 of the spec. We should not check fuzzyness, but approved
        # attribute
        if transunit.isfuzzy():
            thepo.markfuzzy(True)

        return thepo

    def convertstore(self, inputfile, duplicatestyle="msgctxt"):
        """Converts a .xliff file to .po format"""
        # XXX: The inputfile is converted to string because Pootle supplies
        # XXX: a PootleFile object as input which cannot be sent to PoXliffFile
        # XXX: The better way would be to have a consistent conversion API.
        if not isinstance(inputfile, (file, wStringIO.StringIO)):
            inputfile = str(inputfile)
        XliffFile = xliff.xlifffile.parsestring(inputfile)
        thetargetfile = po.pofile()
        targetheader = thetargetfile.header()
        # TODO: support multiple files
        for transunit in XliffFile.units:
            if transunit.isheader():
                thetargetfile.updateheader(add=True, **XliffFile.parseheader())
                if transunit.getnotes('translator'):
                    targetheader.addnote(transunit.getnotes('translator'),
                                         origin='translator',
                                         position='replace')
                if transunit.getnotes('developer'):
                    targetheader.addnote(transunit.getnotes('developer'),
                                         origin='developer',
                                         position='replace')
                targetheader.markfuzzy(transunit.isfuzzy())
                continue
            thepo = self.converttransunit(transunit)
            thetargetfile.addunit(thepo)
        thetargetfile.removeduplicates(duplicatestyle)
        return thetargetfile


def convertxliff(inputfile, outputfile, templates, duplicatestyle="msgctxt"):
    """reads in stdin using fromfileclass, converts using convertorclass,
    writes to stdout"""
    convertor = xliff2po()
    outputstore = convertor.convertstore(inputfile, duplicatestyle)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main(argv=None):
    from translate.convert import convert
    formats = {"xlf": ("po", convertxliff)}
    parser = convert.ConvertOptionParser(formats, usepots=True,
                                         description=__doc__)
    parser.add_duplicates_option()
    parser.run(argv)

########NEW FILE########
__FILENAME__ = autocorrect
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006, 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A set of autocorrect functions that fix common punctuation and space problems automatically"""

from translate.filters import decoration


def correct(source, target):
    """Runs a set of easy and automatic corrections

    Current corrections include:
      - Ellipses - align target to use source form of ellipses (either three dots or the Unicode ellipses characters)
      - Missing whitespace and start or end of the target
      - Missing punction (.:?) at the end of the target
    """
    old_target = target
    if target == "":
        return None
    if u"" in source and "..." in target:
        target = target.replace("...", u"")
    elif "..." in source and u"" in target:
        target = target.replace(u"", "...")
    if decoration.spacestart(source) != decoration.spacestart(target) or decoration.spaceend(source) != decoration.spaceend(target):
        target = decoration.spacestart(source) + target.strip() + decoration.spaceend(source)
    punctuation = (".", ":", ". ", ": ", "?")
    puncendid = decoration.puncend(source, punctuation)
    puncendstr = decoration.puncend(target, punctuation)
    if puncendid != puncendstr:
        if not puncendstr:
            target = target + puncendid
        else:
            target = target[:-len(puncendstr)] + puncendid
    if source[:1].isalpha() and target[:1].isalpha():
        if source[:1].isupper() and target[:1].islower():
            target = target[:1].upper() + target[1:]
        elif source[:1].islower() and target[:1].isupper():
            target = target[:1].lower() + target[1:]
    if old_target != target:
        return target
    else:
        return None

########NEW FILE########
__FILENAME__ = checks
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2011 Zuza Software Foundation
# 2013 F Wolff
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This is a set of validation checks that can be performed on translation
units.

Derivatives of UnitChecker (like StandardUnitChecker) check translation units,
and derivatives of TranslationChecker (like StandardChecker) check
(source, target) translation pairs.

When adding a new test here, please document and explain their behaviour on the
:doc:`pofilter tests </commands/pofilter_tests>` page.
"""

import logging
import re

from translate.filters import decoration, helpers, prefilters, spelling
from translate.filters.decorators import (cosmetic, critical, extraction,
                                          functional)
from translate.lang import data, factory
from translate.misc import lru


logger = logging.getLogger(__name__)

# These are some regular expressions that are compiled for use in some tests

# printf syntax based on http://en.wikipedia.org/wiki/Printf which doesn't
# cover everything we leave \w instead of specifying the exact letters as
# this should capture printf types defined in other platforms.
# Extended to support Python named format specifiers and objective-C special
# "%@" format specifier
# (see https://developer.apple.com/library/mac/documentation/Cocoa/Conceptual/Strings/Articles/formatSpecifiers.html)
printf_pat = re.compile('''
        %(                          # initial %
              (?:(?P<ord>\d+)\$|    # variable order, like %1$s
              \((?P<key>\w+)\))?    # Python style variables, like %(var)s
        (?P<fullvar>
            [+#-]*                  # flags
            (?:\d+)?                # width
            (?:\.\d+)?              # precision
            (hh\|h\|l\|ll)?         # length formatting
            (?P<type>[\w%@]))       # type (%s, %d, etc.)
        )''', re.VERBOSE)

# The name of the XML tag
tagname_re = re.compile("<[\s]*([\w\/]*).*?(/)?[\s]*>", re.DOTALL)

# We allow escaped quotes, probably for old escaping style of OOo helpcontent
#TODO: remove escaped strings once usage is audited
property_re = re.compile(" (\w*)=((\\\\?\".*?\\\\?\")|(\\\\?'.*?\\\\?'))")

# The whole tag
tag_re = re.compile("<[^>]+>")

gconf_attribute_re = re.compile('"[a-z_]+?"')

# XML/HTML tags in LibreOffice help and readme, exclude short tags
lo_tag_re = re.compile('''<[/]??[a-z][a-z_\-]+?(?:| +[a-z]+?=".*?") *>''')

def tagname(string):
    """Returns the name of the XML/HTML tag in string"""
    tagname_match = tagname_re.match(string)
    return tagname_match.groups(1)[0] + tagname_match.groups('')[1]


def intuplelist(pair, list):
    """Tests to see if pair == (a,b,c) is in list, but handles None entries in
    list as wildcards (only allowed in positions "a" and "c"). We take a
    shortcut by only considering "c" if "b" has already matched."""
    a, b, c = pair

    if (b, c) == (None, None):
        #This is a tagname
        return pair

    for pattern in list:
        x, y, z = pattern

        if (x, y) in [(a, b), (None, b)]:
            if z in [None, c]:
                return pattern

    return pair


def tagproperties(strings, ignore):
    """Returns all the properties in the XML/HTML tag string as
    (tagname, propertyname, propertyvalue), but ignore those combinations
    specified in ignore."""
    properties = []

    for string in strings:
        tag = tagname(string)
        properties += [(tag, None, None)]
        #Now we isolate the attribute pairs.
        pairs = property_re.findall(string)

        for property, value, a, b in pairs:
            #Strip the quotes:
            value = value[1:-1]

            canignore = False

            if (tag, property, value) in ignore or \
               intuplelist((tag, property, value), ignore) != (tag, property, value):
                canignore = True
                break

            if not canignore:
                properties += [(tag, property, value)]

    return properties


class FilterFailure(Exception):
    """This exception signals that a Filter didn't pass, and gives an
    explanation or a comment.
    """

    def __init__(self, messages):
        if not isinstance(messages, list):
            messages = [messages]

        assert isinstance(messages[0], unicode)  # Assumption: all of same type

        self.messages = messages

    def __unicode__(self):
        return unicode(u", ".join(self.messages))

    def __str__(self):
        return str(u", ".join(self.messages))


class SeriousFilterFailure(FilterFailure):
    """This exception signals that a Filter didn't pass, and the bad translation
    might break an application (so the string will be marked fuzzy)"""
    pass

#(tag, attribute, value) specifies a certain attribute which can be changed/
#ignored if it exists inside tag. In the case where there is a third element
#in the tuple, it indicates a property value that can be ignored if present
#(like defaults, for example)
#If a certain item is None, it indicates that it is relevant for all values of
#the property/tag that is specified as None. A non-None value of "value"
#indicates that the value of the attribute must be taken into account.
common_ignoretags = [(None, "xml-lang", None)]
common_canchangetags = [
    ("img", "alt", None),
    (None, "title", None),
    (None, "dir", None),
    (None, "lang", None),
]
# Actually the title tag is allowed on many tags in HTML (but probably not all)


class CheckerConfig(object):
    """Object representing the configuration of a checker."""

    def __init__(self, targetlanguage=None, accelmarkers=None, varmatches=None,
                 notranslatewords=None, musttranslatewords=None,
                 validchars=None, punctuation=None, endpunctuation=None,
                 ignoretags=None, canchangetags=None, criticaltests=None,
                 credit_sources=None):
        # Init lists
        self.accelmarkers = self._init_list(accelmarkers)
        self.varmatches = self._init_list(varmatches)
        self.criticaltests = self._init_list(criticaltests)
        self.credit_sources = self._init_list(credit_sources)

        # Lang data
        self.updatetargetlanguage(targetlanguage)
        self.sourcelang = factory.getlanguage('en')

        # Inits with default values
        self.punctuation = self._init_default(data.normalized_unicode(punctuation),
                                              self.lang.punctuation)
        self.endpunctuation = self._init_default(data.normalized_unicode(endpunctuation),
                                                 self.lang.sentenceend)
        self.ignoretags = self._init_default(ignoretags, common_ignoretags)
        self.canchangetags = self._init_default(canchangetags, common_canchangetags)

        # Other data
        # TODO: allow user configuration of untranslatable words
        self.notranslatewords = dict.fromkeys([data.normalized_unicode(key) for key in self._init_list(notranslatewords)])
        self.musttranslatewords = dict.fromkeys([data.normalized_unicode(key) for key in self._init_list(musttranslatewords)])
        validchars = data.normalized_unicode(validchars)
        self.validcharsmap = {}
        self.updatevalidchars(validchars)


    def _init_list(self, list):
        """initialise configuration paramaters that are lists

        :type list: List
        :param list: None (we'll initialise a blank list) or a list paramater
        :rtype: List
        """
        if list is None:
            list = []

        return list


    def _init_default(self, param, default):
        """Initialise parameters that can have default options.

        :param param: the user supplied paramater value
        :param default: default values when param is not specified
        :return: the paramater as specified by the user of the default settings
        """
        if param is None:
            return default

        return param


    def update(self, otherconfig):
        """Combines the info in ``otherconfig`` into this config object."""
        self.targetlanguage = otherconfig.targetlanguage or self.targetlanguage
        self.updatetargetlanguage(self.targetlanguage)
        self.accelmarkers.extend([c for c in otherconfig.accelmarkers if not c in self.accelmarkers])
        self.varmatches.extend(otherconfig.varmatches)
        self.notranslatewords.update(otherconfig.notranslatewords)
        self.musttranslatewords.update(otherconfig.musttranslatewords)
        self.validcharsmap.update(otherconfig.validcharsmap)
        self.punctuation += otherconfig.punctuation
        self.endpunctuation += otherconfig.endpunctuation
        #TODO: consider also updating in the following cases:
        self.ignoretags = otherconfig.ignoretags
        self.canchangetags = otherconfig.canchangetags
        self.criticaltests.extend(otherconfig.criticaltests)
        self.credit_sources = otherconfig.credit_sources


    def updatevalidchars(self, validchars):
        """Updates the map that eliminates valid characters."""
        if validchars is None:
            return True

        validcharsmap = dict([(ord(validchar), None) for validchar in data.normalized_unicode(validchars)])
        self.validcharsmap.update(validcharsmap)


    def updatetargetlanguage(self, langcode):
        """Updates the target language in the config to the given target
        language.
        """
        self.targetlanguage = langcode
        self.lang = factory.getlanguage(langcode)


def cache_results(f):

    def cached_f(self, param1):
        key = (f.__name__, param1)
        res_cache = self.results_cache

        if key in res_cache:
            return res_cache[key]
        else:
            value = f(self, param1)
            res_cache[key] = value
            return value

    return cached_f


class UnitChecker(object):
    """Parent Checker class which does the checking based on functions available
    in derived classes.
    """
    preconditions = {}

    #: Categories where each checking function falls into
    #: Function names are used as keys, categories are the values
    categories = {}


    def __init__(self, checkerconfig=None, excludefilters=None,
                 limitfilters=None, errorhandler=None):
        self.errorhandler = errorhandler

        if checkerconfig is None:
            self.setconfig(CheckerConfig())
        else:
            self.setconfig(checkerconfig)

        # Exclude functions defined in UnitChecker from being treated as tests.
        self.helperfunctions = {}

        for functionname in dir(UnitChecker):
            function = getattr(self, functionname)

            if callable(function):
                self.helperfunctions[functionname] = function

        self.defaultfilters = self.getfilters(excludefilters, limitfilters)
        self.results_cache = {}


    def getfilters(self, excludefilters=None, limitfilters=None):
        """Returns dictionary of available filters, including/excluding those
        in the given lists.
        """
        filters = {}

        if limitfilters is None:
            # use everything available unless instructed
            limitfilters = dir(self)

        if excludefilters is None:
            excludefilters = {}

        for functionname in limitfilters:

            if functionname in excludefilters:
                continue

            if functionname in self.helperfunctions:
                continue

            if functionname == "errorhandler":
                continue

            filterfunction = getattr(self, functionname, None)
            if not callable(filterfunction):
                continue

            filters[functionname] = filterfunction

        return filters


    def setconfig(self, config):
        """Sets the accelerator list."""
        self.config = config
        self.accfilters = [prefilters.filteraccelerators(accelmarker) for accelmarker in self.config.accelmarkers]
        self.varfilters = [prefilters.filtervariables(startmatch, endmatch, prefilters.varname)
                for startmatch, endmatch in self.config.varmatches]
        self.removevarfilter = [prefilters.filtervariables(startmatch, endmatch,
                                                           prefilters.varnone)
                for startmatch, endmatch in self.config.varmatches]


    def setsuggestionstore(self, store):
        """Sets the filename that a checker should use for evaluating
        suggestions.
        """
        self.suggestion_store = store

        if self.suggestion_store:
            self.suggestion_store.require_index()


    def filtervariables(self, str1):
        """Filter out variables from ``str1``."""
        return helpers.multifilter(str1, self.varfilters)
    filtervariables = cache_results(filtervariables)

    def removevariables(self, str1):
        """Remove variables from ``str1``."""
        return helpers.multifilter(str1, self.removevarfilter)
    removevariables = cache_results(removevariables)

    def filteraccelerators(self, str1):
        """Filter out accelerators from ``str1``."""
        return helpers.multifilter(str1, self.accfilters, None)
    filteraccelerators = cache_results(filteraccelerators)


    def filteraccelerators_by_list(self, str1, acceptlist=None):
        """Filter out accelerators from ``str1``."""
        return helpers.multifilter(str1, self.accfilters, acceptlist)


    def filterwordswithpunctuation(self, str1):
        """Replaces words with punctuation with their unpunctuated
        equivalents.
        """
        return prefilters.filterwordswithpunctuation(str1)
    filterwordswithpunctuation = cache_results(filterwordswithpunctuation)


    def filterxml(self, str1):
        """Filter out XML from the string so only text remains."""
        return tag_re.sub("", str1)
    filterxml = cache_results(filterxml)


    def run_test(self, test, unit):
        """Runs the given test on the given unit.

        Note that this can raise a :exc:`FilterFailure` as part of normal operation.
        """
        return test(unit)

    def run_filters(self, unit, categorised=False):
        """Run all the tests in this suite.

        :rtype: Dictionary
        :return: Content of the dictionary is as follows::

           {'testname': { 'message': message_or_exception, 'category': failure_category } }
        """
        self.results_cache = {}
        failures = {}
        ignores = self.config.lang.ignoretests[:]
        functionnames = self.defaultfilters.keys()
        priorityfunctionnames = self.preconditions.keys()
        otherfunctionnames = filter(lambda functionname: functionname not in self.preconditions, functionnames)

        for functionname in priorityfunctionnames + otherfunctionnames:
            if functionname in ignores:
                continue

            filterfunction = getattr(self, functionname, None)

            # This filterfunction may only be defined on another checker if
            # using TeeChecker
            if filterfunction is None:
                continue

            filtermessage = filterfunction.__doc__

            try:
                filterresult = self.run_test(filterfunction, unit)
            except FilterFailure as e:
                filterresult = False
                filtermessage = unicode(e)
            except Exception as e:
                if self.errorhandler is None:
                    raise ValueError("error in filter %s: %r, %r, %s" %
                                     (functionname, unit.source, unit.target, e))
                else:
                    filterresult = self.errorhandler(functionname, unit.source,
                                                     unit.target, e)

            if not filterresult:
                # We test some preconditions that aren't actually a cause for
                # failure
                if functionname in self.defaultfilters:
                    failures[functionname] = {
                            'message': filtermessage,
                            'category': self.categories[functionname],
                            }

                if functionname in self.preconditions:
                    for ignoredfunctionname in self.preconditions[functionname]:
                        ignores.append(ignoredfunctionname)

        self.results_cache = {}

        if not categorised:
            for name, info in failures.iteritems():
                failures[name] = info['message']
        return failures


class TranslationChecker(UnitChecker):
    """A checker that passes source and target strings to the checks, not the
    whole unit.

    This provides some speedup and simplifies testing.
    """

    def __init__(self, checkerconfig=None, excludefilters=None,
                 limitfilters=None, errorhandler=None):
        super(TranslationChecker, self).__init__(checkerconfig, excludefilters,
                                                 limitfilters, errorhandler)

        # caches for spell checking results across units/runs
        self.source_spell_cache = lru.LRUCachingDict(256, cullsize=5, aggressive_gc=False)
        self.target_spell_cache = lru.LRUCachingDict(512, cullsize=5, aggressive_gc=False)

    def run_test(self, test, unit):
        """Runs the given test on the given unit.

        Note that this can raise a :exc:`FilterFailure` as part of normal
        operation.
        """
        if self.hasplural:
            filtermessages = []
            filterresult = True

            for pluralform in unit.target.strings:
                try:
                    if not test(self.str1, unicode(pluralform)):
                        filterresult = False
                except FilterFailure as e:
                    filterresult = False
                    filtermessages.extend(e.messages)

            if not filterresult and filtermessages:
                raise FilterFailure(filtermessages)
            else:
                return filterresult
        else:
            return test(self.str1, self.str2)


    def run_filters(self, unit, categorised=False):
        """Do some optimisation by caching some data of the unit for the
        benefit of :meth:`~TranslationChecker.run_test`.
        """
        self.str1 = data.normalized_unicode(unit.source) or u""
        self.str2 = data.normalized_unicode(unit.target) or u""
        self.hasplural = unit.hasplural()
        self.locations = unit.getlocations()

        return super(TranslationChecker, self).run_filters(unit, categorised)


class TeeChecker:
    """A Checker that controls multiple checkers."""

    #: Categories where each checking function falls into
    #: Function names are used as keys, categories are the values
    categories = {}


    def __init__(self, checkerconfig=None, excludefilters=None,
                 limitfilters=None, checkerclasses=None, errorhandler=None,
                 languagecode=None):
        """construct a TeeChecker from the given checkers"""
        self.limitfilters = limitfilters

        if checkerclasses is None:
            checkerclasses = [StandardChecker]

        self.checkers = [checkerclass(checkerconfig=checkerconfig,
                                      excludefilters=excludefilters,
                                      limitfilters=limitfilters,
                                      errorhandler=errorhandler) for checkerclass in checkerclasses]

        if languagecode:
            for checker in self.checkers:
                checker.config.updatetargetlanguage(languagecode)

            # Let's hook up the language specific checker
            lang_checker = self.checkers[0].config.lang.checker

            if lang_checker:
                self.checkers.append(lang_checker)

        self.combinedfilters = self.getfilters(excludefilters, limitfilters)
        self.config = checkerconfig or self.checkers[0].config


    def getfilters(self, excludefilters=None, limitfilters=None):
        """Returns a dictionary of available filters, including/excluding
        those in the given lists.
        """
        if excludefilters is None:
            excludefilters = {}

        filterslist = [checker.getfilters(excludefilters, limitfilters) for checker in self.checkers]
        self.combinedfilters = {}

        for filters in filterslist:
            self.combinedfilters.update(filters)

        # TODO: move this somewhere more sensible (a checkfilters method?)
        if limitfilters is not None:

            for filtername in limitfilters:

                if not filtername in self.combinedfilters:
                    logger.warning("could not find filter %s", filtername)

        return self.combinedfilters


    def run_filters(self, unit, categorised=False):
        """Run all the tests in the checker's suites."""
        failures = {}

        for checker in self.checkers:
            failures.update(checker.run_filters(unit, categorised))

        return failures


    def setsuggestionstore(self, store):
        """Sets the filename that a checker should use for evaluating
        suggestions.
        """
        for checker in self.checkers:
            checker.setsuggestionstore(store)


class StandardChecker(TranslationChecker):
    """The basic test suite for source -> target translations."""


    @extraction
    def untranslated(self, str1, str2):
        """Checks whether a string has been translated at all."""
        str2 = prefilters.removekdecomments(str2)

        return not (len(str1.strip()) > 0 and len(str2) == 0)


    @functional
    def unchanged(self, str1, str2):
        """Checks whether a translation is basically identical to the original
        string.
        """
        str1 = self.filteraccelerators(self.removevariables(str1)).strip()
        str2 = self.filteraccelerators(self.removevariables(str2)).strip()

        if len(str1) < 2:
            return True

        # If the whole string is upperase, or nothing in the string can go
        # towards uppercase, let's assume there is nothing translatable
        # TODO: reconsider
        if (str1.isupper() or str1.upper() == str1) and str1 == str2:
            return True

        if self.config.notranslatewords:
            words1 = str1.split()
            if len(words1) == 1 and [word for word in words1 if word in self.config.notranslatewords]:
                #currently equivalent to:
                #   if len(words1) == 1 and words1[0] in self.config.notranslatewords:
                #why do we only test for one notranslate word?
                return True

        # we could also check for things like str1.isnumeric(), but the test
        # above (str1.upper() == str1) makes this unnecessary
        if str1.lower() == str2.lower():
            raise FilterFailure(u"Consider translating")

        return True


    @functional
    def blank(self, str1, str2):
        """Checks whether a translation only contains spaces."""
        len1 = len(str1.strip())
        len2 = len(str2.strip())

        if len1 > 0 and len(str2) != 0 and len2 == 0:
            raise FilterFailure(u"Translation is empty")
        else:
            return True


    @functional
    def short(self, str1, str2):
        """Checks whether a translation is much shorter than the original
        string.
        """
        len1 = len(str1.strip())
        len2 = len(str2.strip())

        if (len1 > 0) and (0 < len2 < (len1 * 0.1)) or ((len1 > 1) and (len2 == 1)):
            raise FilterFailure(u"The translation is much shorter than the original")
        else:
            return True


    @functional
    def long(self, str1, str2):
        """Checks whether a translation is much longer than the original
        string.
        """
        len1 = len(str1.strip())
        len2 = len(str2.strip())

        if (len1 > 0) and (0 < len1 < (len2 * 0.1)) or ((len1 == 1) and (len2 > 1)):
            raise FilterFailure(u"The translation is much longer than the original")
        else:
            return True


    @critical
    def escapes(self, str1, str2):
        """Checks whether escaping is consistent between the two strings."""
        if not helpers.countsmatch(str1, str2, (u"\\", u"\\\\")):
            escapes1 = u", ".join([u"'%s'" % word for word in str1.split() if u"\\" in word])
            escapes2 = u", ".join([u"'%s'" % word for word in str2.split() if u"\\" in word])

            raise SeriousFilterFailure(u"Escapes in original (%s) don't match "
                                       "escapes in translation (%s)" %
                                       (escapes1, escapes2))
        else:
            return True


    @critical
    def newlines(self, str1, str2):
        """Checks whether newlines are consistent between the two strings."""
        if not helpers.countsmatch(str1, str2, (u"\n", u"\r")):
            raise FilterFailure(u"Different line endings")

        if str1.endswith(u"\n") and not str2.endswith(u"\n"):
            raise FilterFailure(u"Newlines different at end")

        if str1.startswith(u"\n") and not str2.startswith(u"\n"):
            raise FilterFailure(u"Newlines different at beginning")

        return True


    @critical
    def tabs(self, str1, str2):
        """Checks whether tabs are consistent between the two strings."""
        if not helpers.countmatch(str1, str2, "\t"):
            raise SeriousFilterFailure(u"Different tabs")
        else:
            return True


    @cosmetic
    def singlequoting(self, str1, str2):
        """Checks whether singlequoting is consistent between the two strings."""
        str1 = self.filterwordswithpunctuation(self.filteraccelerators(self.filtervariables(str1)))
        str1 = self.config.lang.punctranslate(str1)

        str2 = self.filterwordswithpunctuation(self.filteraccelerators(self.filtervariables(str2)))

        if helpers.countsmatch(str1, str2, (u"'", u"''", u"\\'")):
            return True
        else:
            raise FilterFailure(u"Different quotation marks")


    @cosmetic
    def doublequoting(self, str1, str2):
        """Checks whether doublequoting is consistent between the
        two strings.
        """
        str1 = self.filteraccelerators(self.filtervariables(str1))
        str1 = self.filterxml(str1)
        str1 = self.config.lang.punctranslate(str1)

        str2 = self.filteraccelerators(self.filtervariables(str2))
        str2 = self.filterxml(str2)

        if helpers.countsmatch(str1, str2, (u'"', u'""', u'\\"', u"",
                                        u"", u"", u"")):
            return True
        else:
            raise FilterFailure(u"Different quotation marks")


    @cosmetic
    def doublespacing(self, str1, str2):
        """Checks for bad double-spaces by comparing to original."""
        str1 = self.filteraccelerators(str1)
        str2 = self.filteraccelerators(str2)

        if helpers.countmatch(str1, str2, u"  "):
            return True
        else:
            raise FilterFailure(u"Different use of double spaces")


    @cosmetic
    def puncspacing(self, str1, str2):
        """Checks for bad spacing after punctuation."""
        # Convert all nbsp to space, and just check spaces. Useful intermediate
        # step to stricter nbsp checking?
        str1 = self.filteraccelerators(self.filtervariables(str1))
        str1 = self.config.lang.punctranslate(str1)
        str1 = str1.replace(u"\u00a0", u" ")

        if str1.find(u" ") == -1:
            return True

        str2 = self.filteraccelerators(self.filtervariables(str2))
        str2 = str2.replace(u"\u00a0", u" ")

        for puncchar in self.config.punctuation:
            plaincount1 = str1.count(puncchar)

            if not plaincount1:
                continue

            plaincount2 = str2.count(puncchar)

            if plaincount1 != plaincount2:
                continue

            spacecount1 = str1.count(puncchar + u" ")
            spacecount2 = str2.count(puncchar + u" ")

            if spacecount1 != spacecount2:
                # Handle extra spaces that are because of transposed punctuation

                if abs(spacecount1 - spacecount2) == 1 and str1.endswith(puncchar) != str2.endswith(puncchar):
                    continue

                raise FilterFailure(u"Different spacing around punctuation")

        return True


    @critical
    def printf(self, str1, str2):
        """Checks whether printf format strings match."""
        count1 = count2 = plural = None

        # self.hasplural only set by run_filters, not always available
        if 'hasplural' in self.__dict__:
            plural = self.hasplural

        for var_num2, match2 in enumerate(printf_pat.finditer(str2)):
            count2 = var_num2 + 1
            str2ord = match2.group('ord')
            str2key = match2.group('key')

            if str2ord:
                str1ord = None

                for var_num1, match1 in enumerate(printf_pat.finditer(str1)):
                    count1 = var_num1 + 1

                    if match1.group('ord'):
                        if str2ord == match1.group('ord'):
                            str1ord = str2ord

                            if match2.group('fullvar') != match1.group('fullvar'):
                                raise FilterFailure(u"Different printf variable: %s" % match2.group())
                    elif int(str2ord) == var_num1 + 1:
                        str1ord = str2ord

                        if match2.group('fullvar') != match1.group('fullvar'):
                            raise FilterFailure(u"Different printf variable: %s" % match2.group())

                if str1ord is None:
                    raise FilterFailure(u"Added printf variable: %s" % match2.group())
            elif str2key:
                str1key = None

                for var_num1, match1 in enumerate(printf_pat.finditer(str1)):
                    count1 = var_num1 + 1

                    if match1.group('key') and str2key == match1.group('key'):
                        str1key = match1.group('key')

                        # '%.0s' "placeholder" in plural will match anything
                        if plural and match2.group('fullvar') == '.0s':
                            continue

                        if match1.group('fullvar') != match2.group('fullvar'):
                            raise FilterFailure(u"Different printf variable: %s" % match2.group())

                if str1key is None:
                    raise FilterFailure(u"Added printf variable: %s" % match2.group())
            else:
                for var_num1, match1 in enumerate(printf_pat.finditer(str1)):
                    count1 = var_num1 + 1

                    # '%.0s' "placeholder" in plural will match anything
                    if plural and match2.group('fullvar') == '.0s':
                        continue

                    if (var_num1 == var_num2) and (match1.group('fullvar') != match2.group('fullvar')):
                        raise FilterFailure(u"Different printf variable: %s" % match2.group())

        if count2 is None:
            str1_variables = list(m.group() for m in printf_pat.finditer(str1))

            if str1_variables:
                raise FilterFailure(u"Missing printf variable: %s" % u", ".join(str1_variables))

        if (count1 or count2) and (count1 != count2):
            raise FilterFailure(u"Different number of printf variables")

        return 1


    @functional
    def accelerators(self, str1, str2):
        """Checks whether accelerators are consistent between the
        two strings.
        """
        str1 = self.filtervariables(str1)
        str2 = self.filtervariables(str2)
        messages = []

        for accelmarker in self.config.accelmarkers:
            counter1 = decoration.countaccelerators(accelmarker, self.config.sourcelang.validaccel)
            counter2 = decoration.countaccelerators(accelmarker, self.config.lang.validaccel)
            count1, countbad1 = counter1(str1)
            count2, countbad2 = counter2(str2)
            getaccel = decoration.getaccelerators(accelmarker, self.config.lang.validaccel)
            accel2, bad2 = getaccel(str2)

            if count1 == count2:
                continue

            if count1 == 1 and count2 == 0:
                if countbad2 == 1:
                    messages.append(u"Accelerator '%s' appears before an invalid "
                                    "accelerator character '%s'" %
                                    (accelmarker, bad2[0]))
                else:
                    messages.append(u"Missing accelerator '%s'" %
                                    accelmarker)
            elif count1 == 0:
                messages.append(u"Added accelerator '%s'" % accelmarker)
            elif count1 == 1 and count2 > count1:
                messages.append(u"Accelerator '%s' is repeated in translation" %
                                accelmarker)
            else:
                messages.append(u"Accelerator '%s' occurs %d time(s) in original "
                                "and %d time(s) in translation" %
                                (accelmarker, count1, count2))

        if messages:
            if "accelerators" in self.config.criticaltests:
                raise SeriousFilterFailure(messages)
            else:
                raise FilterFailure(messages)

        return True

#    def acceleratedvariables(self, str1, str2):
#        """checks that no variables are accelerated"""
#        messages = []
#        for accelerator in self.config.accelmarkers:
#            for variablestart, variableend in self.config.varmatches:
#                error = accelerator + variablestart
#                if str1.find(error) >= 0:
#                    messages.append(u"original has an accelerated variable")
#                if str2.find(error) >= 0:
#                    messages.append(u"translation has an accelerated variable")
#        if messages:
#            raise FilterFailure(messages)
#        return True


    @critical
    def variables(self, str1, str2):
        """Checks whether variables of various forms are consistent between the
        two strings.
        """
        messages = []
        mismatch1, mismatch2 = [], []
        varnames1, varnames2 = [], []

        for startmarker, endmarker in self.config.varmatches:
            varchecker = decoration.getvariables(startmarker, endmarker)

            if startmarker and endmarker:
                if isinstance(endmarker, int):
                    redecorate = lambda var: startmarker + var
                else:
                    redecorate = lambda var: startmarker + var + endmarker
            elif startmarker:
                redecorate = lambda var: startmarker + var
            else:
                redecorate = lambda var: var

            vars1 = varchecker(str1)
            vars2 = varchecker(str2)

            if vars1 != vars2:
                # we use counts to compare so we can handle multiple variables
                vars1, vars2 = [var for var in vars1 if vars1.count(var) > vars2.count(var)], \
                               [var for var in vars2 if vars1.count(var) < vars2.count(var)]
                # filter variable names we've already seen, so they aren't
                # matched by more than one filter...
                vars1, vars2 = [var for var in vars1 if var not in varnames1], [var for var in vars2 if var not in varnames2]
                varnames1.extend(vars1)
                varnames2.extend(vars2)
                vars1 = map(redecorate, vars1)
                vars2 = map(redecorate, vars2)
                mismatch1.extend(vars1)
                mismatch2.extend(vars2)

        if mismatch1:
            messages.append(u"Do not translate: %s" % u", ".join(mismatch1))
        elif mismatch2:
            messages.append(u"Added variables: %s" % u", ".join(mismatch2))

        if messages and mismatch1:
            raise SeriousFilterFailure(messages)
        elif messages:
            raise FilterFailure(messages)

        return True


    @functional
    def functions(self, str1, str2):
        """Checks that function names are not translated."""
        # We can't just use helpers.funcmatch() since it doesn't ignore order
        if not set(decoration.getfunctions(str1)).symmetric_difference(set(decoration.getfunctions(str2))):
            return True
        else:
            raise FilterFailure(u"Different functions")


    @functional
    def emails(self, str1, str2):
        """Checks that emails are not translated."""
        if helpers.funcmatch(str1, str2, decoration.getemails):
            return True
        else:
            raise FilterFailure(u"Different e-mails")


    @functional
    def urls(self, str1, str2):
        """Checks that URLs are not translated."""
        if helpers.funcmatch(str1, str2, decoration.geturls):
            return True
        else:
            raise FilterFailure(u"Different URLs")


    @functional
    def numbers(self, str1, str2):
        """Checks whether numbers of various forms are consistent between the
        two strings.
        """
        if helpers.countsmatch(str1, str2, decoration.getnumbers(str1)):
            return True
        else:
            raise FilterFailure(u"Different numbers")


    @cosmetic
    def startwhitespace(self, str1, str2):
        """Checks whether whitespace at the beginning of the strings
        matches.
        """
        if helpers.funcmatch(str1, str2, decoration.spacestart):
            return True
        else:
            raise FilterFailure(u"Different whitespace at the start")


    @cosmetic
    def endwhitespace(self, str1, str2):
        """Checks whether whitespace at the end of the strings matches."""
        str1 = self.config.lang.punctranslate(str1)

        if helpers.funcmatch(str1, str2, decoration.spaceend):
            return True
        else:
            raise FilterFailure(u"Different whitespace at the end")


    @cosmetic
    def startpunc(self, str1, str2):
        """Checks whether punctuation at the beginning of the strings match."""
        str1 = self.filterxml(self.filterwordswithpunctuation(self.filteraccelerators(self.filtervariables(str1))))
        str1 = self.config.lang.punctranslate(str1)
        str2 = self.filterxml(self.filterwordswithpunctuation(self.filteraccelerators(self.filtervariables(str2))))

        if helpers.funcmatch(str1, str2, decoration.puncstart, self.config.punctuation):
            return True
        else:
            raise FilterFailure(u"Different punctuation at the start")


    @cosmetic
    def endpunc(self, str1, str2):
        """Checks whether punctuation at the end of the strings match."""
        str1 = self.filtervariables(str1)
        str1 = self.config.lang.punctranslate(str1)
        str2 = self.filtervariables(str2)
        str1 = str1.rstrip()
        str2 = str2.rstrip()

        if helpers.funcmatch(str1, str2, decoration.puncend, self.config.endpunctuation + u":"):
            return True
        else:
            raise FilterFailure(u"Different punctuation at the end")


    @functional
    def purepunc(self, str1, str2):
        """Checks that strings that are purely punctuation are not changed."""
        # this test is a subset of startandend
        if (decoration.ispurepunctuation(str1)):
            success = str1 == str2
        else:
            success = not decoration.ispurepunctuation(str2)

        if success:
            return True
        else:
            raise FilterFailure(u"Consider not translating punctuation")


    @cosmetic
    def brackets(self, str1, str2):
        """Checks that the number of brackets in both strings match."""
        str1 = self.filtervariables(str1)
        str2 = self.filtervariables(str2)

        messages = []
        missing = []
        extra = []

        for bracket in (u"[", u"]", u"{", u"}", u"(", u")"):
            count1 = str1.count(bracket)
            count2 = str2.count(bracket)

            if count2 < count1:
                missing.append(u"'%s'" % bracket)
            elif count2 > count1:
                extra.append(u"'%s'" % bracket)

        if missing:
            messages.append(u"Missing %s" % u", ".join(missing))

        if extra:
            messages.append(u"Added %s" % u", ".join(extra))

        if messages:
            raise FilterFailure(messages)

        return True


    @functional
    def sentencecount(self, str1, str2):
        """Checks that the number of sentences in both strings match."""
        str1 = self.filteraccelerators(str1)
        str2 = self.filteraccelerators(str2)

        sentences1 = len(self.config.sourcelang.sentences(str1))
        sentences2 = len(self.config.lang.sentences(str2))

        if not sentences1 == sentences2:
            raise FilterFailure(u"Different number of sentences: "
                                u"%d  %d" % (sentences1, sentences2))

        return True


    @functional
    def options(self, str1, str2):
        """Checks that options are not translated."""
        str1 = self.filtervariables(str1)

        for word1 in str1.split():
            if word1 != u"--" and word1.startswith(u"--") and word1[-1].isalnum():
                parts = word1.split(u"=")

                if not parts[0] in str2:
                    raise FilterFailure(u"Missing or translated option '%s'" % parts[0])

                if len(parts) > 1 and parts[1] in str2:
                    raise FilterFailure(u"Consider translating parameter "
                                        u"'%(param)s' of option '%(option)s'"
                                                                % {"param": parts[1],
                                                                "option": parts[0]})

        return True


    @cosmetic
    def startcaps(self, str1, str2):
        """Checks that the message starts with the correct capitalisation."""
        str1 = self.filteraccelerators(str1)
        str2 = self.filteraccelerators(str2)

        if len(str1) > 1 and len(str2) > 1:
            if self.config.sourcelang.capsstart(str1) == self.config.lang.capsstart(str2):
                return True
            elif self.config.sourcelang.numstart(str1) or self.config.lang.numstart(str2):
                return True
            else:
                raise FilterFailure(u"Different capitalization at the start")

        if len(str1) == 0 and len(str2) == 0:
            return True

        if len(str1) == 0 or len(str2) == 0:
            raise FilterFailure(u"Different capitalization at the start")

        return True


    @cosmetic
    def simplecaps(self, str1, str2):
        """Checks the capitalisation of two strings isn't wildly different."""
        str1 = self.removevariables(str1)
        str2 = self.removevariables(str2)
        # TODO: review this. The 'I' is specific to English, so it probably
        # serves no purpose to get sourcelang.sentenceend
        str1 = re.sub(u"[^%s]( I )" % self.config.sourcelang.sentenceend, u" i ", str1)

        capitals1 = helpers.filtercount(str1, unicode.isupper)
        capitals2 = helpers.filtercount(str2, unicode.isupper)

        alpha1 = helpers.filtercount(str1, unicode.isalpha)
        alpha2 = helpers.filtercount(str2, unicode.isalpha)

        # Capture the all caps case
        if capitals1 == alpha1:
            if capitals2 == alpha2:
                return True
            else:
                raise FilterFailure(u"Different capitalization")

        # some heuristic tests to try and see that the style of capitals is
        # vaguely the same
        if capitals1 == 0 or capitals1 == 1:
            success = capitals2 == capitals1
        elif capitals1 < len(str1) / 10:
            success = capitals2 <= len(str2) / 8
        elif len(str1) < 10:
            success = abs(capitals1 - capitals2) < 3
        elif capitals1 > len(str1) * 6 / 10:
            success = capitals2 > len(str2) * 6 / 10
        else:
            success = abs(capitals1 - capitals2) < (len(str1) + len(str2)) / 6

        if success:
            return True
        else:
            raise FilterFailure(u"Different capitalization")


    @functional
    def acronyms(self, str1, str2):
        """Checks that acronyms that appear are unchanged."""
        acronyms = []
        allowed = []

        for startmatch, endmatch in self.config.varmatches:
            allowed += decoration.getvariables(startmatch, endmatch)(str1)

        allowed += self.config.musttranslatewords.keys()
        str1 = self.filteraccelerators(self.filtervariables(str1))
        iter = self.config.lang.word_iter(str1)
        str2 = self.filteraccelerators(self.filtervariables(str2))

        #TODO: strip XML? - should provide better error messsages
        # see mail/chrome/messanger/smime.properties.po
        #TODO: consider limiting the word length for recognising acronyms to
        #something like 5/6 characters
        for word in iter:
            if word.isupper() and len(word) > 1 and word not in allowed:
                if str2.find(word) == -1:
                    acronyms.append(word)

        if acronyms:
            raise FilterFailure(u"Consider not translating acronyms: %s" %
                                u", ".join(acronyms))

        return True


    @cosmetic
    def doublewords(self, str1, str2):
        """Checks for repeated words in the translation."""
        lastword = ""
        without_newlines = "\n".join(str2.split("\n"))
        words = self.filteraccelerators(self.removevariables(self.filterxml(without_newlines))).replace(u".", u"").lower().split()

        for word in words:
            if word == lastword and word not in self.config.lang.validdoublewords:
                raise FilterFailure(u"The word '%s' is repeated" % word)
            lastword = word

        return True


    @functional
    def notranslatewords(self, str1, str2):
        """Checks that words configured as untranslatable appear in the
        translation too."""
        if not self.config.notranslatewords:
            return True

        str1 = self.filtervariables(str1)
        str2 = self.filtervariables(str2)

        #The above is full of strange quotes and things in utf-8 encoding.
        #single apostrophe perhaps problematic in words like "doesn't"
        for seperator in self.config.punctuation:
            str1 = str1.replace(seperator, u" ")
            str2 = str2.replace(seperator, u" ")

        words1 = self.filteraccelerators(str1).split()
        words2 = self.filteraccelerators(str2).split()
        stopwords = [word for word in words1 if word in self.config.notranslatewords and word not in words2]

        if stopwords:
            raise FilterFailure(u"Do not translate: %s" %
                                (u", ".join(stopwords)))

        return True


    @functional
    def musttranslatewords(self, str1, str2):
        """Checks that words configured as definitely translatable don't appear
        in the translation."""
        if not self.config.musttranslatewords:
            return True

        str1 = self.removevariables(str1)
        str2 = self.removevariables(str2)

        # The above is full of strange quotes and things in utf-8 encoding.
        # single apostrophe perhaps problematic in words like "doesn't"
        for seperator in self.config.punctuation:
            str1 = str1.replace(seperator, u" ")
            str2 = str2.replace(seperator, u" ")

        words1 = self.filteraccelerators(str1).split()
        words2 = self.filteraccelerators(str2).split()
        stopwords = [word for word in words1 if word.lower() in self.config.musttranslatewords and word in words2]

        if stopwords:
            raise FilterFailure(u"Please translate: %s" % (u", ".join(stopwords)))

        return True


    @cosmetic
    def validchars(self, str1, str2):
        """Checks that only characters specified as valid appear in the
        translation.
        """
        if not self.config.validcharsmap:
            return True

        invalid1 = str1.translate(self.config.validcharsmap)
        invalid2 = str2.translate(self.config.validcharsmap)
        invalidchars = [u"'%s' (\\u%04x)" % (invalidchar, ord(invalidchar)) for invalidchar in invalid2 if invalidchar not in invalid1]

        if invalidchars:
            raise FilterFailure(u"Invalid characters: %s" % (u", ".join(invalidchars)))

        return True


    @functional
    def filepaths(self, str1, str2):
        """Checks that file paths have not been translated."""
        for word1 in self.filteraccelerators(self.filterxml(str1)).split():
            if word1.startswith(u"/"):
                if not helpers.countsmatch(str1, str2, (word1,)):
                    raise FilterFailure(u"Different file paths")

        return True


    @critical
    def xmltags(self, str1, str2):
        """Checks that XML/HTML tags have not been translated."""
        tags1 = tag_re.findall(str1)

        if len(tags1) > 0:
            if (len(tags1[0]) == len(str1)) and not u"=" in tags1[0]:
                return True

            tags2 = tag_re.findall(str2)
            properties1 = tagproperties(tags1, self.config.ignoretags)
            properties2 = tagproperties(tags2, self.config.ignoretags)

            filtered1 = []
            filtered2 = []

            for property1 in properties1:
                filtered1 += [intuplelist(property1, self.config.canchangetags)]

            for property2 in properties2:
                filtered2 += [intuplelist(property2, self.config.canchangetags)]

            # TODO: consider the consequences of different ordering of
            # attributes/tags
            if filtered1 != filtered2:
                raise FilterFailure(u"Different XML tags")
        else:
            # No tags in str1, let's just check that none were added in str2.
            # This might be useful for fuzzy strings wrongly unfuzzied.
            tags2 = tag_re.findall(str2)

            if len(tags2) > 0:
                raise FilterFailure(u"Added XML tags")

        return True


    @functional
    def kdecomments(self, str1, str2):
        """Checks to ensure that no KDE style comments appear in the
        translation.
        """
        return str2.find(u"\n_:") == -1 and not str2.startswith(u"_:")


    @extraction
    def compendiumconflicts(self, str1, str2):
        """Checks for Gettext compendium conflicts (#-#-#-#-#)."""
        return str2.find(u"#-#-#-#-#") == -1


    @cosmetic
    def simpleplurals(self, str1, str2):
        """Checks for English style plural(s) for you to review."""

        def numberofpatterns(string, patterns):
            number = 0

            for pattern in patterns:
                number += len(re.findall(pattern, string))

            return number

        sourcepatterns = ["\(s\)"]
        targetpatterns = ["\(s\)"]
        sourcecount = numberofpatterns(str1, sourcepatterns)
        targetcount = numberofpatterns(str2, targetpatterns)

        if self.config.lang.nplurals == 1:
            if targetcount:
                raise FilterFailure(u"Plural(s) were kept in translation")
            else:
                return True

        if sourcecount == targetcount:
            return True
        else:
            raise FilterFailure(u"The original uses plural(s)")


    @functional
    def spellcheck(self, str1, str2):
        """Checks words that don't pass a spell check."""
        if not self.config.targetlanguage:
            return True

        if not spelling.available:
            return True

        # TODO: filterxml?
        str1 = self.filteraccelerators_by_list(self.removevariables(str1),
                                               self.config.sourcelang.validaccel)
        str2 = self.filteraccelerators_by_list(self.removevariables(str2),
                                               self.config.lang.validaccel)
        errors = set()

        # We cache spelling results of source texts:
        ignore1 = self.source_spell_cache.get(str1, None)
        if ignore1 is None:
            ignore1 = set(spelling.simple_check(str1, lang=self.config.sourcelang.code))
            self.source_spell_cache[str1] = ignore1

        # We cache spelling results of target texts sentence-by-sentence. This
        # way we can reuse most of the results while someone is typing a long
        # segment in Virtaal.
        sentences2 = self.config.lang.sentences(str2)
        for sentence in sentences2:
            sentence_errors = self.target_spell_cache.get(sentence, None)
            if sentence_errors is None:
                sentence_errors = spelling.simple_check(sentence, lang=self.config.targetlanguage)
                self.target_spell_cache[sentence] = sentence_errors
            errors.update(sentence_errors)

        errors.difference_update(ignore1, self.config.notranslatewords)

        if errors:
            messages = [u"Check the spelling of: %s" % u", ".join(errors)]
            raise FilterFailure(messages)

        return True


    @extraction
    def credits(self, str1, str2):
        """Checks for messages containing translation credits instead of
        normal translations.
        """
        if str1 in self.config.credit_sources:
            raise FilterFailure(u"Don't translate. Just credit the translators.")
        else:
            return True


    # If the precondition filter is run and fails then the other tests listed are ignored
    preconditions = {
        "untranslated": ("simplecaps", "variables", "startcaps",
                         "accelerators", "brackets", "endpunc",
                         "acronyms", "xmltags", "startpunc",
                         "endwhitespace", "startwhitespace",
                         "escapes", "doublequoting", "singlequoting",
                         "filepaths", "purepunc", "doublespacing",
                         "sentencecount", "numbers", "isfuzzy",
                         "isreview", "notranslatewords", "musttranslatewords",
                         "emails", "simpleplurals", "urls", "printf",
                         "tabs", "newlines", "functions", "options",
                         "blank", "nplurals", "gconf", "dialogsizes",
                         "validxml"),
          "blank": ("simplecaps", "variables", "startcaps",
                    "accelerators", "brackets", "endpunc",
                    "acronyms", "xmltags", "startpunc",
                    "endwhitespace", "startwhitespace",
                    "escapes", "doublequoting", "singlequoting",
                    "filepaths", "purepunc", "doublespacing",
                    "sentencecount", "numbers", "isfuzzy",
                    "isreview", "notranslatewords", "musttranslatewords",
                    "emails", "simpleplurals", "urls", "printf",
                    "tabs", "newlines", "functions", "options",
                    "gconf", "dialogsizes", "validxml"),
          "credits": ("simplecaps", "variables", "startcaps",
                      "accelerators", "brackets", "endpunc",
                      "acronyms", "xmltags", "startpunc",
                      "escapes", "doublequoting", "singlequoting",
                      "filepaths", "doublespacing",
                      "sentencecount", "numbers",
                      "emails", "simpleplurals", "urls", "printf",
                      "tabs", "newlines", "functions", "options",
                      "validxml"),
         "purepunc": ("startcaps", "options"),
         # This is causing some problems since Python 2.6, as
         # startcaps is now seen as an important one to always execute
         # and could now be done before it is blocked by a failing
         # "untranslated" or "blank" test. This is probably happening
         # due to slightly different implementation of the internal
         # dict handling since Python 2.6. We should never have relied
         # on this ordering anyway.
         #"startcaps": ("simplecaps",),
         "endwhitespace": ("endpunc",),
         "startwhitespace": ("startpunc",),
         "unchanged": ("doublewords",),
         "compendiumconflicts": ("accelerators", "brackets", "escapes",
                          "numbers", "startpunc", "long", "variables",
                          "startcaps", "sentencecount", "simplecaps",
                          "doublespacing", "endpunc", "xmltags",
                          "startwhitespace", "endwhitespace",
                          "singlequoting", "doublequoting",
                          "filepaths", "purepunc", "doublewords", "printf",
                          "newlines", "validxml"),
         }

# code to actually run the tests (use unittest?)

openofficeconfig = CheckerConfig(
    accelmarkers=["~"],
    varmatches=[("&", ";"), ("%", "%"), ("%", None), ("%", 0), ("$(", ")"),
                ("$", "$"), ("${", "}"), ("#", "#"), ("#", 1), ("#", 0),
                ("($", ")"), ("$[", "]"), ("[", "]"), ("@", "@"),
                ("$", None)],
    ignoretags=[("alt", "xml-lang", None), ("ahelp", "visibility", "visible"),
                ("img", "width", None), ("img", "height", None)],
    canchangetags=[("link", "name", None)],
)


class OpenOfficeChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(openofficeconfig)
        StandardChecker.__init__(self, **kwargs)

libreofficeconfig = CheckerConfig(
    accelmarkers=["~"],
    varmatches=[("&", ";"), ("%", "%"), ("%", None), ("%", 0), ("$(", ")"),
                ("$", "$"), ("${", "}"), ("#", "#"), ("#", 1), ("#", 0),
                ("($", ")"), ("$[", "]"), ("[", "]"), ("@", "@"),
                ("$", None)],
    ignoretags=[("alt", "xml-lang", None), ("ahelp", "visibility", "visible"),
                ("img", "width", None), ("img", "height", None)],
    canchangetags=[("link", "name", None)],
)


class LibreOfficeChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(libreofficeconfig)
        checkerconfig.update(openofficeconfig)
        StandardChecker.__init__(self, **kwargs)


    @critical
    def validxml(self, str1, str2):
        """Check that all XML/HTML open/close tags has close/open
        pair in the translation."""
        for location in self.locations:
            if location.endswith(".xrm") or location.endswith(".xhp"):
                opentags = []
                match = re.search(lo_tag_re, str2)
                while match:
                    acttag = match.group(0)
                    if acttag.startswith("</"):
                        if len(opentags) == 0:
                            raise FilterFailure(u"There is no open tag for %s" % (acttag))
                        opentag = opentags.pop()
                        if tagname(acttag) != "/" + tagname(opentag):
                            raise FilterFailure(u"Open tag %s and close tag %s "
                                                 "don't match" % (opentag, acttag))
                    else:
                        opentags.append(acttag)
                    str2 = str2[match.end(0):]
                    match = re.search(lo_tag_re, str2)
                if len(opentags) != 0:
                    raise FilterFailure(u"There is no close tag for %s" % (opentags.pop()))
        return True


mozillaconfig = CheckerConfig(
    accelmarkers=["&"],
    varmatches=[("&", ";"), ("%", "%"), ("%", 1), ("$", "$"), ("$", None),
                ("#", 1), ("${", "}"), ("$(^", ")"), ("{{", "}}"), ],
    criticaltests=["accelerators"],
)


class MozillaChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(mozillaconfig)
        StandardChecker.__init__(self, **kwargs)


    @extraction
    def credits(self, str1, str2):
        """Checks for messages containing translation credits instead of
        normal translations.
        """
        for location in self.locations:
            if location in ['MOZ_LANGPACK_CONTRIBUTORS', 'credit.translation']:
                raise FilterFailure(u"Don't translate. Just credit the translators.")

        return True


    mozilla_dialog_re = re.compile("""(                          # option pair "key: value;"
                                      (?P<key>[-a-z]+)           # key
                                      :\s+                       # seperator
                                      (?P<number>\d+(?:[.]\d+)?) # number
                                      (?P<unit>[a-z][a-z]);?     # units
                                      )+                         # multiple pairs
                                   """, re.VERBOSE)
    mozilla_dialog_valid_units = ['em', 'px', 'ch']


    @critical
    def dialogsizes(self, str1, str2):
        """Checks that dialog sizes are not translated."""
        # Example: "width: 635px; height: 400px;"
        if "width" in str1 or "height" in str1:
            str1pairs = self.mozilla_dialog_re.findall(str1)

            if str1pairs:
                str2pairs = self.mozilla_dialog_re.findall(str2)

                if len(str1pairs) != len(str2pairs):
                    raise FilterFailure(u"A dialog pair is missing")

                for i, pair1 in enumerate(str1pairs):
                    pair2 = str2pairs[i]

                    if pair1[0] != pair2[0]:  # Only check pairs that differ
                        if len(pair2) != 4:
                            raise FilterFailure(u"A part of the dialog pair is missing")

                        if pair1[1] not in pair2:  # key
                            raise FilterFailure(u"Do not translate the key '%s'" % pair1[1])

                        # FIXME we could check more carefully for numbers in pair1[2]
                        if pair2[3] not in self.mozilla_dialog_valid_units:
                            raise FilterFailure(u"Units should be one of '%s'. "
                                                 "The source string uses '%s'" % (", ".join(self.mozilla_dialog_valid_units), pair1[3]))

        return True


    @functional
    def numbers(self, str1, str2):
        """Checks that numbers are not translated.

        Special handling for Mozilla to ignore entries that are dialog sizes.
        """
        if self.mozilla_dialog_re.findall(str1):
            return True

        return super(MozillaChecker, self).numbers(str1, str2)


    @functional
    def unchanged(self, str1, str2):
        """Checks whether a translation is basically identical to the original
        string.

        Special handling for Mozilla to ignore entries that are dialog sizes.
        """
        if (self.mozilla_dialog_re.findall(str1) or
            str1.strip().lstrip('0123456789') in self.mozilla_dialog_valid_units):
            return True

        return super(MozillaChecker, self).unchanged(str1, str2)

    @cosmetic
    def accelerators(self, str1, str2):
        """Checks whether accelerators are consistent between the
        two strings.

        For Mozilla we lower the severity to cosmetic.
        """
        return super(MozillaChecker, self).accelerators(str1, str2)

drupalconfig = CheckerConfig(
    varmatches=[("%", None), ("@", None), ("!", None)],
)


class DrupalChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(drupalconfig)
        StandardChecker.__init__(self, **kwargs)


gnomeconfig = CheckerConfig(
    accelmarkers=["_"],
    varmatches=[("%", 1), ("$(", ")")],
    credit_sources=[u"translator-credits"],
)


class GnomeChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(gnomeconfig)
        StandardChecker.__init__(self, **kwargs)


    @functional
    def gconf(self, str1, str2):
        """Checks if we have any gconf config settings translated."""
        for location in self.locations:
            if location.find('schemas.in') != -1 or location.find('gschema.xml.in') != -1:
                gconf_attributes = gconf_attribute_re.findall(str1)
                #stopwords = [word for word in words1 if word in self.config.notranslatewords and word not in words2]
                stopwords = [word for word in gconf_attributes if word[1:-1] not in str2]

                if stopwords:
                    raise FilterFailure(u"Do not translate GConf attributes: %s" %
                                        (u", ".join(stopwords)))

                return True

        return True


kdeconfig = CheckerConfig(
    accelmarkers=["&"],
    varmatches=[("%", 1)],
    credit_sources=[u"Your names", u"Your emails", u"ROLES_OF_TRANSLATORS"],
)


class KdeChecker(StandardChecker):

    def __init__(self, **kwargs):
        # TODO allow setup of KDE plural and translator comments so that they do
        # not create false postives
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(kdeconfig)
        StandardChecker.__init__(self, **kwargs)


cclicenseconfig = CheckerConfig(varmatches=[("@", "@")])


class CCLicenseChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(cclicenseconfig)
        StandardChecker.__init__(self, **kwargs)


termconfig = CheckerConfig()


class TermChecker(StandardChecker):

    def __init__(self, **kwargs):
        checkerconfig = kwargs.get("checkerconfig", None)

        if checkerconfig is None:
            checkerconfig = CheckerConfig()
            kwargs["checkerconfig"] = checkerconfig

        checkerconfig.update(termconfig)
        StandardChecker.__init__(self, **kwargs)


projectcheckers = {
    "openoffice": OpenOfficeChecker,
    "libreoffice": LibreOfficeChecker,
    "mozilla": MozillaChecker,
    "kde": KdeChecker,
    "wx": KdeChecker,
    "gnome": GnomeChecker,
    "creativecommons": CCLicenseChecker,
    "drupal": DrupalChecker,
    "terminology": TermChecker,
}


class StandardUnitChecker(UnitChecker):
    """The standard checks for common checks on translation units."""


    @extraction
    def isfuzzy(self, unit):
        """Check if the unit has been marked fuzzy."""
        return not unit.isfuzzy()


    @extraction
    def isreview(self, unit):
        """Check if the unit has been marked review."""
        return not unit.isreview()


    @critical
    def nplurals(self, unit):
        """Checks for the correct number of noun forms for plural
        translations.
        """
        if unit.hasplural():
            # if we don't have a valid nplurals value, don't run the test
            nplurals = self.config.lang.nplurals

            if nplurals > 0:
                return len(filter(None, unit.target.strings)) == nplurals

        return True


    @extraction
    def hassuggestion(self, unit):
        """Checks if there is at least one suggested translation for this
        unit.
        """
        self.suggestion_store = getattr(self, 'suggestion_store', None)
        suggestions = []

        if self.suggestion_store:
            suggestions = self.suggestion_store.findunits(unit.source)
        elif getattr(unit, "getalttrans", None):
            # TODO: we probably want to filter them somehow
            suggestions = unit.getalttrans()

        return not bool(suggestions)


def runtests(str1, str2, ignorelist=()):
    """Verifies that the tests pass for a pair of strings."""
    from translate.storage import base
    str1 = data.normalized_unicode(str1)
    str2 = data.normalized_unicode(str2)
    unit = base.TranslationUnit(str1)
    unit.target = str2
    checker = StandardChecker(excludefilters=ignorelist)
    failures = checker.run_filters(unit)

    for test in failures:
        print("failure: %s: %s\n  %r\n  %r" % \
              (test, failures[test]['message'], str1, str2))

    return failures


def batchruntests(pairs):
    """Runs test on a batch of string pairs."""
    passed, numpairs = 0, len(pairs)

    for str1, str2 in pairs:
        if runtests(str1, str2):
            passed += 1

    print("\ntotal: %d/%d pairs passed" % (passed, numpairs))


if __name__ == '__main__':
    testset = [(r"simple", r"somple"),
            (r"\this equals \that", r"does \this equal \that?"),
            (r"this \'equals\' that", r"this 'equals' that"),
            (r" start and end! they must match.",
             r"start and end! they must match."),
            (r"check for matching %variables marked like %this",
             r"%this %variable is marked"),
            (r"check for mismatching %variables marked like %this",
             r"%that %variable is marked"),
            (r"check for mismatching %variables% too",
             r"how many %variable% are marked"),
            (r"%% %%", r"%%"),
            (r"Row: %1, Column: %2", r"Mothalo: %1, Kholomo: %2"),
            (r"simple lowercase", r"it is all lowercase"),
            (r"simple lowercase", r"It Is All Lowercase"),
            (r"Simple First Letter Capitals", r"First Letters"),
            (r"SIMPLE CAPITALS", r"First Letters"),
            (r"SIMPLE CAPITALS", r"ALL CAPITALS"),
            (r"forgot to translate", r"  "),
            ]
    batchruntests(testset)

########NEW FILE########
__FILENAME__ = decoration
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""functions to get decorative/informative text out of strings..."""

import re
import unicodedata

from translate.lang import data


def spacestart(str1):
    """returns all the whitespace from the start of the string"""
    newstring = u""
    for c in str1:
        if c.isspace():
            newstring += c
        else:
            break
    return newstring


def spaceend(str1):
    """returns all the whitespace from the end of the string"""
    newstring = u""
    for n in range(len(str1)):
        c = str1[-1-n]
        if c.isspace():
            newstring = c + newstring
        else:
            break
    return newstring


def puncstart(str1, punctuation):
    """returns all the punctuation from the start of the string"""
    newstring = u""
    for c in str1:
        if c in punctuation or c.isspace():
            newstring += c
        else:
            break
    return newstring


def puncend(str1, punctuation):
    """returns all the punctuation from the end of the string"""
    # An implementation with regular expressions was slightly slower.

    newstring = u""
    for n in range(len(str1)):
        c = str1[-1-n]
        if c in punctuation or c.isspace():
            newstring = c + newstring
        else:
            break
    return newstring.replace(u"\u00a0", u" ")


def ispurepunctuation(str1):
    """checks whether the string is entirely punctuation"""
    for c in str1:
        if c.isalnum():
            return False
    return len(str1)


def isvalidaccelerator(accelerator, acceptlist=None):
    """returns whether the given accelerator character is valid

    :type accelerator: character
    :param accelerator: A character to be checked for accelerator validity
    :type acceptlist: String
    :param acceptlist: A list of characters that are permissible as
                       accelerators
    :rtype: Boolean
    :return: True if the supplied character is an acceptable accelerator
    """
    assert isinstance(accelerator, unicode)
    assert isinstance(acceptlist, unicode) or acceptlist is None
    if len(accelerator) == 0:
        return False
    if acceptlist is not None:
        acceptlist = data.normalize(acceptlist)
        if accelerator in acceptlist:
            return True
        return False
    else:
        # Old code path - ensures that we don't get a large number of
        # regressions
        accelerator = accelerator.replace("_", "")
        if accelerator in u"-?":
            return True
        if not accelerator.isalnum():
            return False

        # We don't want to have accelerators on characters with diacritics,
        # so let's see if the character can decompose.
        decomposition = unicodedata.decomposition(accelerator)
        # Next we strip out any extra information like <this>
        decomposition = re.sub("<[^>]+>", "", decomposition).strip()
        return decomposition.count(" ") == 0


def findaccelerators(str1, accelmarker, acceptlist=None):
    """returns all the accelerators and locations in str1 marked with a
    given marker"""
    accelerators = []
    badaccelerators = []
    currentpos = 0
    while currentpos >= 0:
        currentpos = str1.find(accelmarker, currentpos)
        if currentpos >= 0:
            accelstart = currentpos
            currentpos += len(accelmarker)
            # we assume accelerators are single characters
            accelend = currentpos + 1
            if accelend > len(str1):
                break
            accelerator = str1[currentpos:accelend]
            currentpos = accelend
            if isvalidaccelerator(accelerator, acceptlist):
                accelerators.append((accelstart, accelerator))
            else:
                badaccelerators.append((accelstart, accelerator))
    return accelerators, badaccelerators


def findmarkedvariables(str1, startmarker, endmarker, ignorelist=[]):
    """returns all the variables and locations in str1 marked with a given
    marker"""
    variables = []
    currentpos = 0
    while currentpos >= 0:
        variable = None
        currentpos = str1.find(startmarker, currentpos)
        if currentpos >= 0:
            startmatch = currentpos
            currentpos += len(startmarker)
            if endmarker is None:
                # handle case without an end marker - use any non-alphanumeric
                # character as the end marker, var must be len > 1
                endmatch = currentpos
                for n in range(currentpos, len(str1)):
                    if not (str1[n].isalnum() or str1[n] == '_'):
                        endmatch = n
                        break
                if currentpos == endmatch:
                    endmatch = len(str1)
                if currentpos < endmatch:
                    variable = str1[currentpos:endmatch]
                currentpos = endmatch
            elif type(endmarker) == int:
                # setting endmarker to an int means it is a fixed-length
                # variable string (usually endmarker==1)
                endmatch = currentpos + endmarker
                if endmatch > len(str1):
                    break
                variable = str1[currentpos:endmatch]
                currentpos = endmatch
            else:
                endmatch = str1.find(endmarker, currentpos)
                if endmatch == -1:
                    break
                # search backwards in case there's an intervening startmarker
                # (if not it's OK)...
                start2 = str1.rfind(startmarker, currentpos, endmatch)
                if start2 != -1:
                    startmatch2 = start2
                    start2 += len(startmarker)
                    if start2 != currentpos:
                        currentpos = start2
                        startmatch = startmatch2
                variable = str1[currentpos:endmatch]
                currentpos = endmatch + len(endmarker)
            if variable is not None and variable not in ignorelist:
                if (not variable or
                    variable.replace("_", "").replace(".", "").isalnum()):
                    variables.append((startmatch, variable))
    return variables


def getaccelerators(accelmarker, acceptlist=None):
    """returns a function that gets a list of accelerators marked using
    accelmarker"""

    def getmarkedaccelerators(str1):
        """returns all the accelerators in str1 marked with a given marker"""
        acclocs, badlocs = findaccelerators(str1, accelmarker, acceptlist)
        accelerators = [accelerator for accelstart, accelerator in acclocs]
        badaccelerators = [accelerator for accelstart, accelerator in badlocs]
        return accelerators, badaccelerators
    return getmarkedaccelerators


def getvariables(startmarker, endmarker):
    """returns a function that gets a list of variables marked using
    startmarker and endmarker"""

    def getmarkedvariables(str1):
        """returns all the variables in str1 marked with a given marker"""
        varlocs = findmarkedvariables(str1, startmarker, endmarker)
        variables = [variable for accelstart, variable in varlocs]
        return variables
    return getmarkedvariables


def getnumbers(str1):
    """returns any numbers that are in the string"""
    # TODO: handle locale-based periods e.g. 2,5 for Afrikaans
    assert isinstance(str1, unicode)
    numbers = []
    innumber = False
    degreesign = u'\xb0'
    lastnumber = ""
    carryperiod = ""
    for chr1 in str1:
        if chr1.isdigit():
            innumber = True
        elif innumber:
            if not (chr1 == '.' or chr1 == degreesign):
                innumber = False
                if lastnumber:
                    numbers.append(lastnumber)
                lastnumber = ""
        if innumber:
            if chr1 == degreesign:
                lastnumber += chr1
            elif chr1 == '.':
                carryperiod += chr1
            else:
                lastnumber += carryperiod + chr1
                carryperiod = ""
        else:
            carryperiod = ""
    if innumber:
        if lastnumber:
            numbers.append(lastnumber)
    return numbers


_function_re = re.compile(r'''((?:
    [\w\.]+              # function or module name - any alpha-numeric character, _, or .
    (?:(?:::|->|\.)\w+)* # (optional) C++ style Class::Method() syntax or pointer->Method() or module.function()
    \(\)                 # Must close with ()
)+)
''', re.VERBOSE)  # shouldn't be locale aware
# Reference functions:
#   pam_*_item() IO::String NULL() POE::Component::Client::LDAP->new()
#   POE::Wheel::Null mechanize.UserAgent POSIX::sigaction()
#   window.resizeBy() @fptr()


def getfunctions(str1):
    """returns the functions() that are in a string, while ignoring the
    trailing punctuation in the given parameter"""
    if u"()" in str1:
        return _function_re.findall(str1)
    else:
        return []


def getemails(str1):
    """returns the email addresses that are in a string"""
    return re.findall('[\w\.\-]+@[\w\.\-]+', str1)


def geturls(str1):
    """returns the URIs in a string"""
    # TODO turn this into a verbose and compiled regex
    URLPAT = 'https?:[\w/\.:;+\-~\%#\$?=&,()]+|' + \
             'www\.[\w/\.:;+\-~\%#\$?=&,()]+|' + \
             'ftp:[\w/\.:;+\-~\%#?=&,]+'
    return re.findall(URLPAT, str1)


def countaccelerators(accelmarker, acceptlist=None):
    """returns a function that counts the number of accelerators marked
    with the given marker"""

    def countmarkedaccelerators(str1):
        """returns all the variables in str1 marked with a given marker"""
        acclocs, badlocs = findaccelerators(str1, accelmarker, acceptlist)
        return len(acclocs), len(badlocs)
    return countmarkedaccelerators

########NEW FILE########
__FILENAME__ = decorators
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Decorators to categorize pofilter checks."""

from functools import wraps


#: Quality checks' failure categories
class Category(object):
    CRITICAL = 100
    FUNCTIONAL = 60
    COSMETIC = 30
    EXTRACTION = 10
    NO_CATEGORY = 0


def critical(f):

    @wraps(f)
    def critical_f(self, *args, **kwargs):
        if f.__name__ not in self.__class__.categories:
            self.__class__.categories[f.__name__] = Category.CRITICAL

        return f(self, *args, **kwargs)

    return critical_f


def functional(f):

    @wraps(f)
    def functional_f(self, *args, **kwargs):
        if f.__name__ not in self.__class__.categories:
            self.__class__.categories[f.__name__] = Category.FUNCTIONAL

        return f(self, *args, **kwargs)

    return functional_f


def cosmetic(f):

    @wraps(f)
    def cosmetic_f(self, *args, **kwargs):
        if f.__name__ not in self.__class__.categories:
            self.__class__.categories[f.__name__] = Category.COSMETIC

        return f(self, *args, **kwargs)

    return cosmetic_f


def extraction(f):

    @wraps(f)
    def extraction_f(self, *args, **kwargs):
        if f.__name__ not in self.__class__.categories:
            self.__class__.categories[f.__name__] = Category.EXTRACTION

        return f(self, *args, **kwargs)

    return extraction_f

########NEW FILE########
__FILENAME__ = helpers
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""a set of helper functions for filters..."""

import operator


def countmatch(str1, str2, countstr):
    """checks whether countstr occurs the same number of times in str1 and str2"""
    return str1.count(countstr) == str2.count(countstr)


def funcmatch(str1, str2, func, *args):
    """returns whether the result of func is the same for str1 and str2"""
    return func(str1, *args) == func(str2, *args)


def countsmatch(str1, str2, countlist):
    """checks whether each element in countlist occurs the same number of times in str1 and str2"""
    return reduce(operator.and_, [countmatch(str1, str2, countstr) for countstr in countlist], True)


def funcsmatch(str1, str2, funclist):
    """checks whether the results of each func in funclist match for str1 and str2"""
    return reduce(operator.and_, [funcmatch(str1, str2, funcstr) for funcstr in funclist], True)


def filtercount(str1, func):
    """returns the number of characters in str1 that pass func"""
    return len(filter(func, str1))


def filtertestmethod(testmethod, strfilter):
    """returns a version of the testmethod that operates on filtered strings using strfilter"""

    def filteredmethod(str1, str2):
        return testmethod(strfilter(str1), strfilter(str2))
    filteredmethod.__doc__ = testmethod.__doc__
    filteredmethod.name = getattr(testmethod, 'name', testmethod.__name__)
    return filteredmethod


def multifilter(str1, strfilters, *args):
    """passes str1 through a list of filters"""
    for strfilter in strfilters:
        str1 = strfilter(str1, *args)
    return str1


def multifiltertestmethod(testmethod, strfilters):
    """returns a version of the testmethod that operates on filtered strings using strfilter"""

    def filteredmethod(str1, str2):
        return testmethod(multifilter(str1, strfilters), multifilter(str2, strfilters))
    filteredmethod.__doc__ = testmethod.__doc__
    filteredmethod.name = getattr(testmethod, 'name', testmethod.__name__)
    return filteredmethod

########NEW FILE########
__FILENAME__ = pofilter
#!/usr/bin/env python
#
# Copyright 2004-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Perform quality checks on Gettext PO, XLIFF and TMX localization files.

Snippet files are created whenever a test fails.  These can be examined,
corrected and merged back into the originals using pomerge.

See:
http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pofilter.html
for examples and usage instructions and
http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pofilter_tests.html
for full descriptions of all tests.
"""

import os

from translate.filters import autocorrect, checks
from translate.misc import optrecurse
from translate.storage import factory
from translate.storage.poheader import poheader


def build_checkerconfig(options):
    """Prepare the checker config from the given options.  This is mainly
    factored out for the sake of unit tests."""
    checkerconfig = checks.CheckerConfig(targetlanguage=options.targetlanguage)

    if options.notranslatefile:
        options.notranslatefile = os.path.expanduser(options.notranslatefile)

        if not os.path.exists(options.notranslatefile):
            self.error("notranslatefile %r does not exist" % options.notranslatefile)

        notranslatewords = [line.strip() for line in open(options.notranslatefile).readlines()]
        notranslatewords = dict.fromkeys([key for key in notranslatewords])

        checkerconfig.notranslatewords.update(notranslatewords)

    if options.musttranslatefile:
        options.musttranslatefile = os.path.expanduser(options.musttranslatefile)

        if not os.path.exists(options.musttranslatefile):
            self.error("musttranslatefile %r does not exist" % options.musttranslatefile)

        musttranslatewords = [line.strip() for line in open(options.musttranslatefile).readlines()]
        musttranslatewords = dict.fromkeys([key for key in musttranslatewords])

        checkerconfig.musttranslatewords.update(musttranslatewords)

    if options.validcharsfile:
        options.validcharsfile = os.path.expanduser(options.validcharsfile)

        if not os.path.exists(options.validcharsfile):
            self.error("validcharsfile %r does not exist" % options.validcharsfile)

        validchars = open(options.validcharsfile).read()

        checkerconfig.updatevalidchars(validchars)

    return checkerconfig


class pocheckfilter:

    def __init__(self, options, checkerclasses=None, checkerconfig=None):
        # excludefilters={}, limitfilters=None, includefuzzy=True, includereview=True, autocorrect=False):
        """Builds a checkfilter using the given checker (a list is allowed too)"""
        if checkerclasses is None:
            checkerclasses = [checks.StandardChecker, checks.StandardUnitChecker]

        self.checker = checks.TeeChecker(checkerconfig=checkerconfig,
                                         excludefilters=options.excludefilters,
                                         limitfilters=options.limitfilters,
                                         checkerclasses=checkerclasses,
                                         languagecode=checkerconfig.targetlanguage)
        self.options = options


    def getfilterdocs(self):
        """Lists the docs for filters available on checker."""
        filterdict = self.checker.getfilters()
        filterdocs = ["%s\t%s" % (name, filterfunc.__doc__) for (name, filterfunc) in filterdict.iteritems()]
        filterdocs.sort()

        return "\n".join(filterdocs)


    def filterunit(self, unit):
        """Runs filters on an element."""

        if unit.isheader():
            return []

        if not self.options.includefuzzy and unit.isfuzzy():
            return []

        if not self.options.includereview and unit.isreview():
            return []

        failures = self.checker.run_filters(unit, categorised=True)

        if failures and self.options.autocorrect:
            # we can't get away with bad unquoting / requoting if we're going to change the result...
            correction = autocorrect.correct(unit.source, unit.target)

            if correction:
                unit.target = correction
                return autocorrect
            else:
                # ignore failures we can't correct when in autocorrect mode
                return []

        return failures


    def filterfile(self, transfile):
        """Runs filters on a translation store object.

        :param transfile: A translation store object.
        :return: A new translation store object with the results of
                 the filter included.
        """
        newtransfile = type(transfile)()
        newtransfile.setsourcelanguage(transfile.getsourcelanguage())
        newtransfile.settargetlanguage(transfile.gettargetlanguage())

        for unit in transfile.units:
            filter_result = self.filterunit(unit)

            if filter_result:
                if filter_result != autocorrect:
                    for filter_name in filter_result.iterkeys():
                        filter_message = filter_result[filter_name]['message']

                        if self.options.addnotes:
                            unit.adderror(filter_name, filter_message)
                        if isinstance(filter_message, checks.SeriousFilterFailure):
                            unit.markfuzzy()

                newtransfile.addunit(unit)

        if isinstance(newtransfile, poheader):
            newtransfile.updateheader(add=True, **transfile.parseheader())

        return newtransfile


class FilterOptionParser(optrecurse.RecursiveOptionParser):
    """A specialized Option Parser for filter tools..."""

    def __init__(self, formats):
        """Construct the specialized Option Parser."""
        optrecurse.RecursiveOptionParser.__init__(self, formats)

        self.set_usage()
        self.add_option("-l", "--listfilters", action="callback", dest='listfilters',
            default=False, callback_kwargs={'dest_value': True},
            callback=self.parse_noinput, help="list filters available")


    def parse_noinput(self, option, opt, value, parser, *args, **kwargs):
        """This sets an option to *True*, but also sets input to *-*
        to prevent an error."""
        setattr(parser.values, option.dest, kwargs['dest_value'])
        parser.values.input = "-"


    def run(self):
        """Parses the arguments, and runs recursiveprocess with the
        resulting options."""
        (options, args) = self.parse_args()

        if options.filterclass is None:
            checkerclasses = [checks.StandardChecker, checks.StandardUnitChecker]
        else:
            checkerclasses = [options.filterclass, checks.StandardUnitChecker]

        checkerconfig = build_checkerconfig(options)
        options.checkfilter = pocheckfilter(options, checkerclasses, checkerconfig)

        if not options.checkfilter.checker.combinedfilters:
            self.error("No valid filters were specified")

        options.inputformats = self.inputformats
        options.outputoptions = self.outputoptions

        if options.listfilters:
            print(options.checkfilter.getfilterdocs())
        else:
            self.recursiveprocess(options)


def runfilter(inputfile, outputfile, templatefile, checkfilter=None):
    """Reads in inputfile, filters using checkfilter, writes to outputfile."""
    fromfile = factory.getobject(inputfile)
    tofile = checkfilter.filterfile(fromfile)

    if tofile.isempty():
        return 0

    outputfile.write(str(tofile))

    return 1


def cmdlineparser():
    formats = {"po": ("po", runfilter), "pot": ("pot", runfilter),
            "xliff": ("xliff", runfilter), "xlf": ("xlf", runfilter),
            "tmx": ("tmx", runfilter),
            None: ("po", runfilter)}

    parser = FilterOptionParser(formats)

    parser.add_option("", "--review", dest="includereview",
        action="store_true", default=True,
        help="include units marked for review (default)")
    parser.add_option("", "--noreview", dest="includereview",
        action="store_false", default=True,
        help="exclude units marked for review")
    parser.add_option("", "--fuzzy", dest="includefuzzy",
        action="store_true", default=True,
        help="include units marked fuzzy (default)")
    parser.add_option("", "--nofuzzy", dest="includefuzzy",
        action="store_false", default=True,
        help="exclude units marked fuzzy")
    parser.add_option("", "--nonotes", dest="addnotes",
        action="store_false", default=True,
        help="don't add notes about the errors")
    parser.add_option("", "--autocorrect", dest="autocorrect",
        action="store_true", default=False,
        help="output automatic corrections where possible rather than describing issues")
    parser.add_option("", "--language", dest="targetlanguage", default=None,
        help="set target language code (e.g. af-ZA) [required for spell check and recommended in general]", metavar="LANG")
    parser.add_option("", "--openoffice", dest="filterclass",
        action="store_const", default=None, const=checks.OpenOfficeChecker,
        help="use the standard checks for OpenOffice translations")
    parser.add_option("", "--libreoffice", dest="filterclass",
        action="store_const", default=None, const=checks.LibreOfficeChecker,
        help="use the standard checks for LibreOffice translations")
    parser.add_option("", "--mozilla", dest="filterclass",
        action="store_const", default=None, const=checks.MozillaChecker,
        help="use the standard checks for Mozilla translations")
    parser.add_option("", "--drupal", dest="filterclass",
        action="store_const", default=None, const=checks.DrupalChecker,
        help="use the standard checks for Drupal translations")
    parser.add_option("", "--gnome", dest="filterclass",
        action="store_const", default=None, const=checks.GnomeChecker,
        help="use the standard checks for Gnome translations")
    parser.add_option("", "--kde", dest="filterclass",
        action="store_const", default=None, const=checks.KdeChecker,
        help="use the standard checks for KDE translations")
    parser.add_option("", "--wx", dest="filterclass",
        action="store_const", default=None, const=checks.KdeChecker,
        help="use the standard checks for wxWidgets translations")
    parser.add_option("", "--excludefilter", dest="excludefilters",
        action="append", default=[], type="string", metavar="FILTER",
        help="don't use FILTER when filtering")
    parser.add_option("-t", "--test", dest="limitfilters",
        action="append", default=None, type="string", metavar="FILTER",
        help="only use test FILTERs specified with this option when filtering")
    parser.add_option("", "--notranslatefile", dest="notranslatefile",
        default=None, type="string", metavar="FILE",
        help="read list of untranslatable words from FILE (must not be translated)")
    parser.add_option("", "--musttranslatefile", dest="musttranslatefile",
        default=None, type="string", metavar="FILE",
        help="read list of translatable words from FILE (must be translated)")
    parser.add_option("", "--validcharsfile", dest="validcharsfile",
        default=None, type="string", metavar="FILE",
        help="read list of all valid characters from FILE (must be in UTF-8)")

    parser.passthrough.append('checkfilter')
    parser.description = __doc__

    return parser


def main():
    parser = cmdlineparser()
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = prefilters
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Filters that strings can be passed through before certain tests.
"""

import re

from translate.filters import decoration
from translate.misc import quote


def removekdecomments(str1):
    r"""Remove KDE-style PO comments.

    KDE comments start with ``_:[space]`` and end with a literal ``\n``.
    Example::

      "_: comment\n"
    """
    assert isinstance(str1, unicode)
    iskdecomment = False
    lines = str1.split("\n")
    removelines = []
    for linenum in range(len(lines)):
        line = lines[linenum]
        if line.startswith("_:"):
            lines[linenum] = ""
            iskdecomment = True
        if iskdecomment:
            removelines.append(linenum)
        if line.strip() and not iskdecomment:
            break
        if iskdecomment and line.strip().endswith("\\n"):
            iskdecomment = False
    lines = [lines[linenum] for linenum in range(len(lines)) if linenum not in removelines]
    return "\n".join(lines)


def filteraccelerators(accelmarker):
    """Returns a function that filters accelerators marked using *accelmarker*
    from a strings.

    :param string accelmarker: Accelerator marker character
    :rtype: Function
    :return: fn(str1, acceplist=None)
    """
    if accelmarker is None:
        accelmarkerlen = 0
    else:
        accelmarkerlen = len(accelmarker)

    def filtermarkedaccelerators(str1, acceptlist=None):
        """Modifies the accelerators in *str1* marked with the given
        *accelmarker*, using a given *acceptlist* filter.
        """
        acclocs, badlocs = decoration.findaccelerators(str1, accelmarker, acceptlist)
        fstr1, pos = "", 0
        for accelstart, accelerator in acclocs:
            fstr1 += str1[pos:accelstart]
            fstr1 += accelerator
            pos = accelstart + accelmarkerlen + len(accelerator)
        fstr1 += str1[pos:]
        return fstr1
    return filtermarkedaccelerators


def varname(variable, startmarker, endmarker):
    """Variable filter that returns the variable name without the marking
    punctuation.

    .. note:: Currently this function simply returns *variable* unchanged, no
       matter what *\*marker*s are set to.

    :rtype: String
    :return: Variable name with the supplied *startmarker* and *endmarker*
             removed.
    """
    return variable
    # if the punctuation were included, we'd do the following:
    if startmarker is None:
        return variable[:variable.rfind(endmarker)]
    elif endmarker is None:
        return variable[variable.find(startmarker)+len(startmarker):]
    else:
        return variable[variable.find(startmarker)+len(startmarker):variable.rfind(endmarker)]


def varnone(variable, startmarker, endmarker):
    """Variable filter that returns an empty string.

    :rtype: String
    :return: Empty string
    """
    return ""


def filtervariables(startmarker, endmarker, varfilter):
    """Returns a function that filters variables marked using *startmarker* and
    *endmarker* from a string.

    :param string startmarker: Start of variable marker
    :param string endmarker: End of variable marker
    :param Function varfilter: fn(variable, startmarker, endmarker)
    :rtype: Function
    :return: fn(str1)
    """
    if startmarker is None:
        startmarkerlen = 0
    else:
        startmarkerlen = len(startmarker)
    if endmarker is None:
        endmarkerlen = 0
    elif type(endmarker) == int:
        endmarkerlen = 0
    else:
        endmarkerlen = len(endmarker)

    def filtermarkedvariables(str1):
        """Modifies the variables in *str1* marked with a given *\*marker*,
        using a given filter."""
        varlocs = decoration.findmarkedvariables(str1, startmarker, endmarker)
        fstr1, pos = "", 0
        for varstart, variable in varlocs:
            fstr1 += str1[pos:varstart]
            fstr1 += varfilter(variable, startmarker, endmarker)
            pos = varstart + startmarkerlen + len(variable) + endmarkerlen
        fstr1 += str1[pos:]
        return fstr1
    return filtermarkedvariables

# a list of special words with punctuation
# all apostrophes in the middle of the word are handled already
wordswithpunctuation = ["'n", "'t",]  # Afrikaans
# map all the words to their non-punctified equivalent
wordswithpunctuation = dict([(word, filter(str.isalnum, word)) for word in wordswithpunctuation])

word_with_apos_re = re.compile("(?u)\w+'\w+")


def filterwordswithpunctuation(str1):
    """Goes through a list of known words that have punctuation and removes the
    punctuation from them.
    """
    if u"'" not in str1:
        return str1
    occurrences = []
    for word, replacement in wordswithpunctuation.iteritems():
        occurrences.extend([(pos, word, replacement) for pos in quote.find_all(str1, word)])
    for match in word_with_apos_re.finditer(str1):
        word = match.group()
        replacement = filter(unicode.isalnum, word)
        occurrences.append((match.start(), word, replacement))
    occurrences.sort()
    if occurrences:
        lastpos = 0
        newstr1 = ""
        for pos, word, replacement in occurrences:
            newstr1 += str1[lastpos:pos]
            newstr1 += replacement
            lastpos = pos + len(word)
        newstr1 += str1[lastpos:]
        return newstr1
    else:
        return str1

########NEW FILE########
__FILENAME__ = spelling
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
# 2013 F Wolff
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""An API to provide spell checking for use in checks or elsewhere."""

import logging


logger = logging.getLogger(__name__)

available = False

try:
    # Enchant
    from enchant import checker, Error as EnchantError
    available = True
    checkers = {}

    def _get_checker(lang):
        if not lang in checkers:
            try:
                checkers[lang] = checker.SpellChecker(lang)
                # some versions only report an error when checking something
                checkers[lang].check(u'bla')
            except EnchantError as e:
                # sometimes this is raised instead of DictNotFoundError
                logger.error(str(e))
                checkers[lang] = None

        return checkers[lang]

    def check(text, lang):
        spellchecker = _get_checker(lang)
        if not spellchecker:
            return
        spellchecker.set_text(unicode(text))
        for err in spellchecker:
            yield err.word, err.wordpos, err.suggest()

    def simple_check(text, lang):
        spellchecker = _get_checker(lang)
        if not spellchecker:
            return
        spellchecker.set_text(unicode(text))
        for err in spellchecker:
            yield err.word


except ImportError:

    def check(text, lang):
        return []

    def simple_check(text, lang):
        return []

########NEW FILE########
__FILENAME__ = test_autocorrect
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.filters import autocorrect


class TestAutocorrect:

    def correct(self, msgid, msgstr, expected):
        """helper to run correct function from autocorrect module"""
        corrected = autocorrect.correct(msgid, msgstr)
        print(repr(msgid))
        print(repr(msgstr))
        print(msgid.encode('utf-8'))
        print(msgstr.encode('utf-8'))
        print((corrected or u"").encode('utf-8'))
        assert corrected == expected

    def test_empty_target(self):
        """test that we do nothing with an empty target"""
        self.correct(u"String...", u"", None)

    def test_correct_ellipsis(self):
        """test that we convert single  or ... to match source and target"""
        self.correct(u"String...", u"Translated", u"Translated...")
        self.correct(u"String", u"Translated...", u"Translated")

    def test_correct_spacestart_spaceend(self):
        """test that we can correct leading and trailing space errors"""
        self.correct(u"Simple string", u"Dimpled ring  ", u"Dimpled ring")
        self.correct(u"Simple string", u"  Dimpled ring", u"Dimpled ring")
        self.correct(u"  Simple string", u"Dimpled ring", u"  Dimpled ring")
        self.correct(u"Simple string  ", u"Dimpled ring", u"Dimpled ring  ")

    def test_correct_start_capitals(self):
        """test that we can correct the starting capital"""
        self.correct(u"Simple string", u"dimpled ring", u"Dimpled ring")
        self.correct(u"simple string", u"Dimpled ring", u"dimpled ring")

    def test_correct_end_punc(self):
        """test that we can correct end punctuation"""
        self.correct(u"Simple string:", u"Dimpled ring", u"Dimpled ring:")
        #self.correct(u"Simple string: ", u"Dimpled ring", u"Dimpled ring: ")
        self.correct(u"Simple string.", u"Dimpled ring", u"Dimpled ring.")
        #self.correct(u"Simple string. ", u"Dimpled ring", u"Dimpled ring. ")
        self.correct(u"Simple string?", u"Dimpled ring", u"Dimpled ring?")

    def test_correct_combinations(self):
        """test that we can correct combinations of failures"""
        self.correct(u"Simple string:", u"Dimpled ring ", u"Dimpled ring:")
        self.correct(u"simple string ", u"Dimpled ring", u"dimpled ring ")
        self.correct(u"Simple string...", u"dimpled ring..", u"Dimpled ring...")
        self.correct(u"Simple string:", u"Dimpled ring ", u"Dimpled ring:")

    def test_nothing_to_do(self):
        """test that when nothing changes we return None"""
        self.correct(u"Some text", u"A translation", None)

########NEW FILE########
__FILENAME__ = test_checks
# -*- coding: utf-8 -*-

from pytest import mark

from translate.filters import checks
from translate.lang import data
from translate.storage import po, xliff


def strprep(str1, str2, message=None):
    return data.normalized_unicode(str1), data.normalized_unicode(str2), data.normalized_unicode(message)


def check_category(filterfunction):
    """Checks whether ``filterfunction`` has defined a category or not."""
    has_category = []
    classes = (checks.TeeChecker, checks.UnitChecker)

    for klass in classes:
        categories = getattr(klass, 'categories', None)
        has_category.append(categories is not None and
                            filterfunction.__name__ in categories)

    return True in has_category


def passes(filterfunction, str1, str2):
    """returns whether the given strings pass on the given test, handling FilterFailures"""
    str1, str2, no_message = strprep(str1, str2)
    try:
        filterresult = filterfunction(str1, str2)
    except checks.FilterFailure as e:
        filterresult = False

    filterresult = filterresult and check_category(filterfunction)

    return filterresult


def fails(filterfunction, str1, str2, message=None):
    """returns whether the given strings fail on the given test, handling only FilterFailures"""
    str1, str2, message = strprep(str1, str2, message)
    try:
        filterresult = filterfunction(str1, str2)
    except checks.SeriousFilterFailure as e:
        filterresult = True
    except checks.FilterFailure as e:
        if message:
            exc_message = e.messages[0]
            filterresult = exc_message != message
            print(exc_message.encode('utf-8'))
        else:
            filterresult = False

    filterresult = filterresult and check_category(filterfunction)

    return not filterresult


def fails_serious(filterfunction, str1, str2, message=None):
    """returns whether the given strings fail on the given test, handling only SeriousFilterFailures"""
    str1, str2, message = strprep(str1, str2, message)
    try:
        filterresult = filterfunction(str1, str2)
    except checks.SeriousFilterFailure as e:
        if message:
            exc_message = e.messages[0]
            filterresult = exc_message != message
            print(exc_message.encode('utf-8'))
        else:
            filterresult = False

    filterresult = filterresult and check_category(filterfunction)

    return not filterresult


def test_defaults():
    """tests default setup and that checks aren't altered by other constructions"""
    stdchecker = checks.StandardChecker()
    assert stdchecker.config.varmatches == []
    mozillachecker = checks.MozillaChecker()
    stdchecker = checks.StandardChecker()
    assert stdchecker.config.varmatches == []


def test_construct():
    """tests that the checkers can be constructed"""
    stdchecker = checks.StandardChecker()
    mozillachecker = checks.MozillaChecker()
    ooochecker = checks.OpenOfficeChecker()
    loochecker = checks.LibreOfficeChecker()
    gnomechecker = checks.GnomeChecker()
    kdechecker = checks.KdeChecker()


def test_accelerator_markers():
    """test that we have the correct accelerator marker for the various default configs"""
    stdchecker = checks.StandardChecker()
    assert stdchecker.config.accelmarkers == []
    mozillachecker = checks.MozillaChecker()
    assert mozillachecker.config.accelmarkers == ["&"]
    ooochecker = checks.OpenOfficeChecker()
    assert ooochecker.config.accelmarkers == ["~"]
    lochecker = checks.LibreOfficeChecker()
    assert lochecker.config.accelmarkers == ["~"]
    gnomechecker = checks.GnomeChecker()
    assert gnomechecker.config.accelmarkers == ["_"]
    kdechecker = checks.KdeChecker()
    assert kdechecker.config.accelmarkers == ["&"]


def test_messages():
    """test that our helpers can check for messages and that these error messages can contain Unicode"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(validchars='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'))
    assert fails(stdchecker.validchars, "Some unexpected characters", "", "Invalid characters: '' (\\u00a9)")
    stdchecker = checks.StandardChecker()
    assert fails_serious(stdchecker.escapes, r"A tab", r"'n ab\t", r"""Escapes in original () don't match escapes in translation ('ab\t')""")


def test_accelerators():
    """tests accelerators"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&"))
    assert passes(stdchecker.accelerators, "&File", "&Fayile")
    assert fails(stdchecker.accelerators, "&File", "Fayile")
    assert fails(stdchecker.accelerators, "File", "&Fayile")
    assert passes(stdchecker.accelerators, "Mail && News", "Pos en Nuus")
    assert fails(stdchecker.accelerators, "Mail &amp; News", "Pos en Nuus")
    assert passes(stdchecker.accelerators, "&Allow", u'&\ufeb2\ufee3\ufe8e\ufea3')
    assert fails(stdchecker.accelerators, "Open &File", "Vula& Ifayile")
    kdechecker = checks.KdeChecker()
    assert passes(kdechecker.accelerators, "&File", "&Fayile")
    assert fails(kdechecker.accelerators, "&File", "Fayile")
    assert fails(kdechecker.accelerators, "File", "&Fayile")
    gnomechecker = checks.GnomeChecker()
    assert passes(gnomechecker.accelerators, "_File", "_Fayile")
    assert fails(gnomechecker.accelerators, "_File", "Fayile")
    assert fails(gnomechecker.accelerators, "File", "_Fayile")
    assert fails(gnomechecker.accelerators, "_File", "_Fayil_e")
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.accelerators, "&File", "&Fayile")
    assert passes(mozillachecker.accelerators, "Warn me if this will disable any of my add&-ons", "&Waarsku my as dit enige van my byvoegings sal deaktiveer")
    assert fails_serious(mozillachecker.accelerators, "&File", "Fayile")
    assert fails_serious(mozillachecker.accelerators, "File", "&Fayile")
    assert passes(mozillachecker.accelerators, "Mail &amp; News", "Pos en Nuus")
    assert fails_serious(mozillachecker.accelerators, "Mail &amp; News", "Pos en &Nuus")
    assert fails_serious(mozillachecker.accelerators, "&File", "Fayile")
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.accelerators, "~File", "~Fayile")
    assert fails(ooochecker.accelerators, "~File", "Fayile")
    assert fails(ooochecker.accelerators, "File", "~Fayile")

    # We don't want an accelerator for letters with a diacritic
    assert fails(ooochecker.accelerators, "F~ile", "L~er")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.accelerators, "~File", "~Fayile")
    assert fails(lochecker.accelerators, "~File", "Fayile")
    assert fails(lochecker.accelerators, "File", "~Fayile")

    # We don't want an accelerator for letters with a diacritic
    assert fails(lochecker.accelerators, "F~ile", "L~er")

    # Bug 289: accept accented accelerator characters
    afchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&", targetlanguage="fi"))
    assert passes(afchecker.accelerators, "&Reload Frame", "P&ivit kehys")

    trchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&", targetlanguage="tr"))
    assert passes(trchecker.accelerators, "&Download", "&ndir")
    assert passes(trchecker.accelerators, "&Business", "&")
    assert passes(trchecker.accelerators, "&Remove", "Kald&r")
    assert passes(trchecker.accelerators, "&Three", "&")
    assert passes(trchecker.accelerators, "&Three", "&")
    assert passes(trchecker.accelerators, "&Before", "&nce")
    assert passes(trchecker.accelerators, "Fo&ur", "D&rt")
    assert passes(trchecker.accelerators, "Mo&dern", "a&da")
    assert passes(trchecker.accelerators, "Mo&dern", "&ada")
    assert passes(trchecker.accelerators, "&February", "&ubat")
    assert passes(trchecker.accelerators, "P&lain", "D&z")
    assert passes(trchecker.accelerators, "GAR&DEN", "BA&")

    # Problems:
    # Accelerator before variable - see test_acceleratedvariables


@mark.xfail(reason="Accelerated variables needs a better implementation")
def test_acceleratedvariables():
    """test for accelerated variables"""
    # FIXME: disabled since acceleratedvariables has been removed, but these checks are still needed
    mozillachecker = checks.MozillaChecker()
    assert fails(mozillachecker.acceleratedvariables, "%S &Options", "&%S Ikhetho")
    assert passes(mozillachecker.acceleratedvariables, "%S &Options", "%S &Ikhetho")
    ooochecker = checks.OpenOfficeChecker()
    assert fails(ooochecker.acceleratedvariables, "%PRODUCTNAME% ~Options", "~%PRODUCTNAME% Ikhetho")
    assert passes(ooochecker.acceleratedvariables, "%PRODUCTNAME% ~Options", "%PRODUCTNAME% ~Ikhetho")
    lochecker = checks.LibreOfficeChecker()
    assert fails(lochecker.acceleratedvariables, "%PRODUCTNAME% ~Options", "~%PRODUCTNAME% Ikhetho")
    assert passes(lochecker.acceleratedvariables, "%PRODUCTNAME% ~Options", "%PRODUCTNAME% ~Ikhetho")


def test_acronyms():
    """tests acronyms"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.acronyms, "An HTML file", "'n HTML leer")
    assert fails(stdchecker.acronyms, "An HTML file", "'n LMTH leer")
    assert passes(stdchecker.acronyms, "It is HTML.", "Dit is HTML.")
    # We don't mind if you add an acronym to correct bad capitalisation in the original
    assert passes(stdchecker.acronyms, "An html file", "'n HTML leer")
    # We shouldn't worry about acronyms that appear in a musttranslate file
    stdchecker = checks.StandardChecker(checks.CheckerConfig(musttranslatewords=["OK"]))
    assert passes(stdchecker.acronyms, "OK", "Kulungile")
    # Assert punctuation should not hide accronyms
    assert fails(stdchecker.acronyms, "Location (URL) not found", "Blah blah blah")
    # Test '-W' (bug 283)
    assert passes(stdchecker.acronyms, "%s: option `-W %s' is ambiguous", "%s: opsie '-W %s' is dubbelsinnig")


def test_blank():
    """tests blank"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.blank, "Save as", " ")
    assert fails(stdchecker.blank, "_: KDE comment\\n\nSimple string", "  ")


def test_brackets():
    """tests brackets"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.brackets, "N number(s)", "N getal(le)")
    assert fails(stdchecker.brackets, "For {sic} numbers", "Vier getalle")
    assert fails(stdchecker.brackets, "For }sic{ numbers", "Vier getalle")
    assert fails(stdchecker.brackets, "For [sic] numbers", "Vier getalle")
    assert fails(stdchecker.brackets, "For ]sic[ numbers", "Vier getalle")
    assert passes(stdchecker.brackets, "{[(", "[({")


def test_compendiumconflicts():
    """tests compendiumconflicts"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.compendiumconflicts, "File not saved", r"""#-#-#-#-# file1.po #-#-#-#-#\n
Leer nie gestoor gestoor nie\n
#-#-#-#-# file1.po #-#-#-#-#\n
Leer nie gestoor""")


def test_doublequoting():
    """tests double quotes"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.doublequoting, "Hot plate", "\"Ipuleti\" elishisa")
    assert passes(stdchecker.doublequoting, "\"Hot\" plate", "\"Ipuleti\" elishisa")
    assert fails(stdchecker.doublequoting, "'Hot' plate", "\"Ipuleti\" elishisa")
    assert passes(stdchecker.doublequoting, "\\\"Hot\\\" plate", "\\\"Ipuleti\\\" elishisa")

    # We don't want the filter to complain about "untranslated" quotes in xml attributes
    frchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="fr"))
    assert passes(frchecker.doublequoting, "Click <a href=\"page.html\">", "Clique <a href=\"page.html\">")
    assert fails(frchecker.doublequoting, "Do \"this\"", "Do \"this\"")
    assert passes(frchecker.doublequoting, "Do \"this\"", "Do  this ")
    assert fails(frchecker.doublequoting, "Do \"this\"", "Do  this   this ")
    # This used to fail because we strip variables, and was left with an empty quotation that was not converted
    assert passes(frchecker.doublequoting, u"Copying `%s' to `%s'", u"Copie de %s vers %s")

    vichecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="vi"))
    assert passes(vichecker.doublequoting, 'Save "File"', u"Lu  Tp tin ")

    # Had a small exception with such a case:
    eschecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="es"))
    assert passes(eschecker.doublequoting, "<![CDATA[ Enter the name of the Windows workgroup that this server should appear in. ]]>",
            "<![CDATA[ Ingrese el nombre del grupo de trabajo de Windows en el que debe aparecer este servidor. ]]>")


def test_doublespacing():
    """tests double spacing"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.doublespacing, "Sentence.  Another sentence.", "Sin.  'n Ander sin.")
    assert passes(stdchecker.doublespacing, "Sentence. Another sentence.", "Sin. No double spacing.")
    assert fails(stdchecker.doublespacing, "Sentence.  Another sentence.", "Sin. Missing the double space.")
    assert fails(stdchecker.doublespacing, "Sentence. Another sentence.", "Sin.  Uneeded double space in translation.")
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.doublespacing, "Execute %PROGRAMNAME Calc", "Blah %PROGRAMNAME Calc")
    assert passes(ooochecker.doublespacing, "Execute %PROGRAMNAME Calc", "Blah % PROGRAMNAME Calc")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.doublespacing, "Execute %PROGRAMNAME Calc", "Blah %PROGRAMNAME Calc")
    assert passes(lochecker.doublespacing, "Execute %PROGRAMNAME Calc", "Blah % PROGRAMNAME Calc")


def test_doublewords():
    """tests doublewords"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.doublewords, "Save the rhino", "Save the rhino")
    assert fails(stdchecker.doublewords, "Save the rhino", "Save the the rhino")
    # Double variables are not an error
    stdchecker = checks.StandardChecker(checks.CheckerConfig(varmatches=[("%", 1)]))
    assert passes(stdchecker.doublewords, "%s %s installation", "tsenyo ya %s %s")
    # Double XML tags are not an error
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.doublewords, "Line one <br> <br> line two", "Lyn een <br> <br> lyn twee")
    # In some language certain double words are not errors
    st_checker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="st"))
    assert passes(st_checker.doublewords, "Color to draw the name of a message you sent.", "Mmala wa ho taka bitso la molaetsa oo o o rometseng.")
    assert passes(st_checker.doublewords, "Ten men", "Banna ba ba leshome")
    assert passes(st_checker.doublewords, "Give SARS the tax", "Lekgetho le le fe SARS")


def test_endpunc():
    """tests endpunc"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.endpunc, "Question?", "Correct?")
    assert fails(stdchecker.endpunc, " Question?", "Wrong ?")
    # Newlines must not mask end punctuation
    assert fails(stdchecker.endpunc, "Exit change recording mode?\n\n", "Phuma esimeni sekugucula kubhalisa.\n\n")
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.endpunc, "Upgrades an existing $ProductShortName$ installation.", "Ku antswisiwa ka ku nghenisiwa ka $ProductShortName$.")
    # Real examples
    assert passes(stdchecker.endpunc, "A nickname that identifies this publishing site (e.g.: 'MySite')", "Vito ro duvulela leri tirhisiwaka ku kuma sayiti leri ro kandziyisa (xik.: 'Sayiti ra Mina')")
    assert fails(stdchecker.endpunc, "Question", u"Wrong\u2026")
    # Making sure singlequotes don't confuse things
    assert passes(stdchecker.endpunc, "Pseudo-elements can't be negated '%1$S'.", "Pseudo-elemente kan nie '%1$S' ontken word nie.")

    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='km'))
    assert passes(stdchecker.endpunc, "In this new version, there are some minor conversion improvements on complex style in Openoffice.org Writer.", u"    \u00a0")

    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='zh'))
    assert passes(stdchecker.endpunc, "To activate your account, follow this link:\n", u"\n")

    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='vi'))
    assert passes(stdchecker.endpunc, "Do you want to delete the XX dialog?", u"Bn c mun xo hp thoi XX khng?")

    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='fr'))
    assert passes(stdchecker.endpunc, "Header:", u"En-tte :")
    assert passes(stdchecker.endpunc, "Header:", u"En-tte\u00a0:")


def test_endwhitespace():
    """tests endwhitespace"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.endwhitespace, "A setence.", "I'm correct.")
    assert passes(stdchecker.endwhitespace, "A setence. ", "I'm correct. ")
    assert fails(stdchecker.endwhitespace, "A setence. ", "'I'm incorrect.")
    assert passes(stdchecker.endwhitespace, "Problem with something: %s\n", "Probleem met iets: %s\n")

    zh_checker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='zh'))
    # This should pass since the space is not needed in Chinese
    assert passes(zh_checker.endwhitespace, "Init. Limit: ", "")


def test_escapes():
    """tests escapes"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.escapes, r"""A sentence""", "I'm correct.")
    assert passes(stdchecker.escapes, "A file\n", "'n Leer\n")
    assert fails_serious(stdchecker.escapes, r"blah. A file", r"bleah.\n'n leer")
    assert passes(stdchecker.escapes, r"A tab\t", r"'n Tab\t")
    assert fails_serious(stdchecker.escapes, r"A tab\t", r"'n Tab")
    assert passes(stdchecker.escapes, r"An escape escape \\", r"Escape escape \\")
    assert fails_serious(stdchecker.escapes, r"An escape escape \\", "Escape escape")
    assert passes(stdchecker.escapes, r"A double quote \"", r"Double quote \"")
    assert fails_serious(stdchecker.escapes, r"A double quote \"", "Double quote")
    # Escaped escapes
    assert passes(stdchecker.escapes, "An escaped newline \\n", "Escaped newline \\n")
    assert fails_serious(stdchecker.escapes, "An escaped newline \\n", "Escaped newline \n")
    # Real example
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.escapes, ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32", ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.escapes, ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32", ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32")


def test_newlines():
    """tests newlines"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.newlines, "Nothing to see", "Niks te sien")
    assert passes(stdchecker.newlines, "Correct\n", "Korrek\n")
    assert passes(stdchecker.newlines, "Correct\r", "Korrek\r")
    assert passes(stdchecker.newlines, "Correct\r\n", "Korrek\r\n")
    assert fails(stdchecker.newlines, "A file\n", "'n Leer")
    assert fails(stdchecker.newlines, "A file", "'n Leer\n")
    assert fails(stdchecker.newlines, "A file\r", "'n Leer")
    assert fails(stdchecker.newlines, "A file", "'n Leer\r")
    assert fails(stdchecker.newlines, "A file\n", "'n Leer\r\n")
    assert fails(stdchecker.newlines, "A file\r\n", "'n Leer\n")
    assert fails(stdchecker.newlines, "blah.\nA file", "bleah. 'n leer")
    # msgfmt errors
    assert fails(stdchecker.newlines, "One two\n", "Een\ntwee")
    assert fails(stdchecker.newlines, "\nOne two", "Een\ntwee")
    # Real example
    ooochecker = checks.OpenOfficeChecker()
    assert fails(ooochecker.newlines, "The arrowhead was modified without saving.\nWould you like to save the arrowhead now?", "hoho ya musevhe yo khwinifhadzwa hu si na u seiva.Ni khou oda u seiva thoho ya musevhe zwino?")
    lochecker = checks.LibreOfficeChecker()
    assert fails(lochecker.newlines, "The arrowhead was modified without saving.\nWould you like to save the arrowhead now?", "hoho ya musevhe yo khwinifhadzwa hu si na u seiva.Ni khou oda u seiva thoho ya musevhe zwino?")


def test_tabs():
    """tests tabs"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.tabs, "Nothing to see", "Niks te sien")
    assert passes(stdchecker.tabs, "Correct\t", "Korrek\t")
    assert passes(stdchecker.tabs, "Correct\tAA", "Korrek\tAA")
    assert fails_serious(stdchecker.tabs, "A file\t", "'n Leer")
    assert fails_serious(stdchecker.tabs, "A file", "'n Leer\t")
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.tabs, ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32", ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.tabs, ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32", ",\t44\t;\t59\t:\t58\t{Tab}\t9\t{space}\t32")


def test_filepaths():
    """tests filepaths"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.filepaths, "%s to the file /etc/hosts on your system.", "%s na die leer /etc/hosts op jou systeem.")
    assert fails(stdchecker.filepaths, "%s to the file /etc/hosts on your system.", "%s na die leer /etc/gasheer op jou systeem.")
    assert passes(stdchecker.filepaths, "Text with <br />line break", "Teks met <br /> lynbreuk")


def test_kdecomments():
    """tests kdecomments"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.kdecomments, r"""_: I am a comment\n
A string to translate""", "'n String om te vertaal")
    assert fails(stdchecker.kdecomments, r"""_: I am a comment\n
A string to translate""", r"""_: Ek is 'n commment\n
'n String om te vertaal""")
    assert fails(stdchecker.kdecomments, """_: I am a comment\\n\n""", """_: I am a comment\\n\n""")


def test_long():
    """tests long messages"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.long, "I am normal", "Ek is ook normaal")
    assert fails(stdchecker.long, "Short.", "Kort.......................................................................................")
    assert fails(stdchecker.long, "a", "bc")


@mark.xfail(reason="FIXME: All fails() tests are not working")
def test_musttranslatewords():
    """tests stopwords"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(musttranslatewords=[]))
    assert passes(stdchecker.musttranslatewords, "This uses Mozilla of course", "hierdie gebruik le mozille natuurlik")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(musttranslatewords=["Mozilla"]))
    assert passes(stdchecker.musttranslatewords, "This uses Mozilla of course", "hierdie gebruik le mozille natuurlik")
    assert fails(stdchecker.musttranslatewords, "This uses Mozilla of course", "hierdie gebruik Mozilla natuurlik")
    assert passes(stdchecker.musttranslatewords, "This uses Mozilla. Don't you?", "hierdie gebruik le mozille soos jy")
    assert fails(stdchecker.musttranslatewords, "This uses Mozilla. Don't you?", "hierdie gebruik Mozilla soos jy")
    # should always pass if there are no stopwords in the original
    assert passes(stdchecker.musttranslatewords, "This uses something else. Don't you?", "hierdie gebruik Mozilla soos jy")
    # check that we can find words surrounded by punctuation
    assert passes(stdchecker.musttranslatewords, "Click 'Mozilla' button", "Kliek 'Motzille' knoppie")
    assert fails(stdchecker.musttranslatewords, "Click 'Mozilla' button", "Kliek 'Mozilla' knoppie")
    assert passes(stdchecker.musttranslatewords, 'Click "Mozilla" button', 'Kliek "Motzille" knoppie')
    assert fails(stdchecker.musttranslatewords, 'Click "Mozilla" button', 'Kliek "Mozilla" knoppie')
    assert fails(stdchecker.musttranslatewords, 'Click "Mozilla" button', u'Kliek Mozilla knoppie')
    assert passes(stdchecker.musttranslatewords, "Click (Mozilla) button", "Kliek (Motzille) knoppie")
    assert fails(stdchecker.musttranslatewords, "Click (Mozilla) button", "Kliek (Mozilla) knoppie")
    assert passes(stdchecker.musttranslatewords, "Click Mozilla!", "Kliek Motzille!")
    assert fails(stdchecker.musttranslatewords, "Click Mozilla!", "Kliek Mozilla!")
    ## We need to define more word separators to allow us to find those hidden untranslated items
    #assert fails(stdchecker.musttranslatewords, "Click OK", "Blah we-OK")
    # Don't get confused when variables are the same as a musttranslate word
    stdchecker = checks.StandardChecker(checks.CheckerConfig(varmatches=[("%", None), ], musttranslatewords=["OK"]))
    assert passes(stdchecker.musttranslatewords, "Click %OK to start", "Kliek %OK om te begin")
    # Unicode
    assert fails(stdchecker.musttranslatewords, "Click OK", u"Kiikani OK")


def test_notranslatewords():
    """tests stopwords"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=[]))
    assert passes(stdchecker.notranslatewords, "This uses Mozilla of course", "hierdie gebruik le mozille natuurlik")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["Mozilla", "Opera"]))
    assert fails(stdchecker.notranslatewords, "This uses Mozilla of course", "hierdie gebruik le mozille natuurlik")
    assert passes(stdchecker.notranslatewords, "This uses Mozilla of course", "hierdie gebruik Mozilla natuurlik")
    assert fails(stdchecker.notranslatewords, "This uses Mozilla. Don't you?", "hierdie gebruik le mozille soos jy")
    assert passes(stdchecker.notranslatewords, "This uses Mozilla. Don't you?", "hierdie gebruik Mozilla soos jy")
    # should always pass if there are no stopwords in the original
    assert passes(stdchecker.notranslatewords, "This uses something else. Don't you?", "hierdie gebruik Mozilla soos jy")
    # Cope with commas
    assert passes(stdchecker.notranslatewords, "using Mozilla Task Manager", u"omia Selaola Moomo sa Mozilla, gomme")
    # Find words even if they are embedded in punctuation
    assert fails(stdchecker.notranslatewords, "Click 'Mozilla' button", "Kliek 'Motzille' knoppie")
    assert passes(stdchecker.notranslatewords, "Click 'Mozilla' button", "Kliek 'Mozilla' knoppie")
    assert fails(stdchecker.notranslatewords, "Click Mozilla!", "Kliek Motzille!")
    assert passes(stdchecker.notranslatewords, "Click Mozilla!", "Kliek Mozilla!")
    assert fails(stdchecker.notranslatewords, "Searches (From Opera)", "adosako (kusukela ku- Ophera)")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["Sun", "NeXT"]))
    assert fails(stdchecker.notranslatewords, "Sun/NeXT Audio", "Odio dza uvha/TeVHELAHO")
    assert passes(stdchecker.notranslatewords, "Sun/NeXT Audio", "Odio dza Sun/NeXT")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["sendmail"]))
    assert fails(stdchecker.notranslatewords, "because 'sendmail' could", "ngauri 'rumelameii' a yo")
    assert passes(stdchecker.notranslatewords, "because 'sendmail' could", "ngauri 'sendmail' a yo")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["Base"]))
    assert fails(stdchecker.notranslatewords, " - %PRODUCTNAME Base: Relation design", " - %PRODUCTNAME Sisekelo: Umsiko wekuhlobana")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["Writer"]))
    assert fails(stdchecker.notranslatewords, "&[ProductName] Writer/Web", "&[ProductName] Umbhali/iWebhu")
    # Unicode - different decompositions
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=[u"\u1e3cike"]))
    assert passes(stdchecker.notranslatewords, u"You \u1e3cike me", u"Ek \u004c\u032dike jou")


def test_numbers():
    """test numbers"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.numbers, "Netscape 4 was not as good as Netscape 7.", "Netscape 4 was nie so goed soos Netscape 7 nie.")
    # Check for correct detection of degree.  Also check that we aren't getting confused with 1 and 2 byte UTF-8 characters
    assert fails(stdchecker.numbers, "180 turn", "180 turn")
    assert passes(stdchecker.numbers, "180 turn", "180 turn")
    assert fails(stdchecker.numbers, "180 turn", "360 turn")
    assert fails(stdchecker.numbers, "180 turn", "360 turn")
    assert passes(stdchecker.numbers, "180~ turn", "180 turn")
    assert passes(stdchecker.numbers, "180 turn", "180 turn")
    # Numbers with multiple decimal points
    assert passes(stdchecker.numbers, "12.34.56", "12.34.56")
    assert fails(stdchecker.numbers, "12.34.56", "98.76.54")
    # Currency
    # FIXME we should probably be able to handle currency checking with locale inteligence
    assert passes(stdchecker.numbers, "R57.60", "R57.60")
    # FIXME - again locale intelligence should allow us to use other decimal seperators
    assert fails(stdchecker.numbers, "R57.60", "R57,60")
    assert fails(stdchecker.numbers, "1,000.00", "1 000,00")
    # You should be able to reorder numbers
    assert passes(stdchecker.numbers, "40-bit RC2 encryption with RSA and an MD5", "Umbhalo ocashile i-RC2 onamabhithi angu-40 one-RSA ne-MD5")
    # Don't fail the numbers check if the entry is a dialogsize entry
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.numbers, 'width: 12em;', 'width: 20em;')


def test_options():
    """tests command line options e.g. --option"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.options, "--help", "--help")
    assert fails(stdchecker.options, "--help", "--hulp")
    assert fails(stdchecker.options, "--input=FILE", "--input=FILE")
    assert passes(stdchecker.options, "--input=FILE", "--input=LER")
    assert fails(stdchecker.options, "--input=FILE", "--tovoer=LER")
    # We don't want just any '--' to trigger this test - the error will be confusing
    assert passes(stdchecker.options, "Hello! -- Hi", "Hallo! &mdash; Haai")
    assert passes(stdchecker.options, "--blank--", "--vide--")


def test_printf():
    """tests printf style variables"""
    # This should really be a subset of the variable checks
    # Ideally we should be able to adapt based on #, directives also
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.printf, "I am %s", "Ek is %s")
    assert fails(stdchecker.printf, "I am %s", "Ek is %d")
    assert passes(stdchecker.printf, "I am %#100.50hhf", "Ek is %#100.50hhf")
    assert fails(stdchecker.printf, "I am %#100s", "Ek is %10s")
    assert fails(stdchecker.printf, "... for user %.100s on %.100s:", "... lomuntu osebenzisa i-%. I-100s e-100s:")
    assert passes(stdchecker.printf, "%dMB", "%d MG")
    # Reordering
    assert passes(stdchecker.printf, "String %s and number %d", "String %1$s en nommer %2$d")
    assert passes(stdchecker.printf, "String %1$s and number %2$d", "String %1$s en nommer %2$d")
    assert passes(stdchecker.printf, "String %s and number %d", "Nommer %2$d and string %1$s")
    assert passes(stdchecker.printf, "String %s and real number %f and number %d", "String %1$s en nommer %3$d en rele getal %2$f")
    assert passes(stdchecker.printf, "String %1$s and real number %2$f and number %3$d", "String %1$s en nommer %3$d en rele getal %2$f")
    assert passes(stdchecker.printf, "Real number %2$f and string %1$s and number %3$d", "String %1$s en nommer %3$d en rele getal %2$f")
    assert fails(stdchecker.printf, "String %s and number %d", "Nommer %1$d and string %2$s")
    assert fails(stdchecker.printf, "String %s and real number %f and number %d", "String %1$s en nommer %3$d en rele getal %2$d")
    assert fails(stdchecker.printf, "String %s and real number %f and number %d", "String %1$s en nommer %3$d en rele getal %4$f")
    assert fails(stdchecker.printf, "String %s and real number %f and number %d", "String %2$s en nommer %3$d en rele getal %2$f")
    assert fails(stdchecker.printf, "Real number %2$f and string %1$s and number %3$d", "String %1$f en nommer %3$d en rele getal %2$f")
    # checking python format strings
    assert passes(stdchecker.printf, "String %(1)s and number %(2)d", "Nommer %(2)d en string %(1)s")
    assert passes(stdchecker.printf, "String %(str)s and number %(num)d", "Nommer %(num)d en string %(str)s")
    assert fails(stdchecker.printf, "String %(str)s and number %(num)d", "Nommer %(nommer)d en string %(str)s")
    assert fails(stdchecker.printf, "String %(str)s and number %(num)d", "Nommer %(num)d en string %s")
    # checking omitted plural format string placeholder %.0s
    stdchecker.hasplural = 1
    assert passes(stdchecker.printf, "%d plurals", "%.0s plural")
    # checking Objective-C %@ format specification
    assert fails(stdchecker.printf, "I am %@", "Ek is @%")  # typo
    assert fails(stdchecker.printf, "Object %@ and object %@", "String %1$@ en string %3$@")  # out of bounds
    assert fails(stdchecker.printf, "I am %@", "Ek is %s")  # wrong specification
    assert passes(stdchecker.printf, "Object %@ and string %s", "Object %1$@ en string %2$s")  # correct sentence


def test_puncspacing():
    """tests spacing after punctuation"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.puncspacing, "One, two, three.", "Kunye, kubili, kuthathu.")
    assert passes(stdchecker.puncspacing, "One, two, three. ", "Kunye, kubili, kuthathu.")
    assert fails(stdchecker.puncspacing, "One, two, three. ", "Kunye, kubili,kuthathu.")
    assert passes(stdchecker.puncspacing, "One, two, three!?", "Kunye, kubili, kuthathu?")

    # Some languages have padded puntuation marks
    frchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="fr"))
    assert passes(frchecker.puncspacing, "Do \"this\"", "Do  this ")
    assert passes(frchecker.puncspacing, u"Do \"this\"", u"Do \u00a0this\u00a0")
    assert fails(frchecker.puncspacing, "Do \"this\"", "Do this")


def test_purepunc():
    """tests messages containing only punctuation"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.purepunc, ".", ".")
    assert passes(stdchecker.purepunc, "", "")
    assert fails(stdchecker.purepunc, ".", " ")
    assert fails(stdchecker.purepunc, "Find", "'")
    assert fails(stdchecker.purepunc, "'", "Find")
    assert passes(stdchecker.purepunc, "year measurement template|2000", "2000")


def test_sentencecount():
    """tests sentencecount messages"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.sentencecount, "One. Two. Three.", "Een. Twee. Drie.")
    assert passes(stdchecker.sentencecount, "One two three", "Een twee drie.")
    assert fails(stdchecker.sentencecount, "One. Two. Three.", "Een Twee. Drie.")
    assert passes(stdchecker.sentencecount, "Sentence with i.e. in it.", "Sin met d.w.s. in dit.")  # bug 178, description item 8
    el_checker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='el'))
    assert fails(el_checker.sentencecount, "First sentence. Second sentence.", " .  .")


def test_short():
    """tests short messages"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.short, "I am normal", "Ek is ook normaal")
    assert fails(stdchecker.short, "I am a very long sentence", "Ek")
    assert fails(stdchecker.short, "abcde", "c")


def test_singlequoting():
    """tests single quotes"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.singlequoting, "A 'Hot' plate", "Ipuleti 'elishisa' kunye")
    # FIXME this should pass but doesn't probably to do with our logic that got confused at the end of lines
    assert passes(stdchecker.singlequoting, "'Hot' plate", "Ipuleti 'elishisa'")
    # FIXME newlines also confuse our algorithm for single quotes
    assert passes(stdchecker.singlequoting, "File '%s'\n", "'%s' Faele\n")
    assert fails(stdchecker.singlequoting, "'Hot' plate", "Ipuleti \"elishisa\"")
    assert passes(stdchecker.singlequoting, "It's here.", "Dit is hier.")
    # Don't get confused by punctuation that touches a single quote
    assert passes(stdchecker.singlequoting, "File '%s'.", "'%s' Faele.")
    assert passes(stdchecker.singlequoting, "Blah 'format' blah.", "Blah blah 'sebopego'.")
    assert passes(stdchecker.singlequoting, "Blah 'format' blah!", "Blah blah 'sebopego'!")
    assert passes(stdchecker.singlequoting, "Blah 'format' blah?", "Blah blah 'sebopego'?")
    # Real examples
    assert passes(stdchecker.singlequoting, "A nickname that identifies this publishing site (e.g.: 'MySite')", "Vito ro duvulela leri tirhisiwaka ku kuma sayiti leri ro kandziyisa (xik.: 'Sayiti ra Mina')")
    assert passes(stdchecker.singlequoting, "isn't", "ayikho")
    assert passes(stdchecker.singlequoting, "Required (can't send message unless all recipients have certificates)", "Verlang (kan nie boodskappe versend tensy al die ontvangers sertifikate het nie)")
    # Afrikaans 'n
    assert passes(stdchecker.singlequoting, "Please enter a different site name.", "Tik 'n ander werfnaam in.")
    assert passes(stdchecker.singlequoting, "\"%name%\" already exists. Please enter a different site name.", "\"%name%\" bestaan reeds. Tik 'n ander werfnaam in.")
    # Check that accelerators don't mess with removing singlequotes
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.singlequoting, "&Don't import anything", "&Moenie enigiets invoer nie")
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.singlequoting, "~Don't import anything", "~Moenie enigiets invoer nie")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.singlequoting, "~Don't import anything", "~Moenie enigiets invoer nie")

    vichecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="vi"))
    assert passes(vichecker.singlequoting, "Save 'File'", u"Lu  Tp tin ")
    assert passes(vichecker.singlequoting, "Save `File'", u"Lu  Tp tin ")


def test_simplecaps():
    """tests simple caps"""
    # Simple caps is a very vauge test so the checks here are mostly for obviously fixable problem
    # or for checking obviously correct situations that are triggering a failure.
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.simplecaps, "MB of disk space for the cache.", "MB yendzawo yediski etsala.")
    # We should squash 'I' in the source text as it messes with capital detection
    assert passes(stdchecker.simplecaps, "if you say I want", "as jy se ek wil")
    assert passes(stdchecker.simplecaps, "sentence. I want more.", "sin. Ek wil meer he.")
    assert passes(stdchecker.simplecaps, "Where are we? I can't see where we are going.", "Waar is ons? Ek kan nie sien waar ons gaan nie.")
    ## We should remove variables before checking
    stdchecker = checks.StandardChecker(checks.CheckerConfig(varmatches=[("%", 1)]))
    assert passes(stdchecker.simplecaps, "Could not load %s", "A swi koteki ku panga %S")
    assert passes(stdchecker.simplecaps, "The element \"%S\" is not recognized.", "Elemente \"%S\" a yi tiveki.")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(varmatches=[("&", ";")]))
    assert passes(stdchecker.simplecaps, "Determine how &brandShortName; connects to the Internet.", "Kuma &brandShortName; hlanganisa eka Internete.")
    ## If source is ALL CAPS then we should just check that target is also ALL CAPS
    assert passes(stdchecker.simplecaps, "COUPDAYS", "COUPMALANGA")
    # Just some that at times have failed but should always pass
    assert passes(stdchecker.simplecaps, "Create a query  entering an SQL statement directly.", "Yakha sibuti singena SQL inkhomba yesitatimende.")
    ooochecker = checks.OpenOfficeChecker()
    assert passes(ooochecker.simplecaps, "SOLK (%PRODUCTNAME Link)", "SOLK (%PRODUCTNAME Thumanyo)")
    assert passes(ooochecker.simplecaps, "%STAROFFICE Image", "Tshifanyiso tsha %STAROFFICE")
    lochecker = checks.LibreOfficeChecker()
    assert passes(lochecker.simplecaps, "SOLK (%PRODUCTNAME Link)", "SOLK (%PRODUCTNAME Thumanyo)")
    assert passes(lochecker.simplecaps, "%STAROFFICE Image", "Tshifanyiso tsha %STAROFFICE")
    assert passes(stdchecker.simplecaps, "Flies, flies, everywhere! Ack!", u"Vlie, oral vlie! Jig!")


@mark.xfail(reason="FIXME: spell checking test not working")
def test_spellcheck():
    """tests spell checking"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="af"))
    assert passes(stdchecker.spellcheck, "Great trek", "Groot trek")
    assert fails(stdchecker.spellcheck, "Final deadline", "End of the road")
    # Bug 289: filters accelerators before spell checking
    stdchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&", targetlanguage="fi"))
    assert passes(stdchecker.spellcheck, "&Reload Frame", "P&ivit kehys")
    # Ensure we don't check notranslatewords
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="af"))
    assert fails(stdchecker.spellcheck, "Mozilla is wonderful", "Mozillaaa is wonderlik")
    # We should pass the test if the "error" occurs in the English
    assert passes(stdchecker.spellcheck, "Mozillaxxx is wonderful", "Mozillaxxx is wonderlik")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="af", notranslatewords=["Mozilla"]))
    assert passes(stdchecker.spellcheck, "Mozilla is wonderful", "Mozilla is wonderlik")
    # Some variables were still being spell checked
    mozillachecker = checks.MozillaChecker(checkerconfig=checks.CheckerConfig(targetlanguage="af"))
    assert passes(mozillachecker.spellcheck, "&brandShortName.labels; is wonderful", "&brandShortName.label; is wonderlik")


def test_startcaps():
    """tests starting capitals"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.startcaps, "Find", "Vind")
    assert passes(stdchecker.startcaps, "find", "vind")
    assert fails(stdchecker.startcaps, "Find", "vind")
    assert fails(stdchecker.startcaps, "find", "Vind")
    assert passes(stdchecker.startcaps, "'", "'")
    assert passes(stdchecker.startcaps, "\\.,/?!`'\"[]{}()@#$%^&*_-;:<>Find", "\\.,/?!`'\"[]{}()@#$%^&*_-;:<>Vind")
    # With leading whitespace
    assert passes(stdchecker.startcaps, " Find", " Vind")
    assert passes(stdchecker.startcaps, " find", " vind")
    assert fails(stdchecker.startcaps, " Find", " vind")
    assert fails(stdchecker.startcaps, " find", " Vind")
    # Leading punctuation
    assert passes(stdchecker.startcaps, "'Find", "'Vind")
    assert passes(stdchecker.startcaps, "'find", "'vind")
    assert fails(stdchecker.startcaps, "'Find", "'vind")
    assert fails(stdchecker.startcaps, "'find", "'Vind")
    # Unicode
    assert passes(stdchecker.startcaps, "Find", u"ind")
    assert passes(stdchecker.startcaps, "find", u"ind")
    assert fails(stdchecker.startcaps, "Find", u"ind")
    assert fails(stdchecker.startcaps, "find", u"ind")
    # Unicode further down the Unicode tables
    assert passes(stdchecker.startcaps, "A text enclosed...", u"iwalwa o katelwaho...")
    assert fails(stdchecker.startcaps, "A text enclosed...", u"iwalwa o katelwaho...")
    # Accelerators
    stdchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&"))
    assert passes(stdchecker.startcaps, "&Find", "Vi&nd")
    # Numbers - we really can't tell what should happen with numbers, so ignore
    # source or target that start with a number
    assert passes(stdchecker.startcaps, "360 degrees", "Grade 360")
    assert passes(stdchecker.startcaps, "360 degrees", "grade 360")

    # Language specific stuff
    afchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='af'))
    assert passes(afchecker.startcaps, "A cow", "'n Koei")
    assert passes(afchecker.startcaps, "A list of ", "'n Lys van ")
    # should pass:
    #assert passes(afchecker.startcaps, "A 1k file", u"'n 1k-ler")
    assert passes(afchecker.startcaps, "'Do it'", "'Doen dit'")
    assert fails(afchecker.startcaps, "'Closer than'", "'nader as'")
    assert passes(afchecker.startcaps, "List", "Lys")
    assert passes(afchecker.startcaps, "a cow", "'n koei")
    assert fails(afchecker.startcaps, "a cow", "'n Koei")
    assert passes(afchecker.startcaps, "(A cow)", "('n Koei)")
    assert fails(afchecker.startcaps, "(a cow)", "('n Koei)")


def test_startpunc():
    """tests startpunc"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.startpunc, "<< Previous", "<< Correct")
    assert fails(stdchecker.startpunc, " << Previous", "Wrong")
    assert fails(stdchecker.startpunc, "Question", u"\u2026Wrong")

    assert passes(stdchecker.startpunc, "<fish>hello</fish> world", "world <fish>hello</fish>")

    # The inverted Spanish question mark should be accepted
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='es'))
    assert passes(stdchecker.startpunc, "Do you want to reload the file?", u"Quiere recargar el archivo?")

    # The Afrikaans indefinite article should be accepted
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='af'))
    assert passes(stdchecker.startpunc, "A human?", u"'n Mens?")


def test_startwhitespace():
    """tests startwhitespace"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.startwhitespace, "A setence.", "I'm correct.")
    assert fails(stdchecker.startwhitespace, " A setence.", "I'm incorrect.")


def test_unchanged():
    """tests unchanged entries"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig(accelmarkers="&"))
    assert fails(stdchecker.unchanged, "Unchanged", "Unchanged")
    assert fails(stdchecker.unchanged, "&Unchanged", "Un&changed")
    assert passes(stdchecker.unchanged, "Unchanged", "Changed")
    assert passes(stdchecker.unchanged, "1234", "1234")
    assert passes(stdchecker.unchanged, "22", "22")  # bug 178, description item 14
    assert passes(stdchecker.unchanged, "I", "I")
    assert passes(stdchecker.unchanged, "   ", "   ")  # bug 178, description item 5
    assert passes(stdchecker.unchanged, "???", "???")  # bug 178, description item 15
    assert passes(stdchecker.unchanged, "&ACRONYM", "&ACRONYM")  # bug 178, description item 7
    assert passes(stdchecker.unchanged, "F1", "F1")  # bug 178, description item 20
    assert fails(stdchecker.unchanged, "Two words", "Two words")
    #TODO: this still fails
#    assert passes(stdchecker.unchanged, "NOMINAL", "NOMNAL")
    gnomechecker = checks.GnomeChecker()
    assert fails(gnomechecker.unchanged, "Entity references, such as &amp; and &#169;", "Entity references, such as &amp; and &#169;")
    # Variable only and variable plus punctuation messages should be ignored
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.unchanged, "$ProgramName$", "$ProgramName$")
    assert passes(mozillachecker.unchanged, "$file$ : $dir$", "$file$ : $dir$")  # bug 178, description item 13
    assert fails(mozillachecker.unchanged, "$file$ in $dir$", "$file$ in $dir$")
    assert passes(mozillachecker.unchanged, "&brandShortName;", "&brandShortName;")
    # Don't translate words should be ignored
    stdchecker = checks.StandardChecker(checks.CheckerConfig(notranslatewords=["Mozilla"]))
    assert passes(stdchecker.unchanged, "Mozilla", "Mozilla")  # bug 178, description item 10
    # Don't fail unchanged if the entry is a dialogsize, quite plausible that you won't change it
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.unchanged, 'width: 12em;', 'width: 12em;')
    assert fails(stdchecker.unchanged, 'width: 12em;', 'width: 12em;')
    assert passes(mozillachecker.unchanged, '7em', '7em')
    assert fails(stdchecker.unchanged, '7em', '7em')


def test_untranslated():
    """tests untranslated entries"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.untranslated, "I am untranslated", "")
    assert passes(stdchecker.untranslated, "I am translated", "Ek is vertaal")
    # KDE comments that make it into translations should not mask untranslated test
    assert fails(stdchecker.untranslated, "_: KDE comment\\n\nI am untranslated", "_: KDE comment\\n\n")


def test_validchars():
    """tests valid characters"""
    stdchecker = checks.StandardChecker(checks.CheckerConfig())
    assert passes(stdchecker.validchars, "The check always passes if you don't specify chars", "Die toets sal altyd werk as jy nie karacters specifisier")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(validchars='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'))
    assert passes(stdchecker.validchars, "This sentence contains valid characters", "Hierdie sin bevat ware karakters")
    assert fails(stdchecker.validchars, "Some unexpected characters", "")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(validchars=''))
    assert passes(stdchecker.validchars, "Our target language is all non-ascii", "")
    assert fails(stdchecker.validchars, "Our target language is all non-ascii", "Some ascii")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(validchars=u'\u004c\u032d'))
    assert passes(stdchecker.validchars, "This sentence contains valid chars", u"\u004c\u032d")
    assert passes(stdchecker.validchars, "This sentence contains valid chars", u"\u1e3c")
    stdchecker = checks.StandardChecker(checks.CheckerConfig(validchars=u'\u1e3c'))
    assert passes(stdchecker.validchars, "This sentence contains valid chars", u"\u1e3c")
    assert passes(stdchecker.validchars, "This sentence contains valid chars", u"\u004c\u032d")


def test_variables_kde():
    """tests variables in KDE translations"""
    # GNOME variables
    kdechecker = checks.KdeChecker()
    assert passes(kdechecker.variables, "%d files of type %s saved.", "%d leers van %s tipe gestoor.")
    assert fails_serious(kdechecker.variables, "%d files of type %s saved.", "%s leers van %s tipe gestoor.")


def test_variables_gnome():
    """tests variables in GNOME translations"""
    # GNOME variables
    gnomechecker = checks.GnomeChecker()
    assert passes(gnomechecker.variables, "%d files of type %s saved.", "%d leers van %s tipe gestoor.")
    assert fails_serious(gnomechecker.variables, "%d files of type %s saved.", "%s leers van %s tipe gestoor.")
    assert passes(gnomechecker.variables, "Save $(file)", "Stoor $(file)")
    assert fails_serious(gnomechecker.variables, "Save $(file)", "Stoor $(leer)")


def test_variables_mozilla():
    """tests variables in Mozilla translations"""
    # Mozilla variables
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.variables, "Use the &brandShortname; instance.", "Gebruik die &brandShortname; weergawe.")
    assert fails_serious(mozillachecker.variables, "Use the &brandShortname; instance.", "Gebruik die &brandKortnaam; weergawe.")
    assert passes(mozillachecker.variables, "Save %file%", "Stoor %file%")
    assert fails_serious(mozillachecker.variables, "Save %file%", "Stoor %leer%")
    assert passes(mozillachecker.variables, "Save $file$", "Stoor $file$")
    assert fails_serious(mozillachecker.variables, "Save $file$", "Stoor $leer$")
    assert passes(mozillachecker.variables, "%d files of type %s saved.", "%d leers van %s tipe gestoor.")
    assert fails_serious(mozillachecker.variables, "%d files of type %s saved.", "%s leers van %s tipe gestoor.")
    assert passes(mozillachecker.variables, "Save $file", "Stoor $file")
    assert fails_serious(mozillachecker.variables, "Save $file", "Stoor $leer")
    assert passes(mozillachecker.variables, "About $ProgramName$", "Oor $ProgramName$")
    assert fails_serious(mozillachecker.variables, "About $ProgramName$", "Oor $NaamVanProgam$")
    assert passes(mozillachecker.variables, "About $_CLICK", "Oor $_CLICK")
    assert fails_serious(mozillachecker.variables, "About $_CLICK", "Oor $_KLIK")
    assert passes(mozillachecker.variables, "About $_CLICK and more", "Oor $_CLICK en meer")
    assert fails_serious(mozillachecker.variables, "About $_CLICK and more", "Oor $_KLIK en meer")
    assert passes(mozillachecker.variables, "About $(^NameDA)", "Oor $(^NameDA)")
    assert fails_serious(mozillachecker.variables, "About $(^NameDA)", "Oor $(^NaamDA)")
    assert passes(mozillachecker.variables, "Open {{pageCount}} pages", "Make {{pageCount}} bladsye oop")
    assert fails_serious(mozillachecker.variables, "Open {{pageCount}} pages", "Make {{bladTelling}} bladsye oop")
    # Double variable problem
    assert fails_serious(mozillachecker.variables, "Create In &lt;&lt;", "Etsa ka Ho &lt;lt;")
    # Variables at the end of a sentence
    assert fails_serious(mozillachecker.variables, "...time you start &brandShortName;.", "...lekgetlo le latelang ha o qala &LebitsoKgutshwane la kgwebo;.")
    # Ensure that we can detect two variables of the same name with one faulty
    assert fails_serious(mozillachecker.variables, "&brandShortName; successfully downloaded and installed updates. You will have to restart &brandShortName; to complete the update.", "&brandShortName; o dzhenisa na u longela khwinifhadzo zwavhui. Ni o tea u thoma hafhu &Dzinaipfufhi a pfungavhue; u itela u fhedzisa khwinifha dzo.")
    # We must detect entities in their fullform, ie with fullstop in the middle.
    assert fails_serious(mozillachecker.variables, "Welcome to the &pluginWizard.title;", "Wamkelekile kwi&Sihloko Soncedo lwe-plugin;")
    # Variables that are missing in quotes should be detected
    assert fails_serious(mozillachecker.variables, "\"%S\" is an executable file.... Are you sure you want to launch \"%S\"?", ".... Uyaqiniseka ukuthi ufuna ukuqalisa I\"%S\"?")
    # False positive $ style variables
    assert passes(mozillachecker.variables, "for reporting $ProductShortName$ crash information", "okokubika ukwaziswa kokumosheka kwe-$ProductShortName$")
    # We shouldn't mask variables within variables.  This should highlight &brandShortName as missing and &amp as extra
    assert fails_serious(mozillachecker.variables, "&brandShortName;", "&amp;brandShortName;")


def test_variables_openoffice():
    """tests variables in OpenOffice translations"""
    # OpenOffice.org variables
    for ooochecker in (checks.OpenOfficeChecker(), checks.LibreOfficeChecker()):
        assert passes(ooochecker.variables, "Use the &brandShortname; instance.", "Gebruik die &brandShortname; weergawe.")
        assert fails_serious(ooochecker.variables, "Use the &brandShortname; instance.", "Gebruik die &brandKortnaam; weergawe.")
        assert passes(ooochecker.variables, "Save %file%", "Stoor %file%")
        assert fails_serious(ooochecker.variables, "Save %file%", "Stoor %leer%")
        assert passes(ooochecker.variables, "Save %file", "Stoor %file")
        assert fails_serious(ooochecker.variables, "Save %file", "Stoor %leer")
        assert passes(ooochecker.variables, "Save %1", "Stoor %1")
        assert fails_serious(ooochecker.variables, "Save %1", "Stoor %2")
        assert passes(ooochecker.variables, "Save %", "Stoor %")
        assert fails_serious(ooochecker.variables, "Save %", "Stoor")
        assert passes(ooochecker.variables, "Save $(file)", "Stoor $(file)")
        assert fails_serious(ooochecker.variables, "Save $(file)", "Stoor $(leer)")
        assert passes(ooochecker.variables, "Save $file$", "Stoor $file$")
        assert fails_serious(ooochecker.variables, "Save $file$", "Stoor $leer$")
        assert passes(ooochecker.variables, "Save ${file}", "Stoor ${file}")
        assert fails_serious(ooochecker.variables, "Save ${file}", "Stoor ${leer}")
        assert passes(ooochecker.variables, "Save #file#", "Stoor #file#")
        assert fails_serious(ooochecker.variables, "Save #file#", "Stoor #leer#")
        assert passes(ooochecker.variables, "Save #1", "Stoor #1")
        assert fails_serious(ooochecker.variables, "Save #1", "Stoor #2")
        assert passes(ooochecker.variables, "Save #", "Stoor #")
        assert fails_serious(ooochecker.variables, "Save #", "Stoor")
        assert passes(ooochecker.variables, "Save ($file)", "Stoor ($file)")
        assert fails_serious(ooochecker.variables, "Save ($file)", "Stoor ($leer)")
        assert passes(ooochecker.variables, "Save $[file]", "Stoor $[file]")
        assert fails_serious(ooochecker.variables, "Save $[file]", "Stoor $[leer]")
        assert passes(ooochecker.variables, "Save [file]", "Stoor [file]")
        assert fails_serious(ooochecker.variables, "Save [file]", "Stoor [leer]")
        assert passes(ooochecker.variables, "Save $file", "Stoor $file")
        assert fails_serious(ooochecker.variables, "Save $file", "Stoor $leer")
        assert passes(ooochecker.variables, "Use @EXTENSION@", "Gebruik @EXTENSION@")
        assert fails_serious(ooochecker.variables, "Use @EXTENSUION@", "Gebruik @UITBRUIDING@")
        # Same variable name twice
        assert fails_serious(ooochecker.variables, r"""Start %PROGRAMNAME% as %PROGRAMNAME%""", "Begin %PROGRAMNAME%")


def test_variables_cclicense():
    """Tests variables in Creative Commons translations."""
    checker = checks.CCLicenseChecker()
    assert passes(checker.variables, "CC-GNU @license_code@.", "CC-GNU @license_code@.")
    assert fails_serious(checker.variables, "CC-GNU @license_code@.", "CC-GNU @lisensie_kode@.")
    assert passes(checker.variables, "Deed to the @license_name_full@", "Akte vir die @license_name_full@")
    assert fails_serious(checker.variables, "Deed to the @license_name_full@", "Akte vir die @volle_lisensie@")
    assert passes(checker.variables, "The @license_name_full@ is", "Die @license_name_full@ is")
    assert fails_serious(checker.variables, "The @license_name_full@ is", "Die @iiilicense_name_full@ is")
    assert fails_serious(checker.variables, "A @ccvar@", "'n @ccvertaaldeveranderlike@")


def test_xmltags():
    """tests xml tags"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.xmltags, "Do it <b>now</b>", "Doen dit <v>nou</v>")
    assert passes(stdchecker.xmltags, "Do it <b>now</b>", "Doen dit <b>nou</b>")
    assert passes(stdchecker.xmltags, "Click <img src=\"img.jpg\">here</img>", "Klik <img src=\"img.jpg\">hier</img>")
    assert fails(stdchecker.xmltags, "Click <img src=\"image.jpg\">here</img>", "Klik <img src=\"prent.jpg\">hier</img>")
    assert passes(stdchecker.xmltags, "Click <img src=\"img.jpg\" alt=\"picture\">here</img>", "Klik <img src=\"img.jpg\" alt=\"prentjie\">hier</img>")
    assert passes(stdchecker.xmltags, "Click <a title=\"tip\">here</a>", "Klik <a title=\"wenk\">hier</a>")
    assert passes(stdchecker.xmltags, "Click <div title=\"tip\">here</div>", "Klik <div title=\"wenk\">hier</div>")
    assert passes(stdchecker.xmltags, "Start with the &lt;start&gt; tag", "Begin met die &lt;begin&gt;")

    assert fails(stdchecker.xmltags, "Click <a href=\"page.html\">", "Klik <a hverw=\"page.html\">")
    assert passes(stdchecker.xmltags, "Click <a xml-lang=\"en\" href=\"page.html\">", "Klik <a xml-lang=\"af\" href=\"page.html\">")
    assert passes(stdchecker.xmltags, "Click <div lang=\"en\" dir=\"ltr\">", "Klik <div lang=\"ar\" dir=\"rtl\">")
    assert fails(stdchecker.xmltags, "Click <a href=\"page.html\" target=\"koei\">", "Klik <a href=\"page.html\">")
    assert fails(stdchecker.xmltags, "<b>Current Translation</b>", "<b>Traduccin Actual:<b>")
    assert passes(stdchecker.xmltags, "<Error>", "<Fout>")
    assert fails(stdchecker.xmltags, "%d/%d translated\n(%d blank, %d fuzzy)", "<br>%d/%d \n<br>(%d , %d )")
    assert fails(stdchecker.xmltags, '(and <a href="http://www.schoolforge.net/education-software" class="external">other open source software</a>)', '(en <a href="http://www.schoolforge.net/education-software" class="external">ander Vry Sagteware</a)')
    assert fails(stdchecker.xmltags, 'Because Tux Paint (and <a href="http://www.schoolforge.net/education-software" class="external">other open source software</a>) is free of cost and not limited in any way, a school can use it <i>today</i>, without waiting for procurement or a budget!', 'Omdat Tux Paint (en <a href="http://www.schoolforge.net/education-software" class="external">ander Vry Sagteware</a)gratis is en nie beperk is op enige manier nie, kan \'n skool dit vandag</i> gebruik sonder om te wag vir goedkeuring of \'n begroting!')
    assert fails(stdchecker.xmltags, "test <br />", "test <br>")
    assert fails(stdchecker.xmltags, "test <img src='foo.jpg'/ >", "test <img src='foo.jpg'  >")

    # This used to cause an error (traceback), because of mismatch between
    # different regular expressions (because of the newlines)
    assert passes(stdchecker.xmltags, '''<markup>
<span weight="bold" size="large"
style="oblique">
Can't create server !
</span>
</markup>''',
                                    '''<markup>
<span weight="bold" size="large"
style="oblique">
No s'ha pogut crear el servidor
</span>
</markup>''')
    frchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage="fr"))
    assert fails(frchecker.xmltags, "Click <a href=\"page.html\">", "Klik <a href= page.html >")


def test_ooxmltags():
    """Tests the xml tags in OpenOffice.org translations for quality as done in gsicheck"""
    for ooochecker in (checks.OpenOfficeChecker(), checks.LibreOfficeChecker()):
        #some attributes can be changed or removed
        assert fails(ooochecker.xmltags, "<img src=\"a.jpg\" width=\"400\">", "<img src=\"b.jpg\" width=\"500\">")
        assert passes(ooochecker.xmltags, "<img src=\"a.jpg\" width=\"400\">", "<img src=\"a.jpg\" width=\"500\">")
        assert passes(ooochecker.xmltags, "<img src=\"a.jpg\" width=\"400\">", "<img src=\"a.jpg\">")
        assert passes(ooochecker.xmltags, "<img src=\"a.jpg\">", "<img src=\"a.jpg\" width=\"400\">")
        assert passes(ooochecker.xmltags, "<alt xml-lang=\"ab\">text</alt>", "<alt>teks</alt>")
        assert passes(ooochecker.xmltags, "<ahelp visibility=\"visible\">bla</ahelp>", "<ahelp>blu</ahelp>")
        assert fails(ooochecker.xmltags, "<ahelp visibility=\"visible\">bla</ahelp>", "<ahelp visibility=\"invisible\">blu</ahelp>")
        assert fails(ooochecker.xmltags, "<ahelp visibility=\"invisible\">bla</ahelp>", "<ahelp>blu</ahelp>")
        #some attributes can be changed, but not removed
        assert passes(ooochecker.xmltags, "<link name=\"John\">", "<link name=\"Jan\">")
        assert fails(ooochecker.xmltags, "<link name=\"John\">", "<link naam=\"Jan\">")

        # Reported OOo error
        ## Bug 1910
        assert fails(ooochecker.xmltags, u"""<variable id="FehlendesElement">In a database file window, click the <emph>Queries</emph> icon, then choose <emph>Edit - Edit</emph>. When referenced fields no longer exist, you see this dialog</variable>""", u"""<variable id="FehlendesElement">Dans  une fentre de fichier de base de donnes, cliquez sur l'icne <emph>Requtes</emph>, puis choisissez <emph>diter - diter</emp>. Lorsque les champs de rfrence n'existent plus, vous voyez cette bote de dialogue</variable>""")
        assert fails(ooochecker.xmltags, "<variable> <emph></emph> <emph></emph> </variable>", "<variable> <emph></emph> <emph></emp> </variable>")


def test_functions():
    """tests to see that funtions() are not translated"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.functions, "blah rgb() blah", "blee brg() blee")
    assert passes(stdchecker.functions, "blah rgb() blah", "blee rgb() blee")
    assert fails(stdchecker.functions, "percentage in rgb()", "phesenthe kha brg()")
    assert passes(stdchecker.functions, "percentage in rgb()", "phesenthe kha rgb()")
    assert fails(stdchecker.functions, "rgb() in percentage", "brg() kha phesenthe")
    assert passes(stdchecker.functions, "rgb() in percentage", "rgb() kha phesenthe")
    assert fails(stdchecker.functions, "blah string.rgb() blah", "blee bleeb.rgb() blee")
    assert passes(stdchecker.functions, "blah string.rgb() blah", "blee string.rgb() blee")
    assert passes(stdchecker.functions, "or domain().", "domain() verwag.")
    assert passes(stdchecker.functions, "Expected url(), url-prefix(), or domain().", "url(), url-prefix() of domain() verwag.")


def test_emails():
    """tests to see that email addresses are not translated"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.emails, "blah bob@example.net blah", "blee kobus@voorbeeld.net blee")
    assert passes(stdchecker.emails, "blah bob@example.net blah", "blee bob@example.net blee")


def test_urls():
    """tests to see that URLs are not translated"""
    stdchecker = checks.StandardChecker()
    assert fails(stdchecker.urls, "blah http://translate.org.za blah", "blee http://vertaal.org.za blee")
    assert passes(stdchecker.urls, "blah http://translate.org.za blah", "blee http://translate.org.za blee")


def test_simpleplurals():
    """test that we can find English style plural(s)"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.simpleplurals, "computer(s)", "rekenaar(s)")
    assert fails(stdchecker.simpleplurals, "plural(s)", "meervoud(e)")
    assert fails(stdchecker.simpleplurals, "Ungroup Metafile(s)...", "Kuvhanganyululani Metafaela(dzi)...")

    # Test a language that doesn't use plurals
    stdchecker = checks.StandardChecker(checks.CheckerConfig(targetlanguage='vi'))
    assert passes(stdchecker.simpleplurals, "computer(s)", u"My tnh")
    assert fails(stdchecker.simpleplurals, "computer(s)", u"My tnh(s)")


def test_nplurals():
    """Test that we can find the wrong number of plural forms. Note that this
    test uses a UnitChecker, not a translation checker."""
    checker = checks.StandardUnitChecker()
    unit = po.pounit("")

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d ler", u"%d lers"]
    assert checker.nplurals(unit)

    checker = checks.StandardUnitChecker(checks.CheckerConfig(targetlanguage='af'))
    unit.source = "%d files"
    unit.target = "%d ler"
    assert checker.nplurals(unit)

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d ler", u"%d lers"]
    assert checker.nplurals(unit)

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d ler", u"%d lers", u"%d leeeers"]
    assert not checker.nplurals(unit)

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d ler"]
    assert not checker.nplurals(unit)

    checker = checks.StandardUnitChecker(checks.CheckerConfig(targetlanguage='km'))
    unit.source = "%d files"
    unit.target = "%d "
    assert checker.nplurals(unit)

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d "]
    assert checker.nplurals(unit)

    unit.source = ["%d file", "%d files"]
    unit.target = [u"%d ", u"%d lers"]
    assert not checker.nplurals(unit)


def test_credits():
    """tests credits"""
    stdchecker = checks.StandardChecker()
    assert passes(stdchecker.credits, "File", "iFayile")
    assert passes(stdchecker.credits, "&File", "&Fayile")
    assert passes(stdchecker.credits, "translator-credits", "Ekke, ekke!")
    assert passes(stdchecker.credits, "Your names", "Ekke, ekke!")
    assert passes(stdchecker.credits, "ROLES_OF_TRANSLATORS", "Ekke, ekke!")
    kdechecker = checks.KdeChecker()
    assert passes(kdechecker.credits, "File", "iFayile")
    assert passes(kdechecker.credits, "&File", "&Fayile")
    assert passes(kdechecker.credits, "translator-credits", "Ekke, ekke!")
    assert fails(kdechecker.credits, "Your names", "Ekke, ekke!")
    assert fails(kdechecker.credits, "ROLES_OF_TRANSLATORS", "Ekke, ekke!")
    gnomechecker = checks.GnomeChecker()
    assert passes(gnomechecker.credits, "File", "iFayile")
    assert passes(gnomechecker.credits, "&File", "&Fayile")
    assert fails(gnomechecker.credits, "translator-credits", "Ekke, ekke!")
    assert passes(gnomechecker.credits, "Your names", "Ekke, ekke!")
    assert passes(gnomechecker.credits, "ROLES_OF_TRANSLATORS", "Ekke, ekke!")


def test_gconf():
    """test GNOME gconf errors"""
    gnomechecker = checks.GnomeChecker()
    # Let's cheat a bit and prepare the checker as the run_filters() method
    # would do by adding locations needed by the gconf test
    gnomechecker.locations = []
    assert passes(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_setting"')
    assert passes(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_steling"')
    gnomechecker.locations = ['file.schemas.in.h:24']
    assert passes(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_setting"')
    assert fails(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_steling"')
    # redo the same, but with the new location comment:
    gnomechecker.locations = ['file.gschema.xml.in.in.h:24']
    assert passes(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_setting"')
    assert fails(gnomechecker.gconf, 'Blah "gconf_setting"', 'Bleh "gconf_steling"')

def test_validxml():
    """test wheather validxml recognize invalid xml/html expressions"""
    lochecker = checks.LibreOfficeChecker()
    # Test validity only for xrm and xhp files
    lochecker.locations = ["description.xml"]
    assert passes(lochecker.validxml, "","normal string")
    assert passes(lochecker.validxml, "","<emph> only an open tag")
    lochecker.locations = ["readme.xrm"]
    assert passes(lochecker.validxml, "","normal string")
    assert passes(lochecker.validxml, "","<tt>closed formula</tt>")
    assert fails(lochecker.validxml, "","<tt> only an open tag")
    lochecker.locations = ["wikisend.xhp"]
    assert passes(lochecker.validxml, "","A <emph> well formed expression </emph>")
    assert fails(lochecker.validxml, "","Missing <emph> close tag <emph>")
    assert fails(lochecker.validxml, "","Missing open tag </emph>")
    assert fails(lochecker.validxml, "","<ahelp hid=\".\"> open tag not match with close tag</link>")
    assert passes(lochecker.validxml, "","Skip <IMG> because it is with capitalization so it is part of the text")
    assert passes(lochecker.validxml, "","Skip the capitalized <Empty>, because it is just a pseudo tag not a real one")
    assert passes(lochecker.validxml, "","Skip <br/> short tag, because no need to close it.")
    # Larger tests
    assert passes(lochecker.validxml, "","<bookmark_value>yazdrma; izim varsaylanlar</bookmark_value><bookmark_value>izimler; yazdrma varsaylanlar</bookmark_value><bookmark_value>sayfalar;sunumlarda sayfa ad yazdrma</bookmark_value><bookmark_value>yazdrma; sunumlarda tarihler</bookmark_value><bookmark_value>tarihler; sunumlarda  yazdrma</bookmark_value><bookmark_value>zamanlar; sunumlar yazdrrken ekleme</bookmark_value><bookmark_value>yazdrma; sunumlarn gizli sayfalar</bookmark_value><bookmark_value>gizli sayfalar; sunumlarda yazdrma</bookmark_value><bookmark_value>yazdrma; sunumlarda leklendirme olmadan</bookmark_value><bookmark_value>lekleme; sunumlar yazdrlrken</bookmark_value><bookmark_value>yazdrma; sunumlarda sayfalara sdrma</bookmark_value><bookmark_value>sayfalara sdrma; sunumlarda yazdrma ayarlar</bookmark_value><bookmark_value>yazdrma; sunumlarda kapak sayfas</bookmark_value>")
    assert fails(lochecker.validxml, "","Kullanc etkileimi verisinin kaydedilmesini ve bu verilerin gnderilmesini dilediiniz zaman etkinletirebilir veya devre d brakabilirsiniz.  <item type=\"menuitem\"><switchinline select=\"sys\"><caseinline select=\"MAC\">%PRODUCTNAME - Tercihler</caseinline><defaultinline>Aralar - Seenekler</defaultinline></switchinline> - %PRODUCTNAME - Geliim Program</item>'n sein. Daha fazla bilgi iin web sitesinde gezinmek iin <defaultinline>Bilgi</emph> simgesine tklayn.")
    assert fails(lochecker.validxml, "","<caseinline select=\"DRAW\">Bir sayfann ierik mensnde ek komutlar vardr:</caseinline><caseinline select=\"IMPRESS\">Bir sayfann ierik mensnde ek komutlar vardr:</caseinline></switchinline>")
    assert fails(lochecker.validxml, "","<bookmark_value>sunum; sihirbaz balatmak<bookmark_value>nesneler; her zaman tanabilir (Impress/Draw)</bookmark_value><bookmark_value>izimleri eriltme</bookmark_value><bookmark_value>aralama; sunumdaki sekmeler</bookmark_value><bookmark_value>metin nesneleri; sunumlarda ve izimlerde</bookmark_value>")


def test_hassuggestion():
    """test that hassuggestion() works"""
    checker = checks.StandardUnitChecker()

    po_store = po.pofile()
    po_store.addsourceunit("koeie")
    assert checker.hassuggestion(po_store.units[-1])

    xliff_store = xliff.xlifffile.parsestring('''
<xliff version='1.2'
       xmlns='urn:oasis:names:tc:xliff:document:1.2'>
<file original='hello.txt' source-language='en' target-language='fr' datatype='plaintext'>
<body>
    <trans-unit id='hi'>
        <source>Hello world</source>
        <target>Bonjour le monde</target>
        <alt-trans>
            <target xml:lang='es'>Hola mundo</target>
        </alt-trans>
    </trans-unit>
</body>
</file>
</xliff>
''')
    assert not checker.hassuggestion(xliff_store.units[0])


def test_dialogsizes():
    """test Mozilla dialog sizes"""
    mozillachecker = checks.MozillaChecker()
    assert passes(mozillachecker.dialogsizes, 'width: 12em;', 'width: 12em;')
    assert passes(mozillachecker.dialogsizes, 'width: 12em; height: 36em', 'width: 12em; height: 36em')
    assert fails(mozillachecker.dialogsizes, 'height: 12em;', 'hoogde: 12em;')
    assert passes(mozillachecker.dialogsizes, 'height: 12em;', 'height: 24px;')
    assert fails(mozillachecker.dialogsizes, 'height: 12em;', 'height: 24xx;')
    assert fails(mozillachecker.dialogsizes, 'height: 12.5em;', 'height: 12,5em;')
    assert fails(mozillachecker.dialogsizes, 'width: 36em; height: 18em;', 'width: 30em; min-height: 20em;')

########NEW FILE########
__FILENAME__ = test_decoration
# -*- coding: utf-8 -*-

"""tests decoration handling functions that are used by checks"""

from translate.filters import decoration


def test_spacestart():
    """test operation of spacestart()"""
    assert decoration.spacestart("  Start") == "  "
    assert decoration.spacestart(u"\u0020\u00a0Start") == u"\u0020\u00a0"
    # non-breaking space
    assert decoration.spacestart(u"\u00a0\u202fStart") == u"\u00a0\u202f"
    # Some exotic spaces
    assert decoration.spacestart(u"\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200aStart") == u"\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a"


def test_isvalidaccelerator():
    """test the isvalidaccelerator() function"""
    # Mostly this tests the old code path where acceptlist is None
    assert not decoration.isvalidaccelerator(u"")
    assert decoration.isvalidaccelerator(u"a")
    assert decoration.isvalidaccelerator(u"1")
    assert not decoration.isvalidaccelerator(u"")
    # Test new code path where we actually have an acceptlist
    assert decoration.isvalidaccelerator(u"a", u"aeiou")
    assert decoration.isvalidaccelerator(u"", u"")
    assert not decoration.isvalidaccelerator(u"a", u"")


def test_find_marked_variables():
    """check that we can identify variables correctly, the first returned
    value is the start location, the second returned value is the actual
    variable sans decoations"""
    variables = decoration.findmarkedvariables("The <variable> string", "<", ">")
    assert variables == [(4, "variable")]
    variables = decoration.findmarkedvariables("The $variable string", "$", 1)
    assert variables == [(4, "v")]
    variables = decoration.findmarkedvariables("The $variable string", "$", None)
    assert variables == [(4, "variable")]
    variables = decoration.findmarkedvariables("The $variable string", "$", 0)
    assert variables == [(4, "")]
    variables = decoration.findmarkedvariables("The &variable; string", "&", ";")
    assert variables == [(4, "variable")]
    variables = decoration.findmarkedvariables("The &variable.variable; string", "&", ";")
    assert variables == [(4, "variable.variable")]


def test_getnumbers():
    """test operation of getnumbers()"""
    assert decoration.getnumbers(u"") == []
    assert decoration.getnumbers(u"No numbers") == []
    assert decoration.getnumbers(u"Nine 9 nine") == ["9"]
    assert decoration.getnumbers(u"Two numbers: 2 and 3") == ["2", "3"]
    assert decoration.getnumbers(u"R5.99") == ["5.99"]
    # TODO fix these so that we are able to consider locale specific numbers
    #assert decoration.getnumbers(u"R5,99") == ["5.99"]
    #assert decoration.getnumbers(u"1\u00a0000,99") == ["1000.99"]
    assert decoration.getnumbers(u"36") == [u"36"]
    assert decoration.getnumbers(u"English 123, Bengali \u09e7\u09e8\u09e9") == [u"123", u"\u09e7\u09e8\u09e9"]


def test_getfunctions():
    """test operation of getfunctions()"""
    assert decoration.getfunctions(u"") == []
    assert decoration.getfunctions(u"There is no function") == []
    assert decoration.getfunctions(u"Use the getfunction() function.") == ["getfunction()"]
    assert decoration.getfunctions(u"Use the getfunction1() function or the getfunction2() function.") == ["getfunction1()", "getfunction2()"]
    assert decoration.getfunctions(u"The module.getfunction() method") == ["module.getfunction()"]
    assert decoration.getfunctions(u"The module->getfunction() method") == ["module->getfunction()"]
    assert decoration.getfunctions(u"The module::getfunction() method") == ["module::getfunction()"]
    assert decoration.getfunctions(u"The function().function() function") == ["function().function()"]
    assert decoration.getfunctions(u"Deprecated, use function().") == ["function()"]
    assert decoration.getfunctions(u"Deprecated, use function() or other().") == ["function()", "other()"]

########NEW FILE########
__FILENAME__ = test_pofilter
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.filters import checks, pofilter
from translate.misc import wStringIO
from translate.storage import factory, xliff
from translate.storage.test_base import first_translatable, headerless_len


class BaseTestFilter(object):
    """Base class for filter tests."""

    filename = ""

    def parse_text(self, filetext):
        """helper that parses xliff file content without requiring files"""
        dummyfile = wStringIO.StringIO(filetext)
        dummyfile.name = self.filename
        store = factory.getobject(dummyfile)
        return store

    def filter(self, translationstore, checkerconfig=None,
               cmdlineoptions=None):
        """Helper that passes a translations store through a filter, and
        returns the resulting store."""
        if cmdlineoptions is None:
            cmdlineoptions = []
        options, args = pofilter.cmdlineparser().parse_args([self.filename] +
                                                            cmdlineoptions)
        checkerclasses = [checks.StandardChecker, checks.StandardUnitChecker]
        if checkerconfig is None:
            checkerconfig = pofilter.build_checkerconfig(options)
        checkfilter = pofilter.pocheckfilter(options, checkerclasses,
                                             checkerconfig)
        tofile = checkfilter.filterfile(translationstore)
        return tofile

    def test_simplepass(self):
        """checks that an obviously correct string passes"""
        filter_result = self.filter(self.translationstore)
        assert headerless_len(filter_result.units) == 0

    def test_simplefail(self):
        """checks that an obviously wrong string fails"""
        self.unit.target = "REST"
        filter_result = self.filter(self.translationstore)
        print(filter_result)
        print(filter_result.units)
        assert 'startcaps' in first_translatable(filter_result).geterrors()

    def test_variables_across_lines(self):
        """Test that variables can span lines and still fail/pass"""
        self.unit.source = '"At &timeBombURL."\n"label;."'
        self.unit.target = '"Tydens &tydBombURL."\n"etiket;."'
        filter_result = self.filter(self.translationstore)
        assert headerless_len(filter_result.units) == 0

    def test_ignore_if_already_marked(self):
        """check that we don't add another failing marker if the message is
        already marked as failed"""
        self.unit.target = ''
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--test=untranslated"])
        errors = first_translatable(filter_result).geterrors()
        assert len(errors) == 1
        assert 'untranslated' in errors

        # Run a filter test on the result, to check that it doesn't mark the
        # same error twice.
        filter_result2 = self.filter(filter_result,
                                     cmdlineoptions=["--test=untranslated"])
        errors = first_translatable(filter_result2).geterrors()
        assert len(errors) == 1
        assert 'untranslated' in errors

    def test_non_existant_check(self):
        """check that we report an error if a user tries to run a non-existant
        test"""
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["-t nonexistant"])
        # TODO Not sure how to check for the stderror result of: warning: could
        # not find filter  nonexistant
        assert headerless_len(filter_result.units) == 0

    def test_list_all_tests(self):
        """lists all available tests"""
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["-l"])
        # TODO again not sure how to check the stderror output
        assert headerless_len(filter_result.units) == 0

    def test_test_against_fuzzy(self):
        """test whether to run tests against fuzzy translations"""
        self.unit.markfuzzy()

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--fuzzy"])
        assert 'isfuzzy' in first_translatable(filter_result).geterrors()

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--nofuzzy"])
        assert headerless_len(filter_result.units) == 0

        # Re-initialize the translation store object in order to get an unfuzzy
        # unit with no filter notes.
        self.setup_method(self)

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--fuzzy"])
        assert headerless_len(filter_result.units) == 0

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--nofuzzy"])
        assert headerless_len(filter_result.units) == 0

    def test_test_against_review(self):
        """test whether to run tests against translations marked for review"""
        self.unit.markreviewneeded()
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--review"])
        assert first_translatable(filter_result).isreview()

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--noreview"])
        assert headerless_len(filter_result.units) == 0

        # Re-initialize the translation store object.
        self.setup_method(self)

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--review"])
        assert headerless_len(filter_result.units) == 0
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--noreview"])
        assert headerless_len(filter_result.units) == 0

    def test_isfuzzy(self):
        """tests the extraction of items marked fuzzy"""
        self.unit.markfuzzy()

        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--test=isfuzzy"])
        assert "isfuzzy" in first_translatable(filter_result).geterrors()

        self.unit.markfuzzy(False)
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--test=isfuzzy"])
        assert headerless_len(filter_result.units) == 0

    def test_isreview(self):
        """tests the extraction of items marked review"""
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--test=isreview"])
        assert headerless_len(filter_result.units) == 0

        self.unit.markreviewneeded()
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--test=isreview"])
        assert first_translatable(filter_result).isreview()

    def test_notes(self):
        """tests the optional adding of notes"""
        # let's make sure we trigger the 'long' and/or 'doubleword' test
        self.unit.target = u"asdf asdf asdf asdf asdf asdf asdf"
        filter_result = self.filter(self.translationstore)
        assert headerless_len(filter_result.units) == 1
        assert first_translatable(filter_result).geterrors()

        # now we remove the existing error. self.unit is changed since we copy
        # units - very naughty
        if isinstance(self.unit, xliff.xliffunit):
            self.unit.removenotes(origin='pofilter')
        else:
            self.unit.removenotes()
        filter_result = self.filter(self.translationstore,
                                    cmdlineoptions=["--nonotes"])
        assert headerless_len(filter_result.units) == 1
        assert len(first_translatable(filter_result).geterrors()) == 0

    def test_unicode(self):
        """tests that we can handle UTF-8 encoded characters when there is no
        known header specified encoding"""
        self.unit.source = u'Bzier curve'
        self.unit.target = u'Bzier-kurwe'
        filter_result = self.filter(self.translationstore)
        assert headerless_len(filter_result.units) == 0

    def test_preconditions(self):
        """tests that the preconditions work correctly"""
        self.unit.source = "File"
        self.unit.target = ""
        filter_result = self.filter(self.translationstore)
        # We should only get one error (untranslated), and nothing else
        assert headerless_len(filter_result.units) == 1
        unit = first_translatable(filter_result)
        assert len(unit.geterrors()) == 1


class TestPOFilter(BaseTestFilter):
    """Test class for po-specific tests."""
    filetext = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
    filename = 'test.po'

    def setup_method(self, method):
        self.translationstore = self.parse_text(self.filetext)
        self.unit = first_translatable(self.translationstore)

    def test_msgid_comments(self):
        """Tests that msgid comments don't feature anywhere."""
        posource = '''
msgid "_: Capital.  ACRONYMN. (msgid) comment 3. %d Extra sentence.\\n"
"cow"
msgstr "koei"
'''
        pofile = self.parse_text(posource)
        filter_result = self.filter(pofile)
        if headerless_len(filter_result.units):
            print(first_translatable(filter_result))
        assert headerless_len(filter_result.units) == 0


class TestXliffFilter(BaseTestFilter):
    """Test class for xliff-specific tests."""
    filetext = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
<file original='NoName' source-language="en" datatype="plaintext">
  <body>
    <trans-unit approved="yes">
      <source>test</source>
      <target>rest</target>
    </trans-unit>
  </body>
</file>
</xliff>'''
    filename = "test.xlf"

    def set_store_review(self, review=True):
        self.filetext = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
<file datatype="po" original="example.po" source-language="en-US">
  <body>
    <trans-unit approved="yes">
      <source>test</source>
      <target>rest</target>
    </trans-unit>
  </body>
</file>
</xliff>'''

        self.translationstore = self.parse_text(self.filetext)
        self.unit = first_translatable(self.translationstore)

    def setup_method(self, method):
        self.translationstore = self.parse_text(self.filetext)
        self.unit = first_translatable(self.translationstore)


class TestTMXFilter(BaseTestFilter):
    """Test class for TMX-specific tests."""
    filetext = '''<!DOCTYPE tmx SYSTEM "tmx14.dtd">
<tmx version="1.4">
  <header creationtool="Translate Toolkit - po2tmx"
          creationtoolversion="1.1.1rc1" segtype="sentence" o-tmf="UTF-8"
          adminlang="en" srclang="en" datatype="PlainText"/>
  <body>
    <tu>
      <tuv xml:lang="en">
        <seg>test</seg>
      </tuv>
      <tuv xml:lang="af">
        <seg>rest</seg>
      </tuv>
    </tu>
  </body>
</tmx>'''
    filename = "test.tmx"

    def setup_method(self, method):
        self.translationstore = self.parse_text(self.filetext)
        self.unit = first_translatable(self.translationstore)

    def test_test_against_fuzzy(self):
        """TMX doesn't support fuzzy"""
        pass

    def test_test_against_review(self):
        """TMX doesn't support review"""
        pass

    def test_isfuzzy(self):
        """TMX doesn't support fuzzy"""
        pass

    def test_isreview(self):
        """TMX doesn't support review"""
        pass

########NEW FILE########
__FILENAME__ = test_prefilters
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""tests decoration handling functions that are used by checks"""

from translate.filters import prefilters


def test_removekdecomments():
    assert prefilters.removekdecomments(u"Some sring") == u"Some sring"
    assert prefilters.removekdecomments(u"_: Commen\\n\nSome sring") == u"Some sring"
    assert prefilters.removekdecomments(u"_: Commen\\n\n") == u""


def test_filterwordswithpunctuation():
    string = u"Nothing in here."
    filtered = prefilters.filterwordswithpunctuation(string)
    assert filtered == string
    # test listed words (start / end with apostrophe)
    string = u"'n Boom het 'n tak."
    filtered = prefilters.filterwordswithpunctuation(string)
    assert filtered == "n Boom het n tak."
    # test words containing apostrophe
    string = u"It's in it's own place."
    filtered = prefilters.filterwordswithpunctuation(string)
    assert filtered == "Its in its own place."
    # test strings in unicode
    string = u"I'"
    filtered = prefilters.filterwordswithpunctuation(string)
    assert filtered == u"I"

########NEW FILE########
__FILENAME__ = af
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Afrikaans language.

.. seealso:: http://en.wikipedia.org/wiki/Afrikaans_language
"""

import re

from translate.lang import common


articlere = re.compile(r"'n\b")


class af(common.Common):
    """This class represents Afrikaans."""
    validdoublewords = [u"u"]

    punctuation = u"".join([common.Common.commonpunc, common.Common.quotes,
                            common.Common.miscpunc])
    sentenceend = u".!?"
    sentencere = re.compile(r"""
        (?s)        # make . also match newlines
        .*?         # anything, but match non-greedy
        [%s]        # the puntuation for sentence ending
        \s+         # the spacing after the puntuation
        (?='n\s[A-Z]|[^'a-z\d]|'[^n])
        # lookahead that next part starts with caps or 'n followed by caps
        """ % sentenceend, re.VERBOSE)

    specialchars = u""

    @classmethod
    def capsstart(cls, text):
        """Modify this for the indefinite article ('n)."""
        match = articlere.search(text, 0, 20)
        if match:
            #construct a list of non-apostrophe punctuation:
            nonapos = u"".join(cls.punctuation.split(u"'"))
            stripped = text.lstrip().lstrip(nonapos)
            match = articlere.match(stripped)
            if match:
                return common.Common.capsstart(stripped[match.end():])
        return common.Common.capsstart(text)

cyr2lat = {
   u"": "A", u"": "a",
   u"": "B", u"": "b",
   u"": "W", u"": "w",  # Different if at the end of a syllable see rule 2.
   u"": "G", u"": "g",  # see rule 3 and 4
   u"": "D", u"": "d",
   u"": "Dj", u"": "dj",
   u"": "Je", u"": "je",  # Sometimes e need to check when/why see rule 5.
   u"": "Jo", u"": "jo",  # see rule 6
   u"": "Ei", u"": "ei",
   u"": "Zj", u"": "zj",
   u"": "Z", u"": "z",
   u"": "I", u"": "i",
   u"": "J", u"": "j",  # see rule 9 and 10
   u"": "K", u"": "k",  # see note 11
   u"": "L", u"": "l",
   u"": "M", u"": "m",
   u"": "N", u"": "n",
   u"": "O", u"": "o",
   u"": "P", u"": "p",
   u"": "R", u"": "r",
   u"": "S", u"": "s",  # see note 12
   u"": "T", u"": "t",
   u"": "Oe", u"": "oe",
   u"": "F", u"": "f",
   u"": "Ch", u"": "ch",  # see rule 12
   u"": "Ts", u"": "ts",
   u"": "Tj", u"": "tj",
   u"": "Sj", u"": "sj",
   u"": "Sjtsj", u"": "sjtsj",
   u"": "I", u"": "i",  # see note 13
   u"": "", u"": "",  # See note 14
   u"": "", u"": "",  # this letter is not in the AWS we assume it is left out as in the previous letter
   u"": "E", u"": "e",
   u"": "Joe", u"": "joe",
   u"": "Ja", u"": "ja",
}
"""Mapping of Cyrillic to Latin letters for transliteration in Afrikaans"""

cyr_vowels = u""


def tranliterate_cyrillic(text):
    """Convert Cyrillic text to Latin according to the AWS transliteration rules."""
    trans = u""
    for i in text:
        trans += cyr2lat.get(i, i)
    return trans

########NEW FILE########
__FILENAME__ = ak
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Akan language.

.. seealso:: http://en.wikipedia.org/wiki/Akan_language
"""

from translate.lang import common


class ak(common.Common):
    """This class represents Akan."""

    specialchars = ""

########NEW FILE########
__FILENAME__ = am
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Amharic language.

.. seealso:: http://en.wikipedia.org/wiki/Amharic_language
"""

import re

from translate.lang import common


class am(common.Common):
    """This class represents Amharic."""

    listseperator = u" "

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #anything, but match non-greedy
                            [%s]        #the puntuation for sentence ending
                            \s*         #optional spacing after the puntuation
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u".": u"",
        u";": u"",
        u",": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ar
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007,2009,2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Arabic language.

.. seealso:: http://en.wikipedia.org/wiki/Arabic_language
"""

import re

from translate.lang import common


def reverse_quotes(text):
    def convertquotation(match):
        return u"%s" % match.group(1)
    return re.sub(u'([^]+)', convertquotation, text)


class ar(common.Common):
    """This class represents Arabic."""

    listseperator = u" "

    puncdict = {
        u",": u"",
        u";": u"",
        u"?": u"",
        #This causes problems with variables, so commented out for now:
        #u"%": u"",
    }

    ignoretests = ["startcaps", "simplecaps", "acronyms"]

    @classmethod
    def punctranslate(cls, text):
        text = super(cls, cls).punctranslate(text)
        return reverse_quotes(text)

########NEW FILE########
__FILENAME__ = az
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Azerbaijani language.

.. seealso:: http://en.wikipedia.org/wiki/Azerbaijani_language
"""

from translate.lang import common


class az(common.Common):
    """This class represents Azerbaijani."""

    mozilla_nplurals = 1
    mozilla_pluralequation = "0"

########NEW FILE########
__FILENAME__ = bn
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Bengali language.

.. seealso:: http://en.wikipedia.org/wiki/Bengali_language
"""

import re

from translate.lang import common


class bn(common.Common):
    """This class represents Bengali."""

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #anything, but match non-greedy
                            [%s]        #the puntuation for sentence ending
                            \s+         #the spacing after the puntuation
                            (?=[^a-z\d])#lookahead that next part starts with caps
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u". ": u" ",
        u".\n": u"\n",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = code_or
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Oriya language.

.. seealso:: http://en.wikipedia.org/wiki/Oriya_language
"""

import re

from translate.lang import common


class code_or(common.Common):
    """This class represents Oriya."""

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #anything, but match non-greedy
                            [%s]        #the puntuation for sentence ending
                            \s+         #the spacing after the puntuation
                            (?=[^a-z\d])#lookahead that next part starts with caps
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u". ": u" ",
        u".\n": u"\n",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = common
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module contains all the common features for languages.

Supported features:

- language code (km, af)
- language name (Khmer, Afrikaans)
- Plurals

  - Number of plurals (nplurals)
  - Plural equation

- pofilter tests to ignore

Segmentation:

- characters
- words
- sentences

Punctuation:

- End of sentence
- Start of sentence
- Middle of sentence
- Quotes

  - single
  - double

- Valid characters
- Accelerator characters
- Special characters
- Direction (rtl or ltr)

TODOs and Ideas for possible features:

- Language-Team information
- Segmentation

  - phrases
"""

import logging
import re

from translate.lang import data


logger = logging.getLogger(__name__)


class Common(object):
    """This class is the common parent class for all language classes."""

    code = ""
    """The ISO 639 language code, possibly with a country specifier or other
    modifier.

    Examples::

        km
        pt_BR
        sr_YU@Latn
    """

    fullname = ""
    """The full (English) name of this language.

    Dialect codes should have the form of:

    - Khmer
    - Portugese (Brazil)
    - TODO: sr_YU@Latn?
    """

    nplurals = 0
    """The number of plural forms of this language.

    0 is not a valid value - it must be overridden.
    Any positive integer is valid (it should probably be between 1 and 6)

    .. seealso:: :mod:`translate.lang.data`
    """

    pluralequation = "0"
    """The plural equation for selection of plural forms.

    This is used for PO files to fill into the header.

    .. seealso::

       `Gettext manual <http://www.gnu.org/software/gettext/manual/html_node/gettext_150.html#Plural-forms>`_, :mod:`translate.lang.data`
    """
    # Don't change these defaults of nplurals or pluralequation willy-nilly:
    # some code probably depends on these for unrecognised languages

    mozilla_nplurals = 0
    mozilla_pluralequation = "0"
    """This of languages that has different plural formula in Mozilla than the
    standard one in Gettext."""

    listseperator = u", "
    """This string is used to separate lists of textual elements. Most
    languages probably can stick with the default comma, but Arabic and some
    Asian languages might want to override this."""

    specialchars = u""
    """Characters used by the language that might not be easy to input with
    common keyboard layouts"""

    commonpunc = u".,;:!?-@#$%^*_()[]{}/\\'`\"<>"
    """These punctuation marks are common in English and most languages that
    use latin script."""

    quotes = u""
    """These are different quotation marks used by various languages."""

    invertedpunc = u""
    """Inverted punctuation sometimes used at the beginning of sentences in
    Spanish, Asturian, Galician, and Catalan."""

    rtlpunc = u""
    """These punctuation marks are used by Arabic and Persian, for example."""

    CJKpunc = u""
    """These punctuation marks are used in certain circumstances with CJK
    languages."""

    indicpunc = u""
    """These punctuation marks are used by several Indic languages."""

    ethiopicpunc = u""
    """These punctuation marks are used by several Ethiopic languages."""

    miscpunc = u""
    """The middle dot () is used by Greek and Georgian."""

    punctuation = u"".join([commonpunc, quotes, invertedpunc, rtlpunc, CJKpunc,
                            indicpunc, ethiopicpunc, miscpunc])
    """We include many types of punctuation here, simply since this is only
    meant to determine if something is punctuation. Hopefully we catch some
    languages which might not be represented with modules. Most languages won't
    need to override this."""

    sentenceend = u".!?\u06d4"
    """These marks can indicate a sentence end. Once again we try to account
    for many languages. Most langauges won't need to override this."""

    #The following tries to account for a lot of things. For the best idea of
    #what works, see test_common.py. We try to ignore abbreviations, for
    #example, by checking that the following sentence doesn't start with lower
    #case or numbers.
    sentencere = re.compile(ur"""
        (?s)        # make . also match newlines
        .*?         # anything, but match non-greedy
        [%s]        # the puntuation for sentence ending
        \s+         # the spacing after the puntuation
        (?=[^a-z-\d])  # lookahead that next part starts with caps
        """ % sentenceend, re.VERBOSE | re.UNICODE)

    puncdict = {}
    """A dictionary of punctuation transformation rules that can be used by
    punctranslate()."""

    ignoretests = []
    """List of pofilter tests for this language that must be ignored."""

    checker = None
    """A language specific checker (see filters.checks).

    This doesn't need to be supplied, but will be used if it exists."""

    _languages = {}

    validaccel = None
    """Characters that can be used as accelerators (access keys) i.e. Alt+X
    where X is the accelerator.  These can include combining diacritics as
    long as they are accessible from the users keyboard in a single keystroke,
    but normally they would be at least precomposed characters. All characters,
    lower and upper, are included in the list."""

    validdoublewords = []
    """Some languages allow double words in certain cases.  This is a dictionary
    of such words."""

    def __new__(cls, code):
        """This returns the language class for the given code, following a
        singleton like approach (only one object per language)."""
        code = code or ""
        # First see if a language object for this code already exists
        if code in cls._languages:
            return cls._languages[code]
        # No existing language. Let's build a new one and keep a copy
        language = cls._languages[code] = object.__new__(cls)

        language.code = code
        while code:
            langdata = data.get_language(code)
            if langdata:
                language.fullname, language.nplurals, \
                    language.pluralequation = langdata
                break
            code = data.simplercode(code)
        return language

    def __deepcopy__(self, memo={}):
        memo[id(self)] = self
        return self

    def __repr__(self):
        """Give a simple string representation without address information to
        be able to store it in text for comparison later."""
        detail = ""
        if self.code:
            detail = "(%s)" % self.code
        return "<class 'translate.lang.common.Common%s'>" % detail

    @classmethod
    def punctranslate(cls, text):
        """Converts the punctuation in a string according to the rules of the
        language."""
        #TODO: look at po::escapeforpo() for performance idea
        if not text:
            return text
        ellipses_end = text.endswith(u"...")
        if ellipses_end:
            text = text[:-3]
        for source, target in cls.puncdict.iteritems():
            text = text.replace(source, target)
        if ellipses_end:
            if u"..." in cls.puncdict:
                text += cls.puncdict[u"..."]
            else:
                text += u"..."
        # Let's account for cases where a punctuation symbol plus a space is
        # replaced, but the space won't exist at the end of the source message.
        # As a simple improvement for messages ending in ellipses (...), we
        # test that the last character is different from the second last
        # This is only relevant if the string has two characters or more
        if ((text[-1] + u" " in cls.puncdict) and
            (len(text) < 2 or text[-2] != text[-1])):
            text = text[:-1] + cls.puncdict[text[-1] + u" "].rstrip()
        return text

    @classmethod
    def length_difference(cls, length):
        """Returns an estimate to a likely change in length relative to an
        English string of length length."""
        # This is just a rudimentary heuristic guessing that most translations
        # will be somewhat longer than the source language
        expansion_factor = 0
        code = cls.code
        while code:
            expansion_factor = data.expansion_factors.get(cls.code, 0)
            if expansion_factor:
                break
            code = data.simplercode(code)
        else:
            expansion_factor = 0.1  # default
        constant = max(5, int(40 * expansion_factor))
        # The default: return 5 + length/10
        return constant + int(expansion_factor * length)

    @classmethod
    def alter_length(cls, text):
        """Converts the given string by adding or removing characters as an
        estimation of translation length (with English assumed as source
        language)."""

        def alter_it(text):
            l = len(text)
            if l > 9:
                extra = cls.length_difference(l)
                if extra > 0:
                    text = text[:extra].replace(u'\n', u'') + text
                else:
                    text = text[-extra:]
            return text
        expanded = []
        for subtext in text.split(u"\n\n"):
            expanded.append(alter_it(subtext))
        text = u"\n\n".join(expanded)
        return text

    @classmethod
    def character_iter(cls, text):
        """Returns an iterator over the characters in text."""
        #We don't return more than one consecutive whitespace character
        prev = 'A'
        for c in text:
            if c.isspace() and prev.isspace():
                continue
            prev = c
            if not (c in cls.punctuation):
                yield c

    @classmethod
    def characters(cls, text):
        """Returns a list of characters in text."""
        return [c for c in cls.character_iter(text)]

    @classmethod
    def word_iter(cls, text):
        """Returns an iterator over the words in text."""
        #TODO: Consider replacing puctuation with space before split()
        for w in text.split():
            word = w.strip(cls.punctuation)
            if word:
                yield word

    @classmethod
    def words(cls, text):
        """Returns a list of words in text."""
        return [w for w in cls.word_iter(text)]

    @classmethod
    def sentence_iter(cls, text, strip=True):
        """Returns an iterator over the sentences in text."""
        lastmatch = 0
        text = text or ""
        for item in cls.sentencere.finditer(text):
            lastmatch = item.end()
            sentence = item.group()
            if strip:
                sentence = sentence.strip()
            if sentence:
                yield sentence
        remainder = text[lastmatch:]
        if strip:
            remainder = remainder.strip()
        if remainder:
            yield remainder

    @classmethod
    def sentences(cls, text, strip=True):
        """Returns a list of sentences in text."""
        return [s for s in cls.sentence_iter(text, strip=strip)]

    @classmethod
    def capsstart(cls, text):
        """Determines whether the text starts with a capital letter."""
        stripped = text.lstrip().lstrip(cls.punctuation)
        return stripped and stripped[0].isupper()

    @classmethod
    def numstart(cls, text):
        """Determines whether the text starts with a numeric value."""
        stripped = text.lstrip().lstrip(cls.punctuation)
        return stripped and stripped[0].isnumeric()

########NEW FILE########
__FILENAME__ = data
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module stores information and functionality that relates to plurals."""


languages = {
    'af': (u'Afrikaans', 2, '(n != 1)'),
    'ak': (u'Akan', 2, 'n > 1'),
    'am': (u'Amharic', 2, 'n > 1'),
    'an': (u'Aragonese', 2, '(n != 1)'),
    'ar': (u'Arabic', 6,
           'n==0 ? 0 : n==1 ? 1 : n==2 ? 2 : n%100>=3 && n%100<=10 ? 3 : n%100>=11 ? 4 : 5'),
    'arn': (u'Mapudungun; Mapuche', 2, 'n > 1'),
    'ast': (u'Asturian; Bable; Leonese; Asturleonese', 2, '(n != 1)'),
    'az': (u'Azerbaijani', 2, '(n != 1)'),
    'be': (u'Belarusian', 3,
           'n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
    'bg': (u'Bulgarian', 2, '(n != 1)'),
    'bn': (u'Bengali', 2, '(n != 1)'),
    'bn_IN': (u'Bengali (India)', 2, '(n != 1)'),
    'bo': (u'Tibetan', 1, '0'),
    'br': (u'Breton', 2, 'n > 1'),
    'bs': (u'Bosnian', 3,
           'n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
    'ca': (u'Catalan; Valencian', 2, '(n != 1)'),
    'ca@valencia': (u'Catalan; Valencian (Valencia)', 2, '(n != 1)'),
    'cs': (u'Czech', 3, '(n==1) ? 0 : (n>=2 && n<=4) ? 1 : 2'),
    'csb': (u'Kashubian', 3,
            'n==1 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
    'cy': (u'Welsh', 2, '(n==2) ? 1 : 0'),
    'da': (u'Danish', 2, '(n != 1)'),
    'de': (u'German', 2, '(n != 1)'),
    'dz': (u'Dzongkha', 1, '0'),
    'el': (u'Greek, Modern (1453-)', 2, '(n != 1)'),
    'en': (u'English', 2, '(n != 1)'),
    'en_GB': (u'English (United Kingdom)', 2, '(n != 1)'),
    'en_ZA': (u'English (South Africa)', 2, '(n != 1)'),
    'eo': (u'Esperanto', 2, '(n != 1)'),
    'es': (u'Spanish; Castilian', 2, '(n != 1)'),
    'et': (u'Estonian', 2, '(n != 1)'),
    'eu': (u'Basque', 2, '(n != 1)'),
    'fa': (u'Persian', 1, '0'),
    'ff': (u'Fulah', 2, '(n != 1)'),
    'fi': (u'Finnish', 2, '(n != 1)'),
    'fil': (u'Filipino; Pilipino', 2, '(n > 1)'),
    'fo': (u'Faroese', 2, '(n != 1)'),
    'fr': (u'French', 2, '(n > 1)'),
    'fur': (u'Friulian', 2, '(n != 1)'),
    'fy': (u'Frisian', 2, '(n != 1)'),
    'ga': (u'Irish', 5, 'n==1 ? 0 : n==2 ? 1 : n<7 ? 2 : n<11 ? 3 : 4'),
    'gd': (u'Gaelic; Scottish Gaelic', 4, '(n==1 || n==11) ? 0 : (n==2 || n==12) ? 1 : (n > 2 && n < 20) ? 2 : 3'),
    'gl': (u'Galician', 2, '(n != 1)'),
    'gu': (u'Gujarati', 2, '(n != 1)'),
    'gun': (u'Gun', 2, '(n > 1)'),
    'ha': (u'Hausa', 2, '(n != 1)'),
    'he': (u'Hebrew', 2, '(n != 1)'),
    'hi': (u'Hindi', 2, '(n != 1)'),
    'hy': (u'Armenian', 1, '0'),
    'hr': (u'Croatian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'ht': (u'Haitian; Haitian Creole', 2, '(n != 1)'),
    'hu': (u'Hungarian', 2, '(n != 1)'),
    'ia': (u"Interlingua (International Auxiliary Language Association)", 2, '(n != 1)'),
    'id': (u'Indonesian', 1, '0'),
    'is': (u'Icelandic', 2, '(n != 1)'),
    'it': (u'Italian', 2, '(n != 1)'),
    'ja': (u'Japanese', 1, '0'),
    'jv': (u'Javanese', 2, '(n != 1)'),
    'ka': (u'Georgian', 1, '0'),
    'kk': (u'Kazakh', 1, '0'),
    'km': (u'Central Khmer', 1, '0'),
    'kn': (u'Kannada', 2, '(n != 1)'),
    'ko': (u'Korean', 1, '0'),
    'ku': (u'Kurdish', 2, '(n != 1)'),
    'kw': (u'Cornish', 4, '(n==1) ? 0 : (n==2) ? 1 : (n == 3) ? 2 : 3'),
    'ky': (u'Kirghiz; Kyrgyz', 1, '0'),
    'lb': (u'Luxembourgish; Letzeburgesch', 2, '(n != 1)'),
    'ln': (u'Lingala', 2, '(n > 1)'),
    'lo': (u'Lao', 1, '0'),
    'lt': (u'Lithuanian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'lv': (u'Latvian', 3, '(n%10==1 && n%100!=11 ? 0 : n != 0 ? 1 : 2)'),
    'mai': (u'Maithili', 2, '(n != 1)'),
    'mfe': (u'Morisyen', 2, '(n > 1)'),
    'mg': (u'Malagasy', 2, '(n > 1)'),
    'mi': (u'Maori', 2, '(n > 1)'),
    'mk': (u'Macedonian', 2, 'n==1 || n%10==1 ? 0 : 1'),
    'ml': (u'Malayalam', 2, '(n != 1)'),
    'mn': (u'Mongolian', 2, '(n != 1)'),
    'mr': (u'Marathi', 2, '(n != 1)'),
    'ms': (u'Malay', 1, '0'),
    'mt': (u'Maltese', 4,
           '(n==1 ? 0 : n==0 || ( n%100>1 && n%100<11) ? 1 : (n%100>10 && n%100<20 ) ? 2 : 3)'),
    'nah': (u'Nahuatl languages', 2, '(n != 1)'),
    'nap': (u'Neapolitan', 2, '(n != 1)'),
    'nb': (u'Bokml, Norwegian; Norwegian Bokml', 2, '(n != 1)'),
    'ne': (u'Nepali', 2, '(n != 1)'),
    'nl': (u'Dutch; Flemish', 2, '(n != 1)'),
    'nn': (u'Norwegian Nynorsk; Nynorsk, Norwegian', 2, '(n != 1)'),
    'nqo': (u"N'Ko", 2, '(n > 1)'),
    'nso': (u'Pedi; Sepedi; Northern Sotho', 2, '(n != 1)'),
    'oc': (u'Occitan (post 1500)', 2, '(n > 1)'),
    'or': (u'Oriya', 2, '(n != 1)'),
    'pa': (u'Panjabi; Punjabi', 2, '(n != 1)'),
    'pap': (u'Papiamento', 2, '(n != 1)'),
    'pl': (u'Polish', 3,
           '(n==1 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'pms': (u'Piemontese', 2, '(n != 1)'),
    'ps': (u'Pushto; Pashto', 2, '(n != 1)'),
    'pt': (u'Portuguese', 2, '(n != 1)'),
    'pt_BR': (u'Portuguese (Brazil)', 2, '(n != 1)'),
    'rm': (u'Romansh', 2, '(n != 1)'),
    'ro': (u'Romanian', 3, '(n==1 ? 0 : (n==0 || (n%100 > 0 && n%100 < 20)) ? 1 : 2);'),
    'ru': (u'Russian', 3,
          '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'sah': (u'Yakut', 1, '0'),
    'sco': (u'Scots', 2, '(n != 1)'),
    'si': (u'Sinhala; Sinhalese', 2, '(n != 1)'),
    'sk': (u'Slovak', 3, '(n==1) ? 0 : (n>=2 && n<=4) ? 1 : 2'),
    'sl': (u'Slovenian', 4, '(n%100==1 ? 0 : n%100==2 ? 1 : n%100==3 || n%100==4 ? 2 : 3)'),
    'so': (u'Somali', 2, '(n != 1)'),
    'son': (u'Songhai languages', 2, '(n != 1)'),
    'sq': (u'Albanian', 2, '(n != 1)'),
    'sr': (u'Serbian', 3,
           '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'st': (u'Sotho, Southern', 2, '(n != 1)'),
    'su': (u'Sundanese', 1, '0'),
    'sv': (u'Swedish', 2, '(n != 1)'),
    'sw': (u'Swahili', 2, '(n != 1)'),
    'ta': (u'Tamil', 2, '(n != 1)'),
    'te': (u'Telugu', 2, '(n != 1)'),
    'tg': (u'Tajik', 2, '(n != 1)'),
    'ti': (u'Tigrinya', 2, '(n > 1)'),
    'th': (u'Thai', 1, '0'),
    'tk': (u'Turkmen', 2, '(n != 1)'),
    'tr': (u'Turkish', 1, '0'),
    'tt': (u'Tatar', 1, '0'),
    'ug': (u'Uighur; Uyghur', 1, '0'),
    'uk': (u'Ukrainian', 3,
           '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
    'vi': (u'Vietnamese', 1, '0'),
    've': (u'Venda', 2, '(n != 1)'),
    'wa': (u'Walloon', 2, '(n > 1)'),
    'wo': (u'Wolof', 2, '(n != 1)'),
    'yo': (u'Yoruba', 2, '(n != 1)'),
    # Chinese is difficult because the main divide is on script, not really
    # country. Simplified Chinese is used mostly in China, Singapore and Malaysia.
    # Traditional Chinese is used mostly in Hong Kong, Taiwan and Macau.
    'zh_CN': (u'Chinese (China)', 1, '0'),
    'zh_HK': (u'Chinese (Hong Kong)', 1, '0'),
    'zh_TW': (u'Chinese (Taiwan)', 1, '0'),
    'zu': (u'Zulu', 2, '(n != 1)'),
}
"""Dictionary of language data.
The language code is the dictionary key (which may contain country codes
and modifiers).  The value is a tuple: (Full name in English from iso-codes,
nplurals, plural equation).

Note that the English names should not be used in user facing places - it
should always be passed through the function returned from tr_lang(), or at
least passed through _fix_language_name()."""

_fixed_names = {
    u"Asturian; Bable; Leonese; Asturleonese": u"Asturian",
    u"Bokml, Norwegian; Norwegian Bokml": u"Norwegian Bokml",
    u"Catalan; Valencian": u"Catalan",
    u"Central Khmer": u"Khmer",
    u"Chichewa; Chewa; Nyanja": u"Chewa; Nyanja",
    u"Divehi; Dhivehi; Maldivian": u"Divehi",
    u"Dutch; Flemish": u"Dutch",
    u"Filipino; Pilipino": u"Filipino",
    u"Gaelic; Scottish Gaelic": u"Scottish Gaelic",
    u"Greek, Modern (1453-)": u"Greek",
    u"Interlingua (International Auxiliary Language Association)": u"Interlingua",
    u"Kirghiz; Kyrgyz": u"Kirghiz",
    u"Klingon; tlhIngan-Hol": u"Klingon",
    u"Limburgan; Limburger; Limburgish": u"Limburgish",
    u"Low German; Low Saxon; German, Low; Saxon, Low": u"Low German",
    u"Luxembourgish; Letzeburgesch": u"Luxembourgish",
    u"Ndebele, South; South Ndebele": u"Southern Ndebele",
    u"Norwegian Nynorsk; Nynorsk, Norwegian": u"Norwegian Nynorsk",
    u"Occitan (post 1500)": u"Occitan",
    u"Panjabi; Punjabi": u"Punjabi",
    u"Pedi; Sepedi; Northern Sotho": u"Northern Sotho",
    u"Pushto; Pashto": u"Pashto",
    u"Sinhala; Sinhalese": u"Sinhala",
    u"Sotho, Southern": u"Sotho",
    u"Spanish; Castilian": u"Spanish",
    u"Uighur; Uyghur": u"Uyghur",
}


cldr_plural_categories = [
        'zero',
        'one',
        'two',
        'few',
        'many',
        'other',
]


def simplercode(code):
    """This attempts to simplify the given language code by ignoring country
    codes, for example.

    .. seealso::

       - http://www.rfc-editor.org/rfc/bcp/bcp47.txt
       - http://www.rfc-editor.org/rfc/rfc4646.txt
       - http://www.rfc-editor.org/rfc/rfc4647.txt
       - http://www.w3.org/International/articles/language-tags/
    """
    if not code:
        return code

    normalized = normalize_code(code)
    separator = normalized.rfind('-')
    if separator >= 0:
        return code[:separator]
    else:
        return ""


expansion_factors = {
        'af': 0.1,
        'ar': -0.09,
        'es': 0.21,
        'fr': 0.28,
        'it': 0.2,
}
"""Source to target string length expansion factors."""

import gettext
import locale
import os
import re


iso639 = {}
"""ISO 639 language codes"""
iso3166 = {}
"""ISO 3166 country codes"""

langcode_re = re.compile("^[a-z]{2,3}([_-][A-Z]{2,3}|)(@[a-zA-Z0-9]+|)$")
langcode_ire = re.compile("^[a-z]{2,3}([_-][a-z]{2,3})?(@[a-z0-9]+)?$",
                          re.IGNORECASE)
variant_re = re.compile("^[_-][A-Z]{2,3}(@[a-zA-Z0-9]+|)$")


def languagematch(languagecode, otherlanguagecode):
    """matches a languagecode to another, ignoring regions in the second"""
    if languagecode is None:
        return langcode_re.match(otherlanguagecode)
    return languagecode == otherlanguagecode or \
           (otherlanguagecode.startswith(languagecode) and
            variant_re.match(otherlanguagecode[len(languagecode):]))

dialect_name_re = re.compile(r"(.+)\s\(([^)\d]{,25})\)$")
# The limit of 25 characters on the country name is so that "Interlingua (...)"
# (see above) is correctly interpreted.


def tr_lang(langcode=None):
    """Gives a function that can translate a language name, even in the
    form ``"language (country)"``, into the language with iso code langcode,
    or the system language if no language is specified."""
    langfunc = gettext_lang(langcode)
    countryfunc = gettext_country(langcode)

    def handlelanguage(name):
        match = dialect_name_re.match(name)
        if match:
            language, country = match.groups()
            return u"%s (%s)" % (_fix_language_name(langfunc(language)),
                                 countryfunc(country))
        else:
            return _fix_language_name(langfunc(name))

    return handlelanguage


def _fix_language_name(name):
    """Identify and replace some unsightly names present in iso-codes.

    If the name is present in _fixed_names we assume it is untranslated and
    we replace it with a more usable rendering.  If the remaining part is long
    and includes a semi-colon, we only take the text up to the semi-colon to
    keep things neat."""
    if name in _fixed_names:
        return _fixed_names[name]
    elif len(name) > 11:
        # These constants are somewhat arbitrary, but testing with the Japanese
        # translation of ISO codes suggests these as the upper bounds.
        split_point = name[5:].find(u';')
        if split_point >= 0:
            return name[:5+split_point]
    return name


def gettext_lang(langcode=None):
    """Returns a gettext function to translate language names into the given
    language, or the system language if no language is specified."""
    if not langcode in iso639:
        if not langcode:
            langcode = ""
            if os.name == "nt":
                # On Windows the default locale is not used for some reason
                t = gettext.translation('iso_639',
                                        languages=[locale.getdefaultlocale()[0]],
                                        fallback=True)
            else:
                t = gettext.translation('iso_639', fallback=True)
        else:
            t = gettext.translation('iso_639', languages=[langcode],
                                    fallback=True)
        iso639[langcode] = t.ugettext
    return iso639[langcode]


def gettext_country(langcode=None):
    """Returns a gettext function to translate country names into the given
    language, or the system language if no language is specified."""
    if not langcode in iso3166:
        if not langcode:
            langcode = ""
            if os.name == "nt":
                # On Windows the default locale is not used for some reason
                t = gettext.translation('iso_3166',
                                        languages=[locale.getdefaultlocale()[0]],
                                        fallback=True)
            else:
                t = gettext.translation('iso_3166', fallback=True)
        else:
            t = gettext.translation('iso_3166', languages=[langcode],
                                    fallback=True)
        iso3166[langcode] = t.ugettext
    return iso3166[langcode]


def normalize(string, normal_form="NFC"):
    """Return a unicode string in its normalized form

       :param string: The string to be normalized
       :param normal_form: NFC (default), NFD, NFKC, NFKD
       :return: Normalized string
    """
    if string is None:
        return None
    else:
        import unicodedata
        return unicodedata.normalize(normal_form, string)


def forceunicode(string):
    """Ensures that the string is in unicode.

       :param string: A text string
       :type string: Unicode, String
       :return: String converted to Unicode and normalized as needed.
       :rtype: Unicode
    """
    if string is None:
        return None
    from translate.storage.placeables import StringElem
    if isinstance(string, str):
        encoding = getattr(string, "encoding", "utf-8")
        string = string.decode(encoding)
    elif isinstance(string, StringElem):
        string = unicode(string)
    return string


def normalized_unicode(string):
    """Forces the string to unicode and does normalization."""
    return normalize(forceunicode(string))


def normalize_code(code):
    if not code:
        return code
    return code.replace("_", "-").replace("@", "-").lower()


__normalised_languages = set(normalize_code(key) for key in languages.keys())


def simplify_to_common(language_code, languages=languages):
    """Simplify language code to the most commonly used form for the
    language, stripping country information for languages that tend
    not to be localized differently for different countries"""
    simpler = simplercode(language_code)
    if simpler == "":
        return language_code

    if (normalize_code(language_code) in __normalised_languages):
        return language_code
    else:
        return simplify_to_common(simpler)


def get_language(code):
    code = code.replace("-", "_").replace("@", "_").lower()
    if "_" in code:
        # convert ab_cd  ab_CD
        code = "%s_%s" % (code.split("_")[0], code.split("_", 1)[1].upper())
    return languages.get(code, None)

########NEW FILE########
__FILENAME__ = de
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the German language.

.. seealso:: http://en.wikipedia.org/wiki/German_language
"""

from translate.lang import common


class de(common.Common):
    """This class represents German."""

    ignoretests = ["simplecaps"]

########NEW FILE########
__FILENAME__ = dz
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Dzongkha language.

.. seealso:: http://en.wikipedia.org/wiki/Dzongkha_language
"""

from translate.lang import common


class dz(common.Common):
    """This class represents Dzongkha."""

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = el
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2009,2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Greek language.

.. seealso:: http://en.wikipedia.org/wiki/Greek_language
"""

import re

from translate.lang import common


class el(common.Common):
    """This class represents Greek."""

    # Greek uses ; as question mark and the middot instead
    sentenceend = u".!;"

    sentencere = re.compile(ur"""
        (?s)        # make . also match newlines
        .*?         # anything, but match non-greedy
        [%s]        # the puntuation for sentence ending
        \s+         # the spacing after the puntuation
        (?=[^a-z-\d])  # lookahead that next part starts with caps
        """ % sentenceend, re.VERBOSE | re.UNICODE)

    puncdict = {
        u"?": u";",
        u";": u"",
    }

    # Valid latin characters for use as accelerators
    valid_latin_accel = u"abcdefghijklmnopqrstuvwxyz" + \
                        u"ABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
                        u"1234567890"

    # Valid greek characters for use as accelerators (accented characters
    # and "" omitted)
    valid_greek_accel = u"" + \
                        u""

    # Valid accelerators
    validaccel = u"".join([valid_latin_accel, valid_greek_accel])

########NEW FILE########
__FILENAME__ = es
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Spanish language.

.. note:: As it only has special case code for initial inverted punctuation,
   it could also be used for Asturian, Galician, or Catalan.
"""

from translate.lang import common


class es(common.Common):
    """This class represents Spanish."""

    @classmethod
    def punctranslate(cls, text):
        """Implement some extra features for inverted punctuation.
        """
        text = super(cls, cls).punctranslate(text)
        # If the first sentence ends with ? or !, prepend inverted  or 
        firstmatch = cls.sentencere.match(text)
        if firstmatch is None:
            # only one sentence (if any) - use entire string
            first = text
        else:
            first = firstmatch.group()
        # remove trailing whitespace
        first = first.strip()
        # protect against incorrectly handling an empty string
        if not first:
            return text
        if first[-1] == '?':
            text = u"" + text
        elif first[-1] == '!':
            text = u"" + text
        return text

########NEW FILE########
__FILENAME__ = fa
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007, 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Persian language.

.. seealso:: http://en.wikipedia.org/wiki/Persian_language
"""

import re

from translate.lang import common


def guillemets(text):

    def convertquotation(match):
        prefix = match.group(1)
        # Let's see that we didn't perhaps match an XML tag property like
        # <a href="something">
        if prefix == u"=":
            return match.group(0)
        return u"%s%s" % (prefix, match.group(2))

    # Check that there is an even number of double quotes, otherwise it is
    # probably not safe to convert them.
    if text.count(u'"') % 2 == 0:
        text = re.sub('(.|^)"([^"]+)"', convertquotation, text)
    singlecount = text.count(u"'")
    if singlecount:
        if singlecount == text.count(u'`'):
            text = re.sub("(.|^)`([^']+)'", convertquotation, text)
        elif singlecount % 2 == 0:
            text = re.sub("(.|^)'([^']+)'", convertquotation, text)
    text = re.sub(u'(.|^)([^]+)', convertquotation, text)
    return text


class fa(common.Common):
    """This class represents Persian."""

    listseperator = u" "

    puncdict = {
        u",": u"",
        u";": u"",
        u"?": u"",
        #This causes problems with variables, so commented out for now:
        #u"%": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]
    #TODO: check persian numerics
    #TODO: zwj and zwnj?

    @classmethod
    def punctranslate(cls, text):
        """Implement "French" quotation marks."""
        text = super(cls, cls).punctranslate(text)
        return guillemets(text)

########NEW FILE########
__FILENAME__ = factory
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module provides a factory to instantiate language classes."""

from translate.lang import common, data


prefix = "code_"


def getlanguage(code):
    """This returns a language class.

    :param code: The ISO 639 language code
    """
    if code:
        code = code.replace("-", "_").replace("@", "_").lower()
    try:
        if code is None:
            raise ImportError("Can't determine language code")
        if code in ('or', 'is'):
            internal_code = prefix + code
        else:
            internal_code = code
        module = __import__("translate.lang.%s" % internal_code, globals(), {},
                            internal_code)
        langclass = getattr(module, internal_code)
        return langclass(code)
    except ImportError as e:
        simplercode = data.simplercode(code)
        if simplercode:
            relatedlanguage = getlanguage(simplercode)
            if isinstance(relatedlanguage, common.Common):
                relatedlanguage = relatedlanguage.__class__(code)
            return relatedlanguage
        else:
            return common.Common(code)

########NEW FILE########
__FILENAME__ = fi
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Finnish language.
"""

from translate.lang import common


class fi(common.Common):
    """This class represents Finnish."""

    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
                 u"1234567890" + \
                 u""

########NEW FILE########
__FILENAME__ = fr
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the French language.

.. seealso:: http://en.wikipedia.org/wiki/French_language
"""

import re

from translate.lang import common


def guillemets(text):

    def convertquotation(match):
        prefix = match.group(1)
        # Let's see that we didn't perhaps match an XML tag property like
        # <a href="something">
        if prefix == u"=":
            return match.group(0)
        return u"%s\u00a0%s\u00a0" % (prefix, match.group(2))  # \u00a0 is NBSP

    # Check that there is an even number of double quotes, otherwise it is
    # probably not safe to convert them.
    if text.count(u'"') % 2 == 0:
        text = re.sub('(.|^)"([^"]+)"', convertquotation, text)
    singlecount = text.count(u"'")
    if singlecount:
        if singlecount == text.count(u'`'):
            text = re.sub("(.|^)`([^']+)'", convertquotation, text)
        elif singlecount % 2 == 0:
            text = re.sub("(.|^)'([^']+)'", convertquotation, text)
    text = re.sub(u'(.|^)([^]+)', convertquotation, text)
    return text


class fr(common.Common):
    """This class represents French."""

    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
                 u"1234567890" + \
                 u""

    # According to http://french.about.com/library/writing/bl-punctuation.htm,
    # in French, a space is required both before and after all two- (or more)
    # part punctuation marks and symbols, including : ;   ! ? % $ # etc.
    puncdict = {}
    for c in u":;!?#":
        puncdict[c] = u"\u00a0%s" % c
    # TODO: consider adding % and $, but think about the consequences of how
    # they could be part of variables

    @classmethod
    def punctranslate(cls, text):
        """Implement some extra features for quotation marks.

        Known shortcomings:
            - % and $ are not touched yet for fear of variables
            - Double spaces might be introduced
        """
        text = super(cls, cls).punctranslate(text)
        # We might get problems where we got a space in URIs such as
        # http ://
        text = text.replace(u"\u00a0://", "://")
        return guillemets(text)

########NEW FILE########
__FILENAME__ = gd
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Gaelic language.

.. seealso:: http://en.wikipedia.org/wiki/Gaelic_language
"""

from translate.lang import common


class gd(common.Common):
    """This class represents Gaelic."""

    specialchars = u''

########NEW FILE########
__FILENAME__ = gu
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Gujarati language.

.. seealso:: http://en.wikipedia.org/wiki/Gujarati_language
"""

from translate.lang import common


class gu(common.Common):
    """This class represents Gujarati."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = he
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Hebrew language.

.. seealso:: http://en.wikipedia.org/wiki/Hebrew_language
"""

from translate.lang import common


class he(common.Common):
    """This class represents Hebrew."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = hi
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Hindi language.

.. seealso:: http://en.wikipedia.org/wiki/Hindi_language
"""

from translate.lang import common


class hi(common.Common):
    """This class represents Hindi."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = hy
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2008, 2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Armenian language.

.. seealso:: http://en.wikipedia.org/wiki/Armenian_language
"""

import re

from translate.lang import common


class hy(common.Common):
    """This class represents Armenian."""

    armenianpunc = u""

    punctuation = u"".join([common.Common.commonpunc, common.Common.quotes,
                            common.Common.miscpunc, armenianpunc])

    sentenceend = u""

    sentencere = re.compile(ur"""
        (?s)        # make . also match newlines
        .*?         # anything, but match non-greedy
        [%s]        # the puntuation for sentence ending
        \s+         # the spacing after the puntuation
        (?=[^a-z-\d])  # lookahead that next part starts with caps
        """ % sentenceend, re.VERBOSE | re.UNICODE)

    puncdict = {
        u".": u"",
        u":": u"",
        u"!": u"",
        u"?": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = identify
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
This module contains functions for identifying languages based on language
models.
"""

from os import extsep, path

from translate.lang.ngram import NGram
from translate.misc.file_discovery import get_abs_data_filename
from translate.storage.base import TranslationStore


class LanguageIdentifier(object):
    MODEL_DIR = get_abs_data_filename('langmodels')
    """The directory containing the ngram language model files."""
    CONF_FILE = 'fpdb.conf'
    """
    The name of the file that contains language name-code pairs
    (relative to ``MODEL_DIR``).
    """

    def __init__(self, model_dir=None, conf_file=None):
        if model_dir is None:
            model_dir = self.MODEL_DIR
        if not path.isdir(model_dir):
            raise ValueError('Directory does not exist: %s' % (model_dir))

        if conf_file is None:
            conf_file = self.CONF_FILE
        conf_file = path.abspath(path.join(model_dir, conf_file))
        if not path.isfile(conf_file):
            raise ValueError('File does not exist: %s' % (conf_file))

        self._lang_codes = {}
        self._load_config(conf_file)
        self.ngram = NGram(model_dir)

    def _load_config(self, conf_file):
        """Load the mapping of language names to language codes as given in the
            configuration file."""
        lines = open(conf_file).read().splitlines()
        for line in lines:
            parts = line.split()
            if not parts or line.startswith('#'):
                continue  # Skip comment- and empty lines
            lname, lcode = parts[0], parts[1]

            # Make sure lname is not prefixed by directory names
            lname = path.split(lname)[-1]
            if extsep in lname:
                lname = lname[:lname.rindex(extsep)]  # Remove extension if it has

            # Remove trailing '[_-]-utf8' from code
            if lcode.endswith('-utf8'):
                lcode = lcode[:-len('-utf8')]
            if lcode.endswith('-') or lcode.endswith('_'):
                lcode = lcode[:-1]

            self._lang_codes[lname] = lcode

    def identify_lang(self, text):
        """Identify the language of the text in the given string."""
        if not text:
            return None
        result = self.ngram.classify(text)
        if result in self._lang_codes:
            result = self._lang_codes[result]
        return result

    def identify_source_lang(self, instore):
        """Identify the source language of the given translation store or
            units.

            :type  instore: ``TranslationStore`` or list or tuple of
                ``TranslationUnit``s.
            :param instore: The translation store to extract source text from.
            :returns: The identified language's code or ``None`` if the language
                could not be identified."""
        if not isinstance(instore, (TranslationStore, list, tuple)):
            return None

        text = u' '.join(unit.source for unit in instore[:50] if unit.istranslatable() and unit.source)
        if not text:
            return None
        return self.identify_lang(text)

    def identify_target_lang(self, instore):
        """Identify the target language of the given translation store or
            units.

            :type  instore: ``TranslationStore`` or list or tuple of
                ``TranslationUnit``s.
            :param instore: The translation store to extract target text from.
            :returns: The identified language's code or ``None`` if the language
                could not be identified."""
        if not isinstance(instore, (TranslationStore, list, tuple)):
            return None

        text = u' '.join(unit.target for unit in instore[:200] if unit.istranslatable() and unit.target)
        if not text:
            return None
        return self.identify_lang(text)

if __name__ == "__main__":
    from sys import argv
    script_dir = path.abspath(path.dirname(argv[0]))
    identifier = LanguageIdentifier()
    import locale
    encoding = locale.getpreferredencoding()
    text = file(argv[1]).read().decode(encoding)
    print("Language detected:", identifier.identify_lang(text))

########NEW FILE########
__FILENAME__ = ja
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Japanese language.

.. seealso:: http://en.wikipedia.org/wiki/Japanese_language
"""

import re

from translate.lang import common


class ja(common.Common):
    """This class represents Japanese."""

    listseperator = u""

    sentenceend = u"!?"

    # Compared to common.py, we make the space after the sentence ending
    # optional and don't demand an uppercase letter to follow.
    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #any text, but match non-greedy
                            [%s]        #the puntuation for sentence ending
                            \s*         #the optional space after the puntuation
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u". ": u"",
        u", ": u"",
        u".\n": u"\n",
        u",\n": u"\n",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = km
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Khmer language.

.. seealso:: http://en.wikipedia.org/wiki/Khmer_language
"""

import re

from translate.lang import common


class km(common.Common):
    """This class represents Khmer."""

    khmerpunc = u""
    """These marks are only used for Khmer."""

    punctuation = u"".join([common.Common.commonpunc, common.Common.quotes,
                            common.Common.miscpunc, khmerpunc])

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #anything, but match non-greedy
                            [%s]        #the puntuation for sentence ending
                            \s+         #the spacing after the puntuation
                            (?=[^a-z\d])#lookahead that next part starts with caps
                            """ % sentenceend, re.VERBOSE)
    #\u00a0 is non-breaking space
    puncdict = {
        u".": u"\u00a0",
        u":": u"\u00a0",
        u"!": u"\u00a0!",
        u"?": u"\u00a0?",
    }

    ignoretests = ["startcaps", "simplecaps"]

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = kn
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Kannada language.

.. seealso:: http://en.wikipedia.org/wiki/Kannada_language
"""

from translate.lang import common


class kn(common.Common):
    """This class represents Kannada."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ko
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Korean language.

.. seealso:: http://en.wikipedia.org/wiki/Korean_language
"""

from translate.lang import common


class ko(common.Common):
    """This class represents Korean."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = kw
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Cornish language.

.. seealso:: http://en.wikipedia.org/wiki/Cornish_language
"""

from translate.lang import common


class kw(common.Common):
    """This class represents Cornish."""

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = lo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Lao language.

.. seealso:: http://en.wikipedia.org/wiki/Lao_language
"""

from translate.lang import common


class lo(common.Common):
    """This class represents Lao."""

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = ml
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Malayalam language.

.. seealso:: http://en.wikipedia.org/wiki/Malayalam_language
"""

from translate.lang import common


class ml(common.Common):
    """This class represents Malayalam."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = mr
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Marathi language.

.. seealso:: http://en.wikipedia.org/wiki/Marathi_language
"""

from translate.lang import common


class mr(common.Common):
    """This class represents Marathi."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ms
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Malay language.

.. seealso:: http://en.wikipedia.org/wiki/Malay_language
"""

from translate.lang import common


class ms(common.Common):
    """This class represents Malay."""

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = my
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Burmese language.

.. seealso:: http://en.wikipedia.org/wiki/Burmese_language
"""

from translate.lang import common


class my(common.Common):
    """This class represents Burmese."""

    puncdict = {
        u".": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ne
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Nepali language.

.. seealso:: http://en.wikipedia.org/wiki/Nepali_language
"""

import re

from translate.lang import common


class ne(common.Common):
    """This class represents Nepali."""

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    #make . also match newlines
                            .*?         #anything, but match non-greedy
                            \s?         #the single space before the punctuation
                            [%s]        #the puntuation for sentence ending
                            \s+         #the spacing after the puntuation
                            (?=[^a-z\d])#lookahead that next part starts with caps
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u".": u" ",
        u"?": u" ?",
    }

    ignoretests = ["startcaps", "simplecaps", "accelerators"]

########NEW FILE########
__FILENAME__ = ngram
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (c) 2006 Thomas Mangin
# Copyright (c) 2009-2010 Zuza Software Foundation
#
# This program is distributed under Gnu General Public License
# (cf. the file COPYING in distribution). Alternatively, you can use
# the program under the conditions of the Artistic License (as Perl).
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Ngram models for language guessing.

.. note:: Orignal code from http://thomas.mangin.me.uk/data/source/ngram.py
"""

import glob
import re
import sys
from os import path


nb_ngrams = 400
white_space_re = re.compile('\s+')


class _NGram:

    def __init__(self, arg=None):
        if isinstance(arg, basestring):
            self.addText(arg)
            self.normalise()
        elif isinstance(arg, dict):
            # This must already be normalised!
            self.ngrams = arg
        else:
            self.ngrams = dict()

    def addText(self, text):
        if isinstance(text, str):
            text = text.decode('utf-8')

        ngrams = dict()

        for word in white_space_re.split(text):
            word = '_%s_' % word
            size = len(word)
            for i in xrange(size - 1):
                for s in (1, 2, 3, 4):
                    end = i + s
                    if end >= size:
                        break
                    sub = word[i:end]

                    if not sub in ngrams:
                        ngrams[sub] = 0
                    ngrams[sub] += 1

        self.ngrams = ngrams
        return self

    def sorted_by_score(self):
        sorted = [(self.ngrams[k], k) for k in self.ngrams]
        sorted.sort()
        sorted.reverse()
        sorted = sorted[:nb_ngrams]
        return sorted

    def normalise(self):
        ngrams = {}
        for count, (v, k) in enumerate(self.sorted_by_score()):
            ngrams[k] = count

        self.ngrams = ngrams
        return self

    def addValues(self, key, value):
        self.ngrams[key] = value
        return self

    def compare(self, ngram):
        d = 0
        ngrams = ngram.ngrams
        for k in self.ngrams:
            if k in ngrams:
                d += abs(ngrams[k] - self.ngrams[k])
            else:
                d += nb_ngrams
        return d


class NGram:

    def __init__(self, folder, ext='.lm'):
        self.ngrams = dict()
        folder = path.join(folder, '*' + ext)
        size = len(ext)

        for fname in glob.glob(path.normcase(folder)):
            lang = path.split(fname)[-1][:-size]
            ngrams = {}
            try:
                f = open(fname, 'r')
                lines = f.read().decode('utf-8').splitlines()
                try:
                    for i, line in enumerate(lines):
                        ngram, _t, _f = line.partition(u'\t')
                        ngrams[ngram] = i
                except AttributeError as e:
                    # Python2.4 doesn't have unicode.partition()
                    for i, line in enumerate(lines):
                        ngram = line.split(u'\t')[0]
                        ngrams[ngram] = i
            except UnicodeDecodeError as e:
                continue

            if ngrams:
                self.ngrams[lang] = _NGram(ngrams)

        if not self.ngrams:
            raise ValueError("no language files found")

    def classify(self, text):
        ngram = _NGram(text)
        r = 'guess'

        min = sys.maxint

        for lang in self.ngrams:
            d = self.ngrams[lang].compare(ngram)
            if d < min:
                min = d
                r = lang

        if min > 0.8 * (nb_ngrams ** 2):
            r = ''
        return r


class Generate:

    def __init__(self, folder, ext='.txt'):
        self.ngrams = dict()
        folder = path.join(folder, '*' + ext)
        size = len(ext)

        for fname in glob.glob(path.normcase(folder)):
            lang = path.split(fname)[-1][:-size]
            n = _NGram()

            file = open(fname, 'r')
            for line in file.readlines():
                n.addText(line)
            file.close()

            n.normalise()
            self.ngrams[lang] = n

    def save(self, folder, ext='.lm'):
        for lang in self.ngrams.keys():
            fname = path.join(folder, lang + ext)
            file = open(fname, 'w')
            for v, k in self.ngrams[lang].sorted_by_score():
                file.write("%s\t %d\n" % (k, v))
            file.close()

if __name__ == '__main__':
    import sys

    # Should you want to generate your own .lm files
    #conf = Generate('/tmp')
    #conf.save('/tmp')

    text = sys.stdin.readline()
    from translate.misc.file_discovery import get_abs_data_filename
    l = NGram(get_abs_data_filename('langmodels'))
    print(l.classify(text))

########NEW FILE########
__FILENAME__ = nqo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the N'Ko language.

.. seealso:: http://en.wikipedia.org/wiki/N'Ko_language
"""

import re

from translate.lang import common


def reverse_quotes(text):
    def convertquotation(match):
        return u"%s" % match.group(1)
    return re.sub(u'([^]+)', convertquotation, text)


class nqo(common.Common):
    """This class represents N'Ko."""

    listseperator = u" "

    puncdict = {
        u",": u"",
        u";": u"",
        u"?": u"",
        u"!": u"",
    }

    ignoretests = ["startcaps", "simplecaps", "acronyms"]

    @classmethod
    def punctranslate(cls, text):
        text = super(cls, cls).punctranslate(text)
        return reverse_quotes(text)

########NEW FILE########
__FILENAME__ = nso
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Northern Sotho language.

.. seealso:: http://en.wikipedia.org/wiki/Northern_Sotho_language
"""

from translate.lang import common


class nso(common.Common):
    """This class represents Northern Sotho."""

    specialchars = ""

########NEW FILE########
__FILENAME__ = pa
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Punjabi language.

.. seealso:: http://en.wikipedia.org/wiki/Punjabi_language
"""

import re

from translate.lang import common


class pa(common.Common):
    """This class represents Punjabi."""

    sentenceend = u"!?"

    sentencere = re.compile(r"""(?s)    # make . also match newlines
                            .*?         # anything, but match non-greedy
                            [%s]        # the puntuation for sentence ending
                            \s+         # the spacing after the puntuation
                            (?=[^a-z\d])# lookahead that next part starts with
                                        # caps
                            """ % sentenceend, re.VERBOSE)

    puncdict = {
        u". ": u" ",
        u".\n": u"\n",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = poedit
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Functions to manage Poedit's language features.

.. note:: The ISO 639 maps are from Poedit's
   `isocode.cpp
   <https://github.com/vslavik/poedit/blob/v1.4.2/src/isocodes.cpp#L36-227>`_
   (v1.4.2) to ensure that we match currently released versions of Poedit.
"""

lang_codes = {
    "aa": "Afar",
    "ab": "Abkhazian",
    "ae": "Avestan",
    "af": "Afrikaans",
    "am": "Amharic",
    "ar": "Arabic",
    "as": "Assamese",
    "ay": "Aymara",
    "az": "Azerbaijani",
    "ba": "Bashkir",
    "be": "Belarusian",
    "bg": "Bulgarian",
    "bh": "Bihari",
    "bi": "Bislama",
    "bn": "Bengali",
    "bo": "Tibetan",
    "br": "Breton",
    "bs": "Bosnian",
    "ca": "Catalan",
    "ce": "Chechen",
    "ch": "Chamorro",
    "co": "Corsican",
    "cs": "Czech",
    "cu": "Church Slavic",
    "cv": "Chuvash",
    "cy": "Welsh",
    "da": "Danish",
    "de": "German",
    "dz": "Dzongkha",
    "el": "Greek",
    "en": "English",
    "eo": "Esperanto",
    "es": "Spanish",
    "et": "Estonian",
    "eu": "Basque",
    "fa": "Persian",
    "fi": "Finnish",
    "fj": "Fijian",
    "fo": "Faroese",
    "fr": "French",
    "fur": "Friulian",
    "fy": "Frisian",
    "ga": "Irish",
    "gd": "Gaelic",
    "gl": "Galician",
    "gn": "Guarani",
    "gu": "Gujarati",
    "ha": "Hausa",
    "he": "Hebrew",
    "hi": "Hindi",
    "ho": "Hiri Motu",
    "hr": "Croatian",
    "hu": "Hungarian",
    "hy": "Armenian",
    "hz": "Herero",
    "ia": "Interlingua",
    "id": "Indonesian",
    "ie": "Interlingue",
    "ik": "Inupiaq",
    "is": "Icelandic",
    "it": "Italian",
    "iu": "Inuktitut",
    "ja": "Japanese",
    "jw": "Javanese",
    "ka": "Georgian",
    "ki": "Kikuyu",
    "kj": "Kuanyama",
    "kk": "Kazakh",
    "kl": "Kalaallisut",
    "km": "Khmer",
    "kn": "Kannada",
    "ko": "Korean",
    "ks": "Kashmiri",
    "ku": "Kurdish",
    "kv": "Komi",
    "kw": "Cornish",
    "ky": "Kyrgyz",
    "la": "Latin",
    "lb": "Letzeburgesch",
    "ln": "Lingala",
    "lo": "Lao",
    "lt": "Lithuanian",
    "lv": "Latvian",
    "mg": "Malagasy",
    "mh": "Marshall",
    "mi": "Maori",
    "mk": "Macedonian",
    "ml": "Malayalam",
    "mn": "Mongolian",
    "mo": "Moldavian",
    "mr": "Marathi",
    "ms": "Malay",
    "mt": "Maltese",
    "my": "Burmese",
    "na": "Nauru",
    "ne": "Nepali",
    "ng": "Ndonga",
    "nl": "Dutch",
    "nn": "Norwegian Nynorsk",
    "nb": "Norwegian Bokmal",
    "nr": "Ndebele, South",
    "nv": "Navajo",
    "ny": "Chichewa; Nyanja",
    "oc": "Occitan",
    "om": "(Afan) Oromo",
    "or": "Oriya",
    "os": "Ossetian; Ossetic",
    "pa": "Panjabi",
    "pi": "Pali",
    "pl": "Polish",
    "ps": "Pashto, Pushto",
    "pt": "Portuguese",
    "qu": "Quechua",
    "rm": "Rhaeto-Romance",
    "rn": "Rundi",
    "ro": "Romanian",
    "ru": "Russian",
    "rw": "Kinyarwanda",
    "sa": "Sanskrit",
    "sc": "Sardinian",
    "sd": "Sindhi",
    "se": "Northern Sami",
    "sg": "Sangro",
    "sh": "Serbo-Croatian",
    "si": "Sinhalese",
    "sk": "Slovak",
    "sl": "Slovenian",
    "sm": "Samoan",
    "sn": "Shona",
    "so": "Somali",
    "sq": "Albanian",
    "sr": "Serbian",
    "ss": "Siswati",
    "st": "Sesotho",
    "su": "Sundanese",
    "sv": "Swedish",
    "sw": "Swahili",
    "ta": "Tamil",
    "te": "Telugu",
    "tg": "Tajik",
    "th": "Thai",
    "ti": "Tigrinya",
    "tk": "Turkmen",
    "tl": "Tagalog",
    "tn": "Setswana",
    "to": "Tonga",
    "tr": "Turkish",
    "ts": "Tsonga",
    "tt": "Tatar",
    "tw": "Twi",
    "ty": "Tahitian",
    "ug": "Uighur",
    "uk": "Ukrainian",
    "ur": "Urdu",
    "uz": "Uzbek",
    "vi": "Vietnamese",
    "vo": "Volapuk",
    "wa": "Walloon",
    "wo": "Wolof",
    "xh": "Xhosa",
    "yi": "Yiddish",
    "yo": "Yoruba",
    "za": "Zhuang",
    "zh": "Chinese",
    "zu": "Zulu",
}
"""ISO369 codes and names as used by Poedit.
Mostly these are identical to ISO 639, but there are some differences."""

lang_names = dict([(value, key) for (key, value) in lang_codes.items()])
"""Reversed :data:`lang_codes`"""

dialects = {
  "Portuguese": {"PORTUGAL": "pt", "BRAZIL": "pt_BR", "None": "pt"},
  # We choose not to subtype en_US
  "English": {
      "UNITED KINGDOM": "en_GB",
      "SOUTH AFRICA": "en_ZA",
      "None": "en",
  },
  # zh_CN = Simplified, zh_TW = Traditional
  "Chinese": {"CHINA": "zh_CN", "TAIWAN": "zh_TW", "None": "zh_CN"},
}
"""Language dialects based on ISO 3166 country names, 'None' is the
default fallback"""


def isocode(language, country=None):
    """Returns a language code for the given Poedit language name.

    Poedit uses language and country names in the PO header entries:

    - X-Poedit-Language
    - X-Poedit-Country

    This function converts the supplied language name into the required ISO 639
    code. If needed, in the case of :data:`dialects`, the country name is used
    to create an xx_YY style dialect code.

    :param language: Language name
    :type language: String
    :param country: Country name
    :type country: String
    :return: ISO 639 language code
    :rtype: String
    """
    dialect = dialects.get(language, None)
    if dialect:
        return dialect.get(country, dialect["None"])
    return lang_names.get(language, None)

########NEW FILE########
__FILENAME__ = si
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Sinhala language.

.. seealso:: http://en.wikipedia.org/wiki/Sinhala_language
"""

from translate.lang import common


class si(common.Common):
    """This class represents Sinhala."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = son
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Songhai languages.

.. seealso:: http://en.wikipedia.org/wiki/Songhai_languages
"""

from translate.lang import common


class son(common.Common):
    """This class represents Songhai."""

    specialchars = u""

########NEW FILE########
__FILENAME__ = st
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Southern Sotho language.
"""

from translate.lang import common


class st(common.Common):
    """This class represents Southern Sotho."""

    validdoublewords = [u"o", u"le", u"ba"]

########NEW FILE########
__FILENAME__ = su
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Sundanese language.

.. seealso:: http://en.wikipedia.org/wiki/Sundanese_language
"""

from translate.lang import common


class su(common.Common):
    """This class represents Sundanese."""

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = sv
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the the Swedish language.

.. seealso:: http://en.wikipedia.org/wiki/Swedish_language
"""

from translate.lang import common


class sv(common.Common):
    """This class represents Swedish."""

    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
                 u"1234567890" + \
                 u""

########NEW FILE########
__FILENAME__ = ta
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of Virtaal.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Tamil language.

.. seealso:: http://en.wikipedia.org/wiki/Tamil_language
"""

from translate.lang import common


class ta(common.Common):
    """This class represents Tamil."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = te
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Telugu language.

.. seealso:: http://en.wikipedia.org/wiki/Telugu_language
"""

from translate.lang import common


class te(common.Common):
    """This class represents Telugu."""

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = team
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module to guess the language ISO code based on the 'Language-Team' entry in
the header of a Gettext PO file.
"""

import re


__all__ = ['LANG_TEAM_CONTACT_SNIPPETS', 'guess_language']

LANG_TEAM_REGEX = (
   ("@li.org", "([a-z_A-Z]{2,})@li.org", ["LL", "XX", "TEAM"]),
   ("translation-team",
    "translation-team-([a-z_A-Z]+)@lists.sourceforge.net", None),
   ("fedora-trans", "fedora-trans-([a-z_A-Z]+)@redhat.com", ["list"]),
   ("ubuntu-l10n", "ubuntu-l10n-([a-z_A-Z]+)@lists.ubuntu.com", None),
   ("translate-discuss",
    "translate-discuss-([a-z_A-Z]+)@lists.sourceforge.net", None),
   ("kde-i18n", "kde-i18n-([a-z_A-Z]+)@(?:lists\.|mail\.|)kde.org", ["doc"]),
   ("kde-l10n", "kde-l10n-([a-z_A-Z]+)@kde.org", None),
   ("fedoraproject", "trans-([a-z_A-Z]+)@lists.fedoraproject.org", None),
   ("gnome.org", "gnome-([a-z_A-Z]+)-list@gnome.org", ["latin"]),
)
"""Data for regular expression based extraction.  The fieds are: prefilter
information, regex with single group that contains the language code,
postfilter."""

LANG_TEAM_CONTACT_SNIPPETS = {
    "af": ("i18n@af.org.za", "Petri Jooste",),
    "am": ("@geez.org", ),
    "ar": ("arabeyes.org", "Arabeyes", ),
    "as": ("assam@mm.assam-glug.org", ),
    "ast": ("@softastur.org", "launchpad.net/~ubuntu-l10n-ast",
            "softast-xeneral@lists.sourceforge.net", "Softastur",),
    "az": ("linuxaz@azerimal.net", "gnome@azitt.com", u"gnome@aztt.com",),
    "az_IR": ("az-ir@lists.sharif.edu",),
    "be": ("i18n@mova.org", "i18n@tut.by", "mozilla_byx@poczta.fm",),
    "be@latin": ("translation-team-be-latin@lists", "be-latin.open-tran.eu",),
    "bg": ("dict@fsa-bg.org", "dict@linux.zonebg.com", ),
    "bn": ("gnome-translation@bengalinux.org", "core@bengalinux.org",
           "ankur-bd-l10n@googlegroups.com",
           "redhat-translation@bengalinux.org", ),
    "bn_IN": ("anubad@lists.ankur.org.in", ),
    "br": ("drouizig@drouizig.org", "brenux@free.fr",
           "tradgnome@softcatala.net", "fedora@softcatala.org", ),
    "bs": ("lokal@linux.org.ba", "lokal@lugbih.org", ),
    "ca": ("@softcatala.org",),
    "crh": ("tilde-birlik-tercime@lists.sourceforge.net", ),
    "cs": ("fedora-cs-list@redhat.com", "cs-users@lists.fedoraproject.org",
           "debian-l10n-czech@lists.debian.org",
           "kde-czech-apps@lists.sourceforge.net",
           "kde-czech-apps@lists.sf.net", "translations.cs@gnupg.cz"),
    "cy": ("gnome-cy@lists.linux.org.uk", "gnome-cy@pengwyn.linux.org.uk",
           "gnome-cy@www.linux.org", "gnome-cy@www.linux.org.uk",
           "cy@pengwyn.linux.org.uk", ),
    "da": ("dansk@dansk-gruppen.dk", "dansk@klid.dk",
           "sslug-locale@sslug.dk", ),
    "de": ("gnome-de@gnome.org", "debian-l10n-german@lists.debian.org", ),
    "dz": ("pgeyleg@dit.gov.bt", "pgyeleg@dit.gov.bt", ),
    "el": ("debian-l10n-greek@lists.debian.org", "i18ngr@lists.hellug.gr",
           "i18n@hellug.gr", "nls@tux.hellug.gr", "team@gnome.gr",
           "team@lists.gnome.gr", "users@el.openoffice.org", ),
    "en_AU": ("trans@six-by-nine.com.au", ),
    "en_CA": ("adamw@gnome.org", "adamw@freebsd.org", ),
    "en_GB": ("kde-en-gb@kde.me.uk", ),
    "en@shaw": ("ubuntu-l10n-en-shaw@launchpad.net",
                "ubuntu-l10n-en-shaw@lists.launchpad.net", ),
    "eo": ("eo-tradukado@lists.tuxfamily.org",
           "debian-l10n-esperanto@lists.debian.org",
           "ubuntu-l10n-eo@lists.launchpad.net",
           "eo-tradukado.tuxfamily.org", ),
    "es": ("pgsql-es-ayuda@postgresql.org",
           "debian-l10n-spanish@lists.debian.org",
           "gnome-es@gnome.org", "traductores@es.gnome.org", ),
    "et": ("gnome-et@linux.ee", "kde-et@linux.ee", "linux-ee@lists.eenet.ee",
           "linux-et@lists.eenet.ee", "et-gnome@linux.ee",
           "linux-ee@eenet.ee", ),
    "eu": ("debian-l10n-basque@lists.debian.org",
           "debian-l10n-eu@lists.debian.org", "itzulpena@euskalgnu.org",
           "gnome@euskalgnu.org", "librezale@librezale.org",
           "linux-eu@chanae.alphanet.ch", ),
    "fa": ("farsi@lists.sharif.edu", "Farsiweb.info", ),
    "fi": ("debian-l10n-finnish@lists.debian.org",
           "gnome-fi-laatu@lists.sourceforge.net", "laatu@lokalisointi.org",
           "lokalisointi-laatu@linux-aktivaattori.org", "laatu@gnome.fi",
           "yast-trans-fi@kotoistaminen.novell.fi", ),
    "fr": ("debian-l10n-french@lists.debian.org", "gnomefr@traduc.org",
           "kde-francophone@kde.org", "traduc@traduc.org",
           "pgsql-fr-generale@postgresql.org", "rpm-fr@livna.org", ),
    "ga": ("gaeilge-gnulinux@lists.sourceforge.net",
           "gaeilge-a@listserv.heanet.ie", ),
    "gl": ("trasno@ceu.fi.udc.es", "gnome@g11n.net",
           "gpul-traduccion@ceu.fi.udc.es", "proxecto@trasno.net",
           "trasno@gpul.org", ),
    "gu": ("indianoss-gujarati@lists.sourceforge.net", ),
    "he": ("debian-hebrew-common@lists.alioth.debian.org",
           "kde-il@yahoogroups.com", "fedora-he-list@redhat.com",
           "mdk-hebrew@iglu.org.il", ),
    "hi": ("indlinux-hindi-gnome@lists.sourceforge.net",
           "indlinux-hindi@lists.sourceforge.net", ),
    "hr": ("translator-shop.org", "lokalizacija@linux.hr", ),
    "hu": ("debian-l10n-hungarian@lists.debian.org", "gnome@fsf.hu",
           "gnome@gnome.hu", "magyar@lists.linux.hu", ),
    "id": ("@id.gnome.org", "@gnome.linux.or.id", "mdk-id@yahoogroups.com",
           "linux.or.id", "gnome@i15n.org"),
    "io": ("gnome-ido@lists.mterry.name", ),
    "is": ("gnome@techattack.nu", "kde-isl@mmedia.is", "kde-isl@molar.is", ),
    "it": ("debian-l10n-italian@lists.debian.org", "traduzioni@itpug.org",
           "fedora-trans-it@redhat.com", "tp@lists.linux.it", ),
    "ja": ("debian-doc@debian.or.jp", "debian-japanese@lists.debian.org",
           "gnome-translation@gnome.gr.jp", "translation@gnome.gr.jp",
           "jpug-doc@ml.postgresql.jp", ),
    "ka": ("geognome@googlegroups.com",
           "Ubuntu-Georgian-Translators@googlegroups.com", ),
    "kk": ("kk_KZ@googlegroups.com", ),
    "km": ("@khmeros.info", ),
    "kn": ("debian-l10n-kannada@lists.debian.org", ),
    "ko": ("gnome-kr-hackers@list.kldp.net", "gnome-kr-hackers@lists.kldp.net",
           "gnome-kr-translation@lists.kldp.net", "pgsql-kr@postgresql.or.kr",
           "hangul-hackers@lists.kldp.net",
           "debian-l10n-korean@lists.debian.org",
           "gnome-kr-translation@lists.sourceforge.net", ),
    "ks": ("ks-gnome-trans-commits@lists.code.indlinux.net", ),
    "ku": ("gnu-ku-wergerandin@lists.sourceforge.net", ),
    "ky": ("i18n-team-ky-kyrgyz@lists.sourceforge.net", "ky-li@mail.ru", ),
    "la": ("gnome-latin-list@gnome.org", ),
    "li": ("li@gnome.org", ),
    "lt": ("gimp-lt@lists.akl.lt", "gnome-lt@lists.akl.lt",
           "gnome-lt@lists.gnome.org", "komp_lt@konferencijos.lt", ),
    "lv": ("lata-l10n@googlegroups.com", "lata-i18n@groups.google.com",
           "locale@laka.lv", "ll10nt@os.lv", ),
    "mai": ("maithili.sf.net", ),
    "mg": ("i18n-malagasy-gnome@gnome.org", ),
    "mi": ("maori@nzlinux.org.nz", ),
    "mk": ("gnomk-main@lists.sourceforge.net", "lug@lists.linux.net.mk",
           "mkde-l10n@lists.sourceforge.net",
           "ossm-members@hedona.on.net.mk", ),
    "ml": ("smc-discuss@googlegroups.com", ),
    "mn": ("openmn-", "openmn.org", ),
    "ms": ("gabai-penyumbang@lists.sourceforge.net",
           "gabai-penyumbang@lists.sf.net", "kedidiemas@yahoogroups.com", ),
    "nb": ("i18n-nb@lister.ping.uio.no", ),
    "nds": ("nds-lowgerman@lists.sourceforge.net", ),
    "ne": ("info@mpp.org.np", ),
    "nl": ("debian-l10n-dutch@lists.debian.org", "vertaling@nl.gnome.org",
           "vertaling@vrijschrift.org", "nl@vrijschrift.org",
           "vertaling@nl.linux.org", "vertaling@nl.li.org", ),
    "nn": ("i18n-nn@lister.ping.uio.no", ),
    "nso": ("sepedi@translate.org.za", ),
    "or": ("oriya-group@lists.sarovar.org", "oriya-it@googlegroups.com", ),
    "pa": ("punjabi-l10n@users.sf.net", "fedora-pa-list@redhat.com",
           "punjabi-users@lists.sf.net", "punjabi-l10n@lists.sourceforge.net",
           "punlinux-i18n@lists.sourceforge.net", ),
    "pl": ("gnomepl@aviary.pl", "debian-l10n-polish@lists.debian.org",
           "gnome-l10n@lists.aviary.pl", "translators@gnomepl.org", ),
    "ps": ("pathanisation@googelgroups.com", ),
    "pt": ("fedora-trans-pt@redhat.org", "gnome_pt@yahoogroups.com",
           "traduz@debianpt.org", "traduz@debian.pt", ),
    "pt_BR": ("gnome-l10n-br@listas.cipsga.org.br",
              "gnome-pt_br-list@gnome.org", "fedora-docs-br@redhat.com",
              "fedora-trans-pt-br@redhat.com", "ldp-br@bazar.conectiva.com.br",
              "pgbr-dev@postgresql.org.br",
              "pgbr-dev@listas.postgresql.org.br",
              "debian-l10n-portuguese@lists.debian.org", ),
    "ro": ("fedora-ro@googlegroups.com", "gnomero-list@lists.sourceforge.net",
           "debian-l10n-romanian@lists.debian.org", ),
    "ru": ("pgsql-rus@yahoogroups.com", "debian-l10n-russian@lists.debian.org",
           "gnupg-ru@gnupg.org", ),
    "sk": ("sk-i18n@lists.linux.sk", "kde-sk@linux.sk", ),
    "sl": ("gnome-si@googlegroups.com", ),
    "sq": ("gnome-albanian-perkthyesit@lists.sourceforge.net",
           "debian-l10n-albanian@lists.debian.org", ),
    "sr": ("@prevod.org", "serbiangnome-lista@nongnu.org", ),
    "sv": ("debian-l10n-swedish@lists.debian.org", "tp-sv@listor.tp-sv.se", ),
    "ta": ("gnome-tamil-translation@googlegroups.com",
           "tamilinix@yahoogroups.com", "Ubuntu-l10n-tam@lists.ubuntu.com",
           "tamil-DI@yahoogroups.com", ),
    "te": ("localisation@swecha.org",
           "indlinux-telugu@lists.sourceforge.net", ),
    "th": ("l10n@opentle.org", "thai-l10n@googlegroup.com",
           "thailang@buraphalinux.org", "thai-l10n@googlegroups.com",
           "l10n.opentle.org", ),
    "tk": ("kakilikgroup@yahoo.com", ),
    "tl": ("debian-tl@banwa.upm.edu.ph", ),
    "tr": ("debian-l10n-turkish@lists.debian.org", "gnome-turk@gnome.org",
           "gnu-tr-u12a@lists.sourceforge.net", "turkce@pardus.org.tr", ),
    "tt": ("tatarish.l10n@gmail.com", ),
    "ug": ("gnome-uighur@yahoogroups.com", ),
    "uk": ("linux@linux.org.ua", ),
    "ur": ("l10n@urduweb.org", "urdu.scs.gift@gmail.com", ),
    "ve": ("venda@translate.org.za", ),
    "vi": ("gnomevi-list@lists.sourceforge.net", "vi-VN@googlegroups.com", ),
    "wa": ("linux-wa@", ),
    "xh": ("xh-translate@ubuntu.com", "xhosa@translate.org.za",
           "xhosa@ubuntu.com", ),
    "zh_CN": ("i18n-translation@lists.linux.net.cn",
              "i18n-zh@googlegroups.com",
              "translation-team-zh-cn@lists.sourceforge.net",
              "i18n-zh@googlegroup.com", ),
    "zh_TW": ("zh-l10n@lists.linux.org.tw", "chinese-l10n@googlegroups.com",
              "community@linuxhall.org", "zh-l10n@linux.org.tw", ),
    "zu": ("zulu@translate.org.za", ),
}
"""Language codes with snippets of contact information that can be used to
uniquely identify the language"""

LANG_TEAM_LANGUAGE_SNIPPETS = {
    "af": ("Afrikaans",),
    "am": ("Amharic",),
    "ang": ("Old English",),
    "ar": ("Arabic", ),
    "as": ("Assamese", ),
    "ast": ("Asturian", ),
    "az": ("Azerbaijani", u"Azrbaycan", ),
    "bg": ("Bulgarian", ),
    "be@latin": ("Belarusian Latin", ),
    "be": ("Belarusian", "Belorussian", ),
    "bn_IN": ("Bengali (India)", "Bengali INDIA", "Bengali India", ),
    "bn": ("Bangladeshi", "Bengali", ),
    "br": ("Breton", "Britton", ),
    "bs": ("Bosanski", "Bosnian", ),
    "byn": ("Blin", ),
    "ca": ("Catalan", ),
    "ckb": ("Kurdish (Sorani)", ),
    "crh": ("Crimean Tatar", "Crimean Turkish", ),
    "cs": ("Czech", ),
    "cy": ("Cymru", "Welsh", ),
    "da": ("Danish", "Dansk", ),
    "de": ("Deutsch", "German", ),
    "dz": ("Dzongkha", ),
    "el": ("Greek", ),
    "en_GB": ("British English", "en_GB", "English (Great Britain)", ),
    "eo": ("Esperanto", ),
    "es": ("Spanish", "es_ES", u"Espaol", ),
    "et": ("Eesti", "Estonian", ),
    "eu": ("Basque", "Euskara", ),
    "fa": ("Persian", ),
    "fi": ("Finnish", "Suomi", ),
    "fo": ("Faroese", ),
    "fr": ("French", u"Franais", ),
    "fur": ("Friulian", ),
    "ga": ("Irish", ),
    "gez": ("Geez", ),
    "gl": ("Galego", "Galician", "Gallegan", "gl_ES", ),
    "gu": ("Gujarati", ),
    "haw": ("Hawaiian", ),
    "he": ("Hebrew", ),
    "hi": ("Hindi", ),
    "hr": ("Croatian", ),
    "hu": ("Hungarian", ),
    "hy": ("Armenian", ),
    "ia": ("Interlingua", ),
    "id": ("Bahasa Indonesia", "Indonesia", "Indonesian", ),
    "ig": ("Igbo", ),
    "is": ("Icelandic", ),
    "it": ("Italian", ),
    "ja": ("Japanese", ),
    "ka": ("Georgian", ),
    "kk": ("Kazakh", ),
    "km": ("Khmer", ),
    "kn": ("Kannada", ),
    "ko": ("Korean", "Hangul", ),
    "kok": ("Konkani", ),
    "ks": ("Kashmiri", ),
    "ku": ("Kurdish", ),
    "ky": ("Kitghiz", "Kirghiz", ),
    "lg": ("Luganda", ),
    "li": ("Limburgish", ),
    "lt": ("Lithuanian", ),
    "lv": ("Latvian", "lv_LV", "Valoda", u"Latvieu", ),
    "mal": ("Malayalam", ),
    "mg": ("Malagasy", ),
    "mi": ("Maori", ),
    "mk": ("Macedonian", ),
    "ml": ("Malayalam", ),
    "mn": ("Mongolian", ),
    "mt": ("Marathi", ),
    "ms": ("Malay", "Bahasa Melayu", ),
    "my": ("Burmese", ),
    "nb": ("Norwegian Bokmaal", u"Norsk bokml", u"Norwegian Bokml",
           u"Norwegian bokml", ),
    "nds": ("Low Saxon", ),
    "nl": ("Dutch", "Nederlands", ),
    "nn": ("Norwegian nynorsk", "Nynorsk", ),
    "oc": ("Occitan", ),
    "or": ("Oriya", ),
    "pa": ("Punjabi", "Panjabi", ),
    "pl": ("Polish", ),
    "ps": ("Pashto", "Pushto", ),
    "pt_BR": ("Brazilian Portuguese", u"Portugus/Brasil",
              u"Portugus do Brasil", ),
    "pt": ("Portuguese", ),
    "rm": ("Rhaeto-Romance", ),
    "ro": ("Romania", "Romanian", u"Romn", ),
    "ru": ("Russian", ),
    "si": ("Sinhala", "Sinhalese", ),
    "sk": ("Slovak", ),
    "sl": ("Slovene", "Slovenian", ),
    "so": ("Somali", ),
    "sq": ("Albanian", ),
    "sr": ("Serbian", ),
    "sv": ("Swedish", ),
    "sw": ("Swahili", ),
    "ta": ("Tamil", ),
    "te": ("Telugu", ),
    "tet": ("Tetum", ),
    "tg": ("Tajik", ),
    "th": ("Thai", ),
    "ti": ("Tigrinya", ),
    "tig": ("Tigre", ),
    "tl": ("Tagalog", ),
    "tr": ("Turkish", u"Trke", u"Trkiye", ),
    "tt": ("Tatarish", ),
    "ug": ("Uighur", ),
    "uk": ("Ukrainian", ),
    "ur": ("Urdu", ),
    "uz": ("Uzbek", ),
    "ve": ("Venda", u"Tshivena", "Tshivenda", ),
    "vi": ("Vietnamese", ),
    "wa": ("Walloon", ),
    "wal": ("Walamo", ),
    "wo": ("Wolof", ),
    "xh": ("Xhosa", "IsiXhosa", "isiXhosa", ),
    "yi": ("Yiddish", ),
    "yo": ("Yoruba", ),
    "zh_CN": ("Chinese Simplified", "Chinese/Simplified",
              "Chinese (simplified)", "Simplified Chinese", ),
    "zh_HK": ("Chinese (Hong Kong)", ),
    "zh_TW": ("Chinese (traditional)", "Chinese/Traditional",
              "Traditional Chinese", ),
}
"""Language codes with snippets of language names, including English, native
spelling and varients, that can be used to uniquely identify the language"""


def _regex_guesser(prefilter, regex, string, postfilter=None):
    """Use regular expressions to extract the language team

    :param prefilter: simple filter to apply before attempting the regex
    :param regex: regular expression with one group that will contain
    the language code
    :param string: the language team string that should be examined
    :param postfilter: filter to apply to reject any potential matches
    after they have been retreived by the regex
    :return: ISO language code for the found language
    """
    # TODO instead of a posfilter, have a dictionary of transform rules
    # e.g. for debian-l10n-albanian a dict of {'russian': 'ru' would allow
    # transformation.  {'default': None} would ensure that anything we
    # don't understand gets ignored.  Or {'default': 'nothing'} means to
    # nothing.
    if prefilter in string:
        found = re.search(regex, string)
        if found:
            regex_lang = found.groups()[0]
        else:
            return None
        if postfilter is not None and regex_lang in postfilter:
            return None
        if regex_lang and regex_lang != 'en':
            return regex_lang
    return None


def _nofilter(text):
    """Return the supplied text unchanged"""
    return text


def _lower(text):
    """Convert the supplied text to lowercase"""
    return text.lower()


def _snippet_guesser(snippets_dict, string, filter_=_nofilter):
    """Guess the language based on a snippet of text in the language team
    string.

    :param snippets_dict: A dict of snippets that can be used to identify a
    language in the format {'lang': ('snippet1', 'snippet2'), 'lang2'...}
    :param string: The language string to be analysed
    :param filter_: a function to be applied to the string and snippets
    before examination
    """
    string = filter_(string)
    for possible_lang, snippets in snippets_dict.iteritems():
        for snippet in snippets:
            if filter_(snippet) in string:
                return possible_lang
    return None


def guess_language(team_string):
    """Gueses the language of a PO file based on the Language-Team entry"""

    for prefilter, regex, postfilter in LANG_TEAM_REGEX:
        lang = _regex_guesser(prefilter, regex, team_string, postfilter)
        if lang:
            break

    if not lang:
        lang = _snippet_guesser(LANG_TEAM_CONTACT_SNIPPETS, team_string,
                                _lower)

    if not lang:
        lang = _snippet_guesser(LANG_TEAM_LANGUAGE_SNIPPETS, team_string)

    # TODO Maybe clean everything and see of we have a language code only

    if not lang:
        #print (u"MISSED: '%s'" % team_string).encode('utf-8')
        return None
    return lang

if __name__ == "__main__":
    from sys import argv
    from translate.storage import factory
    for fname in argv[1:]:
        store = factory.getobject(fname)
        print(fname, guess_language(store.parseheader().get('Language-Team', u"")))

########NEW FILE########
__FILENAME__ = test_af
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import af, factory


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('af')
    sentences = language.sentences(u"Normal case. Nothing interesting.")
    assert sentences == [u"Normal case.", "Nothing interesting."]
    sentences = language.sentences(u"Wat? 'n Fout?")
    assert sentences == [u"Wat?", "'n Fout?"]
    sentences = language.sentences(u"Dit sal a.g.v. 'n fout gebeur.")
    assert sentences == [u"Dit sal a.g.v. 'n fout gebeur."]
    sentences = language.sentences(u"Weet nie hoe om ler '%s' te open nie.\nMiskien is dit 'n tipe beeld wat nog nie ondersteun word nie.\n\nKies liewer 'n ander prent.")
    assert len(sentences) == 3


def test_capsstart():
    """Tests that the indefinite article ('n) doesn't confuse startcaps()."""
    language = factory.getlanguage('af')
    assert not language.capsstart("")
    assert language.capsstart("Koeie kraam koeie")
    assert language.capsstart("'Koeie' kraam koeie")
    assert not language.capsstart("koeie kraam koeie")
    assert language.capsstart("\n\nKoeie kraam koeie")
    assert language.capsstart("'n Koei kraam koeie")
    assert language.capsstart("'n 'Koei' kraam koeie")
    assert not language.capsstart("'n koei kraam koeie")
    assert language.capsstart("\n\n'n Koei kraam koeie")


def test_transliterate_cyrillic():

    def trans(text):
        print(("Orig: %s" % text).encode("utf-8"))
        trans = af.tranliterate_cyrillic(text)
        print(("Trans: %s" % trans).encode("utf-8"))
        return trans
    assert trans(u"  ") == u"Boris Nikolajewitj Jeltsin"

########NEW FILE########
__FILENAME__ = test_am
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('am')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg"
    assert language.punctranslate(u"abc efg. hij.") == u"abc efg hij"
    assert language.punctranslate(u"abc efg, hij;") == u"abc efg hij"
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file: %s?"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('am')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"         ")
    print(sentences)
    assert sentences == [u"   ", u"     "]

########NEW FILE########
__FILENAME__ = test_ar
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('ar')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc, efg; d?") == u"abc efg d"
    # See http://bugs.locamotion.org/show_bug.cgi?id=1819
    assert language.punctranslate(u"It is called abc") == u"It is called abc"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('ar')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"    \"%s\".   ")
    print(sentences)
    assert sentences == [u"    \"%s\".", u"  "]
    # This probably doesn't make sense: it is just the above reversed, to make sure
    # we test the '' as an end of sentence marker.
    sentences = language.sentences(u"       \"%s\".")
    print(sentences)
    assert sentences == [u"  ", u"    \"%s\"."]

########NEW FILE########
__FILENAME__ = test_common
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.lang import common


def test_characters():
    """Test the basic characters segmentation"""
    language = common.Common
    assert language.characters(u"") == []
    assert language.characters(u"Four") == [u"F", u"o", u"u", u"r"]
    assert language.characters(u"A B") == [u"A", u" ", u"B"]
    # Spaces are compacted, source has 2 returned has only one
    assert language.characters(u"A  B") == [u"A", u" ", u"B"]


def test_words():
    """Tests basic functionality of word segmentation."""
    language = common.Common
    words = language.words(u"")
    assert words == []

    words = language.words(u"test sentence.")
    assert words == [u"test", u"sentence"]

    words = language.words(u"This is a weird test .")
    assert words == [u"This", u"is", u"a", u"weird", u"test"]

    words = language.words(u"Don't send e-mail!")
    assert words == [u"Don't", u"send", u"e-mail"]

    words = language.words(u"Dont send e-mail!")
    assert words == [u"Dont", u"send", u"e-mail"]


@mark.xfail("sys.version_info >= (2, 6)",
            reason="ZWS "
                   "is not considered a space in Python 2.6+. Khmer should extend "
                   "words() to include \\u200b in addition to other word breakers.")
def test_word_khmer():
    language = common.Common
    # Let's test Khmer with zero width space (\u200b)
    words = language.words(u"")
    print(u"")
    print(language.words(u"<200b>"))
    print([u"", u""])
    assert words == [u"", u""]


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = common.Common
    # Check that we correctly handle an empty string:
    sentences = language.sentences(u"")

    sentences = language.sentences(u"This is a sentence.")
    assert sentences == [u"This is a sentence."]
    sentences = language.sentences(u"This is a sentence")
    assert sentences == [u"This is a sentence"]
    sentences = language.sentences(u"This is a sentence. Another one.")
    assert sentences == [u"This is a sentence.", u"Another one."]
    sentences = language.sentences(u"This is a sentence. Another one. Bla.")
    assert sentences == [u"This is a sentence.", u"Another one.", u"Bla."]
    sentences = language.sentences(u"This is a sentence.Not another one.")
    assert sentences == [u"This is a sentence.Not another one."]
    sentences = language.sentences(u"Exclamation! Really? No...")
    assert sentences == [u"Exclamation!", u"Really?", u"No..."]
    sentences = language.sentences(u"Four i.e. 1+3. See?")
    assert sentences == [u"Four i.e. 1+3.", u"See?"]
    sentences = language.sentences(u"Apples, bananas, etc. are nice.")
    assert sentences == [u"Apples, bananas, etc. are nice."]
    sentences = language.sentences(u"Apples, bananas, etc.\nNext part")
    assert sentences == [u"Apples, bananas, etc.", u"Next part"]
    sentences = language.sentences(u"No font for displaying text in encoding '%s' found,\nbut an alternative encoding '%s' is available.\nDo you want to use this encoding (otherwise you will have to choose another one)?")
    assert sentences == [u"No font for displaying text in encoding '%s' found,\nbut an alternative encoding '%s' is available.", u"Do you want to use this encoding (otherwise you will have to choose another one)?"]
    # Test that a newline at the end won't confuse us
    sentences = language.sentences(u"The first sentence. The second sentence.\n")
    assert sentences == [u"The first sentence.", u"The second sentence."]
    sentences = language.sentences(u"P.O. box")
    assert sentences == [u"P.O. box"]
    sentences = language.sentences(u"Doen dit d.m.v. koeie.")
    assert sentences == [u"Doen dit d.m.v. koeie."]


def test_capsstart():
    """Tests for basic sane behaviour in startcaps()."""
    language = common.Common
    assert language.capsstart("Open cow file")
    assert language.capsstart("'Open' cow file")
    assert not language.capsstart("open cow file")
    assert not language.capsstart(":")
    assert not language.capsstart("")


def test_numstart():
    """Tests for basic sane behaviour in startcaps()."""
    language = common.Common
    assert language.numstart("360 degress")
    assert language.numstart("3D file")
    assert not language.numstart("Open 360 degrees")
    assert not language.numstart(":")
    assert not language.numstart("")


def test_punctranslate():
    """Test the basic punctranslate function"""
    language = common.Common
    assert not language.punctranslate(u"A...") == u"A"
    language.puncdict = {u"...": u""}
    assert language.punctranslate(u"A...") == u"A"


def test_length_difference():
    """Test the heuristics of the length difference function"""
    # Expansion with no code
    assert common.Common.length_difference(10) == 6
    assert common.Common.length_difference(100) == 15
    assert common.Common.length_difference(300) == 35


def test_alter_length():
    """Test that we create the correct length by adding or removing characters"""
    assert common.Common.alter_length("One two three") == "One twOne two three"

########NEW FILE########
__FILENAME__ = test_data
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import data


def test_languagematch():
    """test language comparison"""
    # Simple comparison
    assert data.languagematch("af", "af")
    assert not data.languagematch("af", "en")

    # Handle variants
    assert data.languagematch("pt", "pt_PT")
    # FIXME don't think this one is correct
    #assert not data.languagematch("sr", "sr@Latn")

    # No first language code, we just check that the other code is valid
    assert data.languagematch(None, "en")
    assert data.languagematch(None, "en_GB")
    assert data.languagematch(None, "en_GB@Latn")
    assert not data.languagematch(None, "not-a-lang-code")


def test_normalise_code():
    """test the normalisation of language codes"""
    assert data.normalize_code("af_ZA") == "af-za"
    assert data.normalize_code("xx@Latin") == "xx-latin"


def test_simplify_to_common():
    """test language code simplification"""
    assert data.simplify_to_common("af_ZA") == "af"
    assert data.simplify_to_common("pt_PT") == "pt"
    assert data.simplify_to_common("pt_BR") == "pt_BR"


def test_language_names():
    _ = data.tr_lang('en_US')
    assert _(u"Bokml, Norwegian; Norwegian Bokml") == u"Norwegian Bokml"
    assert _(u"Spanish; Castillian") == u"Spanish"
    assert _(u"Mapudungun; Mapuche") == u"Mapudungun"
    assert _(u"Interlingua (International Auxiliary Language Association)") == u"Interlingua"

########NEW FILE########
__FILENAME__ = test_el
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('el')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg. hij.") == u"abc efg. hij."
    assert language.punctranslate(u"abc efg;") == u"abc efg"
    assert language.punctranslate(u"abc efg? hij!") == u"abc efg; hij!"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('el')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"     ; (   -)")
    assert sentences == [u"     ;", u"(   -)"]
    sentences = language.sentences(u" .  .")
    assert sentences == [u" .", u" ."]
    sentences = language.sentences(u" .  .")
    assert sentences == [u" .  ."]

########NEW FILE########
__FILENAME__ = test_es
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('es')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc efg?") == u"abc efg?"
    assert language.punctranslate(u"abc efg!") == u"abc efg!"
    # We have to be a bit more gentle on the code by using capitals correctly.
    # Can we be more robust with this witout affecting sentence segmentation?
    assert language.punctranslate(u"Abc efg? Hij.") == u"Abc efg? Hij."
    assert language.punctranslate(u"Abc efg! Hij.") == u"Abc efg! Hij."
    #TODO: we should be doing better, but at the only we only support the first sentence


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('es')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"El archivo <b>%1</b> ha sido modificado. Desea guardarlo?")
    print(sentences)
    assert sentences == [u"El archivo <b>%1</b> ha sido modificado.", u"Desea guardarlo?"]

########NEW FILE########
__FILENAME__ = test_fa
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('fa')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file: %s"
    assert language.punctranslate(u'"root" is powerful') == u"root is powerful"
    assert language.punctranslate(u"'root' is powerful") == u"root is powerful"
    assert language.punctranslate(u"`root' is powerful") == u"root is powerful"
    assert language.punctranslate(u'The user "root"') == u"The user root"
    assert language.punctranslate(u"The user 'root'") == u"The user root"
    assert language.punctranslate(u"The user `root'") == u"The user root"
    assert language.punctranslate(u'The user "root"?') == u"The user root"
    assert language.punctranslate(u"The user 'root'?") == u"The user root"
    assert language.punctranslate(u"The user `root'?") == u"The user root"
    assert language.punctranslate(u'Watch the " mark') == u'Watch the " mark'
    assert language.punctranslate(u"Watch the ' mark") == u"Watch the ' mark"
    assert language.punctranslate(u"Watch the ` mark") == u"Watch the ` mark"
    assert language.punctranslate(u'Watch the mark') == u"Watch the mark"
    assert language.punctranslate(u'The <a href="info">user</a> "root"?') == u'The <a href="info">user</a> root'
    assert language.punctranslate(u"The <a href='info'>user</a> 'root'?") == u"The <a href='info'>user</a> root"
    #Broken because we test for equal number of ` and ' in the string
    #assert language.punctranslate(u"The <a href='info'>user</a> `root'?") == u"The <a href='info'>user</a> root"
    assert language.punctranslate(u"The <a href='http://koeie'>user</a>") == u"The <a href='http://koeie'>user</a>"

    assert language.punctranslate(u"Copying `%s' to `%s'") == u"Copying %s to %s"
    # We are very careful by checking that the ` and ' match, so we miss this because of internal punctuation:
    #assert language.punctranslate(u"Shlib `%s' didn't contain `%s'") == u"Shlib %s didn't contain %s"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('fa')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"Normal case. Nothing interesting.")
    assert sentences == [u"Normal case.", u"Nothing interesting."]
    sentences = language.sentences(u"Is that the case ? Sounds interesting !")
    assert sentences == [u"Is that the case ?", u"Sounds interesting !"]

########NEW FILE########
__FILENAME__ = test_factory
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_getlanguage():
    """Tests that a basic call to getlanguage() works."""
    kmlanguage = factory.getlanguage('km')
    assert kmlanguage.code == 'km'
    assert kmlanguage.fullname == 'Central Khmer'

    # Test a non-exisint code
    language = factory.getlanguage('zz')
    assert language.nplurals == 0

    # Test a code without a module
    language = factory.getlanguage('fy')
    assert language.nplurals == 2
    assert language.fullname == "Frisian"
    assert "n != 1" in language.pluralequation

    # Test a code without a module and with a country code
    language = factory.getlanguage('de_AT')
    assert language.nplurals == 2
    assert language.fullname == "German"

    # Test with None as language code
    language = factory.getlanguage(None)
    assert language.code == ''

    #Test with a language code that is a reserved word in Python
    language = factory.getlanguage('is')
    assert language.nplurals == 2
    assert language.fullname == 'Icelandic'

    language = factory.getlanguage('or')
    assert "startcaps" in language.ignoretests

    #Test with a language code contains '@'
    language = factory.getlanguage('ca@valencia')
    assert language.nplurals == 2

########NEW FILE########
__FILENAME__ = test_fr
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('fr')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc efg!") == u"abc efg\u00a0!"
    assert language.punctranslate(u"abc efg? hij!") == u"abc efg\u00a0? hij\u00a0!"
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file\u00a0: %s\u00a0?"
    assert language.punctranslate(u'"root" is powerful') == u"\u00a0root\u00a0 is powerful"
    assert language.punctranslate(u"'root' is powerful") == u"\u00a0root\u00a0 is powerful"
    assert language.punctranslate(u"`root' is powerful") == u"\u00a0root\u00a0 is powerful"
    assert language.punctranslate(u'The user "root"') == u"The user \u00a0root\u00a0"
    assert language.punctranslate(u"The user 'root'") == u"The user \u00a0root\u00a0"
    assert language.punctranslate(u"The user `root'") == u"The user \u00a0root\u00a0"
    assert language.punctranslate(u'The user "root"?') == u"The user \u00a0root\u00a0\u00a0?"
    assert language.punctranslate(u"The user 'root'?") == u"The user \u00a0root\u00a0\u00a0?"
    assert language.punctranslate(u"The user `root'?") == u"The user \u00a0root\u00a0\u00a0?"
    assert language.punctranslate(u'Watch the " mark') == u'Watch the " mark'
    assert language.punctranslate(u"Watch the ' mark") == u"Watch the ' mark"
    assert language.punctranslate(u"Watch the ` mark") == u"Watch the ` mark"
    assert language.punctranslate(u'Watch the mark') == u"Watch the \u00a0mark\u00a0"
    assert language.punctranslate(u'The <a href="info">user</a> "root"?') == u'The <a href="info">user</a> \u00a0root\u00a0\u00a0?'
    assert language.punctranslate(u"The <a href='info'>user</a> 'root'?") == u"The <a href='info'>user</a> \u00a0root\u00a0\u00a0?"
    #Broken because we test for equal number of ` and ' in the string
    #assert language.punctranslate(u"The <a href='info'>user</a> `root'?") == u"The <a href='info'>user</a> \u00a0root\u00a0\u00a0?"
    assert language.punctranslate(u"The <a href='http://koeie'>user</a>") == u"The <a href='http://koeie'>user</a>"

    assert language.punctranslate(u"Copying `%s' to `%s'") == u"Copying \u00a0%s\u00a0 to \u00a0%s\u00a0"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('fr')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"Normal case. Nothing interesting.")
    assert sentences == [u"Normal case.", u"Nothing interesting."]
    sentences = language.sentences(u"Is that the case ? Sounds interesting !")
    assert sentences == [u"Is that the case ?", u"Sounds interesting !"]

########NEW FILE########
__FILENAME__ = test_hy
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('hy')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg"
    assert language.punctranslate(u"abc efg. hij.") == u"abc efg hij"
    assert language.punctranslate(u"abc efg!") == u"abc efg"
    assert language.punctranslate(u"Delete file: %s") == u"Delete file %s"
    # TODO: Find out exactly how questions work


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('hy')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"         ")
    assert sentences == [u"   ", u"     "]
    sentences = language.sentences(u"         ")
    assert sentences == [u"         "]

########NEW FILE########
__FILENAME__ = test_identify
#!/usr/bin/env python
# -*- coding: UTF-8 -*-

from pytest import raises

from translate.lang.identify import LanguageIdentifier
from translate.storage.base import TranslationUnit


TEXT = """
sthetik des "Erhabenen" herangezogen.
kostete (hinzu kommen ber 6 630 tote
   O3 steht fr Ozon; es wird in der
NO2 sind wesentlich am "sauren Regen"
Ethik hngt eng mit einer Dauerkrise der
Serumwerk GmbH Dresden, Postfach
,Hundeschlger' fr die Dezimierung der
Momente ihrer Erfahrung".
zusammen.
ihren Kampf um Boden wie um
Unsinn, weil die Leute Unsinn wollen
Ressourcen als soziales Entwicklungsproblem".
der Leunabrcke durch Kommune,
Speiserhre oder bei Atemstrungen von
hob er hervor, da die Knorpel
"Reisekader" wurden zu DDR-Zeiten
fr die soziale Verstndigung zugesprochen
hinaus noch viele Fhigkeiten entwickelte.
Adorno).
Frankfurter Vereine. Und
die erste evangelische Schule hatte
beispielsweise die Pfarrkirche, das
gebracht. Offenbar spielt die Schlafposition
Menschlichkeit oder Rechtsstaatlichkeit
Die nun geplante Strae wrde im
zum Thema "Der psychisch kranke
ergaben im Zeitraum von 1986 bis 1989
junge Leute sind oft zahlungskrftig in
unter die Bettdecke gerate, knne es sich
Schden hinterlt, berechtigt
krperlichen Belastbarkeit. Tatschlich
von der Drogenpolitik zu reden) oder
Parlament unter syrischem Druck diesen
Jahrhunderten der wuchtige Turm fr
auf die Frage aus, wie sich Eltern verhalten
ehemalige Generalsekretr der Partei,
Mark erhht", sagt Hhsam, "das war bei
ber eine Annonce in einem Frankfurter
der Tpfer ein. Anhand von gefundenen
gut kennt, hatte ihm die wahren Tatsachen
Sechzehn Adorno-Schler erinnern
und da ein Weiterdenken der Theorie
fr ihre Festlegung sind drei Jahre
Erschtterung Einblick in die Abhngigkeit
der Bauarbeiten sei erst im Laufe des
als neuen Kometen am Kandidatenhimmel
ergaben im Zeitraum von 1986 bis 1989
- ein neuer Beitrag zur Fortschreibung
Triptychon im Sitzungssaal des Ortsbeirates
Karin gab spter ein bemerkenswertes
mit dem er darber reden konnte?
Kunstwerk niemals das Ganze (der Welt
junge Talente vor, die vielleicht irgendwo
der AG Schweizer Strae, einer Initiative
fr Stickstoffdioxid; diese Substanzen
Ttigkeit in erster Linie das sportliche
kommentiert worden, sowohl skeptisch
auch durch "eine Unmenge Zuschriften
Grundschule, in deren Gebude auch die
gegen Stre und die sexuelle Attraktivitt.
Pablo Tattay und Henry Caballero aus
besteht fr die Leunabrcke keine rechtliche
auf einem Parteikongre mittels Abstimmung
Laurentiuskirche.
spter der SED beitraten?" Es ist der
- und die Leute wollen Unsinn, weil
frh geboren wurden oder an Muskelschwche
Grundlage. "Bei einem Brckenbau
Mensch" auf, als ein automatisch flirtender.
und sich inzwischen als Operateur
xx  = Schadstoff wird dort nicht
sondern auch fr die Geschftsleute.
Kommunismus eintreten wrde. In ihren
NCV-Vorsitzender Rainer Schroth ehrte
Aufsicht bereit sind. *leo
Daseins, in dem sie Mglichkeiten einer
die alten Schwanheimer?" in der
knnen sich ein Lachen nicht verkneifen.
ist". Die "gesunde Mischung" aus edlen
genannt hatte: "Junger Freund,
ist vorbeugend schon 1936 von Adorno
Ruhe ein", sagt Jens. Ruhe vor den
kologie bald auch
englischen Rasen der Nachbarn. Schon
Forschungsarbeit sich doch noch hat habilitieren
dringend davor, Suglinge in den ersten
Milligramm je Kubikmeter
Im Gesprch: Indianer aus Kolumbien
wenige Flle von Pltzlichem Kindstod.
   Fr nicht empfehlenswert hlt er Fuball
   SO2 steht fr Schwefeldioxid, NO2
Schwanheimer Unterfeldes hin. Rund 110
Adorno 1957 auf eine trichte Dissonanz-Rezension
durch Laute, Lcheln und Greifen
und kamen, um abzustimmen." Doch
da genau das nach dem Ende des
Zedillo, erst vor kurzem ins Erziehungsministerium
"andere Geschichte", die "unlogische".
bungen zu integrieren und somit wenigstens
Ausmae angenommen. berall wimmelte
ambulant - einen Namen gemacht hat.
Kiesgruben im Unterfeld als
der in der Verfassung festgeschriebenen
Seit 1975 habe er in seinem Fachgebiet
Feuilletons eingerissene Methode, durch
ganz woanders, schon damals
mehr zu machen." Heute verkauft dort
fr das existentielle Bewutsein belegen.
berhht und verklrt als durchdringt
  Tatschlich hat sich die durchschnittliche
"sehr, sehr schwer". Alle aber entwikkelten
der Bauchlage aufgeklrt wurde, ging der
- und die Leute wollen Unsinn, weil
in der einen Hand das Frhstcksbrtchen,
besitzen. Solche Sportarten
mit einer Aktion zusammen,
nach Bornheim wanderten, um ihren
sind, an den Ausfhrungsbestimmungen.
Um eventuelle Entsorgungskosten zu
junge Leute sind oft zahlungskrftig in
Zwar versicherte der syrische Vizeprsident
einem internen Korrektiv der Ethik.
Eckpfeiler der einstigen Stadtbefestigung
durchstie er, als er den Arm auf die
hat es ihm nachgemacht. Und auch
nachgedacht, wie sein Leben wohl verlaufen
wie hoffnungsvoll: "Wie die Toten wehrlos
und ging besonders auf das Handwerk
Syrien.
KLAUS DALLIBOR
Brche glttete, steht er zuknftig
und erschttert, wird sthetik zu
Fitne-Studio individuell abgestimmte
der Strenge des Bilderverbots.
Carneval-Vereins (NCV) beim traditionellen
bringen, ohne sie dem Diktat vershnender
und in den Karnevalvereinen -
"""

TEXT_LIST = [u"""
sthetik des "Erhabenen" herangezogen.
kostete (hinzu kommen ber 6 630 tote""",
u"""O3 steht fr Ozon; es wird in der
NO2 sind wesentlich am "sauren Regen"
Ethik hngt eng mit einer Dauerkrise der""",
u"""Serumwerk GmbH Dresden, Postfach
,Hundeschlger' fr die Dezimierung der
Momente ihrer Erfahrung".
zusammen.
"""]


class TestLanguageIdentifier(object):

    def setup_class(self):
        self.langident = LanguageIdentifier()

    def test_identify_lang(self):
        assert self.langident.identify_lang('') is None
        assert self.langident.identify_lang(TEXT) == 'de'

    def test_identify_store(self):
        langlist = [TranslationUnit(string) for string in TEXT_LIST]
        assert self.langident.identify_source_lang(langlist) == 'de'
        for i, unit in enumerate(langlist):
            unit.target = TEXT_LIST[i]
        assert self.langident.identify_target_lang(langlist) == 'de'

    def test_bad_init_data(self):
        """Test __init__ with bad conf files and data dirs"""
        assert raises(ValueError, LanguageIdentifier, model_dir='missing')
        assert raises(ValueError, LanguageIdentifier, conf_file='missing')

########NEW FILE########
__FILENAME__ = test_km
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('km')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg\u00a0"
    print(language.punctranslate(u"abc efg. hij.").encode('utf-8'))
    print(u"abc efg\u00a0 hij\u00a0".encode('utf-8'))
    assert language.punctranslate(u"abc efg. hij.") == u"abc efg\u00a0 hij\u00a0"
    assert language.punctranslate(u"abc efg!") == u"abc efg\u00a0!"
    assert language.punctranslate(u"abc efg? hij!") == u"abc efg\u00a0? hij\u00a0!"
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file\u00a0 %s\u00a0?"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('km')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"   ")
    print(sentences)
    assert sentences == [u" ", u" "]

########NEW FILE########
__FILENAME__ = test_ko
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('ko')
    # Nothing should be translated
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc efg. hij.") == u"abc efg. hij."
    assert language.punctranslate(u"abc efg!") == u"abc efg!"
    assert language.punctranslate(u"abc efg? hij!") == u"abc efg? hij!"
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file: %s?"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('ko')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"    .   ?")
    print(sentences)
    assert sentences == [u"    .", u"  ?"]

########NEW FILE########
__FILENAME__ = test_ne
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('ne')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg "
    assert language.punctranslate(u"(abc efg).") == u"(abc efg) "
    assert language.punctranslate(u"abc efg...") == u"abc efg..."
    assert language.punctranslate(u"abc efg?") == u"abc efg ?"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('ne')
    sentences = language.sentences(u"")
    assert sentences == []

    # Without spaces before the punctuation
    sentences = language.sentences(u"                               ,,  . \n")
    assert sentences == [u"                           ", u"   ,,  . "]
    # With spaces before the punctuation
    sentences = language.sentences(u"                                ,,  .  \n")
    assert sentences == [u"                            ", u"   ,,  .  "]

########NEW FILE########
__FILENAME__ = test_nqo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('nqo')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc efg!") == u"abc efg"
    assert language.punctranslate(u"abc, efg; d?") == u"abc efg d"
    # See http://bugs.locamotion.org/show_bug.cgi?id=1819
    assert language.punctranslate(u"It is called abc") == u"It is called abc"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('nqo')
    sentences = language.sentences(u"")
    assert sentences == []

    # this text probably does not make sense, I just copied it from Firefox
    # translation and added some punctuation marks
    sentences = language.sentences(u"          .     .")
    print(sentences)
    assert sentences == [u"          .", u"    ."]
    sentences = language.sentences(u"          ?     .")
    print(sentences)
    assert sentences == [u"          ?", u"    ."]

########NEW FILE########
__FILENAME__ = test_or
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('or')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"Document loaded") == u"Document loaded"
    assert language.punctranslate(u"Document loaded.") == u"Document loaded"
    assert language.punctranslate(u"Document loaded.\n") == u"Document loaded\n"
    assert language.punctranslate(u"Document loaded...") == u"Document loaded..."


def test_country_code():
    """Tests that we get the correct one even if a country code is attached to
    a special code being a reserved word in Python (like 'or')."""
    language = factory.getlanguage('or-IN')
    assert language.fullname == "Oriya"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('or')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"               ")
    assert sentences == [u"         ", u"     "]

########NEW FILE########
__FILENAME__ = test_poedit
from translate.lang.poedit import isocode


def test_isocode():
    """Test the isocode function"""
    # Standard lookup
    assert isocode("French") == "fr"
    # Dialect lookups: Portuguese
    assert isocode("Portuguese") == "pt"  # No country we default to 'None'
    assert isocode("Portuguese", "BRAZIL") == "pt_BR"  # Country with a valid dialect
    assert isocode("Portuguese", "PORTUGAL") == "pt"
    assert isocode("Portuguese", "MOZAMBIQUE") == "pt"  # Country is not a dialect so use default
    # Dialect lookups: English
    assert isocode("English") == "en"
    assert isocode("English", "UNITED KINGDOM") == "en_GB"
    assert isocode("English", "UNITED STATES") == "en"

########NEW FILE########
__FILENAME__ = test_team
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang.team import guess_language


def test_simple():
    """test the regex, team snippet and language name snippets at a high
    level"""
    # standard regex guess
    assert guess_language(u"ab@li.org") == "ab"
    # We never suggest 'en', it's always a mistake
    assert guess_language(u"en@li.org") is None
    # We can't have a single char language code
    assert guess_language(u"C@li.org") is None
    # Testing regex postfilter
    assert guess_language(u"LL@li.org") is None

    # snippet guess based on contact info
    assert guess_language(u"assam@mm.assam-glug.org") == "as"
    # snippet guess based on a language name
    assert guess_language(u"Hawaiian") == "haw"

    # We found nothing
    assert guess_language(u"Bork bork") is None

########NEW FILE########
__FILENAME__ = test_th
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('th')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg"
    assert language.punctranslate(u"abc efg. hij") == u"abc efg hij"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    # We can forget to do this well without extra help.
    language = factory.getlanguage('th')
    sentences = language.sentences(u"")
    assert sentences == []

########NEW FILE########
__FILENAME__ = test_tr
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('tr')
    sentences = language.sentences(u"Normal case. Nothing interesting.")
    assert sentences == [u"Normal case.", u"Nothing interesting."]
    sentences = language.sentences(u"1. say, 2. say.")
    assert sentences == [u"1. say, 2. say."]

########NEW FILE########
__FILENAME__ = test_uk
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('uk')
    sentences = language.sentences(u"")
    assert sentences == []
    sentences = language.sentences(u". ")
    assert sentences == [u". "]

########NEW FILE########
__FILENAME__ = test_vi
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('vi')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg."
    assert language.punctranslate(u"abc efg!") == u"abc efg !"
    assert language.punctranslate(u"abc efg? hij!") == u"abc efg? hij !"
    assert language.punctranslate(u"Delete file: %s?") == u"Delete file : %s?"
    assert language.punctranslate(u'The user "root"') == u"The user \u00a0root\u00a0"
    # More exhaustive testing of the quoting is in test_fr.py
    assert language.punctranslate(u'Lu "Tp tin"') == u"Lu \u00a0Tp tin\u00a0"
    assert language.punctranslate(u"Lu 'Tp tin'") == u"Lu \u00a0Tp tin\u00a0"
    assert language.punctranslate(u"Lu `Tp tin'") == u"Lu \u00a0Tp tin\u00a0"


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('vi')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"Normal case. Nothing interesting.")
    assert sentences == [u"Normal case.", u"Nothing interesting."]
    sentences = language.sentences(u"Is that the case ? Sounds interesting !")
    assert sentences == [u"Is that the case ?", u"Sounds interesting !"]

########NEW FILE########
__FILENAME__ = test_zh
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.lang import factory


def test_punctranslate():
    """Tests that we can translate punctuation."""
    language = factory.getlanguage('zh')
    assert language.punctranslate(u"") == u""
    assert language.punctranslate(u"abc efg") == u"abc efg"
    assert language.punctranslate(u"abc efg.") == u"abc efg"
    assert language.punctranslate(u"(abc efg).") == u"(abc efg)"
    assert language.punctranslate(u"(abc efg). hijk") == u"(abc efg)hijk"
    assert language.punctranslate(u".") == u""
    assert language.punctranslate(u"abc efg...") == u"abc efg..."


def test_sentences():
    """Tests basic functionality of sentence segmentation."""
    language = factory.getlanguage('zh')
    sentences = language.sentences(u"")
    assert sentences == []

    sentences = language.sentences(u"\n")
    assert sentences == [u"", u""]

########NEW FILE########
__FILENAME__ = th
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Thai language.

.. seealso:: http://en.wikipedia.org/wiki/Thai_language
"""

from translate.lang import common


class th(common.Common):
    """This class represents Thai."""

    puncdict = {
        u". ": u" ",
        #u"; ": u" ", # Test interaction with XML entities
    }

    # No capitalisation. While we can't do sentence segmentation, sentencecount
    # is useless.
    ignoretests = ["startcaps", "simplecaps", "sentencecount"]

########NEW FILE########
__FILENAME__ = tr
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009,2013 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Turkish language.
"""

from translate.lang import common


class tr(common.Common):
    """This class represents Turkish."""

    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890" + u""

########NEW FILE########
__FILENAME__ = ug
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Uyghur language.

.. seealso:: http://en.wikipedia.org/wiki/Uyghur_language
"""

from translate.lang import common


class ug(common.Common):
    """This class represents Uyghur."""

    listseperator = u" "

    puncdict = {
        u",": u"",
        u";": u"",
        u"?": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ur
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Urdu language.

.. seealso:: http://en.wikipedia.org/wiki/Urdu_language
"""

from translate.lang import common


class ur(common.Common):
    """This class represents Urdu."""

    listseperator = u" "

    puncdict = {
        u".": u"",
        u",": u"",
        u";": u"",
        u"?": u"",
        #This causes problems with variables, so commented out for now:
        #u"%": u"",
    }

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = ve
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Venda language.

.. seealso:: http://en.wikipedia.org/wiki/Venda_language
"""

from translate.lang import common


class ve(common.Common):
    """This class represents Venda."""

    specialchars = "  "

########NEW FILE########
__FILENAME__ = vi
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Vietnamese language.

.. seealso:: http://en.wikipedia.org/wiki/Vietnamese_language
"""

from translate.lang import common, fr


class vi(common.Common):
    """This class represents Vietnamese."""

    # Vietnamese uses similar rules for spacing two-part punctuation marks as
    # French, but does not use a space before '?'.
    puncdict = {}
    for c in u":;!#":
        puncdict[c] = u" %s" % c

    @classmethod
    def punctranslate(cls, text):
        """Implement some extra features for quotation marks.

        Known shortcomings:
            - % and $ are not touched yet for fear of variables
            - Double spaces might be introduced
        """
        text = super(cls, cls).punctranslate(text)
        return fr.guillemets(text)

    mozilla_nplurals = 2
    mozilla_pluralequation = "n!=1 ? 1 : 0"

########NEW FILE########
__FILENAME__ = wo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Wolof language.
"""

from translate.lang import common


class wo(common.Common):
    """This class represents Wolof."""

    validdoublewords = [
        u"am",
        u"baana",
        u"bgg",
        u"bn",
        u"dagg",
        u"damm",
        u"dgg",
        u"fas ",
        u"fee",
        u"fl",
        u"gaa",
        u"gkk",
        u"jaar",
        u"jafe",
        u"jam",
        u"jkki",
        u"koom",
        u"leg",
        u"matt",
        u"mn",
        u"mutt",
        u"ndank",
        u"un",
        u"pas",
        u"pl",
        u"ra",
        u"ray",
        u"rq",
        u"saay",
        u"sa",
        u"soor",
        u"soox",
        u"sum",
        u"tkk",
        u"tng",
        u"tq",
        u"tipp",
        u"tolof",
        u"tor",
        u"waalo",
        u"waa",
        u"xam",
        u"xar",
        u"xew",
        u"yg",
        u"yopp",
    ]

########NEW FILE########
__FILENAME__ = zh
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Chinese language (Both tradisional and simplified).

.. seealso:: http://en.wikipedia.org/wiki/Chinese_language
"""

import re

from translate.lang import common


class zh(common.Common):
    """This class represents Chinese."""

    listseperator = u""

    sentenceend = u""

    # Compared to common.py, we make the space after the sentence ending
    # optional and don't demand an uppercase letter to follow.
    sentencere = re.compile(r"""(?s) # make . also match newlines
                            .*?      # any text, but match non-greedy
                            [%s]     # the puntuation for sentence ending
                            \s*      # the optional space after the puntuation
                            """ % sentenceend, re.VERBOSE)

    # The following transformation rules should be mostly useful for all types
    # of Chinese. The comma (,) is not handled here, since it maps to two
    # different characters, depending on context.
    # If comma is used as seperation of sentence, it should be converted to a
    # fullwidth comma (""). If comma is used as seperation of list items
    # like "apple, orange, grape, .....", "" is used.
    puncdict = {
        u". ": u"",
        u"; ": u"",
        u": ": u"",
        u"! ": u"",
        u"? ": u"",
        u".\n": u"\n",
        u";\n": u"\n",
        u":\n": u"\n",
        u"!\n": u"\n",
        u"?\n": u"",
        u"% ": u"%",
    }

    @classmethod
    def length_difference(cls, length):
        return 10 - length / 2

    ignoretests = ["startcaps", "simplecaps"]

########NEW FILE########
__FILENAME__ = zh_cn
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Chinese language (simplified).

.. seealso:: http://en.wikipedia.org/wiki/Chinese_language
"""

from translate.lang.zh import zh


class zh_cn(zh):
    specialchars = u""

########NEW FILE########
__FILENAME__ = zh_hk
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Chinese language (traditional).

.. seealso:: http://en.wikipedia.org/wiki/Chinese_language
"""

from translate.lang.zh import zh


class zh_hk(zh):
    specialchars = u""

########NEW FILE########
__FILENAME__ = zh_tw
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module represents the Chinese language (traditional).

.. seealso:: http://en.wikipedia.org/wiki/Chinese_language
"""

from translate.lang.zh import zh


class zh_tw(zh):
    specialchars = u""

########NEW FILE########
__FILENAME__ = autoencode
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Supports a hybrid Unicode string that knows which encoding is preferable,
and uses this when converting to a string."""


# Python 3 compatibility
try:
    unicode
except NameError:
    unicode = str


class autoencode(unicode):

    def __new__(newtype, string=u"", encoding=None, errors=None):
        if isinstance(string, unicode):
            if errors is None:
                newstring = unicode.__new__(newtype, string)
            else:
                newstring = unicode.__new__(newtype, string, errors=errors)
            if encoding is None and isinstance(string, autoencode):
                newstring.encoding = string.encoding
            else:
                newstring.encoding = encoding
        else:
            if errors is None and encoding is None:
                newstring = unicode.__new__(newtype, string)
            elif errors is None:
                try:
                    newstring = unicode.__new__(newtype, string, encoding)
                except LookupError as e:
                    raise ValueError(str(e))
            elif encoding is None:
                newstring = unicode.__new__(newtype, string, errors)
            else:
                newstring = unicode.__new__(newtype, string, encoding, errors)
            newstring.encoding = encoding
        return newstring

    def join(self, seq):
        return autoencode(super(autoencode, self).join(seq))

    def __str__(self):
        if self.encoding is None:
            return super(autoencode, self).__str__()
        else:
            return self.encode(self.encoding)

########NEW FILE########
__FILENAME__ = deprecation
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2014 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation; either version 2 of the License, or (at your option) any later
# version.
#
# translate is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, see <http://www.gnu.org/licenses/>.

import warnings
from functools import wraps


def deprecated(message=""):
    """Decorator that marks functions and methods as deprecated.

    A warning will be emitted when the function or method is used. If a custom
    message is provided, it will be shown after the default warning message.
    """
    def inner_render(func):
        @wraps(func)
        def new_func(*args, **kwargs):
            msg = message  # Hack to avoid UnboundLocalError.
            if msg:
                msg = "\n" + msg
            warnings.warn_explicit(
                "Call to deprecated function {0}.{1}".format(func.__name__,
                                                             msg),
                category=DeprecationWarning,
                filename=func.func_code.co_filename,
                lineno=func.func_code.co_firstlineno + 1
            )
            return func(*args, **kwargs)
        return new_func
    return inner_render

########NEW FILE########
__FILENAME__ = dictutils
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Implements a case-insensitive (on keys) dictionary and
order-sensitive dictionary"""

# Copyright 2002, 2003 St James Software
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


class cidict(dict):

    def __init__(self, fromdict=None):
        """constructs the cidict, optionally using another dict to do so"""
        if fromdict is not None:
            self.update(fromdict)

    def __getitem__(self, key):
        if type(key) != str and type(key) != unicode:
            raise TypeError("cidict can only have str or unicode as key (got %r)" %
                            type(key))
        for akey in self.keys():
            if akey.lower() == key.lower():
                return dict.__getitem__(self, akey)
        raise IndexError

    def __setitem__(self, key, value):
        if type(key) != str and type(key) != unicode:
            raise TypeError("cidict can only have str or unicode as key (got %r)" %
                            type(key))
        for akey in self.keys():
            if akey.lower() == key.lower():
                return dict.__setitem__(self, akey, value)
        return dict.__setitem__(self, key, value)

    def update(self, updatedict):
        """D.update(E) -> None.
           Update D from E: for k in E.keys(): D[k] = E[k]"""
        for key, value in updatedict.iteritems():
            self[key] = value

    def __delitem__(self, key):
        if type(key) != str and type(key) != unicode:
            raise TypeError("cidict can only have str or unicode as key (got %r)" %
                            type(key))
        for akey in self.keys():
            if akey.lower() == key.lower():
                return dict.__delitem__(self, akey)
        raise IndexError

    def __contains__(self, key):
        if type(key) != str and type(key) != unicode:
            raise TypeError("cidict can only have str or unicode as key (got %r)" %
                            type(key))
        for akey in self.keys():
            if akey.lower() == key.lower():
                return 1
        return 0

    def has_key(self, key):
        return self.__contains__(key)

    def get(self, key, default=None):
        if key in self:
            return self[key]
        else:
            return default


class ordereddict(dict):
    """a dictionary which remembers its keys in the order in which they
    were given"""

    def __init__(self, *args):
        if len(args) == 0:
            super(ordereddict, self).__init__()
            self.order = []
        elif len(args) > 1:
            raise TypeError("ordereddict() takes at most 1 argument (%d given)" %
                            len(args))
        else:
            initarg = args[0]
            apply(super(ordereddict, self).__init__, args)
            if hasattr(initarg, "keys"):
                self.order = initarg.keys()
            else:
                # danger: could have duplicate keys...
                self.order = []
                checkduplicates = {}
                for key, value in initarg:
                    if not key in checkduplicates:
                        self.order.append(key)
                        checkduplicates[key] = None

    def __setitem__(self, key, value):
        alreadypresent = key in self
        result = dict.__setitem__(self, key, value)
        if not alreadypresent:
            self.order.append(key)
        return result

    def update(self, updatedict):
        """D.update(E) -> None.
        Update D from E: for k in E.keys(): D[k] = E[k]"""
        for key, value in updatedict.iteritems():
            self[key] = value

    def __delitem__(self, key):
        alreadypresent = key in self
        result = dict.__delitem__(self, key)
        if alreadypresent:
            del self.order[self.order.index(key)]
        return result

    def copy(self):
        """D.copy() -> a shallow copy of D"""
        thecopy = ordereddict(super(ordereddict, self).copy())
        thecopy.order = self.order[:]
        return thecopy

    def items(self):
        """D.items() -> list of D's (key, value) pairs, as 2-tuples"""
        return [(key, self[key]) for key in self.order]

    def iteritems(self):
        """D.iteritems() -> an iterator over the (key, value) items of D"""
        for key in self.order:
            yield (key, self[key])

    def iterkeys(self):
        """D.iterkeys() -> an iterator over the keys of D"""
        for key in self.order:
            yield key

    __iter__ = iterkeys

    def itervalues(self):
        """D.itervalues() -> an iterator over the values of D"""
        for key in self.order:
            yield self[key]

    def keys(self):
        """D.keys() -> list of D's keys"""
        return self.order[:]

    def popitem(self):
        """D.popitem() -> (k, v), remove and return some (key, value) pair
        as a 2-tuple; but raise KeyError if D is empty"""
        if len(self.order) == 0:
            raise KeyError("popitem(): ordered dictionary is empty")
        k = self.order.pop()
        v = self[k]
        del self[k]
        return (k, v)

    def pop(self, key):
        """remove entry from dict and internal list"""
        value = super(ordereddict, self).pop(key)
        del self.order[self.order.index(key)]
        return value

########NEW FILE########
__FILENAME__ = file_discovery
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

__all__ = ['get_abs_data_filename']

import os
import sys


def get_abs_data_filename(path_parts, basedirs=None):
    """Get the absolute path to the given file- or directory name in the
    current running application's data directory.

    :type  path_parts: list
    :param path_parts: The path parts that can be joined by ``os.path.join()``.
    """
    if basedirs is None:
        basedirs = []

    if isinstance(path_parts, str):
        path_parts = [path_parts]

    BASE_DIRS = basedirs + [
        os.path.dirname(unicode(__file__, sys.getfilesystemencoding())),
        os.path.dirname(unicode(sys.executable, sys.getfilesystemencoding())),
    ]

    # Freedesktop standard
    if 'XDG_DATA_HOME' in os.environ:
        BASE_DIRS += [os.environ['XDG_DATA_HOME']]
    if 'XDG_DATA_DIRS' in os.environ:
        BASE_DIRS += os.environ['XDG_DATA_DIRS'].split(os.path.pathsep)

    # Mac OSX app bundles
    if 'RESOURCEPATH' in os.environ:
        BASE_DIRS += os.environ['RESOURCEPATH'].split(os.path.pathsep)

    DATA_DIRS = [
        ["..", "..", "share"],
        ["..", "share"],
        ["share"],
    ]

    for basepath, data_dir in ((x, y) for x in BASE_DIRS for y in DATA_DIRS):
        dir_and_filename = data_dir + path_parts
        datafile = os.path.join(basepath or os.path.dirname(__file__),
                                *dir_and_filename)
        if os.path.exists(datafile):
            return datafile
    raise Exception('Could not find "%s"' % (os.path.join(*path_parts)))

########NEW FILE########
__FILENAME__ = lru
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import gc
from collections import deque
from weakref import WeakValueDictionary


class LRUCachingDict(WeakValueDictionary):
    """Caching dictionary like object that discards the least recently
    used objects when number of cached items exceeds maxsize.

    cullsize is the fraction of items that will be discarded when
    maxsize is reached.
    """

    def __init__(self, maxsize, cullsize=2, peakmult=10, aggressive_gc=True,
                 *args, **kwargs):
        self.cullsize = max(2, cullsize)
        self.maxsize = max(cullsize, maxsize)
        self.aggressive_gc = aggressive_gc
        self.peakmult = peakmult
        self.queue = deque()
        WeakValueDictionary.__init__(self, *args, **kwargs)

    def cull(self):
        """free memory by deleting old items from cache"""
        # maximum cache size exceeded, cull old items
        #
        # note queue is the real cache but its size is boundless
        # since it might have duplicate references.
        #
        # don't bother culling if queue is smaller than weakref,
        # this means there are too many references outside the
        # cache, culling won't free much memory (if any).
        while len(self) >= self.maxsize <= len(self.queue):
            cullsize = max(int(len(self.queue) / self.cullsize), 2)
            try:
                for i in xrange(cullsize):
                    self.queue.popleft()
            except IndexError:
                # queue is empty, bail out.
                #FIXME: should we force garbage collection here too?
                break

            # call garbage collecter manually since objects
            # with circular references take some time to get
            # collected
            if self.aggressive_gc:
                rounds = min(max(int(self.aggressive_gc), 5), 50)
                for i in xrange(rounds):
                    gc.collect()
            else:
                gc.collect()

    def __setitem__(self, key, value):
        # check boundaries to minimize duplicate references
        while len(self.queue) and self.queue[0][0] == key:
            # item at left end of queue pop it since it'll be appended
            # to right
            self.queue.popleft()

        while len(self.queue) and self.queue[-1][0] == key:
            # item at right end of queue pop it since it'll be
            # appended again
            self.queue.pop()

        if (len(self) >= self.maxsize or
            len(self.queue) >= self.maxsize * self.peakmult):
            self.cull()

        self.queue.append((key, value))
        WeakValueDictionary.__setitem__(self, key, value)

    def __getitem__(self, key):
        value = WeakValueDictionary.__getitem__(self, key)
        # check boundaries to minimiza duplicate references
        while len(self.queue) > 0 and self.queue[0][0] == key:
            # item at left end of queue pop it since it'll be appended
            # to right
            self.queue.popleft()

        # only append if item is not at right end of queue
        if not (len(self.queue) and self.queue[-1][0] == key):
            if (len(self) >= self.maxsize or
                len(self.queue) >= self.maxsize * self.peakmult):
                self.cull()
            self.queue.append((key, value))

        return value

    def __delitem__(self, key):
        # can't efficiently find item in queue to delete, check
        # boundaries. otherwise just wait till next cache purge
        while len(self.queue) and self.queue[0][0] == key:
            # item at left end of queue pop it since it'll be appended
            # to right
            self.queue.popleft()

        while len(self.queue) and self.queue[-1][0] == key:
            # item at right end of queue pop it since it'll be
            # appended again
            self.queue.pop()

        return WeakValueDictionary.__delitem__(self, key)

    def clear(self):
        self.queue.clear()
        return WeakValueDictionary.clear(self)

    def setdefault(self, key, default):
        if key not in self:
            self[key] = default

        return self[key]

########NEW FILE########
__FILENAME__ = multistring
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Supports a hybrid Unicode string that can also have a list of alternate
strings in the strings attribute"""

from translate.misc import autoencode


class multistring(autoencode.autoencode):

    def __new__(newtype, string=u"", encoding=None, errors=None):
        if isinstance(string, list):
            if not string:
                raise ValueError("multistring must contain at least one string")
            mainstring = string[0]
            newstring = multistring.__new__(newtype, string[0],
                                            encoding, errors)
            newstring.strings = [newstring] + [autoencode.autoencode.__new__(autoencode.autoencode, altstring, encoding, errors) for altstring in string[1:]]
        else:
            newstring = autoencode.autoencode.__new__(newtype, string,
                                                      encoding, errors)
            newstring.strings = [newstring]
        return newstring

    def __init__(self, *args, **kwargs):
        super(multistring, self).__init__()
        if not hasattr(self, "strings"):
            self.strings = []

    def __cmp__(self, otherstring):
        if isinstance(otherstring, multistring):
            parentcompare = cmp(autoencode.autoencode(self), otherstring)
            if parentcompare:
                return parentcompare
            else:
                return cmp(self.strings[1:], otherstring.strings[1:])
        elif isinstance(otherstring, autoencode.autoencode):
            return cmp(autoencode.autoencode(self), otherstring)
        elif isinstance(otherstring, unicode):
            return cmp(unicode(self), otherstring)
        elif isinstance(otherstring, str):
            return cmp(str(self), otherstring)
        elif isinstance(otherstring, list) and otherstring:
            return cmp(self, multistring(otherstring))
        else:
            return cmp(type(self), type(otherstring))

    def __ne__(self, otherstring):
        return self.__cmp__(otherstring) != 0

    def __eq__(self, otherstring):
        return self.__cmp__(otherstring) == 0

    def __repr__(self):
        parts = [autoencode.autoencode.__repr__(self)] + \
                [repr(a) for a in self.strings[1:]]
        return "multistring([" + ",".join(parts) + "])"

    def replace(self, old, new, count=None):
        if count is None:
            newstr = multistring(super(multistring, self) \
                   .replace(old, new), self.encoding)
        else:
            newstr = multistring(super(multistring, self) \
                   .replace(old, new, count), self.encoding)
        for s in self.strings[1:]:
            if count is None:
                newstr.strings.append(s.replace(old, new))
            else:
                newstr.strings.append(s.replace(old, new, count))
        return newstr

########NEW FILE########
__FILENAME__ = optrecurse
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import fnmatch
import logging
import optparse
import os.path
import re
import sys
import traceback
from cStringIO import StringIO

from translate import __version__
from translate.misc import progressbar


class ManPageOption(optparse.Option, object):
    ACTIONS = optparse.Option.ACTIONS + ("manpage",)

    def take_action(self, action, dest, opt, value, values, parser):
        """take_action that can handle manpage as well as standard actions"""
        if action == "manpage":
            parser.print_manpage()
            sys.exit(0)
        return super(ManPageOption, self).take_action(action, dest, opt, value,
                                                      values, parser)


class ManHelpFormatter(optparse.HelpFormatter):

    def __init__(self,
            indent_increment=0,
            max_help_position=0,
            width=80,
            short_first=1):
        optparse.HelpFormatter.__init__(
            self, indent_increment, max_help_position, width, short_first)

    def format_option_strings(self, option):
        """Return a comma-separated list of option strings & metavariables."""
        if option.takes_value():
            metavar = option.metavar or option.dest.upper()
            metavar = '\\fI%s\\fP' % metavar
            short_opts = [sopt + metavar for sopt in option._short_opts]
            long_opts = [lopt + "\\fR=\\fP" + metavar for lopt in option._long_opts]
        else:
            short_opts = option._short_opts
            long_opts = option._long_opts

        if self.short_first:
            opts = short_opts + long_opts
        else:
            opts = long_opts + short_opts

        return '\\fB%s\\fP' % ("\\fR, \\fP".join(opts))


class RecursiveOptionParser(optparse.OptionParser, object):
    """A specialized Option Parser for recursing through directories."""

    def __init__(self, formats, usetemplates=False, allowmissingtemplate=False,
                 description=None):
        """Construct the specialized Option Parser.

        :type formats: Dictionary
        :param formats: See :meth:`~.RecursiveOptionParser.setformats`
        for an explanation of the formats parameter.
        """

        optparse.OptionParser.__init__(self, version="%prog " + __version__.sver,
                                       description=description)
        self.setmanpageoption()
        self.setprogressoptions()
        self.seterrorleveloptions()
        self.setformats(formats, usetemplates)
        self.passthrough = []
        self.allowmissingtemplate = allowmissingtemplate
        logging.basicConfig(format="%(name)s: %(levelname)s: %(message)s")

    def get_prog_name(self):
        return os.path.basename(sys.argv[0])

    def setmanpageoption(self):
        """creates a manpage option that allows the optionparser to generate a
        manpage"""
        manpageoption = ManPageOption(None, "--manpage", dest="manpage",
                                      default=False, action="manpage",
            help="output a manpage based on the help")
        self.define_option(manpageoption)

    def format_manpage(self):
        """returns a formatted manpage"""
        result = []
        prog = self.get_prog_name()
        formatprog = lambda x: x.replace("%prog", prog)
        formatToolkit = lambda x: x.replace("%prog", "Translate Toolkit")
        result.append('.\\" Autogenerated manpage\n')
        result.append('.TH %s 1 "%s" "" "%s"\n' % (prog,
            formatToolkit(self.version),
            formatToolkit(self.version)))
        result.append('.SH NAME\n')
        result.append('%s \\- %s\n' % (self.get_prog_name(),
                                       self.description.split('\n\n')[0]))
        result.append('.SH SYNOPSIS\n')
        result.append('.PP\n')
        usage = "\\fB%prog "
        usage += " ".join([self.getusageman(option) for option in self.option_list])
        usage += "\\fP"
        result.append('%s\n' % formatprog(usage))
        description_lines = self.description.split('\n\n')[1:]
        if description_lines:
            result.append('.SH DESCRIPTION\n')
            result.append('\n\n'.join([re.sub('\.\. note::', 'Note:', l)
                                              for l in description_lines]))
        result.append('.SH OPTIONS\n')
        ManHelpFormatter().store_option_strings(self)
        result.append('.PP\n')
        for option in self.option_list:
            result.append('.TP\n')
            result.append('%s\n' % str(option).replace('-', '\-'))
            result.append('%s\n' % option.help.replace('-', '\-'))
        return "".join(result)

    def print_manpage(self, file=None):
        """outputs a manpage for the program using the help information"""
        if file is None:
            file = sys.stdout
        file.write(self.format_manpage())

    def set_usage(self, usage=None):
        """sets the usage string - if usage not given, uses getusagestring for
        each option"""
        if usage is None:
            self.usage = "%prog " + " ".join([self.getusagestring(option) for option in self.option_list])
        else:
            super(RecursiveOptionParser, self).set_usage(usage)

    def warning(self, msg, options=None, exc_info=None):
        """Print a warning message incorporating 'msg' to stderr and exit."""
        if options:
            if options.errorlevel == "traceback":
                errorinfo = "\n".join(traceback.format_exception(exc_info[0],
                                      exc_info[1], exc_info[2]))
            elif options.errorlevel == "exception":
                errorinfo = "\n".join(traceback.format_exception_only(exc_info[0], exc_info[1]))
            elif options.errorlevel == "message":
                errorinfo = str(exc_info[1])
            else:
                errorinfo = ""
            if errorinfo:
                msg += ": " + errorinfo
        logging.getLogger(self.get_prog_name()).warning(msg)

    def getusagestring(self, option):
        """returns the usage string for the given option"""
        optionstring = "|".join(option._short_opts + option._long_opts)
        if getattr(option, "optionalswitch", False):
            optionstring = "[%s]" % optionstring
        if option.metavar:
            optionstring += " " + option.metavar
        if getattr(option, "required", False):
            return optionstring
        else:
            return "[%s]" % optionstring

    def getusageman(self, option):
        """returns the usage string for the given option"""
        optionstring = "\\fR|\\fP".join(option._short_opts + option._long_opts)
        if getattr(option, "optionalswitch", False):
            optionstring = "\\fR[\\fP%s\\fR]\\fP" % optionstring
        if option.metavar:
            optionstring += " \\fI%s\\fP" % option.metavar
        if getattr(option, "required", False):
            return optionstring
        else:
            return "\\fR[\\fP%s\\fR]\\fP" % optionstring

    def define_option(self, option):
        """Defines the given option, replacing an existing one of the same short
        name if neccessary..."""
        for short_opt in option._short_opts:
            if self.has_option(short_opt):
                self.remove_option(short_opt)
        for long_opt in option._long_opts:
            if self.has_option(long_opt):
                self.remove_option(long_opt)
        self.add_option(option)

    def setformats(self, formats, usetemplates):
        """Sets the format options using the given format dictionary.

        :type formats: Dictionary
        :param formats: The dictionary *keys* should be:

                        - Single strings (or 1-tuples) containing an
                          input format (if not *usetemplates*)
                        - Tuples containing an input format and
                          template format (if *usetemplates*)
                        - Formats can be *None* to indicate what to do
                          with standard input

                        The dictionary *values* should be tuples of
                        outputformat (string) and processor method.
        """

        inputformats = []
        outputformats = []
        templateformats = []
        self.outputoptions = {}
        self.usetemplates = usetemplates
        for formatgroup, outputoptions in formats.iteritems():
            if isinstance(formatgroup, (str, unicode)) or formatgroup is None:
                formatgroup = (formatgroup, )
            if not isinstance(formatgroup, tuple):
                raise ValueError("formatgroups must be tuples or None/str/unicode")
            if len(formatgroup) < 1 or len(formatgroup) > 2:
                raise ValueError("formatgroups must be tuples of length 1 or 2")
            if len(formatgroup) == 1:
                formatgroup += (None, )
            inputformat, templateformat = formatgroup
            if not isinstance(outputoptions, tuple) or len(outputoptions) != 2:
                raise ValueError("output options must be tuples of length 2")
            outputformat, processor = outputoptions
            if not inputformat in inputformats:
                inputformats.append(inputformat)
            if not outputformat in outputformats:
                outputformats.append(outputformat)
            if not templateformat in templateformats:
                templateformats.append(templateformat)
            self.outputoptions[(inputformat, templateformat)] = (outputformat, processor)
        self.inputformats = inputformats
        inputformathelp = self.getformathelp(inputformats)
        inputoption = optparse.Option("-i", "--input", dest="input",
                default=None, metavar="INPUT",
                help="read from INPUT in %s" % (inputformathelp))
        inputoption.optionalswitch = True
        inputoption.required = True
        self.define_option(inputoption)
        excludeoption = optparse.Option("-x", "--exclude", dest="exclude",
                action="append", type="string", metavar="EXCLUDE",
                default=["CVS", ".svn", "_darcs", ".git", ".hg", ".bzr"],
                help="exclude names matching EXCLUDE from input paths")
        self.define_option(excludeoption)
        outputformathelp = self.getformathelp(outputformats)
        outputoption = optparse.Option("-o", "--output", dest="output",
                default=None, metavar="OUTPUT",
                help="write to OUTPUT in %s" % (outputformathelp))
        outputoption.optionalswitch = True
        outputoption.required = True
        self.define_option(outputoption)
        if self.usetemplates:
            self.templateformats = templateformats
            templateformathelp = self.getformathelp(self.templateformats)
            templateoption = optparse.Option("-t", "--template",
                dest="template", default=None, metavar="TEMPLATE",
                help="read from TEMPLATE in %s" % (templateformathelp))
            self.define_option(templateoption)

    def setprogressoptions(self):
        """Sets the progress options."""
        self.progresstypes = {
                "none": progressbar.NoProgressBar,
                "bar": progressbar.HashProgressBar,
                "dots": progressbar.DotsProgressBar,
                "names": progressbar.MessageProgressBar,
                "verbose": progressbar.VerboseProgressBar,
        }
        progressoption = optparse.Option(None, "--progress", dest="progress",
                default="bar",
                choices=self.progresstypes.keys(), metavar="PROGRESS",
                help="show progress as: %s" % (", ".join(self.progresstypes)))
        self.define_option(progressoption)

    def seterrorleveloptions(self):
        """Sets the errorlevel options."""
        self.errorleveltypes = ["none", "message", "exception", "traceback"]
        errorleveloption = optparse.Option(None, "--errorlevel",
                dest="errorlevel", default="message",
                choices=self.errorleveltypes, metavar="ERRORLEVEL",
                help="show errorlevel as: %s" %
                     (", ".join(self.errorleveltypes)))
        self.define_option(errorleveloption)

    def getformathelp(self, formats):
        """Make a nice help string for describing formats..."""
        formats = sorted(formats)
        if None in formats:
            formats = filter(lambda format: format is not None, formats)
        if len(formats) == 0:
            return ""
        elif len(formats) == 1:
            return "%s format" % (", ".join(formats))
        else:
            return "%s formats" % (", ".join(formats))

    def isrecursive(self, fileoption, filepurpose='input'):
        """Checks if fileoption is a recursive file."""
        if fileoption is None:
            return False
        elif isinstance(fileoption, list):
            return True
        else:
            return os.path.isdir(fileoption)

    def parse_args(self, args=None, values=None):
        """Parses the command line options, handling implicit input/output
        args."""
        (options, args) = super(RecursiveOptionParser, self).parse_args(args, values)
        # some intelligent as to what reasonable people might give on the
        # command line
        if args and not options.input:
            if len(args) > 1:
                options.input = args[:-1]
                args = args[-1:]
            else:
                options.input = args[0]
                args = []
        if args and not options.output:
            options.output = args[-1]
            args = args[:-1]
        if args:
            self.error("You have used an invalid combination of --input, --output and freestanding args")
        if isinstance(options.input, list) and len(options.input) == 1:
            options.input = options.input[0]
        if options.input is None:
            self.error("You need to give an inputfile or use - for stdin ; use --help for full usage instructions")
        elif options.input == '-':
            options.input = None
        return (options, args)

    def getpassthroughoptions(self, options):
        """Get the options required to pass to the filtermethod..."""
        passthroughoptions = {}
        for optionname in dir(options):
            if optionname in self.passthrough:
                passthroughoptions[optionname] = getattr(options, optionname)
        return passthroughoptions

    def getoutputoptions(self, options, inputpath, templatepath):
        """Works out which output format and processor method to use..."""
        if inputpath:
            inputbase, inputext = self.splitinputext(inputpath)
        else:
            inputext = None
        if templatepath:
            templatebase, templateext = self.splittemplateext(templatepath)
        else:
            templateext = None
        if (inputext, templateext) in options.outputoptions:
            return options.outputoptions[inputext, templateext]
        elif (inputext, "*") in options.outputoptions:
            outputformat, fileprocessor = options.outputoptions[inputext, "*"]
        elif ("*", templateext) in options.outputoptions:
            outputformat, fileprocessor = options.outputoptions["*", templateext]
        elif ("*", "*") in options.outputoptions:
            outputformat, fileprocessor = options.outputoptions["*", "*"]
        elif (inputext, None) in options.outputoptions:
            return options.outputoptions[inputext, None]
        elif (None, templateext) in options.outputoptions:
            return options.outputoptions[None, templateext]
        elif ("*", None) in options.outputoptions:
            outputformat, fileprocessor = options.outputoptions["*", None]
        elif (None, "*") in options.outputoptions:
            outputformat, fileprocessor = options.outputoptions[None, "*"]
        else:
            if self.usetemplates:
                if inputext is None:
                    raise ValueError("don't know what to do with input format (no file extension), no template file")
                elif templateext is None:
                    raise ValueError("don't know what to do with input format %s, no template file" %
                                     (os.extsep + inputext))
                else:
                    raise ValueError("don't know what to do with input format %s, template format %s" %
                                     (os.extsep + inputext, os.extsep + templateext))
            else:
                raise ValueError("don't know what to do with input format %s" %
                                 (os.extsep + inputext))
        if outputformat == "*":
            if inputext:
                outputformat = inputext
            elif templateext:
                outputformat = templateext
            elif ("*", "*") in options.outputoptions:
                outputformat = None
            else:
                if self.usetemplates:
                    raise ValueError("don't know what to do with input format (no file extension), no template file")
                else:
                    raise ValueError("don't know what to do with input format (no file extension)")
        return outputformat, fileprocessor

    def initprogressbar(self, allfiles, options):
        """Sets up a progress bar appropriate to the options and files."""
        if options.progress in ('bar', 'verbose'):
            self.progressbar = \
                self.progresstypes[options.progress](0, len(allfiles))
            # should use .getChild("progress") but that is only in 2.7
            logger = logging.getLogger(self.get_prog_name() + ".progress")
            logger.setLevel(logging.INFO)
            logger.propagate = False
            handler = logging.StreamHandler()
            handler.setLevel(logging.INFO)
            handler.setFormatter(logging.Formatter())
            logger.addHandler(handler)
            logger.info("processing %d files...", len(allfiles))
        else:
            self.progressbar = self.progresstypes[options.progress]()

    def getfullinputpath(self, options, inputpath):
        """Gets the absolute path to an input file."""
        if options.input:
            return os.path.join(options.input, inputpath)
        else:
            return inputpath

    def getfulloutputpath(self, options, outputpath):
        """Gets the absolute path to an output file."""
        if options.recursiveoutput and options.output:
            return os.path.join(options.output, outputpath)
        else:
            return outputpath

    def getfulltemplatepath(self, options, templatepath):
        """Gets the absolute path to a template file."""
        if not options.recursivetemplate:
            return templatepath
        elif (templatepath is not None and
              self.usetemplates and options.template):
            return os.path.join(options.template, templatepath)
        else:
            return None

    def run(self):
        """Parses the arguments, and runs recursiveprocess with the resulting
        options..."""
        (options, args) = self.parse_args()
        # this is so derived classes can modify the inputformats etc based on
        # the options
        options.inputformats = self.inputformats
        options.outputoptions = self.outputoptions
        self.recursiveprocess(options)

    def recursiveprocess(self, options):
        """Recurse through directories and process files."""
        if self.isrecursive(options.input, 'input') and getattr(options, "allowrecursiveinput", True):
            if not self.isrecursive(options.output, 'output'):
                if not options.output:
                    self.error(optparse.OptionValueError("No output directory given"))
                try:
                    self.warning("Output directory does not exist. Attempting to create")
                    os.mkdir(options.output)
                except IOError as e:
                    self.error(optparse.OptionValueError("Output directory does not exist, attempt to create failed"))
            if isinstance(options.input, list):
                inputfiles = self.recurseinputfilelist(options)
            else:
                inputfiles = self.recurseinputfiles(options)
        else:
            if options.input:
                inputfiles = [os.path.basename(options.input)]
                options.input = os.path.dirname(options.input)
            else:
                inputfiles = [options.input]
        options.recursiveoutput = (self.isrecursive(options.output, 'output') and
                                   getattr(options, "allowrecursiveoutput", True))
        options.recursivetemplate = (self.usetemplates and
                                     self.isrecursive(options.template, 'template') and
                                     getattr(options, "allowrecursivetemplate", True))
        self.initprogressbar(inputfiles, options)
        for inputpath in inputfiles:
            try:
                templatepath = self.gettemplatename(options, inputpath)
                # If we have a recursive template, but the template doesn't
                # have this input file, let's drop it.
                if (options.recursivetemplate and templatepath is None and
                    not self.allowmissingtemplate):
                    self.warning("No template at %s. Skipping %s." %
                                 (templatepath, inputpath))
                    continue
                outputformat, fileprocessor = self.getoutputoptions(options, inputpath, templatepath)
                fullinputpath = self.getfullinputpath(options, inputpath)
                fulltemplatepath = self.getfulltemplatepath(options,
                                                            templatepath)
                outputpath = self.getoutputname(options, inputpath, outputformat)
                fulloutputpath = self.getfulloutputpath(options, outputpath)
                if options.recursiveoutput and outputpath:
                    self.checkoutputsubdir(options, os.path.dirname(outputpath))
            except Exception as error:
                if isinstance(error, KeyboardInterrupt):
                    raise
                self.warning("Couldn't handle input file %s" %
                             inputpath, options, sys.exc_info())
                continue
            try:
                success = self.processfile(fileprocessor, options,
                                           fullinputpath, fulloutputpath,
                                           fulltemplatepath)
            except Exception as error:
                if isinstance(error, KeyboardInterrupt):
                    raise
                self.warning("Error processing: input %s, output %s, template %s" %
                             (fullinputpath, fulloutputpath,
                              fulltemplatepath), options, sys.exc_info())
                success = False
            self.reportprogress(inputpath, success)
        del self.progressbar

    def openinputfile(self, options, fullinputpath):
        """Opens the input file."""
        if fullinputpath is None:
            return sys.stdin
        return open(fullinputpath, 'r')

    def openoutputfile(self, options, fulloutputpath):
        """Opens the output file."""
        if fulloutputpath is None:
            return sys.stdout
        return open(fulloutputpath, 'w')

    def opentempoutputfile(self, options, fulloutputpath):
        """Opens a temporary output file."""
        return StringIO()

    def finalizetempoutputfile(self, options, outputfile, fulloutputpath):
        """Write the temp outputfile to its final destination."""
        outputfile.reset()
        outputstring = outputfile.read()
        outputfile = self.openoutputfile(options, fulloutputpath)
        outputfile.write(outputstring)
        outputfile.close()

    def opentemplatefile(self, options, fulltemplatepath):
        """Opens the template file (if required)."""
        if fulltemplatepath is not None:
            if os.path.isfile(fulltemplatepath):
                return open(fulltemplatepath, 'r')
            else:
                self.warning("missing template file %s" % fulltemplatepath)
        return None

    def processfile(self, fileprocessor, options, fullinputpath,
                    fulloutputpath, fulltemplatepath):
        """Process an individual file."""
        inputfile = self.openinputfile(options, fullinputpath)
        if (fulloutputpath and
            fulloutputpath in (fullinputpath, fulltemplatepath)):
            outputfile = self.opentempoutputfile(options, fulloutputpath)
            tempoutput = True
        else:
            outputfile = self.openoutputfile(options, fulloutputpath)
            tempoutput = False
        templatefile = self.opentemplatefile(options, fulltemplatepath)
        passthroughoptions = self.getpassthroughoptions(options)
        if fileprocessor(inputfile, outputfile, templatefile,
                         **passthroughoptions):
            if tempoutput:
                self.warning("writing to temporary output...")
                self.finalizetempoutputfile(options, outputfile,
                                            fulloutputpath)
            return True
        else:
            # remove the file if it is a file (could be stdout etc)
            if fulloutputpath and os.path.isfile(fulloutputpath):
                outputfile.close()
                os.unlink(fulloutputpath)
            return False

    def reportprogress(self, filename, success):
        """Shows that we are progressing..."""
        self.progressbar.amount += 1
        self.progressbar.show(filename)

    def mkdir(self, parent, subdir):
        """Makes a subdirectory (recursively if neccessary)."""
        if not os.path.isdir(parent):
            raise ValueError("cannot make child directory %r if parent %r does not exist" %
                             (subdir, parent))
        currentpath = parent
        subparts = subdir.split(os.sep)
        for part in subparts:
            currentpath = os.path.join(currentpath, part)
            if not os.path.isdir(currentpath):
                os.mkdir(currentpath)

    def checkoutputsubdir(self, options, subdir):
        """Checks to see if subdir under options.output needs to be created,
        creates if neccessary."""
        fullpath = os.path.join(options.output, subdir)
        if not os.path.isdir(fullpath):
            self.mkdir(options.output, subdir)

    def isexcluded(self, options, inputpath):
        """Checks if this path has been excluded."""
        basename = os.path.basename(inputpath)
        for excludename in options.exclude:
            if fnmatch.fnmatch(basename, excludename):
                return True
        return False

    def recurseinputfilelist(self, options):
        """Use a list of files, and find a common base directory for them."""
        # find a common base directory for the files to do everything
        # relative to
        commondir = os.path.dirname(os.path.commonprefix(options.input))
        inputfiles = []
        for inputfile in options.input:
            if self.isexcluded(options, inputfile):
                continue
            if inputfile.startswith(commondir + os.sep):
                inputfiles.append(inputfile.replace(commondir + os.sep, "", 1))
            else:
                inputfiles.append(inputfile.replace(commondir, "", 1))
        options.input = commondir
        return inputfiles

    def recurseinputfiles(self, options):
        """Recurse through directories and return files to be processed."""
        dirstack = ['']
        join = os.path.join
        inputfiles = []
        while dirstack:
            top = dirstack.pop(-1)
            names = os.listdir(join(options.input, top))
            dirs = []
            for name in names:
                inputpath = join(top, name)
                if self.isexcluded(options, inputpath):
                    continue
                fullinputpath = self.getfullinputpath(options, inputpath)
                # handle directories...
                if os.path.isdir(fullinputpath):
                    dirs.append(inputpath)
                elif os.path.isfile(fullinputpath):
                    if not self.isvalidinputname(options, name):
                        # only handle names that match recognized input
                        # file extensions
                        continue
                    inputfiles.append(inputpath)
            # make sure the directories are processed next time round.
            dirs.reverse()
            dirstack.extend(dirs)
        return inputfiles

    def splitext(self, pathname):
        """Splits *pathname* into name and ext, and removes the extsep.

        :param pathname: A file path
        :type pathname: string
        :return: root, ext
        :rtype: tuple
        """
        root, ext = os.path.splitext(pathname)
        ext = ext.replace(os.extsep, "", 1)
        return (root, ext)

    def splitinputext(self, inputpath):
        """Splits an *inputpath* into name and extension."""
        return self.splitext(inputpath)

    def splittemplateext(self, templatepath):
        """Splits a *templatepath* into name and extension."""
        return self.splitext(templatepath)

    def templateexists(self, options, templatepath):
        """Returns whether the given template exists..."""
        fulltemplatepath = self.getfulltemplatepath(options, templatepath)
        return os.path.isfile(fulltemplatepath)

    def gettemplatename(self, options, inputname):
        """Gets an output filename based on the input filename."""
        if not self.usetemplates:
            return None
        if not inputname or not options.recursivetemplate:
            return options.template
        inputbase, inputext = self.splitinputext(inputname)
        if options.template:
            for inputext1, templateext1 in options.outputoptions:
                if inputext == inputext1:
                    if templateext1:
                        templatepath = inputbase + os.extsep + templateext1
                        if self.templateexists(options, templatepath):
                            return templatepath
            if "*" in options.inputformats:
                for inputext1, templateext1 in options.outputoptions:
                    if (inputext == inputext1) or (inputext1 == "*"):
                        if templateext1 == "*":
                            templatepath = inputname
                            if self.templateexists(options, templatepath):
                                return templatepath
                        elif templateext1:
                            templatepath = inputbase + os.extsep + templateext1
                            if self.templateexists(options, templatepath):
                                return templatepath
        return None

    def getoutputname(self, options, inputname, outputformat):
        """Gets an output filename based on the input filename."""
        if not inputname or not options.recursiveoutput:
            return options.output
        inputbase, inputext = self.splitinputext(inputname)
        outputname = inputbase
        if outputformat:
            outputname += os.extsep + outputformat
        return outputname

    def isvalidinputname(self, options, inputname):
        """Checks if this is a valid input filename."""
        inputbase, inputext = self.splitinputext(inputname)
        return ((inputext in options.inputformats) or
                ("*" in options.inputformats))

########NEW FILE########
__FILENAME__ = ourdom
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""module that provides modified DOM functionality for our needs

Note that users of ourdom should ensure that no code might still use classes
directly from minidom, like minidom.Element, minidom.Document or methods such
as minidom.parseString, since the functionality provided here will not be in
those objects.
"""

from xml.dom import expatbuilder, minidom


# helper functions we use to do xml the way we want, used by modified
# classes below


def writexml_helper(self, writer, indent="", addindent="", newl=""):
    """A replacement for writexml that formats it like typical XML files.
    Nodes are intendented but text nodes, where whitespace can be
    significant, are not indented."""
    # indent = current indentation
    # addindent = indentation to add to higher levels
    # newl = newline string
    writer.write(indent + "<" + self.tagName)

    attrs = self._get_attributes()
    a_names = attrs.keys()
    a_names.sort()

    for a_name in a_names:
        writer.write(" %s=\"" % a_name)
        minidom._write_data(writer, attrs[a_name].value)
        writer.write("\"")
    if self.childNodes:
        # We need to write text nodes without newline and indentation, so
        # we handle them differently. Note that we here assume that "empty"
        # text nodes can be done away with (see the strip()). Note also that
        # nested tags in a text node (like ph tags in xliff) should also not
        # have newlines and indentation or an extra newline, since that will
        # alter the text node.
        haveText = False
        for childNode in self.childNodes:
            if childNode.nodeType == self.TEXT_NODE and childNode.data.strip():
                haveText = True
                break
        if haveText:
            writer.write(">")
            for node in self.childNodes:
                node.writexml(writer, "", "", "")
            writer.write("</%s>%s" % (self.tagName, newl))
        else:
            # This is the normal case that we do with pretty layout
            writer.write(">%s" % (newl))
            for node in self.childNodes:
                if node.nodeType != self.TEXT_NODE:
                    node.writexml(writer, (indent + addindent), addindent, newl)
            writer.write("%s</%s>%s" % (indent, self.tagName, newl))
    else:
        writer.write("/>%s" % (newl))


def getElementsByTagName_helper(parent, name, dummy=None):
    """A reimplementation of getElementsByTagName as an iterator.

    Note that this is not compatible with getElementsByTagName that returns a
    list, therefore, the class below exposes this through yieldElementsByTagName"""

    for node in parent.childNodes:
        if (node.nodeType == minidom.Node.ELEMENT_NODE and
            (name == "*" or node.tagName == name)):
            yield node
        if node.hasChildNodes():
            for othernode in node.getElementsByTagName(name):
                yield othernode


def searchElementsByTagName_helper(parent, name, onlysearch):
    """limits the search to within tags occuring in onlysearch"""
    for node in parent.childNodes:
        if (node.nodeType == minidom.Node.ELEMENT_NODE and
            (name == "*" or node.tagName == name)):
            yield node
        if (node.nodeType == minidom.Node.ELEMENT_NODE and
            node.tagName in onlysearch):
            for node in node.searchElementsByTagName(name, onlysearch):
                yield node


def getFirstElementByTagName(node, name):
    results = node.yieldElementsByTagName(name)
#  if isinstance(results, list):
#    if len(results) == 0:
#      return None
#    else:
#      return results[0]
    try:
        result = results.next()
        return result
    except StopIteration:
        return None


def getnodetext(node):
    """returns the node's text by iterating through the child nodes"""
    if node is None:
        return ""
    return "".join([t.data for t in node.childNodes if t.nodeType == t.TEXT_NODE])

# various modifications to minidom classes to add functionality we like


class DOMImplementation(minidom.DOMImplementation):

    def _create_document(self):
        return Document()


class Element(minidom.Element):

    def yieldElementsByTagName(self, name):
        return getElementsByTagName_helper(self, name)

    def searchElementsByTagName(self, name, onlysearch):
        return searchElementsByTagName_helper(self, name, onlysearch)

    def writexml(self, writer, indent, addindent, newl):
        return writexml_helper(self, writer, indent, addindent, newl)


class Document(minidom.Document):
    implementation = DOMImplementation()

    def yieldElementsByTagName(self, name):
        return getElementsByTagName_helper(self, name)

    def searchElementsByTagName(self, name, onlysearch):
        return searchElementsByTagName_helper(self, name, onlysearch)

    def createElement(self, tagName):
        e = Element(tagName)
        e.ownerDocument = self
        return e

    def createElementNS(self, namespaceURI, qualifiedName):
        prefix, localName = _nssplit(qualifiedName)
        e = Element(qualifiedName, namespaceURI, prefix)
        e.ownerDocument = self
        return e

theDOMImplementation = DOMImplementation()

# an ExpatBuilder that allows us to use the above modifications


class ExpatBuilderNS(expatbuilder.ExpatBuilderNS):

    def reset(self):
        """Free all data structures used during DOM construction."""
        self.document = theDOMImplementation.createDocument(
          expatbuilder.EMPTY_NAMESPACE, None, None)
        self.curNode = self.document
        self._elem_info = self.document._elem_info
        self._cdata = False
        self._initNamespaces()

    def start_element_handler(self, name, attributes):
        # All we want to do is construct our own Element instead of
        # minidom.Element, unfortunately the only way to do this is to
        # copy this whole function from expatbuilder.py
        if ' ' in name:
            uri, localname, prefix, qname = expatbuilder._parse_ns_name(self, name)
        else:
            uri = expatbuilder.EMPTY_NAMESPACE
            qname = name
            localname = None
            prefix = expatbuilder.EMPTY_PREFIX
        node = Element(qname, uri, prefix, localname)
        node.ownerDocument = self.document
        expatbuilder._append_child(self.curNode, node)
        self.curNode = node

        if self._ns_ordered_prefixes:
            for prefix, uri in self._ns_ordered_prefixes:
                if prefix:
                    a = minidom.Attr(expatbuilder._intern(self,
                                                          'xmlns:' + prefix),
                             expatbuilder.XMLNS_NAMESPACE, prefix, "xmlns")
                else:
                    a = minidom.Attr("xmlns", expatbuilder.XMLNS_NAMESPACE,
                             "xmlns", expatbuilder.EMPTY_PREFIX)
                d = a.childNodes[0].__dict__
                d['data'] = d['nodeValue'] = uri
                d = a.__dict__
                d['value'] = d['nodeValue'] = uri
                d['ownerDocument'] = self.document
                expatbuilder._set_attribute_node(node, a)
            del self._ns_ordered_prefixes[:]

        if attributes:
            _attrs = node._attrs
            _attrsNS = node._attrsNS
            for i in range(0, len(attributes), 2):
                aname = attributes[i]
                value = attributes[i+1]
                if ' ' in aname:
                    uri, localname, prefix, qname = expatbuilder._parse_ns_name(self, aname)
                    a = minidom.Attr(qname, uri, localname, prefix)
                    _attrs[qname] = a
                    _attrsNS[(uri, localname)] = a
                else:
                    a = minidom.Attr(aname, expatbuilder.EMPTY_NAMESPACE,
                             aname, expatbuilder.EMPTY_PREFIX)
                    _attrs[aname] = a
                    _attrsNS[(expatbuilder.EMPTY_NAMESPACE, aname)] = a
                d = a.childNodes[0].__dict__
                d['data'] = d['nodeValue'] = value
                d = a.__dict__
                d['ownerDocument'] = self.document
                d['value'] = d['nodeValue'] = value
                d['ownerElement'] = node

    if __debug__:
        # This only adds some asserts to the original
        # end_element_handler(), so we only define this when -O is not
        # used.  If changing one, be sure to check the other to see if
        # it needs to be changed as well.

        def end_element_handler(self, name):
            curNode = self.curNode
            if ' ' in name:
                uri, localname, prefix, qname = expatbuilder._parse_ns_name(self, name)
                assert (curNode.namespaceURI == uri
                    and curNode.localName == localname
                    and curNode.prefix == prefix), \
                    "element stack messed up! (namespace)"
            else:
                assert curNode.nodeName == name, \
                     "element stack messed up - bad nodeName"
                assert curNode.namespaceURI == expatbuilder.EMPTY_NAMESPACE, \
                     "element stack messed up - bad namespaceURI"
            self.curNode = curNode.parentNode
            self._finish_end_element(curNode)

# parser methods that use our modified xml classes


def parse(file, parser=None, bufsize=None):
    """Parse a file into a DOM by filename or file object."""
    builder = ExpatBuilderNS()
    if isinstance(file, basestring):
        fp = open(file, 'rb')
        try:
            result = builder.parseFile(fp)
        finally:
            fp.close()
    else:
        result = builder.parseFile(file)
    return result


def parseString(string, parser=None):
    """Parse a file into a DOM from a string."""
    builder = ExpatBuilderNS()
    return builder.parseString(string)

########NEW FILE########
__FILENAME__ = progressbar
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004, 2005, 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Progress bar utilities for reporting feedback on the progress of an
application."""


class DotsProgressBar:
    """An ultra-simple progress indicator that just writes a dot for each
    action"""

    def __init__(self):
        import sys
        self.stderr = sys.stderr
        self.amount = 0

    def show(self, verbosemessage):
        """show a dot for progress :-)"""
        # pylint: disable=W0613
        self.stderr.write('.')
        self.stderr.flush()

    def close(self):
        self.stderr.write('\n')
        self.stderr.flush()

    def __del__(self):
        self.close()


class NoProgressBar:
    """An invisible indicator that does nothing."""

    def __init__(self):
        self.amount = 0

    def show(self, verbosemessage):
        """show nothing for progress :-)"""
        pass

    def close(self):
        pass


class ProgressBar:
    """A plain progress bar that doesn't know very much about output."""

    def __init__(self, minValue=0, maxValue=100, totalWidth=50):
        self.progBar = "[]"   # This holds the progress bar string
        self.min = minValue
        self.max = maxValue
        self.span = maxValue - minValue
        self.width = totalWidth
        self.amount = 0       # When amount == max, we are 100% done

    def __str__(self):
        """Produces the string representing the progress bar."""
        if self.amount < self.min:
            self.amount = self.min
        if self.amount > self.max:
            self.amount = self.max

        # Figure out the new percent done, round to an integer
        diffFromMin = float(self.amount - self.min)
        percentDone = (diffFromMin / float(self.span)) * 100.0
        percentDone = round(percentDone)
        percentDone = int(percentDone)

        # Figure out how many hash bars the percentage should be
        allFull = self.width - 7
        numHashes = (percentDone / 100.0) * allFull
        numHashes = int(round(numHashes))

        # build a progress bar with hashes and spaces
        self.progBar = "[%s%s] %3d%%" % ('#' * numHashes,
                                         ' ' * (allFull - numHashes),
                                         percentDone)
        return str(self.progBar)

    def show(self, verbosemessage):
        """displays the progress bar"""
        # pylint: disable=W0613
        print(self)


class MessageProgressBar(ProgressBar):
    """A ProgressBar that just writes out the messages without any progress
    display"""

    def __init__(self, *args, **kwargs):
        import sys
        self.sys = sys
        ProgressBar.__init__(self, *args, **kwargs)

    def show(self, verbosemessage):
        self.sys.stderr.write(verbosemessage + '\n')
        self.sys.stderr.flush()


class HashProgressBar(ProgressBar):
    """A ProgressBar which knows how to go back to the beginning of the
    line."""

    def __init__(self, *args, **kwargs):
        import sys
        self.sys = sys
        ProgressBar.__init__(self, *args, **kwargs)

    def show(self, verbosemessage):
        self.sys.stderr.write(str(self) + '\r')
        self.sys.stderr.flush()

    def close(self):
        self.sys.stderr.write('\n')
        self.sys.stderr.flush()

    def __del__(self):
        self.close()


class VerboseProgressBar(HashProgressBar):

    def __init__(self, *args, **kwargs):
        self.lastwidth = 0
        HashProgressBar.__init__(self, *args, **kwargs)

    def show(self, verbosemessage):
        output = str(self)
        self.sys.stderr.write('\r' + ' ' * self.lastwidth)
        self.sys.stderr.write('\r' + verbosemessage + '\n')
        self.lastwidth = len(output)
        self.sys.stderr.write('\r' + output)
        self.sys.stderr.flush()


def test(progressbar):
    import time
    for n in range(progressbar.min, progressbar.max + 1, 5):
        progressbar.amount = n
        progressbar.show("Some message")
        time.sleep(0.2)

if __name__ == '__main__':
    p = HashProgressBar(0, 100, 50)
    test(p)

########NEW FILE########
__FILENAME__ = quote
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""String processing utilities for extracting strings with various kinds
of delimiters"""

import logging

from six.moves import html_entities


def find_all(searchin, substr):
    """Returns a list of locations where substr occurs in searchin
    locations are not allowed to overlap"""
    location = 0
    locations = []
    substr_len = len(substr)
    while location != -1:
        location = searchin.find(substr, location)
        if location != -1:
            locations.append(location)
            location += substr_len
    return locations


def extract(source, startdelim, enddelim,
            escape=None, startinstring=False, allowreentry=True):
    """Extracts a doublequote-delimited string from a string, allowing for
    backslash-escaping returns tuple of (quoted string with quotes, still in
    string at end).
    """
    # Note that this returns the quote characters as well... even internally
    instring = startinstring
    enteredonce = False
    lenstart = len(startdelim)
    lenend = len(enddelim)
    startdelim_places = find_all(source, startdelim)
    if startdelim == enddelim:
        enddelim_places = startdelim_places[:]
    else:
        enddelim_places = find_all(source, enddelim)
    if escape is not None:
        lenescape = len(escape)
        escape_places = find_all(source, escape)
        # Filter escaped escapes
        true_escape = False
        true_escape_places = []
        for escape_pos in escape_places:
            if escape_pos - lenescape in escape_places:
                true_escape = not true_escape
            else:
                true_escape = True
            if true_escape:
                true_escape_places.append(escape_pos)
        startdelim_places = [pos for pos in startdelim_places if pos - lenescape not in true_escape_places]
        enddelim_places = [pos + lenend for pos in enddelim_places if pos - lenescape not in true_escape_places]
    else:
        enddelim_places = [pos + lenend for pos in enddelim_places]
    # Get a unique sorted list of the significant places in the string
    significant_places = [0] + startdelim_places + enddelim_places + [len(source)-1]
    significant_places.sort()
    extracted = ""
    lastpos = None
    for pos in significant_places:
        if instring and pos in enddelim_places:
            # Make sure that if startdelim == enddelim we don't get confused
            # and count the same string as start and end.
            if lastpos == pos - lenstart and lastpos in startdelim_places:
                continue
            extracted += source[lastpos:pos]
            instring = False
            lastpos = pos
        if ((not instring) and pos in startdelim_places and
            not (enteredonce and not allowreentry)):
            instring = True
            enteredonce = True
            lastpos = pos
    if instring:
        extracted += source[lastpos:]
    return (extracted, instring)


def extractwithoutquotes(source, startdelim, enddelim, escape=None,
                         startinstring=False, includeescapes=True,
                         allowreentry=True):
    """Extracts a doublequote-delimited string from a string, allowing for
    backslash-escaping includeescapes can also be a function that takes the
    whole escaped string and returns the replaced version.
    """
    instring = startinstring
    enteredonce = False
    lenstart = len(startdelim)
    lenend = len(enddelim)
    startdelim_places = find_all(source, startdelim)
    if startdelim == enddelim:
        enddelim_places = startdelim_places[:]
    else:
        enddelim_places = find_all(source, enddelim)
    #hell slow because it is called far too often
    if escape is not None:
        lenescape = len(escape)
        escape_places = find_all(source, escape)
        # filter escaped escapes
        true_escape = False
        true_escape_places = []
        for escape_pos in escape_places:
            if escape_pos - lenescape in escape_places:
                true_escape = not true_escape
            else:
                true_escape = True
            if true_escape:
                true_escape_places.append(escape_pos)
        startdelim_places = [pos for pos in startdelim_places if pos - lenescape not in true_escape_places]
        enddelim_places = [pos + lenend for pos in enddelim_places if pos - lenescape not in true_escape_places]
    else:
        enddelim_places = [pos + lenend for pos in enddelim_places]
    # get a unique sorted list of the significant places in the string
    significant_places = [0] + startdelim_places + enddelim_places + [len(source)-1]
    significant_places.sort()
    extracted = ""
    lastpos = 0
    callable_includeescapes = callable(includeescapes)
    checkescapes = callable_includeescapes or not includeescapes
    for pos in significant_places:
        if instring and pos in enddelim_places and lastpos != pos - lenstart:
            section_start, section_end = lastpos + len(startdelim), pos - len(enddelim)
            section = source[section_start:section_end]
            if escape is not None and checkescapes:
                escape_list = [epos - section_start for epos in true_escape_places if section_start <= epos <= section_end]
                new_section = ""
                last_epos = 0
                for epos in escape_list:
                    new_section += section[last_epos:epos]
                    if callable_includeescapes:
                        replace_escape = includeescapes(section[epos:epos + lenescape + 1])
                        # TODO: deprecate old method of returning boolean from
                        # includeescape, by removing this if block
                        if not isinstance(replace_escape, basestring):
                            if replace_escape:
                                replace_escape = section[epos:epos + lenescape + 1]
                            else:
                                replace_escape = section[epos + lenescape:epos + lenescape + 1]
                        new_section += replace_escape
                        last_epos = epos + lenescape + 1
                    else:
                        last_epos = epos + lenescape
                section = new_section + section[last_epos:]
            extracted += section
            instring = False
            lastpos = pos
        if ((not instring) and pos in startdelim_places and
            not (enteredonce and not allowreentry)):
            instring = True
            enteredonce = True
            lastpos = pos
    if instring:
        section_start = lastpos + len(startdelim)
        section = source[section_start:]
        if escape is not None and not includeescapes:
            escape_list = [epos - section_start for epos in true_escape_places if section_start <= epos]
            new_section = ""
            last_epos = 0
            for epos in escape_list:
                new_section += section[last_epos:epos]
                if (callable_includeescapes and
                    includeescapes(section[epos:epos + lenescape + 1])):
                    last_epos = epos
                else:
                    last_epos = epos + lenescape
            section = new_section + section[last_epos:]
        extracted += section
    return (extracted, instring)


def _encode_entity_char(char, codepoint2name):
    charnum = ord(char)
    if charnum in codepoint2name:
        return u"&%s;" % codepoint2name[charnum]
    else:
        return char

def entityencode(source, codepoint2name):
    """Encode ``source`` using entities from ``codepoint2name``.

    :param unicode source: Source string to encode
    :param dict codepoint2name: Dictionary mapping code points to entity names
           (without the the leading ``&`` or the trailing ``;``)
    """
    output = u""
    inentity = False
    for char in source:
        if char == "&":
            inentity = True
            possibleentity = ""
            continue
        if inentity:
            if char == ";":
                output += "&" + possibleentity + ";"
                inentity = False
            elif char == " ":
                output += _encode_entity_char("&", codepoint2name) + \
                          entityencode(possibleentity + char, codepoint2name)
                inentity = False
            else:
                possibleentity += char
        else:
            output += _encode_entity_char(char, codepoint2name)
    if inentity:
        # Handle nonentities at end of string.
        output += _encode_entity_char("&", codepoint2name) + \
                  entityencode(possibleentity, codepoint2name)

    return output


def _has_entity_end(source):
    for char in source:
        if char == ";":
            return True
        elif char == " ":
            return False
    return False

def entitydecode(source, name2codepoint):
    """Decode ``source`` using entities from ``name2codepoint``.

    :param unicode source: Source string to decode
    :param dict name2codepoint: Dictionary mapping entity names (without the
           the leading ``&`` or the trailing ``;``) to code points
    """
    output = u""
    inentity = False
    for i, char in enumerate(source):
        char = source[i]
        if char == "&":
            inentity = True
            possibleentity = ""
            continue
        if inentity:
            if char == ";":
                if (len(possibleentity) > 0 and
                    possibleentity in name2codepoint):
                    entchar = unichr(name2codepoint[possibleentity])
                    if entchar == u'&' and _has_entity_end(source[i+1:]):
                        output += "&" + possibleentity + ";"
                    else:
                        output += entchar
                    inentity = False
                else:
                    output += "&" + possibleentity + ";"
                    inentity = False
            elif char == " ":
                output += "&" + possibleentity + char
                inentity = False
            else:
                possibleentity += char
        else:
            output += char
    if inentity:
        # Handle nonentities at end of string.
        output += "&" + possibleentity
    return output


def htmlentityencode(source):
    """Encode ``source`` using HTML entities e.g.  -> ``&copy;``

    :param unicode source: Source string to encode
    """
    return entityencode(source, html_entities.codepoint2name)


def htmlentitydecode(source):
    """Decode source using HTML entities e.g. ``&copy;`` -> .

    :param unicode source: Source string to decode
    """
    return entitydecode(source, html_entities.name2codepoint)


def javapropertiesencode(source):
    """Encodes source in the escaped-unicode encoding used by Java
    .properties files
    """
    output = u""
    if source and source[0] == u" ":
        output = u"\\"
    for char in source:
        charnum = ord(char)
        if char in controlchars:
            output += controlchars[char]
        elif 0 <= charnum < 128:
            output += str(char)
        else:
            output += u"\\u%04X" % charnum
    return output


def mozillapropertiesencode(source):
    """Encodes source in the escaped-unicode encoding used by Mozilla
    .properties files.
    """
    output = u""
    for char in source:
        if char in controlchars:
            output += controlchars[char]
        else:
            output += char
    return output

def escapespace(char):
    assert(len(char) == 1)
    if char.isspace():
        return u"\\u%04X" %(ord(char))
    return char

def mozillaescapemarginspaces(source):
    """Escape leading and trailing spaces for Mozilla .properties files."""
    if not source:
        return u""

    if len(source) == 1 and source.isspace():
        # FIXME: This is hack for people using white-space to mark empty
        # Mozilla strings translated, drop this once we have better way to
        # handle this in Pootle.
        return u""

    if len(source) == 1:
        return escapespace(source)
    else:
        return escapespace(source[0]) + source[1:-1] + escapespace(source[-1])

propertyescapes = {
    # escapes that are self-escaping
    "\\": "\\", "'": "'", '"': '"',
    # control characters that we keep
    "f": "\f", "n": "\n", "r": "\r", "t": "\t",
}

controlchars = {
    # the reverse of the above...
    "\\": "\\\\",
    "\f": "\\f", "\n": "\\n", "\r": "\\r", "\t": "\\t",
}


def escapecontrols(source):
    """escape control characters in the given string"""
    for key, value in controlchars.iteritems():
        source = source.replace(key, value)
    return source


def propertiesdecode(source):
    """Decodes source from the escaped-unicode encoding used by .properties
    files.

    Java uses Latin1 by default, and Mozilla uses UTF-8 by default.

    Since the .decode("unicode-escape") routine decodes everything, and we
    don't want to we reimplemented the algorithm from Python Objects/unicode.c
    in Python and modify it to retain escaped control characters.
    """
    output = u""
    s = 0

    def unichr2(i):
        """Returns a Unicode string of one character with ordinal 32 <= i,
        otherwise an escaped control character.
        """
        if 32 <= i:
            return unichr(i)
        elif unichr(i) in controlchars:
            # we just return the character, unescaped
            # if people want to escape them they can use escapecontrols
            return unichr(i)
        return "\\u%04x" % i

    while s < len(source):
        c = source[s]
        if c != '\\':
            output += c
            s += 1
            continue
        s += 1
        if s >= len(source):
            # this is an escape at the end of the line, which implies
            # a continuation..., return the escape to inform the parser
            output += c
            continue
        c = source[s]
        s += 1
        if c == '\n':
            pass
        # propertyescapes lookups
        elif c in propertyescapes:
            output += propertyescapes[c]
        # \uXXXX escapes
        # \UXXXX escapes
        elif c in "uU":
            digits = 4
            x = 0
            for digit in range(digits):
                if s + digit >= len(source):
                    digits = digit
                    break
                c = source[s + digit].lower()
                if c.isdigit() or c in "abcdef":
                    x <<= 4
                    if c.isdigit():
                        x += ord(c) - ord('0')
                    else:
                        x += ord(c) - ord('a') + 10
                else:
                    digits = digit
                    break
            s += digits
            output += unichr2(x)
        elif c == "N":
            if source[s] != "{":
                logging.warn("Invalid named unicode escape: no { after \\N")
                output += "\\" + c
                continue
            s += 1
            e = source.find("}", s)
            if e == -1:
                logging.warn("Invalid named unicode escape: no } after \\N{")
                output += "\\" + c
                continue
            import unicodedata
            name = source[s:e]
            output += unicodedata.lookup(name)
            s = e + 1
        else:
            output += c  # Drop any \ that we don't specifically handle
    return output


def findend(string, substring):
    s = string.find(substring)
    if s != -1:
        s += len(substring)
    return s


def rstripeol(string):
    return string.rstrip("\r\n")


def stripcomment(comment, startstring="<!--", endstring="-->"):
    cstart = comment.find(startstring)
    if cstart == -1:
        cstart = 0
    else:
        cstart += len(startstring)
    cend = comment.find(endstring, cstart)
    return comment[cstart:cend].strip()


def unstripcomment(comment, startstring="<!-- ", endstring=" -->\n"):
    return startstring + comment.strip() + endstring

########NEW FILE########
__FILENAME__ = selector
# -*- coding: latin-1 -*-
"""selector - WSGI delegation based on URL path and method.

(See the docstring of selector.Selector.)

Copyright (C) 2006 Luke Arno - http://lukearno.com/

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to 
the Free Software Foundation, Inc., 51 Franklin Street, 
Fifth Floor, Boston, MA  02110-1301  USA

Luke Arno can be found at http://lukearno.com/

"""

import re
from itertools import starmap
from wsgiref.util import shift_path_info


try:
    from resolver import resolve
except ImportError:
    # resolver not essential for basic featurs
    #FIXME: this library is overkill, simplify
    pass

class MappingFileError(Exception): pass


class PathExpressionParserError(Exception): pass


def method_not_allowed(environ, start_response):
    """Respond with a 405 and appropriate Allow header."""
    start_response("405 Method Not Allowed", 
                   [('Allow', ', '.join(environ['selector.methods'])),
                    ('Content-Type', 'text/plain')])
    return ["405 Method Not Allowed\n\n"
            "The method specified in the Request-Line is not allowed "
            "for the resource identified by the Request-URI."] 


def not_found(environ, start_response):
    """Respond with a 404."""
    start_response("404 Not Found", [('Content-Type', 'text/plain')])
    return ["404 Not Found\n\n"
            "The server has not found anything matching the Request-URI."]


class Selector(object):
    """WSGI middleware for URL paths and HTTP method based delegation.
    
    See http://lukearno.com/projects/selector/

    Mappings are given are an iterable that returns tuples like this::

        (path_expression, http_methods_dict, optional_prefix)
    """
    
    status405 = staticmethod(method_not_allowed)
    status404 = staticmethod(not_found)
    
    def __init__(self, 
                 mappings=None, 
                 prefix="", 
                 parser=None, 
                 wrap=None, 
                 mapfile=None,
                 consume_path=True):
        """Initialize selector."""
        self.mappings = []
        self.prefix = prefix
        if parser is None:
            self.parser = SimpleParser()
        else:
            self.parser = parser
        self.wrap = wrap
        if mapfile is not None:
            self.slurp_file(mapfile)
        if mappings is not None: 
            self.slurp(mappings)
        self.consume_path = consume_path

    def slurp(self, mappings, prefix=None, parser=None, wrap=None):
        """Slurp in a whole list (or iterable) of mappings.
        
        Prefix and parser args will override self.parser and self.args
        for the given mappings.
        """
        if prefix is not None:
            oldprefix = self.prefix
            self.prefix = prefix
        if parser is not None:
            oldparser = self.parser
            self.parser = parser
        if wrap is not None:
            oldwrap = self.wrap
            self.wrap = wrap
        list(starmap(self.add, mappings))
        if wrap is not None:
            self.wrap = oldwrap
        if parser is not None:
            self.parser = oldparser
        if prefix is not None:
            self.prefix = oldprefix

    def add(self, path, method_dict=None, prefix=None, **http_methods):
        """Add a mapping.
        
        HTTP methods can be specified in a dict or using kwargs,
        but kwargs will override if both are given.
        
        Prefix will override self.prefix for this mapping.
        """
        # Thanks to Sbastien Pierre 
        # for suggesting that this accept keyword args.
        if method_dict is None:
            method_dict = {}
        if prefix is None:
            prefix = self.prefix
        method_dict = dict(method_dict)
        method_dict.update(http_methods)
        if self.wrap is not None:
            for meth, cbl in method_dict.items():
                method_dict[meth] = self.wrap(cbl)
        regex = self.parser(self.prefix + path)
        compiled_regex = re.compile(regex, re.DOTALL | re.MULTILINE)
        self.mappings.append((compiled_regex, method_dict))

    def __call__(self, environ, start_response):
        """Delegate request to the appropriate WSGI app."""
        app, svars, methods, matched = \
            self.select(environ['PATH_INFO'], environ['REQUEST_METHOD'])
        unnamed, named = [], {}
        for k, v in svars.iteritems():
            if k.startswith('__pos'):
                k = k[5:]
            named[k] = v
        environ['selector.vars'] = dict(named)
        for k in named.keys():
            if k.isdigit():
                unnamed.append((k, named.pop(k)))
        unnamed.sort(); unnamed = [v for k, v in unnamed]
        cur_unnamed, cur_named = environ.get('wsgiorg.routing_args', ([], {}))
        unnamed = cur_unnamed + unnamed
        named.update(cur_named)
        environ['wsgiorg.routing_args'] = unnamed, named
        environ['selector.methods'] = methods
        environ.setdefault('selector.matches', []).append(matched)
        if self.consume_path:
            environ['SCRIPT_NAME'] = environ.get('SCRIPT_NAME', '') + matched
            environ['PATH_INFO'] = environ['PATH_INFO'][len(matched):]
        return app(environ, start_response)

    def select(self, path, method):
        """Figure out which app to delegate to or send 404 or 405."""
        for regex, method_dict in self.mappings:
            match = regex.search(path)
            if match:
                methods = method_dict.keys()
                if method_dict.has_key(method):
                    return (method_dict[method], 
                            match.groupdict(), 
                            methods, 
                            match.group(0))
                elif method_dict.has_key('_ANY_'):
                    return (method_dict['_ANY_'],
                            match.groupdict(), 
                            methods, 
                            match.group(0))
                else:
                    return self.status405, {}, methods, ''
        return self.status404, {}, [], ''

    def slurp_file(self, the_file, prefix=None, parser=None, wrap=None):
        """Read mappings from a simple text file.
        
        Format looks like this::

            {{{
            
            # Comments if first non-whitespace char on line is '#'
            # Blank lines are ignored

            /foo/{id}[/]
                GET somemodule:some_wsgi_app
                POST pak.subpak.mod:other_wsgi_app
            
            @prefix /myapp
            /path[/]
                GET module:app
                POST package.module:get_app('foo')
                PUT package.module:FooApp('hello', resolve('module.setting'))

            @parser :lambda x: x
            @prefix 
            ^/spam/eggs[/]$
                GET mod:regex_mapped_app

            }}}

        ``@prefix`` and ``@parser`` directives take effect 
        until the end of the file or until changed.
        """
        if isinstance(the_file, str):
            the_file = open(the_file)
        oldprefix = self.prefix
        if prefix is not None:
            self.prefix = prefix
        oldparser = self.parser
        if parser is not None:
            self.parser = parser
        oldwrap = self.wrap
        if parser is not None:
            self.wrap = wrap
        path = methods = None
        lineno = 0
        try:
            #try:
                # accumulate methods (notice add in 2 places)
                for line in the_file:
                    lineno += 1
                    path, methods = self._parse_line(line, path, methods)
                if path and methods:
                    self.add(path, methods)
            #except Exception, e:
            #    raise MappingFileError("Mapping line %s: %s" % (lineno, e))
        finally:
            the_file.close()
            self.wrap = oldwrap
            self.parser = oldparser
            self.prefix = oldprefix

    def _parse_line(self, line, path, methods):
        """Parse one line of a mapping file.
        
        This method is for the use of selector.slurp_file.
        """
        if not line.strip() or line.strip()[0] == '#':
            pass
        elif not line.strip() or line.strip()[0] == '@':
            #   
            if path and methods:
                self.add(path, methods)
            path = line.strip()
            methods = {}
            #
            parts = line.strip()[1:].split(' ', 1)
            if len(parts) == 2:
                directive, rest = parts
            else:
                directive = parts[0]
                rest = ''
            if directive == 'prefix':
                self.prefix = rest.strip()
            if directive == 'parser':
                self.parser = resolve(rest.strip())
            if directive == 'wrap':
                self.wrap = resolve(rest.strip())
        elif line and line[0] not in ' \t':
            if path and methods:
                self.add(path, methods)
            path = line.strip()
            methods = {}
        else:
            meth, app = line.strip().split(' ', 1)
            methods[meth.strip()] = resolve(app)
        return path, methods


class SimpleParser(object):
    """Callable to turn path expressions into regexes with named groups.
    
    For instance ``"/hello/{name}"`` becomes ``r"^\/hello\/(?P<name>[^\^.]+)$"``

    For ``/hello/{name:pattern}``
    you get whatever is in ``self.patterns['pattern']`` instead of ``"[^\^.]+"``

    Optional portions of path expression can be expressed ``[like this]``

    ``/hello/{name}[/]`` (can have trailing slash or not)

    Example::

        /blog/archive/{year:digits}/{month:digits}[/[{article}[/]]]

    This would catch any of these::

        /blog/archive/2005/09
        /blog/archive/2005/09/
        /blog/archive/2005/09/1
        /blog/archive/2005/09/1/

    (I am not suggesting that this example is a best practice.
    I would probably have a separate mapping for listing the month
    and retrieving an individual entry. It depends, though.)
    """

    start, end = '{}'
    ostart, oend = '[]'
    _patterns = {'word': r'\w+',
                 'alpha': r'[a-zA-Z]+',
                 'digits': r'\d+',
                 'number': r'\d*.?\d+',
                 'chunk': r'[^/^.]+',
                 'segment': r'[^/]+',
                 'any': r'.+'}
    default_pattern = 'chunk'
    
    def __init__(self, patterns=None):
        """Initialize with character class mappings."""
        self.patterns = dict(self._patterns)
        if patterns is not None:
            self.patterns.update(patterns)

    def lookup(self, name):
        """Return the replacement for the name found."""
        if ':' in name:
            name, pattern = name.split(':')
            pattern = self.patterns[pattern]
        else:
            pattern = self.patterns[self.default_pattern]
        if name == '':
            name = '__pos%s' % self._pos
            self._pos += 1
        return '(?P<%s>%s)' % (name, pattern)

    def lastly(self, regex):
        """Process the result of __call__ right before it returns.
        
        Adds the ^ and the $ to the beginning and the end, respectively.
        """
        return "^%s$" % regex

    def openended(self, regex):
        """Process the result of ``__call__`` right before it returns.
        
        Adds the ^ to the beginning but no $ to the end.
        Called as a special alternative to lastly.
        """
        return "^%s" % regex

    def outermost_optionals_split(self, text):
        """Split out optional portions by outermost matching delims."""
        parts = []
        buffer = ""
        starts = ends = 0
        for c in text:
            if c == self.ostart:
                if starts == 0:
                    parts.append(buffer)
                    buffer = ""
                else:
                    buffer += c
                starts +=1
            elif c == self.oend:
                ends +=1
                if starts == ends:
                    parts.append(buffer)
                    buffer = ""
                    starts = ends = 0
                else:
                    buffer += c
            else:
                buffer += c
        if not starts == ends == 0:
            raise PathExpressionParserError(
                "Mismatch of optional portion delimiters."
            )
        parts.append(buffer)
        return parts

    def parse(self, text):
        """Turn a path expression into regex."""
        if self.ostart in text:
            parts = self.outermost_optionals_split(text)
            parts = map(self.parse, parts)
            parts[1::2] = ["(%s)?" % p for p in parts[1::2]]
        else:
            parts = [part.split(self.end) 
                     for part in text.split(self.start)]
            parts = [y for x in parts for y in x]
            parts[::2] = map(re.escape, parts[::2])
            parts[1::2] = map(self.lookup, parts[1::2])
        return ''.join(parts)

    def __call__(self, url_pattern):
        """Turn a path expression into regex via parse and lastly."""
        self._pos = 0
        if url_pattern.endswith('|'):
            return self.openended(self.parse(url_pattern[:-1]))
        else:    
            return self.lastly(self.parse(url_pattern))


class EnvironDispatcher(object):
    """Dispatch based on list of rules."""

    def __init__(self, rules):
        """Instantiate with a list of (predicate, wsgiapp) rules."""
        self.rules = rules

    def __call__(self, environ, start_response):
        """Call the first app whose predicate is true.
        
        Each predicate is passes the environ to evaluate.
        """
        for predicate, app in self.rules:
            if predicate(environ):
                return app(environ, start_response)


class MiddlewareComposer(object):
    """Compose middleware based on list of rules."""

    def __init__(self, app, rules):
        """Instantiate with an app and a list of rules."""
        self.app = app
        self.rules = rules

    def __call__(self, environ, start_response):
        """Apply each middleware whose predicate is true.
        
        Each predicate is passes the environ to evaluate.

        Given this set of rules::

            t = lambda x: True; f = lambda x: False
            [(t, a), (f, b), (t, c), (f, d), (t, e)]

        The app composed would be equivalent to this::

            a(c(e(app)))

        """
        app = self.app
        for predicate, middleware in reversed(self.rules):
            if predicate(environ):
                app = middleware(app)
        return app(environ, start_response)


def expose(obj):
    """Set obj._exposed = True and return obj."""
    obj._exposed = True
    return obj


class Naked(object):
    """Naked object style dispatch base class."""

    _not_found = staticmethod(not_found)
    _expose_all = True
    _exposed = True

    def _is_exposed(self, obj):
        """Determine if obj should be exposed.
        
        If ``self._expose_all`` is True, always return True.
        Otherwise, look at obj._exposed.
        """
        return self._expose_all or getattr(obj, '_exposed', False)
    
    def __call__(self, environ, start_response):
        """Dispatch to the method named by the next bit of PATH_INFO."""
        name = shift_path_info(dict(SCRIPT_NAME=environ['SCRIPT_NAME'],
                                    PATH_INFO=environ['PATH_INFO']))
        callable = getattr(self, name or 'index', None)
        if callable is not None and self._is_exposed(callable):
            shift_path_info(environ)
            return callable(environ, start_response)
        else:
            return self._not_found(environ, start_response)
    

class ByMethod(object):
    """Base class for dispatching to method named by ``REQUEST_METHOD``."""

    _method_not_allowed = staticmethod(method_not_allowed)
    
    def __call__(self, environ, start_response):
        """Dispatch based on REQUEST_METHOD."""
        environ['selector.methods'] = \
            [m for m in dir(self) if not m.startswith('_')]
        return getattr(self, 
                       environ['REQUEST_METHOD'], 
                       self._method_not_allowed)(environ, start_response)


def pliant(func):
    """Decorate an unbound wsgi callable taking args from
    ``wsgiorg.routing_args``
    ::

        @pliant
        def app(environ, start_response, arg1, arg2, foo='bar'):
            ...
    """
    def wsgi_func(environ, start_response):
        args, kwargs = environ.get('wsgiorg.routing_args', ([], {}))
        args = list(args)
        args.insert(0, start_response)
        args.insert(0, environ)
        return apply(func, args, dict(kwargs))
    return wsgi_func

        
def opliant(meth):
    """Decorate a bound wsgi callable taking args from
    ``wsgiorg.routing_args``
    ::

        class App(object):
            @opliant
            def __call__(self, environ, start_response, arg1, arg2, foo='bar'):
                ...
    """
    def wsgi_meth(self, environ, start_response):
        args, kwargs = environ.get('wsgiorg.routing_args', ([], {}))
        args = list(args)
        args.insert(0, start_response)
        args.insert(0, environ)
        args.insert(0, self)
        return apply(meth, args, dict(kwargs))
    return wsgi_meth

########NEW FILE########
__FILENAME__ = sparse
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""simple parser / string tokenizer
rather than returning a list of token types etc, we simple return a list
of tokens.  Each tokenizing function takes a string as input and returns
a list of tokens.
"""

# Copyright 2002, 2003 St James Software
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


def stringeval(text):
    """takes away repeated quotes (escapes) and returns the string
    represented by the text"""
    stringchar = text[0]
    if text[-1] != stringchar or stringchar not in ("'", '"'):
        # scratch your head
        raise ValueError("error parsing escaped string: %r" % text)
    return text[1:-1].replace((stringchar + stringchar), stringchar)


def stringquote(text):
    """escapes quotes as neccessary and returns a string representing
    the text"""
    if "'" in text:
        if '"' in text:
            return '"' + text.replace('"', '""') + '"'
        else:
            return '"' + text + '"'
    else:
        return "'" + text + "'"


class ParserError(ValueError):
    """Intelligent parser error"""

    def __init__(self, parser, message, tokennum):
        """takes a message and the number of the token that caused the error"""
        tokenpos = parser.findtokenpos(tokennum)
        line, charpos = parser.getlinepos(tokenpos)
        ValueError.__init__(self, "%s at line %d, char %d (token %r)" %
                                  (message, line, charpos, parser.tokens[tokennum]))
        self.parser = parser
        self.tokennum = tokennum


class SimpleParser:
    """this is a simple parser"""

    def __init__(self, defaulttokenlist=None, whitespacechars=" \t\r\n",
                 includewhitespacetokens=0):
        if defaulttokenlist is None:
            self.defaulttokenlist = ['<=', '>=', '==', '!=',
                                     '+=', '-=', '*=', '/=', '<>']
            self.defaulttokenlist.extend('(),[]:=+-')
        else:
            self.defaulttokenlist = defaulttokenlist
        self.whitespacechars = whitespacechars
        self.includewhitespacetokens = includewhitespacetokens
        self.standardtokenizers = [
            self.stringtokenize, self.removewhitespace, self.separatetokens
        ]
        self.quotechars = ('"', "'")
        self.endquotechars = {'"': '"', "'": "'"}
        self.stringescaping = 1

    def stringtokenize(self, text):
        """makes strings in text into tokens..."""
        tokens = []
        laststart = 0
        instring = 0
        endstringchar, escapechar = '', '\\'
        gotclose, gotescape = 0, 0
        for pos in range(len(text)):
            char = text[pos]
            if instring:
                if (self.stringescaping and
                    (gotescape or char == escapechar) and not gotclose):
                    gotescape = not gotescape
                elif char == endstringchar:
                    gotclose = not gotclose
                elif gotclose:
                    tokens.append(text[laststart:pos])
                    instring, laststart, endstringchar = 0, pos, ''
            if not instring:
                if char in self.quotechars:
                    if pos > laststart:
                        tokens.append(text[laststart:pos])
                    instring, laststart, endstringchar, gotclose = 1, pos, self.endquotechars[char], 0
        if laststart < len(text):
            tokens.append(text[laststart:])
        return tokens

    def keeptogether(self, text):
        """checks whether a token should be kept together"""
        return self.isstringtoken(text)

    def isstringtoken(self, text):
        """checks whether a token is a string token"""
        return text[:1] in self.quotechars

    def separatetokens(self, text, tokenlist=None):
        """this separates out tokens in tokenlist from whitespace etc"""
        if self.keeptogether(text):
            return [text]
        if tokenlist is None:
            tokenlist = self.defaulttokenlist
        # loop through and put tokens into a list
        tokens = []
        pos = 0
        laststart = 0
        lentext = len(text)
        while pos < lentext:
            foundtoken = 0
            for token in tokenlist:
                lentoken = len(token)
                if text[pos:pos+lentoken] == token:
                    if laststart < pos:
                        tokens.append(text[laststart:pos])
                    tokens.append(token)
                    pos += lentoken
                    foundtoken, laststart = 1, pos
                    break
            if not foundtoken:
                pos += 1
        if laststart < lentext:
            tokens.append(text[laststart:])
        return tokens

    def removewhitespace(self, text):
        """this removes whitespace but lets it separate things out into
        separate tokens"""
        if self.keeptogether(text):
            return [text]
        # loop through and put tokens into a list
        tokens = []
        pos = 0
        inwhitespace = 0
        laststart = 0
        for pos in range(len(text)):
            char = text[pos]
            if inwhitespace:
                if char not in self.whitespacechars:
                    if laststart < pos and self.includewhitespacetokens:
                        tokens.append(text[laststart:pos])
                    inwhitespace, laststart = 0, pos
            else:
                if char in self.whitespacechars:
                    if laststart < pos:
                        tokens.append(text[laststart:pos])
                    inwhitespace, laststart = 1, pos
        if (laststart < len(text) and
            (not inwhitespace or self.includewhitespacetokens)):
            tokens.append(text[laststart:])
        return tokens

    def applytokenizer(self, inputlist, tokenizer):
        """apply a tokenizer to a set of text, flattening the result"""
        tokenizedlists = [tokenizer(text) for text in inputlist]
        joined = []
        map(joined.extend, tokenizedlists)
        return joined

    def applytokenizers(self, inputlist, tokenizers):
        """apply a set of tokenizers to a set of text, flattening each time"""
        for tokenizer in tokenizers:
            inputlist = self.applytokenizer(inputlist, tokenizer)
        return inputlist

    def tokenize(self, source, tokenizers=None):
        """tokenize the text string with the standard tokenizers"""
        self.source = source
        if tokenizers is None:
            tokenizers = self.standardtokenizers
        self.tokens = self.applytokenizers([self.source], tokenizers)
        return self.tokens

    def findtokenpos(self, tokennum):
        """finds the position of the given token in the text"""
        currenttokenpos = 0
        for currenttokennum in range(tokennum + 1):
            currenttokenpos = self.source.find(self.tokens[currenttokennum],
                                               currenttokenpos)
        return currenttokenpos

    def getlinepos(self, tokenpos):
        """finds the line and character position of the given character"""
        sourcecut = self.source[:tokenpos]
        line = sourcecut.count("\n") + 1
        charpos = tokenpos - sourcecut.rfind("\n")
        return line, charpos

    def raiseerror(self, message, tokennum):
        """raises a ParserError"""
        raise ParserError(self, message, tokennum)

########NEW FILE########
__FILENAME__ = stdiotell
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A wrapper for sys.stdout etc that provides tell() for current position"""


class StdIOWrapper:

    def __init__(self, stream):
        self.stream = stream
        self.pos = 0
        self.closed = 0

    def __getattr__(self, attrname, default=None):
        return getattr(self.stream, attrname, default)

    def close(self):
        if not self.closed:
            self.closed = 1
            self.stream.close()

    def seek(self, pos, mode=0):
        raise ValueError("I/O operation on closed file")

    def tell(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        return self.pos

    def write(self, s):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.stream.write(s)
        self.pos += len(s)

    def writelines(self, lines):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.stream.writelines(lines)
        self.pos += len("".join(lines))

########NEW FILE########
__FILENAME__ = test_autoencode
#!/usr/bin/env python

import pytest

from translate.misc import autoencode


class TestAutoencode:
    type2test = autoencode.autoencode

    def test_default_encoding(self):
        """tests that conversion to string uses the encoding attribute"""
        s = self.type2test(u'unicode string', 'utf-8')
        assert s.encoding == 'utf-8'
        assert str(s) == 'unicode string'
        s = self.type2test(u'\u20ac')
        assert str(self.type2test(u'\u20ac', 'utf-8')) == '\xe2\x82\xac'

    def test_uniqueness(self):
        """tests constructor creates unique objects"""
        s1 = unicode(u'unicode string')
        s2 = unicode(u'unicode string')
        assert s1 == s2
        assert s1 is s2
        s1 = self.type2test(u'unicode string', 'utf-8')
        s2 = self.type2test(u'unicode string', 'ascii')
        s3 = self.type2test(u'unicode string', 'utf-8')
        assert s1 == s2 == s3
        assert s1 is not s2
        # even though all the attributes are the same, this is a mutable type
        # so the objects created must be different
        assert s1 is not s3

    def test_bad_encoding(self):
        """tests that we throw an exception if we don't know the encoding"""
        assert pytest.raises(ValueError, self.type2test, 'text', 'some-encoding')

########NEW FILE########
__FILENAME__ = test_dictutils
#!/usr/bin/env python

from translate.misc import dictutils


def test_add():
    d = dictutils.ordereddict()
    d[2] = 3
    assert len(d.order) == 1


def test_delete():
    d = dictutils.ordereddict()
    d[2] = 3
    del d[2]
    assert len(d.order) == 0


def test_pop():
    d = dictutils.ordereddict()
    d[2] = 3
    value = d.pop(2)
    assert len(d.order) == 0
    assert value == 3


def test_cidict_has_key():
    cid = dictutils.cidict()
    cid['lower'] = 'lowercase'
    assert 'lower' in cid
    assert 'Lower' in cid
    assert 'LOWER' in cid
    assert 'upper' not in cid

########NEW FILE########
__FILENAME__ = test_multistring
#!/usr/bin/env python

import pytest

from translate.misc import multistring, test_autoencode


class TestMultistring(test_autoencode.TestAutoencode):
    type2test = multistring.multistring

    def test_constructor(self):
        t = self.type2test
        s1 = t("test")
        assert type(s1) == t
        assert s1 == "test"
        assert s1.strings == ["test"]
        s2 = t(["test", "me"])
        assert type(s2) == t
        assert s2 == "test"
        assert s2.strings == ["test", "me"]
        assert s2 != s1
        pytest.raises(ValueError, t, [])

    def test_replace(self):
        t = self.type2test
        s1 = t(["abcdef", "def"])

        result = s1.replace("e", "")
        assert type(result) == t
        assert result == t(["abcdf", "df"])

        result = s1.replace("e", "xx")
        assert result == t(["abcdxxf", "dxxf"])

        result = s1.replace("e", u"\xe9")
        assert result == t([u"abcd\xe9f", u"d\xe9f"])

        result = s1.replace("e", "\n")
        assert result == t([u"abcd\nf", u"d\nf"])

        result = result.replace("\n", "\\n")
        assert result == t([u"abcd\\nf", u"d\\nf"])

        result = result.replace("\\n", "\n")
        assert result == t([u"abcd\nf", u"d\nf"])

        s2 = t(["abcdeef", "deef"])

        result = s2.replace("e", "g")
        assert result == t([u"abcdggf", u"dggf"])

        result = s2.replace("e", "g", 1)
        assert result == t([u"abcdgef", u"dgef"])

########NEW FILE########
__FILENAME__ = test_optrecurse
#!/usr/bin/env python

import os

from translate.misc import optrecurse


class TestRecursiveOptionParser:

    def test_splitext(self):
        """test the ``optrecurse.splitext`` function"""
        self.parser = optrecurse.RecursiveOptionParser({"txt": ("po", None)})
        name = "name"
        extension = "ext"
        filename = name + os.extsep + extension
        dirname = os.path.join("some", "path", "to")
        fullpath = os.path.join(dirname, filename)
        root = os.path.join(dirname, name)
        print(fullpath)
        assert self.parser.splitext(fullpath) == (root, extension)

########NEW FILE########
__FILENAME__ = test_progressbar
#!/usr/bin/env python

from translate.misc import progressbar


def test_hashprogressbar():
    """Test the [###   ] progress bar"""
    bar = progressbar.HashProgressBar()
    assert str(bar) == "[                                           ]   0%"
    bar.amount = 50
    assert str(bar) == "[######################                     ]  50%"
    bar.amount = 100
    assert str(bar) == "[###########################################] 100%"

########NEW FILE########
__FILENAME__ = test_quote
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.misc import quote


def test_find_all():
    """tests the find_all function"""
    assert quote.find_all("", "a") == []
    assert quote.find_all("a", "b") == []
    assert quote.find_all("a", "a") == [0]
    assert quote.find_all("aa", "a") == [0, 1]
    assert quote.find_all("abba", "ba") == [2]
    # check we skip the whole instance
    assert quote.find_all("banana", "ana") == [1]


def test_extract():
    """tests the extract function"""
    assert quote.extract("the <quoted> part", "<", ">", "\\", 0) == ("<quoted>", False)
    assert quote.extract("the 'quoted' part", "'", "'", "\\", 0) == ("'quoted'", False)
    assert quote.extract("the 'isn\\'t escaping fun' part", "'", "'", "\\", 0) == ("'isn\\'t escaping fun'", False)
    assert quote.extract("the 'isn\\'t something ", "'", "'", "\\", 0) == ("'isn\\'t something ", True)
    assert quote.extract("<quoted>\\", "<", ">", "\\", 0) == ("<quoted>", False)
    assert quote.extract("<quoted><again>", "<", ">", "\\", 0) == ("<quoted><again>", False)
    assert quote.extract("<quoted>\\\\<again>", "<", ">", "\\", 0) == ("<quoted><again>", False)
    assert quote.extract("<quoted\\>", "<", ">", "\\", 0) == ("<quoted\\>", True)
    assert quote.extract(' -->\n<!ENTITY blah "Some">', "<!--", "-->", None, 1) == (" -->", False)
    assert quote.extract('">\n', '"', '"', None, True) == ('"', False)


def test_extractwithoutquotes():
    """tests the extractwithoutquotes function"""
    assert quote.extractwithoutquotes("the <quoted> part", "<", ">", "\\", 0) == ("quoted", False)
    assert quote.extractwithoutquotes("the 'quoted' part", "'", "'", "\\", 0) == ("quoted", False)
    assert quote.extractwithoutquotes("the 'isn\\'t escaping fun' part", "'", "'", "\\", 0) == ("isn\\'t escaping fun", False)
    assert quote.extractwithoutquotes("the 'isn\\'t something ", "'", "'", "\\", 0) == ("isn\\'t something ", True)
    assert quote.extractwithoutquotes("<quoted>\\", "<", ">", "\\", 0) == ("quoted", False)
    assert quote.extractwithoutquotes("<quoted>\\\\<again>", "<", ">", "\\", 0) == ("quotedagain", False)
    assert quote.extractwithoutquotes("<quoted><again\\\\", "<", ">", "\\", 0, True) == ("quotedagain\\\\", True)
    # don't include escapes...
    assert quote.extractwithoutquotes("the 'isn\\'t escaping fun' part", "'", "'", "\\", 0, False) == ("isn't escaping fun", False)
    assert quote.extractwithoutquotes("the 'isn\\'t something ", "'", "'", "\\", 0, False) == ("isn't something ", True)
    assert quote.extractwithoutquotes("<quoted\\", "<", ">", "\\", 0, False) == ("quoted", True)
    assert quote.extractwithoutquotes("<quoted><again\\\\", "<", ">", "\\", 0, False) == ("quotedagain\\", True)
    # escaping of quote char
    assert quote.extractwithoutquotes("<quoted\\>", "<", ">", "\\", 0, False) == ("quoted>", True)


def isnewlineortabescape(escape):
    if escape == "\\n" or escape == "\\t":
        return escape
    return escape[-1]


def test_extractwithoutquotes_passfunc():
    """tests the extractwithoutquotes function with a function for includeescapes as a parameter"""
    assert quote.extractwithoutquotes("<test \\r \\n \\t \\\\>", "<", ">", "\\", 0, isnewlineortabescape) == ("test r \\n \\t \\", False)


def test_stripcomment():
    assert quote.stripcomment("<!-- Comment -->") == "Comment"


class TestEncoding:

    def test_javepropertiesencode(self):
        assert quote.javapropertiesencode(u"abc") == u"abc"
        assert quote.javapropertiesencode(u"abc") == "abc\u1E13"
        assert quote.javapropertiesencode(u"abc\n") == u"abc\\n"

    def test_mozillapropertiesencode(self):
        assert quote.mozillapropertiesencode(u"abc") == u"abc"
        assert quote.mozillapropertiesencode(u"abc") == u"abc"
        assert quote.mozillapropertiesencode(u"abc\n") == u"abc\\n"

    def test_escapespace(self):
        assert quote.escapespace(u" ") == u"\\u0020"
        assert quote.escapespace(u"\t") == u"\\u0009"

    def test_mozillaescapemarginspaces(self):
        assert quote.mozillaescapemarginspaces(u" ") == u""
        assert quote.mozillaescapemarginspaces(u"A") == u"A"
        assert quote.mozillaescapemarginspaces(u" abc ") == u"\\u0020abc\\u0020"
        assert quote.mozillaescapemarginspaces(u"  abc ") == u"\\u0020 abc\\u0020"

    def test_mozilla_control_escapes(self):
        r"""test that we do \uNNNN escapes for certain control characters instead of converting to UTF-8 characters"""
        prefix, suffix = "bling", "blang"
        for control in (u"\u0005", u"\u0006", u"\u0007", u"\u0011"):
            string = prefix + control + suffix
            assert quote.escapecontrols(string) == string

    def test_propertiesdecode(self):
        assert quote.propertiesdecode(u"abc") == u"abc"
        assert quote.propertiesdecode(u"abc\u1e13") == u"abc"
        assert quote.propertiesdecode(u"abc\u1E13") == u"abc"
        assert quote.propertiesdecode(u"abc\N{LEFT CURLY BRACKET}") == u"abc{"
        assert quote.propertiesdecode(u"abc\\") == u"abc\\"
        assert quote.propertiesdecode(u"abc\\") == u"abc\\"

    def test_properties_decode_slashu(self):
        assert quote.propertiesdecode(u"abc\u1e13") == u"abc"
        assert quote.propertiesdecode(u"abc\u0020") == u"abc "
        # NOTE Java only accepts 4 digit unicode, Mozilla accepts two
        # unfortunately, but it seems harmless to accept both.
        assert quote.propertiesdecode("abc\u20") == u"abc "

    def _html_encoding_helper(self, pairs):
        for from_, to in pairs:
            assert quote.htmlentityencode(from_) == to
            assert quote.htmlentitydecode(to) == from_

    def test_htmlencoding(self):
        """test that we can encode and decode simple HTML entities"""
        raw_encoded = [(u"", u"&euro;"), (u"", u"&copy;"), (u'"', u"&quot;")]
        self._html_encoding_helper(raw_encoded)

    def test_htmlencoding_existing_entities(self):
        """test that we don't mess existing entities"""
        assert quote.htmlentityencode(u"&amp;") == u"&amp;"

    def test_htmlencoding_passthrough(self):
        """test that we can encode and decode things that look like HTML entities but aren't"""
        raw_encoded = [(u"copy quot", u"copy quot"),]     # Raw text should have nothing done to it.
        self._html_encoding_helper(raw_encoded)

    def test_htmlencoding_nonentities(self):
        """tests to give us full coverage"""
        for encoded, real in [(u"Some &; text", u"Some &; text"),
                              (u"&copy ", u"&copy "),
                              (u"&copy", u"&copy"),
                              (u"&rogerrabbit;", u"&rogerrabbit;"),]:
            assert quote.htmlentitydecode(encoded) == real

        for decoded, real in [(u"Some &; text", u"Some &; text"),
                              (u"&copy ", u"&amp;copy "),
                              (u"&copy", u"&amp;copy"),
                              (u"&rogerrabbit;", u"&rogerrabbit;"),]:
            assert quote.htmlentityencode(decoded) == real

########NEW FILE########
__FILENAME__ = wsgi
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Wrapper to launch the bundled CherryPy server."""

import logging


def launch_server(host, port, app, **kwargs):
    """Use CherryPy's WSGI server, a multithreaded scallable server."""
    from translate.misc.wsgiserver import CherryPyWSGIServer

    server = CherryPyWSGIServer((host, port), app, **kwargs)
    logging.info("Starting CherryPy server, listening on port %s", port)
    try:
        server.start()
    except KeyboardInterrupt:
        server.stop()

########NEW FILE########
__FILENAME__ = ssl_builtin
"""A library for integrating Python's builtin ``ssl`` library with CherryPy.

The ssl module must be importable for SSL functionality.

To use this module, set ``CherryPyWSGIServer.ssl_adapter`` to an instance of
``BuiltinSSLAdapter``.
"""

try:
    import ssl
except ImportError:
    ssl = None

try:
    from _pyio import DEFAULT_BUFFER_SIZE
except ImportError:
    try:
        from io import DEFAULT_BUFFER_SIZE
    except ImportError:
        DEFAULT_BUFFER_SIZE = -1

import sys

from cherrypy import wsgiserver


class BuiltinSSLAdapter(wsgiserver.SSLAdapter):
    """A wrapper for integrating Python's builtin ssl module with CherryPy."""

    certificate = None
    """The filename of the server SSL certificate."""

    private_key = None
    """The filename of the server's private key file."""

    def __init__(self, certificate, private_key, certificate_chain=None):
        if ssl is None:
            raise ImportError("You must install the ssl module to use HTTPS.")
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain

    def bind(self, sock):
        """Wrap and return the given socket."""
        return sock

    def wrap(self, sock):
        """Wrap and return the given socket, plus WSGI environ entries."""
        try:
            s = ssl.wrap_socket(sock, do_handshake_on_connect=True,
                    server_side=True, certfile=self.certificate,
                    keyfile=self.private_key, ssl_version=ssl.PROTOCOL_SSLv23)
        except ssl.SSLError:
            e = sys.exc_info()[1]
            if e.errno == ssl.SSL_ERROR_EOF:
                # This is almost certainly due to the cherrypy engine
                # 'pinging' the socket to assert it's connectable;
                # the 'ping' isn't SSL.
                return None, {}
            elif e.errno == ssl.SSL_ERROR_SSL:
                if e.args[1].endswith('http request'):
                    # The client is speaking HTTP to an HTTPS server.
                    raise wsgiserver.NoSSLError
                elif e.args[1].endswith('unknown protocol'):
                    # The client is speaking some non-HTTP protocol.
                    # Drop the conn.
                    return None, {}
            raise
        return s, self.get_environ(s)

    # TODO: fill this out more with mod ssl env
    def get_environ(self, sock):
        """Create WSGI environ entries to be merged into each request."""
        cipher = sock.cipher()
        ssl_environ = {
            "wsgi.url_scheme": "https",
            "HTTPS": "on",
            'SSL_PROTOCOL': cipher[1],
            'SSL_CIPHER': cipher[0]
##            SSL_VERSION_INTERFACE 	string 	The mod_ssl program version
##            SSL_VERSION_LIBRARY 	string 	The OpenSSL program version
            }
        return ssl_environ

    if sys.version_info >= (3, 0):
        def makefile(self, sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):
            return wsgiserver.CP_makefile(sock, mode, bufsize)
    else:
        def makefile(self, sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):
            return wsgiserver.CP_fileobject(sock, mode, bufsize)


########NEW FILE########
__FILENAME__ = ssl_pyopenssl
"""A library for integrating pyOpenSSL with CherryPy.

The OpenSSL module must be importable for SSL functionality.
You can obtain it from http://pyopenssl.sourceforge.net/

To use this module, set CherryPyWSGIServer.ssl_adapter to an instance of
SSLAdapter. There are two ways to use SSL:

Method One
----------

 * ``ssl_adapter.context``: an instance of SSL.Context.

If this is not None, it is assumed to be an SSL.Context instance,
and will be passed to SSL.Connection on bind(). The developer is
responsible for forming a valid Context object. This approach is
to be preferred for more flexibility, e.g. if the cert and key are
streams instead of files, or need decryption, or SSL.SSLv3_METHOD
is desired instead of the default SSL.SSLv23_METHOD, etc. Consult
the pyOpenSSL documentation for complete options.

Method Two (shortcut)
---------------------

 * ``ssl_adapter.certificate``: the filename of the server SSL certificate.
 * ``ssl_adapter.private_key``: the filename of the server's private key file.

Both are None by default. If ssl_adapter.context is None, but .private_key
and .certificate are both given and valid, they will be read, and the
context will be automatically created from them.
"""

import socket
import threading
import time

from cherrypy import wsgiserver

try:
    from OpenSSL import SSL
    from OpenSSL import crypto
except ImportError:
    SSL = None


class SSL_fileobject(wsgiserver.CP_fileobject):
    """SSL file object attached to a socket object."""

    ssl_timeout = 3
    ssl_retry = .01

    def _safe_call(self, is_reader, call, *args, **kwargs):
        """Wrap the given call with SSL error-trapping.

        is_reader: if False EOF errors will be raised. If True, EOF errors
        will return "" (to emulate normal sockets).
        """
        start = time.time()
        while True:
            try:
                return call(*args, **kwargs)
            except SSL.WantReadError:
                # Sleep and try again. This is dangerous, because it means
                # the rest of the stack has no way of differentiating
                # between a "new handshake" error and "client dropped".
                # Note this isn't an endless loop: there's a timeout below.
                time.sleep(self.ssl_retry)
            except SSL.WantWriteError:
                time.sleep(self.ssl_retry)
            except SSL.SysCallError, e:
                if is_reader and e.args == (-1, 'Unexpected EOF'):
                    return ""

                errnum = e.args[0]
                if is_reader and errnum in wsgiserver.socket_errors_to_ignore:
                    return ""
                raise socket.error(errnum)
            except SSL.Error, e:
                if is_reader and e.args == (-1, 'Unexpected EOF'):
                    return ""

                thirdarg = None
                try:
                    thirdarg = e.args[0][0][2]
                except IndexError:
                    pass

                if thirdarg == 'http request':
                    # The client is talking HTTP to an HTTPS server.
                    raise wsgiserver.NoSSLError()

                raise wsgiserver.FatalSSLAlert(*e.args)
            except:
                raise

            if time.time() - start > self.ssl_timeout:
                raise socket.timeout("timed out")

    def recv(self, *args, **kwargs):
        buf = []
        r = super(SSL_fileobject, self).recv
        while True:
            data = self._safe_call(True, r, *args, **kwargs)
            buf.append(data)
            p = self._sock.pending()
            if not p:
                return "".join(buf)

    def sendall(self, *args, **kwargs):
        return self._safe_call(False, super(SSL_fileobject, self).sendall,
                               *args, **kwargs)

    def send(self, *args, **kwargs):
        return self._safe_call(False, super(SSL_fileobject, self).send,
                               *args, **kwargs)


class SSLConnection:
    """A thread-safe wrapper for an SSL.Connection.

    ``*args``: the arguments to create the wrapped ``SSL.Connection(*args)``.
    """

    def __init__(self, *args):
        self._ssl_conn = SSL.Connection(*args)
        self._lock = threading.RLock()

    for f in ('get_context', 'pending', 'send', 'write', 'recv', 'read',
              'renegotiate', 'bind', 'listen', 'connect', 'accept',
              'setblocking', 'fileno', 'close', 'get_cipher_list',
              'getpeername', 'getsockname', 'getsockopt', 'setsockopt',
              'makefile', 'get_app_data', 'set_app_data', 'state_string',
              'sock_shutdown', 'get_peer_certificate', 'want_read',
              'want_write', 'set_connect_state', 'set_accept_state',
              'connect_ex', 'sendall', 'settimeout', 'gettimeout'):
        exec("""def %s(self, *args):
        self._lock.acquire()
        try:
            return self._ssl_conn.%s(*args)
        finally:
            self._lock.release()
""" % (f, f))

    def shutdown(self, *args):
        self._lock.acquire()
        try:
            # pyOpenSSL.socket.shutdown takes no args
            return self._ssl_conn.shutdown()
        finally:
            self._lock.release()


class pyOpenSSLAdapter(wsgiserver.SSLAdapter):
    """A wrapper for integrating pyOpenSSL with CherryPy."""

    context = None
    """An instance of SSL.Context."""

    certificate = None
    """The filename of the server SSL certificate."""

    private_key = None
    """The filename of the server's private key file."""

    certificate_chain = None
    """Optional. The filename of CA's intermediate certificate bundle.

    This is needed for cheaper "chained root" SSL certificates, and should be
    left as None if not required."""

    def __init__(self, certificate, private_key, certificate_chain=None):
        if SSL is None:
            raise ImportError("You must install pyOpenSSL to use HTTPS.")

        self.context = None
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain
        self._environ = None

    def bind(self, sock):
        """Wrap and return the given socket."""
        if self.context is None:
            self.context = self.get_context()
        conn = SSLConnection(self.context, sock)
        self._environ = self.get_environ()
        return conn

    def wrap(self, sock):
        """Wrap and return the given socket, plus WSGI environ entries."""
        return sock, self._environ.copy()

    def get_context(self):
        """Return an SSL.Context from self attributes."""
        # See http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/442473
        c = SSL.Context(SSL.SSLv23_METHOD)
        c.use_privatekey_file(self.private_key)
        if self.certificate_chain:
            c.load_verify_locations(self.certificate_chain)
        c.use_certificate_file(self.certificate)
        return c

    def get_environ(self):
        """Return WSGI environ entries to be merged into each request."""
        ssl_environ = {
            "HTTPS": "on",
            # pyOpenSSL doesn't provide access to any of these AFAICT
##            'SSL_PROTOCOL': 'SSLv2',
##            SSL_CIPHER 	string 	The cipher specification name
##            SSL_VERSION_INTERFACE 	string 	The mod_ssl program version
##            SSL_VERSION_LIBRARY 	string 	The OpenSSL program version
            }

        if self.certificate:
            # Server certificate attributes
            cert = open(self.certificate, 'rb').read()
            cert = crypto.load_certificate(crypto.FILETYPE_PEM, cert)
            ssl_environ.update({
                'SSL_SERVER_M_VERSION': cert.get_version(),
                'SSL_SERVER_M_SERIAL': cert.get_serial_number(),
##                'SSL_SERVER_V_START': Validity of server's certificate (start time),
##                'SSL_SERVER_V_END': Validity of server's certificate (end time),
                })

            for prefix, dn in [("I", cert.get_issuer()),
                               ("S", cert.get_subject())]:
                # X509Name objects don't seem to have a way to get the
                # complete DN string. Use str() and slice it instead,
                # because str(dn) == "<X509Name object '/C=US/ST=...'>"
                dnstr = str(dn)[18:-2]

                wsgikey = 'SSL_SERVER_%s_DN' % prefix
                ssl_environ[wsgikey] = dnstr

                # The DN should be of the form: /k1=v1/k2=v2, but we must allow
                # for any value to contain slashes itself (in a URL).
                while dnstr:
                    pos = dnstr.rfind("=")
                    dnstr, value = dnstr[:pos], dnstr[pos + 1:]
                    pos = dnstr.rfind("/")
                    dnstr, key = dnstr[:pos], dnstr[pos + 1:]
                    if key and value:
                        wsgikey = 'SSL_SERVER_%s_DN_%s' % (prefix, key)
                        ssl_environ[wsgikey] = value

        return ssl_environ

    def makefile(self, sock, mode='r', bufsize=-1):
        if SSL and isinstance(sock, SSL.ConnectionType):
            timeout = sock.gettimeout()
            f = SSL_fileobject(sock, mode, bufsize)
            f.ssl_timeout = timeout
            return f
        else:
            return wsgiserver.CP_fileobject(sock, mode, bufsize)


########NEW FILE########
__FILENAME__ = wsgiserver2
"""A high-speed, production ready, thread pooled, generic HTTP server.

Simplest example on how to use this module directly
(without using CherryPy's application machinery)::

    from cherrypy import wsgiserver

    def my_crazy_app(environ, start_response):
        status = '200 OK'
        response_headers = [('Content-type','text/plain')]
        start_response(status, response_headers)
        return ['Hello world!']

    server = wsgiserver.CherryPyWSGIServer(
                ('0.0.0.0', 8070), my_crazy_app,
                server_name='www.cherrypy.example')
    server.start()

The CherryPy WSGI server can serve as many WSGI applications
as you want in one instance by using a WSGIPathInfoDispatcher::

    d = WSGIPathInfoDispatcher({'/': my_crazy_app, '/blog': my_blog_app})
    server = wsgiserver.CherryPyWSGIServer(('0.0.0.0', 80), d)

Want SSL support? Just set server.ssl_adapter to an SSLAdapter instance.

This won't call the CherryPy engine (application side) at all, only the
HTTP server, which is independent from the rest of CherryPy. Don't
let the name "CherryPyWSGIServer" throw you; the name merely reflects
its origin, not its coupling.

For those of you wanting to understand internals of this module, here's the
basic call flow. The server's listening thread runs a very tight loop,
sticking incoming connections onto a Queue::

    server = CherryPyWSGIServer(...)
    server.start()
    while True:
        tick()
        # This blocks until a request comes in:
        child = socket.accept()
        conn = HTTPConnection(child, ...)
        server.requests.put(conn)

Worker threads are kept in a pool and poll the Queue, popping off and then
handling each connection in turn. Each connection can consist of an arbitrary
number of requests and their responses, so we run a nested loop::

    while True:
        conn = server.requests.get()
        conn.communicate()
        ->  while True:
                req = HTTPRequest(...)
                req.parse_request()
                ->  # Read the Request-Line, e.g. "GET /page HTTP/1.1"
                    req.rfile.readline()
                    read_headers(req.rfile, req.inheaders)
                req.respond()
                ->  response = app(...)
                    try:
                        for chunk in response:
                            if chunk:
                                req.write(chunk)
                    finally:
                        if hasattr(response, "close"):
                            response.close()
                if req.close_connection:
                    return
"""

__all__ = ['HTTPRequest', 'HTTPConnection', 'HTTPServer',
           'SizeCheckWrapper', 'KnownLengthRFile', 'ChunkedRFile',
           'CP_fileobject',
           'MaxSizeExceeded', 'NoSSLError', 'FatalSSLAlert',
           'WorkerThread', 'ThreadPool', 'SSLAdapter',
           'CherryPyWSGIServer',
           'Gateway', 'WSGIGateway', 'WSGIGateway_10', 'WSGIGateway_u0',
           'WSGIPathInfoDispatcher', 'get_ssl_adapter_class']

import os
try:
    import queue
except:
    import Queue as queue
import re
import rfc822
import socket
import sys
if 'win' in sys.platform and not hasattr(socket, 'IPPROTO_IPV6'):
    socket.IPPROTO_IPV6 = 41
try:
    import cStringIO as StringIO
except ImportError:
    import StringIO
DEFAULT_BUFFER_SIZE = -1

_fileobject_uses_str_type = isinstance(socket._fileobject(None)._rbuf, basestring)

import threading
import time
import traceback
def format_exc(limit=None):
    """Like print_exc() but return a string. Backport for Python 2.3."""
    try:
        etype, value, tb = sys.exc_info()
        return ''.join(traceback.format_exception(etype, value, tb, limit))
    finally:
        etype = value = tb = None

import operator

from urllib import unquote
import warnings

if sys.version_info >= (3, 0):
    bytestr = bytes
    unicodestr = str
    basestring = (bytes, str)
    def ntob(n, encoding='ISO-8859-1'):
        """Return the given native string as a byte string in the given encoding."""
        # In Python 3, the native string type is unicode
        return n.encode(encoding)
else:
    bytestr = str
    unicodestr = unicode
    basestring = basestring
    def ntob(n, encoding='ISO-8859-1'):
        """Return the given native string as a byte string in the given encoding."""
        # In Python 2, the native string type is bytes. Assume it's already
        # in the given encoding, which for ISO-8859-1 is almost always what
        # was intended.
        return n

LF = ntob('\n')
CRLF = ntob('\r\n')
TAB = ntob('\t')
SPACE = ntob(' ')
COLON = ntob(':')
SEMICOLON = ntob(';')
EMPTY = ntob('')
NUMBER_SIGN = ntob('#')
QUESTION_MARK = ntob('?')
ASTERISK = ntob('*')
FORWARD_SLASH = ntob('/')
quoted_slash = re.compile(ntob("(?i)%2F"))

import errno

def plat_specific_errors(*errnames):
    """Return error numbers for all errors in errnames on this platform.

    The 'errno' module contains different global constants depending on
    the specific platform (OS). This function will return the list of
    numeric values for a given list of potential names.
    """
    errno_names = dir(errno)
    nums = [getattr(errno, k) for k in errnames if k in errno_names]
    # de-dupe the list
    return list(dict.fromkeys(nums).keys())

socket_error_eintr = plat_specific_errors("EINTR", "WSAEINTR")

socket_errors_to_ignore = plat_specific_errors(
    "EPIPE",
    "EBADF", "WSAEBADF",
    "ENOTSOCK", "WSAENOTSOCK",
    "ETIMEDOUT", "WSAETIMEDOUT",
    "ECONNREFUSED", "WSAECONNREFUSED",
    "ECONNRESET", "WSAECONNRESET",
    "ECONNABORTED", "WSAECONNABORTED",
    "ENETRESET", "WSAENETRESET",
    "EHOSTDOWN", "EHOSTUNREACH",
    )
socket_errors_to_ignore.append("timed out")
socket_errors_to_ignore.append("The read operation timed out")

socket_errors_nonblocking = plat_specific_errors(
    'EAGAIN', 'EWOULDBLOCK', 'WSAEWOULDBLOCK')

comma_separated_headers = [ntob(h) for h in
    ['Accept', 'Accept-Charset', 'Accept-Encoding',
     'Accept-Language', 'Accept-Ranges', 'Allow', 'Cache-Control',
     'Connection', 'Content-Encoding', 'Content-Language', 'Expect',
     'If-Match', 'If-None-Match', 'Pragma', 'Proxy-Authenticate', 'TE',
     'Trailer', 'Transfer-Encoding', 'Upgrade', 'Vary', 'Via', 'Warning',
     'WWW-Authenticate']]


import logging
if not hasattr(logging, 'statistics'): logging.statistics = {}


def read_headers(rfile, hdict=None):
    """Read headers from the given stream into the given header dict.

    If hdict is None, a new header dict is created. Returns the populated
    header dict.

    Headers which are repeated are folded together using a comma if their
    specification so dictates.

    This function raises ValueError when the read bytes violate the HTTP spec.
    You should probably return "400 Bad Request" if this happens.
    """
    if hdict is None:
        hdict = {}

    while True:
        line = rfile.readline()
        if not line:
            # No more data--illegal end of headers
            raise ValueError("Illegal end of headers.")

        if line == CRLF:
            # Normal end of headers
            break
        if not line.endswith(CRLF):
            raise ValueError("HTTP requires CRLF terminators")

        if line[0] in (SPACE, TAB):
            # It's a continuation line.
            v = line.strip()
        else:
            try:
                k, v = line.split(COLON, 1)
            except ValueError:
                raise ValueError("Illegal header line.")
            # TODO: what about TE and WWW-Authenticate?
            k = k.strip().title()
            v = v.strip()
            hname = k

        if k in comma_separated_headers:
            existing = hdict.get(hname)
            if existing:
                v = ", ".join((existing, v))
        hdict[hname] = v

    return hdict


class MaxSizeExceeded(Exception):
    pass

class SizeCheckWrapper(object):
    """Wraps a file-like object, raising MaxSizeExceeded if too large."""

    def __init__(self, rfile, maxlen):
        self.rfile = rfile
        self.maxlen = maxlen
        self.bytes_read = 0

    def _check_length(self):
        if self.maxlen and self.bytes_read > self.maxlen:
            raise MaxSizeExceeded()

    def read(self, size=None):
        data = self.rfile.read(size)
        self.bytes_read += len(data)
        self._check_length()
        return data

    def readline(self, size=None):
        if size is not None:
            data = self.rfile.readline(size)
            self.bytes_read += len(data)
            self._check_length()
            return data

        # User didn't specify a size ...
        # We read the line in chunks to make sure it's not a 100MB line !
        res = []
        while True:
            data = self.rfile.readline(256)
            self.bytes_read += len(data)
            self._check_length()
            res.append(data)
            # See http://www.cherrypy.org/ticket/421
            if len(data) < 256 or data[-1:] == "\n":
                return EMPTY.join(res)

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline()
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline()
        return lines

    def close(self):
        self.rfile.close()

    def __iter__(self):
        return self

    def __next__(self):
        data = next(self.rfile)
        self.bytes_read += len(data)
        self._check_length()
        return data

    def next(self):
        data = self.rfile.next()
        self.bytes_read += len(data)
        self._check_length()
        return data


class KnownLengthRFile(object):
    """Wraps a file-like object, returning an empty string when exhausted."""

    def __init__(self, rfile, content_length):
        self.rfile = rfile
        self.remaining = content_length

    def read(self, size=None):
        if self.remaining == 0:
            return ''
        if size is None:
            size = self.remaining
        else:
            size = min(size, self.remaining)

        data = self.rfile.read(size)
        self.remaining -= len(data)
        return data

    def readline(self, size=None):
        if self.remaining == 0:
            return ''
        if size is None:
            size = self.remaining
        else:
            size = min(size, self.remaining)

        data = self.rfile.readline(size)
        self.remaining -= len(data)
        return data

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline(sizehint)
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)
        return lines

    def close(self):
        self.rfile.close()

    def __iter__(self):
        return self

    def __next__(self):
        data = next(self.rfile)
        self.remaining -= len(data)
        return data


class ChunkedRFile(object):
    """Wraps a file-like object, returning an empty string when exhausted.

    This class is intended to provide a conforming wsgi.input value for
    request entities that have been encoded with the 'chunked' transfer
    encoding.
    """

    def __init__(self, rfile, maxlen, bufsize=8192):
        self.rfile = rfile
        self.maxlen = maxlen
        self.bytes_read = 0
        self.buffer = EMPTY
        self.bufsize = bufsize
        self.closed = False

    def _fetch(self):
        if self.closed:
            return

        line = self.rfile.readline()
        self.bytes_read += len(line)

        if self.maxlen and self.bytes_read > self.maxlen:
            raise MaxSizeExceeded("Request Entity Too Large", self.maxlen)

        line = line.strip().split(SEMICOLON, 1)

        try:
            chunk_size = line.pop(0)
            chunk_size = int(chunk_size, 16)
        except ValueError:
            raise ValueError("Bad chunked transfer size: " + repr(chunk_size))

        if chunk_size <= 0:
            self.closed = True
            return

##            if line: chunk_extension = line[0]

        if self.maxlen and self.bytes_read + chunk_size > self.maxlen:
            raise IOError("Request Entity Too Large")

        chunk = self.rfile.read(chunk_size)
        self.bytes_read += len(chunk)
        self.buffer += chunk

        crlf = self.rfile.read(2)
        if crlf != CRLF:
            raise ValueError(
                 "Bad chunked transfer coding (expected '\\r\\n', "
                 "got " + repr(crlf) + ")")

    def read(self, size=None):
        data = EMPTY
        while True:
            if size and len(data) >= size:
                return data

            if not self.buffer:
                self._fetch()
                if not self.buffer:
                    # EOF
                    return data

            if size:
                remaining = size - len(data)
                data += self.buffer[:remaining]
                self.buffer = self.buffer[remaining:]
            else:
                data += self.buffer

    def readline(self, size=None):
        data = EMPTY
        while True:
            if size and len(data) >= size:
                return data

            if not self.buffer:
                self._fetch()
                if not self.buffer:
                    # EOF
                    return data

            newline_pos = self.buffer.find(LF)
            if size:
                if newline_pos == -1:
                    remaining = size - len(data)
                    data += self.buffer[:remaining]
                    self.buffer = self.buffer[remaining:]
                else:
                    remaining = min(size - len(data), newline_pos)
                    data += self.buffer[:remaining]
                    self.buffer = self.buffer[remaining:]
            else:
                if newline_pos == -1:
                    data += self.buffer
                else:
                    data += self.buffer[:newline_pos]
                    self.buffer = self.buffer[newline_pos:]

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline(sizehint)
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)
        return lines

    def read_trailer_lines(self):
        if not self.closed:
            raise ValueError(
                "Cannot read trailers until the request body has been read.")

        while True:
            line = self.rfile.readline()
            if not line:
                # No more data--illegal end of headers
                raise ValueError("Illegal end of headers.")

            self.bytes_read += len(line)
            if self.maxlen and self.bytes_read > self.maxlen:
                raise IOError("Request Entity Too Large")

            if line == CRLF:
                # Normal end of headers
                break
            if not line.endswith(CRLF):
                raise ValueError("HTTP requires CRLF terminators")

            yield line

    def close(self):
        self.rfile.close()

    def __iter__(self):
        # Shamelessly stolen from StringIO
        total = 0
        line = self.readline(sizehint)
        while line:
            yield line
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)


class HTTPRequest(object):
    """An HTTP Request (and response).

    A single HTTP connection may consist of multiple request/response pairs.
    """

    server = None
    """The HTTPServer object which is receiving this request."""

    conn = None
    """The HTTPConnection object on which this request connected."""

    inheaders = {}
    """A dict of request headers."""

    outheaders = []
    """A list of header tuples to write in the response."""

    ready = False
    """When True, the request has been parsed and is ready to begin generating
    the response. When False, signals the calling Connection that the response
    should not be generated and the connection should close."""

    close_connection = False
    """Signals the calling Connection that the request should close. This does
    not imply an error! The client and/or server may each request that the
    connection be closed."""

    chunked_write = False
    """If True, output will be encoded with the "chunked" transfer-coding.

    This value is set automatically inside send_headers."""

    def __init__(self, server, conn):
        self.server= server
        self.conn = conn

        self.ready = False
        self.started_request = False
        self.scheme = ntob("http")
        if self.server.ssl_adapter is not None:
            self.scheme = ntob("https")
        # Use the lowest-common protocol in case read_request_line errors.
        self.response_protocol = 'HTTP/1.0'
        self.inheaders = {}

        self.status = ""
        self.outheaders = []
        self.sent_headers = False
        self.close_connection = self.__class__.close_connection
        self.chunked_read = False
        self.chunked_write = self.__class__.chunked_write

    def parse_request(self):
        """Parse the next HTTP request start-line and message-headers."""
        self.rfile = SizeCheckWrapper(self.conn.rfile,
                                      self.server.max_request_header_size)
        try:
            success = self.read_request_line()
        except MaxSizeExceeded:
            self.simple_response("414 Request-URI Too Long",
                "The Request-URI sent with the request exceeds the maximum "
                "allowed bytes.")
            return
        else:
            if not success:
                return

        try:
            success = self.read_request_headers()
        except MaxSizeExceeded:
            self.simple_response("413 Request Entity Too Large",
                "The headers sent with the request exceed the maximum "
                "allowed bytes.")
            return
        else:
            if not success:
                return

        self.ready = True

    def read_request_line(self):
        # HTTP/1.1 connections are persistent by default. If a client
        # requests a page, then idles (leaves the connection open),
        # then rfile.readline() will raise socket.error("timed out").
        # Note that it does this based on the value given to settimeout(),
        # and doesn't need the client to request or acknowledge the close
        # (although your TCP stack might suffer for it: cf Apache's history
        # with FIN_WAIT_2).
        request_line = self.rfile.readline()

        # Set started_request to True so communicate() knows to send 408
        # from here on out.
        self.started_request = True
        if not request_line:
            return False

        if request_line == CRLF:
            # RFC 2616 sec 4.1: "...if the server is reading the protocol
            # stream at the beginning of a message and receives a CRLF
            # first, it should ignore the CRLF."
            # But only ignore one leading line! else we enable a DoS.
            request_line = self.rfile.readline()
            if not request_line:
                return False

        if not request_line.endswith(CRLF):
            self.simple_response("400 Bad Request", "HTTP requires CRLF terminators")
            return False

        try:
            method, uri, req_protocol = request_line.strip().split(SPACE, 2)
            rp = int(req_protocol[5]), int(req_protocol[7])
        except (ValueError, IndexError):
            self.simple_response("400 Bad Request", "Malformed Request-Line")
            return False

        self.uri = uri
        self.method = method

        # uri may be an abs_path (including "http://host.domain.tld");
        scheme, authority, path = self.parse_request_uri(uri)
        if NUMBER_SIGN in path:
            self.simple_response("400 Bad Request",
                                 "Illegal #fragment in Request-URI.")
            return False

        if scheme:
            self.scheme = scheme

        qs = EMPTY
        if QUESTION_MARK in path:
            path, qs = path.split(QUESTION_MARK, 1)

        # Unquote the path+params (e.g. "/this%20path" -> "/this path").
        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2
        #
        # But note that "...a URI must be separated into its components
        # before the escaped characters within those components can be
        # safely decoded." http://www.ietf.org/rfc/rfc2396.txt, sec 2.4.2
        # Therefore, "/this%2Fpath" becomes "/this%2Fpath", not "/this/path".
        try:
            atoms = [unquote(x) for x in quoted_slash.split(path)]
        except ValueError:
            ex = sys.exc_info()[1]
            self.simple_response("400 Bad Request", ex.args[0])
            return False
        path = "%2F".join(atoms)
        self.path = path

        # Note that, like wsgiref and most other HTTP servers,
        # we "% HEX HEX"-unquote the path but not the query string.
        self.qs = qs

        # Compare request and server HTTP protocol versions, in case our
        # server does not support the requested protocol. Limit our output
        # to min(req, server). We want the following output:
        #     request    server     actual written   supported response
        #     protocol   protocol  response protocol    feature set
        # a     1.0        1.0           1.0                1.0
        # b     1.0        1.1           1.1                1.0
        # c     1.1        1.0           1.0                1.0
        # d     1.1        1.1           1.1                1.1
        # Notice that, in (b), the response will be "HTTP/1.1" even though
        # the client only understands 1.0. RFC 2616 10.5.6 says we should
        # only return 505 if the _major_ version is different.
        sp = int(self.server.protocol[5]), int(self.server.protocol[7])

        if sp[0] != rp[0]:
            self.simple_response("505 HTTP Version Not Supported")
            return False

        self.request_protocol = req_protocol
        self.response_protocol = "HTTP/%s.%s" % min(rp, sp)

        return True

    def read_request_headers(self):
        """Read self.rfile into self.inheaders. Return success."""

        # then all the http headers
        try:
            read_headers(self.rfile, self.inheaders)
        except ValueError:
            ex = sys.exc_info()[1]
            self.simple_response("400 Bad Request", ex.args[0])
            return False

        mrbs = self.server.max_request_body_size
        if mrbs and int(self.inheaders.get("Content-Length", 0)) > mrbs:
            self.simple_response("413 Request Entity Too Large",
                "The entity sent with the request exceeds the maximum "
                "allowed bytes.")
            return False

        # Persistent connection support
        if self.response_protocol == "HTTP/1.1":
            # Both server and client are HTTP/1.1
            if self.inheaders.get("Connection", "") == "close":
                self.close_connection = True
        else:
            # Either the server or client (or both) are HTTP/1.0
            if self.inheaders.get("Connection", "") != "Keep-Alive":
                self.close_connection = True

        # Transfer-Encoding support
        te = None
        if self.response_protocol == "HTTP/1.1":
            te = self.inheaders.get("Transfer-Encoding")
            if te:
                te = [x.strip().lower() for x in te.split(",") if x.strip()]

        self.chunked_read = False

        if te:
            for enc in te:
                if enc == "chunked":
                    self.chunked_read = True
                else:
                    # Note that, even if we see "chunked", we must reject
                    # if there is an extension we don't recognize.
                    self.simple_response("501 Unimplemented")
                    self.close_connection = True
                    return False

        # From PEP 333:
        # "Servers and gateways that implement HTTP 1.1 must provide
        # transparent support for HTTP 1.1's "expect/continue" mechanism.
        # This may be done in any of several ways:
        #   1. Respond to requests containing an Expect: 100-continue request
        #      with an immediate "100 Continue" response, and proceed normally.
        #   2. Proceed with the request normally, but provide the application
        #      with a wsgi.input stream that will send the "100 Continue"
        #      response if/when the application first attempts to read from
        #      the input stream. The read request must then remain blocked
        #      until the client responds.
        #   3. Wait until the client decides that the server does not support
        #      expect/continue, and sends the request body on its own.
        #      (This is suboptimal, and is not recommended.)
        #
        # We used to do 3, but are now doing 1. Maybe we'll do 2 someday,
        # but it seems like it would be a big slowdown for such a rare case.
        if self.inheaders.get("Expect", "") == "100-continue":
            # Don't use simple_response here, because it emits headers
            # we don't want. See http://www.cherrypy.org/ticket/951
            msg = self.server.protocol + " 100 Continue\r\n\r\n"
            try:
                self.conn.wfile.sendall(msg)
            except socket.error:
                x = sys.exc_info()[1]
                if x.args[0] not in socket_errors_to_ignore:
                    raise
        return True

    def parse_request_uri(self, uri):
        """Parse a Request-URI into (scheme, authority, path).

        Note that Request-URI's must be one of::

            Request-URI    = "*" | absoluteURI | abs_path | authority

        Therefore, a Request-URI which starts with a double forward-slash
        cannot be a "net_path"::

            net_path      = "//" authority [ abs_path ]

        Instead, it must be interpreted as an "abs_path" with an empty first
        path segment::

            abs_path      = "/"  path_segments
            path_segments = segment *( "/" segment )
            segment       = *pchar *( ";" param )
            param         = *pchar
        """
        if uri == ASTERISK:
            return None, None, uri

        i = uri.find('://')
        if i > 0 and QUESTION_MARK not in uri[:i]:
            # An absoluteURI.
            # If there's a scheme (and it must be http or https), then:
            # http_URL = "http:" "//" host [ ":" port ] [ abs_path [ "?" query ]]
            scheme, remainder = uri[:i].lower(), uri[i + 3:]
            authority, path = remainder.split(FORWARD_SLASH, 1)
            path = FORWARD_SLASH + path
            return scheme, authority, path

        if uri.startswith(FORWARD_SLASH):
            # An abs_path.
            return None, None, uri
        else:
            # An authority.
            return None, uri, None

    def respond(self):
        """Call the gateway and write its iterable output."""
        mrbs = self.server.max_request_body_size
        if self.chunked_read:
            self.rfile = ChunkedRFile(self.conn.rfile, mrbs)
        else:
            cl = int(self.inheaders.get("Content-Length", 0))
            if mrbs and mrbs < cl:
                if not self.sent_headers:
                    self.simple_response("413 Request Entity Too Large",
                        "The entity sent with the request exceeds the maximum "
                        "allowed bytes.")
                return
            self.rfile = KnownLengthRFile(self.conn.rfile, cl)

        self.server.gateway(self).respond()

        if (self.ready and not self.sent_headers):
            self.sent_headers = True
            self.send_headers()
        if self.chunked_write:
            self.conn.wfile.sendall("0\r\n\r\n")

    def simple_response(self, status, msg=""):
        """Write a simple response back to the client."""
        status = str(status)
        buf = [self.server.protocol + SPACE +
               status + CRLF,
               "Content-Length: %s\r\n" % len(msg),
               "Content-Type: text/plain\r\n"]

        if status[:3] in ("413", "414"):
            # Request Entity Too Large / Request-URI Too Long
            self.close_connection = True
            if self.response_protocol == 'HTTP/1.1':
                # This will not be true for 414, since read_request_line
                # usually raises 414 before reading the whole line, and we
                # therefore cannot know the proper response_protocol.
                buf.append("Connection: close\r\n")
            else:
                # HTTP/1.0 had no 413/414 status nor Connection header.
                # Emit 400 instead and trust the message body is enough.
                status = "400 Bad Request"

        buf.append(CRLF)
        if msg:
            if isinstance(msg, unicodestr):
                msg = msg.encode("ISO-8859-1")
            buf.append(msg)

        try:
            self.conn.wfile.sendall("".join(buf))
        except socket.error:
            x = sys.exc_info()[1]
            if x.args[0] not in socket_errors_to_ignore:
                raise

    def write(self, chunk):
        """Write unbuffered data to the client."""
        if self.chunked_write and chunk:
            buf = [hex(len(chunk))[2:], CRLF, chunk, CRLF]
            self.conn.wfile.sendall(EMPTY.join(buf))
        else:
            self.conn.wfile.sendall(chunk)

    def send_headers(self):
        """Assert, process, and send the HTTP response message-headers.

        You must set self.status, and self.outheaders before calling this.
        """
        hkeys = [key.lower() for key, value in self.outheaders]
        status = int(self.status[:3])

        if status == 413:
            # Request Entity Too Large. Close conn to avoid garbage.
            self.close_connection = True
        elif "content-length" not in hkeys:
            # "All 1xx (informational), 204 (no content),
            # and 304 (not modified) responses MUST NOT
            # include a message-body." So no point chunking.
            if status < 200 or status in (204, 205, 304):
                pass
            else:
                if (self.response_protocol == 'HTTP/1.1'
                    and self.method != 'HEAD'):
                    # Use the chunked transfer-coding
                    self.chunked_write = True
                    self.outheaders.append(("Transfer-Encoding", "chunked"))
                else:
                    # Closing the conn is the only way to determine len.
                    self.close_connection = True

        if "connection" not in hkeys:
            if self.response_protocol == 'HTTP/1.1':
                # Both server and client are HTTP/1.1 or better
                if self.close_connection:
                    self.outheaders.append(("Connection", "close"))
            else:
                # Server and/or client are HTTP/1.0
                if not self.close_connection:
                    self.outheaders.append(("Connection", "Keep-Alive"))

        if (not self.close_connection) and (not self.chunked_read):
            # Read any remaining request body data on the socket.
            # "If an origin server receives a request that does not include an
            # Expect request-header field with the "100-continue" expectation,
            # the request includes a request body, and the server responds
            # with a final status code before reading the entire request body
            # from the transport connection, then the server SHOULD NOT close
            # the transport connection until it has read the entire request,
            # or until the client closes the connection. Otherwise, the client
            # might not reliably receive the response message. However, this
            # requirement is not be construed as preventing a server from
            # defending itself against denial-of-service attacks, or from
            # badly broken client implementations."
            remaining = getattr(self.rfile, 'remaining', 0)
            if remaining > 0:
                self.rfile.read(remaining)

        if "date" not in hkeys:
            self.outheaders.append(("Date", rfc822.formatdate()))

        if "server" not in hkeys:
            self.outheaders.append(("Server", self.server.server_name))

        buf = [self.server.protocol + SPACE + self.status + CRLF]
        for k, v in self.outheaders:
            buf.append(k + COLON + SPACE + v + CRLF)
        buf.append(CRLF)
        self.conn.wfile.sendall(EMPTY.join(buf))


class NoSSLError(Exception):
    """Exception raised when a client speaks HTTP to an HTTPS socket."""
    pass


class FatalSSLAlert(Exception):
    """Exception raised when the SSL implementation signals a fatal alert."""
    pass


class CP_fileobject(socket._fileobject):
    """Faux file object attached to a socket object."""

    def __init__(self, *args, **kwargs):
        self.bytes_read = 0
        self.bytes_written = 0
        socket._fileobject.__init__(self, *args, **kwargs)

    def sendall(self, data):
        """Sendall for non-blocking sockets."""
        while data:
            try:
                bytes_sent = self.send(data)
                data = data[bytes_sent:]
            except socket.error, e:
                if e.args[0] not in socket_errors_nonblocking:
                    raise

    def send(self, data):
        bytes_sent = self._sock.send(data)
        self.bytes_written += bytes_sent
        return bytes_sent

    def flush(self):
        if self._wbuf:
            buffer = "".join(self._wbuf)
            self._wbuf = []
            self.sendall(buffer)

    def recv(self, size):
        while True:
            try:
                data = self._sock.recv(size)
                self.bytes_read += len(data)
                return data
            except socket.error, e:
                if (e.args[0] not in socket_errors_nonblocking
                    and e.args[0] not in socket_error_eintr):
                    raise

    if not _fileobject_uses_str_type:
        def read(self, size=-1):
            # Use max, disallow tiny reads in a loop as they are very inefficient.
            # We never leave read() with any leftover data from a new recv() call
            # in our internal buffer.
            rbufsize = max(self._rbufsize, self.default_bufsize)
            # Our use of StringIO rather than lists of string objects returned by
            # recv() minimizes memory usage and fragmentation that occurs when
            # rbufsize is large compared to the typical return value of recv().
            buf = self._rbuf
            buf.seek(0, 2)  # seek end
            if size < 0:
                # Read until EOF
                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
                while True:
                    data = self.recv(rbufsize)
                    if not data:
                        break
                    buf.write(data)
                return buf.getvalue()
            else:
                # Read until size bytes or EOF seen, whichever comes first
                buf_len = buf.tell()
                if buf_len >= size:
                    # Already have size bytes in our buffer?  Extract and return.
                    buf.seek(0)
                    rv = buf.read(size)
                    self._rbuf = StringIO.StringIO()
                    self._rbuf.write(buf.read())
                    return rv

                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
                while True:
                    left = size - buf_len
                    # recv() will malloc the amount of memory given as its
                    # parameter even though it often returns much less data
                    # than that.  The returned data string is short lived
                    # as we copy it into a StringIO and free it.  This avoids
                    # fragmentation issues on many platforms.
                    data = self.recv(left)
                    if not data:
                        break
                    n = len(data)
                    if n == size and not buf_len:
                        # Shortcut.  Avoid buffer data copies when:
                        # - We have no data in our buffer.
                        # AND
                        # - Our call to recv returned exactly the
                        #   number of bytes we were asked to read.
                        return data
                    if n == left:
                        buf.write(data)
                        del data  # explicit free
                        break
                    assert n <= left, "recv(%d) returned %d bytes" % (left, n)
                    buf.write(data)
                    buf_len += n
                    del data  # explicit free
                    #assert buf_len == buf.tell()
                return buf.getvalue()

        def readline(self, size=-1):
            buf = self._rbuf
            buf.seek(0, 2)  # seek end
            if buf.tell() > 0:
                # check if we already have it in our buffer
                buf.seek(0)
                bline = buf.readline(size)
                if bline.endswith('\n') or len(bline) == size:
                    self._rbuf = StringIO.StringIO()
                    self._rbuf.write(buf.read())
                    return bline
                del bline
            if size < 0:
                # Read until \n or EOF, whichever comes first
                if self._rbufsize <= 1:
                    # Speed up unbuffered case
                    buf.seek(0)
                    buffers = [buf.read()]
                    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
                    data = None
                    recv = self.recv
                    while data != "\n":
                        data = recv(1)
                        if not data:
                            break
                        buffers.append(data)
                    return "".join(buffers)

                buf.seek(0, 2)  # seek end
                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
                while True:
                    data = self.recv(self._rbufsize)
                    if not data:
                        break
                    nl = data.find('\n')
                    if nl >= 0:
                        nl += 1
                        buf.write(data[:nl])
                        self._rbuf.write(data[nl:])
                        del data
                        break
                    buf.write(data)
                return buf.getvalue()
            else:
                # Read until size bytes or \n or EOF seen, whichever comes first
                buf.seek(0, 2)  # seek end
                buf_len = buf.tell()
                if buf_len >= size:
                    buf.seek(0)
                    rv = buf.read(size)
                    self._rbuf = StringIO.StringIO()
                    self._rbuf.write(buf.read())
                    return rv
                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
                while True:
                    data = self.recv(self._rbufsize)
                    if not data:
                        break
                    left = size - buf_len
                    # did we just receive a newline?
                    nl = data.find('\n', 0, left)
                    if nl >= 0:
                        nl += 1
                        # save the excess data to _rbuf
                        self._rbuf.write(data[nl:])
                        if buf_len:
                            buf.write(data[:nl])
                            break
                        else:
                            # Shortcut.  Avoid data copy through buf when returning
                            # a substring of our first recv().
                            return data[:nl]
                    n = len(data)
                    if n == size and not buf_len:
                        # Shortcut.  Avoid data copy through buf when
                        # returning exactly all of our first recv().
                        return data
                    if n >= left:
                        buf.write(data[:left])
                        self._rbuf.write(data[left:])
                        break
                    buf.write(data)
                    buf_len += n
                    #assert buf_len == buf.tell()
                return buf.getvalue()
    else:
        def read(self, size=-1):
            if size < 0:
                # Read until EOF
                buffers = [self._rbuf]
                self._rbuf = ""
                if self._rbufsize <= 1:
                    recv_size = self.default_bufsize
                else:
                    recv_size = self._rbufsize

                while True:
                    data = self.recv(recv_size)
                    if not data:
                        break
                    buffers.append(data)
                return "".join(buffers)
            else:
                # Read until size bytes or EOF seen, whichever comes first
                data = self._rbuf
                buf_len = len(data)
                if buf_len >= size:
                    self._rbuf = data[size:]
                    return data[:size]
                buffers = []
                if data:
                    buffers.append(data)
                self._rbuf = ""
                while True:
                    left = size - buf_len
                    recv_size = max(self._rbufsize, left)
                    data = self.recv(recv_size)
                    if not data:
                        break
                    buffers.append(data)
                    n = len(data)
                    if n >= left:
                        self._rbuf = data[left:]
                        buffers[-1] = data[:left]
                        break
                    buf_len += n
                return "".join(buffers)

        def readline(self, size=-1):
            data = self._rbuf
            if size < 0:
                # Read until \n or EOF, whichever comes first
                if self._rbufsize <= 1:
                    # Speed up unbuffered case
                    assert data == ""
                    buffers = []
                    while data != "\n":
                        data = self.recv(1)
                        if not data:
                            break
                        buffers.append(data)
                    return "".join(buffers)
                nl = data.find('\n')
                if nl >= 0:
                    nl += 1
                    self._rbuf = data[nl:]
                    return data[:nl]
                buffers = []
                if data:
                    buffers.append(data)
                self._rbuf = ""
                while True:
                    data = self.recv(self._rbufsize)
                    if not data:
                        break
                    buffers.append(data)
                    nl = data.find('\n')
                    if nl >= 0:
                        nl += 1
                        self._rbuf = data[nl:]
                        buffers[-1] = data[:nl]
                        break
                return "".join(buffers)
            else:
                # Read until size bytes or \n or EOF seen, whichever comes first
                nl = data.find('\n', 0, size)
                if nl >= 0:
                    nl += 1
                    self._rbuf = data[nl:]
                    return data[:nl]
                buf_len = len(data)
                if buf_len >= size:
                    self._rbuf = data[size:]
                    return data[:size]
                buffers = []
                if data:
                    buffers.append(data)
                self._rbuf = ""
                while True:
                    data = self.recv(self._rbufsize)
                    if not data:
                        break
                    buffers.append(data)
                    left = size - buf_len
                    nl = data.find('\n', 0, left)
                    if nl >= 0:
                        nl += 1
                        self._rbuf = data[nl:]
                        buffers[-1] = data[:nl]
                        break
                    n = len(data)
                    if n >= left:
                        self._rbuf = data[left:]
                        buffers[-1] = data[:left]
                        break
                    buf_len += n
                return "".join(buffers)


class HTTPConnection(object):
    """An HTTP connection (active socket).

    server: the Server object which received this connection.
    socket: the raw socket object (usually TCP) for this connection.
    makefile: a fileobject class for reading from the socket.
    """

    remote_addr = None
    remote_port = None
    ssl_env = None
    rbufsize = DEFAULT_BUFFER_SIZE
    wbufsize = DEFAULT_BUFFER_SIZE
    RequestHandlerClass = HTTPRequest

    def __init__(self, server, sock, makefile=CP_fileobject):
        self.server = server
        self.socket = sock
        self.rfile = makefile(sock, "rb", self.rbufsize)
        self.wfile = makefile(sock, "wb", self.wbufsize)
        self.requests_seen = 0

    def communicate(self):
        """Read each request and respond appropriately."""
        request_seen = False
        try:
            while True:
                # (re)set req to None so that if something goes wrong in
                # the RequestHandlerClass constructor, the error doesn't
                # get written to the previous request.
                req = None
                req = self.RequestHandlerClass(self.server, self)

                # This order of operations should guarantee correct pipelining.
                req.parse_request()
                if self.server.stats['Enabled']:
                    self.requests_seen += 1
                if not req.ready:
                    # Something went wrong in the parsing (and the server has
                    # probably already made a simple_response). Return and
                    # let the conn close.
                    return

                request_seen = True
                req.respond()
                if req.close_connection:
                    return
        except socket.error:
            e = sys.exc_info()[1]
            errnum = e.args[0]
            # sadly SSL sockets return a different (longer) time out string
            if errnum == 'timed out' or errnum == 'The read operation timed out':
                # Don't error if we're between requests; only error
                # if 1) no request has been started at all, or 2) we're
                # in the middle of a request.
                # See http://www.cherrypy.org/ticket/853
                if (not request_seen) or (req and req.started_request):
                    # Don't bother writing the 408 if the response
                    # has already started being written.
                    if req and not req.sent_headers:
                        try:
                            req.simple_response("408 Request Timeout")
                        except FatalSSLAlert:
                            # Close the connection.
                            return
            elif errnum not in socket_errors_to_ignore:
                self.server.error_log("socket.error %s" % repr(errnum),
                                      level=logging.WARNING, traceback=True)
                if req and not req.sent_headers:
                    try:
                        req.simple_response("500 Internal Server Error")
                    except FatalSSLAlert:
                        # Close the connection.
                        return
            return
        except (KeyboardInterrupt, SystemExit):
            raise
        except FatalSSLAlert:
            # Close the connection.
            return
        except NoSSLError:
            if req and not req.sent_headers:
                # Unwrap our wfile
                self.wfile = CP_fileobject(self.socket._sock, "wb", self.wbufsize)
                req.simple_response("400 Bad Request",
                    "The client sent a plain HTTP request, but "
                    "this server only speaks HTTPS on this port.")
                self.linger = True
        except Exception:
            e = sys.exc_info()[1]
            self.server.error_log(repr(e), level=logging.ERROR, traceback=True)
            if req and not req.sent_headers:
                try:
                    req.simple_response("500 Internal Server Error")
                except FatalSSLAlert:
                    # Close the connection.
                    return

    linger = False

    def close(self):
        """Close the socket underlying this connection."""
        self.rfile.close()

        if not self.linger:
            # Python's socket module does NOT call close on the kernel socket
            # when you call socket.close(). We do so manually here because we
            # want this server to send a FIN TCP segment immediately. Note this
            # must be called *before* calling socket.close(), because the latter
            # drops its reference to the kernel socket.
            if hasattr(self.socket, '_sock'):
                self.socket._sock.close()
            self.socket.close()
        else:
            # On the other hand, sometimes we want to hang around for a bit
            # to make sure the client has a chance to read our entire
            # response. Skipping the close() calls here delays the FIN
            # packet until the socket object is garbage-collected later.
            # Someday, perhaps, we'll do the full lingering_close that
            # Apache does, but not today.
            pass


class TrueyZero(object):
    """An object which equals and does math like the integer '0' but evals True."""
    def __add__(self, other):
        return other
    def __radd__(self, other):
        return other
trueyzero = TrueyZero()


_SHUTDOWNREQUEST = None

class WorkerThread(threading.Thread):
    """Thread which continuously polls a Queue for Connection objects.

    Due to the timing issues of polling a Queue, a WorkerThread does not
    check its own 'ready' flag after it has started. To stop the thread,
    it is necessary to stick a _SHUTDOWNREQUEST object onto the Queue
    (one for each running WorkerThread).
    """

    conn = None
    """The current connection pulled off the Queue, or None."""

    server = None
    """The HTTP Server which spawned this thread, and which owns the
    Queue and is placing active connections into it."""

    ready = False
    """A simple flag for the calling server to know when this thread
    has begun polling the Queue."""


    def __init__(self, server):
        self.ready = False
        self.server = server

        self.requests_seen = 0
        self.bytes_read = 0
        self.bytes_written = 0
        self.start_time = None
        self.work_time = 0
        self.stats = {
            'Requests': lambda s: self.requests_seen + ((self.start_time is None) and trueyzero or self.conn.requests_seen),
            'Bytes Read': lambda s: self.bytes_read + ((self.start_time is None) and trueyzero or self.conn.rfile.bytes_read),
            'Bytes Written': lambda s: self.bytes_written + ((self.start_time is None) and trueyzero or self.conn.wfile.bytes_written),
            'Work Time': lambda s: self.work_time + ((self.start_time is None) and trueyzero or time.time() - self.start_time),
            'Read Throughput': lambda s: s['Bytes Read'](s) / (s['Work Time'](s) or 1e-6),
            'Write Throughput': lambda s: s['Bytes Written'](s) / (s['Work Time'](s) or 1e-6),
        }
        threading.Thread.__init__(self)

    def run(self):
        self.server.stats['Worker Threads'][self.getName()] = self.stats
        try:
            self.ready = True
            while True:
                conn = self.server.requests.get()
                if conn is _SHUTDOWNREQUEST:
                    return

                self.conn = conn
                if self.server.stats['Enabled']:
                    self.start_time = time.time()
                try:
                    conn.communicate()
                finally:
                    conn.close()
                    if self.server.stats['Enabled']:
                        self.requests_seen += self.conn.requests_seen
                        self.bytes_read += self.conn.rfile.bytes_read
                        self.bytes_written += self.conn.wfile.bytes_written
                        self.work_time += time.time() - self.start_time
                        self.start_time = None
                    self.conn = None
        except (KeyboardInterrupt, SystemExit):
            exc = sys.exc_info()[1]
            self.server.interrupt = exc


class ThreadPool(object):
    """A Request Queue for an HTTPServer which pools threads.

    ThreadPool objects must provide min, get(), put(obj), start()
    and stop(timeout) attributes.
    """

    def __init__(self, server, min=10, max=-1):
        self.server = server
        self.min = min
        self.max = max
        self._threads = []
        self._queue = queue.Queue()
        self.get = self._queue.get

    def start(self):
        """Start the pool of threads."""
        for i in range(self.min):
            self._threads.append(WorkerThread(self.server))
        for worker in self._threads:
            worker.setName("CP Server " + worker.getName())
            worker.start()
        for worker in self._threads:
            while not worker.ready:
                time.sleep(.1)

    def _get_idle(self):
        """Number of worker threads which are idle. Read-only."""
        return len([t for t in self._threads if t.conn is None])
    idle = property(_get_idle, doc=_get_idle.__doc__)

    def put(self, obj):
        self._queue.put(obj)
        if obj is _SHUTDOWNREQUEST:
            return

    def grow(self, amount):
        """Spawn new worker threads (not above self.max)."""
        if self.max > 0:
            budget = max(self.max - len(self._threads), 0)
        else:
            # self.max <= 0 indicates no maximum
            budget = float('inf')

        n_new = min(amount, budget)

        workers = [self._spawn_worker() for i in range(n_new)]
        while not self._all(operator.attrgetter('ready'), workers):
            time.sleep(.1)
        self._threads.extend(workers)

    def _spawn_worker(self):
        worker = WorkerThread(self.server)
        worker.setName("CP Server " + worker.getName())
        worker.start()
        return worker

    def _all(func, items):
        results = [func(item) for item in items]
        return reduce(operator.and_, results, True)
    _all = staticmethod(_all)

    def shrink(self, amount):
        """Kill off worker threads (not below self.min)."""
        # Grow/shrink the pool if necessary.
        # Remove any dead threads from our list
        for t in self._threads:
            if not t.isAlive():
                self._threads.remove(t)
                amount -= 1

        # calculate the number of threads above the minimum
        n_extra = max(len(self._threads) - self.min, 0)

        # don't remove more than amount
        n_to_remove = min(amount, n_extra)

        # put shutdown requests on the queue equal to the number of threads
        # to remove. As each request is processed by a worker, that worker
        # will terminate and be culled from the list.
        for n in range(n_to_remove):
            self._queue.put(_SHUTDOWNREQUEST)

    def stop(self, timeout=5):
        # Must shut down threads here so the code that calls
        # this method can know when all threads are stopped.
        for worker in self._threads:
            self._queue.put(_SHUTDOWNREQUEST)

        # Don't join currentThread (when stop is called inside a request).
        current = threading.currentThread()
        if timeout and timeout >= 0:
            endtime = time.time() + timeout
        while self._threads:
            worker = self._threads.pop()
            if worker is not current and worker.isAlive():
                try:
                    if timeout is None or timeout < 0:
                        worker.join()
                    else:
                        remaining_time = endtime - time.time()
                        if remaining_time > 0:
                            worker.join(remaining_time)
                        if worker.isAlive():
                            # We exhausted the timeout.
                            # Forcibly shut down the socket.
                            c = worker.conn
                            if c and not c.rfile.closed:
                                try:
                                    c.socket.shutdown(socket.SHUT_RD)
                                except TypeError:
                                    # pyOpenSSL sockets don't take an arg
                                    c.socket.shutdown()
                            worker.join()
                except (AssertionError,
                        # Ignore repeated Ctrl-C.
                        # See http://www.cherrypy.org/ticket/691.
                        KeyboardInterrupt):
                    pass

    def _get_qsize(self):
        return self._queue.qsize()
    qsize = property(_get_qsize)



try:
    import fcntl
except ImportError:
    try:
        from ctypes import windll, WinError
    except ImportError:
        def prevent_socket_inheritance(sock):
            """Dummy function, since neither fcntl nor ctypes are available."""
            pass
    else:
        def prevent_socket_inheritance(sock):
            """Mark the given socket fd as non-inheritable (Windows)."""
            if not windll.kernel32.SetHandleInformation(sock.fileno(), 1, 0):
                raise WinError()
else:
    def prevent_socket_inheritance(sock):
        """Mark the given socket fd as non-inheritable (POSIX)."""
        fd = sock.fileno()
        old_flags = fcntl.fcntl(fd, fcntl.F_GETFD)
        fcntl.fcntl(fd, fcntl.F_SETFD, old_flags | fcntl.FD_CLOEXEC)


class SSLAdapter(object):
    """Base class for SSL driver library adapters.

    Required methods:

        * ``wrap(sock) -> (wrapped socket, ssl environ dict)``
        * ``makefile(sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE) -> socket file object``
    """

    def __init__(self, certificate, private_key, certificate_chain=None):
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain

    def wrap(self, sock):
        raise NotImplemented

    def makefile(self, sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):
        raise NotImplemented


class HTTPServer(object):
    """An HTTP server."""

    _bind_addr = "127.0.0.1"
    _interrupt = None

    gateway = None
    """A Gateway instance."""

    minthreads = None
    """The minimum number of worker threads to create (default 10)."""

    maxthreads = None
    """The maximum number of worker threads to create (default -1 = no limit)."""

    server_name = None
    """The name of the server; defaults to socket.gethostname()."""

    protocol = "HTTP/1.1"
    """The version string to write in the Status-Line of all HTTP responses.

    For example, "HTTP/1.1" is the default. This also limits the supported
    features used in the response."""

    request_queue_size = 5
    """The 'backlog' arg to socket.listen(); max queued connections (default 5)."""

    shutdown_timeout = 5
    """The total time, in seconds, to wait for worker threads to cleanly exit."""

    timeout = 10
    """The timeout in seconds for accepted connections (default 10)."""

    version = "CherryPy/3.2.4"
    """A version string for the HTTPServer."""

    software = None
    """The value to set for the SERVER_SOFTWARE entry in the WSGI environ.

    If None, this defaults to ``'%s Server' % self.version``."""

    ready = False
    """An internal flag which marks whether the socket is accepting connections."""

    max_request_header_size = 0
    """The maximum size, in bytes, for request headers, or 0 for no limit."""

    max_request_body_size = 0
    """The maximum size, in bytes, for request bodies, or 0 for no limit."""

    nodelay = True
    """If True (the default since 3.1), sets the TCP_NODELAY socket option."""

    ConnectionClass = HTTPConnection
    """The class to use for handling HTTP connections."""

    ssl_adapter = None
    """An instance of SSLAdapter (or a subclass).

    You must have the corresponding SSL driver library installed."""

    def __init__(self, bind_addr, gateway, minthreads=10, maxthreads=-1,
                 server_name=None):
        self.bind_addr = bind_addr
        self.gateway = gateway

        self.requests = ThreadPool(self, min=minthreads or 1, max=maxthreads)

        if not server_name:
            server_name = socket.gethostname()
        self.server_name = server_name
        self.clear_stats()

    def clear_stats(self):
        self._start_time = None
        self._run_time = 0
        self.stats = {
            'Enabled': False,
            'Bind Address': lambda s: repr(self.bind_addr),
            'Run time': lambda s: (not s['Enabled']) and -1 or self.runtime(),
            'Accepts': 0,
            'Accepts/sec': lambda s: s['Accepts'] / self.runtime(),
            'Queue': lambda s: getattr(self.requests, "qsize", None),
            'Threads': lambda s: len(getattr(self.requests, "_threads", [])),
            'Threads Idle': lambda s: getattr(self.requests, "idle", None),
            'Socket Errors': 0,
            'Requests': lambda s: (not s['Enabled']) and -1 or sum([w['Requests'](w) for w
                                       in s['Worker Threads'].values()], 0),
            'Bytes Read': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Read'](w) for w
                                         in s['Worker Threads'].values()], 0),
            'Bytes Written': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Written'](w) for w
                                            in s['Worker Threads'].values()], 0),
            'Work Time': lambda s: (not s['Enabled']) and -1 or sum([w['Work Time'](w) for w
                                         in s['Worker Threads'].values()], 0),
            'Read Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                [w['Bytes Read'](w) / (w['Work Time'](w) or 1e-6)
                 for w in s['Worker Threads'].values()], 0),
            'Write Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                [w['Bytes Written'](w) / (w['Work Time'](w) or 1e-6)
                 for w in s['Worker Threads'].values()], 0),
            'Worker Threads': {},
            }
        logging.statistics["CherryPy HTTPServer %d" % id(self)] = self.stats

    def runtime(self):
        if self._start_time is None:
            return self._run_time
        else:
            return self._run_time + (time.time() - self._start_time)

    def __str__(self):
        return "%s.%s(%r)" % (self.__module__, self.__class__.__name__,
                              self.bind_addr)

    def _get_bind_addr(self):
        return self._bind_addr
    def _set_bind_addr(self, value):
        if isinstance(value, tuple) and value[0] in ('', None):
            # Despite the socket module docs, using '' does not
            # allow AI_PASSIVE to work. Passing None instead
            # returns '0.0.0.0' like we want. In other words:
            #     host    AI_PASSIVE     result
            #      ''         Y         192.168.x.y
            #      ''         N         192.168.x.y
            #     None        Y         0.0.0.0
            #     None        N         127.0.0.1
            # But since you can get the same effect with an explicit
            # '0.0.0.0', we deny both the empty string and None as values.
            raise ValueError("Host values of '' or None are not allowed. "
                             "Use '0.0.0.0' (IPv4) or '::' (IPv6) instead "
                             "to listen on all active interfaces.")
        self._bind_addr = value
    bind_addr = property(_get_bind_addr, _set_bind_addr,
        doc="""The interface on which to listen for connections.

        For TCP sockets, a (host, port) tuple. Host values may be any IPv4
        or IPv6 address, or any valid hostname. The string 'localhost' is a
        synonym for '127.0.0.1' (or '::1', if your hosts file prefers IPv6).
        The string '0.0.0.0' is a special IPv4 entry meaning "any active
        interface" (INADDR_ANY), and '::' is the similar IN6ADDR_ANY for
        IPv6. The empty string or None are not allowed.

        For UNIX sockets, supply the filename as a string.""")

    def start(self):
        """Run the server forever."""
        # We don't have to trap KeyboardInterrupt or SystemExit here,
        # because cherrpy.server already does so, calling self.stop() for us.
        # If you're using this server with another framework, you should
        # trap those exceptions in whatever code block calls start().
        self._interrupt = None

        if self.software is None:
            self.software = "%s Server" % self.version

        # SSL backward compatibility
        if (self.ssl_adapter is None and
            getattr(self, 'ssl_certificate', None) and
            getattr(self, 'ssl_private_key', None)):
            warnings.warn(
                    "SSL attributes are deprecated in CherryPy 3.2, and will "
                    "be removed in CherryPy 3.3. Use an ssl_adapter attribute "
                    "instead.",
                    DeprecationWarning
                )
            try:
                from translate.misc.wsgiserver.ssl_pyopenssl import pyOpenSSLAdapter
            except ImportError:
                pass
            else:
                self.ssl_adapter = pyOpenSSLAdapter(
                    self.ssl_certificate, self.ssl_private_key,
                    getattr(self, 'ssl_certificate_chain', None))

        # Select the appropriate socket
        if isinstance(self.bind_addr, basestring):
            # AF_UNIX socket

            # So we can reuse the socket...
            try: os.unlink(self.bind_addr)
            except: pass

            # So everyone can access the socket...
            try: os.chmod(self.bind_addr, 511) # 0777
            except: pass

            info = [(socket.AF_UNIX, socket.SOCK_STREAM, 0, "", self.bind_addr)]
        else:
            # AF_INET or AF_INET6 socket
            # Get the correct address family for our host (allows IPv6 addresses)
            host, port = self.bind_addr
            try:
                info = socket.getaddrinfo(host, port, socket.AF_UNSPEC,
                                          socket.SOCK_STREAM, 0, socket.AI_PASSIVE)
            except socket.gaierror:
                if ':' in self.bind_addr[0]:
                    info = [(socket.AF_INET6, socket.SOCK_STREAM,
                             0, "", self.bind_addr + (0, 0))]
                else:
                    info = [(socket.AF_INET, socket.SOCK_STREAM,
                             0, "", self.bind_addr)]

        self.socket = None
        msg = "No socket could be created"
        for res in info:
            af, socktype, proto, canonname, sa = res
            try:
                self.bind(af, socktype, proto)
            except socket.error:
                if self.socket:
                    self.socket.close()
                self.socket = None
                continue
            break
        if not self.socket:
            raise socket.error(msg)

        # Timeout so KeyboardInterrupt can be caught on Win32
        self.socket.settimeout(1)
        self.socket.listen(self.request_queue_size)

        # Create worker threads
        self.requests.start()

        self.ready = True
        self._start_time = time.time()
        while self.ready:
            try:
                self.tick()
            except (KeyboardInterrupt, SystemExit):
                raise
            except:
                self.error_log("Error in HTTPServer.tick", level=logging.ERROR,
                               traceback=True)

            if self.interrupt:
                while self.interrupt is True:
                    # Wait for self.stop() to complete. See _set_interrupt.
                    time.sleep(0.1)
                if self.interrupt:
                    raise self.interrupt

    def error_log(self, msg="", level=20, traceback=False):
        # Override this in subclasses as desired
        sys.stderr.write(msg + '\n')
        sys.stderr.flush()
        if traceback:
            tblines = format_exc()
            sys.stderr.write(tblines)
            sys.stderr.flush()

    def bind(self, family, type, proto=0):
        """Create (or recreate) the actual socket object."""
        self.socket = socket.socket(family, type, proto)
        prevent_socket_inheritance(self.socket)
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        if self.nodelay and not isinstance(self.bind_addr, str):
            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

        if self.ssl_adapter is not None:
            self.socket = self.ssl_adapter.bind(self.socket)

        # If listening on the IPV6 any address ('::' = IN6ADDR_ANY),
        # activate dual-stack. See http://www.cherrypy.org/ticket/871.
        if (hasattr(socket, 'AF_INET6') and family == socket.AF_INET6
            and self.bind_addr[0] in ('::', '::0', '::0.0.0.0')):
            try:
                self.socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
            except (AttributeError, socket.error):
                # Apparently, the socket option is not available in
                # this machine's TCP stack
                pass

        self.socket.bind(self.bind_addr)

    def tick(self):
        """Accept a new connection and put it on the Queue."""
        try:
            s, addr = self.socket.accept()
            if self.stats['Enabled']:
                self.stats['Accepts'] += 1
            if not self.ready:
                return

            prevent_socket_inheritance(s)
            if hasattr(s, 'settimeout'):
                s.settimeout(self.timeout)

            makefile = CP_fileobject
            ssl_env = {}
            # if ssl cert and key are set, we try to be a secure HTTP server
            if self.ssl_adapter is not None:
                try:
                    s, ssl_env = self.ssl_adapter.wrap(s)
                except NoSSLError:
                    msg = ("The client sent a plain HTTP request, but "
                           "this server only speaks HTTPS on this port.")
                    buf = ["%s 400 Bad Request\r\n" % self.protocol,
                           "Content-Length: %s\r\n" % len(msg),
                           "Content-Type: text/plain\r\n\r\n",
                           msg]

                    wfile = makefile(s, "wb", DEFAULT_BUFFER_SIZE)
                    try:
                        wfile.sendall("".join(buf))
                    except socket.error:
                        x = sys.exc_info()[1]
                        if x.args[0] not in socket_errors_to_ignore:
                            raise
                    return
                if not s:
                    return
                makefile = self.ssl_adapter.makefile
                # Re-apply our timeout since we may have a new socket object
                if hasattr(s, 'settimeout'):
                    s.settimeout(self.timeout)

            conn = self.ConnectionClass(self, s, makefile)

            if not isinstance(self.bind_addr, basestring):
                # optional values
                # Until we do DNS lookups, omit REMOTE_HOST
                if addr is None: # sometimes this can happen
                    # figure out if AF_INET or AF_INET6.
                    if len(s.getsockname()) == 2:
                        # AF_INET
                        addr = ('0.0.0.0', 0)
                    else:
                        # AF_INET6
                        addr = ('::', 0)
                conn.remote_addr = addr[0]
                conn.remote_port = addr[1]

            conn.ssl_env = ssl_env

            self.requests.put(conn)
        except socket.timeout:
            # The only reason for the timeout in start() is so we can
            # notice keyboard interrupts on Win32, which don't interrupt
            # accept() by default
            return
        except socket.error:
            x = sys.exc_info()[1]
            if self.stats['Enabled']:
                self.stats['Socket Errors'] += 1
            if x.args[0] in socket_error_eintr:
                # I *think* this is right. EINTR should occur when a signal
                # is received during the accept() call; all docs say retry
                # the call, and I *think* I'm reading it right that Python
                # will then go ahead and poll for and handle the signal
                # elsewhere. See http://www.cherrypy.org/ticket/707.
                return
            if x.args[0] in socket_errors_nonblocking:
                # Just try again. See http://www.cherrypy.org/ticket/479.
                return
            if x.args[0] in socket_errors_to_ignore:
                # Our socket was closed.
                # See http://www.cherrypy.org/ticket/686.
                return
            raise

    def _get_interrupt(self):
        return self._interrupt
    def _set_interrupt(self, interrupt):
        self._interrupt = True
        self.stop()
        self._interrupt = interrupt
    interrupt = property(_get_interrupt, _set_interrupt,
                         doc="Set this to an Exception instance to "
                             "interrupt the server.")

    def stop(self):
        """Gracefully shutdown a server that is serving forever."""
        self.ready = False
        if self._start_time is not None:
            self._run_time += (time.time() - self._start_time)
        self._start_time = None

        sock = getattr(self, "socket", None)
        if sock:
            if not isinstance(self.bind_addr, basestring):
                # Touch our own socket to make accept() return immediately.
                try:
                    host, port = sock.getsockname()[:2]
                except socket.error:
                    x = sys.exc_info()[1]
                    if x.args[0] not in socket_errors_to_ignore:
                        # Changed to use error code and not message
                        # See http://www.cherrypy.org/ticket/860.
                        raise
                else:
                    # Note that we're explicitly NOT using AI_PASSIVE,
                    # here, because we want an actual IP to touch.
                    # localhost won't work if we've bound to a public IP,
                    # but it will if we bound to '0.0.0.0' (INADDR_ANY).
                    for res in socket.getaddrinfo(host, port, socket.AF_UNSPEC,
                                                  socket.SOCK_STREAM):
                        af, socktype, proto, canonname, sa = res
                        s = None
                        try:
                            s = socket.socket(af, socktype, proto)
                            # See http://groups.google.com/group/cherrypy-users/
                            #        browse_frm/thread/bbfe5eb39c904fe0
                            s.settimeout(1.0)
                            s.connect((host, port))
                            s.close()
                        except socket.error:
                            if s:
                                s.close()
            if hasattr(sock, "close"):
                sock.close()
            self.socket = None

        self.requests.stop(self.shutdown_timeout)


class Gateway(object):
    """A base class to interface HTTPServer with other systems, such as WSGI."""

    def __init__(self, req):
        self.req = req

    def respond(self):
        """Process the current request. Must be overridden in a subclass."""
        raise NotImplemented


# These may either be wsgiserver.SSLAdapter subclasses or the string names
# of such classes (in which case they will be lazily loaded).
ssl_adapters = {
    'builtin': 'translate.misc.wsgiserver.ssl_builtin.BuiltinSSLAdapter',
    'pyopenssl': 'translate.misc.wsgiserver.ssl_pyopenssl.pyOpenSSLAdapter',
    }

def get_ssl_adapter_class(name='pyopenssl'):
    """Return an SSL adapter class for the given name."""
    adapter = ssl_adapters[name.lower()]
    if isinstance(adapter, basestring):
        last_dot = adapter.rfind(".")
        attr_name = adapter[last_dot + 1:]
        mod_path = adapter[:last_dot]

        try:
            mod = sys.modules[mod_path]
            if mod is None:
                raise KeyError()
        except KeyError:
            # The last [''] is important.
            mod = __import__(mod_path, globals(), locals(), [''])

        # Let an AttributeError propagate outward.
        try:
            adapter = getattr(mod, attr_name)
        except AttributeError:
            raise AttributeError("'%s' object has no attribute '%s'"
                                 % (mod_path, attr_name))

    return adapter

# -------------------------------- WSGI Stuff -------------------------------- #


class CherryPyWSGIServer(HTTPServer):
    """A subclass of HTTPServer which calls a WSGI application."""

    wsgi_version = (1, 0)
    """The version of WSGI to produce."""

    def __init__(self, bind_addr, wsgi_app, numthreads=10, server_name=None,
                 max=-1, request_queue_size=5, timeout=10, shutdown_timeout=5):
        self.requests = ThreadPool(self, min=numthreads or 1, max=max)
        self.wsgi_app = wsgi_app
        self.gateway = wsgi_gateways[self.wsgi_version]

        self.bind_addr = bind_addr
        if not server_name:
            server_name = socket.gethostname()
        self.server_name = server_name
        self.request_queue_size = request_queue_size

        self.timeout = timeout
        self.shutdown_timeout = shutdown_timeout
        self.clear_stats()

    def _get_numthreads(self):
        return self.requests.min
    def _set_numthreads(self, value):
        self.requests.min = value
    numthreads = property(_get_numthreads, _set_numthreads)


class WSGIGateway(Gateway):
    """A base class to interface HTTPServer with WSGI."""

    def __init__(self, req):
        self.req = req
        self.started_response = False
        self.env = self.get_environ()
        self.remaining_bytes_out = None

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        raise NotImplemented

    def respond(self):
        """Process the current request."""
        response = self.req.server.wsgi_app(self.env, self.start_response)
        try:
            for chunk in response:
                # "The start_response callable must not actually transmit
                # the response headers. Instead, it must store them for the
                # server or gateway to transmit only after the first
                # iteration of the application return value that yields
                # a NON-EMPTY string, or upon the application's first
                # invocation of the write() callable." (PEP 333)
                if chunk:
                    if isinstance(chunk, unicodestr):
                        chunk = chunk.encode('ISO-8859-1')
                    self.write(chunk)
        finally:
            if hasattr(response, "close"):
                response.close()

    def start_response(self, status, headers, exc_info = None):
        """WSGI callable to begin the HTTP response."""
        # "The application may call start_response more than once,
        # if and only if the exc_info argument is provided."
        if self.started_response and not exc_info:
            raise AssertionError("WSGI start_response called a second "
                                 "time with no exc_info.")
        self.started_response = True

        # "if exc_info is provided, and the HTTP headers have already been
        # sent, start_response must raise an error, and should raise the
        # exc_info tuple."
        if self.req.sent_headers:
            try:
                raise exc_info[0], exc_info[1], exc_info[2]
            finally:
                exc_info = None

        self.req.status = status
        for k, v in headers:
            if not isinstance(k, str):
                raise TypeError("WSGI response header key %r is not of type str." % k)
            if not isinstance(v, str):
                raise TypeError("WSGI response header value %r is not of type str." % v)
            if k.lower() == 'content-length':
                self.remaining_bytes_out = int(v)
        self.req.outheaders.extend(headers)

        return self.write

    def write(self, chunk):
        """WSGI callable to write unbuffered data to the client.

        This method is also used internally by start_response (to write
        data from the iterable returned by the WSGI application).
        """
        if not self.started_response:
            raise AssertionError("WSGI write called before start_response.")

        chunklen = len(chunk)
        rbo = self.remaining_bytes_out
        if rbo is not None and chunklen > rbo:
            if not self.req.sent_headers:
                # Whew. We can send a 500 to the client.
                self.req.simple_response("500 Internal Server Error",
                    "The requested resource returned more bytes than the "
                    "declared Content-Length.")
            else:
                # Dang. We have probably already sent data. Truncate the chunk
                # to fit (so the client doesn't hang) and raise an error later.
                chunk = chunk[:rbo]

        if not self.req.sent_headers:
            self.req.sent_headers = True
            self.req.send_headers()

        self.req.write(chunk)

        if rbo is not None:
            rbo -= chunklen
            if rbo < 0:
                raise ValueError(
                    "Response body exceeds the declared Content-Length.")


class WSGIGateway_10(WSGIGateway):
    """A Gateway class to interface HTTPServer with WSGI 1.0.x."""

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        req = self.req
        env = {
            # set a non-standard environ entry so the WSGI app can know what
            # the *real* server protocol is (and what features to support).
            # See http://www.faqs.org/rfcs/rfc2145.html.
            'ACTUAL_SERVER_PROTOCOL': req.server.protocol,
            'PATH_INFO': req.path,
            'QUERY_STRING': req.qs,
            'REMOTE_ADDR': req.conn.remote_addr or '',
            'REMOTE_PORT': str(req.conn.remote_port or ''),
            'REQUEST_METHOD': req.method,
            'REQUEST_URI': req.uri,
            'SCRIPT_NAME': '',
            'SERVER_NAME': req.server.server_name,
            # Bah. "SERVER_PROTOCOL" is actually the REQUEST protocol.
            'SERVER_PROTOCOL': req.request_protocol,
            'SERVER_SOFTWARE': req.server.software,
            'wsgi.errors': sys.stderr,
            'wsgi.input': req.rfile,
            'wsgi.multiprocess': False,
            'wsgi.multithread': True,
            'wsgi.run_once': False,
            'wsgi.url_scheme': req.scheme,
            'wsgi.version': (1, 0),
            }

        if isinstance(req.server.bind_addr, basestring):
            # AF_UNIX. This isn't really allowed by WSGI, which doesn't
            # address unix domain sockets. But it's better than nothing.
            env["SERVER_PORT"] = ""
        else:
            env["SERVER_PORT"] = str(req.server.bind_addr[1])

        # Request headers
        for k, v in req.inheaders.iteritems():
            env["HTTP_" + k.upper().replace("-", "_")] = v

        # CONTENT_TYPE/CONTENT_LENGTH
        ct = env.pop("HTTP_CONTENT_TYPE", None)
        if ct is not None:
            env["CONTENT_TYPE"] = ct
        cl = env.pop("HTTP_CONTENT_LENGTH", None)
        if cl is not None:
            env["CONTENT_LENGTH"] = cl

        if req.conn.ssl_env:
            env.update(req.conn.ssl_env)

        return env


class WSGIGateway_u0(WSGIGateway_10):
    """A Gateway class to interface HTTPServer with WSGI u.0.

    WSGI u.0 is an experimental protocol, which uses unicode for keys and values
    in both Python 2 and Python 3.
    """

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        req = self.req
        env_10 = WSGIGateway_10.get_environ(self)
        env = dict([(k.decode('ISO-8859-1'), v) for k, v in env_10.iteritems()])
        env[u'wsgi.version'] = ('u', 0)

        # Request-URI
        env.setdefault(u'wsgi.url_encoding', u'utf-8')
        try:
            for key in [u"PATH_INFO", u"SCRIPT_NAME", u"QUERY_STRING"]:
                env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])
        except UnicodeDecodeError:
            # Fall back to latin 1 so apps can transcode if needed.
            env[u'wsgi.url_encoding'] = u'ISO-8859-1'
            for key in [u"PATH_INFO", u"SCRIPT_NAME", u"QUERY_STRING"]:
                env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])

        for k, v in sorted(env.items()):
            if isinstance(v, str) and k not in ('REQUEST_URI', 'wsgi.input'):
                env[k] = v.decode('ISO-8859-1')

        return env

wsgi_gateways = {
    (1, 0): WSGIGateway_10,
    ('u', 0): WSGIGateway_u0,
}

class WSGIPathInfoDispatcher(object):
    """A WSGI dispatcher for dispatch based on the PATH_INFO.

    apps: a dict or list of (path_prefix, app) pairs.
    """

    def __init__(self, apps):
        try:
            apps = list(apps.items())
        except AttributeError:
            pass

        # Sort the apps by len(path), descending
        apps.sort(cmp=lambda x,y: cmp(len(x[0]), len(y[0])))
        apps.reverse()

        # The path_prefix strings must start, but not end, with a slash.
        # Use "" instead of "/".
        self.apps = [(p.rstrip("/"), a) for p, a in apps]

    def __call__(self, environ, start_response):
        path = environ["PATH_INFO"] or "/"
        for p, app in self.apps:
            # The apps list should be sorted by length, descending.
            if path.startswith(p + "/") or path == p:
                environ = environ.copy()
                environ["SCRIPT_NAME"] = environ["SCRIPT_NAME"] + p
                environ["PATH_INFO"] = path[len(p):]
                return app(environ, start_response)

        start_response('404 Not Found', [('Content-Type', 'text/plain'),
                                         ('Content-Length', '0')])
        return ['']


########NEW FILE########
__FILENAME__ = wsgiserver3
"""A high-speed, production ready, thread pooled, generic HTTP server.

Simplest example on how to use this module directly
(without using CherryPy's application machinery)::

    from cherrypy import wsgiserver

    def my_crazy_app(environ, start_response):
        status = '200 OK'
        response_headers = [('Content-type','text/plain')]
        start_response(status, response_headers)
        return ['Hello world!']

    server = wsgiserver.CherryPyWSGIServer(
                ('0.0.0.0', 8070), my_crazy_app,
                server_name='www.cherrypy.example')
    server.start()

The CherryPy WSGI server can serve as many WSGI applications
as you want in one instance by using a WSGIPathInfoDispatcher::

    d = WSGIPathInfoDispatcher({'/': my_crazy_app, '/blog': my_blog_app})
    server = wsgiserver.CherryPyWSGIServer(('0.0.0.0', 80), d)

Want SSL support? Just set server.ssl_adapter to an SSLAdapter instance.

This won't call the CherryPy engine (application side) at all, only the
HTTP server, which is independent from the rest of CherryPy. Don't
let the name "CherryPyWSGIServer" throw you; the name merely reflects
its origin, not its coupling.

For those of you wanting to understand internals of this module, here's the
basic call flow. The server's listening thread runs a very tight loop,
sticking incoming connections onto a Queue::

    server = CherryPyWSGIServer(...)
    server.start()
    while True:
        tick()
        # This blocks until a request comes in:
        child = socket.accept()
        conn = HTTPConnection(child, ...)
        server.requests.put(conn)

Worker threads are kept in a pool and poll the Queue, popping off and then
handling each connection in turn. Each connection can consist of an arbitrary
number of requests and their responses, so we run a nested loop::

    while True:
        conn = server.requests.get()
        conn.communicate()
        ->  while True:
                req = HTTPRequest(...)
                req.parse_request()
                ->  # Read the Request-Line, e.g. "GET /page HTTP/1.1"
                    req.rfile.readline()
                    read_headers(req.rfile, req.inheaders)
                req.respond()
                ->  response = app(...)
                    try:
                        for chunk in response:
                            if chunk:
                                req.write(chunk)
                    finally:
                        if hasattr(response, "close"):
                            response.close()
                if req.close_connection:
                    return
"""

__all__ = ['HTTPRequest', 'HTTPConnection', 'HTTPServer',
           'SizeCheckWrapper', 'KnownLengthRFile', 'ChunkedRFile',
           'CP_makefile',
           'MaxSizeExceeded', 'NoSSLError', 'FatalSSLAlert',
           'WorkerThread', 'ThreadPool', 'SSLAdapter',
           'CherryPyWSGIServer',
           'Gateway', 'WSGIGateway', 'WSGIGateway_10', 'WSGIGateway_u0',
           'WSGIPathInfoDispatcher', 'get_ssl_adapter_class']

import os
try:
    import queue
except:
    import Queue as queue
import re
import email.utils
import socket
import sys
if 'win' in sys.platform and not hasattr(socket, 'IPPROTO_IPV6'):
    socket.IPPROTO_IPV6 = 41
if sys.version_info < (3,1):
    import io
else:
    import _pyio as io
DEFAULT_BUFFER_SIZE = io.DEFAULT_BUFFER_SIZE

import threading
import time
from traceback import format_exc

if sys.version_info >= (3, 0):
    bytestr = bytes
    unicodestr = str
    basestring = (bytes, str)
    def ntob(n, encoding='ISO-8859-1'):
        """Return the given native string as a byte string in the given encoding."""
        # In Python 3, the native string type is unicode
        return n.encode(encoding)
else:
    bytestr = str
    unicodestr = unicode
    basestring = basestring
    def ntob(n, encoding='ISO-8859-1'):
        """Return the given native string as a byte string in the given encoding."""
        # In Python 2, the native string type is bytes. Assume it's already
        # in the given encoding, which for ISO-8859-1 is almost always what
        # was intended.
        return n

LF = ntob('\n')
CRLF = ntob('\r\n')
TAB = ntob('\t')
SPACE = ntob(' ')
COLON = ntob(':')
SEMICOLON = ntob(';')
EMPTY = ntob('')
NUMBER_SIGN = ntob('#')
QUESTION_MARK = ntob('?')
ASTERISK = ntob('*')
FORWARD_SLASH = ntob('/')
quoted_slash = re.compile(ntob("(?i)%2F"))

import errno

def plat_specific_errors(*errnames):
    """Return error numbers for all errors in errnames on this platform.

    The 'errno' module contains different global constants depending on
    the specific platform (OS). This function will return the list of
    numeric values for a given list of potential names.
    """
    errno_names = dir(errno)
    nums = [getattr(errno, k) for k in errnames if k in errno_names]
    # de-dupe the list
    return list(dict.fromkeys(nums).keys())

socket_error_eintr = plat_specific_errors("EINTR", "WSAEINTR")

socket_errors_to_ignore = plat_specific_errors(
    "EPIPE",
    "EBADF", "WSAEBADF",
    "ENOTSOCK", "WSAENOTSOCK",
    "ETIMEDOUT", "WSAETIMEDOUT",
    "ECONNREFUSED", "WSAECONNREFUSED",
    "ECONNRESET", "WSAECONNRESET",
    "ECONNABORTED", "WSAECONNABORTED",
    "ENETRESET", "WSAENETRESET",
    "EHOSTDOWN", "EHOSTUNREACH",
    )
socket_errors_to_ignore.append("timed out")
socket_errors_to_ignore.append("The read operation timed out")

socket_errors_nonblocking = plat_specific_errors(
    'EAGAIN', 'EWOULDBLOCK', 'WSAEWOULDBLOCK')

comma_separated_headers = [ntob(h) for h in
    ['Accept', 'Accept-Charset', 'Accept-Encoding',
     'Accept-Language', 'Accept-Ranges', 'Allow', 'Cache-Control',
     'Connection', 'Content-Encoding', 'Content-Language', 'Expect',
     'If-Match', 'If-None-Match', 'Pragma', 'Proxy-Authenticate', 'TE',
     'Trailer', 'Transfer-Encoding', 'Upgrade', 'Vary', 'Via', 'Warning',
     'WWW-Authenticate']]


import logging
if not hasattr(logging, 'statistics'): logging.statistics = {}


def read_headers(rfile, hdict=None):
    """Read headers from the given stream into the given header dict.

    If hdict is None, a new header dict is created. Returns the populated
    header dict.

    Headers which are repeated are folded together using a comma if their
    specification so dictates.

    This function raises ValueError when the read bytes violate the HTTP spec.
    You should probably return "400 Bad Request" if this happens.
    """
    if hdict is None:
        hdict = {}

    while True:
        line = rfile.readline()
        if not line:
            # No more data--illegal end of headers
            raise ValueError("Illegal end of headers.")

        if line == CRLF:
            # Normal end of headers
            break
        if not line.endswith(CRLF):
            raise ValueError("HTTP requires CRLF terminators")

        if line[0] in (SPACE, TAB):
            # It's a continuation line.
            v = line.strip()
        else:
            try:
                k, v = line.split(COLON, 1)
            except ValueError:
                raise ValueError("Illegal header line.")
            # TODO: what about TE and WWW-Authenticate?
            k = k.strip().title()
            v = v.strip()
            hname = k

        if k in comma_separated_headers:
            existing = hdict.get(hname)
            if existing:
                v = b", ".join((existing, v))
        hdict[hname] = v

    return hdict


class MaxSizeExceeded(Exception):
    pass

class SizeCheckWrapper(object):
    """Wraps a file-like object, raising MaxSizeExceeded if too large."""

    def __init__(self, rfile, maxlen):
        self.rfile = rfile
        self.maxlen = maxlen
        self.bytes_read = 0

    def _check_length(self):
        if self.maxlen and self.bytes_read > self.maxlen:
            raise MaxSizeExceeded()

    def read(self, size=None):
        data = self.rfile.read(size)
        self.bytes_read += len(data)
        self._check_length()
        return data

    def readline(self, size=None):
        if size is not None:
            data = self.rfile.readline(size)
            self.bytes_read += len(data)
            self._check_length()
            return data

        # User didn't specify a size ...
        # We read the line in chunks to make sure it's not a 100MB line !
        res = []
        while True:
            data = self.rfile.readline(256)
            self.bytes_read += len(data)
            self._check_length()
            res.append(data)
            # See http://www.cherrypy.org/ticket/421
            if len(data) < 256 or data[-1:].decode() == "\n":
                return EMPTY.join(res)

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline()
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline()
        return lines

    def close(self):
        self.rfile.close()

    def __iter__(self):
        return self

    def __next__(self):
        data = next(self.rfile)
        self.bytes_read += len(data)
        self._check_length()
        return data

    def next(self):
        data = self.rfile.next()
        self.bytes_read += len(data)
        self._check_length()
        return data


class KnownLengthRFile(object):
    """Wraps a file-like object, returning an empty string when exhausted."""

    def __init__(self, rfile, content_length):
        self.rfile = rfile
        self.remaining = content_length

    def read(self, size=None):
        if self.remaining == 0:
            return b''
        if size is None:
            size = self.remaining
        else:
            size = min(size, self.remaining)

        data = self.rfile.read(size)
        self.remaining -= len(data)
        return data

    def readline(self, size=None):
        if self.remaining == 0:
            return b''
        if size is None:
            size = self.remaining
        else:
            size = min(size, self.remaining)

        data = self.rfile.readline(size)
        self.remaining -= len(data)
        return data

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline(sizehint)
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)
        return lines

    def close(self):
        self.rfile.close()

    def __iter__(self):
        return self

    def __next__(self):
        data = next(self.rfile)
        self.remaining -= len(data)
        return data


class ChunkedRFile(object):
    """Wraps a file-like object, returning an empty string when exhausted.

    This class is intended to provide a conforming wsgi.input value for
    request entities that have been encoded with the 'chunked' transfer
    encoding.
    """

    def __init__(self, rfile, maxlen, bufsize=8192):
        self.rfile = rfile
        self.maxlen = maxlen
        self.bytes_read = 0
        self.buffer = EMPTY
        self.bufsize = bufsize
        self.closed = False

    def _fetch(self):
        if self.closed:
            return

        line = self.rfile.readline()
        self.bytes_read += len(line)

        if self.maxlen and self.bytes_read > self.maxlen:
            raise MaxSizeExceeded("Request Entity Too Large", self.maxlen)

        line = line.strip().split(SEMICOLON, 1)

        try:
            chunk_size = line.pop(0)
            chunk_size = int(chunk_size, 16)
        except ValueError:
            raise ValueError("Bad chunked transfer size: " + repr(chunk_size))

        if chunk_size <= 0:
            self.closed = True
            return

##            if line: chunk_extension = line[0]

        if self.maxlen and self.bytes_read + chunk_size > self.maxlen:
            raise IOError("Request Entity Too Large")

        chunk = self.rfile.read(chunk_size)
        self.bytes_read += len(chunk)
        self.buffer += chunk

        crlf = self.rfile.read(2)
        if crlf != CRLF:
            raise ValueError(
                 "Bad chunked transfer coding (expected '\\r\\n', "
                 "got " + repr(crlf) + ")")

    def read(self, size=None):
        data = EMPTY
        while True:
            if size and len(data) >= size:
                return data

            if not self.buffer:
                self._fetch()
                if not self.buffer:
                    # EOF
                    return data

            if size:
                remaining = size - len(data)
                data += self.buffer[:remaining]
                self.buffer = self.buffer[remaining:]
            else:
                data += self.buffer

    def readline(self, size=None):
        data = EMPTY
        while True:
            if size and len(data) >= size:
                return data

            if not self.buffer:
                self._fetch()
                if not self.buffer:
                    # EOF
                    return data

            newline_pos = self.buffer.find(LF)
            if size:
                if newline_pos == -1:
                    remaining = size - len(data)
                    data += self.buffer[:remaining]
                    self.buffer = self.buffer[remaining:]
                else:
                    remaining = min(size - len(data), newline_pos)
                    data += self.buffer[:remaining]
                    self.buffer = self.buffer[remaining:]
            else:
                if newline_pos == -1:
                    data += self.buffer
                else:
                    data += self.buffer[:newline_pos]
                    self.buffer = self.buffer[newline_pos:]

    def readlines(self, sizehint=0):
        # Shamelessly stolen from StringIO
        total = 0
        lines = []
        line = self.readline(sizehint)
        while line:
            lines.append(line)
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)
        return lines

    def read_trailer_lines(self):
        if not self.closed:
            raise ValueError(
                "Cannot read trailers until the request body has been read.")

        while True:
            line = self.rfile.readline()
            if not line:
                # No more data--illegal end of headers
                raise ValueError("Illegal end of headers.")

            self.bytes_read += len(line)
            if self.maxlen and self.bytes_read > self.maxlen:
                raise IOError("Request Entity Too Large")

            if line == CRLF:
                # Normal end of headers
                break
            if not line.endswith(CRLF):
                raise ValueError("HTTP requires CRLF terminators")

            yield line

    def close(self):
        self.rfile.close()

    def __iter__(self):
        # Shamelessly stolen from StringIO
        total = 0
        line = self.readline(sizehint)
        while line:
            yield line
            total += len(line)
            if 0 < sizehint <= total:
                break
            line = self.readline(sizehint)


class HTTPRequest(object):
    """An HTTP Request (and response).

    A single HTTP connection may consist of multiple request/response pairs.
    """

    server = None
    """The HTTPServer object which is receiving this request."""

    conn = None
    """The HTTPConnection object on which this request connected."""

    inheaders = {}
    """A dict of request headers."""

    outheaders = []
    """A list of header tuples to write in the response."""

    ready = False
    """When True, the request has been parsed and is ready to begin generating
    the response. When False, signals the calling Connection that the response
    should not be generated and the connection should close."""

    close_connection = False
    """Signals the calling Connection that the request should close. This does
    not imply an error! The client and/or server may each request that the
    connection be closed."""

    chunked_write = False
    """If True, output will be encoded with the "chunked" transfer-coding.

    This value is set automatically inside send_headers."""

    def __init__(self, server, conn):
        self.server= server
        self.conn = conn

        self.ready = False
        self.started_request = False
        self.scheme = ntob("http")
        if self.server.ssl_adapter is not None:
            self.scheme = ntob("https")
        # Use the lowest-common protocol in case read_request_line errors.
        self.response_protocol = 'HTTP/1.0'
        self.inheaders = {}

        self.status = ""
        self.outheaders = []
        self.sent_headers = False
        self.close_connection = self.__class__.close_connection
        self.chunked_read = False
        self.chunked_write = self.__class__.chunked_write

    def parse_request(self):
        """Parse the next HTTP request start-line and message-headers."""
        self.rfile = SizeCheckWrapper(self.conn.rfile,
                                      self.server.max_request_header_size)
        try:
            success = self.read_request_line()
        except MaxSizeExceeded:
            self.simple_response("414 Request-URI Too Long",
                "The Request-URI sent with the request exceeds the maximum "
                "allowed bytes.")
            return
        else:
            if not success:
                return

        try:
            success = self.read_request_headers()
        except MaxSizeExceeded:
            self.simple_response("413 Request Entity Too Large",
                "The headers sent with the request exceed the maximum "
                "allowed bytes.")
            return
        else:
            if not success:
                return

        self.ready = True

    def read_request_line(self):
        # HTTP/1.1 connections are persistent by default. If a client
        # requests a page, then idles (leaves the connection open),
        # then rfile.readline() will raise socket.error("timed out").
        # Note that it does this based on the value given to settimeout(),
        # and doesn't need the client to request or acknowledge the close
        # (although your TCP stack might suffer for it: cf Apache's history
        # with FIN_WAIT_2).
        request_line = self.rfile.readline()

        # Set started_request to True so communicate() knows to send 408
        # from here on out.
        self.started_request = True
        if not request_line:
            return False

        if request_line == CRLF:
            # RFC 2616 sec 4.1: "...if the server is reading the protocol
            # stream at the beginning of a message and receives a CRLF
            # first, it should ignore the CRLF."
            # But only ignore one leading line! else we enable a DoS.
            request_line = self.rfile.readline()
            if not request_line:
                return False

        if not request_line.endswith(CRLF):
            self.simple_response("400 Bad Request", "HTTP requires CRLF terminators")
            return False

        try:
            method, uri, req_protocol = request_line.strip().split(SPACE, 2)
            # The [x:y] slicing is necessary for byte strings to avoid getting ord's
            rp = int(req_protocol[5:6]), int(req_protocol[7:8])
        except ValueError:
            self.simple_response("400 Bad Request", "Malformed Request-Line")
            return False

        self.uri = uri
        self.method = method

        # uri may be an abs_path (including "http://host.domain.tld");
        scheme, authority, path = self.parse_request_uri(uri)
        if NUMBER_SIGN in path:
            self.simple_response("400 Bad Request",
                                 "Illegal #fragment in Request-URI.")
            return False

        if scheme:
            self.scheme = scheme

        qs = EMPTY
        if QUESTION_MARK in path:
            path, qs = path.split(QUESTION_MARK, 1)

        # Unquote the path+params (e.g. "/this%20path" -> "/this path").
        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2
        #
        # But note that "...a URI must be separated into its components
        # before the escaped characters within those components can be
        # safely decoded." http://www.ietf.org/rfc/rfc2396.txt, sec 2.4.2
        # Therefore, "/this%2Fpath" becomes "/this%2Fpath", not "/this/path".
        try:
            atoms = [self.unquote_bytes(x) for x in quoted_slash.split(path)]
        except ValueError:
            ex = sys.exc_info()[1]
            self.simple_response("400 Bad Request", ex.args[0])
            return False
        path = b"%2F".join(atoms)
        self.path = path

        # Note that, like wsgiref and most other HTTP servers,
        # we "% HEX HEX"-unquote the path but not the query string.
        self.qs = qs

        # Compare request and server HTTP protocol versions, in case our
        # server does not support the requested protocol. Limit our output
        # to min(req, server). We want the following output:
        #     request    server     actual written   supported response
        #     protocol   protocol  response protocol    feature set
        # a     1.0        1.0           1.0                1.0
        # b     1.0        1.1           1.1                1.0
        # c     1.1        1.0           1.0                1.0
        # d     1.1        1.1           1.1                1.1
        # Notice that, in (b), the response will be "HTTP/1.1" even though
        # the client only understands 1.0. RFC 2616 10.5.6 says we should
        # only return 505 if the _major_ version is different.
        # The [x:y] slicing is necessary for byte strings to avoid getting ord's
        sp = int(self.server.protocol[5:6]), int(self.server.protocol[7:8])

        if sp[0] != rp[0]:
            self.simple_response("505 HTTP Version Not Supported")
            return False

        self.request_protocol = req_protocol
        self.response_protocol = "HTTP/%s.%s" % min(rp, sp)
        return True

    def read_request_headers(self):
        """Read self.rfile into self.inheaders. Return success."""

        # then all the http headers
        try:
            read_headers(self.rfile, self.inheaders)
        except ValueError:
            ex = sys.exc_info()[1]
            self.simple_response("400 Bad Request", ex.args[0])
            return False

        mrbs = self.server.max_request_body_size
        if mrbs and int(self.inheaders.get(b"Content-Length", 0)) > mrbs:
            self.simple_response("413 Request Entity Too Large",
                "The entity sent with the request exceeds the maximum "
                "allowed bytes.")
            return False

        # Persistent connection support
        if self.response_protocol == "HTTP/1.1":
            # Both server and client are HTTP/1.1
            if self.inheaders.get(b"Connection", b"") == b"close":
                self.close_connection = True
        else:
            # Either the server or client (or both) are HTTP/1.0
            if self.inheaders.get(b"Connection", b"") != b"Keep-Alive":
                self.close_connection = True

        # Transfer-Encoding support
        te = None
        if self.response_protocol == "HTTP/1.1":
            te = self.inheaders.get(b"Transfer-Encoding")
            if te:
                te = [x.strip().lower() for x in te.split(b",") if x.strip()]

        self.chunked_read = False

        if te:
            for enc in te:
                if enc == b"chunked":
                    self.chunked_read = True
                else:
                    # Note that, even if we see "chunked", we must reject
                    # if there is an extension we don't recognize.
                    self.simple_response("501 Unimplemented")
                    self.close_connection = True
                    return False

        # From PEP 333:
        # "Servers and gateways that implement HTTP 1.1 must provide
        # transparent support for HTTP 1.1's "expect/continue" mechanism.
        # This may be done in any of several ways:
        #   1. Respond to requests containing an Expect: 100-continue request
        #      with an immediate "100 Continue" response, and proceed normally.
        #   2. Proceed with the request normally, but provide the application
        #      with a wsgi.input stream that will send the "100 Continue"
        #      response if/when the application first attempts to read from
        #      the input stream. The read request must then remain blocked
        #      until the client responds.
        #   3. Wait until the client decides that the server does not support
        #      expect/continue, and sends the request body on its own.
        #      (This is suboptimal, and is not recommended.)
        #
        # We used to do 3, but are now doing 1. Maybe we'll do 2 someday,
        # but it seems like it would be a big slowdown for such a rare case.
        if self.inheaders.get(b"Expect", b"") == b"100-continue":
            # Don't use simple_response here, because it emits headers
            # we don't want. See http://www.cherrypy.org/ticket/951
            msg = self.server.protocol.encode('ascii') + b" 100 Continue\r\n\r\n"
            try:
                self.conn.wfile.write(msg)
            except socket.error:
                x = sys.exc_info()[1]
                if x.args[0] not in socket_errors_to_ignore:
                    raise
        return True

    def parse_request_uri(self, uri):
        """Parse a Request-URI into (scheme, authority, path).

        Note that Request-URI's must be one of::

            Request-URI    = "*" | absoluteURI | abs_path | authority

        Therefore, a Request-URI which starts with a double forward-slash
        cannot be a "net_path"::

            net_path      = "//" authority [ abs_path ]

        Instead, it must be interpreted as an "abs_path" with an empty first
        path segment::

            abs_path      = "/"  path_segments
            path_segments = segment *( "/" segment )
            segment       = *pchar *( ";" param )
            param         = *pchar
        """
        if uri == ASTERISK:
            return None, None, uri

        scheme, sep, remainder = uri.partition(b'://')
        if sep and QUESTION_MARK not in scheme:
            # An absoluteURI.
            # If there's a scheme (and it must be http or https), then:
            # http_URL = "http:" "//" host [ ":" port ] [ abs_path [ "?" query ]]
            authority, path_a, path_b = remainder.partition(FORWARD_SLASH)
            return scheme.lower(), authority, path_a+path_b

        if uri.startswith(FORWARD_SLASH):
            # An abs_path.
            return None, None, uri
        else:
            # An authority.
            return None, uri, None

    def unquote_bytes(self, path):
        """takes quoted string and unquotes % encoded values"""
        res = path.split(b'%')

        for i in range(1, len(res)):
            item = res[i]
            try:
                res[i] = bytes([int(item[:2], 16)]) + item[2:]
            except ValueError:
                raise
        return b''.join(res)

    def respond(self):
        """Call the gateway and write its iterable output."""
        mrbs = self.server.max_request_body_size
        if self.chunked_read:
            self.rfile = ChunkedRFile(self.conn.rfile, mrbs)
        else:
            cl = int(self.inheaders.get(b"Content-Length", 0))
            if mrbs and mrbs < cl:
                if not self.sent_headers:
                    self.simple_response("413 Request Entity Too Large",
                        "The entity sent with the request exceeds the maximum "
                        "allowed bytes.")
                return
            self.rfile = KnownLengthRFile(self.conn.rfile, cl)

        self.server.gateway(self).respond()

        if (self.ready and not self.sent_headers):
            self.sent_headers = True
            self.send_headers()
        if self.chunked_write:
            self.conn.wfile.write(b"0\r\n\r\n")

    def simple_response(self, status, msg=""):
        """Write a simple response back to the client."""
        status = str(status)
        buf = [bytes(self.server.protocol, "ascii") + SPACE +
               bytes(status, "ISO-8859-1") + CRLF,
               bytes("Content-Length: %s\r\n" % len(msg), "ISO-8859-1"),
               b"Content-Type: text/plain\r\n"]

        if status[:3] in ("413", "414"):
            # Request Entity Too Large / Request-URI Too Long
            self.close_connection = True
            if self.response_protocol == 'HTTP/1.1':
                # This will not be true for 414, since read_request_line
                # usually raises 414 before reading the whole line, and we
                # therefore cannot know the proper response_protocol.
                buf.append(b"Connection: close\r\n")
            else:
                # HTTP/1.0 had no 413/414 status nor Connection header.
                # Emit 400 instead and trust the message body is enough.
                status = "400 Bad Request"

        buf.append(CRLF)
        if msg:
            if isinstance(msg, unicodestr):
                msg = msg.encode("ISO-8859-1")
            buf.append(msg)

        try:
            self.conn.wfile.write(b"".join(buf))
        except socket.error:
            x = sys.exc_info()[1]
            if x.args[0] not in socket_errors_to_ignore:
                raise

    def write(self, chunk):
        """Write unbuffered data to the client."""
        if self.chunked_write and chunk:
            buf = [bytes(hex(len(chunk)), 'ASCII')[2:], CRLF, chunk, CRLF]
            self.conn.wfile.write(EMPTY.join(buf))
        else:
            self.conn.wfile.write(chunk)

    def send_headers(self):
        """Assert, process, and send the HTTP response message-headers.

        You must set self.status, and self.outheaders before calling this.
        """
        hkeys = [key.lower() for key, value in self.outheaders]
        status = int(self.status[:3])

        if status == 413:
            # Request Entity Too Large. Close conn to avoid garbage.
            self.close_connection = True
        elif b"content-length" not in hkeys:
            # "All 1xx (informational), 204 (no content),
            # and 304 (not modified) responses MUST NOT
            # include a message-body." So no point chunking.
            if status < 200 or status in (204, 205, 304):
                pass
            else:
                if (self.response_protocol == 'HTTP/1.1'
                    and self.method != b'HEAD'):
                    # Use the chunked transfer-coding
                    self.chunked_write = True
                    self.outheaders.append((b"Transfer-Encoding", b"chunked"))
                else:
                    # Closing the conn is the only way to determine len.
                    self.close_connection = True

        if b"connection" not in hkeys:
            if self.response_protocol == 'HTTP/1.1':
                # Both server and client are HTTP/1.1 or better
                if self.close_connection:
                    self.outheaders.append((b"Connection", b"close"))
            else:
                # Server and/or client are HTTP/1.0
                if not self.close_connection:
                    self.outheaders.append((b"Connection", b"Keep-Alive"))

        if (not self.close_connection) and (not self.chunked_read):
            # Read any remaining request body data on the socket.
            # "If an origin server receives a request that does not include an
            # Expect request-header field with the "100-continue" expectation,
            # the request includes a request body, and the server responds
            # with a final status code before reading the entire request body
            # from the transport connection, then the server SHOULD NOT close
            # the transport connection until it has read the entire request,
            # or until the client closes the connection. Otherwise, the client
            # might not reliably receive the response message. However, this
            # requirement is not be construed as preventing a server from
            # defending itself against denial-of-service attacks, or from
            # badly broken client implementations."
            remaining = getattr(self.rfile, 'remaining', 0)
            if remaining > 0:
                self.rfile.read(remaining)

        if b"date" not in hkeys:
            self.outheaders.append(
                (b"Date", email.utils.formatdate(usegmt=True).encode('ISO-8859-1')))

        if b"server" not in hkeys:
            self.outheaders.append(
                (b"Server", self.server.server_name.encode('ISO-8859-1')))

        buf = [self.server.protocol.encode('ascii') + SPACE + self.status + CRLF]
        for k, v in self.outheaders:
            buf.append(k + COLON + SPACE + v + CRLF)
        buf.append(CRLF)
        self.conn.wfile.write(EMPTY.join(buf))


class NoSSLError(Exception):
    """Exception raised when a client speaks HTTP to an HTTPS socket."""
    pass


class FatalSSLAlert(Exception):
    """Exception raised when the SSL implementation signals a fatal alert."""
    pass


class CP_BufferedWriter(io.BufferedWriter):
    """Faux file object attached to a socket object."""

    def write(self, b):
        self._checkClosed()
        if isinstance(b, str):
            raise TypeError("can't write str to binary stream")

        with self._write_lock:
            self._write_buf.extend(b)
            self._flush_unlocked()
            return len(b)

    def _flush_unlocked(self):
        self._checkClosed("flush of closed file")
        while self._write_buf:
            try:
                # ssl sockets only except 'bytes', not bytearrays
                # so perhaps we should conditionally wrap this for perf?
                n = self.raw.write(bytes(self._write_buf))
            except io.BlockingIOError as e:
                n = e.characters_written
            del self._write_buf[:n]


def CP_makefile(sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):
    if 'r' in mode:
        return io.BufferedReader(socket.SocketIO(sock, mode), bufsize)
    else:
        return CP_BufferedWriter(socket.SocketIO(sock, mode), bufsize)

class HTTPConnection(object):
    """An HTTP connection (active socket).

    server: the Server object which received this connection.
    socket: the raw socket object (usually TCP) for this connection.
    makefile: a fileobject class for reading from the socket.
    """

    remote_addr = None
    remote_port = None
    ssl_env = None
    rbufsize = DEFAULT_BUFFER_SIZE
    wbufsize = DEFAULT_BUFFER_SIZE
    RequestHandlerClass = HTTPRequest

    def __init__(self, server, sock, makefile=CP_makefile):
        self.server = server
        self.socket = sock
        self.rfile = makefile(sock, "rb", self.rbufsize)
        self.wfile = makefile(sock, "wb", self.wbufsize)
        self.requests_seen = 0

    def communicate(self):
        """Read each request and respond appropriately."""
        request_seen = False
        try:
            while True:
                # (re)set req to None so that if something goes wrong in
                # the RequestHandlerClass constructor, the error doesn't
                # get written to the previous request.
                req = None
                req = self.RequestHandlerClass(self.server, self)

                # This order of operations should guarantee correct pipelining.
                req.parse_request()
                if self.server.stats['Enabled']:
                    self.requests_seen += 1
                if not req.ready:
                    # Something went wrong in the parsing (and the server has
                    # probably already made a simple_response). Return and
                    # let the conn close.
                    return

                request_seen = True
                req.respond()
                if req.close_connection:
                    return
        except socket.error:
            e = sys.exc_info()[1]
            errnum = e.args[0]
            # sadly SSL sockets return a different (longer) time out string
            if errnum == 'timed out' or errnum == 'The read operation timed out':
                # Don't error if we're between requests; only error
                # if 1) no request has been started at all, or 2) we're
                # in the middle of a request.
                # See http://www.cherrypy.org/ticket/853
                if (not request_seen) or (req and req.started_request):
                    # Don't bother writing the 408 if the response
                    # has already started being written.
                    if req and not req.sent_headers:
                        try:
                            req.simple_response("408 Request Timeout")
                        except FatalSSLAlert:
                            # Close the connection.
                            return
            elif errnum not in socket_errors_to_ignore:
                self.server.error_log("socket.error %s" % repr(errnum),
                                      level=logging.WARNING, traceback=True)
                if req and not req.sent_headers:
                    try:
                        req.simple_response("500 Internal Server Error")
                    except FatalSSLAlert:
                        # Close the connection.
                        return
            return
        except (KeyboardInterrupt, SystemExit):
            raise
        except FatalSSLAlert:
            # Close the connection.
            return
        except NoSSLError:
            if req and not req.sent_headers:
                # Unwrap our wfile
                self.wfile = CP_makefile(self.socket._sock, "wb", self.wbufsize)
                req.simple_response("400 Bad Request",
                    "The client sent a plain HTTP request, but "
                    "this server only speaks HTTPS on this port.")
                self.linger = True
        except Exception:
            e = sys.exc_info()[1]
            self.server.error_log(repr(e), level=logging.ERROR, traceback=True)
            if req and not req.sent_headers:
                try:
                    req.simple_response("500 Internal Server Error")
                except FatalSSLAlert:
                    # Close the connection.
                    return

    linger = False

    def close(self):
        """Close the socket underlying this connection."""
        self.rfile.close()

        if not self.linger:
            # Python's socket module does NOT call close on the kernel socket
            # when you call socket.close(). We do so manually here because we
            # want this server to send a FIN TCP segment immediately. Note this
            # must be called *before* calling socket.close(), because the latter
            # drops its reference to the kernel socket.
            # Python 3 *probably* fixed this with socket._real_close; hard to tell.
##            self.socket._sock.close()
            self.socket.close()
        else:
            # On the other hand, sometimes we want to hang around for a bit
            # to make sure the client has a chance to read our entire
            # response. Skipping the close() calls here delays the FIN
            # packet until the socket object is garbage-collected later.
            # Someday, perhaps, we'll do the full lingering_close that
            # Apache does, but not today.
            pass


class TrueyZero(object):
    """An object which equals and does math like the integer '0' but evals True."""
    def __add__(self, other):
        return other
    def __radd__(self, other):
        return other
trueyzero = TrueyZero()


_SHUTDOWNREQUEST = None

class WorkerThread(threading.Thread):
    """Thread which continuously polls a Queue for Connection objects.

    Due to the timing issues of polling a Queue, a WorkerThread does not
    check its own 'ready' flag after it has started. To stop the thread,
    it is necessary to stick a _SHUTDOWNREQUEST object onto the Queue
    (one for each running WorkerThread).
    """

    conn = None
    """The current connection pulled off the Queue, or None."""

    server = None
    """The HTTP Server which spawned this thread, and which owns the
    Queue and is placing active connections into it."""

    ready = False
    """A simple flag for the calling server to know when this thread
    has begun polling the Queue."""


    def __init__(self, server):
        self.ready = False
        self.server = server

        self.requests_seen = 0
        self.bytes_read = 0
        self.bytes_written = 0
        self.start_time = None
        self.work_time = 0
        self.stats = {
            'Requests': lambda s: self.requests_seen + ((self.start_time is None) and trueyzero or self.conn.requests_seen),
            'Bytes Read': lambda s: self.bytes_read + ((self.start_time is None) and trueyzero or self.conn.rfile.bytes_read),
            'Bytes Written': lambda s: self.bytes_written + ((self.start_time is None) and trueyzero or self.conn.wfile.bytes_written),
            'Work Time': lambda s: self.work_time + ((self.start_time is None) and trueyzero or time.time() - self.start_time),
            'Read Throughput': lambda s: s['Bytes Read'](s) / (s['Work Time'](s) or 1e-6),
            'Write Throughput': lambda s: s['Bytes Written'](s) / (s['Work Time'](s) or 1e-6),
        }
        threading.Thread.__init__(self)

    def run(self):
        self.server.stats['Worker Threads'][self.getName()] = self.stats
        try:
            self.ready = True
            while True:
                conn = self.server.requests.get()
                if conn is _SHUTDOWNREQUEST:
                    return

                self.conn = conn
                if self.server.stats['Enabled']:
                    self.start_time = time.time()
                try:
                    conn.communicate()
                finally:
                    conn.close()
                    if self.server.stats['Enabled']:
                        self.requests_seen += self.conn.requests_seen
                        self.bytes_read += self.conn.rfile.bytes_read
                        self.bytes_written += self.conn.wfile.bytes_written
                        self.work_time += time.time() - self.start_time
                        self.start_time = None
                    self.conn = None
        except (KeyboardInterrupt, SystemExit):
            exc = sys.exc_info()[1]
            self.server.interrupt = exc


class ThreadPool(object):
    """A Request Queue for an HTTPServer which pools threads.

    ThreadPool objects must provide min, get(), put(obj), start()
    and stop(timeout) attributes.
    """

    def __init__(self, server, min=10, max=-1):
        self.server = server
        self.min = min
        self.max = max
        self._threads = []
        self._queue = queue.Queue()
        self.get = self._queue.get

    def start(self):
        """Start the pool of threads."""
        for i in range(self.min):
            self._threads.append(WorkerThread(self.server))
        for worker in self._threads:
            worker.setName("CP Server " + worker.getName())
            worker.start()
        for worker in self._threads:
            while not worker.ready:
                time.sleep(.1)

    def _get_idle(self):
        """Number of worker threads which are idle. Read-only."""
        return len([t for t in self._threads if t.conn is None])
    idle = property(_get_idle, doc=_get_idle.__doc__)

    def put(self, obj):
        self._queue.put(obj)
        if obj is _SHUTDOWNREQUEST:
            return

    def grow(self, amount):
        """Spawn new worker threads (not above self.max)."""
        if self.max > 0:
            budget = max(self.max - len(self._threads), 0)
        else:
            # self.max <= 0 indicates no maximum
            budget = float('inf')

        n_new = min(amount, budget)

        workers = [self._spawn_worker() for i in range(n_new)]
        while not all(worker.ready for worker in workers):
            time.sleep(.1)
        self._threads.extend(workers)

    def _spawn_worker(self):
        worker = WorkerThread(self.server)
        worker.setName("CP Server " + worker.getName())
        worker.start()
        return worker

    def shrink(self, amount):
        """Kill off worker threads (not below self.min)."""
        # Grow/shrink the pool if necessary.
        # Remove any dead threads from our list
        for t in self._threads:
            if not t.isAlive():
                self._threads.remove(t)
                amount -= 1

        # calculate the number of threads above the minimum
        n_extra = max(len(self._threads) - self.min, 0)

        # don't remove more than amount
        n_to_remove = min(amount, n_extra)

        # put shutdown requests on the queue equal to the number of threads
        # to remove. As each request is processed by a worker, that worker
        # will terminate and be culled from the list.
        for n in range(n_to_remove):
            self._queue.put(_SHUTDOWNREQUEST)

    def stop(self, timeout=5):
        # Must shut down threads here so the code that calls
        # this method can know when all threads are stopped.
        for worker in self._threads:
            self._queue.put(_SHUTDOWNREQUEST)

        # Don't join currentThread (when stop is called inside a request).
        current = threading.currentThread()
        if timeout and timeout >= 0:
            endtime = time.time() + timeout
        while self._threads:
            worker = self._threads.pop()
            if worker is not current and worker.isAlive():
                try:
                    if timeout is None or timeout < 0:
                        worker.join()
                    else:
                        remaining_time = endtime - time.time()
                        if remaining_time > 0:
                            worker.join(remaining_time)
                        if worker.isAlive():
                            # We exhausted the timeout.
                            # Forcibly shut down the socket.
                            c = worker.conn
                            if c and not c.rfile.closed:
                                try:
                                    c.socket.shutdown(socket.SHUT_RD)
                                except TypeError:
                                    # pyOpenSSL sockets don't take an arg
                                    c.socket.shutdown()
                            worker.join()
                except (AssertionError,
                        # Ignore repeated Ctrl-C.
                        # See http://www.cherrypy.org/ticket/691.
                        KeyboardInterrupt):
                    pass

    def _get_qsize(self):
        return self._queue.qsize()
    qsize = property(_get_qsize)



try:
    import fcntl
except ImportError:
    try:
        from ctypes import windll, WinError
    except ImportError:
        def prevent_socket_inheritance(sock):
            """Dummy function, since neither fcntl nor ctypes are available."""
            pass
    else:
        def prevent_socket_inheritance(sock):
            """Mark the given socket fd as non-inheritable (Windows)."""
            if not windll.kernel32.SetHandleInformation(sock.fileno(), 1, 0):
                raise WinError()
else:
    def prevent_socket_inheritance(sock):
        """Mark the given socket fd as non-inheritable (POSIX)."""
        fd = sock.fileno()
        old_flags = fcntl.fcntl(fd, fcntl.F_GETFD)
        fcntl.fcntl(fd, fcntl.F_SETFD, old_flags | fcntl.FD_CLOEXEC)


class SSLAdapter(object):
    """Base class for SSL driver library adapters.

    Required methods:

        * ``wrap(sock) -> (wrapped socket, ssl environ dict)``
        * ``makefile(sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE) -> socket file object``
    """

    def __init__(self, certificate, private_key, certificate_chain=None):
        self.certificate = certificate
        self.private_key = private_key
        self.certificate_chain = certificate_chain

    def wrap(self, sock):
        raise NotImplemented

    def makefile(self, sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):
        raise NotImplemented


class HTTPServer(object):
    """An HTTP server."""

    _bind_addr = "127.0.0.1"
    _interrupt = None

    gateway = None
    """A Gateway instance."""

    minthreads = None
    """The minimum number of worker threads to create (default 10)."""

    maxthreads = None
    """The maximum number of worker threads to create (default -1 = no limit)."""

    server_name = None
    """The name of the server; defaults to socket.gethostname()."""

    protocol = "HTTP/1.1"
    """The version string to write in the Status-Line of all HTTP responses.

    For example, "HTTP/1.1" is the default. This also limits the supported
    features used in the response."""

    request_queue_size = 5
    """The 'backlog' arg to socket.listen(); max queued connections (default 5)."""

    shutdown_timeout = 5
    """The total time, in seconds, to wait for worker threads to cleanly exit."""

    timeout = 10
    """The timeout in seconds for accepted connections (default 10)."""

    version = "CherryPy/3.2.4"
    """A version string for the HTTPServer."""

    software = None
    """The value to set for the SERVER_SOFTWARE entry in the WSGI environ.

    If None, this defaults to ``'%s Server' % self.version``."""

    ready = False
    """An internal flag which marks whether the socket is accepting connections."""

    max_request_header_size = 0
    """The maximum size, in bytes, for request headers, or 0 for no limit."""

    max_request_body_size = 0
    """The maximum size, in bytes, for request bodies, or 0 for no limit."""

    nodelay = True
    """If True (the default since 3.1), sets the TCP_NODELAY socket option."""

    ConnectionClass = HTTPConnection
    """The class to use for handling HTTP connections."""

    ssl_adapter = None
    """An instance of SSLAdapter (or a subclass).

    You must have the corresponding SSL driver library installed."""

    def __init__(self, bind_addr, gateway, minthreads=10, maxthreads=-1,
                 server_name=None):
        self.bind_addr = bind_addr
        self.gateway = gateway

        self.requests = ThreadPool(self, min=minthreads or 1, max=maxthreads)

        if not server_name:
            server_name = socket.gethostname()
        self.server_name = server_name
        self.clear_stats()

    def clear_stats(self):
        self._start_time = None
        self._run_time = 0
        self.stats = {
            'Enabled': False,
            'Bind Address': lambda s: repr(self.bind_addr),
            'Run time': lambda s: (not s['Enabled']) and -1 or self.runtime(),
            'Accepts': 0,
            'Accepts/sec': lambda s: s['Accepts'] / self.runtime(),
            'Queue': lambda s: getattr(self.requests, "qsize", None),
            'Threads': lambda s: len(getattr(self.requests, "_threads", [])),
            'Threads Idle': lambda s: getattr(self.requests, "idle", None),
            'Socket Errors': 0,
            'Requests': lambda s: (not s['Enabled']) and -1 or sum([w['Requests'](w) for w
                                       in s['Worker Threads'].values()], 0),
            'Bytes Read': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Read'](w) for w
                                         in s['Worker Threads'].values()], 0),
            'Bytes Written': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Written'](w) for w
                                            in s['Worker Threads'].values()], 0),
            'Work Time': lambda s: (not s['Enabled']) and -1 or sum([w['Work Time'](w) for w
                                         in s['Worker Threads'].values()], 0),
            'Read Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                [w['Bytes Read'](w) / (w['Work Time'](w) or 1e-6)
                 for w in s['Worker Threads'].values()], 0),
            'Write Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                [w['Bytes Written'](w) / (w['Work Time'](w) or 1e-6)
                 for w in s['Worker Threads'].values()], 0),
            'Worker Threads': {},
            }
        logging.statistics["CherryPy HTTPServer %d" % id(self)] = self.stats

    def runtime(self):
        if self._start_time is None:
            return self._run_time
        else:
            return self._run_time + (time.time() - self._start_time)

    def __str__(self):
        return "%s.%s(%r)" % (self.__module__, self.__class__.__name__,
                              self.bind_addr)

    def _get_bind_addr(self):
        return self._bind_addr
    def _set_bind_addr(self, value):
        if isinstance(value, tuple) and value[0] in ('', None):
            # Despite the socket module docs, using '' does not
            # allow AI_PASSIVE to work. Passing None instead
            # returns '0.0.0.0' like we want. In other words:
            #     host    AI_PASSIVE     result
            #      ''         Y         192.168.x.y
            #      ''         N         192.168.x.y
            #     None        Y         0.0.0.0
            #     None        N         127.0.0.1
            # But since you can get the same effect with an explicit
            # '0.0.0.0', we deny both the empty string and None as values.
            raise ValueError("Host values of '' or None are not allowed. "
                             "Use '0.0.0.0' (IPv4) or '::' (IPv6) instead "
                             "to listen on all active interfaces.")
        self._bind_addr = value
    bind_addr = property(_get_bind_addr, _set_bind_addr,
        doc="""The interface on which to listen for connections.

        For TCP sockets, a (host, port) tuple. Host values may be any IPv4
        or IPv6 address, or any valid hostname. The string 'localhost' is a
        synonym for '127.0.0.1' (or '::1', if your hosts file prefers IPv6).
        The string '0.0.0.0' is a special IPv4 entry meaning "any active
        interface" (INADDR_ANY), and '::' is the similar IN6ADDR_ANY for
        IPv6. The empty string or None are not allowed.

        For UNIX sockets, supply the filename as a string.""")

    def start(self):
        """Run the server forever."""
        # We don't have to trap KeyboardInterrupt or SystemExit here,
        # because cherrpy.server already does so, calling self.stop() for us.
        # If you're using this server with another framework, you should
        # trap those exceptions in whatever code block calls start().
        self._interrupt = None

        if self.software is None:
            self.software = "%s Server" % self.version

        # Select the appropriate socket
        if isinstance(self.bind_addr, basestring):
            # AF_UNIX socket

            # So we can reuse the socket...
            try: os.unlink(self.bind_addr)
            except: pass

            # So everyone can access the socket...
            try: os.chmod(self.bind_addr, 511) # 0777
            except: pass

            info = [(socket.AF_UNIX, socket.SOCK_STREAM, 0, "", self.bind_addr)]
        else:
            # AF_INET or AF_INET6 socket
            # Get the correct address family for our host (allows IPv6 addresses)
            host, port = self.bind_addr
            try:
                info = socket.getaddrinfo(host, port, socket.AF_UNSPEC,
                                          socket.SOCK_STREAM, 0, socket.AI_PASSIVE)
            except socket.gaierror:
                if ':' in self.bind_addr[0]:
                    info = [(socket.AF_INET6, socket.SOCK_STREAM,
                             0, "", self.bind_addr + (0, 0))]
                else:
                    info = [(socket.AF_INET, socket.SOCK_STREAM,
                             0, "", self.bind_addr)]

        self.socket = None
        msg = "No socket could be created"
        for res in info:
            af, socktype, proto, canonname, sa = res
            try:
                self.bind(af, socktype, proto)
            except socket.error:
                if self.socket:
                    self.socket.close()
                self.socket = None
                continue
            break
        if not self.socket:
            raise socket.error(msg)

        # Timeout so KeyboardInterrupt can be caught on Win32
        self.socket.settimeout(1)
        self.socket.listen(self.request_queue_size)

        # Create worker threads
        self.requests.start()

        self.ready = True
        self._start_time = time.time()
        while self.ready:
            try:
                self.tick()
            except (KeyboardInterrupt, SystemExit):
                raise
            except:
                self.error_log("Error in HTTPServer.tick", level=logging.ERROR,
                               traceback=True)
            if self.interrupt:
                while self.interrupt is True:
                    # Wait for self.stop() to complete. See _set_interrupt.
                    time.sleep(0.1)
                if self.interrupt:
                    raise self.interrupt

    def error_log(self, msg="", level=20, traceback=False):
        # Override this in subclasses as desired
        sys.stderr.write(msg + '\n')
        sys.stderr.flush()
        if traceback:
            tblines = format_exc()
            sys.stderr.write(tblines)
            sys.stderr.flush()

    def bind(self, family, type, proto=0):
        """Create (or recreate) the actual socket object."""
        self.socket = socket.socket(family, type, proto)
        prevent_socket_inheritance(self.socket)
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        if self.nodelay and not isinstance(self.bind_addr, str):
            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

        if self.ssl_adapter is not None:
            self.socket = self.ssl_adapter.bind(self.socket)

        # If listening on the IPV6 any address ('::' = IN6ADDR_ANY),
        # activate dual-stack. See http://www.cherrypy.org/ticket/871.
        if (hasattr(socket, 'AF_INET6') and family == socket.AF_INET6
            and self.bind_addr[0] in ('::', '::0', '::0.0.0.0')):
            try:
                self.socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
            except (AttributeError, socket.error):
                # Apparently, the socket option is not available in
                # this machine's TCP stack
                pass

        self.socket.bind(self.bind_addr)

    def tick(self):
        """Accept a new connection and put it on the Queue."""
        try:
            s, addr = self.socket.accept()
            if self.stats['Enabled']:
                self.stats['Accepts'] += 1
            if not self.ready:
                return

            prevent_socket_inheritance(s)
            if hasattr(s, 'settimeout'):
                s.settimeout(self.timeout)

            makefile = CP_makefile
            ssl_env = {}
            # if ssl cert and key are set, we try to be a secure HTTP server
            if self.ssl_adapter is not None:
                try:
                    s, ssl_env = self.ssl_adapter.wrap(s)
                except NoSSLError:
                    msg = ("The client sent a plain HTTP request, but "
                           "this server only speaks HTTPS on this port.")
                    buf = ["%s 400 Bad Request\r\n" % self.protocol,
                           "Content-Length: %s\r\n" % len(msg),
                           "Content-Type: text/plain\r\n\r\n",
                           msg]

                    wfile = makefile(s, "wb", DEFAULT_BUFFER_SIZE)
                    try:
                        wfile.write("".join(buf).encode('ISO-8859-1'))
                    except socket.error:
                        x = sys.exc_info()[1]
                        if x.args[0] not in socket_errors_to_ignore:
                            raise
                    return
                if not s:
                    return
                makefile = self.ssl_adapter.makefile
                # Re-apply our timeout since we may have a new socket object
                if hasattr(s, 'settimeout'):
                    s.settimeout(self.timeout)

            conn = self.ConnectionClass(self, s, makefile)

            if not isinstance(self.bind_addr, basestring):
                # optional values
                # Until we do DNS lookups, omit REMOTE_HOST
                if addr is None: # sometimes this can happen
                    # figure out if AF_INET or AF_INET6.
                    if len(s.getsockname()) == 2:
                        # AF_INET
                        addr = ('0.0.0.0', 0)
                    else:
                        # AF_INET6
                        addr = ('::', 0)
                conn.remote_addr = addr[0]
                conn.remote_port = addr[1]

            conn.ssl_env = ssl_env

            self.requests.put(conn)
        except socket.timeout:
            # The only reason for the timeout in start() is so we can
            # notice keyboard interrupts on Win32, which don't interrupt
            # accept() by default
            return
        except socket.error:
            x = sys.exc_info()[1]
            if self.stats['Enabled']:
                self.stats['Socket Errors'] += 1
            if x.args[0] in socket_error_eintr:
                # I *think* this is right. EINTR should occur when a signal
                # is received during the accept() call; all docs say retry
                # the call, and I *think* I'm reading it right that Python
                # will then go ahead and poll for and handle the signal
                # elsewhere. See http://www.cherrypy.org/ticket/707.
                return
            if x.args[0] in socket_errors_nonblocking:
                # Just try again. See http://www.cherrypy.org/ticket/479.
                return
            if x.args[0] in socket_errors_to_ignore:
                # Our socket was closed.
                # See http://www.cherrypy.org/ticket/686.
                return
            raise

    def _get_interrupt(self):
        return self._interrupt
    def _set_interrupt(self, interrupt):
        self._interrupt = True
        self.stop()
        self._interrupt = interrupt
    interrupt = property(_get_interrupt, _set_interrupt,
                         doc="Set this to an Exception instance to "
                             "interrupt the server.")

    def stop(self):
        """Gracefully shutdown a server that is serving forever."""
        self.ready = False
        if self._start_time is not None:
            self._run_time += (time.time() - self._start_time)
        self._start_time = None

        sock = getattr(self, "socket", None)
        if sock:
            if not isinstance(self.bind_addr, basestring):
                # Touch our own socket to make accept() return immediately.
                try:
                    host, port = sock.getsockname()[:2]
                except socket.error:
                    x = sys.exc_info()[1]
                    if x.args[0] not in socket_errors_to_ignore:
                        # Changed to use error code and not message
                        # See http://www.cherrypy.org/ticket/860.
                        raise
                else:
                    # Note that we're explicitly NOT using AI_PASSIVE,
                    # here, because we want an actual IP to touch.
                    # localhost won't work if we've bound to a public IP,
                    # but it will if we bound to '0.0.0.0' (INADDR_ANY).
                    for res in socket.getaddrinfo(host, port, socket.AF_UNSPEC,
                                                  socket.SOCK_STREAM):
                        af, socktype, proto, canonname, sa = res
                        s = None
                        try:
                            s = socket.socket(af, socktype, proto)
                            # See http://groups.google.com/group/cherrypy-users/
                            #        browse_frm/thread/bbfe5eb39c904fe0
                            s.settimeout(1.0)
                            s.connect((host, port))
                            s.close()
                        except socket.error:
                            if s:
                                s.close()
            if hasattr(sock, "close"):
                sock.close()
            self.socket = None

        self.requests.stop(self.shutdown_timeout)


class Gateway(object):
    """A base class to interface HTTPServer with other systems, such as WSGI."""

    def __init__(self, req):
        self.req = req

    def respond(self):
        """Process the current request. Must be overridden in a subclass."""
        raise NotImplemented


# These may either be wsgiserver.SSLAdapter subclasses or the string names
# of such classes (in which case they will be lazily loaded).
ssl_adapters = {
    'builtin': 'translate.misc.wsgiserver.ssl_builtin.BuiltinSSLAdapter',
    }

def get_ssl_adapter_class(name='builtin'):
    """Return an SSL adapter class for the given name."""
    adapter = ssl_adapters[name.lower()]
    if isinstance(adapter, basestring):
        last_dot = adapter.rfind(".")
        attr_name = adapter[last_dot + 1:]
        mod_path = adapter[:last_dot]

        try:
            mod = sys.modules[mod_path]
            if mod is None:
                raise KeyError()
        except KeyError:
            # The last [''] is important.
            mod = __import__(mod_path, globals(), locals(), [''])

        # Let an AttributeError propagate outward.
        try:
            adapter = getattr(mod, attr_name)
        except AttributeError:
            raise AttributeError("'%s' object has no attribute '%s'"
                                 % (mod_path, attr_name))

    return adapter

# -------------------------------- WSGI Stuff -------------------------------- #


class CherryPyWSGIServer(HTTPServer):
    """A subclass of HTTPServer which calls a WSGI application."""

    wsgi_version = (1, 0)
    """The version of WSGI to produce."""

    def __init__(self, bind_addr, wsgi_app, numthreads=10, server_name=None,
                 max=-1, request_queue_size=5, timeout=10, shutdown_timeout=5):
        self.requests = ThreadPool(self, min=numthreads or 1, max=max)
        self.wsgi_app = wsgi_app
        self.gateway = wsgi_gateways[self.wsgi_version]

        self.bind_addr = bind_addr
        if not server_name:
            server_name = socket.gethostname()
        self.server_name = server_name
        self.request_queue_size = request_queue_size

        self.timeout = timeout
        self.shutdown_timeout = shutdown_timeout
        self.clear_stats()

    def _get_numthreads(self):
        return self.requests.min
    def _set_numthreads(self, value):
        self.requests.min = value
    numthreads = property(_get_numthreads, _set_numthreads)


class WSGIGateway(Gateway):
    """A base class to interface HTTPServer with WSGI."""

    def __init__(self, req):
        self.req = req
        self.started_response = False
        self.env = self.get_environ()
        self.remaining_bytes_out = None

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        raise NotImplemented

    def respond(self):
        """Process the current request."""
        response = self.req.server.wsgi_app(self.env, self.start_response)
        try:
            for chunk in response:
                # "The start_response callable must not actually transmit
                # the response headers. Instead, it must store them for the
                # server or gateway to transmit only after the first
                # iteration of the application return value that yields
                # a NON-EMPTY string, or upon the application's first
                # invocation of the write() callable." (PEP 333)
                if chunk:
                    if isinstance(chunk, unicodestr):
                        chunk = chunk.encode('ISO-8859-1')
                    self.write(chunk)
        finally:
            if hasattr(response, "close"):
                response.close()

    def start_response(self, status, headers, exc_info = None):
        """WSGI callable to begin the HTTP response."""
        # "The application may call start_response more than once,
        # if and only if the exc_info argument is provided."
        if self.started_response and not exc_info:
            raise AssertionError("WSGI start_response called a second "
                                 "time with no exc_info.")
        self.started_response = True

        # "if exc_info is provided, and the HTTP headers have already been
        # sent, start_response must raise an error, and should raise the
        # exc_info tuple."
        if self.req.sent_headers:
            try:
                raise exc_info[0](exc_info[1]).with_traceback(exc_info[2])
            finally:
                exc_info = None

        # According to PEP 3333, when using Python 3, the response status
        # and headers must be bytes masquerading as unicode; that is, they
        # must be of type "str" but are restricted to code points in the
        # "latin-1" set.
        if not isinstance(status, str):
            raise TypeError("WSGI response status is not of type str.")
        self.req.status = status.encode('ISO-8859-1')

        for k, v in headers:
            if not isinstance(k, str):
                raise TypeError("WSGI response header key %r is not of type str." % k)
            if not isinstance(v, str):
                raise TypeError("WSGI response header value %r is not of type str." % v)
            if k.lower() == 'content-length':
                self.remaining_bytes_out = int(v)
            self.req.outheaders.append((k.encode('ISO-8859-1'), v.encode('ISO-8859-1')))

        return self.write

    def write(self, chunk):
        """WSGI callable to write unbuffered data to the client.

        This method is also used internally by start_response (to write
        data from the iterable returned by the WSGI application).
        """
        if not self.started_response:
            raise AssertionError("WSGI write called before start_response.")

        chunklen = len(chunk)
        rbo = self.remaining_bytes_out
        if rbo is not None and chunklen > rbo:
            if not self.req.sent_headers:
                # Whew. We can send a 500 to the client.
                self.req.simple_response("500 Internal Server Error",
                    "The requested resource returned more bytes than the "
                    "declared Content-Length.")
            else:
                # Dang. We have probably already sent data. Truncate the chunk
                # to fit (so the client doesn't hang) and raise an error later.
                chunk = chunk[:rbo]

        if not self.req.sent_headers:
            self.req.sent_headers = True
            self.req.send_headers()

        self.req.write(chunk)

        if rbo is not None:
            rbo -= chunklen
            if rbo < 0:
                raise ValueError(
                    "Response body exceeds the declared Content-Length.")


class WSGIGateway_10(WSGIGateway):
    """A Gateway class to interface HTTPServer with WSGI 1.0.x."""

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        req = self.req
        env = {
            # set a non-standard environ entry so the WSGI app can know what
            # the *real* server protocol is (and what features to support).
            # See http://www.faqs.org/rfcs/rfc2145.html.
            'ACTUAL_SERVER_PROTOCOL': req.server.protocol,
            'PATH_INFO': req.path.decode('ISO-8859-1'),
            'QUERY_STRING': req.qs.decode('ISO-8859-1'),
            'REMOTE_ADDR': req.conn.remote_addr or '',
            'REMOTE_PORT': str(req.conn.remote_port or ''),
            'REQUEST_METHOD': req.method.decode('ISO-8859-1'),
            'REQUEST_URI': req.uri,
            'SCRIPT_NAME': '',
            'SERVER_NAME': req.server.server_name,
            # Bah. "SERVER_PROTOCOL" is actually the REQUEST protocol.
            'SERVER_PROTOCOL': req.request_protocol.decode('ISO-8859-1'),
            'SERVER_SOFTWARE': req.server.software,
            'wsgi.errors': sys.stderr,
            'wsgi.input': req.rfile,
            'wsgi.multiprocess': False,
            'wsgi.multithread': True,
            'wsgi.run_once': False,
            'wsgi.url_scheme': req.scheme.decode('ISO-8859-1'),
            'wsgi.version': (1, 0),
            }

        if isinstance(req.server.bind_addr, basestring):
            # AF_UNIX. This isn't really allowed by WSGI, which doesn't
            # address unix domain sockets. But it's better than nothing.
            env["SERVER_PORT"] = ""
        else:
            env["SERVER_PORT"] = str(req.server.bind_addr[1])

        # Request headers
        for k, v in req.inheaders.items():
            k = k.decode('ISO-8859-1').upper().replace("-", "_")
            env["HTTP_" + k] = v.decode('ISO-8859-1')

        # CONTENT_TYPE/CONTENT_LENGTH
        ct = env.pop("HTTP_CONTENT_TYPE", None)
        if ct is not None:
            env["CONTENT_TYPE"] = ct
        cl = env.pop("HTTP_CONTENT_LENGTH", None)
        if cl is not None:
            env["CONTENT_LENGTH"] = cl

        if req.conn.ssl_env:
            env.update(req.conn.ssl_env)

        return env


class WSGIGateway_u0(WSGIGateway_10):
    """A Gateway class to interface HTTPServer with WSGI u.0.

    WSGI u.0 is an experimental protocol, which uses unicode for keys and values
    in both Python 2 and Python 3.
    """

    def get_environ(self):
        """Return a new environ dict targeting the given wsgi.version"""
        req = self.req
        env_10 = WSGIGateway_10.get_environ(self)
        env = env_10.copy()
        env['wsgi.version'] = ('u', 0)

        # Request-URI
        env.setdefault('wsgi.url_encoding', 'utf-8')
        try:
            # SCRIPT_NAME is the empty string, who cares what encoding it is?
            env["PATH_INFO"] = req.path.decode(env['wsgi.url_encoding'])
            env["QUERY_STRING"] = req.qs.decode(env['wsgi.url_encoding'])
        except UnicodeDecodeError:
            # Fall back to latin 1 so apps can transcode if needed.
            env['wsgi.url_encoding'] = 'ISO-8859-1'
            env["PATH_INFO"] = env_10["PATH_INFO"]
            env["QUERY_STRING"] = env_10["QUERY_STRING"]

        return env

wsgi_gateways = {
    (1, 0): WSGIGateway_10,
    ('u', 0): WSGIGateway_u0,
}

class WSGIPathInfoDispatcher(object):
    """A WSGI dispatcher for dispatch based on the PATH_INFO.

    apps: a dict or list of (path_prefix, app) pairs.
    """

    def __init__(self, apps):
        try:
            apps = list(apps.items())
        except AttributeError:
            pass

        # Sort the apps by len(path), descending
        apps.sort()
        apps.reverse()

        # The path_prefix strings must start, but not end, with a slash.
        # Use "" instead of "/".
        self.apps = [(p.rstrip("/"), a) for p, a in apps]

    def __call__(self, environ, start_response):
        path = environ["PATH_INFO"] or "/"
        for p, app in self.apps:
            # The apps list should be sorted by length, descending.
            if path.startswith(p + "/") or path == p:
                environ = environ.copy()
                environ["SCRIPT_NAME"] = environ["SCRIPT_NAME"] + p
                environ["PATH_INFO"] = path[len(p):]
                return app(environ, start_response)

        start_response('404 Not Found', [('Content-Type', 'text/plain'),
                                         ('Content-Length', '0')])
        return ['']


########NEW FILE########
__FILENAME__ = wStringIO
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A wrapper for cStringIO that provides more of the functions of
StringIO at the speed of cStringIO"""

import cStringIO


class StringIO:

    def __init__(self, buf=''):
        if not isinstance(buf, (str, unicode)):
            buf = str(buf)
        if isinstance(buf, unicode):
            buf = buf.encode('utf-8')
        self.len = len(buf)
        self.buf = cStringIO.StringIO()
        self.buf.write(buf)
        self.buf.seek(0)
        self.pos = 0
        self.closed = 0

    def __iter__(self):
        return self

    def next(self):
        if self.closed:
            raise StopIteration
        r = self.readline()
        if not r:
            raise StopIteration
        return r

    def close(self):
        """Free the memory buffer.
        """
        if not self.closed:
            self.closed = 1
            del self.buf, self.pos

    def isatty(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        return False

    def seek(self, pos, mode=0):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.buf.seek(pos, mode)
        self.pos = self.buf.tell()

    def tell(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        return self.pos

    def read(self, n=None):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        if n is None:
            r = self.buf.read()
        else:
            r = self.buf.read(n)
        self.pos = self.buf.tell()
        return r

    def readline(self, length=None):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        if length is not None:
            r = self.buf.readline(length)
        else:
            r = self.buf.readline()
        self.pos = self.buf.tell()
        return r

    def readlines(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        lines = self.buf.readlines()
        self.pos = self.buf.tell()
        return lines

    def truncate(self, size=None):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.buf.truncate(size)
        self.pos = self.buf.tell()
        self.buf.seek(0, 2)
        self.len = self.buf.tell()
        self.buf.seek(self.pos)

    def write(self, s):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        origpos = self.buf.tell()
        self.buf.write(s)
        self.pos = self.buf.tell()
        if origpos + len(s) > self.len:
            self.buf.seek(0, 2)
            self.len = self.buf.tell()
            self.buf.seek(self.pos)

    def writelines(self, lines):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.buf.writelines(lines)
        self.pos = self.buf.tell()
        self.buf.seek(0, 2)
        self.len = self.buf.tell()
        self.buf.seek(self.pos)

    def flush(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        self.buf.flush()

    def getvalue(self):
        if self.closed:
            raise ValueError("I/O operation on closed file")
        return self.buf.getvalue()


class CatchStringOutput(StringIO, object):
    """catches the output before it is closed and sends it to an onclose
    method"""

    def __init__(self, onclose):
        """Set up the output stream, and remember a method to call on
        closing"""
        StringIO.__init__(self)
        self.onclose = onclose

    def close(self):
        """wrap the underlying close method, to pass the value to onclose
        before it goes"""
        value = self.getvalue()
        self.onclose(value)
        super(CatchStringOutput, self).close()

    def slam(self):
        """use this method to force the closing of the stream if it isn't
        closed yet"""
        if not self.closed:
            self.close()

########NEW FILE########
__FILENAME__ = xml_helpers
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Helper functions for working with XML."""

import re

from lxml import etree


# some useful xpath expressions
xml_preserve_ancestors = etree.XPath("ancestor-or-self::*[attribute::xml:space='preserve']")
"""All ancestors with xml:space='preserve'"""

xml_space_ancestors = etree.XPath("ancestor-or-self::*/attribute::xml:space")
"""All xml:space attributes in the ancestors"""

string_xpath = etree.XPath("string()")
"""Return a non-normalized string in the node subtree"""

string_xpath_normalized = etree.XPath("normalize-space()")
"""Return a (space) normalized string in the node subtree"""


def getText(node, xml_space="preserve"):
    """Extracts the plain text content out of the given node.

    This method checks the xml:space attribute of the given node, and takes
    an optional default to use in case nothing is specified in this node."""
    xml_space = getXMLspace(node, xml_space)
    if xml_space == "default":
        return unicode(string_xpath_normalized(node))  # specific to lxml.etree
    else:
        return unicode(string_xpath(node))  # specific to lxml.etree

    # If we want to normalise space and only preserve it when the directive
    # xml:space="preserve" is given in node or in parents, consider this code:
    #xml_preserves = xml_preserve_ancestors(node)
    #if xml_preserves and xml_preserves[-1] == "preserve":
    #    return unicode(string_xpath(node)) # specific to lxml.etree
    #else:
    #    return unicode(string_xpath_normalized(node)) # specific to lxml.etree


XML_NS = 'http://www.w3.org/XML/1998/namespace'


def getXMLlang(node):
    """Gets the xml:lang attribute on node"""
    return node.get("{%s}lang" % XML_NS)


def setXMLlang(node, lang):
    """Sets the xml:lang attribute on node"""
    node.set("{%s}lang" % XML_NS, lang)


def getXMLspace(node, default=None):
    """Gets the xml:space attribute on node"""
    value = node.get("{%s}space" % XML_NS)
    if value is None:
        value = default
    return value


def setXMLspace(node, value):
    """Sets the xml:space attribute on node"""
    node.set("{%s}space" % XML_NS, value)


def namespaced(namespace, name):
    """Returns name in Clark notation within the given namespace.

       For example namespaced("source") in an XLIFF document might return::
           {urn:oasis:names:tc:xliff:document:1.1}source

       This is needed throughout lxml.
    """
    if namespace:
        return "{%s}%s" % (namespace, name)
    else:
        return name

MULTIWHITESPACE_PATTERN = r"[\n\r\t ]+"
MULTIWHITESPACE_RE = re.compile(MULTIWHITESPACE_PATTERN, re.MULTILINE)


def normalize_space(text):
    """Normalize the given text for implementation of
    ``xml:space="default"``."""
    text = MULTIWHITESPACE_RE.sub(u" ", text)
    return text


def normalize_xml_space(node, xml_space, remove_start=False):
    """normalize spaces following the nodes xml:space, or alternatively the
    given xml_space parameter."""
    xml_space = getXMLspace(node) or xml_space
    if xml_space == 'preserve':
        return
    if node.text:
        node.text = normalize_space(node.text)
        if remove_start and node.text[0] == u" ":
            node.text = node.text.lstrip()
            remove_start = False
        if len(node.text) > 0 and node.text.endswith(u" "):
            remove_start = True
        if len(node) == 0:
            node.text = node.text.rstrip()
    if node.tail:
        node.tail = normalize_space(node.tail)

    for child in node:
        normalize_xml_space(child, remove_start)

########NEW FILE########
__FILENAME__ = CommonIndexer
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


"""
base class for interfaces to indexing engines for pootle
"""

import os

import translate.lang.data


def is_available():
    """Check if this indexing engine interface is usable.

    This function must exist in every module that contains indexing engine
    interfaces.

    :return: is this interface usable?
    :rtype: bool
    """
    return False


class CommonDatabase(object):
    """Base class for indexing support.

    Any real implementation must override most methods of this class.
    """

    field_analyzers = {}
    """mapping of field names and analyzers - see
    :meth:`~.CommonDatabase.set_field_analyzers`"""

    ANALYZER_EXACT = 0
    """exact matching: the query string must equal the whole term string"""

    ANALYZER_PARTIAL = 1 << 1
    """partial matching: a document matches, even if the query string only
    matches the beginning of the term value."""

    ANALYZER_TOKENIZE = 1 << 2
    """tokenize terms and queries automatically"""

    ANALYZER_DEFAULT = ANALYZER_TOKENIZE | ANALYZER_PARTIAL
    """the default analyzer to be used if nothing is configured"""

    QUERY_TYPE = None
    """override this with the query class of the implementation"""

    INDEX_DIRECTORY_NAME = None
    """override this with a string to be used as the name of the indexing
    directory/file in the filesystem
    """

    def __init__(self, basedir, analyzer=None, create_allowed=True):
        """initialize or open an indexing database

        Any derived class must override ``__init__``.

        Any implementation can rely on the "self.location" attribute to be set
        by the ``__init__`` function of the super class.

        :raise ValueError: the given location exists, but the database type
                           is incompatible (e.g. created by a different
                           indexing engine)
        :raise OSError: the database failed to initialize

        :param basedir: the parent directory of the database
        :type basedir: str
        :param analyzer: bitwise combination of possible analyzer flags
                         to be used as the default analyzer for this
                         database. Leave it empty to use the system
                         default analyzer (``self.ANALYZER_DEFAULT``).
                         see :attr:`CommonDatabase.ANALYZER_TOKENIZE`,
                         :attr:`CommonDatabase.ANALYZER_PARTIAL`, ...
        :type analyzer: int
        :param create_allowed: create the database, if necessary.
        :type create_allowed: bool
        """
        # just do some checks
        if self.QUERY_TYPE is None:
            raise NotImplementedError("Incomplete indexer implementation: "
                                      "'QUERY_TYPE' is undefined")
        if self.INDEX_DIRECTORY_NAME is None:
            raise NotImplementedError("Incomplete indexer implementation: "
                                      "'INDEX_DIRECTORY_NAME' is undefined")
        self.location = os.path.join(basedir, self.INDEX_DIRECTORY_NAME)
        if (not create_allowed) and (not os.path.exists(self.location)):
            raise OSError("Indexer: the database does not exist - and I am"
                          " not configured to create it.")
        if analyzer is None:
            self.analyzer = self.ANALYZER_DEFAULT
        else:
            self.analyzer = analyzer
        self.field_analyzers = {}

    def flush(self, optimize=False):
        """Flush the content of the database - to force changes to be written
        to disk.

        Some databases also support index optimization.

        :param optimize: should the index be optimized if possible?
        :type optimize: bool
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'flush' is missing")

    def make_query(self, args, require_all=True, analyzer=None):
        """Create simple queries (strings or field searches) or
        combine multiple queries (AND/OR).

        To specifiy rules for field searches, you may want to take a look at
        :meth:`~.CommonDatabase.set_field_analyzers`. The parameter
        'match_text_partial' can override the previously defined
        default setting.

        :param args: queries or search string or description of field query
                     examples::

                        [xapian.Query("foo"), xapian.Query("bar")]
                        xapian.Query("foo")
                        "bar"
                        {"foo": "bar", "foobar": "foo"}

        :type args: list of queries | single query | str | dict
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: boolean
        :param analyzer: (only applicable for 'dict' or 'str')
                         Define query options (partial matching, exact
                         matching, tokenizing, ...) as bitwise
                         combinations of *CommonIndexer.ANALYZER_???*.

                         This can override previously defined field
                         analyzer settings.

                         If analyzer is ``None`` (default), then the
                         configured analyzer for the field is used.
        :type analyzer: int
        :return: the combined query
        :rtype: query type of the specific implementation
        """
        # turn a dict into a list if necessary
        if isinstance(args, dict):
            args = args.items()
        # turn 'args' into a list if necessary
        if not isinstance(args, list):
            args = [args]
        # combine all given queries
        result = []
        for query in args:
            # just add precompiled queries
            if isinstance(query, self.QUERY_TYPE):
                result.append(self._create_query_for_query(query))
            # create field/value queries out of a tuple
            elif isinstance(query, tuple):
                field, value = query
                # perform unicode normalization
                field = translate.lang.data.normalize(unicode(field))
                value = translate.lang.data.normalize(unicode(value))
                # check for the choosen match type
                if analyzer is None:
                    analyzer = self.get_field_analyzers(field)
                result.append(self._create_query_for_field(field, value,
                        analyzer=analyzer))
            # parse plaintext queries
            elif isinstance(query, basestring):
                if analyzer is None:
                    analyzer = self.analyzer
                # perform unicode normalization
                query = translate.lang.data.normalize(unicode(query))
                result.append(self._create_query_for_string(query,
                        require_all=require_all, analyzer=analyzer))
            else:
                # other types of queries are not supported
                raise ValueError("Unable to handle query type: %s" %
                                 str(type(query)))
        # return the combined query
        return self._create_query_combined(result, require_all)

    def _create_query_for_query(self, query):
        """Generate a query based on an existing query object.

        Basically this function should just create a copy of the original.

        :param query: the original query object
        :type query: ``xapian.Query``
        :return: the resulting query object
        :rtype: ``xapian.Query`` | ``PyLucene.Query``
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_create_query_for_query' is missing")

    def _create_query_for_string(self, text, require_all=True,
            analyzer=None):
        """Generate a query for a plain term of a string query.

        Basically this function parses the string and returns the resulting
        query.

        :param text: the query string
        :type text: str
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :param analyzer: Define query options (partial matching, exact matching,
                         tokenizing, ...) as bitwise combinations of
                         *CommonIndexer.ANALYZER_???*.
                         This can override previously defined field
                         analyzer settings.
                         If analyzer is None (default), then the configured
                         analyzer for the field is used.
        :type analyzer: int
        :return: resulting query object
        :rtype: xapian.Query | PyLucene.Query
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_create_query_for_string' is missing")

    def _create_query_for_field(self, field, value, analyzer=None):
        """Generate a field query.

        This functions creates a field->value query.

        :param field: the fieldname to be used
        :type field: str
        :param value: the wanted value of the field
        :type value: str
        :param analyzer: Define query options (partial matching, exact matching,
                         tokenizing, ...) as bitwise combinations of
                         *CommonIndexer.ANALYZER_???*.
                         This can override previously defined field
                         analyzer settings.
                         If analyzer is None (default), then the configured
                         analyzer for the field is used.
        :type analyzer: int
        :return: resulting query object
        :rtype: ``xapian.Query`` | ``PyLucene.Query``
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_create_query_for_field' is missing")

    def _create_query_combined(self, queries, require_all=True):
        """generate a combined query

        :param queries: list of the original queries
        :type queries: list of xapian.Query
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :return: the resulting combined query object
        :rtype: ``xapian.Query`` | ``PyLucene.Query``
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_create_query_combined' is missing")

    def index_document(self, data):
        """Add the given data to the database.

        :param data: the data to be indexed.
                     A dictionary will be treated as ``fieldname:value``
                     combinations.
                     If the fieldname is None then the value will be
                     interpreted as a plain term or as a list of plain terms.
                     Lists of terms are indexed separately.
                     Lists of strings are treated as plain terms.
        :type data: dict | list of str
        """
        doc = self._create_empty_document()
        if isinstance(data, dict):
            data = data.items()
        # add all data
        for dataset in data:
            if isinstance(dataset, tuple):
                # the dataset tuple consists of '(key, value)'
                key, value = dataset
                if key is None:
                    if isinstance(value, list):
                        terms = value[:]
                    elif isinstance(value, basestring):
                        terms = [value]
                    else:
                        raise ValueError("Invalid data type to be indexed: %s" %
                                         str(type(data)))
                    for one_term in terms:
                        self._add_plain_term(doc, self._decode(one_term),
                                (self.ANALYZER_DEFAULT & self.ANALYZER_TOKENIZE > 0))
                else:
                    analyze_settings = self.get_field_analyzers(key)
                    # handle multiple terms
                    if not isinstance(value, list):
                        value = [value]
                    for one_term in value:
                        self._add_field_term(doc, key, self._decode(one_term),
                                (analyze_settings & self.ANALYZER_TOKENIZE > 0))
            elif isinstance(dataset, basestring):
                self._add_plain_term(doc, self._decode(dataset),
                        (self.ANALYZER_DEFAULT & self.ANALYZER_TOKENIZE > 0))
            else:
                raise ValueError("Invalid data type to be indexed: %s" %
                                 str(type(data)))
        self._add_document_to_index(doc)

    def _create_empty_document(self):
        """Create an empty document to be filled and added to the index later.

        :return: the new document object
        :rtype: ``xapian.Document`` | ``PyLucene.Document``
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_create_empty_document' is missing")

    def _add_plain_term(self, document, term, tokenize=True):
        """Add a term to a document.

        :param document: the document to be changed
        :type document: ``xapian.Document`` | ``PyLucene.Document``
        :param term: a single term to be added
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_add_plain_term' is missing")

    def _add_field_term(self, document, field, term, tokenize=True):
        """Add a field term to a document.

        :param document: the document to be changed
        :type document: ``xapian.Document`` | ``PyLucene.Document``
        :param field: name of the field
        :type field: str
        :param term: term to be associated to the field
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_add_field_term' is missing")

    def _add_document_to_index(self, document):
        """Add a prepared document to the index database.

        :param document: the document to be added
        :type document: xapian.Document | PyLucene.Document
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'_add_document_to_index' is missing")

    def begin_transaction(self):
        """begin a transaction

        You can group multiple modifications of a database as a transaction.
        This prevents time-consuming database flushing and helps, if you want
        that a changeset is committed either completely or not at all.
        No changes will be written to disk until 'commit_transaction'.
        'cancel_transaction' can be used to revert an ongoing transaction.

        Database types that do not support transactions may silently ignore it.
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'begin_transaction' is missing")

    def cancel_transaction(self):
        """cancel an ongoing transaction

        See 'start_transaction' for details.
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'cancel_transaction' is missing")

    def commit_transaction(self):
        """Submit the currently ongoing transaction and write changes to disk.

        See 'start_transaction' for details.
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'commit_transaction' is missing")

    def get_query_result(self, query):
        """return an object containing the results of a query

        :param query: a pre-compiled query
        :type query: a query object of the real implementation
        :return: an object that allows access to the results
        :rtype: subclass of CommonEnquire
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'get_query_result' is missing")

    def delete_document_by_id(self, docid):
        """Delete a specified document.

        :param docid: the document ID to be deleted
        :type docid: int
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'delete_document_by_id' is missing")

    def search(self, query, fieldnames):
        """Return a list of the contents of specified fields for all
        matches of a query.

        :param query: the query to be issued
        :type query: a query object of the real implementation
        :param fieldnames: the name(s) of a field of the document content
        :type fieldnames: string | list of strings
        :return: a list of dicts containing the specified field(s)
        :rtype: list of dicts
        """
        raise NotImplementedError("Incomplete indexer implementation: "
                                  "'search' is missing")

    def delete_doc(self, ident):
        """Delete the documents returned by a query.

        :param ident: [list of] document IDs | dict describing a query | query
        :type ident: int | list of tuples | dict | list of dicts |
                     query (e.g. xapian.Query) | list of queries
        """
        # turn a doc-ID into a list of doc-IDs
        if isinstance(ident, list):
            # it is already a list
            ident_list = ident
        else:
            ident_list = [ident]
        if len(ident_list) == 0:
            # no matching items
            return 0
        if isinstance(ident_list[0], int) or isinstance(ident_list[0], long):
            # create a list of IDs of all successfully removed documents
            success_delete = [match for match in ident_list
                    if self.delete_document_by_id(match)]
            return len(success_delete)
        if isinstance(ident_list[0], dict):
            # something like: { "msgid": "foobar" }
            # assemble all queries
            query = self.make_query([self.make_query(query_dict,
                    require_all=True) for query_dict in ident_list],
                    require_all=True)
        elif isinstance(ident_list[0], object):
            # assume a query object (with 'AND')
            query = self.make_query(ident_list, require_all=True)
        else:
            # invalid element type in list (not necessarily caught in the
            # lines above)
            raise TypeError("description of documents to-be-deleted is not "
                            "supported: list of %s" % type(ident_list[0]))
        # we successfully created a query - now iterate through the result
        # no documents deleted so far ...
        remove_list = []
        # delete all resulting documents step by step

        def add_docid_to_list(match):
            """Collect every document ID."""
            remove_list.append(match["docid"])
        self._walk_matches(query, add_docid_to_list)
        return self.delete_doc(remove_list)

    def _walk_matches(self, query, function, arg_for_function=None):
        """Use this function if you want to do something with every single match
        of a query.

        Example::

            self._walk_matches(query, function_for_match, arg_for_func)

        *function_for_match* expects only one argument: the matched object

        :param query: a query object of the real implementation
        :type query: xapian.Query | PyLucene.Query
        :param function: the function to execute with every match
        :type function: function
        :param arg_for_function: an optional argument for the function
        :type arg_for_function: anything
        """
        # execute the query
        enquire = self.get_query_result(query)
        # start with the first element
        start = 0
        # do the loop at least once
        size, avail = (0, 1)
        # how many results per 'get_matches'?
        steps = 2
        while start < avail:
            (size, avail, matches) = enquire.get_matches(start, steps)
            for match in matches:
                if arg_for_function is None:
                    function(match)
                else:
                    function(match, arg_for_function)
            start += size

    def set_field_analyzers(self, field_analyzers):
        """Set the analyzers for different fields of the database documents.

        All bitwise combinations of *CommonIndexer.ANALYZER_???* are possible.

        :param field_analyzers: mapping of field names and analyzers
        :type field_analyzers: dict containing field names and analyzers
        :raise TypeError: invalid values in *field_analyzers*
        """
        for field, analyzer in field_analyzers.items():
            # check for invald input types
            if not isinstance(field, (str, unicode)):
                raise TypeError("field name must be a string")
            if not isinstance(analyzer, int):
                raise TypeError("the analyzer must be a whole number (int)")
            # map the analyzer to the field name
            self.field_analyzers[field] = analyzer

    def get_field_analyzers(self, fieldnames=None):
        """Return the analyzer that was mapped to a specific field.

        See :meth:`~.CommonDatabase.set_field_analyzers` for details.

        :param fieldnames: the analyzer of this field (or all/multiple fields)
                           is requested; leave empty (or *None*) to
                           request all fields.
        :type fieldnames: str | list of str | None
        :return: The analyzer setting of the field - see
                 *CommonDatabase.ANALYZER_???* or a dict of field names
                 and analyzers
        :rtype: int | dict
        """
        # all field analyzers are requested
        if fieldnames is None:
            # return a copy
            return dict(self.field_analyzers)
        # one field is requested
        if isinstance(fieldnames, (str, unicode)):
            if fieldnames in self.field_analyzers:
                return self.field_analyzers[fieldnames]
            else:
                return self.analyzer
        # a list of fields is requested
        if isinstance(fieldnames, list):
            result = {}
            for field in fieldnames:
                result[field] = self.get_field_analyzers(field)
            return result
        return self.analyzer

    def _decode(self, text):
        """Decode the string from utf-8 or charmap perform
        unicode normalization."""
        if isinstance(text, str):
            try:
                result = unicode(text.decode("UTF-8"))
            except UnicodeEncodeError as e:
                result = unicode(text.decode("charmap"))
        elif not isinstance(text, unicode):
            result = unicode(text)
        else:
            result = text
        # perform unicode normalization
        return translate.lang.data.normalize(result)


class CommonEnquire(object):
    """An enquire object contains the information about the result of a request.
    """

    def __init__(self, enquire):
        """Intialization of a wrapper around enquires of different backends

        :param enquire: a previous enquire
        :type enquire: xapian.Enquire | pylucene-enquire
        """
        self.enquire = enquire

    def get_matches(self, start, number):
        """Return a specified number of qualified matches of a previous query.

        :param start: index of the first match to return (starting from zero)
        :type start: int
        :param number: the number of matching entries to return
        :type number: int
        :return: a set of matching entries and some statistics
        :rtype: tuple of (returned number, available number, matches)
                "matches" is a dictionary of::

                    ["rank", "percent", "document", "docid"]

        """
        raise NotImplementedError("Incomplete indexing implementation: "
                                  "'get_matches' for the 'Enquire' class is missing")

    def get_matches_count(self):
        """Return the estimated number of matches.

        Use :meth:`translate.search.indexing.CommonIndexer.search`
        to retrieve the exact number of matches

        :return: The estimated number of matches
        :rtype: int
        """
        (returned, estimate_count, matches) = self.get_matches(0, 1)
        return estimate_count

########NEW FILE########
__FILENAME__ = PyLuceneIndexer
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


"""
interface for the PyLucene (v2.x) indexing engine

take a look at PyLuceneIndexer1.py for the PyLucene v1.x interface
"""

import logging
import os
import time

# try to import the PyLucene package (with the two possible names)
# remember the type of the detected package (compiled with jcc (>=v2.3) or
# with gcj (<=v2.2)
try:
    import PyLucene
    _COMPILER = 'gcj'
except ImportError:
    # if this fails, then there is no pylucene installed
    import lucene
    PyLucene = lucene
    PyLucene.initVM(PyLucene.CLASSPATH)
    _COMPILER = 'jcc'

import CommonIndexer


UNNAMED_FIELD_NAME = "FieldWithoutAName"
MAX_FIELD_SIZE = 1048576


def is_available():
    return _get_pylucene_version() == 2


class PyLuceneDatabase(CommonIndexer.CommonDatabase):
    """Manage and use a pylucene indexing database."""

    QUERY_TYPE = PyLucene.Query
    INDEX_DIRECTORY_NAME = "lucene"

    def __init__(self, basedir, analyzer=None, create_allowed=True):
        """Initialize or open an indexing database.

        Any derived class must override __init__.

        :raise ValueError: The given location exists, but the database type
                           is incompatible (e.g. created by a different indexing engine)
        :raise OSError: the database failed to initialize

        :param basedir: The parent directory of the database
        :type basedir: str
        :param analyzer: Bitwise combination of possible analyzer flags
                         to be used as the default analyzer for this database.
                         Leave it empty to use the system default analyzer
                         (self.ANALYZER_DEFAULT). See self.ANALYZER_TOKENIZE,
                         self.ANALYZER_PARTIAL, ...
        :type analyzer: int
        :param create_allowed: create the database, if necessary; default: True
        :type create_allowed: bool
        """
        jvm = PyLucene.getVMEnv()
        jvm.attachCurrentThread()
        super(PyLuceneDatabase, self).__init__(basedir, analyzer=analyzer,
                create_allowed=create_allowed)
        self.pyl_analyzer = PyLucene.StandardAnalyzer()
        self.writer = None
        self.reader = None
        self.index_version = None
        try:
            # try to open an existing database
            tempreader = PyLucene.IndexReader.open(self.location)
            tempreader.close()
        except PyLucene.JavaError as err_msg:
            # Write an error out, in case this is a real problem instead of an absence of an index
            # TODO: turn the following two lines into debug output
            #errorstr = str(e).strip() + "\n" + self.errorhandler.traceback_str()
            #DEBUG_FOO("could not open index, so going to create: " + errorstr)
            # Create the index, so we can open cached readers on it
            if not create_allowed:
                raise OSError("Indexer: skipping database creation")
            try:
                # create the parent directory if it does not exist
                parent_path = os.path.dirname(self.location)
                if not os.path.isdir(parent_path):
                    # recursively create all directories up to parent_path
                    os.makedirs(parent_path)
            except IOError as err_msg:
                raise OSError("Indexer: failed to create the parent "
                              "directory (%s) of the indexing database: %s" % (
                              parent_path, err_msg))
            try:
                tempwriter = PyLucene.IndexWriter(self.location,
                        self.pyl_analyzer, True)
                tempwriter.close()
            except PyLucene.JavaError as err_msg:
                raise OSError("Indexer: failed to open or create a Lucene"
                              " database (%s): %s" % (self.location, err_msg))
        # the indexer is initialized - now we prepare the searcher
        # windows file locking seems inconsistent, so we try 10 times
        numtries = 0
        #self.dir_lock.acquire(blocking=True)
        # read "self.reader", "self.indexVersion" and "self.searcher"
        try:
            while numtries < 10:
                try:
                    self.reader = PyLucene.IndexReader.open(self.location)
                    self.indexVersion = self.reader.getCurrentVersion(
                        self.location)
                    self.searcher = PyLucene.IndexSearcher(self.reader)
                    break
                except PyLucene.JavaError as e:
                    # store error message for possible later re-raise (below)
                    lock_error_msg = e
                    time.sleep(0.01)
                    numtries += 1
            else:
                # locking failed for 10 times
                raise OSError("Indexer: failed to lock index database"
                              " (%s)" % lock_error_msg)
        finally:
            pass
        #    self.dir_lock.release()
        # initialize the searcher and the reader
        self._index_refresh()

    def __del__(self):
        """remove lock and close writer after loosing the last reference"""
        jvm = PyLucene.getVMEnv()
        jvm.attachCurrentThread()
        self._writer_close()
        if hasattr(self, "reader") and self.reader is not None:
            self.reader.close()
            self.reader = None
        if hasattr(self, "searcher") and self.searcher is not None:
            self.searcher.close()
            self.searcher = None

    def flush(self, optimize=False):
        """flush the content of the database - to force changes to be written
        to disk

        some databases also support index optimization

        :param optimize: should the index be optimized if possible?
        :type optimize: bool
        """
        keep_open = self._writer_is_open()
        self._writer_open()
        try:
            if optimize:
                self.writer.optimize()
        finally:
            self.writer.flush()
        if not keep_open:
            self._writer_close()

    def make_query(self, *args, **kwargs):
        jvm = PyLucene.getVMEnv()
        jvm.attachCurrentThread()
        return super(PyLuceneDatabase, self).make_query(*args, **kwargs)

    def _create_query_for_query(self, query):
        """generate a query based on an existing query object

        basically this function should just create a copy of the original

        :param query: the original query object
        :type query: PyLucene.Query
        :return: resulting query object
        :rtype: PyLucene.Query
        """
        # TODO: a deep copy or a clone would be safer
        # somehow not working (returns "null"): copy.deepcopy(query)
        return query

    def _escape_term_value(self, value):
        """Escapes special :param:`value` characters."""
        # The indexer seems to strip hyphens, but not the analyzer. If we
        # didn't replace it with space, searching for words with hyphen fails
        value = value.replace("-", " ")
        return PyLucene.QueryParser.escape(value)

    def _create_query_for_string(self, text, require_all=True,
            analyzer=None):
        """generate a query for a plain term of a string query

        basically this function parses the string and returns the resulting
        query

        :param text: The query string
        :type text: str
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :param analyzer: The analyzer to be used
                         Possible analyzers are:
                         -  :attr:`CommonDatabase.ANALYZER_TOKENIZE`
                            the field value is splitted to be matched word-wise
                         -  :attr:`CommonDatabase.ANALYZER_PARTIAL`
                            the field value must start with the query string
                         -  :attr:`CommonDatabase.ANALYZER_EXACT`
                            keep special characters and the like
        :type analyzer: bool
        :return: resulting query object
        :rtype: PyLucene.Query
        """
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer == self.ANALYZER_EXACT:
            analyzer_obj = PyLucene.KeywordAnalyzer()
        else:
            text = self._escape_term_value(text)
            analyzer_obj = PyLucene.StandardAnalyzer()
        qp = PyLucene.QueryParser(UNNAMED_FIELD_NAME, analyzer_obj)
        if (analyzer & self.ANALYZER_PARTIAL > 0):
            # PyLucene uses explicit wildcards for partial matching
            text += "*"
        if require_all:
            qp.setDefaultOperator(qp.Operator.AND)
        else:
            qp.setDefaultOperator(qp.Operator.OR)
        return qp.parse(text)

    def _create_query_for_field(self, field, value, analyzer=None):
        """generate a field query

        this functions creates a field->value query

        :param field: The fieldname to be used
        :type field: str
        :param value: The wanted value of the field
        :type value: str
        :param analyzer: The analyzer to be used
                         Possible analyzers are:
                         - :attr:`CommonDatabase.ANALYZER_TOKENIZE`
                           the field value is splitted to be matched word-wise
                         - :attr:`CommonDatabase.ANALYZER_PARTIAL`
                           the field value must start with the query string
                         - :attr:`CommonDatabase.ANALYZER_EXACT`
                           keep special characters and the like
        :type analyzer: bool
        :return: resulting query object
        :rtype: PyLucene.Query
        """
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer == self.ANALYZER_EXACT:
            analyzer_obj = PyLucene.KeywordAnalyzer()
        else:
            value = self._escape_term_value(value)
            analyzer_obj = PyLucene.StandardAnalyzer()
        qp = PyLucene.QueryParser(field, analyzer_obj)
        if (analyzer & self.ANALYZER_PARTIAL > 0):
            # PyLucene uses explicit wildcards for partial matching
            value += "*"
        return qp.parse(value)

    def _create_query_combined(self, queries, require_all=True):
        """generate a combined query

        :param queries: list of the original queries
        :type queries: list of PyLucene.Query
        :param require_all: boolean operator
            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :return: the resulting combined query object
        :rtype: PyLucene.Query
        """
        combined_query = PyLucene.BooleanQuery()
        for query in queries:
            combined_query.add(
                    PyLucene.BooleanClause(query, _occur(require_all, False)))
        return combined_query

    def _create_empty_document(self):
        """create an empty document to be filled and added to the index later

        :return: the new document object
        :rtype: PyLucene.Document
        """
        return PyLucene.Document()

    def _add_plain_term(self, document, term, tokenize=True):
        """add a term to a document

        :param document: the document to be changed
        :type document: PyLucene.Document
        :param term: a single term to be added
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        if tokenize:
            token_flag = PyLucene.Field.Index.TOKENIZED
        else:
            token_flag = PyLucene.Field.Index.UN_TOKENIZED
        document.add(PyLucene.Field(str(UNNAMED_FIELD_NAME), term,
                PyLucene.Field.Store.YES, token_flag))

    def _add_field_term(self, document, field, term, tokenize=True):
        """add a field term to a document

        :param document: the document to be changed
        :type document: PyLucene.Document
        :param field: name of the field
        :type field: str
        :param term: term to be associated to the field
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        if tokenize:
            token_flag = PyLucene.Field.Index.TOKENIZED
        else:
            token_flag = PyLucene.Field.Index.UN_TOKENIZED
        document.add(PyLucene.Field(str(field), term,
                PyLucene.Field.Store.YES, token_flag))

    def _add_document_to_index(self, document):
        """add a prepared document to the index database

        :param document: the document to be added
        :type document: PyLucene.Document
        """
        self._writer_open()
        self.writer.addDocument(document)

    def begin_transaction(self):
        """PyLucene does not support transactions

        Thus this function just opens the database for write access.
        Call "cancel_transaction" or "commit_transaction" to close write
        access in order to remove the exclusive lock from the database
        directory.
        """
        jvm = PyLucene.getVMEnv()
        jvm.attachCurrentThread()
        self._writer_open()

    def cancel_transaction(self):
        """PyLucene does not support transactions

        Thus this function just closes the database write access and removes
        the exclusive lock.

        See 'start_transaction' for details.
        """
        if self._writer_is_open():
            self.writer.abort()
        self._writer_close()

    def commit_transaction(self):
        """PyLucene does not support transactions

        Thus this function just closes the database write access and removes
        the exclusive lock.

        See 'start_transaction' for details.
        """
        self._writer_close()
        self._index_refresh()

    def get_query_result(self, query):
        """return an object containing the results of a query

        :param query: a pre-compiled query
        :type query: a query object of the real implementation
        :return: an object that allows access to the results
        :rtype: subclass of CommonEnquire
        """
        return PyLuceneHits(self.searcher.search(query))

    def delete_doc(self, ident):
        super(PyLuceneDatabase, self).delete_doc(ident)
        self.reader.flush()
        self._index_refresh()

    def delete_document_by_id(self, docid):
        """delete a specified document

        :param docid: the document ID to be deleted
        :type docid: int
        """
        if self._writer_is_open():
            self._writer_close()
        try:
            self.reader.deleteDocument(docid)
        except PyLucene.JavaError:
            self._index_refresh()
            self.reader.deleteDocument(docid)

    def search(self, query, fieldnames):
        """Return a list of the contents of specified fields for all matches of
        a query.

        :param query: the query to be issued
        :type query: a query object of the real implementation
        :param fieldnames: the name(s) of a field of the document content
        :type fieldnames: string | list of strings
        :return: a list of dicts containing the specified field(s)
        :rtype: list of dicts
        """
        if isinstance(fieldnames, basestring):
            fieldnames = [fieldnames]
        hits = self.searcher.search(query)
        if _COMPILER == 'jcc':
            # add the ranking number and the retrieved document to the array
            hits = [(hit, hits.doc(hit)) for hit in range(hits.length())]
        result = []
        for hit, doc in hits:
            fields = {}
            for fieldname in fieldnames:
                # take care for the special field "None"
                if fieldname is None:
                    pyl_fieldname = UNNAMED_FIELD_NAME
                else:
                    pyl_fieldname = fieldname
                fields[fieldname] = doc.getValues(pyl_fieldname)
            result.append(fields)
        return result

    def _delete_stale_lock(self):
        if self.reader.isLocked(self.location):
            #HACKISH: there is a lock but Lucene api can't tell us how old it
            # is, will have to check the filesystem
            try:
                # in try block just in case lock disappears on us while testing it
                stat = os.stat(os.path.join(self.location, 'write.lock'))
                age = (time.time() - stat.st_mtime) / 60
                if age > 15:
                    logging.warning("stale lock found in %s, removing.", self.location)
                    self.reader.unlock(self.reader.directory())
            except:
                pass

    def _writer_open(self):
        """open write access for the indexing database and acquire an
        exclusive lock
        """
        if not self._writer_is_open():
            self._delete_stale_lock()
            self.writer = PyLucene.IndexWriter(self.location, self.pyl_analyzer,
                    False)
            # "setMaxFieldLength" is available since PyLucene v2
            # we must stay compatible to v1 for the derived class
            # (PyLuceneIndexer1) - thus we make this step optional
            if hasattr(self.writer, "setMaxFieldLength"):
                self.writer.setMaxFieldLength(MAX_FIELD_SIZE)
        # do nothing, if it is already open

    def _writer_close(self):
        """close indexing write access and remove the database lock"""
        if self._writer_is_open():
            self.writer.close()
            self.writer = None

    def _writer_is_open(self):
        """check if the indexing write access is currently open"""
        return hasattr(self, "writer") and not self.writer is None

    def _index_refresh(self):
        """re-read the indexer database"""
        try:
            if self.reader is None or self.searcher is None:
                self.reader = PyLucene.IndexReader.open(self.location)
                self.searcher = PyLucene.IndexSearcher(self.reader)
            elif (self.index_version !=
                      self.reader.getCurrentVersion(self.location)):
                self.searcher.close()
                self.reader.close()
                self.reader = PyLucene.IndexReader.open(self.location)
                self.searcher = PyLucene.IndexSearcher(self.reader)
                self.index_version = self.reader.getCurrentVersion(self.location)
        except PyLucene.JavaError as e:
            # TODO: add some debugging output?
            #self.errorhandler.logerror("Error attempting to read index - try reindexing: "+str(e))
            pass


class PyLuceneHits(CommonIndexer.CommonEnquire):
    """an enquire object contains the information about the result of a request
    """

    def get_matches(self, start, number):
        """return a specified number of qualified matches of a previous query

        :param start: index of the first match to return (starting from zero)
        :type start: int
        :param number: the number of matching entries to return
        :type number: int
        :return: a set of matching entries and some statistics
        :rtype: tuple of (returned number, available number, matches)
                "matches" is a dictionary of::

                    ["rank", "percent", "document", "docid"]
        """
        # check if requested results do not exist
        # stop is the lowest index number to be ommitted
        stop = start + number
        if stop > self.enquire.length():
            stop = self.enquire.length()
        # invalid request range
        if stop <= start:
            return (0, self.enquire.length(), [])
        result = []
        for index in range(start, stop):
            item = {}
            item["rank"] = index
            item["docid"] = self.enquire.id(index)
            item["percent"] = self.enquire.score(index)
            item["document"] = self.enquire.doc(index)
            result.append(item)
        return ((stop - start), self.enquire.length(), result)


def _occur(required, prohibited):
    if required and not prohibited:
        return PyLucene.BooleanClause.Occur.MUST
    elif not required and not prohibited:
        return PyLucene.BooleanClause.Occur.SHOULD
    elif not required and prohibited:
        return PyLucene.BooleanClause.Occur.MUST_NOT
    else:
        # It is an error to specify a clause as both required
        # and prohibited
        return None


def _get_pylucene_version():
    """get the installed pylucene version

    :return: 1 -> PyLucene v1.x / 2 -> PyLucene v2.x / 0 -> unknown
    :rtype: int
    """
    version = PyLucene.VERSION
    if version.startswith("1."):
        return 1
    elif version.startswith("2."):
        return 2
    else:
        return 0

########NEW FILE########
__FILENAME__ = PyLuceneIndexer1
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


"""
interface for the pylucene (v1.x) indexing engine

take a look at PyLuceneIndexer.py for PyLucene v2.x support
"""

# this module is based on PyLuceneIndexer (for PyLucene v2.x)
import PyLuceneIndexer
import PyLucene


def is_available():
    return PyLuceneIndexer._get_pylucene_version() == 1


class PyLuceneDatabase(PyLuceneIndexer.PyLuceneDatabase):
    """manage and use a pylucene indexing database"""

    def _create_query_for_string(self, text, require_all=True,
                analyzer=None):
        """generate a query for a plain term of a string query

        basically this function parses the string and returns the resulting
        query

        :param text: The query string
        :type text: str
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :param analyzer: The analyzer to be used
                         Possible analyzers are:
                         - :attr:`CommonDatabase.ANALYZER_TOKENIZE`
                           the field value is splitted to be matched word-wise
                         - :attr:`CommonDatabase.ANALYZER_PARTIAL`
                           the field value must start with the query string
                         - :attr:`CommonDatabase.ANALYZER_EXACT`
                           keep special characters and the like
        :type analyzer: bool
        :return: resulting query object
        :rtype: PyLucene.Query
        """
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer == self.ANALYZER_EXACT:
            # exact matching - no substitution ...
            # for PyLucene: nothing special is necessary
            pass
        # don't care about special characters ...
        if analyzer == self.ANALYZER_EXACT:
            analyzer_obj = self.ExactAnalyzer()
        else:
            text = _escape_term_value(text)
            analyzer_obj = PyLucene.StandardAnalyzer()
        qp = PyLucene.QueryParser(analyzer=analyzer_obj)
        if require_all:
            qp.setDefaultOperator(qp.Operator.AND)
        else:
            qp.setDefaultOperator(qp.Operator.OR)
        if (analyzer & self.ANALYZER_PARTIAL) > 0:
            # PyLucene uses explicit wildcards for partial matching
            text += "*"
        return qp.parse(text)

    def _create_query_for_field(self, field, value, analyzer=None):
        """Generate a field query.

        This functions creates a field->value query.

        :param field: The fieldname to be used
        :type field: str
        :param value: The wanted value of the field
        :type value: str
        :param analyzer: The analyzer to be used
                         Possible analyzers are:
                         - :attr:`CommonDatabase.ANALYZER_TOKENIZE`
                           the field value is splitted to be matched word-wise
                         - :attr:`CommonDatabase.ANALYZER_PARTIAL`
                           the field value must start with the query string
                         - :attr:`CommonDatabase.ANALYZER_EXACT`
                           keep special characters and the like
        :type analyzer: bool
        :return: Resulting query object
        :rtype: PyLucene.Query
        """
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer == self.ANALYZER_EXACT:
            analyzer_obj = self.ExactAnalyzer()
        else:
            value = _escape_term_value(value)
            analyzer_obj = PyLucene.StandardAnalyzer()
        if (analyzer & self.ANALYZER_PARTIAL) > 0:
            # PyLucene uses explicit wildcards for partial matching
            value += "*"
        return PyLucene.QueryParser.parse(value, field, analyzer_obj)

    def _create_query_combined(self, queries, require_all=True):
        """generate a combined query

        :param queries: list of the original queries
        :type queries: list of xapian.Query
        :param require_all: boolean operator
            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :return: the resulting combined query object
        :rtype: PyLucene.Query
        """
        combined_query = PyLucene.BooleanQuery()
        for query in queries:
            combined_query.add(
                    PyLucene.BooleanClause(query, require_all, False))
        return combined_query

    def _add_plain_term(self, document, term, tokenize=True):
        """add a term to a document

        :param document: the document to be changed
        :type document: xapian.Document | PyLucene.Document
        :param term: a single term to be added
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        # Field parameters: name, string, store, index, token
        document.add(PyLucene.Field(str(PyLuceneIndex.UNNAMED_FIELD_NAME), term,
                True, True, tokenize))

    def _add_field_term(self, document, field, term, tokenize=True):
        """add a field term to a document

        :param document: the document to be changed
        :type document: xapian.Document | PyLucene.Document
        :param field: name of the field
        :type field: str
        :param term: term to be associated to the field
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        # TODO: decoding (utf-8) is missing
        # Field parameters: name, string, store, index, token
        document.add(PyLucene.Field(str(field), term,
                True, True, tokenize))

    def get_query_result(self, query):
        """return an object containing the results of a query

        :param query: a pre-compiled query
        :type query: a query object of the real implementation
        :return: an object that allows access to the results
        :rtype: subclass of CommonEnquire
        """
        return PyLucene.indexSearcher.search(query)

    def search(self, query, fieldnames):
        """return a list of the contents of specified fields for all matches of
        a query

        :param query: the query to be issued
        :type query: a query object of the real implementation
        :param fieldnames: the name(s) of a field of the document content
        :type fieldnames: string | list of strings
        :return: a list of dicts containing the specified field(s)
        :rtype: list of dicts
        """
        if isinstance(fieldnames, basestring):
            fieldnames = [fieldnames]
        hits = PyLucene.indexSearcher.search(query)
        result = []
        for hit, doc in hits:
            fields = {}
            for fieldname in fieldnames:
                content = doc.get(fieldname)
                if not content is None:
                    fields[fieldname] = content
            result.append(fields)
        return result

    def _writer_open(self):
        """open write access for the indexing database and acquire an
        exclusive lock
        """
        super(PyLuceneIndexer1, self)._writer_open_()
        self.writer.maxFieldLength = PyLuceneIndexer.MAX_FIELD_SIZE

########NEW FILE########
__FILENAME__ = test_indexers
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


import os
import shutil
import sys

import pytest

import __init__ as indexing
import CommonIndexer


# following block only needs running under pytest; unclear how to detect it?

# check whether any indexer is present at all
noindexer = True
for indexer in ["lucene", "PyLucene", "xapian"]:
    try:
        __import__(indexer)
    except ImportError:
        continue
    noindexer = False
    break
# mark entire module as skipped for pytest if no indexer available
pytestmark = pytest.mark.skipif("noindexer")

# FIXME (bug 2819) need to rename most test_* functions, add new "parametrized"
# test_indexer function to normalize operation whether run directly as script
# or via pytest

DATABASE = "tmp-index"

# overwrite this value to change the preferred indexing engine
default_engine = ""

# order of tests to be done
ORDER_OF_TESTS = ["XapianIndexer", "PyLuceneIndexer", "PyLuceneIndexer1"]


def _get_indexer(location):
    """wrapper around "indexer.get_indexer" to enable a globally preferred
    indexing engine selection

    create an indexer based on the preference order 'default_engine'

    :param location: the path of the database to be created/opened
    :type location: str
    :return: the resulting indexing engine instance
    :rtype: CommonIndexer.CommonDatabase
    """
    return indexing.get_indexer(location, [default_engine])


def clean_database():
    """remove an existing database"""
    dbase_dir = os.path.abspath(DATABASE)
    # the database directory does not exist
    if not os.path.exists(dbase_dir):
        return
    # recursively remove the directory
    shutil.rmtree(dbase_dir)


def create_example_content(database):
    """add some defined documents to the database

    this may be used to check some specific queries

    :param database: a indexing database object
    :type database: CommonIndexer.CommonDatabase
    """
    # a reasonable foo-bar entry
    database.index_document(["foo", "bar", "med"])
    # and something more for another document with a unicode string
    database.index_document(["foo", "bar", u"HELO"])
    # another similar one - but with "barr" instead of "bar"
    database.index_document(["foo", "barr", "med", "HELO"])
    # some field indexed document data
    database.index_document({"fname1": "foo_field1", "fname2": "foo_field2"})
    database.index_document({"fname1": "bar_field1", "fname2": "foo_field2",
            None: ["HELO", "foo"]})
    database.index_document({None: "med"})
    # for tokenizing tests
    database.set_field_analyzers({
            "fname1": database.ANALYZER_PARTIAL | database.ANALYZER_TOKENIZE,
            "fname2": database.ANALYZER_EXACT})
    database.index_document({"fname1": "qaz wsx", None: "edc rfv"})
    database.index_document({"fname2": "qaz wsx", None: "edc rfv"})
    # check a filename with the exact analyzer
    database.index_document({"fname2": "foo-bar.po"})
    # add a list of terms for a keyword
    database.index_document({"multiple": ["foo", "bar"]})
    database.flush()
    assert _get_number_of_docs(database) == 10


def test_create_database():
    """create a new database from scratch"""
    # clean up everything first
    clean_database()
    new_db = _get_indexer(DATABASE)
    assert isinstance(new_db, CommonIndexer.CommonDatabase)
    assert os.path.exists(DATABASE)
    # clean up
    clean_database()


def test_open_database():
    """open an existing database"""
    # clean up everything first
    clean_database()
    # create a new database - it will be closed immediately afterwards
    # since the reference is lost again
    _get_indexer(DATABASE)
    # open the existing database again
    opened_db = _get_indexer(DATABASE)
    assert isinstance(opened_db, CommonIndexer.CommonDatabase)
    # clean up
    clean_database()


def test_make_queries():
    """create a simple query from a plain string"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # plaintext queries
    q_plain1 = new_db.make_query("foo")
    q_plain2 = new_db.make_query("foo bar")
    assert str(q_plain1) != str(q_plain2)
    # list 'and/or'
    q_combined_and = new_db.make_query([new_db.make_query("foo"),
        new_db.make_query("bar")])
    q_combined_or = new_db.make_query([new_db.make_query("foo"),
        new_db.make_query("bar")], require_all=False)
    assert str(q_combined_or) != str(q_combined_and)


def test_partial_text_matching():
    """check if implicit and explicit partial text matching works"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # this query should return three matches (disabled partial matching)
    q_plain_partial1 = new_db.make_query("bar",
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_plain_partial1 = new_db.get_query_result(q_plain_partial1).get_matches(0, 10)
    assert r_plain_partial1[0] == 2
    # this query should return three matches (wildcard works)
    q_plain_partial2 = new_db.make_query("bar", analyzer=new_db.ANALYZER_PARTIAL)
    r_plain_partial2 = new_db.get_query_result(q_plain_partial2).get_matches(0, 10)
    assert r_plain_partial2[0] == 3
    # return two matches (the wildcard is ignored without PARTIAL)
    q_plain_partial3 = new_db.make_query("bar*",
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_plain_partial3 = new_db.get_query_result(q_plain_partial3).get_matches(0, 10)
    assert r_plain_partial3[0] == 2
    # partial matching at the start of the string
    # TODO: enable this as soon, as partial matching works at the beginning of text
    #q_plain_partial4 = new_db.make_query("*ar",
    #        analyzer=new_db.ANALYZER_EXACT)
    #        analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    #r_plain_partial4 = new_db.get_query_result(q_plain_partial4).get_matches(0,10)
    #assert r_plain_partial4[0] == 2
    # clean up
    clean_database()


def test_field_matching():
    """test if field specific searching works"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # do a field search with a tuple argument
    q_field1 = new_db.make_query(("fname1", "foo_field1"))
    r_field1 = new_db.get_query_result(q_field1).get_matches(0, 10)
    assert r_field1[0] == 1
    # do a field search with a dict argument
    q_field2 = new_db.make_query({"fname1": "bar_field1"})
    r_field2 = new_db.get_query_result(q_field2).get_matches(0, 10)
    assert r_field2[0] == 1
    # do an incomplete field search with a dict argument - should fail
    q_field3 = new_db.make_query({"fname2": "foo_field"})
    r_field3 = new_db.get_query_result(q_field3).get_matches(0, 10)
    assert r_field3[0] == 0
    # do an AND field search with a dict argument
    q_field4 = new_db.make_query({"fname1": "foo_field1", "fname2": "foo_field2"}, require_all=True)
    r_field4 = new_db.get_query_result(q_field4).get_matches(0, 10)
    assert r_field4[0] == 1
    # do an OR field search with a dict argument
    q_field5 = new_db.make_query({"fname1": "foo_field1", "fname2": "foo_field2"}, require_all=False)
    r_field5 = new_db.get_query_result(q_field5).get_matches(0, 10)
    assert r_field5[0] == 2
    # do an incomplete field search with a partial field analyzer
    q_field6 = new_db.make_query({"fname1": "foo_field"}, analyzer=new_db.ANALYZER_PARTIAL)
    r_field6 = new_db.get_query_result(q_field6).get_matches(0, 10)
    assert r_field6[0] == 1
    # clean up
    clean_database()


def test_field_analyzers():
    """test if we can change the analyzer of specific fields"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # do an incomplete field search with partial analyzer (configured for this field)
    q_field1 = new_db.make_query({"fname1": "bar_field"})
    r_field1 = new_db.get_query_result(q_field1).get_matches(0, 10)
    assert r_field1[0] == 1
    # check the get/set field analyzer functions
    old_analyzer = new_db.get_field_analyzers("fname1")
    new_db.set_field_analyzers({"fname1": new_db.ANALYZER_EXACT})
    assert new_db.get_field_analyzers("fname1") == new_db.ANALYZER_EXACT
    new_db.set_field_analyzers({"fname1": new_db.ANALYZER_PARTIAL})
    assert new_db.get_field_analyzers("fname1") == new_db.ANALYZER_PARTIAL
    # restore previous setting
    new_db.set_field_analyzers({"fname1": old_analyzer})
    # check if ANALYZER_TOKENIZE is the default
    assert (new_db.get_field_analyzers("thisFieldDoesNotExist") & new_db.ANALYZER_TOKENIZE) > 0
    # do an incomplete field search - now we use the partial analyzer
    q_field2 = new_db.make_query({"fname1": "bar_field"}, analyzer=new_db.ANALYZER_PARTIAL)
    r_field2 = new_db.get_query_result(q_field2).get_matches(0, 10)
    assert r_field2[0] == 1
    # clean up
    clean_database()


def test_and_queries():
    """test if AND queries work as expected"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # do an AND query (partial matching disabled)
    q_and1 = new_db.make_query("foo bar",
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_and1 = new_db.get_query_result(q_and1).get_matches(0, 10)
    assert r_and1[0] == 2
    # do the same AND query in a different way
    q_and2 = new_db.make_query(["foo", "bar"],
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_and2 = new_db.get_query_result(q_and2).get_matches(0, 10)
    assert r_and2[0] == 2
    # do an AND query without results
    q_and3 = new_db.make_query(["HELO", "bar", "med"],
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_and3 = new_db.get_query_result(q_and3).get_matches(0, 10)
    assert r_and3[0] == 0
    # clean up
    clean_database()


def test_or_queries():
    """test if OR queries work as expected"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # do an OR query
    q_or1 = new_db.make_query("foo bar", require_all=False)
    r_or1 = new_db.get_query_result(q_or1).get_matches(0, 10)
    assert r_or1[0] == 4
    # do the same or query in a different way
    q_or2 = new_db.make_query(["foo", "bar"], require_all=False)
    r_or2 = new_db.get_query_result(q_or2).get_matches(0, 10)
    assert r_or2[0] == r_or1[0]
    # do an OR query with lots of results
    q_or3 = new_db.make_query(["HELO", "bar", "med"], require_all=False)
    r_or3 = new_db.get_query_result(q_or3).get_matches(0, 10)
    assert r_or3[0] == 5
    # clean up
    clean_database()


def test_string_queries():
    """test if string queries work as expected"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # do string query
    q_string1 = new_db.make_query("foo bar")
    r_string1 = new_db.get_query_result(q_string1).get_matches(0, 10)
    assert r_string1[0] == 3
    # do string query with non contagious words
    q_string2 = new_db.make_query("foo HELO")
    r_string2 = new_db.get_query_result(q_string2).get_matches(0, 10)
    assert r_string2[0] == 3
    # do string query with a named field
    q_string3 = new_db.make_query({"multiple": "foo bar"})
    r_string3 = new_db.get_query_result(q_string3).get_matches(0, 10)
    assert r_string3[0] == 1
    # clean up
    clean_database()


def test_lower_upper_case():
    """test if case is ignored for queries and for indexed terms"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # use upper case search terms for lower case indexed terms
    q_case1 = new_db.make_query("BAR",
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_case1 = new_db.get_query_result(q_case1).get_matches(0, 10)
    assert r_case1[0] == 2
    # use lower case search terms for upper case indexed terms
    q_case2 = new_db.make_query("helo")
    r_case2 = new_db.get_query_result(q_case2).get_matches(0, 10)
    assert r_case2[0] == 3
    # use lower case search terms for lower case indexed terms
    q_case3 = new_db.make_query("bar",
            analyzer=(new_db.analyzer ^ new_db.ANALYZER_PARTIAL))
    r_case3 = new_db.get_query_result(q_case3).get_matches(0, 10)
    assert r_case3[0] == 2
    # use upper case search terms for upper case indexed terms
    q_case4 = new_db.make_query("HELO")
    r_case4 = new_db.get_query_result(q_case4).get_matches(0, 10)
    assert r_case4[0] == 3
    # clean up
    clean_database()


def test_tokenizing():
    """test if the TOKENIZE analyzer field setting is honoured"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # check if the plain term was tokenized
    q_token1 = new_db.make_query("rfv")
    r_token1 = new_db.get_query_result(q_token1).get_matches(0, 10)
    assert r_token1[0] == 2
    # check if the field term was tokenized
    q_token2 = new_db.make_query({"fname1": "wsx"})
    r_token2 = new_db.get_query_result(q_token2).get_matches(0, 10)
    assert r_token2[0] == 1
    # check that the other field term was not tokenized
    q_token3 = new_db.make_query({"fname2": "wsx"})
    r_token3 = new_db.get_query_result(q_token3).get_matches(0, 10)
    assert r_token3[0] == 0
    # check that the other field term was not tokenized
    q_token4 = new_db.make_query({"fname2": "foo-bar.po"})
    #q_token4 = new_db.make_query("poo-foo.po")
    r_token4 = new_db.get_query_result(q_token4).get_matches(0, 10)
    # problem can be fixed by adding "TOKENIZE" to the field before populating the database -> this essentially splits the document term into pieces
    assert r_token4[0] == 1
    # clean up
    clean_database()


def test_searching():
    """test if searching (retrieving specified field values) works"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    q_search1 = new_db.make_query({"fname1": "bar_field1"})
    r_search1 = new_db.search(q_search1, ["fname2", None])
    assert len(r_search1) == 1
    dict_search1 = r_search1[0]
    assert "fname2" in dict_search1 and \
            (dict_search1["fname2"] == ["foo_field2"])
    # a stupid way for checking, if the second field list is also correct
    # (without caring for the order of the list)
    assert None in dict_search1
    # TODO: for now PyLucene cares for case, while xapian doesn't - FIXME
    list_search1_sorted = [item.lower() for item in dict_search1[None]]
    list_search1_sorted.sort()
    assert list_search1_sorted == ["foo", "helo"]
    # clean up
    clean_database()


def test_multiple_terms():
    """test if multiple terms can be added to a keyword"""
    # clean up everything first
    clean_database()
    # initialize the database with example content
    new_db = _get_indexer(DATABASE)
    create_example_content(new_db)
    # check for the first item ("foo")
    q_multiple1 = new_db.make_query({"multiple": "f"},
            analyzer=new_db.ANALYZER_PARTIAL)
    r_multiple1 = new_db.get_query_result(q_multiple1).get_matches(0, 10)
    assert r_multiple1[0] == 1
    # check for the second item ("bar")
    q_multiple2 = new_db.make_query({"multiple": "bar"})
    r_multiple2 = new_db.get_query_result(q_multiple2).get_matches(0, 10)
    assert r_multiple2[0] == 1
    # clean up
    clean_database()


def show_database(database):
    """print the complete database - for debugging purposes"""
    if database.INDEX_DIRECTORY_NAME == "xapian":
        _show_database_xapian(database)
    else:
        _show_database_pylucene(database)


def _show_database_pylucene(database):
    database.flush()
    reader = database.reader
    for index in range(reader.maxDoc()):
        print(reader.document(index).toString().encode("charmap"))


def _show_database_xapian(database):
    import xapian
    doccount = database.reader.get_doccount()
    max_doc_index = database.reader.get_lastdocid()
    print("Database overview: %d items up to index %d" % (doccount, max_doc_index))
    for index in range(1, max_doc_index + 1):
        try:
            document = database.reader.get_document(index)
        except xapian.DocNotFoundError:
            continue
        # print the document's terms and their positions
        print("\tDocument [%d]: %s" % (index,
                str([(one_term.term, [posi for posi in one_term.positer])
                for one_term in document.termlist()])))


def _get_number_of_docs(database):
    if database.INDEX_DIRECTORY_NAME == "xapian":
        # xapian
        return database.reader.get_doccount()
    else:
        # pylucene
        database._writer_close()
        database._index_refresh()
        return database.reader.numDocs()


def get_engine_name(database):
    return database.__module__


def report_whitelisted_success(db, name):
    """ Output a warning message regarding a successful unittest, that was
    supposed to fail for a specific indexing engine.
    As this test works now for the engine, the whitelisting should be removed.
    """
    print("the test '%s' works again for '%s' - please remove the exception"
        % (name, get_engine_name(db)))


def report_whitelisted_failure(db, name):
    """ Output a warning message regarding a unittest, that was supposed to fail
    for a specific indexing engine.
    Since the test behaves as expected (it fails), this is just for reminding
    developers on these open issues of the indexing engine support.
    """
    print("the test '%s' fails - as expected for '%s'" % (name,
            get_engine_name(db)))


def assert_whitelisted(db, assert_value, white_list_engines, name_of_check):
    """ Do an assertion, but ignoring failure for specific indexing engines.
    This can be used for almost-complete implementations, that just need
    a little bit of improvement for full compliance.
    """
    try:
        assert assert_value
        if get_engine_name(db) in white_list_engines:
            report_whitelisted_success(db, name_of_check)
    except AssertionError:
        if get_engine_name(db) in white_list_engines:
            report_whitelisted_failure(db, name_of_check)
        else:
            raise


if __name__ == "__main__":
    # if an argument is given: use it as a database directory and show it
    if len(sys.argv) > 1:
        db = _get_indexer(sys.argv[1])
        show_database(db)
        sys.exit(0)
    for engine in ORDER_OF_TESTS:
        default_engine = engine
        # cleanup the database after interrupted tests
        clean_database()
        engine_name = get_engine_name(_get_indexer(DATABASE))
        if engine_name == default_engine:
            print("****** running tests for '%s' ******" % engine_name)
        else:
            print("****** SKIPPING tests for '%s' ******" % default_engine)
            continue
        test_create_database()
        test_open_database()
        test_make_queries()
        test_partial_text_matching()
        test_field_matching()
        test_field_analyzers()
        test_and_queries()
        test_or_queries()
        test_string_queries()
        test_lower_upper_case()
        test_tokenizing()
        test_searching()
        test_multiple_terms()
        # TODO: add test for document deletion
        # TODO: add test for transaction handling
        # TODO: add test for multiple engine/database handling in "get_indexer"
    clean_database()

########NEW FILE########
__FILENAME__ = XapianIndexer
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""
Interface to the Xapian indexing engine for the Translate Toolkit

Xapian v1.0 or higher is supported.

If you are interested in writing an interface for Xapian 0.x, then
you should checkout the following::

    svn export -r 7235 https://translate.svn.sourceforge.net/svnroot/translate/src/branches/translate-search-indexer-generic-merging/translate/search/indexer/

It is not completely working, but it should give you a good start.
"""

# xapian module versions before 1.0.13 hangs apache under mod_python
import sys
import re

# detect if running under apache
if 'apache' in sys.modules or '_apache' in sys.modules or 'mod_wsgi' in sys.modules:

    def _str2version(version):
        return [int(i) for i in version.split('.')]

    import subprocess
    # even checking xapian version leads to deadlock under apache, must figure version from command line
    try:
        command = subprocess.Popen(['xapian-check', '--version'], stdout=subprocess.PIPE)
        stdout, stderr = command.communicate()
        if _str2version(re.match('.*([0-9]+\.[0-9]+\.[0-9]+).*', stdout).groups()[0]) < [1, 0, 13]:
            raise ImportError("Running under apache, can't load xapain")
    except:
        #FIXME: report is xapian-check command is missing?
        raise ImportError("Running under apache, can't load xapian")

import CommonIndexer
import xapian
import os
import time
import logging


def is_available():
    return xapian.major_version() > 0


# in xapian there is a length restriction for term strings
# see http://osdir.com/ml/search.xapian.general/2006-11/msg00210.html
# a maximum length of around 240 is described there - but we need less anyway
_MAX_TERM_LENGTH = 128


class XapianDatabase(CommonIndexer.CommonDatabase):
    """Interface to the `Xapian indexer <http://xapian.org>`_."""

    QUERY_TYPE = xapian.Query
    INDEX_DIRECTORY_NAME = "xapian"

    def __init__(self, basedir, analyzer=None, create_allowed=True):
        """Initialize or open a Xapian database.

        :raise ValueError: the given location exists, but the database type
                is incompatible (e.g. created by a different indexing engine)
        :raise OSError: the database failed to initialize

        :param basedir: the parent directory of the database
        :type basedir: str
        :param analyzer: Bitwise combination of possible analyzer flags
                         to be used as the default analyzer for this
                         database. Leave it empty to use the system default
                         analyzer (self.ANALYZER_DEFAULT).

                         See self.ANALYZER_TOKENIZE, self.ANALYZER_PARTIAL, ...
        :type analyzer: int
        :param create_allowed: create the database, if necessary; default: True
        :type create_allowed: bool
        """
        # call the __init__ function of our parent
        super(XapianDatabase, self).__init__(basedir, analyzer=analyzer,
                create_allowed=create_allowed)
        self.reader = None
        self.writer = None
        if os.path.exists(self.location):
            # try to open an existing database
            try:
                self.reader = xapian.Database(self.location)
            except xapian.DatabaseOpeningError as err_msg:
                raise ValueError("Indexer: failed to open xapian database "
                                 "(%s) - maybe it is not a xapian database: %s" % (
                                 self.location, str(err_msg)))
        else:
            # create a new database
            if not create_allowed:
                raise OSError("Indexer: skipping database creation")
            try:
                # create the parent directory if it does not exist
                parent_path = os.path.dirname(self.location)
                if not os.path.isdir(parent_path):
                    # recursively create all directories up to parent_path
                    os.makedirs(parent_path)
            except IOError as err_msg:
                raise OSError("Indexer: failed to create the parent "
                              "directory (%s) of the indexing database: %s" % (
                              parent_path, str(err_msg)))
            try:
                self.writer = xapian.WritableDatabase(self.location,
                        xapian.DB_CREATE_OR_OPEN)
                self.flush()
            except xapian.DatabaseOpeningError as err_msg:
                raise OSError("Indexer: failed to open or create a xapian "
                              "database (%s): %s" % (self.location, str(err_msg)))

    def __del__(self):
        self.reader = None
        self._writer_close()

    def flush(self, optimize=False):
        """force to write the current changes to disk immediately

        :param optimize: ignored for xapian
        :type optimize: bool
        """
        # write changes to disk (only if database is read-write)
        if self._writer_is_open():
            self._writer_close()
        self._index_refresh()

    def make_query(self, *args, **kwargs):
        try:
            return super(XapianDatabase, self).make_query(*args, **kwargs)
        except xapian.DatabaseModifiedError:
            self._index_refresh()
            return super(XapianDatabase, self).make_query(*args, **kwargs)

    def _create_query_for_query(self, query):
        """generate a query based on an existing query object

        basically this function should just create a copy of the original

        :param query: the original query object
        :type query: xapian.Query
        :return: the resulting query object
        :rtype: xapian.Query
        """
        # create a copy of the original query
        return xapian.Query(query)

    def _create_query_for_string(self, text, require_all=True,
            analyzer=None):
        """generate a query for a plain term of a string query

        basically this function parses the string and returns the resulting
        query

        :param text: the query string
        :type text: str
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :param analyzer: Define query options (partial matching, exact matching,
                         tokenizing, ...) as bitwise combinations of
                         *CommonIndexer.ANALYZER_???*.

                         This can override previously defined field
                         analyzer settings.

                         If analyzer is None (default), then the configured
                         analyzer for the field is used.
        :type analyzer: int
        :return: resulting query object
        :rtype: xapian.Query
        """
        qp = xapian.QueryParser()
        qp.set_database(self.reader)
        if require_all:
            qp.set_default_op(xapian.Query.OP_AND)
        else:
            qp.set_default_op(xapian.Query.OP_OR)
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer & self.ANALYZER_PARTIAL > 0:
            match_flags = xapian.QueryParser.FLAG_PARTIAL
            return qp.parse_query(text, match_flags)
        elif analyzer == self.ANALYZER_EXACT:
            # exact matching -
            return xapian.Query(text)
        else:
            # everything else (not partial and not exact)
            match_flags = 0
            return qp.parse_query(text, match_flags)

    def _create_query_for_field(self, field, value, analyzer=None):
        """generate a field query

        this functions creates a field->value query

        :param field: the fieldname to be used
        :type field: str
        :param value: the wanted value of the field
        :type value: str
        :param analyzer: Define query options (partial matching, exact
                         matching, tokenizing, ...) as bitwise combinations of
                         *CommonIndexer.ANALYZER_???*.

                         This can override previously defined field
                         analyzer settings.

                         If analyzer is None (default), then the configured
                         analyzer for the field is used.
        :type analyzer: int
        :return: the resulting query object
        :rtype: xapian.Query
        """
        if analyzer is None:
            analyzer = self.analyzer
        if analyzer == self.ANALYZER_EXACT:
            # exact matching -> keep special characters
            return xapian.Query("%s%s" % (field.upper(), value))
        # other queries need a parser object
        qp = xapian.QueryParser()
        qp.set_database(self.reader)
        if (analyzer & self.ANALYZER_PARTIAL > 0):
            # partial matching
            match_flags = xapian.QueryParser.FLAG_PARTIAL
            return qp.parse_query(value, match_flags, field.upper())
        else:
            # everything else (not partial and not exact)
            match_flags = 0
            return qp.parse_query(value, match_flags, field.upper())

    def _create_query_combined(self, queries, require_all=True):
        """generate a combined query

        :param queries: list of the original queries
        :type queries: list of xapian.Query
        :param require_all: boolean operator
                            (True -> AND (default) / False -> OR)
        :type require_all: bool
        :return: the resulting combined query object
        :rtype: xapian.Query
        """
        if require_all:
            query_op = xapian.Query.OP_AND
        else:
            query_op = xapian.Query.OP_OR
        return xapian.Query(query_op, queries)

    def _create_empty_document(self):
        """create an empty document to be filled and added to the index later

        :return: the new document object
        :rtype: xapian.Document
        """
        return xapian.Document()

    def _add_plain_term(self, document, term, tokenize=True):
        """add a term to a document

        :param document: the document to be changed
        :type document: xapian.Document
        :param term: a single term to be added
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        if tokenize:
            term_gen = xapian.TermGenerator()
            term_gen.set_document(document)
            term_gen.index_text(term)
        else:
            document.add_term(_truncate_term_length(term))

    def _add_field_term(self, document, field, term, tokenize=True):
        """add a field term to a document

        :param document: the document to be changed
        :type document: xapian.Document
        :param field: name of the field
        :type field: str
        :param term: term to be associated to the field
        :type term: str
        :param tokenize: should the term be tokenized automatically
        :type tokenize: bool
        """
        if tokenize:
            term_gen = xapian.TermGenerator()
            term_gen.set_document(document)
            term_gen.index_text(term, 1, field.upper())
        else:
            document.add_term(_truncate_term_length("%s%s" % (field.upper(), term)))

    def _add_document_to_index(self, document):
        """add a prepared document to the index database

        :param document: the document to be added
        :type document: xapian.Document
        """
        # open the database for writing
        self._writer_open()
        self.writer.add_document(document)

    def begin_transaction(self):
        """Begin a transaction.

        Xapian supports transactions to group multiple database modifications.
        This avoids intermediate flushing and therefore increases performance.
        """
        self._writer_open()
        self.writer.begin_transaction()

    def cancel_transaction(self):
        """cancel an ongoing transaction

        no changes since the last execution of 'begin_transcation' are written
        """
        self.writer.cancel_transaction()
        self._writer_close()

    def commit_transaction(self):
        """Submit the changes of an ongoing transaction.

        All changes since the last execution of 'begin_transaction'
        are written.
        """
        self.writer.commit_transaction()
        self._writer_close()

    def get_query_result(self, query):
        """Return an object containing the results of a query.

        :param query: a pre-compiled xapian query
        :type query: xapian.Query
        :return: an object that allows access to the results
        :rtype: XapianIndexer.CommonEnquire
        """
        enquire = xapian.Enquire(self.reader)
        enquire.set_query(query)
        return XapianEnquire(enquire)

    def delete_document_by_id(self, docid):
        """Delete a specified document.

        :param docid: the document ID to be deleted
        :type docid: int
        """
        # open the database for writing
        self._writer_open()
        try:
            self.writer.delete_document(docid)
            return True
        except xapian.DocNotFoundError:
            return False

    def search(self, query, fieldnames):
        """Return a list of the contents of specified fields for all matches
        of a query.

        :param query: the query to be issued
        :type query: xapian.Query
        :param fieldnames: the name(s) of a field of the document content
        :type fieldnames: string | list of strings
        :return: a list of dicts containing the specified field(s)
        :rtype: list of dicts
        """
        result = []
        if isinstance(fieldnames, basestring):
            fieldnames = [fieldnames]
        try:
            self._walk_matches(query, _extract_fieldvalues,
                               (result, fieldnames))
        except xapian.DatabaseModifiedError:
            self._index_refresh()
            self._walk_matches(query, _extract_fieldvalues,
                               (result, fieldnames))
        return result

    def _delete_stale_lock(self):
        if not self._writer_is_open():
            lockfile = os.path.join(self.location, 'flintlock')
            if (os.path.exists(lockfile) and
                (time.time() - os.path.getmtime(lockfile)) / 60 > 15):
                logging.warning("Stale lock found in %s, removing.",
                                self.location)
                os.remove(lockfile)

    def _writer_open(self):
        """Open write access for the indexing database and acquire an
        exclusive lock.
        """
        if not self._writer_is_open():
            self._delete_stale_lock()
            try:
                self.writer = xapian.WritableDatabase(self.location, xapian.DB_OPEN)
            except xapian.DatabaseOpeningError as err_msg:

                raise ValueError("Indexer: failed to open xapian database "
                                 "(%s) - maybe it is not a xapian database: %s" % (
                                 self.location, str(err_msg)))

    def _writer_close(self):
        """close indexing write access and remove database lock"""
        if self._writer_is_open():
            self.writer.flush()
            self.writer = None

    def _writer_is_open(self):
        """check if the indexing write access is currently open"""
        return hasattr(self, "writer") and not self.writer is None

    def _index_refresh(self):
        """re-read the indexer database"""
        try:
            if self.reader is None:
                self.reader = xapian.Database(self.location)
            else:
                self.reader.reopen()
        except xapian.DatabaseOpeningError as err_msg:
            raise ValueError("Indexer: failed to open xapian database "
                             "(%s) - maybe it is not a xapian database: %s" % (
                             self.location, str(err_msg)))


class XapianEnquire(CommonIndexer.CommonEnquire):
    """interface to the xapian object for storing sets of matches
    """

    def get_matches(self, start, number):
        """Return a specified number of qualified matches of a previous query.

        :param start: index of the first match to return (starting from zero)
        :type start: int
        :param number: the number of matching entries to return
        :type number: int
        :return: a set of matching entries and some statistics
        :rtype: tuple of (returned number, available number, matches)
                "matches" is a dictionary of::

                    ["rank", "percent", "document", "docid"]
        """
        matches = self.enquire.get_mset(start, number)
        result = []
        for match in matches:
            elem = {}
            elem["rank"] = match.rank
            elem["docid"] = match.docid
            elem["percent"] = match.percent
            elem["document"] = match.document
            result.append(elem)
        return (matches.size(), matches.get_matches_estimated(), result)


def _truncate_term_length(term, taken=0):
    """truncate the length of a term string length to the maximum allowed
    for xapian terms

    :param term: the value of the term, that should be truncated
    :type term: str
    :param taken: since a term consists of the name of the term and its
        actual value, this additional parameter can be used to reduce the
        maximum count of possible characters
    :type taken: int
    :return: the truncated string
    :rtype: str
    """
    if len(term) > _MAX_TERM_LENGTH - taken:
        return term[0:_MAX_TERM_LENGTH - taken - 1]
    else:
        return term


def _extract_fieldvalues(match, (result, fieldnames)):
    """Add a dict of field values to a list.

    Usually this function should be used together with :func:`_walk_matches`
    for traversing a list of matches.

    :param match: a single match object
    :type match: xapian.MSet
    :param result: the resulting dict will be added to this list
    :type result: list of dict
    :param fieldnames: the names of the fields to be added to the dict
    :type fieldnames: list of str
    """
    # prepare empty dict
    item_fields = {}
    # fill the dict
    for term in match["document"].termlist():
        for fname in fieldnames:
            if ((fname is None) and re.match("[^A-Z]", term.term)):
                value = term.term
            elif re.match("%s[^A-Z]" % str(fname).upper(), term.term):
                value = term.term[len(fname):]
            else:
                continue
            # we found a matching field/term
            if fname in item_fields:
                item_fields[fname].append(value)
            else:
                item_fields[fname] = [value]
    result.append(item_fields)

########NEW FILE########
__FILENAME__ = lshtein
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A class to calculate a similarity based on the Levenshtein
distance.

See http://en.wikipedia.org/wiki/Levenshtein_distance.

If available, the `python-Levenshtein
<https://pypi.python.org/pypi/python-Levenshtein>`_ will be used which will
provide better performance as it is implemented natively.
"""

import math


def python_distance(a, b, stopvalue=-1):
    """Calculates the distance for use in similarity calculation. Python
    version."""
    l1 = len(a)
    l2 = len(b)
    if stopvalue == -1:
        stopvalue = l2
    current = range(l1 + 1)
    for i in range(1, l2 + 1):
        previous, current = current, ([i] + [0] * l1)
        least = l2
        for j in range(1, l1 + 1):
            change = previous[j-1]
            if a[j-1] != b[i-1]:
                change = change + 1
            insert = previous[j] + 1
            delete = current[j-1] + 1
            current[j] = min(insert, delete, change)
            if least > current[j]:
                least = current[j]
        #The smallest value in the current array is the best (lowest) value
        #that can be attained in the end if the strings are identical further
        if least > stopvalue:
            return least

    return current[l1]


def native_distance(a, b, stopvalue=0):
    """Same as python_distance in functionality. This uses the fast C
    version if we detected it earlier.

    Note that this does not support arbitrary sequence types, but only
    string types."""
    return Levenshtein.distance(a, b)

try:
    import Levenshtein as Levenshtein
    distance = native_distance
except ImportError:
    import logging
    logging.warning("Python-Levenshtein not found. Continuing with built-in (slower) fuzzy matching.")
    distance = python_distance


class LevenshteinComparer:

    def __init__(self, max_len=200):
        self.MAX_LEN = max_len

    def similarity(self, a, b, stoppercentage=40):
        similarity = self.similarity_real(a, b, stoppercentage)
        measurements = 1

#        chr_a = segment.characters(a)
#        chr_b = segment.characters(b)
#        if chr_a and chr_b and abs(len(chr_a) - len(a)) + abs(len(chr_b) - len(b)):
#            similarity += self.similarity_real(chr_a, chr_b, stoppercentage)
#            measurements += 1
#        else:
#            similarity *= 2
#            measurements += 1
#
#        wrd_a = segment.words(a)
#        wrd_b = segment.words(b)
#        if len(wrd_a) + len(wrd_b) > 2:
#            similarity += self.similarity_real(wrd_a, wrd_b, 0)
#            measurements += 1
        return similarity / measurements

    def similarity_real(self, a, b, stoppercentage=40):
        """Returns the similarity between a and b based on Levenshtein distance. It
           can stop prematurely as soon as it sees that a and b will be no simmilar than
           the percentage specified in stoppercentage.

           The Levenshtein distance is calculated, but the following should be noted:
               - Only the first MAX_LEN characters are considered. Long strings differing
                 at the end will therefore seem to match better than they should. See the
                 use of the variable penalty to lessen the effect of this.
               - Strings with widely different lengths give the opportunity for shortcut.
                 This is by definition of the Levenshtein distance: the distance will be
                 at least as much as the difference in string length.
               - Calculation is stopped as soon as a similarity of stoppercentage becomes
                 unattainable. See the use of the variable stopvalue.
               - Implementation uses memory O(min(len(a), len(b))
               - Excecution time is O(len(a)*len(b))
        """
        l1, l2 = len(a), len(b)
        if l1 == 0 or l2 == 0:
            return 0
        #Let's make l1 the smallest
        if l1 > l2:
            l1, l2 = l2, l1
            a, b = b, a

        #maxsimilarity is the maximum similarity that can be attained as constrained
        #by the difference in string length
        maxsimilarity = 100 - 100.0 * (l2 - l1) / l2
        if maxsimilarity < stoppercentage:
            return maxsimilarity * 1.0

        #Let's penalise the score in cases where we shorten strings
        penalty = 0
        if l2 > self.MAX_LEN:
            b = b[:self.MAX_LEN]
            l2 = self.MAX_LEN
            penalty += 7
            if l1 > self.MAX_LEN:
                a = a[:self.MAX_LEN]
                l1 = self.MAX_LEN
                penalty += 7

        #The actual value in the array that would represent a giveup situation:
        stopvalue = math.ceil((100.0 - stoppercentage) / 100 * l2)
        dist = distance(a, b, stopvalue)
        if dist > stopvalue:
            return stoppercentage - 1.0

        #If MAX_LEN came into play, we consider the calculated distance to be
        #representative of the distance between the whole, untrimmed strings
        if dist != 0:
            penalty = 0
        return 100 - (dist * 1.0 / l2) * 100 - penalty


if __name__ == "__main__":
    from sys import argv
    comparer = LevenshteinComparer()
    print("Similarity:\n%s" % comparer.similarity(argv[1], argv[2], 50))

########NEW FILE########
__FILENAME__ = match
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Class to perform translation memory matching from a store of
translation units."""

import heapq
import itertools
import re

from translate.misc.multistring import multistring
from translate.search import lshtein, terminology
from translate.storage import base, po


def sourcelen(unit):
    """Returns the length of the source string."""
    return len(unit.source)


def _sort_matches(matches, match_info):

    def _matches_cmp(x, y):
        # This function will sort a list of matches according to the match's starting
        # position, putting the one with the longer source text first, if two are the same.
        c = cmp(match_info[x.source]['pos'], match_info[y.source]['pos'])
        return c and c or cmp(len(y.source), len(x.source))
    matches.sort(_matches_cmp)


class matcher(object):
    """A class that will do matching and store configuration for the
    matching process."""

    sort_reverse = False

    def __init__(self, store, max_candidates=10, min_similarity=75, max_length=70, comparer=None, usefuzzy=False):
        """max_candidates is the maximum number of candidates that should be assembled,
        min_similarity is the minimum similarity that must be attained to be included in
        the result, comparer is an optional Comparer with similarity() function"""
        if comparer is None:
            comparer = lshtein.LevenshteinComparer(max_length)
        self.comparer = comparer
        self.setparameters(max_candidates, min_similarity, max_length)
        self.usefuzzy = usefuzzy
        self.inittm(store)
        self.addpercentage = True

    def usable(self, unit):
        """Returns whether this translation unit is usable for TM"""
        #TODO: We might want to consider more attributes, such as approved, reviewed, etc.
        source = unit.source
        target = unit.target
        if source and target and (self.usefuzzy or not unit.isfuzzy()):
            if len(source) < 2:
                return False
            if source in self.existingunits and self.existingunits[source] == target:
                return False
            else:
                self.existingunits[source] = target
                return True
        return False

    def inittm(self, stores, reverse=False):
        """Initialises the memory for later use. We use simple base units for
        speedup."""
        # reverse is deprectated - just use self.sort_reverse
        self.existingunits = {}
        self.candidates = base.TranslationStore()

        if isinstance(stores, base.TranslationStore):
            stores = [stores]
        for store in stores:
            self.extendtm(store.units, store=store, sort=False)
        self.candidates.units.sort(key=sourcelen, reverse=self.sort_reverse)
        # print "TM initialised with %d candidates (%d to %d characters long)" % \
        #        (len(self.candidates.units), len(self.candidates.units[0].source), len(self.candidates.units[-1].source))

    def extendtm(self, units, store=None, sort=True):
        """Extends the memory with extra unit(s).

        :param units: The units to add to the TM.
        :param store: Optional store from where some metadata can be retrieved
                      and associated with each unit.
        :param sort: Optional parameter that can be set to False to supress
                     sorting of the candidates list. This should probably
                     only be used in :meth:`matcher.inittm`.
        """
        if isinstance(units, base.TranslationUnit):
            units = [units]
        for candidate in itertools.ifilter(self.usable, units):
            simpleunit = base.TranslationUnit("")
            # We need to ensure that we don't pass multistrings futher, since
            # some modules (like the native Levenshtein) can't use it.
            if isinstance(candidate.source, multistring):
                if len(candidate.source.strings) > 1:
                    simpleunit.orig_source = candidate.source
                    simpleunit.orig_target = candidate.target
                simpleunit.source = unicode(candidate.source)
                simpleunit.target = unicode(candidate.target)
            else:
                simpleunit.source = candidate.source
                simpleunit.target = candidate.target
            # If we now only get translator comments, we don't get programmer
            # comments in TM suggestions (in Pootle, for example). If we get all
            # notes, pot2po adds all previous comments as translator comments
            # in the new po file
            simpleunit.addnote(candidate.getnotes(origin="translator"))
            simpleunit.fuzzy = candidate.isfuzzy()
            self.candidates.units.append(simpleunit)
        if sort:
            self.candidates.units.sort(key=sourcelen, reverse=self.sort_reverse)

    def setparameters(self, max_candidates=10, min_similarity=75, max_length=70):
        """Sets the parameters without reinitialising the tm. If a parameter
        is not specified, it is set to the default, not ignored"""
        self.MAX_CANDIDATES = max_candidates
        self.MIN_SIMILARITY = min_similarity
        self.MAX_LENGTH = max_length

    def getstoplength(self, min_similarity, text):
        """Calculates a length beyond which we are not interested.
        The extra fat is because we don't use plain character distance only."""
        return min(len(text) / (min_similarity / 100.0), self.MAX_LENGTH)

    def getstartlength(self, min_similarity, text):
        """Calculates the minimum length we are interested in.
        The extra fat is because we don't use plain character distance only."""
        return max(len(text) * (min_similarity / 100.0), 1)

    def matches(self, text):
        """Returns a list of possible matches for given source text.

        :type text: String
        :param text: The text that will be search for in the translation memory
        :rtype: list
        :return: a list of units with the source and target strings from the
                 translation memory. If :attr:`self.addpercentage` is
                 *True* (default) the match quality is given as a
                 percentage in the notes.
        """
        bestcandidates = [(0.0, None)] * self.MAX_CANDIDATES
        #We use self.MIN_SIMILARITY, but if we already know we have max_candidates
        #that are better, we can adjust min_similarity upwards for speedup
        min_similarity = self.MIN_SIMILARITY

        # We want to limit our search in self.candidates, so we want to ignore
        # all units with a source string that is too short or too long. We use
        # a binary search to find the shortest string, from where we start our
        # search in the candidates.

        # minimum source string length to be considered
        startlength = self.getstartlength(min_similarity, text)
        startindex = 0
        endindex = len(self.candidates.units)
        while startindex < endindex:
            mid = (startindex + endindex) // 2
            if sourcelen(self.candidates.units[mid]) < startlength:
                startindex = mid + 1
            else:
                endindex = mid

        # maximum source string length to be considered
        stoplength = self.getstoplength(min_similarity, text)
        lowestscore = 0

        for candidate in self.candidates.units[startindex:]:
            cmpstring = candidate.source
            if len(cmpstring) > stoplength:
                break
            similarity = self.comparer.similarity(text, cmpstring, min_similarity)
            if similarity < min_similarity:
                continue
            if similarity > lowestscore:
                heapq.heapreplace(bestcandidates, (similarity, candidate))
                lowestscore = bestcandidates[0][0]
                if lowestscore >= 100:
                    break
                if min_similarity < lowestscore:
                    min_similarity = lowestscore
                    stoplength = self.getstoplength(min_similarity, text)

        #Remove the empty ones:
        def notzero(item):
            score = item[0]
            return score != 0
        bestcandidates = filter(notzero, bestcandidates)
        #Sort for use as a general list, and reverse so the best one is at index 0
        bestcandidates.sort(reverse=True)
        return self.buildunits(bestcandidates)

    def buildunits(self, candidates):
        """Builds a list of units conforming to base API, with the score
        in the comment."""
        units = []
        for score, candidate in candidates:
            if hasattr(candidate, "orig_source"):
                candidate.source = candidate.orig_source
                candidate.target = candidate.orig_target
            newunit = po.pounit(candidate.source)
            newunit.target = candidate.target
            newunit.markfuzzy(candidate.fuzzy)
            candidatenotes = candidate.getnotes().strip()
            if candidatenotes:
                newunit.addnote(candidatenotes)
            if self.addpercentage:
                newunit.addnote("%d%%" % score)
            units.append(newunit)
        return units


# We don't want to miss certain forms of words that only change a little
# at the end. Now we are tying this code to English, but it should serve
# us well. For example "category" should be found in "categories",
# "copy" should be found in "copied"
#
# The tuples define a regular expression to search for, and with what it
# should be replaced.
ignorepatterns = [
    ("y\s*$", "ie"),          # category/categories, identify/identifies, apply/applied
    ("[\s-]+", ""),           # down time / downtime, pre-order / preorder
    ("-", " "),               # pre-order / pre order
    (" ", "-"),               # pre order / pre-order
]
ignorepatterns_re = [(re.compile(a), b) for (a, b) in ignorepatterns]

context_re = re.compile("\s+\(.*\)\s*$")


class terminologymatcher(matcher):
    """A matcher with settings specifically for terminology matching."""

    sort_reverse = True

    def __init__(self, store, max_candidates=10, min_similarity=75, max_length=500, comparer=None):
        if comparer is None:
            comparer = terminology.TerminologyComparer(max_length)
        matcher.__init__(self, store, max_candidates, min_similarity=10, max_length=max_length, comparer=comparer)
        self.addpercentage = False
        self.match_info = {}

    def inittm(self, store):
        """Normal initialisation, but convert all source strings to lower case"""
        matcher.inittm(self, store)
        extras = []
        for unit in self.candidates.units:
            source = unit.source = context_re.sub("", unit.source).lower()
            for ignorepattern_re, replacement in ignorepatterns_re:
                (newterm, occurrences) = ignorepattern_re.subn(replacement, source)
                # we'll add it as long as we only replaced one thing, but not
                # something like "are-you-sure-you-want-to" due to (" ", "-")
                if occurrences == 1:
                    new_unit = type(unit).buildfromunit(unit)
                    new_unit.source = newterm
                    # We mark it fuzzy to indicate that it isn't pristine
                    unit.markfuzzy()
                    extras.append(new_unit)
        self.candidates.units.sort(key=sourcelen, reverse=self.sort_reverse)
        if extras:
            # We don't sort, so that the altered forms are at the back and
            # considered last.
            self.extendtm(extras, sort=False)

    def getstartlength(self, min_similarity, text):
        # Let's number false matches by not working with terms of two
        # characters or less
        return 3

    def getstoplength(self, min_similarity, text):
        # Let's ignore terms with more than 50 characters. Perhaps someone
        # gave a file with normal (long) translations
        return 50

    def usable(self, unit):
        """Returns whether this translation unit is usable for terminology."""
        if not unit.istranslated():
            return False
        l = len(context_re.sub("", unit.source))
        return l <= self.MAX_LENGTH and l >= self.getstartlength(None, None)

    def matches(self, text):
        """Normal matching after converting text to lower case. Then replace
        with the original unit to retain comments, etc."""
        text_l = len(text)
        if text_l < self.getstartlength(0, ''):  # parameters unused
            # impossible to return anything
            return []
        text = text.lower()
        comparer = self.comparer
        comparer.match_info = {}
        match_info = {}
        matches = []
        known = set()

        # We want to limit our search in self.candidates, so we want to ignore
        # all units with a source string that is too long. We use binary search
        # to find the first string short enough to occur in text, from where we
        # start our search in the candidates.

        # the maximum possible length is text_l
        startindex = 0
        endindex = len(self.candidates.units)
        while startindex < endindex:
            mid = (startindex + endindex) // 2
            if sourcelen(self.candidates.units[mid]) > text_l:
                startindex = mid + 1
            else:
                endindex = mid

        for cand in self.candidates.units[startindex:]:
            source = cand.source
            if (source, cand.target) in known:
                continue
            if comparer.similarity(text, source, self.MIN_SIMILARITY):
                match_info[source] = {'pos': comparer.match_info[source]['pos']}
                matches.append(cand)
                known.add((source, cand.target))

        final_matches = []
        lastend = 0
        _sort_matches(matches, match_info)
        for match in matches:
            start_pos = match_info[match.source]['pos']
            if start_pos < lastend:
                continue
            end = start_pos + len(match.source)

            final_matches.append(match)

            # Get translations for the placeable
            for m in matches:
                if m is match:
                    continue
                m_info = match_info[m.source]
                m_end = m_info['pos']
                if m_end > start_pos:
                    # we past valid possibilities in the list
                    break
                m_end += len(m.source)
                if start_pos == m_info['pos'] and end == m_end:
                    # another match for the same term
                    final_matches.append(m)

            lastend = end
        if final_matches:
            self.match_info = match_info
        return final_matches


# utility functions used by virtaal and tmserver to convert matching units in easily marshallable dictionaries
def unit2dict(unit):
    """converts a pounit to a simple dict structure for use over the web"""
    return {
        "source": unit.source,
        "target": unit.target,
        "quality": _parse_quality(unit.getnotes()),
        "context": unit.getcontext()
    }


def _parse_quality(comment):
    """Extracts match quality from po comments."""
    quality = re.search('([0-9]+)%', comment)
    if quality:
        return quality.group(1)

########NEW FILE########
__FILENAME__ = segment
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module to deal with different types and uses of segmentation"""

#XXX: This module is now deprecated: Use language specific segmenters in the
# lang package (character_iter, word_iter, sentence_iter, etc.).

punctuation = u".,;:!?-@#$%^*_()[]{}/\\'\"<>"


def character_iter(text):
    """Returns an iterator over the characters in text."""
    #We don't return more than one consecutive whitespace character
    prev = 'A'
    for c in text:
        if c.isspace() and prev.isspace():
            continue
        prev = c
        if not (c in punctuation):
            yield c.lower()


def characters(text):
    """Returns a list of characters in text."""
    return [c for c in character_iter(text)]


def word_iter(text):
    """Returns an iterator over the words in text."""
    #TODO: Consider replacing puctuation with space before split()
    for w in text.split():
        yield w.strip(punctuation).lower()


def words(text):
    """Returns a list of words in text."""
    return [w for w in word_iter(text)]


def sentence_iter(text):
    """Returns an iterator over the senteces in text."""
    #TODO: This is very nave. We really should consider all punctuation,
    #and return the punctuation with the sentence.
    #TODO: Search for capital letter start with next sentence to avoid
    #confusion with abbreviations. And remember Afrikaans "'n" :-)
    for s in text.split(". "):
        yield s.strip()


def sentences(text):
    """Returns a list of senteces in text."""
    return [s for s in sentence_iter(text)]

########NEW FILE########
__FILENAME__ = terminology
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A class that does terminology matching"""


class TerminologyComparer:

    def __init__(self, max_len=500):
        self.match_info = {}
        self.MAX_LEN = max_len

    def similarity(self, text, term, stoppercentage=40):
        """Returns the match quality of ``term`` in the ``text``"""
        # We could segment the words, but mostly it will give less ideal
        # results, since we'll miss plurals, etc. Then we also can't search for
        # multiword terms, such as "Free Software". Ideally we should use a
        # stemmer, like the Porter stemmer.

        # So we just see if the word occurs anywhere. This is not perfect since
        # we might get more than we bargained for. The term "form" will be found
        # in the word "format", for example. A word like "at" will trigger too
        # many false positives.

        text = text[:self.MAX_LEN]

        pos = text.find(term)
        if pos >= 0:
            self.match_info[term] = {'pos': pos}
            return 100
        return 0

########NEW FILE########
__FILENAME__ = test_lshtein
from translate.search import lshtein


class TestLevenshtein:
    """Test whether Levenshtein distance calculations are correct"""

    def test_basic_distance(self):
        """Tests distance correctness with a few basic values"""
        levenshtein = lshtein.LevenshteinComparer()
        assert lshtein.distance("word", "word") == 0
        assert lshtein.distance("word", "") == 4
        assert lshtein.distance("", "word") == 4
        assert lshtein.distance("word", "word 2") == 2
        assert lshtein.distance("words", "word") == 1
        assert lshtein.distance("word", "woord") == 1

    def test_basic_similarity(self):
        """Tests similarity correctness with a few basic values"""
        levenshtein = lshtein.LevenshteinComparer()
        assert levenshtein.similarity("word", "word") == 100
        assert levenshtein.similarity("word", "words") == 80
        assert levenshtein.similarity("word", "wood") == 75
        assert levenshtein.similarity("aaa", "bbb", 0) == 0

    def test_long_similarity(self):
        """Tests that very long strings are handled well."""
        #A sentence with 240 characters:
        sentence = "A long, dreary sentence about a cow that never new his mother. Actually it didn't known its father either. One day he decided that enough is enough, and that he would stop making long, dreary sentences just for the sake of making sentences."
        levenshtein = lshtein.LevenshteinComparer()
        assert levenshtein.similarity("Cow", sentence, 10) < 10
        assert levenshtein.similarity(sentence, "Cow", 10) < 10
        #The difference in the next comparison is supposed to be 25.83, but
        #since the sentence is long it might be chopped and report higher.
        assert levenshtein.similarity(sentence, sentence[0:62], 0) > 25
        assert levenshtein.similarity(sentence, sentence[0:62], 0) < 50

########NEW FILE########
__FILENAME__ = test_match
from translate.search import match
from translate.storage import csvl10n


class TestMatch:
    """Test the matching class"""

    def candidatestrings(self, units):
        """returns only the candidate strings out of the list with (score, string) tuples"""
        return [unit.source for unit in units]

    def buildcsv(self, sources, targets=None):
        """Build a csvfile store with the given source and target strings"""
        if targets is None:
            targets = sources
        else:
            assert len(sources) == len(targets)
        csvfile = csvl10n.csvfile()
        for source, target in zip(sources, targets):
            unit = csvfile.addsourceunit(source)
            unit.target = target
        return csvfile

    def test_matching(self):
        """Test basic matching"""
        csvfile = self.buildcsv(["hand", "asdf", "fdas", "haas", "pond"])
        matcher = match.matcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("hond"))
        candidates.sort()
        assert candidates == ["hand", "pond"]
        message = "Ek skop die bal"
        csvfile = self.buildcsv(
            ["Hy skop die bal",
            message,
            "Jannie skop die bal",
            "Ek skop die balle",
            "Niemand skop die bal nie"])
        matcher = match.matcher(csvfile)
        candidates = self.candidatestrings(matcher.matches(message))
        assert len(candidates) == 3
        #test that the 100% match is indeed first:
        assert candidates[0] == message
        candidates.sort()
        assert candidates[1:] == ["Ek skop die balle", "Hy skop die bal"]

    def test_multiple_store(self):
        """Test using multiple datastores"""
        csvfile1 = self.buildcsv(["hand", "asdf", "fdas"])
        csvfile2 = self.buildcsv(["haas", "pond"])
        matcher = match.matcher([csvfile1, csvfile2])
        candidates = self.candidatestrings(matcher.matches("hond"))
        candidates.sort()
        assert candidates == ["hand", "pond"]
        message = "Ek skop die bal"
        csvfile1 = self.buildcsv(
            ["Hy skop die bal",
            message,
            "Jannie skop die bal"])
        csvfile2 = self.buildcsv(
            ["Ek skop die balle",
            "Niemand skop die bal nie"])
        matcher = match.matcher([csvfile1, csvfile2])
        candidates = self.candidatestrings(matcher.matches(message))
        assert len(candidates) == 3
        #test that the 100% match is indeed first:
        assert candidates[0] == message
        candidates.sort()
        assert candidates[1:] == ["Ek skop die balle", "Hy skop die bal"]

    def test_extendtm(self):
        """Test that we can extend the TM after creation."""
        message = "Open file..."
        csvfile1 = self.buildcsv(["Close application", "Do something"])
        matcher = match.matcher([csvfile1])
        candidates = self.candidatestrings(matcher.matches(message))
        assert len(candidates) == 0
        csvfile2 = self.buildcsv(["Open file"])
        matcher.extendtm(csvfile2.units, store=csvfile2)
        candidates = self.candidatestrings(matcher.matches(message))
        assert len(candidates) == 1
        assert candidates[0] == "Open file"

    def test_terminology(self):
        csvfile = self.buildcsv(["file", "computer", "directory"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("Copy the files from your computer"))
        candidates.sort()
        assert candidates == ["computer", "file"]

    def test_brackets(self):
        """Tests that brackets at the end of a term are ignored"""
        csvfile = self.buildcsv(["file (noun)", "ISP (Internet Service Provider)"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("Open File"))
        assert candidates == ["file"]
        candidates = self.candidatestrings(matcher.matches("Contact your ISP"))
        # we lowercase everything - that is why we get it back differerntly.
        # we don't change the target text, though
        assert candidates == ["isp"]

    def test_past_tences(self):
        """Tests matching of some past tenses"""
        csvfile = self.buildcsv(["submit", "certify"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("The bug was submitted"))
        assert candidates == ["submit"]
        candidates = self.candidatestrings(matcher.matches("The site is certified"))

    def test_space_mismatch(self):
        """Tests that we can match with some spacing mismatch"""
        csvfile = self.buildcsv(["down time"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("%d minutes downtime"))
        assert candidates == ["downtime"]

    def test_hyphen_mismatch(self):
        """Tests that we can match with some spacing mismatch"""
        csvfile = self.buildcsv(["pre-order"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("You can preorder"))
        assert candidates == ["preorder"]
        candidates = self.candidatestrings(matcher.matches("You can pre order"))
        assert candidates == ["pre order"]

        csvfile = self.buildcsv(["pre order"])
        matcher = match.terminologymatcher(csvfile)
        candidates = self.candidatestrings(matcher.matches("You can preorder"))
        assert candidates == ["preorder"]
        candidates = self.candidatestrings(matcher.matches("You can pre order"))
        assert candidates == ["pre order"]

########NEW FILE########
__FILENAME__ = test_terminology
from translate.search import terminology


class TestTerminology:
    """Test terminology matching"""

    def test_basic(self):
        """Tests basic functionality"""
        termmatcher = terminology.TerminologyComparer()
        assert termmatcher.similarity("Open the file", "file") > 75

########NEW FILE########
__FILENAME__ = tmserver
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A translation memory server using tmdb for storage, communicates
with clients using JSON over HTTP."""

import json
import logging
from argparse import ArgumentParser
from urlparse import parse_qs

from translate.misc import selector, wsgi
from translate.storage import base, tmdb


class TMServer(object):
    """A RESTful JSON TM server."""

    def __init__(self, tmdbfile, tmfiles, max_candidates=3, min_similarity=75,
            max_length=1000, prefix="", source_lang=None, target_lang=None):
        if not isinstance(tmdbfile, unicode):
            import sys
            tmdbfile = tmdbfile.decode(sys.getfilesystemencoding())

        self.tmdb = tmdb.TMDB(tmdbfile, max_candidates, min_similarity,
                              max_length)

        if tmfiles:
            self._load_files(tmfiles, source_lang, target_lang)

        #initialize url dispatcher
        self.rest = selector.Selector(prefix=prefix)
        self.rest.add("/{slang}/{tlang}/unit/{uid:any}",
                      GET=self.translate_unit,
                      POST=self.update_unit,
                      PUT=self.add_unit,
                      DELETE=self.forget_unit)

        self.rest.add("/{slang}/{tlang}/store/{sid:any}",
                      GET=self.get_store_stats,
                      PUT=self.upload_store,
                      POST=self.add_store,
                      DELETE=self.forget_store)

    def _load_files(self, tmfiles, source_lang, target_lang):
        from translate.storage import factory
        if isinstance(tmfiles, list):
            [self.tmdb.add_store(factory.getobject(tmfile),
                                 source_lang, target_lang) \
                    for tmfile in tmfiles]
        elif tmfiles:
            self.tmdb.add_store(factory.getobject(tmfiles), source_lang,
                                target_lang)

    @selector.opliant
    def translate_unit(self, environ, start_response, uid, slang, tlang):
        start_response("200 OK", [('Content-type', 'text/plain')])
        candidates = self.tmdb.translate_unit(uid, slang, tlang)
        logging.debug("candidates: %s", unicode(candidates))
        response = json.dumps(candidates, indent=4)
        params = parse_qs(environ.get('QUERY_STRING', ''))
        try:
            callback = params.get('callback', [])[0]
            response = "%s(%s)" % (callback, response)
        except IndexError:
            pass
        return [response]

    @selector.opliant
    def add_unit(self, environ, start_response, uid, slang, tlang):
        start_response("200 OK", [('Content-type', 'text/plain')])
        #uid = unicode(urllib.unquote_plus(uid), "utf-8")
        data = json.loads(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
        unit = base.TranslationUnit(data['source'])
        unit.target = data['target']
        self.tmdb.add_unit(unit, slang, tlang)
        return [""]

    @selector.opliant
    def update_unit(self, environ, start_response, uid, slang, tlang):
        start_response("200 OK", [('Content-type', 'text/plain')])
        #uid = unicode(urllib.unquote_plus(uid), "utf-8")
        data = json.loads(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
        unit = base.TranslationUnit(data['source'])
        unit.target = data['target']
        self.tmdb.add_unit(unit, slang, tlang)
        return [""]

    @selector.opliant
    def forget_unit(self, environ, start_response, uid):
        #FIXME: implement me
        start_response("200 OK", [('Content-type', 'text/plain')])
        #uid = unicode(urllib.unquote_plus(uid), "utf-8")

        return [response]

    @selector.opliant
    def get_store_stats(self, environ, start_response, sid):
        #FIXME: implement me
        start_response("200 OK", [('Content-type', 'text/plain')])
        #sid = unicode(urllib.unquote_plus(sid), "utf-8")

        return [response]

    @selector.opliant
    def upload_store(self, environ, start_response, sid, slang, tlang):
        """add units from uploaded file to tmdb"""
        from cStringIO import StringIO
        from translate.storage import factory
        start_response("200 OK", [('Content-type', 'text/plain')])
        data = StringIO(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
        data.name = sid
        store = factory.getobject(data)
        count = self.tmdb.add_store(store, slang, tlang)
        response = "added %d units from %s" % (count, sid)
        return [response]

    @selector.opliant
    def add_store(self, environ, start_response, sid, slang, tlang):
        """Add unit from POST data to tmdb."""
        start_response("200 OK", [('Content-type', 'text/plain')])
        units = json.loads(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
        count = self.tmdb.add_list(units, slang, tlang)
        response = "added %d units from %s" % (count, sid)
        return [response]

    @selector.opliant
    def forget_store(self, environ, start_response, sid):
        #FIXME: implement me
        start_response("200 OK", [('Content-type', 'text/plain')])
        #sid = unicode(urllib.unquote_plus(sid), "utf-8")

        return [response]


def main():
    parser = ArgumentParser()
    parser.add_argument("-d", "--tmdb", dest="tmdbfile", default=":memory:",
                        help="translation memory database file")
    parser.add_argument("-f", "--import-translation-file", dest="tmfiles",
                        action="append",
                        help="translation file to import into the database")
    parser.add_argument("-t", "--import-target-lang", dest="target_lang",
                        help="target language of translation files")
    parser.add_argument("-s", "--import-source-lang", dest="source_lang",
                        help="source language of translation files")
    parser.add_argument("-b", "--bind", dest="bind", default="localhost",
                        help="adress to bind server to (default: localhost)")
    parser.add_argument("-p", "--port", dest="port", type=int, default=8888,
                        help="port to listen on (default: 8888)")
    parser.add_argument("--max-candidates", dest="max_candidates", type=int,
                        default=3,
                        help="Maximum number of candidates")
    parser.add_argument("--min-similarity", dest="min_similarity", type=int,
                        default=75,
                        help="minimum similarity")
    parser.add_argument("--max-length", dest="max_length", type=int,
                        default=1000,
                        help="Maxmimum string length")
    parser.add_argument("--debug", action="store_true", dest="debug",
                        default=False,
                        help="enable debugging features")

    args = parser.parse_args()

    #setup debugging
    format = '%(asctime)s %(levelname)s %(message)s'
    level = args.debug and logging.DEBUG or logging.WARNING
    if args.debug:
        format = '%(levelname)7s %(module)s.%(funcName)s:%(lineno)d: %(message)s'

    logging.basicConfig(level=level, format=format)

    application = TMServer(args.tmdbfile, args.tmfiles,
                           max_candidates=args.max_candidates,
                           min_similarity=args.min_similarity,
                           max_length=args.max_length,
                           prefix="/tmserver",
                           source_lang=args.source_lang,
                           target_lang=args.target_lang)
    wsgi.launch_server(args.bind, args.port, application.rest)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = aresource
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2012 Michal iha
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module for handling Android String resource files."""

import re

from lxml import etree

from translate.lang import data
from translate.storage import base, lisa


EOF = None
WHITESPACE = ' \n\t'  # Whitespace that we collapse.
MULTIWHITESPACE = re.compile('[ \n\t]{2}')


class AndroidResourceUnit(base.TranslationUnit):
    """A single entry in the Android String resource file."""
    rootNode = "string"
    languageNode = "string"

    @classmethod
    def createfromxmlElement(cls, element):
        term = cls(None, xmlelement=element)
        return term

    def __init__(self, source, empty=False, xmlelement=None, **kwargs):
        if xmlelement is not None:
            self.xmlelement = xmlelement
        else:
            self.xmlelement = etree.Element(self.rootNode)
            self.xmlelement.tail = '\n'
        if source is not None:
            self.setid(source)
        super(AndroidResourceUnit, self).__init__(source)

    def istranslatable(self):
        return (
            bool(self.getid())
            and self.xmlelement.get('translatable') != 'false'
        )

    def isblank(self):
        return not bool(self.getid())

    def getid(self):
        return self.xmlelement.get("name")

    def setid(self, newid):
        return self.xmlelement.set("name", newid)

    def getcontext(self):
        return self.xmlelement.get("name")

    def unescape(self, text):
        '''
        Remove escaping from Android resource.

        Code stolen from android2po
        <https://github.com/miracle2k/android2po>
        '''
        # Return text for empty elements
        if text is None:
            return ''

        # We need to collapse multiple whitespace while paying
        # attention to Android's quoting and escaping.
        space_count = 0
        active_quote = False
        active_percent = False
        active_escape = False
        formatted = False
        i = 0
        text = list(text) + [EOF]
        while i < len(text):
            c = text[i]

            # Handle whitespace collapsing
            if c is not EOF and c in WHITESPACE:
                space_count += 1
            elif space_count > 1:
                # Remove duplicate whitespace; Pay attention: We
                # don't do this if we are currently inside a quote,
                # except for one special case: If we have unbalanced
                # quotes, e.g. we reach eof while a quote is still
                # open, we *do* collapse that trailing part; this is
                # how Android does it, for some reason.
                if not active_quote or c is EOF:
                    # Replace by a single space, will get rid of
                    # non-significant newlines/tabs etc.
                    text[i-space_count : i] = ' '
                    i -= space_count - 1
                space_count = 0
            elif space_count == 1:
                # At this point we have a single whitespace character,
                # but it might be a newline or tab. If we write this
                # kind of insignificant whitespace into the .po file,
                # it will be considered significant on import. So,
                # make sure that this kind of whitespace is always a
                # standard space.
                text[i-1] = ' '
                space_count = 0
            else:
                space_count = 0

            # Handle quotes
            if c == '"' and not active_escape:
                active_quote = not active_quote
                del text[i]
                i -= 1

            # If the string is run through a formatter, it will have
            # percentage signs for String.format
            if c == '%' and not active_escape:
                active_percent = not active_percent
            elif not active_escape and active_percent:
                formatted = True
                active_percent = False

            # Handle escapes
            if c == '\\':
                if not active_escape:
                    active_escape = True
                else:
                    # A double-backslash represents a single;
                    # simply deleting the current char will do.
                    del text[i]
                    i -= 1
                    active_escape = False
            else:
                if active_escape:
                    # Handle the limited amount of escape codes
                    # that we support.
                    # TODO: What about \r, or \r\n?
                    if c is EOF:
                        # Basically like any other char, but put
                        # this first so we can use the ``in`` operator
                        # in the clauses below without issue.
                        pass
                    elif c == 'n' or c == 'N':
                        text[i-1 : i+1] = '\n'  # an actual newline
                        i -= 1
                    elif c == 't' or c == 'T':
                        text[i-1 : i+1] = '\t'  # an actual tab
                        i -= 1
                    elif c == ' ':
                        text[i-1 : i+1] = ' '  # an actual space
                        i -= 1
                    elif c in '"\'@':
                        text[i-1 : i] = ''  # remove the backslash
                        i -= 1
                    elif c == 'u':
                        # Unicode sequence. Android is nice enough to deal
                        # with those in a way which let's us just capture
                        # the next 4 characters and raise an error if they
                        # are not valid (rather than having to use a new
                        # state to parse the unicode sequence).
                        # Exception: In case we are at the end of the
                        # string, we support incomplete sequences by
                        # prefixing the missing digits with zeros.
                        # Note: max(len()) is needed in the slice due to
                        # trailing ``None`` element.
                        max_slice = min(i+5, len(text)-1)
                        codepoint_str = "".join(text[i+1 : max_slice])
                        if len(codepoint_str) < 4:
                            codepoint_str = u"0" * (4-len(codepoint_str)) + codepoint_str
                        try:
                            # We can't trust int() to raise a ValueError,
                            # it will ignore leading/trailing whitespace.
                            if not codepoint_str.isalnum():
                                raise ValueError(codepoint_str)
                            codepoint = unichr(int(codepoint_str, 16))
                        except ValueError:
                            raise ValueError('bad unicode escape sequence')

                        text[i-1 : max_slice] = codepoint
                        i -= 1
                    else:
                        # All others, remove, like Android does as well.
                        text[i-1 : i+1] = ''
                        i -= 1
                    active_escape = False

            i += 1

        # Join the string together again, but w/o EOF marker
        return "".join(text[:-1])

    def escape(self, text):
        '''
        Escape all the characters which need to be escaped in an Android XML file.
        '''
        if text is None:
            return
        if len(text) == 0:
            return ''
        text = text.replace('\\', '\\\\')
        text = text.replace('\n', '\\n')
        # This will add non intrusive real newlines to
        # ones in translation improving readability of result
        text = text.replace(' \\n', '\n\\n')
        text = text.replace('\t', '\\t')
        text = text.replace('\'', '\\\'')
        text = text.replace('"', '\\"')

        # @ needs to be escaped at start
        if text.startswith('@'):
            text = '\\@' + text[1:]
        # Quote strings with more whitespace
        if text[0] in WHITESPACE or text[-1] in WHITESPACE or len(MULTIWHITESPACE.findall(text)) > 0:
            return '"%s"' % text
        return text

    def setsource(self, source):
        super(AndroidResourceUnit, self).setsource(source)

    def getsource(self, lang=None):
        if (super(AndroidResourceUnit, self).source is None):
            return self.target
        else:
            return super(AndroidResourceUnit, self).source

    source = property(getsource, setsource)

    def settarget(self, target):
        if '<' in target:
            # Handle text with possible markup
            target = target.replace('&', '&amp;')
            try:
                # Try as XML
                newstring = etree.fromstring('<string>%s</string>' % target)
            except:
                # Fallback to string with XML escaping
                target = target.replace('<', '&lt;')
                newstring = etree.fromstring('<string>%s</string>' % target)
            # Update text
            if newstring.text is None:
                self.xmlelement.text = ''
            else:
                self.xmlelement.text = newstring.text
            # Remove old elements
            for x in self.xmlelement.iterchildren():
                self.xmlelement.remove(x)
            # Add new elements
            for x in newstring.iterchildren():
                self.xmlelement.append(x)
        else:
            # Handle text only
            self.xmlelement.text = self.escape(target)
        super(AndroidResourceUnit, self).settarget(target)

    def gettarget(self, lang=None):
        # Grab inner text
        target = self.unescape(self.xmlelement.text or u'')
        # Include markup as well
        target += u''.join([data.forceunicode(etree.tostring(child, encoding='utf-8')) for child in self.xmlelement.iterchildren()])
        return target

    target = property(gettarget, settarget)

    def getlanguageNode(self, lang=None, index=None):
        return self.xmlelement

    # Notes are handled as previous sibling comments.
    def addnote(self, text, origin=None, position="append"):
        if origin in ['programmer', 'developer', 'source code', None]:
            self.xmlelement.addprevious(etree.Comment(text))
        else:
            return super(AndroidResourceUnit, self).addnote(text, origin=origin,
                                                 position=position)

    def getnotes(self, origin=None):
        if origin in ['programmer', 'developer', 'source code', None]:
            comments = []
            if (self.xmlelement is not None):
                prevSibling = self.xmlelement.getprevious()
                while ((prevSibling is not None) and (prevSibling.tag is etree.Comment)):
                    comments.insert(0, prevSibling.text)
                    prevSibling = prevSibling.getprevious()

            return u'\n'.join(comments)
        else:
            return super(AndroidResourceUnit, self).getnotes(origin)

    def removenotes(self):
        if ((self.xmlelement is not None) and (self.xmlelement.getparent is not None)):
            prevSibling = self.xmlelement.getprevious()
            while ((prevSibling is not None) and (prevSibling.tag is etree.Comment)):
                prevSibling.getparent().remove(prevSibling)
                prevSibling = self.xmlelement.getprevious()

        super(AndroidResourceUnit, self).removenotes()

    def __str__(self):
        return etree.tostring(self.xmlelement, pretty_print=True,
                              encoding='utf-8')

    def __eq__(self, other):
        return (str(self) == str(other))


class AndroidResourceFile(lisa.LISAfile):
    """Class representing an Android String resource file store."""
    UnitClass = AndroidResourceUnit
    Name = "Android String Resource"
    Mimetypes = ["application/xml"]
    Extensions = ["xml"]
    rootNode = "resources"
    bodyNode = "resources"
    XMLskeleton = '''<?xml version="1.0" encoding="utf-8"?>
<resources></resources>'''

    def initbody(self):
        """Initialises self.body so it never needs to be retrieved from the
        XML again."""
        self.namespace = self.document.getroot().nsmap.get(None, None)
        self.body = self.document.getroot()

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Base classes for storage interfaces."""

import logging
try:
    import cPickle as pickle
except ImportError:
    import pickle

from translate.misc.multistring import multistring
from translate.storage.placeables import (StringElem, general,
                                          parse as rich_parse)
from translate.storage.workflow import StateEnum as states


def force_override(method, baseclass):
    """Forces derived classes to override method."""

    if type(method.im_self) == type(baseclass):
        # then this is a classmethod and im_self is the actual class
        actualclass = method.im_self
    else:
        actualclass = method.im_class
    if actualclass != baseclass:
        raise NotImplementedError(
            "%s does not reimplement %s as required by %s" %
            (actualclass.__name__, method.__name__, baseclass.__name__))


class ParseError(Exception):

    def __init__(self, inner_exc):
        self.inner_exc = inner_exc

    def __str__(self):
        return repr(self.inner_exc)


class TranslationUnit(object):
    """Base class for translation units.

    Our concept of a *translation unit* is influenced heavily by `XLIFF
    <http://docs.oasis-open.org/xliff/xliff-core/xliff-core.html>`_.

    As such most of the method- and variable names borrows from XLIFF
    terminology.

    A translation unit consists of the following:

    - A *source* string. This is the original translatable text.
    - A *target* string. This is the translation of the *source*.
    - Zero or more *notes* on the unit. Notes would typically be some comments
      from a translator on the unit, or some comments originating from the
      source code.
    - Zero or more *locations*. Locations indicate where in the original source
      code this unit came from.
    - Zero or more *errors*. Some tools (eg.
      :mod:`~translate.filters.pofilter`) can run checks on translations and
      produce error messages.

    """

    rich_parsers = []
    """A list of functions to use for parsing a string into a rich string
    tree."""

    # State constants
    S_OBSOLETE = states.OBSOLETE
    S_EMPTY = states.EMPTY
    S_NEEDS_WORK = states.NEEDS_WORK
    S_REJECTED = states.REJECTED
    S_NEEDS_REVIEW = states.NEEDS_REVIEW
    S_UNREVIEWED = states.UNREVIEWED
    S_FINAL = states.FINAL

    # Elaborate state support could look something like this:
    # STATE = {
    #     S_OBSOLETE: (states.OBSOLETE, states.EMPTY),
    #     S_EMPTY: (states.EMPTY, states.NEEDS_WORK),
    #     S_NEEDS_WORK: (states.NEEDS_WORK, states.REJECTED),
    #     S_REJECTED: (states.REJECTED, states.NEEDS_REVIEW),
    #     S_NEEDS_REVIEW: (states.NEEDS_REVIEW, states.UNREVIEWED),
    #     S_UNREVIEWED: (states.UNREVIEWED, states.FINAL),
    #     S_FINAL: (states.FINAL, states.MAX),
    # }
    # """
    # Default supported states:
    #     * obsolete: The unit is not to be used.
    #     * empty: The unit has not been translated before.
    #     * needs work: Some translation has been done, but is not complete.
    #     * rejected: The unit has been reviewed, but was rejected.
    #     * needs review: The unit has been translated, but review was requested.
    #     * unreviewed: The unit has been translated, but not reviewed.
    #     * final: The unit is translated, reviewed and accepted.
    # """
    #
    # ... but by default a format will not support state:
    STATE = {}

    _store = None
    _source = None
    _target = None
    _rich_source = None
    _rich_target = None
    _state_n = 0
    notes = ""

    def __init__(self, source=None):
        """Constructs a TranslationUnit containing the given source string."""
        if source is not None:
            self.source = source

    def __eq__(self, other):
        """Compares two TranslationUnits.

        :type other: :class:`TranslationUnit`
        :param other: Another :class:`TranslationUnit`
        :rtype: Boolean
        :return: Returns *True* if the supplied :class:`TranslationUnit`
                 equals this unit.
        """
        return self.source == other.source and self.target == other.target

    def __str__(self):
        """Converts to a string representation that can be parsed back using
        :meth:`~.TranslationStore.parsestring`."""
        # no point in pickling store object, so let's hide it for a while.
        store = getattr(self, "_store", None)
        self._store = None
        dump = pickle.dumps(self)
        self._store = store
        return dump

    @classmethod
    def rich_to_multistring(cls, elem_list):
        """Convert a "rich" string tree to a ``multistring``:

           >>> from translate.storage.placeables.interfaces import X
           >>> rich = [StringElem(['foo', X(id='xxx', sub=[' ']), 'bar'])]
           >>> TranslationUnit.rich_to_multistring(rich)
           multistring(u'foo bar')
        """
        return multistring([unicode(elem) for elem in elem_list])

    def multistring_to_rich(self, mulstring):
        """Convert a multistring to a list of "rich" string trees:

           >>> target = multistring([u'foo', u'bar', u'baz'])
           >>> TranslationUnit.multistring_to_rich(target)
           [<StringElem([<StringElem([u'foo'])>])>,
            <StringElem([<StringElem([u'bar'])>])>,
            <StringElem([<StringElem([u'baz'])>])>]
        """
        if isinstance(mulstring, multistring):
            return [rich_parse(s, self.rich_parsers) for s in mulstring.strings]
        return [rich_parse(mulstring, self.rich_parsers)]

    def setsource(self, source):
        """Set the source string to the given value."""
        self._rich_source = None
        self._source = source
    source = property(lambda self: self._source, setsource)

    def settarget(self, target):
        """Set the target string to the given value."""
        self._rich_target = None
        self._target = target
    target = property(lambda self: self._target, settarget)

    def _get_rich_source(self):
        if self._rich_source is None:
            self._rich_source = self.multistring_to_rich(self.source)
        return self._rich_source

    def _set_rich_source(self, value):
        if not hasattr(value, '__iter__'):
            raise ValueError('value must be iterable')
        if len(value) < 1:
            raise ValueError('value must have at least one element.')
        if not isinstance(value[0], StringElem):
            raise ValueError('value[0] must be of type StringElem.')
        self._rich_source = list(value)
        multi = self.rich_to_multistring(value)
        if self.source != multi:
            self.source = multi
    rich_source = property(_get_rich_source, _set_rich_source)
    """
    .. seealso:: :meth:`.rich_to_multistring`, :meth:`multistring_to_rich`
    """

    def _get_rich_target(self):
        if self._rich_target is None:
            self._rich_target = self.multistring_to_rich(self.target)
        return self._rich_target

    def _set_rich_target(self, value):
        if not hasattr(value, '__iter__'):
            raise ValueError('value must be iterable')
        if len(value) < 1:
            raise ValueError('value must have at least one element.')
        if not isinstance(value[0], StringElem):
            raise ValueError('value[0] must be of type StringElem.')
        self._rich_target = list(value)
        self.target = self.rich_to_multistring(value)
    rich_target = property(_get_rich_target, _set_rich_target)
    """
    .. seealso:: :meth:`.rich_to_multistring`, :meth:`.multistring_to_rich`
    """

    def gettargetlen(self):
        """Returns the length of the target string.

        :rtype: Integer

        .. note::

           Plural forms might be combined.
        """
        length = len(self.target or "")
        strings = getattr(self.target, "strings", [])
        if strings:
            length += sum([len(pluralform) for pluralform in strings[1:]])
        return length

    def getid(self):
        """A unique identifier for this unit.

        :rtype: string
        :return: an identifier for this unit that is unique in the store

        Derived classes should override this in a way that guarantees a unique
        identifier for each unit in the store.
        """
        return self.source

    def setid(self, value):
        """Sets the unique identified for this unit.

        only implemented if format allows ids independant from other
        unit properties like source or context"""
        pass

    def getlocations(self):
        """A list of source code locations.

        :rtype: List

        .. note::

           Shouldn't be implemented if the format doesn't support it.
        """
        return []

    def addlocation(self, location):
        """Add one location to the list of locations.

        .. note::

           Shouldn't be implemented if the format doesn't support it.
        """
        pass

    def addlocations(self, location):
        """Add a location or a list of locations.

        .. note::

           Most classes shouldn't need to implement this, but should rather
           implement :meth:`TranslationUnit.addlocation`.

        .. warning::

           This method might be removed in future.
        """
        if isinstance(location, list):
            for item in location:
                self.addlocation(item)
        else:
            self.addlocation(location)

    def getcontext(self):
        """Get the message context."""
        return ""

    def setcontext(self, context):
        """Set the message context"""
        pass

    def getnotes(self, origin=None):
        """Returns all notes about this unit.

        It will probably be freeform text or something reasonable that can be
        synthesised by the format.
        It should not include location comments (see
        :meth:`~.TranslationUnit.getlocations`).
        """
        return getattr(self, "notes", "")

    def addnote(self, text, origin=None, position="append"):
        """Adds a note (comment).

        :type text: string
        :param text: Usually just a sentence or two.
        :type origin: string
        :param origin: Specifies who/where the comment comes from.
                       Origin can be one of the following text strings:
                       - 'translator'
                       - 'developer', 'programmer', 'source code' (synonyms)
        """
        if position == "append" and getattr(self, "notes", None):
            self.notes += '\n' + text
        else:
            self.notes = text

    def removenotes(self):
        """Remove all the translator's notes."""
        self.notes = u''

    def adderror(self, errorname, errortext):
        """Adds an error message to this unit.

        :type errorname: string
        :param errorname: A single word to id the error.
        :type errortext: string
        :param errortext: The text describing the error.
        """
        pass

    def geterrors(self):
        """Get all error messages.

        :rtype: Dictionary
        """
        return {}

    def markreviewneeded(self, needsreview=True, explanation=None):
        """Marks the unit to indicate whether it needs review.

        :keyword needsreview: Defaults to True.
        :keyword explanation: Adds an optional explanation as a note.
        """
        pass

    def istranslated(self):
        """Indicates whether this unit is translated.

        This should be used rather than deducing it from .target,
        to ensure that other classes can implement more functionality
        (as XLIFF does).
        """
        return bool(self.target) and not self.isfuzzy()

    def istranslatable(self):
        """Indicates whether this unit can be translated.

        This should be used to distinguish real units for translation from
        header, obsolete, binary or other blank units.
        """
        return bool(self.source)

    def isfuzzy(self):
        """Indicates whether this unit is fuzzy."""
        return False

    def markfuzzy(self, value=True):
        """Marks the unit as fuzzy or not."""
        pass

    def isobsolete(self):
        """indicate whether a unit is obsolete"""
        return False

    def makeobsolete(self):
        """Make a unit obsolete"""
        pass

    def isheader(self):
        """Indicates whether this unit is a header."""
        return False

    def isreview(self):
        """Indicates whether this unit needs review."""
        return False

    def isblank(self):
        """Used to see if this unit has no source or target string.

        .. note::

           This is probably used more to find translatable units,
           and we might want to move in that direction rather and
           get rid of this.
        """
        return not (self.source or self.target)

    def hasplural(self):
        """Tells whether or not this specific unit has plural strings."""
        #TODO: Reconsider
        return False

    def getsourcelanguage(self):
        return self._store.getsourcelanguage()

    def gettargetlanguage(self):
        return self._store.gettargetlanguage()

    def merge(self, otherunit, overwrite=False, comments=True,
              authoritative=False):
        """Do basic format agnostic merging."""
        if not self.target or overwrite:
            self.rich_target = otherunit.rich_target

    def unit_iter(self):
        """Iterator that only returns this unit."""
        yield self

    def getunits(self):
        """This unit in a list."""
        return [self]

    @classmethod
    def buildfromunit(cls, unit):
        """Build a native unit from a foreign unit, preserving as much
        information as possible."""
        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
            return unit.copy()
        newunit = cls(unit.source)
        newunit.target = unit.target
        newunit.markfuzzy(unit.isfuzzy())
        locations = unit.getlocations()
        if locations:
            newunit.addlocations(locations)
        notes = unit.getnotes()
        if notes:
            newunit.addnote(notes)
        return newunit

    xid = property(lambda self: None, lambda self, value: None)
    rid = property(lambda self: None, lambda self, value: None)

    def get_state_id(self, n=None):
        if n is None:
            n = self.get_state_n()
        for state_id, state_range in self.STATE.iteritems():
            if state_range[0] <= n < state_range[1]:
                return state_id
        if self.STATE:
            raise ValueError('No state containing value %s' % (n))
        else:
            return n

    def get_state_n(self):
        if self.STATE:
            return self._state_n
        else:
            return self.istranslated() and self.S_UNREVIEWED or self.S_EMPTY

    def set_state_n(self, value):
        self._state_n = value

    def infer_state(self):
        """Empty method that should be overridden in sub-classes to infer the
            current state(_n) of the unit from its current state."""
        pass


class TranslationStore(object):
    """Base class for stores for multiple translation units of type
    UnitClass."""

    UnitClass = TranslationUnit
    """The class of units that will be instantiated and used by this class"""
    Name = "Base translation store"
    """The human usable name of this store type"""
    Mimetypes = None
    """A list of MIME types associated with this store type"""
    Extensions = None
    """A list of file extentions associated with this store type"""
    _binary = False
    """Indicates whether a file should be accessed as a binary file."""
    suggestions_in_format = False
    """Indicates if format can store suggestions and alternative translation
    for a unit"""

    sourcelanguage = None
    targetlanguage = None

    def __init__(self, unitclass=None):
        """Construct a blank TranslationStore."""
        self.units = []
        if unitclass:
            self.UnitClass = unitclass

    def getsourcelanguage(self):
        """Get the source language for this store."""
        return self.sourcelanguage

    def setsourcelanguage(self, sourcelanguage):
        """Set the source language for this store."""
        self.sourcelanguage = sourcelanguage

    def gettargetlanguage(self):
        """Get the target language for this store."""
        return self.targetlanguage

    def settargetlanguage(self, targetlanguage):
        """Set the target language for this store."""
        self.targetlanguage = targetlanguage

    def getprojectstyle(self):
        """Get the project type for this store."""
        return getattr(self, '_project_style', None)

    def setprojectstyle(self, project_style):
        """Set the project type for this store."""
        self._project_style = project_style

    def unit_iter(self):
        """Iterator over all the units in this store."""
        for unit in self.units:
            yield unit

    def getunits(self):
        """Return a list of all units in this store."""
        return [unit for unit in self.unit_iter()]

    def addunit(self, unit):
        """Append the given unit to the object's list of units.

        This method should always be used rather than trying to modify the
        list manually.

        :type unit: :class:`TranslationUnit`
        :param unit: The unit that will be added.
        """
        unit._store = self
        self.units.append(unit)

    def addsourceunit(self, source):
        """Add and returns a new unit with the given source string.

        :rtype: :class:`TranslationUnit`
        """
        unit = self.UnitClass(source)
        self.addunit(unit)
        return unit

    def findid(self, id):
        """find unit with matching id by checking id_index"""
        self.require_index()
        return self.id_index.get(id, None)

    def findunit(self, source):
        """Find the unit with the given source string.

        :rtype: :class:`TranslationUnit` or None
        """
        if len(getattr(self, "sourceindex", [])):
            if source in self.sourceindex:
                return self.sourceindex[source][0]
        else:
            for unit in self.units:
                if unit.source == source:
                    return unit
        return None

    def findunits(self, source):
        """Find the units with the given source string.

        :rtype: :class:`TranslationUnit` or None
        """
        if len(getattr(self, "sourceindex", [])):
            if source in self.sourceindex:
                return self.sourceindex[source]
        else:
            #FIXME: maybe we should generate index here instead since
            #we'll scan all units anyway
            result = []
            for unit in self.units:
                if unit.source == source:
                    result.append(unit)
            return result
        return None

    def translate(self, source):
        """Return the translated string for a given source string.

        :rtype: String or None
        """
        unit = self.findunit(source)
        if unit and unit.target:
            return unit.target
        else:
            return None

    def remove_unit_from_index(self, unit):
        """Remove a unit from source and locaton indexes"""

        def remove_unit(source):
            if source in self.sourceindex:
                try:
                    self.sourceindex[source].remove(unit)
                    if len(self.sourceindex[source]) == 0:
                        del(self.sourceindex[source])
                except ValueError:
                    pass

        if unit.hasplural():
            for source in unit.source.strings:
                remove_unit(source)
        else:
            remove_unit(unit.source)

        for location in unit.getlocations():
            if location in self.locationindex \
                   and self.locationindex[location] is not None \
                   and self.locationindex[location] == unit:
                del(self.locationindex[location])

    def add_unit_to_index(self, unit):
        """Add a unit to source and location idexes"""
        self.id_index[unit.getid()] = unit

        def insert_unit(source):
            if not source in self.sourceindex:
                self.sourceindex[source] = [unit]
            else:
                self.sourceindex[source].append(unit)

        if unit.hasplural():
            for source in unit.source.strings:
                insert_unit(source)
        else:
            insert_unit(unit.source)

        for location in unit.getlocations():
            if location in self.locationindex:
                # if sources aren't unique, don't use them
                #FIXME: maybe better store a list of units like sourceindex
                self.locationindex[location] = None
            else:
                self.locationindex[location] = unit

    def makeindex(self):
        """Indexes the items in this store. At least .sourceindex should be
        useful."""
        self.locationindex = {}
        self.sourceindex = {}
        self.id_index = {}
        for index, unit in enumerate(self.units):
            unit.index = index
            if not (unit.isheader() or unit.isblank()):
                self.add_unit_to_index(unit)

    def require_index(self):
        """make sure source index exists"""
        if not hasattr(self, "id_index"):
            self.makeindex()

    def getids(self, filename=None):
        """return a list of unit ids"""
        self.require_index()
        return self.id_index.keys()

    def __getstate__(self):
        odict = self.__dict__.copy()
        odict['fileobj'] = None
        return odict

    def __setstate__(self, dict):
        self.__dict__.update(dict)
        if getattr(self, "filename", False):
            self.fileobj = open(self.filename)

    def __str__(self):
        """Converts to a string representation that can be parsed back using
        :meth:`~.TranslationStore.parsestring`."""
        # We can't pickle fileobj if it is there, so let's hide it for a while.
        fileobj = getattr(self, "fileobj", None)
        self.fileobj = None
        dump = pickle.dumps(self)
        self.fileobj = fileobj
        return dump

    def isempty(self):
        """Return True if the object doesn't contain any translation units."""
        if len(self.units) == 0:
            return True
        for unit in self.units:
            if unit.istranslatable():
                return False
        return True

    def _assignname(self):
        """Tries to work out what the name of the filesystem file is and
        assigns it to .filename."""
        fileobj = getattr(self, "fileobj", None)
        if fileobj:
            filename = getattr(fileobj, "name",
                               getattr(fileobj, "filename", None))
            if filename:
                self.filename = filename

    @classmethod
    def parsestring(cls, storestring):
        """Convert the string representation back to an object."""
        newstore = cls()
        if storestring:
            newstore.parse(storestring)
        return newstore

    def detect_encoding(self, text, default_encodings=None):
        if not default_encodings:
            default_encodings = ['utf-8']
        try:
            import chardet
            # many false complaints with ellipse (see bug 1825)
            detected_encoding = chardet.detect(text.replace("", ""))
            if detected_encoding['confidence'] < 0.48:
                detected_encoding = None
            elif detected_encoding['encoding'] == 'ascii':
                detected_encoding['encoding'] = 'utf-8'
        except ImportError:
            detected_encoding = None

        encodings = []
        if self.encoding == 'auto':
            if detected_encoding and detected_encoding['encoding'] not in encodings:
                encodings.append(detected_encoding['encoding'])
            for encoding in default_encodings:
                if encoding not in encodings:
                    encodings.append(encoding)
        else:
            encodings.append(self.encoding)
            if (detected_encoding and
                    detected_encoding['encoding'] != self.encoding and
                    detected_encoding['confidence'] != 1.0):
                logging.warn("trying to parse %s with encoding: %s but "
                             "detected encoding is %s (confidence: %s)",
                             self.filename, self.encoding,
                             detected_encoding['encoding'],
                             detected_encoding['confidence'])
            encodings.append(self.encoding)

        for encoding in encodings:
            try:
                r_text = unicode(text, encoding)
                r_encoding = encoding
                break
            except UnicodeDecodeError:
                r_text = None
                r_encoding = None
        if r_encoding == 'ascii':
            r_encoding = 'utf-8'
        return r_text, r_encoding

    def parse(self, data):
        """parser to process the given source string"""
        self.units = pickle.loads(data).units

    def savefile(self, storefile):
        """Write the string representation to the given file (or filename)."""
        storestring = str(self)
        if isinstance(storefile, basestring):
            mode = 'w'
            if self._binary:
                mode = 'wb'
            storefile = open(storefile, mode)
        self.fileobj = storefile
        self._assignname()
        storefile.write(storestring)
        storefile.close()

    def save(self):
        """Save to the file that data was originally read from, if
        available."""
        fileobj = getattr(self, "fileobj", None)
        mode = 'w'
        if self._binary:
            mode = 'wb'
        if not fileobj:
            filename = getattr(self, "filename", None)
            if filename:
                fileobj = file(filename, mode)
        else:
            fileobj.close()
            filename = getattr(fileobj, "name",
                               getattr(fileobj, "filename", None))
            if not filename:
                raise ValueError("No file or filename to save to")
            fileobj = fileobj.__class__(filename, mode)
        self.savefile(fileobj)

    @classmethod
    def parsefile(cls, storefile):
        """Reads the given file (or opens the given filename) and parses back
        to an object."""
        mode = 'r'
        if cls._binary:
            mode = 'rb'
        if isinstance(storefile, basestring):
            storefile = open(storefile, mode)
        mode = getattr(storefile, "mode", mode)
        #For some reason GzipFile returns 1, so we have to test for that here
        if mode == 1 or "r" in mode:
            storestring = storefile.read()
            storefile.close()
        else:
            storestring = ""
        newstore = cls.parsestring(storestring)
        newstore.fileobj = storefile
        newstore._assignname()
        return newstore

    @property
    def merge_on(self):
        """The matching criterion to use when merging on.

        :return: The default matching criterion for all the subclasses.
        :rtype: string
        """
        return "id"

########NEW FILE########
__FILENAME__ = benchmark
#!/usr/bin/env python
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import argparse
import cProfile
import os
import pstats
import random
import sys

from translate.storage import factory, placeables


class TranslateBenchmarker:
    """class to aid in benchmarking Translate Toolkit stores"""

    def __init__(self, test_dir, storeclass):
        """sets up benchmarking on the test directory"""
        self.test_dir = os.path.abspath(test_dir)
        self.StoreClass = storeclass
        self.extension = self.StoreClass.Extensions[0]
        self.project_dir = os.path.join(self.test_dir, "benchmark")
        self.file_dir = os.path.join(self.project_dir, "zxx")
        self.parsedfiles = []

    def clear_test_dir(self):
        """removes the given directory"""
        if os.path.exists(self.test_dir):
            for dirpath, subdirs, filenames in os.walk(self.test_dir, topdown=False):
                for name in filenames:
                    os.remove(os.path.join(dirpath, name))
                for name in subdirs:
                    os.rmdir(os.path.join(dirpath, name))
        if os.path.exists(self.test_dir):
            os.rmdir(self.test_dir)
        assert not os.path.exists(self.test_dir)

    def create_sample_files(self, num_dirs, files_per_dir, strings_per_file, source_words_per_string, target_words_per_string):
        """creates sample files for benchmarking"""
        if not os.path.exists(self.test_dir):
            os.mkdir(self.test_dir)
        if not os.path.exists(self.project_dir):
            os.mkdir(self.project_dir)
        if not os.path.exists(self.file_dir):
            os.mkdir(self.file_dir)
        for dirnum in range(num_dirs):
            if num_dirs > 1:
                dirname = os.path.join(self.file_dir, "sample_%d" % dirnum)
                if not os.path.exists(dirname):
                    os.mkdir(dirname)
            else:
                dirname = self.file_dir
            for filenum in range(files_per_dir):
                sample_file = self.StoreClass()
                for stringnum in range(strings_per_file):
                    source_string = " ".join(["word%d" % (random.randint(0, strings_per_file) * i) for i in range(source_words_per_string)])
                    sample_unit = sample_file.addsourceunit(source_string)
                    sample_unit.target = " ".join(["drow%d" % (random.randint(0, strings_per_file) * i) for i in range(target_words_per_string)])
                sample_file.savefile(os.path.join(dirname, "file_%d.%s" % (filenum, self.extension)))

    def parse_files(self, file_dir=None):
        """parses all the files in the test directory into memory"""
        count = 0
        self.parsedfiles = []
        if file_dir is None:
            file_dir = self.file_dir
        for dirpath, subdirs, filenames in os.walk(file_dir, topdown=False):
            for name in filenames:
                pofilename = os.path.join(dirpath, name)
                parsedfile = self.StoreClass(open(pofilename, 'r'))
                count += len(parsedfile.units)
                self.parsedfiles.append(parsedfile)
        print("counted %d units" % count)

    def parse_placeables(self):
        """parses placeables"""
        count = 0
        for parsedfile in self.parsedfiles:
            for unit in parsedfile.units:
                placeables.parse(unit.source, placeables.general.parsers)
                placeables.parse(unit.target, placeables.general.parsers)
            count += len(parsedfile.units)
        print("counted %d units" % count)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('podir', metavar='DIR', type=str, nargs='?',
                        help='PO dir to use (default: create sample files)')
    parser.add_argument('--store-type', dest='storetype',
                        action='store_const', const='po', default="po",
                        help='type of the store to benchmark (default: po)')
    parser.add_argument('--check-parsing', dest='check_parsing',
                        action='store_true',
                        help='benchmark parsing files')
    parser.add_argument('--check-placeables', dest='check_placeables',
                        action='store_true',
                        help='benchmark placeables')
    args = parser.parse_args()

    storetype = args.storetype

    if storetype in factory.classes_str:
        _module, _class = factory.classes_str[storetype]
        module = __import__("translate.storage.%s" % _module,
                            globals(), fromlist=_module)
        storeclass = getattr(module, _class)
    else:
        print("StoreClass: '%s' is not a base class that the class factory can load" % storetype)
        sys.exit()

    sample_files = [
      # num_dirs, files_per_dir, strings_per_file, source_words_per_string, target_words_per_string
      # (1, 1, 2, 2, 2),
      (1, 1, 10000, 5, 10),   # Creat 1 very large file with German like ratios or source to target
      # (100, 10, 10, 5, 10),   # Create lots of directories and files with smaller then avarage size
      # (1, 5, 10, 10, 10),
      # (1, 10, 10, 10, 10),
      # (5, 10, 10, 10, 10),
      # (5, 10, 100, 20, 20),
      # (10, 20, 100, 10, 10),
      # (10, 20, 100, 10, 10),
      # (100, 2, 140, 3, 3),  # OpenOffice.org approximate ratios
    ]

    for sample_file_sizes in sample_files:
        benchmarker = TranslateBenchmarker("BenchmarkDir", storeclass)
        benchmarker.clear_test_dir()
        if args.podir is None:
            benchmarker.create_sample_files(*sample_file_sizes)
        benchmarker.parse_files(file_dir=args.podir)
        methods = []  # [("create_sample_files", "*sample_file_sizes")]

        if args.check_parsing:
            methods.append(("parse_files", ""))

        if args.check_placeables:
            methods.append(("parse_placeables", ""))

        for methodname, methodparam in methods:
            #print methodname, "%d dirs, %d files, %d strings, %d/%d words" % sample_file_sizes
            print("_______________________________________________________")
            statsfile = "%s_%s" % (methodname, storetype) + '_%d_%d_%d_%d_%d.stats' % sample_file_sizes
            cProfile.run('benchmarker.%s(%s)' % (methodname, methodparam), statsfile)
            stats = pstats.Stats(statsfile)
            stats.sort_stats('time').print_stats(20)
            print("_______________________________________________________")
        benchmarker.clear_test_dir()

########NEW FILE########
__FILENAME__ = bundleprojstore
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os
import shutil
import tempfile
from zipfile import ZipFile

from translate.storage.projstore import *


__all__ = ['BundleProjectStore', 'InvalidBundleError']


class InvalidBundleError(Exception):
    pass


class BundleProjectStore(ProjectStore):
    """Represents a translate project bundle (zip archive)."""

    # INITIALIZERS #
    def __init__(self, fname):
        super(BundleProjectStore, self).__init__()
        self._tempfiles = {}
        if fname and os.path.isfile(fname):
            self.load(fname)
        else:
            self.zip = ZipFile(fname, 'w')
            self.save()
            self.zip.close()
            self.zip = ZipFile(fname, 'a')

    # CLASS METHODS #
    @classmethod
    def from_project(cls, proj, fname=None):
        if fname is None:
            fname = 'bundle.zip'

        bundle = BundleProjectStore(fname)
        for fn in proj.sourcefiles:
            bundle.append_sourcefile(proj.get_file(fn))
        for fn in proj.transfiles:
            bundle.append_transfile(proj.get_file(fn))
        for fn in proj.targetfiles:
            bundle.append_targetfile(proj.get_file(fn))
        bundle.settings = proj.settings.copy()
        bundle.save()
        return bundle

    # METHODS #
    def append_file(self, afile, fname, ftype='trans', delete_orig=False):
        """Append the given file to the project with the given filename, marked
            to be of type ``ftype`` ('src', 'trans', 'tgt').

            :param delete_orig: If ``True``, as set by
                                :meth:`~translate.storage.Project.convert_forward`,
                                ``afile`` is deleted after appending, if
                                possible.

            .. note:: For this implementation, the appended file will be deleted
                      from disk if ``delete_orig`` is ``True``.
            """
        if fname and fname in self.zip.namelist():
            raise ValueError("File already in bundle archive: %s" % (fname))
        if not fname and isinstance(afile, basestring) and afile in self.zip.namelist():
            raise ValueError("File already in bundle archive: %s" % (afile))

        afile, fname = super(BundleProjectStore, self).append_file(afile, fname, ftype)
        self._zip_add(fname, afile)

        if delete_orig and hasattr(afile, 'name') and afile.name not in self._tempfiles:
            try:
                os.unlink(afile.name)
            except Exception:
                pass

        return self.get_file(fname), fname

    def remove_file(self, fname, ftype=None):
        """Remove the file with the given project name from the project."""
        super(BundleProjectStore, self).remove_file(fname, ftype)
        self._zip_delete([fname])
        tempfiles = [tmpf for tmpf, prjf in self._tempfiles.iteritems() if prjf == fname]
        if tempfiles:
            for tmpf in tempfiles:
                try:
                    os.unlink(tmpf)
                except Exception:
                    pass
                del self._tempfiles[tmpf]

    def close(self):
        super(BundleProjectStore, self).close()
        self.cleanup()
        self.zip.close()

    def cleanup(self):
        """Clean up our mess: remove temporary files."""
        for tempfname in self._tempfiles:
            if os.path.isfile(tempfname):
                os.unlink(tempfname)
        self._tempfiles = {}

    def get_file(self, fname):
        """Retrieve a project file (source, translation or target file) from the
            project archive."""
        retfile = None
        if fname in self._files or fname in self.zip.namelist():
            # Check if the file has not already been extracted to a temp file
            tempfname = [tfn for tfn in self._tempfiles if self._tempfiles[tfn] == fname]
            if tempfname and os.path.isfile(tempfname[0]):
                tempfname = tempfname[0]
            else:
                tempfname = ''
            if not tempfname:
                # Extract the file to a temporary file
                zfile = self.zip.open(fname)
                tempfname = os.path.split(fname)[-1]
                tempfd, tempfname = tempfile.mkstemp(suffix='_' + tempfname)
                os.close(tempfd)
                open(tempfname, 'w').write(zfile.read())
            retfile = open(tempfname)
            self._tempfiles[tempfname] = fname

        if not retfile:
            raise FileNotInProjectError(fname)
        return retfile

    def get_proj_filename(self, realfname):
        """Try and find a project file name for the given real file name."""
        try:
            fname = super(BundleProjectStore, self).get_proj_filename(realfname)
        except ValueError as ve:
            fname = None
        if fname:
            return fname
        if realfname in self._tempfiles:
            return self._tempfiles[realfname]
        raise ValueError('Real file not in project store: %s' % (realfname))

    def load(self, zipname):
        """Load the bundle project from the zip file of the given name."""
        self.zip = ZipFile(zipname, mode='a')
        self._load_settings()

        append_section = {
            'sources': self._sourcefiles.append,
            'targets': self._targetfiles.append,
            'transfiles': self._transfiles.append,
        }
        for section in ('sources', 'targets', 'transfiles'):
            if section in self.settings:
                for fname in self.settings[section]:
                    append_section[section](fname)
                    self._files[fname] = None

    def save(self, filename=None):
        """Save all project files to the bundle zip file."""
        self._update_from_tempfiles()

        if filename:
            newzip = ZipFile(filename, 'w')
        else:
            newzip = self._create_temp_zipfile()

        # Write project file for the new zip bundle
        newzip.writestr('project.xtp', self._generate_settings())
        # Copy project files from project to the new zip file
        project_files = self._sourcefiles + self._transfiles + self._targetfiles
        for fname in project_files:
            newzip.writestr(fname, self.get_file(fname).read())
        # Copy any extra (non-project) files from the current zip
        for fname in self.zip.namelist():
            if fname in project_files or fname == 'project.xtp':
                continue
            newzip.writestr(fname, self.zip.read(fname))

        self._replace_project_zip(newzip)

    def update_file(self, pfname, infile):
        """Updates the file with the given project file name with the contents
            of ``infile``.

            :returns: the results from :meth:`BundleProjStore.append_file`."""
        if pfname not in self._files:
            raise FileNotInProjectError(pfname)

        if pfname not in self.zip.namelist():
            return super(BundleProjectStore, self).update_file(pfname, infile)

        self._zip_delete([pfname])
        self._zip_add(pfname, infile)

    def _load_settings(self):
        """Grab the project.xtp file from the zip file and load it."""
        if 'project.xtp' not in self.zip.namelist():
            raise InvalidBundleError('Not a translate project bundle')
        super(BundleProjectStore, self)._load_settings(self.zip.open('project.xtp').read())

    def _create_temp_zipfile(self):
        """Create a new zip file with a temporary file name (with mode 'w')."""
        newzipfd, newzipfname = tempfile.mkstemp(prefix='translate_bundle', suffix='.zip')
        os.close(newzipfd)
        return ZipFile(newzipfname, 'w')

    def _replace_project_zip(self, zfile):
        """Replace the currently used zip file (``self.zip``) with the given zip
            file. Basically, ``os.rename(zfile.filename, self.zip.filename)``."""
        if not zfile.fp.closed:
            zfile.close()
        if not self.zip.fp.closed:
            self.zip.close()
        shutil.move(zfile.filename, self.zip.filename)
        self.zip = ZipFile(self.zip.filename, mode='a')

    def _update_from_tempfiles(self):
        """Update project files from temporary files."""
        for tempfname in self._tempfiles:
            tmp = open(tempfname)
            self.update_file(self._tempfiles[tempfname], tmp)
            if not tmp.closed:
                tmp.close()

    def _zip_add(self, pfname, infile):
        """Add the contents of ``infile`` to the zip with file name ``pfname``."""
        if hasattr(infile, 'seek'):
            infile.seek(0)
        self.zip.writestr(pfname, infile.read())
        # Clear the cached file object to force the file to be read from the
        # zip file.
        self._files[pfname] = None

    def _zip_delete(self, fnames):
        """Delete the files with the given names from the zip file (``self.zip``)."""
        # Sanity checking
        if not isinstance(fnames, (list, tuple)):
            raise ValueError("fnames must be list or tuple: %s" % (fnames))
        if not self.zip:
            raise ValueError("No zip file to work on")
        zippedfiles = self.zip.namelist()
        for fn in fnames:
            if fn not in zippedfiles:
                raise KeyError("File not in zip archive: %s" % (fn))

        newzip = self._create_temp_zipfile()
        newzip.writestr('project.xtp', self._generate_settings())

        for fname in zippedfiles:
            # Copy all files from self.zip that are not project.xtp (already
            # in the new zip file) or in fnames (they are to be removed, after
            # all.
            if fname in fnames or fname == 'project.xtp':
                continue
            newzip.writestr(fname, self.zip.read(fname))

        self._replace_project_zip(newzip)

########NEW FILE########
__FILENAME__ = catkeys
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Manage the Haiku catkeys translation format

The Haiku catkeys format is the translation format used for localisation of
the `Haiku <http://www.haiku-os.org/>`_ operating system.

It is a bilingual base class derived format with :class:`CatkeysFile` and
:class:`CatkeysUnit` providing file and unit level access.  The file format is
described here:
http://www.haiku-os.org/blog/pulkomandy/2009-09-24_haiku_locale_kit_translator_handbook

Implementation
    The implementation covers the full requirements of a catkeys file. The
    files are simple Tab Separated Value (TSV) files that can be read
    by Microsoft Excel and other spreadsheet programs. They use the .txt
    extension which does make it more difficult to automatically identify
    such files.

    The dialect of the TSV files is specified by :class:`CatkeysDialect`.

Encoding
    The files are UTF-8 encoded.

Header
    :class:`CatkeysHeader` provides header management support.

Escaping
    catkeys seem to escape things like in C++ (strings are just extracted from
    the source code unchanged, it seems.

    Functions allow for :func:`._escape` and :func:`._unescape`.
"""

import csv

from translate.lang import data
from translate.storage import base


FIELDNAMES_HEADER = ["version", "language", "mimetype", "checksum"]
"""Field names for the catkeys header"""

FIELDNAMES = ["source", "context", "comment", "target"]
"""Field names for a catkeys TU"""

FIELDNAMES_HEADER_DEFAULTS = {
    "version": "1",
    "language": "",
    "mimetype": "",
    "checksum": "",
}
"""Default or minimum header entries for a catkeys file"""

_unescape_map = {"\\r": "\r", "\\t": "\t", '\\n': '\n', '\\\\': '\\'}
_escape_map = dict([(value, key) for (key, value) in _unescape_map.items()])
# We don't yet do escaping correctly, just for lack of time to do it.  The
# current implementation is just based on something simple that will work with
# investaged files.  The only escapes found were "\n", "\t", "\\n"


def _escape(string):
    if string:
        string = string.replace(r"\n", r"\\n").replace("\n", "\\n").replace("\t", "\\t")
    return string


def _unescape(string):
    if string:
        string = string.replace("\\n", "\n").replace("\\t", "\t").replace(r"\n", r"\\n")
    return string


class CatkeysDialect(csv.Dialect):
    """Describe the properties of a catkeys generated TAB-delimited file."""
    delimiter = "\t"
    lineterminator = "\n"
    quoting = csv.QUOTE_NONE
csv.register_dialect("catkeys", CatkeysDialect)


class CatkeysHeader(object):
    """A catkeys translation memory header"""

    def __init__(self, header=None):
        self._header_dict = {}
        if not header:
            self._header_dict = self._create_default_header()
        elif isinstance(header, dict):
            self._header_dict = header

    def _create_default_header(self):
        """Create a default catkeys header"""
        defaultheader = FIELDNAMES_HEADER_DEFAULTS.copy()
        return defaultheader

    def settargetlanguage(self, newlang):
        """Set a human readable target language"""
        if not newlang or newlang not in data.languages:
            return
        #XXX assumption about the current structure of the languages dict in data
        self._header_dict['language'] = data.languages[newlang][0].lower()
    targetlanguage = property(None, settargetlanguage)


class CatkeysUnit(base.TranslationUnit):
    """A catkeys translation memory unit"""

    def __init__(self, source=None):
        self._dict = {}
        if source:
            self.source = source
        super(CatkeysUnit, self).__init__(source)

    def getdict(self):
        """Get the dictionary of values for a catkeys line"""
        return self._dict

    def setdict(self, newdict):
        """Set the dictionary of values for a catkeys line

        :param newdict: a new dictionary with catkeys line elements
        :type newdict: Dict
        """
        # TODO First check that the values are OK
        self._dict = newdict
    dict = property(getdict, setdict)

    def _get_source_or_target(self, key):
        if self._dict.get(key, None) is None:
            return None
        elif self._dict[key]:
            return _unescape(self._dict[key]).decode('utf-8')
        else:
            return ""

    def _set_source_or_target(self, key, newvalue):
        if newvalue is None:
            self._dict[key] = None
        if isinstance(newvalue, unicode):
            newvalue = newvalue.encode('utf-8')
        newvalue = _escape(newvalue)
        if not key in self._dict or newvalue != self._dict[key]:
            self._dict[key] = newvalue

    def getsource(self):
        return self._get_source_or_target('source')

    def setsource(self, newsource):
        self._rich_source = None
        return self._set_source_or_target('source', newsource)
    source = property(getsource, setsource)

    def gettarget(self):
        return self._get_source_or_target('target')

    def settarget(self, newtarget):
        self._rich_target = None
        return self._set_source_or_target('target', newtarget)
    target = property(gettarget, settarget)

    def getnotes(self, origin=None):
        if not origin or origin in ["programmer", "developer", "source code"]:
            return self._dict["comment"].decode('utf-8')
        return u""

    def getcontext(self):
        return self._dict["context"].decode('utf-8')

    def getid(self):
        context = self.getcontext()
        notes = self.getnotes()
        id = self.source
        if notes:
            id = u"%s\04%s" % (notes, id)
        if context:
            id = u"%s\04%s" % (context, id)
        return id

    def markfuzzy(self, present=True):
        if present:
            self.target = u""

    def settargetlang(self, newlang):
        self._dict['target-lang'] = newlang
    targetlang = property(None, settargetlang)

    def __str__(self):
        return str(self._dict)

    def istranslated(self):
        if not self._dict.get('source', None):
            return False
        return bool(self._dict.get('target', None))

    def merge(self, otherunit, overwrite=False, comments=True,
              authoritative=False):
        """Do basic format agnostic merging."""
        # We can't go fuzzy, so just do nothing
        if self.source != otherunit.source or self.getcontext() != otherunit.getcontext() or otherunit.isfuzzy():
            return
        if not self.istranslated() or overwrite:
            self.rich_target = otherunit.rich_target


class CatkeysFile(base.TranslationStore):
    """A catkeys translation memory file"""
    Name = "Haiku catkeys file"
    Mimetypes = ["application/x-catkeys"]
    Extensions = ["catkeys"]

    def __init__(self, inputfile=None, unitclass=CatkeysUnit):
        """Construct a catkeys store, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self.header = CatkeysHeader()
        self._encoding = 'utf-8'
        if inputfile is not None:
            self.parse(inputfile)

    def settargetlanguage(self, newlang):
        self.header.settargetlanguage(newlang)

    def parse(self, input):
        """parsse the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            tmsrc = input.read()
            input.close()
            input = tmsrc
        for header in csv.DictReader(input.split("\n")[:1], fieldnames=FIELDNAMES_HEADER, dialect="catkeys"):
            self.header = CatkeysHeader(header)
        lines = csv.DictReader(input.split("\n")[1:], fieldnames=FIELDNAMES, dialect="catkeys")
        for line in lines:
            newunit = CatkeysUnit()
            newunit.dict = line
            self.addunit(newunit)

    def __str__(self):
        output = csv.StringIO()
        writer = csv.DictWriter(output, fieldnames=FIELDNAMES_HEADER, dialect="catkeys")
        writer.writerow(self.header._header_dict)
        writer = csv.DictWriter(output, fieldnames=FIELDNAMES, dialect="catkeys")
        for unit in self.units:
            writer.writerow(unit.dict)
        return output.getvalue()

########NEW FILE########
__FILENAME__ = cpo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of .po files (pounit) or entire files (pofile).

Gettext-style .po (or .pot) files are used in translations for KDE, GNOME and
many other projects.

This uses libgettextpo from the gettext package. Any version before 0.17 will
at least cause some subtle bugs or may not work at all. Developers might want
to have a look at gettext-tools/libgettextpo/gettext-po.h from the gettext
package for the public API of the library.
"""

import ctypes.util
import logging
import os
import re
import sys
import tempfile
from ctypes import (CFUNCTYPE, POINTER, Structure, c_char_p, c_int, c_long,
                    c_size_t, c_uint, cdll)

from translate.lang import data
from translate.misc.multistring import multistring
from translate.storage import base, pocommon, pypo
from translate.storage.pocommon import encodingToUse


logger = logging.getLogger(__name__)

lsep = " "
"""Separator for #: entries"""

STRING = c_char_p


# Structures
class po_message(Structure):
    _fields_ = []

# Function prototypes
xerror_prototype = CFUNCTYPE(None, c_int, POINTER(po_message), STRING, c_uint,
                             c_uint, c_int, STRING)
xerror2_prototype = CFUNCTYPE(None, c_int, POINTER(po_message), STRING,
                              c_uint, c_uint, c_int, STRING,
                              POINTER(po_message), STRING, c_uint, c_uint,
                              c_int, STRING)


# Structures (error handler)
class po_xerror_handler(Structure):
    _fields_ = [('xerror', xerror_prototype),
                ('xerror2', xerror2_prototype)]


class po_error_handler(Structure):
    _fields_ = [
        ('error', CFUNCTYPE(None, c_int, c_int, STRING)),
        ('error_at_line', CFUNCTYPE(None, c_int, c_int, STRING, c_uint, STRING)),
        ('multiline_warning', CFUNCTYPE(None, STRING, STRING)),
        ('multiline_error', CFUNCTYPE(None, STRING, STRING)),
    ]


# Callback functions for po_xerror_handler
def xerror_cb(severity, message, filename, lineno, column, multiline_p,
              message_text):
    logger.error("xerror_cb" + severity + message +
                 filename + lineno + column + multiline_p + message_text)
    if severity >= 1:
        raise ValueError(message_text)


def xerror2_cb(severity, message1, filename1, lineno1, column1, multiline_p1,
               message_text1, message2, filename2, lineno2, column2,
               multiline_p2, message_text2):
    logger.error("xerror2_cb" + severity +
                 message1 +
                 filename1 + lineno1 + column1 + multiline_p1 + message_text1,
                 filename2 + lineno2 + column2 + multiline_p2 + message_text2)
    if severity >= 1:
        raise ValueError(message_text1)


# Setup return and parameter types
def setup_call_types(gpo):
    # File access
    gpo.po_file_read_v3.argtypes = [STRING, POINTER(po_xerror_handler)]
    gpo.po_file_write_v2.argtypes = [c_int, STRING, POINTER(po_xerror_handler)]
    gpo.po_file_write_v2.retype = c_int

    # Header
    gpo.po_file_domain_header.restype = STRING
    gpo.po_header_field.restype = STRING
    gpo.po_header_field.argtypes = [STRING, STRING]

    # Locations (filepos)
    gpo.po_filepos_file.restype = STRING
    gpo.po_message_filepos.restype = c_int
    gpo.po_message_filepos.argtypes = [c_int, c_int]
    gpo.po_message_add_filepos.argtypes = [c_int, STRING, c_size_t]

    # Message (get methods)
    gpo.po_message_comments.restype = STRING
    gpo.po_message_extracted_comments.restype = STRING
    gpo.po_message_prev_msgctxt.restype = STRING
    gpo.po_message_prev_msgid.restype = STRING
    gpo.po_message_prev_msgid_plural.restype = STRING
    gpo.po_message_is_format.restype = c_int
    gpo.po_message_is_format.argtypes = [c_int, STRING]
    gpo.po_message_set_format.argtypes = [c_int, STRING, c_int]
    gpo.po_message_msgctxt.restype = STRING
    gpo.po_message_msgid.restype = STRING
    gpo.po_message_msgid_plural.restype = STRING
    gpo.po_message_msgstr.restype = STRING
    gpo.po_message_msgstr_plural.restype = STRING

    # Message (set methods)
    gpo.po_message_set_comments.argtypes = [c_int, STRING]
    gpo.po_message_set_extracted_comments.argtypes = [c_int, STRING]
    gpo.po_message_set_fuzzy.argtypes = [c_int, c_int]
    gpo.po_message_set_msgctxt.argtypes = [c_int, STRING]

# Load libgettextpo
gpo = None
# 'gettextpo' is recognised on Unix, while only 'libgettextpo' is recognised on
# windows. Therefore we test both.
names = ['gettextpo', 'libgettextpo']
for name in names:
    lib_location = ctypes.util.find_library(name)
    if lib_location:
        gpo = cdll.LoadLibrary(lib_location)
        if gpo:
            break
else:
    # Don't raise exception in Sphinx autodoc [where xml is Mock()ed]. There is
    # nothing special about use of xml here - any of the Mock classes set up
    # in docs/conf.py would work as well, but xml is likely always to be there.
    gpo = None
    if 'xml' not in sys.modules or sys.modules['xml'].__path__ != '/dev/null':

        # Now we are getting desperate, so let's guess a unix type DLL that
        # might be in LD_LIBRARY_PATH or loaded with LD_PRELOAD
        try:
            gpo = cdll.LoadLibrary('libgettextpo.so')
        except OSError:
            raise ImportError("gettext PO library not found")

if gpo:
    setup_call_types(gpo)

# Setup the po_xerror_handler
xerror_handler = po_xerror_handler()
xerror_handler.xerror = xerror_prototype(xerror_cb)
xerror_handler.xerror2 = xerror2_prototype(xerror2_cb)


def escapeforpo(text):
    return pypo.escapeforpo(text)


def quoteforpo(text):
    return pypo.quoteforpo(text)


def unquotefrompo(postr):
    return pypo.unquotefrompo(postr)


def get_libgettextpo_version():
    """Returns the libgettextpo version

       :rtype: three-value tuple
       :return: libgettextpo version in the following format::
           (major version, minor version, subminor version)
    """
    libversion = c_long.in_dll(gpo, 'libgettextpo_version')
    major = libversion.value >> 16
    minor = (libversion.value >> 8) & 0xff
    subminor = libversion.value - (major << 16) - (minor << 8)
    return major, minor, subminor


class pounit(pocommon.pounit):

    #: fixed encoding that is always used for cPO structure (self._gpo_message)
    CPO_ENC = 'utf-8'

    def __init__(self, source=None, encoding='utf-8', gpo_message=None):
        self._rich_source = None
        self._rich_target = None
        encoding = encoding or 'utf-8'
        if not gpo_message:
            self._gpo_message = gpo.po_message_create()
        if source or source == "":
            self.source = source
            self.target = ""
        elif gpo_message:
            if encoding.lower() != self.CPO_ENC:
                features = ['msgctxt', 'msgid', 'msgid_plural']
                features += ['prev_' + x for x in features]
                features += ['comments', 'extracted_comments',
                             'msgstr', 'msgstr_plural']
                for feature in features:
                    text = getattr(gpo, 'po_message_' + feature)(gpo_message)
                    if text:
                        getattr(gpo, 'po_message_set_' + feature)(
                            gpo_message,
                            text.decode(encoding).encode(self.CPO_ENC))
            self._gpo_message = gpo_message
        self.infer_state()

    def infer_state(self):
        #FIXME: do obsolete
        if gpo.po_message_is_obsolete(self._gpo_message):
            if gpo.po_message_is_fuzzy(self._gpo_message):
                self.set_state_n(self.STATE[self.S_FUZZY_OBSOLETE][0])
            else:
                self.set_state_n(self.STATE[self.S_OBSOLETE][0])
        elif gpo.po_message_is_fuzzy(self._gpo_message):
            self.set_state_n(self.STATE[self.S_FUZZY][0])
        elif self.gettarget():
            self.set_state_n(self.STATE[self.S_TRANSLATED][0])
        else:
            self.set_state_n(self.STATE[self.S_UNTRANSLATED][0])

    def setmsgid_plural(self, msgid_plural):
        if isinstance(msgid_plural, list):
            msgid_plural = "".join(msgid_plural)
        gpo.po_message_set_msgid_plural(self._gpo_message, msgid_plural)
    msgid_plural = property(None, setmsgid_plural)

    def getsource(self):

        def remove_msgid_comments(text):
            if not text:
                return text
            if text.startswith("_:"):
                remainder = re.search(r"_: .*\n(.*)", text)
                if remainder:
                    return remainder.group(1)
                else:
                    return u""
            else:
                return text
        singular = remove_msgid_comments((gpo.po_message_msgid(self._gpo_message) or "").decode(self.CPO_ENC))
        if singular:
            if self.hasplural():
                multi = multistring(singular, self.CPO_ENC)
                pluralform = (gpo.po_message_msgid_plural(self._gpo_message) or "").decode(self.CPO_ENC)
                multi.strings.append(pluralform)
                return multi
            else:
                return singular
        else:
            return u""

    def setsource(self, source):
        if isinstance(source, multistring):
            source = source.strings
        if isinstance(source, unicode):
            source = source.encode(self.CPO_ENC)
        if isinstance(source, list):
            gpo.po_message_set_msgid(self._gpo_message, source[0].encode(self.CPO_ENC))
            if len(source) > 1:
                gpo.po_message_set_msgid_plural(self._gpo_message, source[1].encode(self.CPO_ENC))
        else:
            gpo.po_message_set_msgid(self._gpo_message, source)
            gpo.po_message_set_msgid_plural(self._gpo_message, None)
    source = property(getsource, setsource)

    def gettarget(self):
        if self.hasplural():
            plurals = []
            nplural = 0
            plural = gpo.po_message_msgstr_plural(self._gpo_message, nplural)
            while plural:
                plurals.append(plural.decode(self.CPO_ENC))
                nplural += 1
                plural = gpo.po_message_msgstr_plural(self._gpo_message, nplural)
            if plurals:
                multi = multistring(plurals, encoding=self.CPO_ENC)
            else:
                multi = multistring(u"")
        else:
            multi = (gpo.po_message_msgstr(self._gpo_message) or "").decode(self.CPO_ENC)
        return multi

    def settarget(self, target):
        # for plural strings: convert 'target' into a list
        if self.hasplural():
            if isinstance(target, multistring):
                target = target.strings
            elif isinstance(target, basestring):
                target = [target]
        # for non-plurals: check number of items in 'target'
        elif isinstance(target, (dict, list)):
            if len(target) == 1:
                target = target[0]
            else:
                raise ValueError("po msgid element has no plural but msgstr has %d elements (%s)" % (len(target), target))
        # empty the previous list of messages
        # TODO: the "pypo" implementation does not remove the previous items of
        #   the target, if self.target == target (essentially: comparing only
        #   the first item of a plural string with the single new string)
        #   Maybe this behaviour should be unified.
        if isinstance(target, (dict, list)):
            i = 0
            message = gpo.po_message_msgstr_plural(self._gpo_message, i)
            while message is not None:
                gpo.po_message_set_msgstr_plural(self._gpo_message, i, None)
                i += 1
                message = gpo.po_message_msgstr_plural(self._gpo_message, i)
        # add the items of a list
        if isinstance(target, list):
            for i in range(len(target)):
                targetstring = target[i]
                if isinstance(targetstring, unicode):
                    targetstring = targetstring.encode(self.CPO_ENC)
                gpo.po_message_set_msgstr_plural(self._gpo_message, i, targetstring)
        # add the values of a dict
        elif isinstance(target, dict):
            for i, targetstring in enumerate(target.itervalues()):
                gpo.po_message_set_msgstr_plural(self._gpo_message, i, targetstring)
        # add a single string
        else:
            if isinstance(target, unicode):
                target = target.encode(self.CPO_ENC)
            if target is None:
                gpo.po_message_set_msgstr(self._gpo_message, "")
            else:
                gpo.po_message_set_msgstr(self._gpo_message, target)
    target = property(gettarget, settarget)

    def getid(self):
        """The unique identifier for this unit according to the conventions in
        .mo files."""
        id = (gpo.po_message_msgid(self._gpo_message) or "").decode(self.CPO_ENC)
        # Gettext does not consider the plural to determine duplicates, only
        # the msgid. For generation of .mo files, we might want to use this
        # code to generate the entry for the hash table, but for now, it is
        # commented out for conformance to gettext.
#        plural = gpo.po_message_msgid_plural(self._gpo_message)
#        if not plural is None:
#            id = '%s\0%s' % (id, plural)
        context = gpo.po_message_msgctxt(self._gpo_message)
        if context:
            id = u"%s\04%s" % (context.decode(self.CPO_ENC), id)
        return id

    def getnotes(self, origin=None):
        if origin is None:
            comments = gpo.po_message_comments(self._gpo_message) + \
                       gpo.po_message_extracted_comments(self._gpo_message)
        elif origin == "translator":
            comments = gpo.po_message_comments(self._gpo_message)
        elif origin in ["programmer", "developer", "source code"]:
            comments = gpo.po_message_extracted_comments(self._gpo_message)
        else:
            raise ValueError("Comment type not valid")

        if comments and get_libgettextpo_version() < (0, 17, 0):
            comments = "\n".join([line for line in comments.split("\n")])
        # Let's drop the last newline
        return comments[:-1].decode(self.CPO_ENC)

    def addnote(self, text, origin=None, position="append"):
        # ignore empty strings and strings without non-space characters
        if not (text and text.strip()):
            return
        text = data.forceunicode(text)
        oldnotes = self.getnotes(origin)
        newnotes = None
        if oldnotes:
            if position == "append":
                newnotes = oldnotes + "\n" + text
            elif position == "merge":
                if oldnotes != text:
                    oldnoteslist = oldnotes.split("\n")
                    for newline in text.split("\n"):
                        newline = newline.rstrip("\r")
                        # avoid duplicate comment lines (this might cause some problems)
                        if newline not in oldnotes or len(newline) < 5:
                            oldnoteslist.append(newline)
                    newnotes = "\n".join(oldnoteslist)
            else:
                newnotes = text + '\n' + oldnotes
        else:
            newnotes = "\n".join([line.rstrip("\r") for line in text.split("\n")])

        if newnotes:
            newlines = []
            needs_space = get_libgettextpo_version() < (0, 17, 0)
            for line in newnotes.split("\n"):
                if line and needs_space:
                    newlines.append(" " + line)
                else:
                    newlines.append(line)
            newnotes = "\n".join(newlines).encode(self.CPO_ENC)
            if origin in ["programmer", "developer", "source code"]:
                gpo.po_message_set_extracted_comments(self._gpo_message, newnotes)
            else:
                gpo.po_message_set_comments(self._gpo_message, newnotes)

    def removenotes(self):
        gpo.po_message_set_comments(self._gpo_message, "")

    def copy(self):
        newpo = self.__class__()
        newpo._gpo_message = self._gpo_message
        return newpo

    def merge(self, otherpo, overwrite=False, comments=True, authoritative=False):
        """Merges the otherpo (with the same msgid) into this one.

        Overwrite non-blank self.msgstr only if overwrite is True
        merge comments only if comments is True
        """

        if not isinstance(otherpo, pounit):
            super(pounit, self).merge(otherpo, overwrite, comments)
            return
        if comments:
            self.addnote(otherpo.getnotes("translator"), origin="translator", position="merge")
            # FIXME mergelists(self.typecomments, otherpo.typecomments)
            if not authoritative:
                # We don't bring across otherpo.automaticcomments as we consider ourself
                # to be the the authority.  Same applies to otherpo.msgidcomments
                self.addnote(otherpo.getnotes("developer"), origin="developer", position="merge")
                self.msgidcomment = otherpo._extract_msgidcomments() or None
                self.addlocations(otherpo.getlocations())
        if not self.istranslated() or overwrite:
            # Remove kde-style comments from the translation (if any).
            if self._extract_msgidcomments(otherpo.target):
                otherpo.target = otherpo.target.replace('_: ' + otherpo._extract_msgidcomments() + '\n', '')
            self.target = otherpo.target
            if self.source != otherpo.source or self.getcontext() != otherpo.getcontext():
                self.markfuzzy()
            else:
                self.markfuzzy(otherpo.isfuzzy())
        elif not otherpo.istranslated():
            if self.source != otherpo.source:
                self.markfuzzy()
        else:
            if self.target != otherpo.target:
                self.markfuzzy()

    def isheader(self):
        #return self.source == u"" and self.target != u""
        # we really want to make sure that there is no msgidcomment or msgctxt
        return self.getid() == "" and len(self.target) > 0

    def isblank(self):
        return len(self.source) == len(self.target) == len(self.getcontext()) == 0

    def hastypecomment(self, typecomment):
        return gpo.po_message_is_format(self._gpo_message, typecomment)

    def settypecomment(self, typecomment, present=True):
        gpo.po_message_set_format(self._gpo_message, typecomment, present)

    def hasmarkedcomment(self, commentmarker):
        commentmarker = "(%s)" % commentmarker
        for comment in self.getnotes("translator").split("\n"):
            if comment.startswith(commentmarker):
                return True
        return False

    def isfuzzy(self):
        return gpo.po_message_is_fuzzy(self._gpo_message)

    def _domarkfuzzy(self, present=True):
        gpo.po_message_set_fuzzy(self._gpo_message, present)

    def makeobsolete(self):
        # FIXME: libgettexpo currently does not reset other data, we probably want to do that
        # but a better solution would be for libgettextpo to output correct data on serialisation
        gpo.po_message_set_obsolete(self._gpo_message, True)
        self.infer_state()

    def resurrect(self):
        gpo.po_message_set_obsolete(self._gpo_message, False)
        self.infer_state()

    def hasplural(self):
        return gpo.po_message_msgid_plural(self._gpo_message) is not None

    def _extract_msgidcomments(self, text=None):
        """Extract KDE style msgid comments from the unit.

        :rtype: String
        :return: Returns the extracted msgidcomments found in this unit's msgid.
        """
        if not text:
            text = (gpo.po_message_msgid(self._gpo_message) or "").decode(self.CPO_ENC)
        if text:
            return pocommon.extract_msgid_comment(text)
        return u""

    def setmsgidcomment(self, msgidcomment):
        if msgidcomment:
            self.source = u"_: %s\n%s" % (msgidcomment, self.source)
    msgidcomment = property(_extract_msgidcomments, setmsgidcomment)

    def __str__(self):
        pf = pofile(noheader=True)
        pf.addunit(self)
        return str(pf)

    def getlocations(self):
        locations = []
        i = 0
        location = gpo.po_message_filepos(self._gpo_message, i)
        while location:
            locname = gpo.po_filepos_file(location).decode(self.CPO_ENC)
            locline = gpo.po_filepos_start_line(location)
            if locline == -1:
                locstring = locname
            else:
                locstring = locname + u":" + unicode(locline)
            locations.append(pocommon.unquote_plus(locstring))
            i += 1
            location = gpo.po_message_filepos(self._gpo_message, i)
        return locations

    def addlocation(self, location):
        if location.find(" ") != -1:
            location = pocommon.quote_plus(location)
        parts = location.split(":")
        if len(parts) == 2 and parts[1].isdigit():
            file = parts[0]
            line = int(parts[1] or "0")
        else:
            file = location
            line = -1
        gpo.po_message_add_filepos(self._gpo_message, file, line)

    def getcontext(self):
        msgctxt = gpo.po_message_msgctxt(self._gpo_message)
        if msgctxt:
            return msgctxt.decode(self.CPO_ENC)
        else:
            msgidcomment = self._extract_msgidcomments()
            return msgidcomment

    def setcontext(self, context):
        context = data.forceunicode(context)
        gpo.po_message_set_msgctxt(self._gpo_message, context.encode(self.CPO_ENC))

    @classmethod
    def buildfromunit(cls, unit, encoding=None):
        """Build a native unit from a foreign unit, preserving as much
        information as possible."""
        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
            return unit.copy()
        elif isinstance(unit, pocommon.pounit):
            newunit = cls(unit.source, encoding)
            newunit.target = unit.target
            #context
            newunit.msgidcomment = unit._extract_msgidcomments()
            context = unit.getcontext()
            if not newunit.msgidcomment and context:
                newunit.setcontext(context)

            locations = unit.getlocations()
            if locations:
                newunit.addlocations(locations)
            notes = unit.getnotes("developer")
            if notes:
                newunit.addnote(notes, "developer")
            notes = unit.getnotes("translator")
            if notes:
                newunit.addnote(notes, "translator")
            if unit.isobsolete():
                newunit.makeobsolete()
            newunit.markfuzzy(unit.isfuzzy())
            for tc in ['python-format', 'c-format', 'php-format']:
                if unit.hastypecomment(tc):
                    newunit.settypecomment(tc)
                    # We assume/guess/hope that there will only be one
                    break
            return newunit
        else:
            return base.TranslationUnit.buildfromunit(unit)


class pofile(pocommon.pofile):
    UnitClass = pounit

    def __init__(self, inputfile=None, encoding=None, unitclass=pounit, noheader=False):
        self._gpo_memory_file = None
        self._gpo_message_iterator = None
        self.units = []
        self.sourcelanguage = None
        self.targetlanguage = None
        self._encoding = 'utf-8'
        if inputfile is None:
            self._gpo_memory_file = gpo.po_file_create()
            self._gpo_message_iterator = gpo.po_message_iterator(self._gpo_memory_file, None)
            if not noheader:
                self.init_headers()
        else:
            super(pofile, self).__init__(inputfile=inputfile, encoding=encoding)

    def addunit(self, unit, new=True):
        if new:
            gpo.po_message_insert(self._gpo_message_iterator, unit._gpo_message)
        super(pofile, self).addunit(unit)

    def _insert_header(self, header):
        header._store = self
        self.units.insert(0, header)
        gpo.po_message_iterator_free(self._gpo_message_iterator)
        self._gpo_message_iterator = gpo.po_message_iterator(self._gpo_memory_file, None)
        gpo.po_message_insert(self._gpo_message_iterator, header._gpo_message)
        while gpo.po_next_message(self._gpo_message_iterator):
            pass

    def removeduplicates(self, duplicatestyle="merge"):
        """make sure each msgid is unique ; merge comments etc from duplicates into original"""
        # TODO: can we handle consecutive calls to removeduplicates()? What
        # about files already containing msgctxt? - test
        id_dict = {}
        uniqueunits = []
        # TODO: this is using a list as the pos aren't hashable, but this is slow.
        # probably not used frequently enough to worry about it, though.
        markedpos = []

        def addcomment(thepo):
            thepo.msgidcomment = " ".join(thepo.getlocations())
            markedpos.append(thepo)
        for thepo in self.units:
            id = thepo.getid()
            if thepo.isheader() and not thepo.getlocations():
                # header msgids shouldn't be merged...
                uniqueunits.append(thepo)
            elif id in id_dict:
                if duplicatestyle == "merge":
                    if id:
                        id_dict[id].merge(thepo)
                    else:
                        addcomment(thepo)
                        uniqueunits.append(thepo)
                elif duplicatestyle == "msgctxt":
                    origpo = id_dict[id]
                    if origpo not in markedpos:
                        origpo.setcontext(" ".join(origpo.getlocations()))
                        markedpos.append(thepo)
                    thepo.setcontext(" ".join(thepo.getlocations()))
                    uniqueunits.append(thepo)
            else:
                if not id:
                    if duplicatestyle == "merge":
                        addcomment(thepo)
                    else:
                        thepo.setcontext(" ".join(thepo.getlocations()))
                id_dict[id] = thepo
                uniqueunits.append(thepo)
        new_gpo_memory_file = gpo.po_file_create()
        new_gpo_message_iterator = gpo.po_message_iterator(new_gpo_memory_file, None)
        for unit in uniqueunits:
            gpo.po_message_insert(new_gpo_message_iterator, unit._gpo_message)
        gpo.po_message_iterator_free(self._gpo_message_iterator)
        self._gpo_message_iterator = new_gpo_message_iterator
        self._gpo_memory_file = new_gpo_memory_file
        self.units = uniqueunits

    def __str__(self):

        def obsolete_workaround():
            # Remove all items that are not output by msgmerge when a unit is obsolete.  This is a work
            # around for bug in libgettextpo
            # FIXME Do version test in case they fix this bug
            for unit in self.units:
                if unit.isobsolete():
                    gpo.po_message_set_extracted_comments(unit._gpo_message, "")
                    location = gpo.po_message_filepos(unit._gpo_message, 0)
                    while location:
                        gpo.po_message_remove_filepos(unit._gpo_message, 0)
                        location = gpo.po_message_filepos(unit._gpo_message, 0)

        def writefile(filename):
            self._gpo_memory_file = gpo.po_file_write_v2(self._gpo_memory_file, filename, xerror_handler)
            with open(filename) as tfile:
                return tfile.read()

        outputstring = ""
        if self._gpo_memory_file:
            obsolete_workaround()
            f, fname = tempfile.mkstemp(prefix='translate', suffix='.po')
            os.close(f)
            outputstring = writefile(fname)
            if self._encoding != pounit.CPO_ENC:
                try:
                    outputstring = outputstring.decode(pounit.CPO_ENC).encode(self._encoding)
                except UnicodeEncodeError:
                    self._encoding = pounit.CPO_ENC
                    self.updateheader(content_type="text/plain; charset=UTF-8",
                                      content_transfer_encoding="8bit")
                    outputstring = writefile(fname)
            os.remove(fname)
        return outputstring

    def isempty(self):
        """Returns True if the object doesn't contain any translation units."""
        if len(self.units) == 0:
            return True
        # Skip the first unit if it is a header.
        if self.units[0].isheader():
            units = self.units[1:]
        else:
            units = self.units

        for unit in units:
            if not unit.isblank() and not unit.isobsolete():
                return False
        return True

    def parse(self, input):
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''

        if hasattr(input, "read"):
            posrc = input.read()
            input.close()
            input = posrc

        needtmpfile = not os.path.isfile(input)
        if needtmpfile:
            # This is not a file - we write the string to a temporary file
            fd, fname = tempfile.mkstemp(prefix='translate', suffix='.po')
            os.write(fd, input)
            input = fname
            os.close(fd)

        self._gpo_memory_file = gpo.po_file_read_v3(input, xerror_handler)
        if self._gpo_memory_file is None:
            logger.error("Error:")

        if needtmpfile:
            os.remove(input)

        self.units = []
        # Handle xerrors here
        self._header = gpo.po_file_domain_header(self._gpo_memory_file, None)
        if self._header:
            charset = gpo.po_header_field(self._header, "Content-Type")
            if charset:
                charset = re.search("charset=([^\\s]+)", charset).group(1)
            self._encoding = encodingToUse(charset)
        self._gpo_message_iterator = gpo.po_message_iterator(self._gpo_memory_file, None)
        newmessage = gpo.po_next_message(self._gpo_message_iterator)
        while newmessage:
            newunit = pounit(gpo_message=newmessage, encoding=self._encoding)
            self.addunit(newunit, new=False)
            newmessage = gpo.po_next_message(self._gpo_message_iterator)
        self._free_iterator()

    def __del__(self):
        # We currently disable this while we still get segmentation faults.
        # Note that this is definitely leaking memory because of this.
        return
        self._free_iterator()
        if self._gpo_memory_file is not None:
            gpo.po_file_free(self._gpo_memory_file)
            self._gpo_memory_file = None

    def _free_iterator(self):
        # We currently disable this while we still get segmentation faults.
        # Note that this is definitely leaking memory because of this.
        return
        if self._gpo_message_iterator is not None:
            gpo.po_message_iterator_free(self._gpo_message_iterator)
            self._gpo_message_iterator = None

########NEW FILE########
__FILENAME__ = csvl10n
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""classes that hold units of comma-separated values (.csv) files (csvunit)
or entire files (csvfile) for use with localisation
"""

import codecs
import csv
from cStringIO import StringIO

from translate.misc import sparse
from translate.storage import base


class SimpleDictReader:

    def __init__(self, fileobj, fieldnames):
        self.fieldnames = fieldnames
        self.contents = fileobj.read()
        self.parser = sparse.SimpleParser(defaulttokenlist=[",", "\n"], whitespacechars="\r")
        self.parser.stringescaping = 0
        self.parser.quotechars = '"'
        self.tokens = self.parser.tokenize(self.contents)
        self.tokenpos = 0

    def __iter__(self):
        return self

    def getvalue(self, value):
        """returns a value, evaluating strings as neccessary"""
        if (value.startswith("'") and value.endswith("'")) or (value.startswith('"') and value.endswith('"')):
            return sparse.stringeval(value)
        else:
            return value

    def next(self):
        lentokens = len(self.tokens)
        while self.tokenpos < lentokens and self.tokens[self.tokenpos] == "\n":
            self.tokenpos += 1
        if self.tokenpos >= lentokens:
            raise StopIteration()
        thistokens = []
        while self.tokenpos < lentokens and self.tokens[self.tokenpos] != "\n":
            thistokens.append(self.tokens[self.tokenpos])
            self.tokenpos += 1
        while self.tokenpos < lentokens and self.tokens[self.tokenpos] == "\n":
            self.tokenpos += 1
        fields = []
        # patch together fields since we can have quotes inside a field
        currentfield = ''
        fieldparts = 0
        for token in thistokens:
            if token == ',':
                # a field is only quoted if the whole thing is quoted
                if fieldparts == 1:
                    currentfield = self.getvalue(currentfield)
                fields.append(currentfield)
                currentfield = ''
                fieldparts = 0
            else:
                currentfield += token
                fieldparts += 1
        # things after the last comma...
        if fieldparts:
            if fieldparts == 1:
                currentfield = self.getvalue(currentfield)
            fields.append(currentfield)
        values = {}
        for fieldnum in range(len(self.fieldnames)):
            if fieldnum >= len(fields):
                values[self.fieldnames[fieldnum]] = ""
            else:
                values[self.fieldnames[fieldnum]] = fields[fieldnum]
        return values


class DefaultDialect(csv.excel):
    skipinitialspace = True
    quoting = csv.QUOTE_NONNUMERIC
    escapechar = '\\'

csv.register_dialect('default', DefaultDialect)


def from_unicode(text, encoding='utf-8'):
    if encoding == 'auto':
        encoding = 'utf-8'
    if isinstance(text, unicode):
        return text.encode(encoding)
    return text


def to_unicode(text, encoding='utf-8'):
    if encoding == 'auto':
        encoding = 'utf-8'
    if isinstance(text, unicode):
        return text
    return text.decode(encoding)


class csvunit(base.TranslationUnit):
    spreadsheetescapes = [("+", "\\+"), ("-", "\\-"), ("=", "\\="), ("'", "\\'")]

    def __init__(self, source=None):
        super(csvunit, self).__init__(source)
        self.location = ""
        self.source = source or ""
        self.target = ""
        self.id = ""
        self.fuzzy = 'False'
        self.developer_comments = ""
        self.translator_comments = ""
        self.context = ""

    def getid(self):
        if self.id:
            return self.id

        result = self.source
        context = self.context
        if context:
            result = u"%s\04%s" % (context, result)

        return result

    def setid(self, value):
        self.id = value

    def getlocations(self):
        #FIXME: do we need to support more than one location
        return [self.location]

    def addlocation(self, location):
        self.location = location

    def getcontext(self):
        return self.context

    def setcontext(self, value):
        self.context = value

    def getnotes(self, origin=None):
        if origin is None:
            result = self.translator_comments
            if self.developer_comments:
                if result:
                    result += '\n' + self.developer_comments
                else:
                    result = self.developer_comments
            return result
        elif origin == "translator":
            return self.translator_comments
        elif origin in ('programmer', 'developer', 'source code'):
            return self.developer_comments
        else:
            raise ValueError("Comment type not valid")

    def addnote(self, text, origin=None, position="append"):
        if origin in ('programmer', 'developer', 'source code'):
            if position == 'append' and self.developer_comments:
                self.developer_comments += '\n' + text
            elif position == 'prepend' and self.developer_comments:
                self.developer_comments = text + '\n' + self.developer_comments
            else:
                self.developer_comments = text
        else:
            if position == 'append' and self.translator_comments:
                self.translator_comments += '\n' + text
            elif position == 'prepend' and self.translator_comments:
                self.translator_comments = self.translator_comments + '\n' + text
            else:
                self.translator_comments = text

    def removenotes(self):
        self.translator_comments = u''

    def isfuzzy(self):
        if self.fuzzy.lower() in ('1', 'x', 'true', 'yes', 'fuzzy'):
            return True
        return False

    def markfuzzy(self, value=True):
        if value:
            self.fuzzy = 'True'
        else:
            self.fuzzy = 'False'

    def match_header(self):
        """see if unit might be a header"""
        some_value = False
        for key, value in self.todict().iteritems():
            if value:
                some_value = True
            if key.lower() != 'fuzzy' and value and key.lower() != value.lower():
                return False
        return some_value

    def add_spreadsheet_escapes(self, source, target):
        """add common spreadsheet escapes to two strings"""
        for unescaped, escaped in self.spreadsheetescapes:
            if source.startswith(unescaped):
                source = source.replace(unescaped, escaped, 1)
            if target.startswith(unescaped):
                target = target.replace(unescaped, escaped, 1)
        return source, target

    def remove_spreadsheet_escapes(self, source, target):
        """remove common spreadsheet escapes from two strings"""
        for unescaped, escaped in self.spreadsheetescapes:
            if source.startswith(escaped):
                source = source.replace(escaped, unescaped, 1)
            if target.startswith(escaped):
                target = target.replace(escaped, unescaped, 1)
        return source, target

    def fromdict(self, cedict, encoding='utf-8'):
        for key, value in cedict.iteritems():
            rkey = fieldname_map.get(key, key)
            if value is None or key is None or key == EXTRA_KEY:
                continue
            value = to_unicode(value, encoding)
            if rkey == "id":
                self.id = value
            elif rkey == "source":
                self.source = value
            elif rkey == "target":
                self.target = value
            elif rkey == "location":
                self.location = value
            elif rkey == "fuzzy":
                self.fuzzy = value
            elif rkey == "context":
                self.context = value
            elif rkey == "translator_comments":
                self.translator_comments = value
            elif rkey == "developer_comments":
                self.developer_comments = value

        #self.source, self.target = self.remove_spreadsheet_escapes(self.source, self.target)

    def todict(self, encoding='utf-8'):
        #FIXME: use apis?
        #source, target = self.add_spreadsheet_escapes(self.source, self.target)
        source = self.source
        target = self.target
        output = {
            'location': from_unicode(self.location, encoding),
            'source': from_unicode(source, encoding),
            'target': from_unicode(target, encoding),
            'id': from_unicode(self.id, encoding),
            'fuzzy': str(self.fuzzy),
            'context': from_unicode(self.context, encoding),
            'translator_comments': from_unicode(self.translator_comments, encoding),
            'developer_comments': from_unicode(self.developer_comments, encoding),
        }

        return output

    def __str__(self):
        return str(self.todict())

canonical_field_names = ('location', 'source', 'target', 'id', 'fuzzy', 'context', 'translator_comments', 'developer_comments')
fieldname_map = {
    'original': 'source',
    'untranslated': 'source',
    'translated': 'target',
    'translation': 'target',
    'identified': 'id',
    'key': 'id',
    'label': 'id',
    'transaltor comments': 'translator_comments',
    'notes': 'translator_comments',
    'developer comments': 'developer_comments',
    'state': 'fuzzy',
}


EXTRA_KEY = '__CSVL10N__EXTRA__'


def try_dialects(inputfile, fieldnames, dialect):
    #FIXME: does it verify at all if we don't actually step through the file?
    try:
        inputfile.seek(0)
        reader = csv.DictReader(inputfile, fieldnames=fieldnames, dialect=dialect, restkey=EXTRA_KEY)
    except csv.Error:
        try:
            inputfile.seek(0)
            reader = csv.DictReader(inputfile, fieldnames=fieldnames, dialect='default', restkey=EXTRA_KEY)
        except csv.Error:
            inputfile.seek(0)
            reader = csv.DictReader(inputfile, fieldnames=fieldnames, dialect='excel', restkey=EXTRA_KEY)
    return reader


def valid_fieldnames(fieldnames):
    """check if fieldnames are valid"""
    for fieldname in fieldnames:
        if fieldname in canonical_field_names and fieldname == 'source':
            return True
        elif fieldname in fieldname_map and fieldname_map[fieldname] == 'source':
            return True
    return False


def detect_header(sample, dialect, fieldnames):
    """Test if file has a header or not, also returns number of columns in first row"""
    inputfile = StringIO(sample)
    try:
        reader = csv.reader(inputfile, dialect)
    except csv.Error:
        try:
            inputfile.seek(0)
            reader = csv.reader(inputfile, 'default')
        except csv.Error:
            inputfile.seek(0)
            reader = csv.reader(inputfile, 'excel')

    header = reader.next()
    columncount = max(len(header), 3)
    if valid_fieldnames(header):
        return header
    return fieldnames[:columncount]


class csvfile(base.TranslationStore):
    """This class represents a .csv file with various lines.
    The default format contains three columns: location, source, target"""
    UnitClass = csvunit
    Name = "Comma Separated Value"
    Mimetypes = ['text/comma-separated-values', 'text/csv']
    Extensions = ["csv"]

    def __init__(self, inputfile=None, fieldnames=None, encoding="auto"):
        base.TranslationStore.__init__(self, unitclass=self.UnitClass)
        self.units = []
        self.encoding = encoding or 'utf-8'
        if not fieldnames:
            self.fieldnames = ['location', 'source', 'target', 'id', 'fuzzy', 'context', 'translator_comments', 'developer_comments']
        else:
            if isinstance(fieldnames, basestring):
                fieldnames = [fieldname.strip() for fieldname in fieldnames.split(",")]
            self.fieldnames = fieldnames
        self.filename = getattr(inputfile, 'name', '')
        self.dialect = 'default'
        if inputfile is not None:
            csvsrc = inputfile.read()
            inputfile.close()
            self.parse(csvsrc)

    def parse(self, csvsrc):
        text, encoding = self.detect_encoding(csvsrc, default_encodings=['utf-8', 'utf-16'])
        #FIXME: raise parse error if encoding detection fails?
        if encoding and encoding.lower() != 'utf-8':
            csvsrc = text.encode('utf-8').lstrip(codecs.BOM_UTF8)
        self.encoding = encoding or 'utf-8'

        sniffer = csv.Sniffer()
        # FIXME: maybe we should sniff a smaller sample
        sample = csvsrc[:1024]
        if isinstance(sample, unicode):
            sample = sample.encode('utf-8')

        try:
            self.dialect = sniffer.sniff(sample)
            if not self.dialect.escapechar:
                self.dialect.escapechar = '\\'
                if self.dialect.quoting == csv.QUOTE_MINIMAL:
                    #HACKISH: most probably a default, not real detection
                    self.dialect.quoting = csv.QUOTE_ALL
                    self.dialect.doublequote = True
        except csv.Error:
            self.dialect = 'default'

        try:
            fieldnames = detect_header(sample, self.dialect, self.fieldnames)
            self.fieldnames = fieldnames
        except csv.Error:
            pass

        inputfile = csv.StringIO(csvsrc)
        reader = try_dialects(inputfile, self.fieldnames, self.dialect)

        #reader = SimpleDictReader(csvfile, fieldnames=fieldnames, dialect=dialect)
        first_row = True
        for row in reader:
            newce = self.UnitClass()
            newce.fromdict(row)
            if not first_row or not newce.match_header():
                self.addunit(newce)
            first_row = False

    def __str__(self):
        """convert to a string. double check that unicode is handled somehow here"""
        source = self.getoutput()
        if not isinstance(source, unicode):
            source = source.decode('utf-8')
        if not self.encoding or self.encoding == 'auto':
            encoding = 'utf-8'
        else:
            encoding = self.encoding
        return source.encode(encoding)

    def getoutput(self):
        outputfile = StringIO()
        writer = csv.DictWriter(outputfile, self.fieldnames, extrasaction='ignore', dialect=self.dialect)
        # write header
        hdict = dict(map(None, self.fieldnames, self.fieldnames))
        writer.writerow(hdict)
        for ce in self.units:
            cedict = ce.todict()
            writer.writerow(cedict)
        return outputfile.getvalue()

########NEW FILE########
__FILENAME__ = directory
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module provides functionality to work with directories."""

# Perhaps all methods should work with a wildcard to limit searches in some
# way (examples: *.po, base.xlf, pootle-terminology.tbx)

#TODO: consider also providing directories as we currently provide files

import os

from translate.storage import factory


class Directory:
    """This class represents a directory."""

    def __init__(self, dir=None):
        self.dir = dir
        self.filedata = []

    def file_iter(self):
        """Iterator over (dir, filename) for all files in this directory."""
        if not self.filedata:
            self.scanfiles()
        for filetuple in self.filedata:
            yield filetuple

    def getfiles(self):
        """Returns a list of (dir, filename) tuples for all the file names in
        this directory."""
        return [filetuple for filetuple in self.file_iter()]

    def unit_iter(self):
        """Iterator over all the units in all the files in this directory."""
        for dirname, filename in self.file_iter():
            store = factory.getobject(os.path.join(dirname, filename))
            #TODO: don't regenerate all the storage objects
            for unit in store.unit_iter():
                yield unit

    def getunits(self):
        """List of all the units in all the files in this directory."""
        return [unit for unit in self.unit_iter()]

    def scanfiles(self):
        """Populate the internal file data."""
        self.filedata = []

        for dirpath, dirnames, filenames in os.walk(self.dir):
            fnames = dirnames + filenames
            for fname in fnames:
                if os.path.isfile(os.path.join(dirpath, fname)):
                    self.filedata.append((dirpath, fname))

########NEW FILE########
__FILENAME__ = dtd
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of .dtd files (:class:`dtdunit`) or entire files
(:class:`dtdfile`).

These are specific .dtd files for localisation used by mozilla.

Specifications
    The following information is provided by Mozilla:

    `Specification <http://www.w3.org/TR/REC-xml/#sec-entexpand>`_

    There is a grammar for entity definitions, which isn't really precise,
    as the spec says.  There's no formal specification for DTD files, it's
    just "whatever makes this work" basically. The whole piece is clearly not
    the strongest point of the xml spec

    XML elements are allowed in entity values. A number of things that are
    allowed will just break the resulting document, Mozilla forbids these
    in their DTD parser.

Dialects
    There are two dialects:

    - Regular DTD
    - Android DTD

    Both dialects are similar, but the Android DTD uses some particular escapes
    that regular DTDs don't have.

Escaping in regular DTD
    In DTD usually there are characters escaped in the entities. In order to
    ease the translation some of those escaped characters are unescaped when
    reading from, or converting, the DTD, and that are escaped again when
    saving, or converting to a DTD.

    In regular DTD the following characters are usually or sometimes escaped:

    - The % character is escaped using &#037; or &#37; or &#x25;
    - The " character is escaped using &quot;
    - The ' character is escaped using &apos; (partial roundtrip)
    - The & character is escaped using &amp;
    - The < character is escaped using &lt; (not yet implemented)
    - The > character is escaped using &gt; (not yet implemented)

    Besides the previous ones there are a lot of escapes for a huge number of
    characters. This escapes usually have the form of &#NUMBER; where NUMBER
    represents the numerical code for the character.

    There are a few particularities in DTD escaping. Some of the escapes are
    not yet implemented since they are not really necessary, or because its
    implementation is too hard.

    A special case is the ' escaping using &apos; which doesn't provide a full
    roundtrip conversion in order to support some special Mozilla DTD files.

    Also the " character is never escaped in the case that the previous
    character is = (the sequence =" is present on the string) in order to avoid
    escaping the " character indicating an attribute assignment, for example in
    a href attribute for an a tag in HTML (anchor tag).

Escaping in Android DTD
    It has the sames escapes as in regular DTD, plus this ones:

    - The ' character is escaped using \&apos; or \' or \u0027
    - The " character is escaped using \&quot;
"""

import re
import warnings
from cStringIO import StringIO
try:
    from lxml import etree
except ImportError:
    etree = None

from translate.misc import quote
from translate.storage import base


labelsuffixes = (".label", ".title")
"""Label suffixes: entries with this suffix are able to be comibed with accesskeys
found in in entries ending with :attr:`.accesskeysuffixes`"""
accesskeysuffixes = (".accesskey", ".accessKey", ".akey")
"""Accesskey Suffixes: entries with this suffix may be combined with labels
ending in :attr:`.labelsuffixes` into accelerator notation"""


def quoteforandroid(source):
    """Escapes a line for Android DTD files. """
    # Replace "'" character with the \u0027 escape. Other possible replaces are
    # "\\&apos;" or "\\'".
    source = source.replace(u"'", u"\\u0027")
    source = source.replace(u"\"", u"\\&quot;")
    value = quotefordtd(source)  # value is an UTF-8 encoded string.
    return value


def unquotefromandroid(source):
    """Unquotes a quoted Android DTD definition."""
    value = unquotefromdtd(source)  # value is an UTF-8 encoded string.
    value = value.replace(u"\\&apos;", u"'")
    value = value.replace(u"\\'", u"'")
    value = value.replace(u"\\u0027", u"'")
    value = value.replace("\\\"", "\"")  # This converts \&quot; to ".
    return value


_DTD_CODEPOINT2NAME = {
    ord("%"): "#037",  # Always escape % sign as &#037;.
    ord("&"): "amp",
   #ord("<"): "lt",  # Not really so useful.
   #ord(">"): "gt",  # Not really so useful.
}

def quotefordtd(source):
    """Quotes and escapes a line for regular DTD files."""
    source = quote.entityencode(source, _DTD_CODEPOINT2NAME)
    if '"' in source:
        source = source.replace("'", "&apos;")  # This seems not to runned.
        if '="' not in source:  # Avoid escaping " chars in href attributes.
            source = source.replace("\"", "&quot;")
            value = "\"" + source + "\""  # Quote using double quotes.
        else:
            value = "'" + source + "'"  # Quote using single quotes.
    else:
        value = "\"" + source + "\""  # Quote using double quotes.
    return value.encode('utf-8')


_DTD_NAME2CODEPOINT = {
    "quot":   ord('"'),
    "amp":    ord("&"),
   #"lt":     ord("<"),  # Not really so useful.
   #"gt":     ord(">"),  # Not really so useful.
   # FIXME these should probably be handled in a more general way
    "#x0022": ord('"'),
    "#187":   ord(u""),
    "#037":   ord("%"),
    "#37":    ord("%"),
    "#x25":   ord("%"),
}

def unquotefromdtd(source):
    """unquotes a quoted dtd definition"""
    # extract the string, get rid of quoting
    if len(source) == 0:
        source = '""'
    # The quote characters should be the first and last characters in the
    # string. Of course there could also be quote characters within the string.
    quotechar = source[0]
    extracted, quotefinished = quote.extractwithoutquotes(source, quotechar, quotechar, allowreentry=False)
    extracted = extracted.decode('utf-8')
    if quotechar == "'":
        extracted = extracted.replace("&apos;", "'")
    extracted = quote.entitydecode(extracted, _DTD_NAME2CODEPOINT)
    return extracted


def removeinvalidamps(name, value):
    """Find and remove ampersands that are not part of an entity definition.

    A stray & in a DTD file can break an application's ability to parse the
    file. In Mozilla localisation this is very important and these can break the
    parsing of files used in XUL and thus break interface rendering. Tracking
    down the problem is very difficult, thus by removing potential broken
    ampersand and warning the users we can ensure that the output DTD will
    always be parsable.

    :type name: String
    :param name: Entity name
    :type value: String
    :param value: Entity text value
    :rtype: String
    :return: Entity value without bad ampersands
    """

    def is_valid_entity_name(name):
        """Check that supplied *name* is a valid entity name."""
        if name.replace('.', '').replace('_', '').isalnum():
            return True
        elif name[0] == '#' and name[1:].isalnum():
            return True
        return False

    amppos = 0
    invalid_amps = []
    while amppos >= 0:
        amppos = value.find("&", amppos)
        if amppos != -1:
            amppos += 1
            semipos = value.find(";", amppos)
            if semipos != -1:
                if is_valid_entity_name(value[amppos:semipos]):
                    continue
            invalid_amps.append(amppos - 1)
    if len(invalid_amps) > 0:
        warnings.warn("invalid ampersands in dtd entity %s" % (name))
        adjustment = 0
        for amppos in invalid_amps:
            value = value[:amppos-adjustment] + value[amppos-adjustment+1:]
            adjustment += 1
    return value


class dtdunit(base.TranslationUnit):
    """An entity definition from a DTD file (and any associated comments)."""

    def __init__(self, source="", android=False):
        """construct the dtdunit, prepare it for parsing"""
        self.android = android

        super(dtdunit, self).__init__(source)
        self.comments = []
        self.unparsedlines = []
        self.incomment = False
        self.inentity = False
        self.entity = "FakeEntityOnlyForInitialisationAndTesting"
        self.source = source
        self.space_pre_entity = ' '
        self.space_pre_definition = ' '
        self.closing = ">"

    # Note that source and target are equivalent for monolingual units
    def setsource(self, source):
        """Sets the definition to the quoted value of source"""
        if self.android:
            self.definition = quoteforandroid(source)
        else:
            self.definition = quotefordtd(source)
        self._rich_source = None

    def getsource(self):
        """gets the unquoted source string"""
        if self.android:
            return unquotefromandroid(self.definition)
        else:
            return unquotefromdtd(self.definition)
    source = property(getsource, setsource)

    def settarget(self, target):
        """Sets the definition to the quoted value of target"""
        if target is None:
            target = ""
        if self.android:
            self.definition = quoteforandroid(target)
        else:
            self.definition = quotefordtd(target)
        self._rich_target = None

    def gettarget(self):
        """gets the unquoted target string"""
        if self.android:
            return unquotefromandroid(self.definition)
        else:
            return unquotefromdtd(self.definition)
    target = property(gettarget, settarget)

    def getid(self):
        return self.entity

    def setid(self, new_id):
        self.entity = new_id

    def getlocations(self):
        """Return the entity as location (identifier)."""
        assert quote.rstripeol(self.entity) == self.entity
        return [self.entity]

    def addlocation(self, location):
        """Set the entity to the given "location"."""
        self.entity = location

    def isnull(self):
        """returns whether this dtdunit doesn't actually have an entity definition"""
        # for dtds, we currently return a blank string if there is no .entity (==location in other files)
        # TODO: this needs to work better with base class expectations
        return self.entity is None

    def istranslatable(self):
        if getattr(self, "entityparameter", None) == "SYSTEM" or self.isnull():
            return False
        return True

    def parse(self, dtdsrc):
        """read the first dtd element from the source code into this object, return linesprocessed"""
        self.comments = []
        # make all the lists the same
        self._locfilenotes = self.comments
        self._locgroupstarts = self.comments
        self._locgroupends = self.comments
        self._locnotes = self.comments
        # self._locfilenotes = []
        # self._locgroupstarts = []
        # self._locgroupends = []
        # self._locnotes = []
        # self.comments = []
        self.entity = None
        self.definition = ''
        if not dtdsrc:
            return 0
        lines = dtdsrc.split("\n")
        linesprocessed = 0
        comment = ""
        for line in lines:
            line += "\n"
            linesprocessed += 1
            # print "line(%d,%d): " % (self.incomment,self.inentity),line[:-1]
            if not self.incomment:
                if (line.find('<!--') != -1):
                    self.incomment = True
                    self.continuecomment = False
                    # now work out the type of comment, and save it (remember we're not in the comment yet)
                    (comment, dummy) = quote.extract(line, "<!--", "-->", None, 0)
                    if comment.find('LOCALIZATION NOTE') != -1:
                        l = quote.findend(comment, 'LOCALIZATION NOTE')
                        while (comment[l] == ' '):
                            l += 1
                        if comment.find('FILE', l) == l:
                            self.commenttype = "locfile"
                        elif comment.find('BEGIN', l) == l:
                            self.commenttype = "locgroupstart"
                        elif comment.find('END', l) == l:
                            self.commenttype = "locgroupend"
                        else:
                            self.commenttype = "locnote"
                    else:
                        # plain comment
                        self.commenttype = "comment"
                #FIXME: bloody entity might share a line with something important
                elif not self.inentity and re.search("%.*;", line):
                    # now work out the type of comment, and save it (remember we're not in the comment yet)
                    self.comments.append(("comment", line))
                    line = ""
                    continue

            if self.incomment:
                # some kind of comment
                (comment, self.incomment) = quote.extract(line, "<!--", "-->", None, self.continuecomment)
                # print "comment(%d,%d): " % (self.incomment,self.continuecomment),comment
                self.continuecomment = self.incomment
                # strip the comment out of what will be parsed
                line = line.replace(comment, "", 1)
                # add a end of line of this is the end of the comment
                if not self.incomment:
                    if line.isspace():
                        comment += line
                        line = ''
                    else:
                        comment += '\n'
                # check if there's actually an entity definition that's commented out
                # TODO: parse these, store as obsolete messages
                # if comment.find('<!ENTITY') != -1:
                #     # remove the entity from the comment
                #     comment, dummy = quote.extractwithoutquotes(comment, ">", "<!ENTITY", None, 1)
                # depending on the type of comment (worked out at the start), put it in the right place
                # make it record the comment and type as a tuple
                commentpair = (self.commenttype, comment)
                if self.commenttype == "locfile":
                    self._locfilenotes.append(commentpair)
                elif self.commenttype == "locgroupstart":
                    self._locgroupstarts.append(commentpair)
                elif self.commenttype == "locgroupend":
                    self._locgroupends.append(commentpair)
                elif self.commenttype == "locnote":
                    self._locnotes.append(commentpair)
                elif self.commenttype == "comment":
                    self.comments.append(commentpair)

            if not self.inentity and not self.incomment:
                entitypos = line.find('<!ENTITY')
                if entitypos != -1:
                    self.inentity = True
                    beforeentity = line[:entitypos].strip()
                    if beforeentity.startswith("#"):
                        self.hashprefix = beforeentity
                    self.entitypart = "start"
                else:
                    self.unparsedlines.append(line)

            if self.inentity:
                if self.entitypart == "start":
                    # the entity definition
                    e = quote.findend(line, '<!ENTITY')
                    line = line[e:]
                    self.entitypart = "name"
                    self.entitytype = "internal"
                if self.entitypart == "name":
                    s = 0
                    e = 0
                    while (e < len(line) and line[e].isspace()):
                        e += 1
                    self.space_pre_entity = ' ' * (e - s)
                    s = e
                    self.entity = ''
                    if (e < len(line) and line[e] == '%'):
                        self.entitytype = "external"
                        self.entityparameter = ""
                        e += 1
                        while (e < len(line) and line[e].isspace()):
                            e += 1
                    while (e < len(line) and not line[e].isspace()):
                        self.entity += line[e]
                        e += 1
                    s = e

                    assert quote.rstripeol(self.entity) == self.entity
                    while (e < len(line) and line[e].isspace()):
                        e += 1
                    self.space_pre_definition = ' ' * (e - s)
                    if self.entity:
                        if self.entitytype == "external":
                            self.entitypart = "parameter"
                        else:
                            self.entitypart = "definition"
                        # remember the start position and the quote character
                        if e == len(line):
                            self.entityhelp = None
                            e = 0
                            continue
                        elif self.entitypart == "definition":
                            self.entityhelp = (e, line[e])
                            self.instring = False
                if self.entitypart == "parameter":
                    while (e < len(line) and line[e].isspace()):
                        e += 1
                    paramstart = e
                    while (e < len(line) and line[e].isalnum()):
                        e += 1
                    self.entityparameter += line[paramstart:e]
                    while (e < len(line) and line[e].isspace()):
                        e += 1
                    line = line[e:]
                    e = 0
                    if not line:
                        continue
                    if line[0] in ('"', "'"):
                        self.entitypart = "definition"
                        self.entityhelp = (e, line[e])
                        self.instring = False
                if self.entitypart == "definition":
                    if self.entityhelp is None:
                        e = 0
                        while (e < len(line) and line[e].isspace()):
                            e += 1
                        if e == len(line):
                            continue
                        self.entityhelp = (e, line[e])
                        self.instring = False
                    # actually the lines below should remember instring, rather than using it as dummy
                    e = self.entityhelp[0]
                    if (self.entityhelp[1] == "'"):
                        (defpart, self.instring) = quote.extract(line[e:], "'", "'", startinstring=self.instring, allowreentry=False)
                    elif (self.entityhelp[1] == '"'):
                        (defpart, self.instring) = quote.extract(line[e:], '"', '"', startinstring=self.instring, allowreentry=False)
                    else:
                        raise ValueError("Unexpected quote character... %r" % (self.entityhelp[1]))
                    # for any following lines, start at the beginning of the line. remember the quote character
                    self.entityhelp = (0, self.entityhelp[1])
                    self.definition += defpart
                    if not self.instring:
                        self.closing = line[e+len(defpart):].rstrip("\n\r")
                        self.inentity = False
                        break

        # uncomment this line to debug processing
        if 0:
            for attr in dir(self):
                r = repr(getattr(self, attr))
                if len(r) > 60:
                    r = r[:57] + "..."
                self.comments.append(("comment", "self.%s = %s" % (attr, r)))
        return linesprocessed

    def __str__(self):
        """convert to a string. double check that unicode is handled somehow here"""
        source = self.getoutput()
        if isinstance(source, unicode):
            return source.encode(getattr(self, "encoding", "UTF-8"))
        return source

    def getoutput(self):
        """convert the dtd entity back to string form"""
        lines = []
        lines.extend([comment for commenttype, comment in self.comments])
        lines.extend(self.unparsedlines)
        if self.isnull():
            result = "".join(lines)
            return result.rstrip() + "\n"
        # for f in self._locfilenotes: yield f
        # for ge in self._locgroupends: yield ge
        # for gs in self._locgroupstarts: yield gs
        # for n in self._locnotes: yield n
        if len(self.entity) > 0:
            if getattr(self, 'entitytype', None) == 'external':
                entityline = '<!ENTITY % ' + self.entity + ' ' + self.entityparameter + ' ' + self.definition + self.closing
            else:
                entityline = '<!ENTITY' + self.space_pre_entity + self.entity + self.space_pre_definition + self.definition + self.closing
            if getattr(self, 'hashprefix', None):
                entityline = self.hashprefix + " " + entityline
            if isinstance(entityline, unicode):
                entityline = entityline.encode('UTF-8')
            lines.append(entityline + '\n')
        return "".join(lines)


class dtdfile(base.TranslationStore):
    """A .dtd file made up of dtdunits."""
    UnitClass = dtdunit

    def __init__(self, inputfile=None, android=False):
        """construct a dtdfile, optionally reading in from inputfile"""
        base.TranslationStore.__init__(self, unitclass=self.UnitClass)
        self.filename = getattr(inputfile, 'name', '')
        self.android = android
        if inputfile is not None:
            dtdsrc = inputfile.read()
            self.parse(dtdsrc)
            self.makeindex()

    def parse(self, dtdsrc):
        """read the source code of a dtd file in and include them as dtdunits in self.units"""
        start = 0
        end = 0
        lines = dtdsrc.split("\n")
        while end < len(lines):
            if (start == end):
                end += 1
            foundentity = False
            while end < len(lines):
                if end >= len(lines):
                    break
                if lines[end].find('<!ENTITY') > -1:
                    foundentity = True
                if foundentity and re.match("[\"']\s*>", lines[end]):
                    end += 1
                    break
                end += 1
            # print "processing from %d to %d" % (start,end)

            linesprocessed = 1  # to initialise loop
            while linesprocessed >= 1:
                newdtd = dtdunit(android=self.android)
                try:
                    linesprocessed = newdtd.parse("\n".join(lines[start:end]))
                    if linesprocessed >= 1 and (not newdtd.isnull() or newdtd.unparsedlines):
                        self.units.append(newdtd)
                except Exception as e:
                    warnings.warn("%s\nError occured between lines %d and %d:\n%s" % (e, start + 1, end, "\n".join(lines[start:end])))
                start += linesprocessed

    def __str__(self):
        """convert to a string. double check that unicode is handled somehow here"""
        source = self.getoutput()
        if not self._valid_store():
            warnings.warn("DTD file '%s' does not validate" % self.filename)
            return None
        if isinstance(source, unicode):
            return source.encode(getattr(self, "encoding", "UTF-8"))
        return source

    def getoutput(self):
        """convert the units back to source"""
        sources = [str(dtd) for dtd in self.units]
        return "".join(sources)

    def makeindex(self):
        """makes self.id_index dictionary keyed on entities"""
        self.id_index = {}
        for dtd in self.units:
            if not dtd.isnull():
                self.id_index[dtd.entity] = dtd

    def _valid_store(self):
        """Validate the store to determine if it is valid

        This uses ElementTree to parse the DTD

        :return: If the store passes validation
        :rtype: Boolean
        """
        # Android files are invalid DTDs
        if etree is not None and not self.android:
            try:
                # #expand is a Mozilla hack and are removed as they are not valid in DTDs
                dtd = etree.DTD(StringIO(re.sub("#expand", "", self.getoutput())))
            except etree.DTDParseError as e:
                warnings.warn("DTD parse error: %s" % e.error_log)
                return False
        return True

########NEW FILE########
__FILENAME__ = factory
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""factory methods to build real storage objects that conform to base.py"""

import os


#TODO: Monolingual formats (with template?)

decompressclass = {
    'gz': ("gzip", "GzipFile"),
    'bz2': ("bz2", "BZ2File"),
}


classes_str = {
           "csv": ("csvl10n", "csvfile"),
           "tab": ("omegat", "OmegaTFileTab"), "utf8": ("omegat", "OmegaTFile"),
           "po": ("po", "pofile"), "pot": ("po", "pofile"),
           "mo": ("mo", "mofile"), "gmo": ("mo", "mofile"),
           "qm": ("qm", "qmfile"),
           "lang": ("mozilla_lang", "LangStore"),
           "utx": ("utx", "UtxFile"),
           "_wftm": ("wordfast", "WordfastTMFile"),
           "_trados_txt_tm": ("trados", "TradosTxtTmFile"),
           "catkeys": ("catkeys", "CatkeysFile"),

           "qph": ("qph", "QphFile"),
           "tbx": ("tbx", "tbxfile"),
           "tmx": ("tmx", "tmxfile"),
           "ts": ("ts2", "tsfile"),
           "xliff": ("xliff", "xlifffile"), "xlf": ("xliff", "xlifffile"),
           "sdlxliff": ("xliff", "xlifffile"),
}
###  XXX:  if you add anything here, you must also add it to translate.storage.

"""Dictionary of file extensions and the names of their associated class.

Used for dynamic lazy loading of modules.
_ext is a pseudo extension, that is their is no real extension by that name.
"""


def _examine_txt(storefile):
    """Determine the true filetype for a .txt file"""
    if isinstance(storefile, basestring) and os.path.exists(storefile):
        storefile = open(storefile)
    try:
        start = storefile.read(600).strip()
    except AttributeError:
        raise ValueError("Need to read object to determine type")
    # Some encoding magic for Wordfast
    from translate.storage import wordfast
    if wordfast.TAB_UTF16 in start.split("\n")[0]:
        encoding = 'utf-16'
    else:
        encoding = 'iso-8859-1'
    start = start.decode(encoding).encode('utf-8')
    if '%Wordfast TM' in start:
        pseudo_extension = '_wftm'
    elif '<RTF Preamble>' in start:
        pseudo_extension = '_trados_txt_tm'
    else:
        raise ValueError("Failed to guess file type.")
    storefile.seek(0)
    return pseudo_extension

hiddenclasses = {"txt": _examine_txt}


def _guessextention(storefile):
    """Guesses the type of a file object by looking at the first few characters.
    The return value is a file extention ."""
    start = storefile.read(300).strip()
    if '<xliff ' in start:
        extention = 'xlf'
    elif 'msgid "' in start:
        extention = 'po'
    elif '%Wordfast TM' in start:
        extention = 'txt'
    elif '<!DOCTYPE TS>' in start:
        extention = 'ts'
    elif '<tmx ' in start:
        extention = 'tmx'
    elif '#UTX' in start:
        extention = 'utx'
    else:
        raise ValueError("Failed to guess file type.")
    storefile.seek(0)
    return extention


def _getdummyname(storefile):
    """Provides a dummy name for a file object without a name attribute, by guessing the file type."""
    return 'dummy.' + _guessextention(storefile)


def _getname(storefile):
    """returns the filename"""
    if storefile is None:
        raise ValueError("This method cannot magically produce a filename when given None as input.")
    if not isinstance(storefile, basestring):
        if not hasattr(storefile, "name"):
            storefilename = _getdummyname(storefile)
        else:
            storefilename = storefile.name
    else:
        storefilename = storefile
    return storefilename


def getclass(storefile, ignore=None, classes=None, classes_str=classes_str, hiddenclasses=hiddenclasses):
    """Factory that returns the applicable class for the type of file presented.
    Specify ignore to ignore some part at the back of the name (like .gz). """
    storefilename = _getname(storefile)
    if ignore and storefilename.endswith(ignore):
        storefilename = storefilename[:-len(ignore)]
    root, ext = os.path.splitext(storefilename)
    ext = ext[len(os.path.extsep):].lower()
    decomp = None
    if ext in decompressclass:
        decomp = ext
        root, ext = os.path.splitext(root)
        ext = ext[len(os.path.extsep):].lower()
    if ext in hiddenclasses:
        guesserfn = hiddenclasses[ext]
        if decomp:
            _module, _class = decompressclass[decomp]
            module = __import__(_module, globals(), {}, [])
            _file = getattr(module, _class)
            ext = guesserfn(_file(storefile))
        else:
            ext = guesserfn(storefile)
    try:
        # we prefer classes (if given) since that is the older API that Pootle uses
        if classes:
            storeclass = classes[ext]
        else:
            _module, _class = classes_str[ext]
            module = __import__("translate.storage.%s" % _module, globals(), {}, _module)
            storeclass = getattr(module, _class)
    except KeyError:
        raise ValueError("Unknown filetype (%s)" % storefilename)
    return storeclass


def getobject(storefile, ignore=None, classes=None, classes_str=classes_str, hiddenclasses=hiddenclasses):
    """Factory that returns a usable object for the type of file presented.

    :type storefile: file or str
    :param storefile: File object or file name.

    Specify ignore to ignore some part at the back of the name (like .gz).
    """

    if isinstance(storefile, basestring):
        if os.path.isdir(storefile) or storefile.endswith(os.path.sep):
            from translate.storage import directory
            return directory.Directory(storefile)
    storefilename = _getname(storefile)
    storeclass = getclass(storefile, ignore, classes=classes, classes_str=classes_str, hiddenclasses=hiddenclasses)
    if os.path.exists(storefilename) or not getattr(storefile, "closed", True):
        name, ext = os.path.splitext(storefilename)
        ext = ext[len(os.path.extsep):].lower()
        if ext in decompressclass:
            _module, _class = decompressclass[ext]
            module = __import__(_module, globals(), {}, [])
            _file = getattr(module, _class)
            storefile = _file(storefilename)
        store = storeclass.parsefile(storefile)
    else:
        store = storeclass()
        store.filename = storefilename
    return store


supported = [
        ('Gettext PO file', ['po', 'pot'], ["text/x-gettext-catalog", "text/x-gettext-translation", "text/x-po", "text/x-pot"]),
        ('XLIFF Translation File', ['xlf', 'xliff', 'sdlxliff'], ["application/x-xliff", "application/x-xliff+xml"]),
        ('Gettext MO file', ['mo', 'gmo'], ["application/x-gettext-catalog", "application/x-mo"]),
        ('Qt .qm file', ['qm'], ["application/x-qm"]),
        ('TBX Glossary', ['tbx'], ['application/x-tbx']),
        ('TMX Translation Memory', ['tmx'], ["application/x-tmx"]),
        ('Qt Linguist Translation File', ['ts'], ["application/x-linguist"]),
        ('Qt Phrase Book', ['qph'], ["application/x-qph"]),
        ('OmegaT Glossary', ['utf8', 'tab'], ["application/x-omegat-glossary"]),
        ('UTX Dictionary', ['utx'], ["text/x-utx"]),
        ('Haiku catkeys file', ['catkeys'], ["application/x-catkeys"]),
]


def supported_files():
    """Returns data about all supported files

    :return: list of type that include (name, extensions, mimetypes)
    :rtype: list
    """
    return supported[:]

########NEW FILE########
__FILENAME__ = fpo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes for the support of Gettext .po and .pot files.

This implementation assumes that cpo is working. This should not be used
directly, but can be used once cpo has been established to work."""

#TODO:
# - handle headerless PO files better
# - previous msgid and msgctxt
# - accept only unicodes everywhere

import copy
import re
from cStringIO import StringIO

from translate.lang import data
from translate.misc.multistring import multistring
from translate.storage import base, cpo, pocommon, poparser
from translate.storage.pocommon import encodingToUse


lsep = " "
"""Separator for #: entries"""

basic_header = r'''msgid ""
msgstr ""
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
'''


class pounit(pocommon.pounit):
    # othercomments = []      #   # this is another comment
    # automaticcomments = []  #   #. comment extracted from the source code
    # sourcecomments = []     #   #: sourcefile.xxx:35
    # prev_msgctxt = []       #   #| The previous values that msgctxt and msgid held
    # prev_msgid = []         #
    # prev_msgid_plural = []  #
    # typecomments = []       #   #, fuzzy
    # msgidcomment = u""      #   _: within msgid
    # msgctxt
    # msgid = []
    # msgstr = []

    # Our homegrown way to indicate what must be copied in a shallow
    # fashion
    __shallow__ = ['_store']

    def __init__(self, source=None, encoding="UTF-8"):
        pocommon.pounit.__init__(self, source)
        self._encoding = encodingToUse(encoding)
        self._initallcomments(blankall=True)
        self._msgctxt = u""

        self.target = u""

    def _initallcomments(self, blankall=False):
        """Initialises allcomments"""
        if blankall:
            self.othercomments = []
            self.automaticcomments = []
            self.sourcecomments = []
            self.typecomments = []
            self.msgidcomment = u""

    def getsource(self):
        return self._source

    def setsource(self, source):
        self._rich_source = None
#        assert isinstance(source, unicode)
        source = data.forceunicode(source or u"")
        source = source or u""
        if isinstance(source, multistring):
            self._source = source
        elif isinstance(source, unicode):
            self._source = source
        else:
            #unicode, list, dict
            self._source = multistring(source)
    source = property(getsource, setsource)

    def gettarget(self):
        """Returns the unescaped msgstr"""
        return self._target

    def settarget(self, target):
        """Sets the msgstr to the given (unescaped) value"""
        self._rich_target = None
#        assert isinstance(target, unicode)
#        target = data.forceunicode(target)
        if self.hasplural():
            if isinstance(target, multistring):
                self._target = target
            else:
                #unicode, list, dict
                self._target = multistring(target)
        elif isinstance(target, (dict, list)):
            if len(target) == 1:
                self._target = target[0]
            else:
                raise ValueError("po msgid element has no plural but msgstr has %d elements (%s)" % (len(target), target))
        else:
            self._target = target
    target = property(gettarget, settarget)

    def getnotes(self, origin=None):
        """Return comments based on origin value (programmer, developer, source code and translator)"""
        if origin is None:
            comments = u"\n".join(self.othercomments)
            comments += u"\n".join(self.automaticcomments)
        elif origin == "translator":
            comments = u"\n".join(self.othercomments)
        elif origin in ["programmer", "developer", "source code"]:
            comments = u"\n".join(self.automaticcomments)
        else:
            raise ValueError("Comment type not valid")
        return comments

    def addnote(self, text, origin=None, position="append"):
        """This is modeled on the XLIFF method. See xliff.py::xliffunit.addnote"""
        # ignore empty strings and strings without non-space characters
        if not (text and text.strip()):
            return
        text = data.forceunicode(text)
        commentlist = self.othercomments
        autocomments = False
        if origin in ["programmer", "developer", "source code"]:
            autocomments = True
            commentlist = self.automaticcomments
        if text.endswith(u'\n'):
            text = text[:-1]
        newcomments = text.split(u"\n")
        if position == "append":
            newcomments = commentlist + newcomments
        elif position == "prepend":
            newcomments = newcomments + commentlist

        if autocomments:
            self.automaticcomments = newcomments
        else:
            self.othercomments = newcomments

    def removenotes(self):
        """Remove all the translator's notes (other comments)"""
        self.othercomments = []

    def __deepcopy__(self, memo={}):
        # Make an instance to serve as the copy
        new_unit = self.__class__()
        # We'll be testing membership frequently, so make a set from
        # self.__shallow__
        shallow = set(self.__shallow__)
        # Make deep copies of all members which are not in shallow
        for key, value in self.__dict__.iteritems():
            if key not in shallow:
                setattr(new_unit, key, copy.deepcopy(value))
        # Make shallow copies of all members which are in shallow
        for key in set(shallow):
            setattr(new_unit, key, getattr(self, key))
        # Mark memo with ourself, so that we won't get deep copied
        # again
        memo[id(self)] = self
        # Return our copied unit
        return new_unit

    def copy(self):
        return copy.deepcopy(self)

    def _msgidlen(self):
        if self.hasplural():
            len("".join([string for string in self.source.strings]))
        else:
            return len(self.source)

    def _msgstrlen(self):
        if self.hasplural():
            len("".join([string for string in self.target.strings]))
        else:
            return len(self.target)

    def merge(self, otherpo, overwrite=False, comments=True, authoritative=False):
        """Merges the otherpo (with the same msgid) into this one.

        Overwrite non-blank self.msgstr only if overwrite is True
        merge comments only if comments is True
        """

        def mergelists(list1, list2, split=False):
            #decode where necessary
            if unicode in [type(item) for item in list2] + [type(item) for item in list1]:
                for position, item in enumerate(list1):
                    if isinstance(item, str):
                        list1[position] = item.decode("utf-8")
                for position, item in enumerate(list2):
                    if isinstance(item, str):
                        list2[position] = item.decode("utf-8")

            #Determine the newline style of list2
            lineend = ""
            if list2 and list2[0]:
                for candidate in ["\n", "\r", "\n\r"]:
                    if list2[0].endswith(candidate):
                        lineend = candidate
                if not lineend:
                    lineend = ""

            #Split if directed to do so:
            if split:
                splitlist1 = []
                splitlist2 = []
                for item in list1:
                    splitlist1.extend(item.split())
                for item in list2:
                    splitlist2.extend(item.split())
                list1.extend([item for item in splitlist2 if not item in splitlist1])
            else:
                #Normal merge, but conform to list1 newline style
                if list1 != list2:
                    for item in list2:
                        item = item.rstrip(lineend)
                        # avoid duplicate comment lines (this might cause some problems)
                        if item not in list1 or len(item) < 5:
                            list1.append(item)

        if not isinstance(otherpo, pounit):
            super(pounit, self).merge(otherpo, overwrite, comments)
            return
        if comments:
            mergelists(self.othercomments, otherpo.othercomments)
            mergelists(self.typecomments, otherpo.typecomments)
            if not authoritative:
                # We don't bring across otherpo.automaticcomments as we consider ourself
                # to be the the authority.  Same applies to otherpo.msgidcomments
                mergelists(self.automaticcomments, otherpo.automaticcomments)
#                mergelists(self.msgidcomments, otherpo.msgidcomments) #XXX?
                mergelists(self.sourcecomments, otherpo.sourcecomments, split=True)
        if not self.istranslated() or overwrite:
            # Remove kde-style comments from the translation (if any). XXX - remove
            if pocommon.extract_msgid_comment(otherpo.target):
                otherpo.target = otherpo.target.replace('_: ' + otherpo._extract_msgidcomments() + '\n', '')
            self.target = otherpo.target
            if self.source != otherpo.source or self.getcontext() != otherpo.getcontext():
                self.markfuzzy()
            else:
                self.markfuzzy(otherpo.isfuzzy())
        elif not otherpo.istranslated():
            if self.source != otherpo.source:
                self.markfuzzy()
        else:
            if self.target != otherpo.target:
                self.markfuzzy()

    def isheader(self):
        #TODO: fix up nicely
        return not self.getid() and len(self.target) > 0

    def isblank(self):
        if self.isheader() or self.msgidcomment:
            return False
        if (self._msgidlen() == 0) and (self._msgstrlen() == 0) and len(self._msgctxt) == 0:
            return True
        return False

    def hastypecomment(self, typecomment):
        """Check whether the given type comment is present"""
        # check for word boundaries properly by using a regular expression...
        return sum(map(lambda tcline: len(re.findall("\\b%s\\b" % typecomment, tcline)), self.typecomments)) != 0

    def hasmarkedcomment(self, commentmarker):
        """Check whether the given comment marker is present as # (commentmarker) ..."""
#        raise DeprecationWarning
        commentmarker = "(%s)" % commentmarker
        for comment in self.othercomments:
            if comment.startswith(commentmarker):
                return True
        return False

    def settypecomment(self, typecomment, present=True):
        """Alters whether a given typecomment is present"""
        if self.hastypecomment(typecomment) != present:
            if present:
                self.typecomments.append("#, %s\n" % typecomment)
            else:
                # this should handle word boundaries properly ...
                typecomments = map(lambda tcline: re.sub("\\b%s\\b[ \t,]*" % typecomment, "", tcline), self.typecomments)
                self.typecomments = filter(lambda tcline: tcline.strip() != "#,", typecomments)

    def istranslated(self):
        return super(pounit, self).istranslated() and not self.isobsolete()

    def istranslatable(self):
        return not (self.isheader() or self.isblank() or self.isobsolete())

    def isfuzzy(self):
        return self.hastypecomment("fuzzy")

    def _domarkfuzzy(self, present=True):
        self.settypecomment("fuzzy", present)

    def makeobsolete(self):
        """Makes this unit obsolete"""
        self.sourcecomments = []
        self.automaticcomments = []
        super(pounit, self).makeobsolete()

    def hasplural(self):
        """returns whether this pounit contains plural strings..."""
        source = self.source
        return isinstance(source, multistring) and len(source.strings) > 1

    def parse(self, src):
        raise DeprecationWarning("Should not be parsing with a unit")
        return poparser.parse_unit(poparser.ParseState(StringIO(src), pounit), self)

    def __str__(self):
        """convert to a string. double check that unicode is handled somehow here"""
        _cpo_unit = cpo.pounit.buildfromunit(self)
        return str(_cpo_unit)

    def getlocations(self):
        """Get a list of locations from sourcecomments in the PO unit.

        rtype: List
        return: A list of the locations with '#: ' stripped

        """
        #TODO: rename to .locations
        return self.sourcecomments

    def addlocation(self, location):
        """Add a location to sourcecomments in the PO unit.

        :param location: Text location e.g. 'file.c:23' does not include #:
        :type location: String
        """
        self.sourcecomments.append(location)

    def _extract_msgidcomments(self, text=None):
        """Extract KDE style msgid comments from the unit.

        :rtype: String
        :return: Returns the extracted msgidcomments found in this unit's msgid.
        """
        if text:
            return pocommon.extract_msgid_comment(text)
        else:
            return self.msgidcomment

    def getcontext(self):
        """Get the message context."""
        return self._msgctxt + self.msgidcomment

    def setcontext(self, context):
        context = data.forceunicode(context or u"")
        self._msgctxt = context

    def getid(self):
        """Returns a unique identifier for this unit."""
        context = self.getcontext()
        # Gettext does not consider the plural to determine duplicates, only
        # the msgid. For generation of .mo files, we might want to use this
        # code to generate the entry for the hash table, but for now, it is
        # commented out for conformance to gettext.
#        id = '\0'.join(self.source.strings)
        id = self.source
        if self.msgidcomment:
            id = u"_: %s\n%s" % (context, id)
        elif context:
            id = u"%s\04%s" % (context, id)
        return id

    @classmethod
    def buildfromunit(cls, unit):
        """Build a native unit from a foreign unit, preserving as much
        information as possible."""
        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
            return unit.copy()
        elif isinstance(unit, pocommon.pounit):
            newunit = cls(unit.source)
            newunit.target = unit.target
            #context
            newunit.msgidcomment = unit._extract_msgidcomments()
            if not newunit.msgidcomment:
                newunit.setcontext(unit.getcontext())

            locations = unit.getlocations()
            if locations:
                newunit.addlocations(locations)
            notes = unit.getnotes("developer")
            if notes:
                newunit.addnote(notes, "developer")
            notes = unit.getnotes("translator")
            if notes:
                newunit.addnote(notes, "translator")
            newunit.markfuzzy(unit.isfuzzy())
            if unit.isobsolete():
                newunit.makeobsolete()
            for tc in ['python-format', 'c-format', 'php-format']:
                if unit.hastypecomment(tc):
                    newunit.settypecomment(tc)
                    break
            return newunit
        else:
            return base.TranslationUnit.buildfromunit(unit)


class pofile(pocommon.pofile):
    """A .po file containing various units"""
    UnitClass = pounit

    def changeencoding(self, newencoding):
        """Deprecated: changes the encoding on the file."""
        # This should not be here but in poheader. It also shouldn't mangle the
        # header itself, but use poheader methods. All users are removed, so
        # we can deprecate after one release.
        raise DeprecationWarning

        self._encoding = encodingToUse(newencoding)
        if not self.units:
            return
        header = self.header()
        if not header or header.isblank():
            return
        charsetline = None
        headerstr = header.target
        for line in headerstr.split("\n"):
            if not ":" in line:
                continue
            key, value = line.strip().split(":", 1)
            if key.strip() != "Content-Type":
                continue
            charsetline = line
        if charsetline is None:
            headerstr += "Content-Type: text/plain; charset=%s" % self._encoding
        else:
            charset = re.search("charset=([^ ]*)", charsetline)
            if charset is None:
                newcharsetline = charsetline
                if not newcharsetline.strip().endswith(";"):
                    newcharsetline += ";"
                newcharsetline += " charset=%s" % self._encoding
            else:
                charset = charset.group(1)
                newcharsetline = charsetline.replace("charset=%s" % charset, "charset=%s" % self._encoding, 1)
            headerstr = headerstr.replace(charsetline, newcharsetline, 1)
        header.target = headerstr

    def _build_self_from_cpo(self):
        """Builds up this store from the internal cpo store.

        A user must ensure that self._cpo_store already exists, and that it is
        deleted afterwards."""
        for unit in self._cpo_store.units:
            self.addunit(self.UnitClass.buildfromunit(unit))
        self._encoding = self._cpo_store._encoding

    def _build_cpo_from_self(self):
        """Builds the internal cpo store from the data in self.

        A user must ensure that self._cpo_store does not exist, and should
        delete it after using it."""
        self._cpo_store = cpo.pofile(noheader=True)
        for unit in self.units:
            if not unit.isblank():
                self._cpo_store.addunit(cpo.pofile.UnitClass.buildfromunit(unit, self._encoding))
        if not self._cpo_store.header():
            #only add a temporary header
            self._cpo_store.makeheader(charset=self._encoding, encoding="8bit")

    def parse(self, input):
        """Parses the given file or file source string."""
        try:
            if hasattr(input, 'name'):
                self.filename = input.name
            elif not getattr(self, 'filename', ''):
                self.filename = ''
            tmp_header_added = False
#            if isinstance(input, str) and '"Content-Type: text/plain; charset=' not in input[:200]:
#                input = basic_header + input
#                tmp_header_added = True
            self.units = []
            self._cpo_store = cpo.pofile(input, noheader=True)
            self._build_self_from_cpo()
            del self._cpo_store
            if tmp_header_added:
                self.units = self.units[1:]
        except Exception as e:
            raise base.ParseError(e)

    def removeduplicates(self, duplicatestyle="merge"):
        """Make sure each msgid is unique ; merge comments etc from duplicates into original"""
        # TODO: can we handle consecutive calls to removeduplicates()? What
        # about files already containing msgctxt? - test
        id_dict = {}
        uniqueunits = []
        # TODO: this is using a list as the pos aren't hashable, but this is slow.
        # probably not used frequently enough to worry about it, though.
        markedpos = []

        def addcomment(thepo):
            thepo.msgidcomment = " ".join(thepo.getlocations())
            markedpos.append(thepo)
        for thepo in self.units:
            id = thepo.getid()
            if thepo.isheader() and not thepo.getlocations():
                # header msgids shouldn't be merged...
                uniqueunits.append(thepo)
            elif id in id_dict:
                if duplicatestyle == "merge":
                    if id:
                        id_dict[id].merge(thepo)
                    else:
                        addcomment(thepo)
                        uniqueunits.append(thepo)
                elif duplicatestyle == "msgctxt":
                    origpo = id_dict[id]
                    if origpo not in markedpos and id:
                        # if it doesn't have an id, we already added msgctxt
                        origpo._msgctxt += " ".join(origpo.getlocations())
                        markedpos.append(thepo)
                    thepo._msgctxt += " ".join(thepo.getlocations())
                    uniqueunits.append(thepo)
            else:
                if not id:
                    if duplicatestyle == "merge":
                        addcomment(thepo)
                    else:
                        thepo._msgctxt += u" ".join(thepo.getlocations())
                id_dict[id] = thepo
                uniqueunits.append(thepo)
        self.units = uniqueunits

    def __str__(self):
        """Convert to a string. double check that unicode is handled somehow here"""
        self._cpo_store = cpo.pofile(encoding=self._encoding, noheader=True)
        try:
            self._build_cpo_from_self()
        except UnicodeEncodeError as e:
            self._encoding = "utf-8"
            self.updateheader(add=True, Content_Type="text/plain; charset=UTF-8")
            self._build_cpo_from_self()
        output = str(self._cpo_store)
        del self._cpo_store
        return output

########NEW FILE########
__FILENAME__ = html
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006,2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""module for parsing html files for translation"""

import re

from six.moves import html_parser
from six.moves.html_entities import name2codepoint

from translate.storage import base
from translate.storage.base import ParseError


# Override the piclose tag from simple > to ?> otherwise we consume HTML
# within the processing instructions
html_parser.piclose = re.compile('\?>')


strip_html_re = re.compile(r'''
(?s)^       # We allow newlines, and match start of line
<(?P<tag>[^\s?>]+)  # Match start of tag and the first character (not ? or >)
(?:
  (?:
    [^>]    # Anything that's not a > is valid tag material
      |
    (?:<\?.*?\?>) # Matches <? foo ?> lazily; PHP is valid
  )*        # Repeat over valid tag material
  [^?>]     # If we have > 1 char, the last char can't be ? or >
)?          # The repeated chars are optional, so that <a>, <p> work
>           # Match ending > of opening tag

(.*)        # Match actual contents of tag

</(?P=tag)>   # Match ending tag; can't end with ?> and must be >=1 char
$           # Match end of line
''', re.VERBOSE)


def strip_html(text):
    """Strip unnecessary html from the text.

    HTML tags are deemed unnecessary if it fully encloses the translatable
    text, eg. '<a href="index.html">Home Page</a>'.

    HTML tags that occurs within the normal flow of text will not be removed,
    eg. 'This is a link to the <a href="index.html">Home Page</a>.'
    """
    text = text.strip()

    # If all that is left is PHP, return ""
    result = re.findall('(?s)^<\?.*?\?>$', text)
    if len(result) == 1:
        return ""

    result = strip_html_re.findall(text)
    if len(result) == 1:
        text = strip_html(result[0][1])
    return text


normalize_re = re.compile("\s\s+")


def normalize_html(text):
    """Remove double spaces from HTML snippets"""
    return normalize_re.sub(" ", text)


def safe_escape(html):
    """Escape &, < and >"""
    # FIXME we need to relook at these.  Escaping to cleanup htmlentity codes
    # is important but we can't mix "<code>&lt;".  In these cases we should
    # then abort the escaping
    return re.sub("&(?![a-zA-Z0-9]+;)", "&amp;", html)


class htmlunit(base.TranslationUnit):
    """A unit of translatable/localisable HTML content"""

    def __init__(self, source=None):
        self.locations = []
        self.setsource(source)

    def getsource(self):
        #TODO: Rethink how clever we should try to be with html entities.
        text = self._text.replace("&amp;", "&")
        text = text.replace("\r\n", " ").replace("\n", " ").replace("\r", " ")
        return text

    def setsource(self, source):
        self._rich_source = None
        self._text = safe_escape(source)
    source = property(getsource, setsource)

    def addlocation(self, location):
        self.locations.append(location)

    def getlocations(self):
        return self.locations


class htmlfile(html_parser.HTMLParser, base.TranslationStore):
    UnitClass = htmlunit

    MARKINGTAGS = [
        "address",
        "caption",
        "div",
        "dt", "dd",
        "figcaption",
        "h1", "h2", "h3", "h4", "h5", "h6",
        "li",
        "p",
        "pre",
        "title",
        "th", "td",
    ]
    """Text in these tags that will be extracted from the HTML document"""

    MARKINGATTRS = []
    """Text from tags with these attributes will be extracted from the HTML
    document"""

    INCLUDEATTRS = [
        "alt",
        "abbr",
        "content",
        "standby",
        "summary",
        "title"
    ]
    """Text from these attributes are extracted"""

    SELF_CLOSING_TAGS = [
        u"area",
        u"base",
        u"basefont",
        u"br",
        u"col",
        u"frame",
        u"hr",
        u"img",
        u"input",
        u"link",
        u"meta",
        u"param",
    ]
    """HTML self-closing tags.  Tags that should be specified as <img /> but
    might be <img>.
    `Reference <http://learnwebsitemaking.com/htmlselfclosingtags.html>`_"""

    def __init__(self, includeuntaggeddata=None, inputfile=None,
                 callback=None):
        self.units = []
        self.filename = getattr(inputfile, 'name', None)
        self.currentblock = u""
        self.currentcomment = u""
        self.currenttag = None
        self.currentpos = -1
        self.tag_path = []
        self.filesrc = u""
        self.currentsrc = u""
        self.pidict = {}
        if callback is None:
            self.callback = self._simple_callback
        else:
            self.callback = callback
        self.includeuntaggeddata = includeuntaggeddata
        html_parser.HTMLParser.__init__(self)

        if inputfile is not None:
            htmlsrc = inputfile.read()
            inputfile.close()
            self.parse(htmlsrc)

    def _simple_callback(self, string):
        return string

    ENCODING_RE = re.compile('''<meta.*
                                content.*=.*?charset.*?=\s*?
                                ([^\s]*)
                                \s*?["']\s*?>
                             ''', re.VERBOSE | re.IGNORECASE)

    def guess_encoding(self, htmlsrc):
        """Returns the encoding of the html text.

        We look for 'charset=' within a meta tag to do this.
        """

        result = self.ENCODING_RE.findall(htmlsrc)
        encoding = None
        if result:
            encoding = result[0]
        return encoding

    def do_encoding(self, htmlsrc):
        """Return the html text properly encoded based on a charset."""
        charset = self.guess_encoding(htmlsrc)
        if charset:
            return htmlsrc.decode(charset)
        else:
            return htmlsrc.decode('utf-8')

    def pi_escape(self, text):
        """Replaces all instances of process instruction with placeholders,
        and returns the new text and a dictionary of tags.  The current
        implementation replaces <?foo?> with <?md5(foo)?>.  The hash => code
        conversions are stored in self.pidict for later use in restoring the
        real PHP.

        The purpose of this is to remove all potential "tag-like" code from
        inside PHP.  The hash looks nothing like an HTML tag, but the following
        PHP::
          $a < $b ? $c : ($d > $e ? $f : $g)
        looks like it contains an HTML tag::
          < $b ? $c : ($d >
        to nearly any regex.  Hence, we replace all contents of PHP with simple
        strings to help our regexes out.

        """
        result = re.findall('(?s)<\?(.*?)\?>', text)
        for pi in result:
            pi_escaped = pi.replace("<", "%lt;").replace(">", "%gt;")
            self.pidict[pi_escaped] = pi
            text = text.replace(pi, pi_escaped)
        return text

    def pi_unescape(self, text):
        """Replaces the PHP placeholders in text with the real code"""
        for pi_escaped, pi in self.pidict.items():
            text = text.replace(pi_escaped, pi)
        return text

    def parse(self, htmlsrc):
        htmlsrc = self.do_encoding(htmlsrc)
        htmlsrc = self.pi_escape(htmlsrc)  # Clear out the PHP before parsing
        self.feed(htmlsrc)

    def addhtmlblock(self, text):
        text = strip_html(text)
        text = self.pi_unescape(text)  # Before adding anything, restore PHP
        text = normalize_html(text)
        if self.has_translatable_content(text):
            unit = self.addsourceunit(text)
            unit.addlocation("%s+%s:%d" %
                              (self.filename, ".".join(self.tag_path),
                               self.currentpos))
            unit.addnote(self.currentcomment)

    def has_translatable_content(self, text):
        """Check if the supplied HTML snippet has any content that needs to be
        translated."""

        text = text.strip()
        result = re.findall('(?i).*(charset.*=.*)', text)
        if len(result) == 1:
            return False

        # TODO: Get a better way to find untranslatable entities.
        if text == '&nbsp;':
            return False

        pattern = '<\?.*?\?>'  # Lazily strip all PHP
        result = re.sub(pattern, '', text).strip()
        pattern = '<[^>]*>'  # Strip all HTML tags
        result = re.sub(pattern, '', result).strip()
        if result:
            return True
        else:
            return False

    def buildtag(self, tag, attrs=None, startend=False):
        """Create an HTML tag"""
        selfclosing = u""
        if startend:
            selfclosing = u" /"
        if attrs != [] and attrs is not None:
            return u"<%(tag)s %(attrs)s%(selfclosing)s>" % \
                    {"tag": tag,
                     "attrs": " ".join(['%s="%s"' % pair for pair in attrs]),
                     "selfclosing": selfclosing}
        else:
            return u"<%(tag)s%(selfclosing)s>" % {"tag": tag,
                                                  "selfclosing": selfclosing}

#From here on below, follows the methods of the HTMLParser

    def startblock(self, tag, attrs=None):
        self.addhtmlblock(self.currentblock)
        if self.callback(normalize_html(strip_html(self.currentsrc))):
            self.filesrc += self.currentsrc.replace(strip_html(self.currentsrc),
                                                    self.callback(normalize_html(strip_html(self.currentsrc)).replace("\n", " ")))
        else:
            self.filesrc += self.currentsrc
        self.currentblock = ""
        self.currentcomment = ""
        self.currenttag = tag
        self.currentpos = self.getpos()[0]
        self.currentsrc = self.buildtag(tag, attrs)

    def endblock(self):
        self.addhtmlblock(self.currentblock)
        if self.callback(normalize_html(strip_html(self.currentsrc))) is not None:
            self.filesrc += self.currentsrc.replace(strip_html(self.currentsrc),
                                                    self.callback(normalize_html(strip_html(self.currentsrc).replace("\n", " "))))
        else:
            self.filesrc += self.currentsrc
        self.currentblock = ""
        self.currentcomment = ""
        self.currenttag = None
        self.currentpos = -1
        self.currentsrc = ""

    def handle_starttag(self, tag, attrs):
        newblock = False
        if self.tag_path != [] \
           and self.tag_path[-1:][0] in self.SELF_CLOSING_TAGS:
            self.tag_path.pop()
        self.tag_path.append(tag)
        if tag in self.MARKINGTAGS:
            newblock = True
        for i, attr in enumerate(attrs):
            attrname, attrvalue = attr
            if attrname in self.MARKINGATTRS:
                newblock = True
            if attrname in self.INCLUDEATTRS and self.currentblock == "":
                self.addhtmlblock(attrvalue)
                attrs[i] = (attrname,
                            self.callback(normalize_html(attrvalue).replace("\n", " ")))

        if newblock:
            self.startblock(tag, attrs)
        elif self.currenttag is not None:
            self.currentblock += self.get_starttag_text()
            self.currentsrc += self.get_starttag_text()
        else:
            self.filesrc += self.buildtag(tag, attrs)

    def handle_startendtag(self, tag, attrs):
        for i, attr in enumerate(attrs):
            attrname, attrvalue = attr
            if attrname in self.INCLUDEATTRS and self.currentblock == "":
                self.addhtmlblock(attrvalue)
                attrs[i] = (attrname,
                            self.callback(normalize_html(attrvalue).replace("\n", " ")))
        if self.currenttag is not None:
            self.currentblock += self.get_starttag_text()
            self.currentsrc += self.get_starttag_text()
        else:
            self.filesrc += self.buildtag(tag, attrs, startend=True)

    def handle_endtag(self, tag):
        if tag == self.currenttag:
            self.currentsrc += "</%(tag)s>" % {"tag": tag}
            self.endblock()
        elif self.currenttag is not None:
            self.currentblock += '</%s>' % tag
            self.currentsrc += '</%s>' % tag
        else:
            self.filesrc += '</%s>' % tag
        try:
            popped = self.tag_path.pop()
        except IndexError:
            raise ParseError("Mismatched tags: no more tags: line %s" %
                             self.getpos()[0])
        while popped in self.SELF_CLOSING_TAGS:
            popped = self.tag_path.pop()
        if popped != tag:
            raise ParseError("Mismatched closing tag: "
                             "expected '%s' got '%s' at line %s" %
                             (popped, tag, self.getpos()[0]))

    def handle_data(self, data):
        if self.currenttag is not None:
            self.currentblock += data
            self.currentsrc += self.callback(data)
        elif self.includeuntaggeddata:
            self.startblock(None)
            self.currentblock += data
            self.currentsrc += data
        else:
            self.filesrc += self.callback(data)

    def handle_charref(self, name):
        """Handle entries in the form &#NNNN; e.g. &#8417;"""
        self.handle_data(unichr(int(name)))

    def handle_entityref(self, name):
        """Handle named entities of the form &aaaa; e.g. &rsquo;"""
        if name in ['gt', 'lt', 'amp']:
            self.handle_data("&%s;" % name)
        else:
            self.handle_data(unichr(name2codepoint.get(name, u"&%s;" % name)))

    def handle_comment(self, data):
        # we can place comments above the msgid as translator comments!
        if self.currentcomment == "":
            self.currentcomment = data
        else:
            self.currentcomment += u'\n' + data
        self.filesrc += "<!--%s-->" % data

    def handle_pi(self, data):
        self.handle_data("<?%s?>" % self.pi_unescape(data))


class POHTMLParser(htmlfile):
    pass

########NEW FILE########
__FILENAME__ = ical
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Class that manages iCalender files for translation.

iCalendar files follow the `RFC2445 <http://tools.ietf.org/html/rfc2445>`_
specification.

The iCalendar specification uses the following naming conventions:

- Component: an event, journal entry, timezone, etc
- Property: a property of a component: summary, description, start time, etc
- Attribute: an attribute of a property, e.g. language

The following are localisable in this implementation:

- VEVENT component: SUMMARY, DESCRIPTION, COMMENT and LOCATION properties

While other items could be localised this is not seen as important until use
cases arise.  In such a case simply adjusting the component.name and
property.name lists to include these will allow expanded localisation.

LANGUAGE Attribute
    While the iCalendar format allows items to have a language attribute this is
    not used. The reason being that for most of the items that we localise they
    are only allowed to occur zero or once.  Thus 'summary' would ideally
    be present in multiple languages in one file, the format does not allow
    such multiple entries.  This is unfortunate as it prevents the creation
    of a single multilingual iCalendar file.

Future Format Support
    As this format used `vobject <http://vobject.skyhouseconsulting.com/>`_
    which supports various formats including
    `vCard <http://en.wikipedia.org/wiki/VCard>`_ it is possible to expand
    this format to understand those if needed.
"""

import re
from cStringIO import StringIO

import vobject

from translate.storage import base


class icalunit(base.TranslationUnit):
    """An ical entry that is translatable"""

    def __init__(self, source=None, encoding="UTF-8"):
        self.location = ""
        if source:
            self.source = source
        super(icalunit, self).__init__(source)

    def addlocation(self, location):
        self.location = location

    def getlocations(self):
        return [self.location]


class icalfile(base.TranslationStore):
    """An ical file"""
    UnitClass = icalunit

    def __init__(self, inputfile=None, unitclass=icalunit):
        """construct an ical file, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.units = []
        self.filename = ''
        self._icalfile = None
        if inputfile is not None:
            self.parse(inputfile)

    def __str__(self):
        _outicalfile = self._icalfile
        for unit in self.units:
            for location in unit.getlocations():
                match = re.match('\\[(?P<uid>.+)\\](?P<property>.+)', location)
                for component in self._icalfile.components():
                    if component.name != "VEVENT":
                        continue
                    if component.uid.value != match.groupdict()['uid']:
                        continue
                    for property in component.getChildren():
                        if property.name == match.groupdict()['property']:
                            property.value = unit.target

        if _outicalfile:
            return str(_outicalfile.serialize())
        else:
            return ""

    def parse(self, input):
        """parse the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            inisrc = input.read()
            input.close()
            input = inisrc
        if isinstance(input, str):
            input = StringIO(input)
            self._icalfile = vobject.readComponents(input).next()
        else:
            self._icalfile = vobject.readComponents(open(input)).next()
        for component in self._icalfile.components():
            if component.name == "VEVENT":
                for property in component.getChildren():
                    if property.name in ('SUMMARY', 'DESCRIPTION', 'COMMENT', 'LOCATION'):
                        newunit = self.addsourceunit(property.value)
                        newunit.addnote("Start date: %s" % component.dtstart.value)
                        newunit.addlocation("[%s]%s" % (component.uid.value, property.name))

########NEW FILE########
__FILENAME__ = ini
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007,2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Class that manages .ini files for translation

.. note::: A simple summary of what is permissible follows.

# a comment
; a comment

[Section]
a = a string
b : a string
"""

import re
from cStringIO import StringIO

from iniparse import INIConfig

from translate.storage import base


dialects = {}


def register_dialect(dialect):
    """Decorator that registers the dialect."""
    dialects[dialect.name] = dialect
    return dialect


class Dialect(object):
    """Base class for differentiating dialect options and functions"""
    name = None


@register_dialect
class DialectDefault(Dialect):
    name = 'default'

    def unescape(self, text):
        return text

    def escape(self, text):
        return text.encode('utf-8')


@register_dialect
class DialectInno(DialectDefault):
    name = 'inno'

    def unescape(self, text):
        return text.replace("%n", "\n").replace("%t", "\t")

    def escape(self, text):
        return text.replace("\t", "%t").replace("\n", "%n").encode('utf-8')


class iniunit(base.TranslationUnit):
    """A INI file entry"""

    def __init__(self, source=None, encoding="UTF-8"):
        self.location = ""
        if source:
            self.source = source
        super(iniunit, self).__init__(source)

    def addlocation(self, location):
        self.location = location

    def getlocations(self):
        return [self.location]


class inifile(base.TranslationStore):
    """An INI file"""
    UnitClass = iniunit

    def __init__(self, inputfile=None, unitclass=iniunit, dialect="default"):
        """construct an INI file, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        self._dialect = dialects.get(dialect, DialectDefault)()  # fail correctly/use getattr/
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.units = []
        self.filename = ''
        self._inifile = None
        if inputfile is not None:
            self.parse(inputfile)

    def __str__(self):
        _outinifile = self._inifile
        for unit in self.units:
            for location in unit.getlocations():
                match = re.match('\\[(?P<section>.+)\\](?P<entry>.+)', location)
                _outinifile[match.groupdict()['section']][match.groupdict()['entry']] = self._dialect.escape(unit.target)
        if _outinifile:
            return str(_outinifile)
        else:
            return ""

    def parse(self, input):
        """Parse the given file or file source string."""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            inisrc = input.read()
            input.close()
            input = inisrc

        if isinstance(input, str):
            input = StringIO(input)
            self._inifile = INIConfig(input, optionxformvalue=None)
        else:
            self._inifile = INIConfig(file(input), optionxformvalue=None)

        for section in self._inifile:
            for entry in self._inifile[section]:
                source = self._dialect.unescape(self._inifile[section][entry])
                newunit = self.addsourceunit(source)
                newunit.addlocation("[%s]%s" % (section, entry))

########NEW FILE########
__FILENAME__ = jsonl10n
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007,2009-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

r"""Class that manages JSON data files for translation

JSON is an acronym for JavaScript Object Notation, it is an open standard
designed for human-readable data interchange.

JSON basic types:

- Number (integer or real)
- String (double-quoted Unicode with backslash escaping)
- Boolean (true or false)
- Array (an ordered sequence of values, comma-separated and enclosed in square
  brackets)
- Object (a collection of key:value pairs, comma-separated and enclosed in
  curly braces)
- null

Example:

.. code-block:: json

   {
        "firstName": "John",
        "lastName": "Smith",
        "age": 25,
        "address": {
            "streetAddress": "21 2nd Street",
            "city": "New York",
            "state": "NY",
            "postalCode": "10021"
        },
        "phoneNumber": [
            {
              "type": "home",
              "number": "212 555-1234"
            },
            {
              "type": "fax",
              "number": "646 555-4567"
            }
        ]
   }


TODO:

- Handle ``\u`` and other escapes in Unicode
- Manage data type storage and conversion. True --> "True" --> True
- Sort the extracted data to the order of the JSON file

"""

import json
import os
from cStringIO import StringIO

from translate.storage import base


class JsonUnit(base.TranslationUnit):
    """A JSON entry"""

    def __init__(self, source=None, ref=None, item=None, encoding="UTF-8"):
        self._id = None
        self._item = str(os.urandom(30))
        if item is not None:
            self._item = item
        self._ref = {}
        if ref is not None:
            self._ref = ref
        if ref is None and item is None:
            self._ref[self._item] = ""
        if source:
            self.source = source
        super(JsonUnit, self).__init__(source)

    def getsource(self):
        return self.gettarget()

    def setsource(self, source):
        self.settarget(source)
    source = property(getsource, setsource)

    def gettarget(self):

        def change_type(value):
            if isinstance(value, bool):
                return str(value)
            return value

            return newvalue
        if isinstance(self._ref, list):
            return change_type(self._ref[self._item])
        elif isinstance(self._ref, dict):
            return change_type(self._ref[self._item])

    def settarget(self, target):

        def change_type(oldvalue, newvalue):
            if isinstance(oldvalue, bool):
                newvalue = bool(newvalue)
            return newvalue

        if isinstance(self._ref, list):
            self._ref[int(self._item)] = change_type(self._ref[int(self._item)],
                                                     target)
        elif isinstance(self._ref, dict):
            self._ref[self._item] = change_type(self._ref[self._item], target)
        else:
            raise ValueError("We don't know how to handle:\n"
                             "Type: %s\n"
                             "Value: %s" % (type(self._ref), target))
    target = property(gettarget, settarget)

    def setid(self, value):
        self._id = value

    def getid(self):
        return self._id

    def getlocations(self):
        return [self.getid()]


class JsonFile(base.TranslationStore):
    """A JSON file"""
    UnitClass = JsonUnit

    def __init__(self, inputfile=None, unitclass=UnitClass, filter=None):
        """construct a JSON file, optionally reading in from inputfile."""
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self._filter = filter
        self.filename = ''
        self._file = u''
        if inputfile is not None:
            self.parse(inputfile)

    def __str__(self):
        return json.dumps(self._file, sort_keys=True,
                          indent=4, ensure_ascii=False).encode('utf-8')

    def _extract_translatables(self, data, stop=None, prev="", name_node=None,
                               name_last_node=None, last_node=None):
        """Recursive function to extract items from the data files

        :param data: the current branch to walk down
        :param stop: a list of leaves to extract or None to extract everything
        :param prev: the heirarchy of the tree at this iteration
        :param name_node:
        :param name_last_node: the name of the last node
        :param last_node: the last list or dict
        """
        if isinstance(data, dict):
            for k, v in data.iteritems():
                for x in self._extract_translatables(v, stop,
                                                          "%s.%s" % (prev, k),
                                                          k, None, data):
                    yield x
        elif isinstance(data, list):
            for i, item in enumerate(data):
                for x in self._extract_translatables(item, stop,
                                                          "%s[%s]" % (prev, i),
                                                          i, name_node, data):
                    yield x
        # apply filter
        elif (stop is None or
              (isinstance(last_node, dict) and name_node in stop) or
              (isinstance(last_node, list) and name_last_node in stop)):

            if isinstance(data, str) or isinstance(data, unicode):
                yield (prev, data, last_node, name_node)
            elif isinstance(data, bool):
                yield (prev, str(data), last_node, name_node)
            elif data is None:
                pass
            else:
                raise ValueError("We don't handle these values:\n"
                                 "Type: %s\n"
                                 "Data: %s\n"
                                 "Previous: %s" % (type(data), data, prev))

    def parse(self, input):
        """parse the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            src = input.read()
            input.close()
            input = src
        if isinstance(input, str):
            input = StringIO(input)
        try:
            self._file = json.load(input)
        except ValueError as e:
            raise base.ParseError(e.message)

        for k, data, ref, item in self._extract_translatables(self._file,
                                                stop=self._filter):
            unit = self.UnitClass(data, ref, item)
            unit.setid(k)
            self.addunit(unit)

########NEW FILE########
__FILENAME__ = lisa
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Parent class for LISA standards (TMX, TBX, XLIFF)"""

try:
    from lxml import etree
    from translate.misc.xml_helpers import (getText, getXMLlang, getXMLspace,
                                            namespaced, setXMLlang, setXMLspace)
except ImportError as e:
    raise ImportError("lxml is not installed. It might be possible to continue without support for XML formats.")

from translate.lang import data
from translate.storage import base


class LISAunit(base.TranslationUnit):
    """
    A single unit in the file.  Provisional work is done to make several
    languages possible.
    """

    #The name of the root element of this unit type:(termEntry, tu, trans-unit)
    rootNode = ""
    # The name of the per language element of this unit type:(termEntry, tu,
    # trans-unit)
    languageNode = ""
    #The name of the innermost element of this unit type:(term, seg)
    textNode = ""

    namespace = None
    _default_xml_space = "preserve"
    """The default handling of spacing in the absense of an xml:space
    attribute.

    This is mostly for correcting XLIFF behaviour."""

    def __init__(self, source, empty=False, **kwargs):
        """Constructs a unit containing the given source string"""
        self._rich_source = None
        self._rich_target = None
        if empty:
            self._state_n = 0
            return
        self.xmlelement = etree.Element(self.namespaced(self.rootNode))
        #add descrip, note, etc.
        super(LISAunit, self).__init__(source)

    def __eq__(self, other):
        """Compares two units"""
        if not isinstance(other, LISAunit):
            return super(LISAunit, self).__eq__(other)
        languageNodes = self.getlanguageNodes()
        otherlanguageNodes = other.getlanguageNodes()
        if len(languageNodes) != len(otherlanguageNodes):
            return False
        for i in range(len(languageNodes)):
            mytext = self.getNodeText(languageNodes[i],
                                      getXMLspace(self.xmlelement,
                                                  self._default_xml_space))
            othertext = other.getNodeText(otherlanguageNodes[i],
                                          getXMLspace(self.xmlelement,
                                                      self._default_xml_space))
            if mytext != othertext:
                #TODO:^ maybe we want to take children and notes into account
                return False
        return True

    def namespaced(self, name):
        """Returns name in Clark notation.

        For example ``namespaced("source")`` in an XLIFF document
        might return::

            {urn:oasis:names:tc:xliff:document:1.1}source

        This is needed throughout lxml.
        """
        return namespaced(self.namespace, name)

    def set_source_dom(self, dom_node):
        languageNodes = self.getlanguageNodes()
        if len(languageNodes) > 0:
            self.xmlelement.replace(languageNodes[0], dom_node)
        else:
            self.xmlelement.append(dom_node)

    def get_source_dom(self):
        return self.getlanguageNode(lang=None, index=0)
    source_dom = property(get_source_dom, set_source_dom)

    def setsource(self, text, sourcelang='en'):
        if self._rich_source is not None:
            self._rich_source = None
        text = data.forceunicode(text)
        self.source_dom = self.createlanguageNode(sourcelang, text, "source")

    def getsource(self):
        return self.getNodeText(self.source_dom,
                                getXMLspace(self.xmlelement,
                                            self._default_xml_space))
    source = property(getsource, setsource)

    def set_target_dom(self, dom_node, append=False):
        languageNodes = self.getlanguageNodes()
        assert len(languageNodes) > 0
        if dom_node is not None:
            if append or len(languageNodes) == 0:
                self.xmlelement.append(dom_node)
            else:
                self.xmlelement.insert(1, dom_node)
        if not append and len(languageNodes) > 1:
            self.xmlelement.remove(languageNodes[1])

    def get_target_dom(self, lang=None):
        if lang:
            return self.getlanguageNode(lang=lang)
        else:
            return self.getlanguageNode(lang=None, index=1)
    target_dom = property(get_target_dom)

    def settarget(self, text, lang='xx', append=False):
        """Sets the "target" string (second language), or alternatively
        appends to the list"""
        #XXX: we really need the language - can't really be optional, and we
        # need to propagate it
        if self._rich_target is not None:
            self._rich_target = None
        text = data.forceunicode(text)
        # Firstly deal with reinitialising to None or setting to identical
        # string
        if self.gettarget() == text:
            return
        languageNode = self.get_target_dom(None)
        if not text is None:
            if languageNode is None:
                languageNode = self.createlanguageNode(lang, text, "target")
                self.set_target_dom(languageNode, append)
            else:
                if self.textNode:
                    terms = languageNode.iter(self.namespaced(self.textNode))
                    try:
                        languageNode = terms.next()
                    except StopIteration as e:
                        pass
                languageNode.text = text
        else:
            self.set_target_dom(None, False)

    def gettarget(self, lang=None):
        """retrieves the "target" text (second entry), or the entry in the
        specified language, if it exists"""
        return self.getNodeText(self.get_target_dom(lang),
                                getXMLspace(self.xmlelement,
                                            self._default_xml_space))
    target = property(gettarget, settarget)

    def createlanguageNode(self, lang, text, purpose=None):
        """Returns a xml Element setup with given parameters to represent a
        single language entry. Has to be overridden."""
        return None

    def createPHnodes(self, parent, text):
        """Create the text node in parent containing all the ph tags"""
        matches = _getPhMatches(text)
        if not matches:
            parent.text = text
            return

        # Now we know there will definitely be some ph tags
        start = matches[0].start()
        pretext = text[:start]
        if pretext:
            parent.text = pretext
        lasttag = parent
        for i, m in enumerate(matches):
            #pretext
            pretext = text[start:m.start()]
            # this will never happen with the first ph tag
            if pretext:
                lasttag.tail = pretext
            #ph node
            phnode = etree.SubElement(parent, self.namespaced("ph"))
            phnode.set("id", str(i + 1))
            phnode.text = m.group()
            lasttag = phnode
            start = m.end()
        #post text
        if text[start:]:
            lasttag.tail = text[start:]

    def getlanguageNodes(self):
        """Returns a list of all nodes that contain per language information.
        """
        return list(self.xmlelement.iterchildren(self.namespaced(self.languageNode)))

    def getlanguageNode(self, lang=None, index=None):
        """Retrieves a :attr:`languageNode` either by language or by index."""
        if lang is None and index is None:
            raise KeyError("No criteria for languageNode given")
        languageNodes = self.getlanguageNodes()
        if lang:
            for set in languageNodes:
                if getXMLlang(set) == lang:
                    return set
        else:  # have to use index
            if index >= len(languageNodes):
                return None
            else:
                return languageNodes[index]
        return None

    def getNodeText(self, languageNode, xml_space="preserve"):
        """Retrieves the term from the given :attr:`languageNode`."""
        if languageNode is None:
            return None
        if self.textNode:
            terms = languageNode.iterdescendants(self.namespaced(self.textNode))
            if terms is None:
                return None
            try:
                return getText(terms.next(), xml_space)
            except StopIteration:
                # didn't have the structure we expected
                return None
        else:
            return getText(languageNode, xml_space)

    def __str__(self):
        return etree.tostring(self.xmlelement, pretty_print=True,
                              encoding='utf-8')

    def _set_property(self, name, value):
        self.xmlelement.attrib[name] = value

    xid = property(lambda self: self.xmlelement.attrib[self.namespaced('xid')],
                   lambda self, value: self._set_property(self.namespaced('xid'), value))

    rid = property(lambda self: self.xmlelement.attrib[self.namespaced('rid')],
                   lambda self, value: self._set_property(self.namespaced('rid'), value))

    @classmethod
    def createfromxmlElement(cls, element):
        term = cls(None, empty=True)
        term.xmlelement = element
        return term


class LISAfile(base.TranslationStore):
    """A class representing a file store for one of the LISA file formats."""
    UnitClass = LISAunit
    #The root node of the XML document:
    rootNode = ""
    #The root node of the content section:
    bodyNode = ""
    #The XML skeleton to use for empty construction:
    XMLskeleton = ""

    namespace = None

    def __init__(self, inputfile=None, sourcelanguage='en',
                 targetlanguage=None, unitclass=None):
        super(LISAfile, self).__init__(unitclass=unitclass)
        if inputfile is not None:
            self.parse(inputfile)
            assert self.document.getroot().tag == self.namespaced(self.rootNode)
        else:
            # We strip out newlines to ensure that spaces in the skeleton
            # doesn't interfere with the the pretty printing of lxml
            self.parse(self.XMLskeleton.replace("\n", ""))
            self.setsourcelanguage(sourcelanguage)
            self.settargetlanguage(targetlanguage)
            self.addheader()
        self._encoding = "UTF-8"

    def addheader(self):
        """Method to be overridden to initialise headers, etc."""
        pass

    def namespaced(self, name):
        """Returns name in Clark notation.

        For example ``namespaced("source")`` in an XLIFF document
        might return::

            {urn:oasis:names:tc:xliff:document:1.1}source

        This is needed throughout lxml.
        """
        return namespaced(self.namespace, name)

    def initbody(self):
        """Initialises self.body so it never needs to be retrieved from the
        XML again."""
        self.namespace = self.document.getroot().nsmap.get(None, None)
        self.body = self.document.find('//%s' % self.namespaced(self.bodyNode))

    def addsourceunit(self, source):
        """Adds and returns a new unit with the given string as first entry."""
        newunit = self.UnitClass(source)
        self.addunit(newunit)
        return newunit

    def addunit(self, unit, new=True):
        unit.namespace = self.namespace
        super(LISAfile, self).addunit(unit)
        if new:
            self.body.append(unit.xmlelement)

    def __str__(self):
        """Converts to a string containing the file's XML"""
        return etree.tostring(self.document, pretty_print=True,
                              xml_declaration=True, encoding='utf-8')

    def parse(self, xml):
        """Populates this object from the given xml string"""
        if not hasattr(self, 'filename'):
            self.filename = getattr(xml, 'name', '')
        if hasattr(xml, "read"):
            xml.seek(0)
            posrc = xml.read()
            xml = posrc
        parser = etree.XMLParser(strip_cdata=False)
        self.document = etree.fromstring(xml, parser).getroottree()
        self._encoding = self.document.docinfo.encoding
        self.initbody()
        assert self.document.getroot().tag == self.namespaced(self.rootNode)
        for entry in self.document.getroot().iterdescendants(self.namespaced(self.UnitClass.rootNode)):
            term = self.UnitClass.createfromxmlElement(entry)
            self.addunit(term, new=False)

########NEW FILE########
__FILENAME__ = mo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# the function "__str__" was derived from Python v2.4
#       (Tools/i18n/msgfmt.py - function "generate"):
#   Written by Martin v. Lwis <loewis@informatik.hu-berlin.de>
#   Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
#   All rights reserved.
#   original license: Python Software Foundation (version 2)
#
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Module for parsing Gettext .mo files for translation.

The coding of .mo files was produced from `Gettext documentation
<http://www.gnu.org/software/gettext/manual/gettext.html#MO-Files>`_,
Pythons msgfmt.py and by observing and testing existing .mo files in the wild.

The hash algorithm is implemented for MO files, this should result in
faster access of the MO file.  The hash is optional for Gettext
and is not needed for reading or writing MO files, in this implementation
it is always on and does produce sometimes different results to Gettext
in very small files.
"""

import array
import re
import struct

from translate.misc.multistring import multistring
from translate.storage import base, po, poheader


MO_MAGIC_NUMBER = 0x950412de


def mounpack(filename='messages.mo'):
    """Helper to unpack Gettext MO files into a Python string"""
    f = open(filename)
    s = f.read()
    print "\\x%02x" * len(s) % tuple(map(ord, s))
    f.close()


def my_swap4(result):
    c0 = (result >> 0) & 0xff
    c1 = (result >> 8) & 0xff
    c2 = (result >> 16) & 0xff
    c3 = (result >> 24) & 0xff

    return (c0 << 24) | (c1 << 16) | (c2 << 8) | c3


def hashpjw(str_param):
    HASHWORDBITS = 32
    hval = 0
    g = None
    s = str_param
    for s in str_param:
        hval = hval << 4
        hval += ord(s)
        g = hval & 0xf << (HASHWORDBITS - 4)
        if (g != 0):
            hval = hval ^ g >> (HASHWORDBITS - 8)
            hval = hval ^ g
    return hval


def get_next_prime_number(start):
    # find the smallest prime number that is greater or equal "start"

    def is_prime(num):
        # special small numbers
        if (num < 2) or (num == 4):
            return False
        if (num == 2) or (num == 3):
            return True
        # check for numbers > 4
        for divider in range(2, num / 2):
            if num % divider == 0:
                return False
        return True

    candidate = start
    while not is_prime(candidate):
        candidate += 1
    return candidate


class mounit(base.TranslationUnit):
    """A class representing a .mo translation message."""

    def __init__(self, source=None, encoding=None):
        #Since the units are really dumb, we ignore encoding for now
        self.msgctxt = []
        self.msgidcomments = []
        super(mounit, self).__init__(source)

    def getcontext(self):
        """Get the message context"""
        # Still need to handle KDE comments
        if self.msgctxt is None:
            return None
        return "".join(self.msgctxt)

    def isheader(self):
        """Is this a header entry?"""
        return self.source == u""

    def istranslatable(self):
        """Is this message translateable?"""
        return bool(self.source)


class mofile(poheader.poheader, base.TranslationStore):
    """A class representing a .mo file."""
    UnitClass = mounit
    Name = "Gettext MO file"
    Mimetypes = ["application/x-gettext-catalog", "application/x-mo"]
    Extensions = ["mo", "gmo"]
    _binary = True

    def __init__(self, inputfile=None, unitclass=mounit):
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self._encoding = "UTF-8"
        if inputfile is not None:
            self.parsestring(inputfile)

    def __str__(self):
        """Output a string representation of the MO data file"""
        # check the header of this file for the copyright note of this function

        def add_to_hash_table(string, i):
            V = hashpjw(string)
            # Taken from gettext-0.17:gettext-tools/src/write-mo.c:408-409
            S = hash_size <= 2 and 3 or hash_size
            hash_cursor = V % S
            orig_hash_cursor = hash_cursor
            increment = 1 + (V % (S - 2))
            while True:
                index = hash_table[hash_cursor]
                if (index == 0):
                    hash_table[hash_cursor] = i + 1
                    break
                hash_cursor += increment
                hash_cursor = hash_cursor % S
                assert (hash_cursor != orig_hash_cursor)

        # hash_size should be the smallest prime number that is greater
        # or equal (4 / 3 * N) - where N is the number of keys/units.
        # see gettext-0.17:gettext-tools/src/write-mo.c:406
        hash_size = get_next_prime_number(int((len(self.units) * 4) / 3))
        if hash_size <= 2:
            hash_size = 3
        MESSAGES = {}
        for unit in self.units:
            # If the unit is not translated, we should rather omit it entirely
            if not unit.istranslated():
                continue
            if isinstance(unit.source, multistring):
                source = "".join(unit.msgidcomments) + \
                         "\0".join(unit.source.strings)
            else:
                source = "".join(unit.msgidcomments) + unit.source
            if unit.msgctxt:
                source = "".join(unit.msgctxt) + "\x04" + source
            if isinstance(unit.target, multistring):
                target = "\0".join(unit.target.strings)
            else:
                target = unit.target
            if unit.target:
                MESSAGES[source.encode("utf-8")] = target
        # using "I" works for 32- and 64-bit systems, but not for 16-bit!
        hash_table = array.array("I", [0] * hash_size)
        keys = MESSAGES.keys()
        # the keys are sorted in the .mo file
        keys.sort()
        offsets = []
        ids = strs = ''
        for i, id in enumerate(keys):
            # For each string, we need size and file offset.  Each string is
            # NUL terminated; the NUL does not count into the size.
            # TODO: We don't do any encoding detection from the PO Header
            add_to_hash_table(id, i)
            string = MESSAGES[id]  # id already encoded for use as dictionary key
            if isinstance(string, unicode):
                string = string.encode('utf-8')
            offsets.append((len(ids), len(id), len(strs), len(string)))
            ids = ids + id + '\0'
            strs = strs + string + '\0'
        output = ''
        # The header is 7 32-bit unsigned integers
        keystart = 7 * 4 + 16 * len(keys) + hash_size * 4
        # and the values start after the keys
        valuestart = keystart + len(ids)
        koffsets = []
        voffsets = []
        # The string table first has the list of keys, then the list of values.
        # Each entry has first the size of the string, then the file offset.
        for o1, l1, o2, l2 in offsets:
            koffsets = koffsets + [l1, o1 + keystart]
            voffsets = voffsets + [l2, o2 + valuestart]
        offsets = koffsets + voffsets
        output = struct.pack("Iiiiiii",
                             MO_MAGIC_NUMBER,   # Magic
                             0,                 # Version
                             len(keys),         # # of entries
                             7 * 4,             # start of key index
                             7 * 4 + len(keys) * 8,  # start of value index
                             hash_size,         # size of hash table
                             7 * 4 + 2 * (len(keys) * 8))  # offset of hash table
        # additional data is not necessary for empty mo files
        if (len(keys) > 0):
            output = output + array.array("i", offsets).tostring()
            output = output + hash_table.tostring()
            output = output + ids
            output = output + strs
        return output

    def parse(self, input):
        """parses the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            mosrc = input.read()
            input.close()
            input = mosrc
        little, = struct.unpack("<L", input[:4])
        big, = struct.unpack(">L", input[:4])
        if little == MO_MAGIC_NUMBER:
            endian = "<"
        elif big == MO_MAGIC_NUMBER:
            endian = ">"
        else:
            raise ValueError("This is not an MO file")
        magic, version_maj, version_min, lenkeys, startkey, \
        startvalue, sizehash, offsethash = struct.unpack("%sLHHiiiii" % endian,
                                                         input[:(7 * 4)])
        if version_maj >= 1:
            raise base.ParseError("""Unable to process version %d.%d MO files""" % (version_maj, version_min))
        for i in range(lenkeys):
            nextkey = startkey + (i * 2 * 4)
            nextvalue = startvalue + (i * 2 * 4)
            klength, koffset = struct.unpack("%sii" % endian,
                                             input[nextkey:nextkey + (2 * 4)])
            vlength, voffset = struct.unpack("%sii" % endian,
                                             input[nextvalue:nextvalue + (2 * 4)])
            source = input[koffset:koffset + klength]
            context = None
            if "\x04" in source:
                context, source = source.split("\x04")
            # Still need to handle KDE comments
            source = multistring(source.split("\0"), encoding=self._encoding)
            if source == "":
                charset = re.search("charset=([^\\s]+)",
                                    input[voffset:voffset + vlength])
                if charset:
                    self._encoding = po.encodingToUse(charset.group(1))
            target = multistring(input[voffset:voffset + vlength].split("\0"),
                                 encoding=self._encoding)
            newunit = mounit(source)
            newunit.settarget(target)
            if context is not None:
                newunit.msgctxt.append(context)
            self.addunit(newunit)

########NEW FILE########
__FILENAME__ = mozilla_lang
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008, 2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

# Original Author: Dan Schafer <dschafer@mozilla.com>
# Date: 10 Jun 2008

"""A class to manage Mozilla .lang files."""

from translate.storage import base, txt


class LangUnit(base.TranslationUnit):
    """This is just a normal unit with a weird string output"""

    def __init__(self, source=None):
        self.locations = []
        base.TranslationUnit.__init__(self, source)

    def __str__(self):
        if self.source == self.target:
            unchanged = " {ok}"
        else:
            unchanged = ""
        if not self.istranslated():
            target = self.source
        else:
            target = self.target
        if self.getnotes():
            notes = ('\n').join(["# %s" % note for note in self.getnotes('developer').split("\n")])
            return u"%s\n;%s\n%s%s" % (notes, self.source, target, unchanged)
        return u";%s\n%s%s" % (self.source, target, unchanged)

    def getlocations(self):
        return self.locations

    def addlocation(self, location):
        self.locations.append(location)


class LangStore(txt.TxtFile):
    """We extend TxtFile, since that has a lot of useful stuff for encoding"""
    UnitClass = LangUnit

    Name = "Mozilla .lang"
    Extensions = ['lang']

    def __init__(self, inputfile=None, flavour=None, encoding="utf-8", mark_active=False):
        self.is_active = False
        self.mark_active = mark_active
        super(LangStore, self).__init__(inputfile, flavour, encoding)

    def parse(self, lines):
        #Have we just seen a ';' line, and so are ready for a translation
        readyTrans = False
        comment = ""

        if not isinstance(lines, list):
            lines = lines.split("\n")
        for lineoffset, line in enumerate(lines):
            line = line.decode(self.encoding).rstrip("\n").rstrip("\r")

            if lineoffset == 0 and line == "## active ##":
                self.is_active = True
                continue

            if len(line) == 0 and not readyTrans:  # Skip blank lines
                continue

            if readyTrans:  # If we are expecting a translation, set the target
                if line != u.source:
                    u.target = line.replace(" {ok}", "")
                else:
                    u.target = ""
                readyTrans = False  # We already have our translation
                continue

            if line.startswith('#'):  # A comment
                comment += line[1:].strip() + "\n"

            if line.startswith(';'):
                u = self.addsourceunit(line[1:])
                readyTrans = True  # Now expecting a translation on the next line
                u.addlocation("%s:%d" % (self.filename, lineoffset + 1))
                if comment is not None:
                    u.addnote(comment[:-1], 'developer')
                    comment = ""

    def __str__(self):
        ret_string = ""
        if self.is_active or self.mark_active:
            ret_string += "## active ##\n"
        ret_string += u"\n\n\n".join([unicode(unit) for unit in self.units]).encode('utf-8')
        ret_string += "\n"
        return ret_string

########NEW FILE########
__FILENAME__ = odf_io
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

import zipfile

from lxml import etree

from translate.storage.xml_name import XmlNamer


def open_odf(filename):
    z = zipfile.ZipFile(filename, 'r')
    return {'content.xml': z.read("content.xml"),
            'meta.xml': z.read("meta.xml"),
            'styles.xml': z.read("styles.xml")}


def copy_odf(input_zip, output_zip, exclusion_list):
    for name in [name for name in input_zip.namelist() if name not in exclusion_list]:
        output_zip.writestr(name, input_zip.read(name))
    return output_zip


def namespaced(nsmap, short_namespace, tag):
    return '{%s}%s' % (nsmap[short_namespace], tag)


def add_file(output_zip, manifest_data, new_filename, new_data):
    root = etree.fromstring(manifest_data)
    namer = XmlNamer(root)
    namespacer = namer.namespace('manifest')
    file_entry_tag = namespacer.name('file-entry')
    media_type_attr = namespacer.name('media-type')
    full_path_attr = namespacer.name('full-path')

    root.append(etree.Element(file_entry_tag, {media_type_attr: 'application/x-xliff+xml',
                                               full_path_attr: new_filename}))
    output_zip.writestr(new_filename, new_data)
    output_zip.writestr('META-INF/manifest.xml', etree.tostring(root, xml_declaration=True, encoding="UTF-8"))
    return output_zip

########NEW FILE########
__FILENAME__ = odf_shared
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


def define_tables():
    # Copied from git commit 96b9f1419453d8079dd1631c329f04d6e005baae from
    # git://hforge.org/itools.git
    config_uri = 'urn:oasis:names:tc:opendocument:xmlns:config:1.0'
    dc_uri = 'http://purl.org/dc/elements/1.1/'
    form_uri = 'urn:oasis:names:tc:opendocument:xmlns:form:1.0'
    meta_uri = 'urn:oasis:names:tc:opendocument:xmlns:meta:1.0'
    number_uri = 'urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0'
    office_uri = 'urn:oasis:names:tc:opendocument:xmlns:office:1.0'
    presentation_uri = 'urn:oasis:names:tc:opendocument:xmlns:presentation:1.0'
    text_uri = 'urn:oasis:names:tc:opendocument:xmlns:text:1.0'
    svg_uri = 'urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0'

    inline_elements = [
        (text_uri, 'page-count'),
        (text_uri, 'page-number'),

        (text_uri, 'a'),
        (text_uri, 'line-break'),
        (text_uri, 'ruby-base'),
        (text_uri, 's'),
        (text_uri, 'span'),
        (text_uri, 'tab')
    ]

    no_translate_content_elements = [

        # Config
        (config_uri, 'config-item'),

        # Dublin core
        (dc_uri, 'creator'),
        (dc_uri, 'date'),
        #(dc_uri, 'description'),
        (dc_uri, 'language'),
        #(dc_uri, 'subject'),
        #(dc_uri, 'title'),

        # Form
        (form_uri, 'item'),
        (form_uri, 'option'),

        # Meta
        (meta_uri, 'creation-date'),
        (meta_uri, 'date-string'),
        (meta_uri, 'editing-cycles'),
        (meta_uri, 'editing-duration'),
        (meta_uri, 'generator'),
        (meta_uri, 'initial-creator'),
        #(meta_uri, 'keyword'),
        (meta_uri, 'printed-by'),
        (meta_uri, 'print-date'),
        (meta_uri, 'user-defined'),

        # Number
        (number_uri, 'currency-symbol'),
        (number_uri, 'embedded-text'),
        (number_uri, 'text'),

        # Office
        (office_uri, 'binary-data'),

        # Presentation
        (presentation_uri, 'date-time-decl'),
        #(presentation_uri, 'footer-decl'),
        #(presentation_uri, 'header-decl'),

        # Text
        (text_uri, 'author-initials'),
        (text_uri, 'author-name'),
        # XXX (text_uri, 'bibliography-mark'),
        (text_uri, 'bookmark-ref'),
        #(text_uri, 'chapter'),
        (text_uri, 'character-count'),
        #(text_uri, 'conditional-text'),
        (text_uri, 'creation-date'),
        (text_uri, 'creation-time'),
        (text_uri, 'creator'),
        (text_uri, 'date'),
        (text_uri, 'dde-connection'),
        #(text_uri, 'description'),
        (text_uri, 'editing-cycles'),
        (text_uri, 'editing-duration'),
        (text_uri, 'expression'),
        (text_uri, 'file-name'),
        #(text_uri, 'hidden-paragraph'),
        #(text_uri, 'hidden-text'),
        (text_uri, 'image-count'),
        #(text_uri, 'index-entry-span'),
        (text_uri, 'index-title-template'),
        (text_uri, 'initial-creator'),
        #(text_uri, 'keywords'),
        (text_uri, 'linenumbering-separator'),
        (text_uri, 'measure'),
        (text_uri, 'modification-date'),
        (text_uri, 'modification-time'),
        #(text_uri, 'note-citation'),
        #(text_uri, 'note-continuation-notice-backward'),
        #(text_uri, 'note-continuation-notice-forward'),
        (text_uri, 'note-ref'),
        (text_uri, 'number'),
        (text_uri, 'object-count'),
        (text_uri, 'page-continuation'),
        (text_uri, 'page-count'),
        (text_uri, 'page-number'),
        (text_uri, 'page-variable-get'),
        (text_uri, 'page-variable-set'),
        (text_uri, 'paragraph-count'),
        #(text_uri, 'placeholder'),
        (text_uri, 'print-date'),
        (text_uri, 'print-time'),
        (text_uri, 'printed-by'),
        (text_uri, 'reference-ref'),
        #(text_uri, 'ruby-text'),
        (text_uri, 'script'),
        (text_uri, 'sender-city'),
        (text_uri, 'sender-company'),
        (text_uri, 'sender-country'),
        (text_uri, 'sender-email'),
        (text_uri, 'sender-fax'),
        (text_uri, 'sender-firstname'),
        (text_uri, 'sender-initials'),
        (text_uri, 'sender-lastname'),
        (text_uri, 'sender-phone-private'),
        (text_uri, 'sender-phone-work'),
        #(text_uri, 'sender-position'),
        (text_uri, 'sender-postal-code'),
        (text_uri, 'sender-state-or-province'),
        (text_uri, 'sender-street'),
        #(text_uri, 'sender-title'),
        (text_uri, 'sequence'),
        (text_uri, 'sequence-ref'),
        (text_uri, 'sheet-name'),
        #(text_uri, 'subject'),
        (text_uri, 'table-count'),
        (text_uri, 'table-formula'),
        (text_uri, 'template-name'),
        (text_uri, 'text-input'),
        (text_uri, 'time'),
        #(text_uri, 'title'),
        (text_uri, 'user-defined'),
        (text_uri, 'user-field-get'),
        (text_uri, 'user-field-input'),
        (text_uri, 'variable-get'),
        (text_uri, 'variable-input'),
        (text_uri, 'variable-set'),
        (text_uri, 'word-count'),

        # SVG
        #(svg_uri, 'title'),
        #(svg_uri, 'desc')

        # From translate
        (text_uri, 'tracked-changes'),
    ]

    globals()['inline_elements'] = inline_elements
    globals()['no_translate_content_elements'] = no_translate_content_elements

try:
    from itools.odf.schema import inline_elements
    from itools.odf.schema import no_translate_content_elements

except:
    define_tables()

########NEW FILE########
__FILENAME__ = omegat
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Manage the OmegaT glossary format

OmegaT glossary format is used by the
`OmegaT <http://www.omegat.org/en/omegat.html>`_ computer aided
translation tool.

It is a bilingual base class derived format with :class:`OmegaTFile`
and :class:`OmegaTUnit` providing file and unit level access.

Format Implementation
    The OmegaT glossary format is a simple Tab Separated Value (TSV) file
    with the columns: source, target, comment.

    The dialect of the TSV files is specified by :class:`OmegaTDialect`.

Encoding
    The files are either UTF-8 or encoded using the system default.  UTF-8
    encoded files use the .utf8 extension while system encoded files use
    the .tab extension.
"""

import csv
import locale

from translate.storage import base


OMEGAT_FIELDNAMES = ["source", "target", "comment"]
"""Field names for an OmegaT glossary unit"""


class OmegaTDialect(csv.Dialect):
    """Describe the properties of an OmegaT generated TAB-delimited glossary
    file."""
    delimiter = "\t"
    lineterminator = "\r\n"
    quoting = csv.QUOTE_NONE
csv.register_dialect("omegat", OmegaTDialect)


class OmegaTUnit(base.TranslationUnit):
    """An OmegaT glossary unit"""

    def __init__(self, source=None):
        self._dict = {}
        if source:
            self.source = source
        super(OmegaTUnit, self).__init__(source)

    def getdict(self):
        """Get the dictionary of values for a OmegaT line"""
        return self._dict

    def setdict(self, newdict):
        """Set the dictionary of values for a OmegaT line

        :param newdict: a new dictionary with OmegaT line elements
        :type newdict: Dict
        """
        # TODO First check that the values are OK
        self._dict = newdict
    dict = property(getdict, setdict)

    def _get_field(self, key):
        if key not in self._dict:
            return None
        elif self._dict[key]:
            return self._dict[key].decode('utf-8')
        else:
            return ""

    def _set_field(self, key, newvalue):
        if newvalue is None:
            self._dict[key] = None
        if isinstance(newvalue, unicode):
            newvalue = newvalue.encode('utf-8')
        if not key in self._dict or newvalue != self._dict[key]:
            self._dict[key] = newvalue

    def getnotes(self, origin=None):
        return self._get_field('comment')

    def addnote(self, text, origin=None, position="append"):
        currentnote = self._get_field('comment')
        if position == "append" and currentnote is not None and currentnote != u'':
            self._set_field('comment', currentnote + '\n' + text)
        else:
            self._set_field('comment', text)

    def removenotes(self):
        self._set_field('comment', u'')

    def getsource(self):
        return self._get_field('source')

    def setsource(self, newsource):
        self._rich_source = None
        return self._set_field('source', newsource)
    source = property(getsource, setsource)

    def gettarget(self):
        return self._get_field('target')

    def settarget(self, newtarget):
        self._rich_target = None
        return self._set_field('target', newtarget)
    target = property(gettarget, settarget)

    def settargetlang(self, newlang):
        self._dict['target-lang'] = newlang
    targetlang = property(None, settargetlang)

    def __str__(self):
        return str(self._dict)

    def istranslated(self):
        return bool(self._dict.get('target', None))


class OmegaTFile(base.TranslationStore):
    """An OmegaT glossary file"""
    Name = "OmegaT Glossary"
    Mimetypes = ["application/x-omegat-glossary"]
    Extensions = ["utf8"]

    def __init__(self, inputfile=None, unitclass=OmegaTUnit):
        """Construct an OmegaT glossary, optionally reading in from
        inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self.extension = ''
        self._encoding = self._get_encoding()
        if inputfile is not None:
            self.parse(inputfile)

    def _get_encoding(self):
        return 'utf-8'

    def parse(self, input):
        """parsese the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            tmsrc = input.read()
            input.close()
            input = tmsrc
        try:
            input = input.decode(self._encoding).encode('utf-8')
        except:
            raise ValueError("OmegaT files are either UTF-8 encoded or use the default system encoding")
        lines = csv.DictReader(input.split("\n"), fieldnames=OMEGAT_FIELDNAMES,
                               dialect="omegat")
        for line in lines:
            newunit = OmegaTUnit()
            newunit.dict = line
            self.addunit(newunit)

    def __str__(self):
        output = csv.StringIO()
        writer = csv.DictWriter(output, fieldnames=OMEGAT_FIELDNAMES,
                                dialect="omegat")
        unit_count = 0
        for unit in self.units:
            if unit.istranslated():
                unit_count += 1
                writer.writerow(unit.dict)
        if unit_count == 0:
            return ""
        output.reset()
        decoded = "".join(output.readlines()).decode('utf-8')
        try:
            return decoded.encode(self._encoding)
        except UnicodeEncodeError:
            return decoded.encode('utf-8')


class OmegaTFileTab(OmegaTFile):
    """An OmegaT glossary file in the default system encoding"""
    Name = "OmegaT Glossary"
    Mimetypes = ["application/x-omegat-glossary"]
    Extensions = ["tab"]

    def _get_encoding(self):
        return locale.getdefaultlocale()[1]

########NEW FILE########
__FILENAME__ = oo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Classes that hold units of .oo files (oounit) or entire files (oofile).

These are specific .oo files for localisation exported by OpenOffice.org - SDF
format (previously knows as GSI files).

.. There used to be an overview of the format here
   http://l10n.openoffice.org/L10N_Framework/Intermediate_file_format.html

The behaviour in terms of escaping is explained in detail in the programming
comments.
"""
# FIXME: add simple test which reads in a file and writes it out again

import os
import re
import warnings

from translate.misc import quote, wStringIO


# File normalisation

normalfilenamechars = "/#.0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
normalizetable = ""
for i in map(chr, range(256)):
    if i in normalfilenamechars:
        normalizetable += i
    else:
        normalizetable += "_"


class unormalizechar(dict):

    def __init__(self, normalchars):
        self.normalchars = {}
        for char in normalchars:
            self.normalchars[ord(char)] = char

    def __getitem__(self, key):
        return self.normalchars.get(key, u"_")

unormalizetable = unormalizechar(normalfilenamechars.decode("ascii"))


def normalizefilename(filename):
    """converts any non-alphanumeric (standard roman) characters to _"""
    if isinstance(filename, str):
        return filename.translate(normalizetable)
    else:
        return filename.translate(unormalizetable)


def makekey(ookey, long_keys):
    """converts an oo key tuple into a unique identifier

    :param ookey: an oo key
    :type ookey: tuple
    :param long_keys: Use long keys
    :type long_keys: Boolean
    :rtype: str
    :return: unique ascii identifier
    """
    project, sourcefile, resourcetype, groupid, localid, platform = ookey
    sourcefile = sourcefile.replace('\\', '/')
    if long_keys:
        sourcebase = os.path.join(project, sourcefile)
    else:
        sourceparts = sourcefile.split('/')
        sourcebase = "".join(sourceparts[-1:])
    if len(groupid) == 0 or len(localid) == 0:
        fullid = groupid + localid
    else:
        fullid = groupid + "." + localid
    if resourcetype:
        fullid = fullid + "." + resourcetype
    key = "%s#%s" % (sourcebase, fullid)
    return normalizefilename(key)

# These are functions that deal with escaping and unescaping of the text fields
# of the SDF file. These should only be applied to the text column.
# The fields quickhelptext and title are assumed to carry no escaping.
#
# The escaping of all strings except those coming from .xhp (helpcontent2)
# sourcefiles work as follows:
#   (newline)         ->  \n
#   (carriage return) ->  \r
#   (tab)             ->  \t
# Backslash characters (\) and single quotes (') are not consistently escaped,
# and are therefore left as they are.
#
# For strings coming from .xhp (helpcontent2) sourcefiles the following
# characters are escaped inside XML tags only:
#   <  ->  \<  when used with lowercase tagnames (with some exceptions)
#   >  ->  \>  when used with lowercase tagnames (with some exceptions)
#   "  ->  \"  around XML properties
# The following is consistently escaped in .xhp strings (not only in XML tags):
#   \  ->  \\


def escape_text(text):
    """Escapes SDF text to be suitable for unit consumption."""
    return text.replace("\n", "\\n").replace("\t", "\\t").replace("\r", "\\r")


def unescape_text(text):
    """Unescapes SDF text to be suitable for unit consumption."""
    return text.replace("\\\\", "\a").replace("\\n", "\n").replace("\\t", "\t").\
           replace("\\r", "\r").replace("\a", "\\\\")

helptagre = re.compile('''<[/]??[a-z_\-]+?(?:| +[a-z]+?=".*?") *[/]??>''')


def escape_help_text(text):
    """Escapes the help text as it would be in an SDF file.

    <, >, " are only escaped in <[[:lower:]]> tags. Some HTML tags make it in in
    lowercase so those are dealt with. Some OpenOffice.org help tags are not
    escaped.
    """
    text = text.replace("\\", "\\\\")
    for tag in helptagre.findall(text):
        escapethistag = False
        for escape_tag in ["ahelp", "link", "item", "emph", "defaultinline",
                           "switchinline", "caseinline", "variable",
                           "bookmark_value", "image", "embedvar", "alt"]:
            if tag.startswith("<%s" % escape_tag) or tag == "</%s>" % escape_tag:
                escapethistag = True
        if tag in ["<br/>", "<help-id-missing/>"]:
            escapethistag = True
        if escapethistag:
            escaped_tag = ("\\<" + tag[1:-1] + "\\>").replace('"', '\\"')
            text = text.replace(tag, escaped_tag)
    return text


def unescape_help_text(text):
    """Unescapes normal text to be suitable for writing to the SDF file."""
    return text.replace(r"\<", "<").replace(r"\>", ">").replace(r'\"', '"').replace(r"\\", "\\")


def encode_if_needed_utf8(text):
    """Encode a Unicode string the the specified encoding"""
    if isinstance(text, unicode):
        return text.encode('UTF-8')
    return text


class ooline(object):
    """this represents one line, one translation in an .oo file"""

    def __init__(self, parts=None):
        """construct an ooline from its parts"""
        if parts is None:
            self.project, self.sourcefile, self.dummy, self.resourcetype, \
                self.groupid, self.localid, self.helpid, self.platform, \
                self.width, self.languageid, self.text, self.helptext, \
                self.quickhelptext, self.title, self.timestamp = [""] * 15
        else:
            self.setparts(parts)

    def setparts(self, parts):
        """create a line from its tab-delimited parts"""
        if len(parts) != 15:
            warnings.warn("oo line contains %d parts, it should contain 15: %r" %
                          (len(parts), parts))
            newparts = list(parts)
            if len(newparts) < 15:
                newparts = newparts + [""] * (15 - len(newparts))
            else:
                newparts = newparts[:15]
            parts = tuple(newparts)
        self.project, self.sourcefile, self.dummy, self.resourcetype, \
            self.groupid, self.localid, self.helpid, self.platform, \
            self.width, self.languageid, self._text, self.helptext, \
            self.quickhelptext, self.title, self.timestamp = parts

    def getparts(self):
        """return a list of parts in this line"""
        return (self.project, self.sourcefile, self.dummy, self.resourcetype,
                self.groupid, self.localid, self.helpid, self.platform,
                self.width, self.languageid, self._text, self.helptext,
                self.quickhelptext, self.title, self.timestamp)

    def gettext(self):
        """Obtains the text column and handle escaping."""
        if self.sourcefile.endswith(".xhp"):
            return unescape_help_text(self._text)
        else:
            return unescape_text(self._text)

    def settext(self, text):
        """Sets the text column and handle escaping."""
        if self.sourcefile.endswith(".xhp"):
            self._text = escape_help_text(text)
        else:
            self._text = escape_text(text)
    text = property(gettext, settext)

    def __str__(self):
        """convert to a string. double check that unicode is handled"""
        return encode_if_needed_utf8(self.getoutput())

    def getoutput(self):
        """return a line in tab-delimited form"""
        parts = self.getparts()
        return "\t".join(parts)

    def getkey(self):
        """get the key that identifies the resource"""
        return (self.project, self.sourcefile, self.resourcetype, self.groupid,
                self.localid, self.platform)


class oounit:
    """this represents a number of translations of a resource"""

    def __init__(self):
        """construct the oounit"""
        self.languages = {}
        self.lines = []

    def addline(self, line):
        """add a line to the oounit"""
        self.languages[line.languageid] = line
        self.lines.append(line)

    def __str__(self):
        """convert to a string. double check that unicode is handled"""
        return encode_if_needed_utf8(self.getoutput())

    def getoutput(self, skip_source=False, fallback_lang=None):
        """return the lines in tab-delimited form"""
        if skip_source:
            lines = self.lines[1:]
            if not lines:
                # Untranslated, so let's do fall-back: (bug 1883)
                new_line = ooline(self.lines[0].getparts())
                new_line.languageid = fallback_lang
                lines = [new_line]
        else:
            lines = self.lines
        return "\r\n".join([str(line) for line in lines])


class oofile:
    """this represents an entire .oo file"""
    UnitClass = oounit

    def __init__(self, input=None):
        """constructs the oofile"""
        self.oolines = []
        self.units = []
        self.ookeys = {}
        self.filename = ""
        self.languages = []
        if input is not None:
            self.parse(input)

    def addline(self, thisline):
        """adds a parsed line to the file"""
        key = thisline.getkey()
        element = self.ookeys.get(key, None)
        if element is None:
            element = self.UnitClass()
            self.units.append(element)
            self.ookeys[key] = element
        element.addline(thisline)
        self.oolines.append(thisline)
        if thisline.languageid not in self.languages:
            self.languages.append(thisline.languageid)

    def parse(self, input):
        """parses lines and adds them to the file"""
        if not self.filename:
            self.filename = getattr(input, 'name', '')
        if hasattr(input, "read"):
            src = input.read()
            input.close()
        else:
            src = input
        for line in src.split("\n"):
            line = quote.rstripeol(line)
            if not line:
                continue
            parts = line.split("\t")
            thisline = ooline(parts)
            self.addline(thisline)

    def __str__(self, skip_source=False, fallback_lang=None):
        """convert to a string. double check that unicode is handled"""
        return encode_if_needed_utf8(self.getoutput(skip_source, fallback_lang))

    def getoutput(self, skip_source=False, fallback_lang=None):
        """converts all the lines back to tab-delimited form"""
        lines = []
        for oe in self.units:
            if len(oe.lines) > 2:
                warnings.warn("contains %d lines (should be 2 at most): languages %r" % (len(oe.lines), oe.languages))
                oekeys = [line.getkey() for line in oe.lines]
                warnings.warn("contains %d lines (should be 2 at most): keys %r" % (len(oe.lines), oekeys))
            oeline = oe.getoutput(skip_source, fallback_lang) + "\r\n"
            lines.append(oeline)
        return "".join(lines)


class oomultifile:
    """this takes a huge GSI file and represents it as multiple smaller files..."""

    def __init__(self, filename, mode=None, multifilestyle="single"):
        """initialises oomultifile from a seekable inputfile or writable outputfile"""
        self.filename = filename
        if mode is None:
            if os.path.exists(filename):
                mode = 'r'
            else:
                mode = 'w'
        self.mode = mode
        self.multifilestyle = multifilestyle
        self.multifilename = os.path.splitext(filename)[0]
        self.multifile = open(filename, mode)
        self.subfilelines = {}
        if mode == "r":
            self.createsubfileindex()

    def createsubfileindex(self):
        """reads in all the lines and works out the subfiles"""
        linenum = 0
        for line in self.multifile:
            subfile = self.getsubfilename(line)
            if not subfile in self.subfilelines:
                self.subfilelines[subfile] = []
            self.subfilelines[subfile].append(linenum)
            linenum += 1

    def getsubfilename(self, line):
        """looks up the subfile name for the line"""
        if line.count("\t") < 2:
            raise ValueError("invalid tab-delimited line: %r" % line)
        lineparts = line.split("\t", 2)
        module, filename = lineparts[0], lineparts[1]
        if self.multifilestyle == "onefile":
            ooname = self.multifilename
        elif self.multifilestyle == "toplevel":
            ooname = module
        else:
            filename = filename.replace("\\", "/")
            fileparts = [module] + filename.split("/")
            ooname = os.path.join(*fileparts[:-1])
        return ooname + os.extsep + "oo"

    def listsubfiles(self):
        """returns a list of subfiles in the file"""
        return self.subfilelines.keys()

    def __iter__(self):
        """iterates through the subfile names"""
        for subfile in self.listsubfiles():
            yield subfile

    def __contains__(self, pathname):
        """checks if this pathname is a valid subfile"""
        return pathname in self.subfilelines

    def getsubfilesrc(self, subfile):
        """returns the list of lines matching the subfile"""
        lines = []
        requiredlines = dict.fromkeys(self.subfilelines[subfile])
        linenum = 0
        self.multifile.seek(0)
        for line in self.multifile:
            if linenum in requiredlines:
                lines.append(line)
            linenum += 1
        return "".join(lines)

    def openinputfile(self, subfile):
        """returns a pseudo-file object for the given subfile"""
        subfilesrc = self.getsubfilesrc(subfile)
        inputfile = wStringIO.StringIO(subfilesrc)
        inputfile.filename = subfile
        return inputfile

    def openoutputfile(self, subfile):
        """returns a pseudo-file object for the given subfile"""

        def onclose(contents):
            self.multifile.write(contents)
            self.multifile.flush()
        outputfile = wStringIO.CatchStringOutput(onclose)
        outputfile.filename = subfile
        return outputfile

    def getoofile(self, subfile):
        """returns an oofile built up from the given subfile's lines"""
        subfilesrc = self.getsubfilesrc(subfile)
        oosubfile = oofile()
        oosubfile.filename = subfile
        oosubfile.parse(subfilesrc)
        return oosubfile

########NEW FILE########
__FILENAME__ = php
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of PHP localisation files :class:`phpunit` or
entire files :class:`phpfile`. These files are used in translating many
PHP based applications.

Only PHP files written with these conventions are supported:

.. code-block:: php

   <?php
   $lang['item'] = "vale";  # Array of values
   $some_entity = "value";  # Named variables
   define("ENTITY", "value");
   $lang = array(
      'item1' => 'value1'    ,   #Supports space before comma
      'item2' => 'value2',
   );
   $lang = array(    # Nested arrays
      'item1' => 'value1',
      'item2' => array(
         'key' => 'value'    ,   #Supports space before comma
         'key2' => 'value2',
      ),
   );

Nested arrays without key for nested array are not supported:

.. code-block:: php

   <?php
   $lang = array(array('key' => 'value'));

The working of PHP strings and specifically the escaping conventions which
differ between single quote (') and double quote (") characters are
implemented as outlined in the PHP documentation for the
`String type <http://www.php.net/language.types.string>`_.
"""

import logging
import re

from translate.storage import base


def phpencode(text, quotechar="'"):
    """Convert Python string to PHP escaping.

    The encoding is implemented for
    `'single quote' <http://www.php.net/manual/en/language.types.string.php#language.types.string.syntax.single>`_
    and `"double quote" <http://www.php.net/manual/en/language.types.string.php#language.types.string.syntax.double>`_
    syntax.

    heredoc and nowdoc are not implemented and it is not certain whether this
    would ever be needed for PHP localisation needs.
    """
    if not text:
        return text
    if quotechar == '"':
        # \n may be converted to \\n but we don't.  This allows us to preserve
        # pretty layout that might have appeared in muliline entries we might
        # lose some "blah\nblah" layouts but that's probably not the most
        # frequent use case. See bug 588
        escapes = [
            ("\\", "\\\\"), ("\r", "\\r"), ("\t", "\\t"),
            ("\v", "\\v"), ("\f", "\\f"), ("\\\\$", "\\$"),
            ('"', '\\"'), ("\\\\", "\\"),
        ]
        for a, b in escapes:
            text = text.replace(a, b)
        return text
    else:
        return text.replace("%s" % quotechar, "\\%s" % quotechar)


def phpdecode(text, quotechar="'"):
    """Convert PHP escaped string to a Python string."""

    def decode_octal_hex(match):
        r"""decode Octal \NNN and Hex values"""
        if "octal" in match.groupdict():
            return match.groupdict()['octal'].decode("string_escape")
        elif "hex" in match.groupdict():
            return match.groupdict()['hex'].decode("string_escape")
        else:
            return match.group

    if not text:
        return text
    if quotechar == '"':
        # We do not escape \$ as it is used by variables and we can't
        # roundtrip that item.
        escapes = [
            ('\\"', '"'), ("\\\\", "\\"), ("\\n", "\n"), ("\\r", "\r"),
            ("\\t", "\t"), ("\\v", "\v"), ("\\f", "\f"),
        ]
        for a, b in escapes:
            text = text.replace(a, b)
        text = re.sub(r"(?P<octal>\\[0-7]{1,3})", decode_octal_hex, text)
        text = re.sub(r"(?P<hex>\\x[0-9A-Fa-f]{1,2})", decode_octal_hex, text)
        return text
    else:
        return text.replace("\\'", "'").replace("\\\\", "\\")


class phpunit(base.TranslationUnit):
    """A unit of a PHP file: a name, a value, and any comments associated."""

    def __init__(self, source=""):
        """Construct a blank phpunit."""
        self.escape_type = None
        super(phpunit, self).__init__(source)
        self.name = ""
        self.value = ""
        self.translation = ""
        self._comments = []
        self.source = source

    def setsource(self, source):
        """Set the source AND the target to be equal."""
        self._rich_source = None
        self.value = phpencode(source, self.escape_type)

    def getsource(self):
        return phpdecode(self.value, self.escape_type)
    source = property(getsource, setsource)

    def settarget(self, target):
        self._rich_target = None
        self.translation = phpencode(target, self.escape_type)

    def gettarget(self):
        return phpdecode(self.translation, self.escape_type)
    target = property(gettarget, settarget)

    def __str__(self):
        """Convert to a string. Double check that unicode is handled somehow."""
        source = self.getoutput()
        if isinstance(source, unicode):
            return source.encode(getattr(self, "encoding", "UTF-8"))
        return source

    def getoutput(self):
        """Convert the unit back into formatted lines for a php file."""
        return "\n".join(self._comments + ["%s='%s';\n" % (self.name, self.translation or self.value)])

    def addlocation(self, location):
        self.name = location

    def getlocations(self):
        return [self.name]

    def addnote(self, text, origin=None, position="append"):
        if origin in ['programmer', 'developer', 'source code', None]:
            if position == "append":
                self._comments.append(text)
            else:
                self._comments = [text]
        else:
            return super(phpunit, self).addnote(text, origin=origin,
                                                position=position)

    def getnotes(self, origin=None):
        if origin in ['programmer', 'developer', 'source code', None]:
            return '\n'.join(self._comments)
        else:
            return super(phpunit, self).getnotes(origin)

    def removenotes(self):
        self._comments = []

    def isblank(self):
        """Return whether this is a blank element, containing only comments."""
        return not (self.name or self.value)

    def getid(self):
        return self.name


class phpfile(base.TranslationStore):
    """This class represents a PHP file, made up of phpunits."""
    UnitClass = phpunit

    def __init__(self, inputfile=None, encoding='utf-8'):
        """Construct a phpfile, optionally reading in from inputfile."""
        super(phpfile, self).__init__(unitclass=self.UnitClass)
        self.filename = getattr(inputfile, 'name', '')
        self._encoding = encoding
        if inputfile is not None:
            phpsrc = inputfile.read()
            inputfile.close()
            self.parse(phpsrc)

    def __str__(self):
        """Convert the units back to lines."""
        lines = []
        for unit in self.units:
            lines.append(str(unit))
        return "".join(lines)

    def parse(self, phpsrc):
        """Read the source of a PHP file in and include them as units."""
        newunit = phpunit()
        lastvalue = ""
        value = ""
        invalue = False
        incomment = False
        inarray = False
        valuequote = ""  # Either ' or ".
        equaldel = "="
        enddel = ";"
        prename = ""
        keys_dict = {}
        line_number = 0

        # For each line in the PHP translation file.
        for line in phpsrc.decode(self._encoding).split("\n"):
            line_number += 1
            commentstartpos = line.find("/*")
            commentendpos = line.rfind("*/")

            # If a multiline comment starts in the current line.
            if commentstartpos != -1:
                incomment = True

                # If a comment ends in the current line.
                if commentendpos != -1:
                    newunit.addnote(line[commentstartpos:commentendpos+2],
                                    "developer")
                    incomment = False
                else:
                    newunit.addnote(line[commentstartpos:], "developer")

            # If this a multiline comment that ends in the current line.
            if commentendpos != -1 and incomment:
                newunit.addnote(line[:commentendpos+2], "developer")
                incomment = False

            # If this is a multiline comment which started in a previous line.
            if incomment and commentstartpos == -1:
                newunit.addnote(line, "developer")
                continue

            # If an array starts in the current line and is using array syntax
            if (line.lower().replace(" ", "").find('array(') != -1 and
                line.lower().replace(" ", "").find('array()') == -1):
                # If this is a nested array.
                if inarray:
                    prename = prename + line[:line.find('=')].strip() + "->"
                else:
                    equaldel = "=>"
                    enddel = ","
                    inarray = True
                    prename = line[:line.find('=')].strip() + "->"
                continue

            # If an array ends in the current line, reset variables to default
            # values.
            if inarray and line.find(');') != -1:
                equaldel = "="
                enddel = ";"
                inarray = False
                prename = ""
                continue

            # If a nested array ends in the current line, reset prename to its
            # parent array default value by stripping out the last part.
            if inarray and line.find('),') != -1:
                prename = prename[:prename.find("->")+2]
                continue

            # If the current line hosts a define syntax translation.
            if line.lstrip().startswith("define("):
                equaldel = ","
                enddel = ");"

            equalpos = line.find(equaldel)
            hashpos = line.find("#")
            doubleslashpos = line.lstrip().find("//")

            # If this is a '#' comment line or a '//' comment that starts at
            # the line begining.
            if 0 <= hashpos < equalpos or doubleslashpos == 0:
                # Assume that this is a '#' comment line
                newunit.addnote(line.strip(), "developer")
                continue

            # If equalpos is present in the current line and this line is not
            # part of a multiline translation.
            if equalpos != -1 and not invalue:
                # Get the quoting character which encloses the translation
                # (either ' or ").
                valuequote = line[equalpos+len(equaldel):].lstrip()[0]

                if valuequote in ['"', "'"]:
                    # Get the location for the translation unit. prename is the
                    # array name, or blank if no array is present. The line
                    # (until the equal delimiter) is appended to the location.
                    location = prename + line[:equalpos].strip()

                    # Check for duplicate entries.
                    if location in keys_dict.keys():
                        # TODO Get the logger from the code that is calling
                        # this class.
                        logging.error("Duplicate key %s in %s:%d, first "
                                      "occurrence in line %d", location,
                                      self.filename, line_number,
                                      keys_dict[location])
                    else:
                        keys_dict[location] = line_number

                    # Add the location to the translation unit.
                    newunit.addlocation(location)

                    # Save the translation in the value variable.
                    value = line[equalpos+len(equaldel):].lstrip()[1:]
                    lastvalue = ""
                    invalue = True
            else:
                # If no equalpos is present in the current line, but this is a
                # multiline translation.
                if invalue:
                    value = line

            # Get the end delimiter position.
            enddelpos = value.rfind(enddel)

            # Process the current line until all entries on it are parsed.
            while enddelpos != -1:
                # Check if the latest non-whitespace character before the end
                # delimiter is the valuequote.
                if value[:enddelpos].rstrip()[-1] == valuequote:
                    # Save the value string without trailing whitespaces and
                    # without the ending quotes.
                    newunit.value = lastvalue + value[:enddelpos].rstrip()[:-1]
                    newunit.escape_type = valuequote
                    lastvalue = ""
                    invalue = False

                # If there is more text (a comment) after the translation.
                if not invalue and enddelpos != (len(value) - 1):
                    commentinlinepos = value.find("//", enddelpos)
                    if commentinlinepos != -1:
                        newunit.addnote(value[commentinlinepos+2:].strip(),
                                        "developer")

                # If the translation is already parsed, save it and initialize
                # a new translation unit.
                if not invalue:
                    self.addunit(newunit)
                    value = ""
                    newunit = phpunit()

                # Update end delimiter position to the previous last appearance
                # of the end delimiter, because it might be several entries in
                # the same line.
                enddelpos = value.rfind(enddel, 0, enddelpos)
            else:
                # After processing current line, if we are not in an array,
                # fall back to default dialect (PHP simple variable syntax).
                if not inarray:
                    equaldel = "="
                    enddel = ";"

            # If this is part of a multiline translation, just append it to the
            # previous translation lines.
            if invalue:
                lastvalue = lastvalue + value + "\n"

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Contains base placeable classes with names based on XLIFF placeables. See the
XLIFF standard for more information about what the names mean.
"""

from translate.storage.placeables.interfaces import *
from translate.storage.placeables.strelem import StringElem


__all__ = [
    'Bpt', 'Ept', 'Ph', 'It', 'G', 'Bx',
    'Ex', 'X', 'Sub', 'to_base_placeables'
]


# Basic placeable types.
class Bpt(MaskingPlaceable, PairedDelimiter):
    has_content = True


class Ept(MaskingPlaceable, PairedDelimiter):
    has_content = True


class Ph(MaskingPlaceable):
    has_content = True
    istranslatable = False


class It(MaskingPlaceable, Delimiter):
    has_content = True


class G(ReplacementPlaceable):
    has_content = True


class Bx(ReplacementPlaceable, PairedDelimiter):
    has_content = False
    istranslatable = False

    def __init__(self, id=None, xid=None, **kwargs):
        # kwargs is ignored
        ReplacementPlaceable.__init__(self, id=id, xid=xid, **kwargs)


class Ex(ReplacementPlaceable, PairedDelimiter):
    has_content = False
    istranslatable = False

    def __init__(self, id=None, xid=None, **kwargs):
        # kwargs is ignored
        ReplacementPlaceable.__init__(self, id=id, xid=xid, **kwargs)


class X(ReplacementPlaceable, Delimiter):
    has_content = False
    iseditable = False
    isfragile = True
    istranslatable = False

    def __init__(self, id=None, xid=None, **kwargs):
        ReplacementPlaceable.__init__(self, id=id, xid=xid, **kwargs)


class Sub(SubflowPlaceable):
    has_content = True


def to_base_placeables(tree):
    if not isinstance(tree, StringElem):
        return tree

    base_class = [klass for klass in tree.__class__.__bases__ \
                  if klass in [Bpt, Ept, Ph, It, G, Bx, Ex, X, Sub]]

    if not base_class:
        base_class = tree.__class__
    else:
        base_class = base_class[0]

    newtree = base_class()
    newtree.id = tree.id
    newtree.rid = tree.rid
    newtree.xid = tree.xid
    newtree.sub = []

    for subtree in tree.sub:
        newtree.sub.append(to_base_placeables(subtree))

    return newtree

########NEW FILE########
__FILENAME__ = general
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Contains general placeable implementations. That is placeables that does not
fit into any other sub-category.
"""

import re

from translate.storage.placeables.base import G, Ph, StringElem


__all__ = ['AltAttrPlaceable', 'XMLEntityPlaceable', 'XMLTagPlaceable', 'parsers', 'to_general_placeables']


def regex_parse(cls, pstr):
    """A parser method to extract placeables from a string based on a regular
        expression. Use this function as the ``@parse()`` method of a placeable
        class."""
    if cls.regex is None:
        return None
    matches = []
    oldend = 0
    for match in cls.regex.finditer(pstr):
        start, end = match.start(), match.end()
        if oldend != start:
            matches.append(StringElem(pstr[oldend:start]))
        matches.append(cls([pstr[start:end]]))
        oldend = end
    if oldend != len(pstr) and matches:
        matches.append(StringElem(pstr[oldend:]))
    return matches or None


class AltAttrPlaceable(G):
    """Placeable for the "alt=..." attributes inside XML tags."""

    regex = re.compile(r'alt=".*?"')
    parse = classmethod(regex_parse)


class NewlinePlaceable(Ph):
    """Placeable for new-lines."""

    iseditable = False
    isfragile = True
    istranslatable = False
    regex = re.compile(r'\r\n|\n|\r')
    parse = classmethod(regex_parse)


class NumberPlaceable(Ph):
    """Placeable for numbers."""

    istranslatable = False
    regex = re.compile(ur"[-+]?[0-9]+([\u00a0.,][0-9]+)*")
    parse = classmethod(regex_parse)


class QtFormattingPlaceable(Ph):
    """Placeable representing a Qt string formatting variable.

    Implemented following Qt documentation on
    `QString::arg <http://doc.trolltech.com/4.5/qstring.html#arg>`_ where
    the placeables are refered to as 'place markers'

    Notes:
      - Place markers can be reordered
      - Place markers may be repeated
      - 'L' use a localised representation e.g. in a number
      - %% some in the wild to escape real %, not documented (not in regex)
    """
    iseditable = False
    istranslatable = False
    regex = re.compile(r"""(?x)
                       %                 # Start of a place marker
                       L?                # The sequence is replaced with a localized representation (optional)
                       [1-9]\d{0,1}      # Place marker numbers must be in the range 1 to 99.
                       (?=([^\d]|$))     # Double check that we aren't matching %100+ (non consuming match)
                       """)
    parse = classmethod(regex_parse)


class PythonFormattingPlaceable(Ph):
    """Placeable representing a Python string formatting variable.

    Implemented following Python documentation on
    `String Formatting Operations <http://docs.python.org/library/stdtypes.html#string-formatting-operations>`_"""

    iseditable = False
    istranslatable = False
    # Need to correctly define a python identifier.
    regex = re.compile(r"""(?x)
                       %                     # Start of formatting specifier
                       (%|                   # No argument converted %% creates a %
                       (\([a-z_]+\)){0,1}    # Mapping key value (optional)
                       [\-\+0\s\#]{0,1}      # Conversion flags (optional)
                       (\d+|\*){0,1}         # Minimum field width (optional)
                       (\.(\d+|\*)){0,1}     # Precision (optional)
                       [hlL]{0,1}            # Length modifier (optional)
                       [diouxXeEfFgGcrs]{1}) # Conversion type""")
    parse = classmethod(regex_parse)


class JavaMessageFormatPlaceable(Ph):
    """Placeable representing a Java MessageFormat formatting variable.

    Implemented according to the Java `MessageFormat documentation
    <http://java.sun.com/j2se/1.4.2/docs/api/java/text/MessageFormat.html>`_.

    Information about custom formats:
      - number - `DecimalFormat <http://java.sun.com/j2se/1.4.2/docs/api/java/text/DecimalFormat.html>`_
      - date/time - `SimpleDateFormat <http://java.sun.com/j2se/1.4.2/docs/api/java/text/SimpleDateFormat.html>`_
      - choice - `ChoiceFormat <http://java.sun.com/j2se/1.4.2/docs/api/java/text/ChoiceFormat.html>`_
    """

    iseditable = False  # TODO: Technically incorrect as you need to change
    istranslatable = False
    # things in a choice entry
    regex = re.compile(r"""(?x)
      {                      # Start of MessageFormat
      [0-9]+                 # Number, positive array reference
      (,\s*                  # FormatType (optional) one of number,date,time,choice
        (number(,\s*(integer|currency|percent|[-0#.,E;%\u2030\u00a4']+)?)?|  # number FormatStyle (optional)
         (date|time)(,\s*(short|medium|long|full|.+?))?|                  # date/time FormatStyle (optional)
         choice,([^{]+({.+})?)+)?                                      # choice with format, format required
      )?                     # END: (optional) FormatType
      }                      # END: MessageFormat""")
    parse = classmethod(regex_parse)


class FormattingPlaceable(Ph):
    """Placeable representing string formatting variables."""
    # For more information, see  man 3 printf
    # We probably don't want to support absolutely everything

    iseditable = False
    istranslatable = False
    regex = re.compile(r"""
        %                         # introduction
        (\d+\$)?                  # selection of non-next variable (reordering)
        [\-\+0 \#'I]?             # optional flag
        ((\d+)|[*])?              # field width
        (\.\d+)?                  # precision
        [hlI]?                    # length
        [cCdiouxXeEfgGnpsS]       # conversion specifier
        """, re.VERBOSE)
    parse = classmethod(regex_parse)


class UrlPlaceable(Ph):
    """Placeable handling URI."""

    istranslatable = False
    regex = re.compile(r"""
    ((((news|nttp|file|https?|ftp|irc)://)       # has to start with a protocol
    |((www|ftp)[-A-Za-z0-9]*\.))                 # or www... or ftp... hostname
    ([-A-Za-z0-9]+(\.[-A-Za-z0-9]+)*)            # hostname
    |(\d{1,3}(\.\d{1,3}){3,3}))                  # or IP address
    (:[0-9]{1,5})?                               # optional port
    (/[-A-Za-z0-9_\$\.\+\!\*\(\),;:@&=\?/~\#\%]*)?     # optional trailing path
    (?=$|\s|([]'}>\),\"]))
    """, re.VERBOSE)
    parse = classmethod(regex_parse)


class FilePlaceable(Ph):
    """Placeable handling file locations."""

    istranslatable = False
    regex = re.compile(r"(~/|/|\./)([-A-Za-z0-9_\$\.\+\!\*\(\),;:@&=\?/~\#\%]|\\){3,}")
    # TODO: Handle Windows drive letters. Some common Windows paths won't be
    # handled correctly while not allowing spaces, such as
    #     "C:\Documents and Settings"
    #     "C:\Program Files"
    parse = classmethod(regex_parse)


class EmailPlaceable(Ph):
    """Placeable handling emails."""

    istranslatable = False
    regex = re.compile(r"((mailto:)|)[A-Za-z0-9]+[-a-zA-Z0-9._%]*@(([-A-Za-z0-9]+)\.)+[a-zA-Z]{2,4}")
    # TODO: What about internationalised domain names? ;-)
    parse = classmethod(regex_parse)


class PunctuationPlaceable(Ph):
    """Placeable handling punctuation."""

    iseditable = False
    istranslatable = False
    # FIXME this should really be a list created as being the inverse of what
    # is available on the translators keyboard.  Or easily expanded by their
    # configuration.
    regex = re.compile(ur'''([]|          # Marks
                             []|          # Degree related
                             []| # Maths
                             []|    # Quote characters
                             []|           # Guillemets
                             []|          # Currencies
                             |              # U2026 - horizontal ellipsis
                             |              # U2014 - em dash
                             |              # U2013 - en dash
                             []             # U202F - narrow no-break space
                            )+''', re.VERBOSE)
    parse = classmethod(regex_parse)


class XMLEntityPlaceable(Ph):
    """Placeable handling XML entities (``&xxxxx;``-style entities)."""

    iseditable = False
    istranslatable = False
    regex = re.compile(r'''&(
        ([a-zA-Z][a-zA-Z0-9\.-]*)            #named entity
         |([#](\d{1,5}|x[a-fA-F0-9]{1,5})+)  #numeric entity
        );''', re.VERBOSE)
    parse = classmethod(regex_parse)


class CapsPlaceable(Ph):
    """Placeable handling long all-caps strings."""

    iseditable = True
    regex = re.compile(r'\b[A-Z][A-Z_/\-:*0-9]{2,}\b[+]?')
    parse = classmethod(regex_parse)


class CamelCasePlaceable(Ph):
    """Placeable handling camel case strings."""

    iseditable = True
    regex = re.compile(r'''(?x)
            \b(
               [a-z]+[A-Z]|         #Not that strict if we start with lower (iPod)
               [A-Z]+[a-z]+[A-Z]|   #One capital at the start is not enough (OpenTran)
               [A-Z]{2,}[a-z]       #Two capitals at the start is enough (KBabel)
            )[a-zA-Z0-9]*           #Let's allow any final lower/upper/digit
            \b''')
    parse = classmethod(regex_parse)


class SpacesPlaceable(Ph):
    """Placeable handling unusual spaces in strings."""

    iseditable = True
    istranslatable = False
    regex = re.compile(r"""(?m)  #Multiline expression
        [ ]{2,}|     #More than two consecutive
        ^[ ]+|       #At start of a line
        [ ]+$        #At end of line""", re.VERBOSE)

    parse = classmethod(regex_parse)


class XMLTagPlaceable(Ph):
    """Placeable handling XML tags."""

    iseditable = True
    istranslatable = False
    regex = re.compile(r'''
        <                         # start of opening tag
        ([\w.:]+)                 # tag name, possibly namespaced
        (\s([\w.:]+=              # space and attribute name followed by =
            ((".*?")|('.*?'))     # attribute value, single or double quoted
        )?)*/?>                   # end of opening tag, possibly self closing
        |</([\w.]+)>              # or a closing tag
        ''', re.VERBOSE)
    parse = classmethod(regex_parse)


class OptionPlaceable(Ph):
    """Placeble handling command line options e.g. --help"""

    istranslatable = False
    regex = re.compile(r'''(?x)
                      \B(             # Empty string at the start of a non-word, ensures [space]-
                        -[a-zA-Z]|    # Single letter options: -i, -I
                        --[a-z\-]+    # Word options: --help
                      )\b''')
    #regex = re.compile(r'''(-[a-zA-Z]|--[-a-z]+)\b''')
    parse = classmethod(regex_parse)


def to_general_placeables(tree, classmap={
                                    G: (AltAttrPlaceable,),
                                    Ph: (NumberPlaceable,
                                         XMLEntityPlaceable,
                                         XMLTagPlaceable,
                                         UrlPlaceable,
                                         FilePlaceable,
                                         EmailPlaceable,
                                         OptionPlaceable,
                                         PunctuationPlaceable,),
                                }):
    if not isinstance(tree, StringElem):
        return tree

    newtree = None

    for baseclass, gclasslist in classmap.items():
        if isinstance(tree, baseclass):
            gclass = [c for c in gclasslist if c.parse(unicode(tree))]
            if gclass:
                newtree = gclass[0]()

    if newtree is None:
        newtree = tree.__class__()

    newtree.id = tree.id
    newtree.rid = tree.rid
    newtree.xid = tree.xid
    newtree.sub = []

    for subtree in tree.sub:
        newtree.sub.append(to_general_placeables(subtree))

    return newtree

# The order of these parsers are very important
parsers = [
    NewlinePlaceable.parse,
    XMLTagPlaceable.parse,
    AltAttrPlaceable.parse,
    XMLEntityPlaceable.parse,
    PythonFormattingPlaceable.parse,
    JavaMessageFormatPlaceable.parse,
    FormattingPlaceable.parse,
    # The Qt variables can consume the %1 in %1$s which will mask a printf
    # placeable, so it has to come later.
    QtFormattingPlaceable.parse,
    UrlPlaceable.parse,
    FilePlaceable.parse,
    EmailPlaceable.parse,
    CapsPlaceable.parse,
    CamelCasePlaceable.parse,
    OptionPlaceable.parse,
    PunctuationPlaceable.parse,
    NumberPlaceable.parse,
]

########NEW FILE########
__FILENAME__ = interfaces
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
This file contains abstract (semantic) interfaces for placeable
 implementations.
"""

from translate.storage.placeables.strelem import StringElem


class BasePlaceable(StringElem):
    """Base class for all placeables."""
    parse = None


class InvisiblePlaceable(BasePlaceable):
    pass


class MaskingPlaceable(BasePlaceable):
    pass


class ReplacementPlaceable(BasePlaceable):
    pass


class SubflowPlaceable(BasePlaceable):
    pass


class Delimiter(object):
    pass


class PairedDelimiter(object):
    pass

########NEW FILE########
__FILENAME__ = lisa
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009,2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

from lxml import etree

from translate.misc.xml_helpers import normalize_xml_space
from translate.storage.placeables import StringElem, base, xliff
from translate.storage.xml_extract import misc


__all__ = ['xml_to_strelem', 'strelem_to_xml']
# Use the above functions as entry points into this module. The rest are
# used by these functions.


def make_empty_replacement_placeable(klass, node, xml_space="preserve"):
    try:
        return klass(
            id=node.attrib[u'id'],
            rid=node.attrib.get('rid', None),
            xid=node.attrib.get('xid', None),
            xml_attrib=node.attrib,
        )
    except KeyError:
        pass
    return klass()


def make_g_placeable(klass, node, xml_space="default"):
    return klass(
        id=node.attrib[u'id'],
        sub=xml_to_strelem(node, xml_space).sub,
        xml_attrib=node.attrib,
    )


def not_yet_implemented(klass, node, xml_space="preserve"):
    raise NotImplementedError


def make_unknown(klass, node, xml_space="preserve"):
    assert klass is xliff.UnknownXML

    sub = xml_to_strelem(node, xml_space).sub
    id = node.get('id', None)
    rid = node.get('rid', None)
    xid = node.get('xid', None)

    return klass(sub=sub, id=id, rid=rid, xid=xid, xml_node=node)

_class_dictionary = {
    #u'bpt': (xliff.Bpt, not_yet_implemented),
    u'bx': (xliff.Bx, make_empty_replacement_placeable),
    #u'ept': (xliff.Ept, not_yet_implemented),
    u'ex': (xliff.Ex, make_empty_replacement_placeable),
    u'g': (xliff.G, make_g_placeable),
    #u'it': (xliff.It, not_yet_implemented),
    #u'ph': (xliff.Ph, not_yet_implemented),
    #u'sub': (xliff.Sub, not_yet_implemented),
    u'x': (xliff.X, make_empty_replacement_placeable),
}


def make_placeable(node, xml_space):
    _namespace, tag = misc.parse_tag(node.tag)
    if tag in _class_dictionary:
        klass, maker = _class_dictionary[tag]
    else:
        klass, maker = xliff.UnknownXML, make_unknown
    return maker(klass, node, xml_space)


def as_unicode(string):
    if isinstance(string, unicode):
        return string
    elif isinstance(string, StringElem):
        return unicode(string)
    else:
        return unicode(string.decode('utf-8'))


def xml_to_strelem(dom_node, xml_space="preserve"):
    if dom_node is None:
        return StringElem()
    if isinstance(dom_node, basestring):
        dom_node = etree.fromstring(dom_node)
    normalize_xml_space(dom_node, xml_space, remove_start=True)
    result = StringElem()
    sub = result.sub  # just an optimisation
    for child_dom_node in dom_node:
        if child_dom_node.tag is etree.Comment:
            continue
        sub.append(make_placeable(child_dom_node, xml_space))
        if child_dom_node.tail:
            sub.append(StringElem(unicode(child_dom_node.tail)))

    # This is just a strange way of inserting the first text and avoiding a
    # call to .prune() which is very expensive. We assume the tree is optimal.
    node_text = dom_node.text
    if sub and node_text:
        sub.insert(0, StringElem(unicode(node_text)))
    elif node_text:
        sub.append(unicode(node_text))
    return result

# ==========================================================


def placeable_as_dom_node(placeable, tagname):
    dom_node = etree.Element(tagname)
    if placeable.id is not None:
        dom_node.attrib['id'] = placeable.id
    if placeable.xid is not None:
        dom_node.attrib['xid'] = placeable.xid
    if placeable.rid is not None:
        dom_node.attrib['rid'] = placeable.rid

    if hasattr(placeable, 'xml_attrib'):
        for attrib, value in placeable.xml_attrib.items():
            dom_node.set(attrib, value)

    return dom_node


def unknown_placeable_as_dom_node(placeable):
    assert type(placeable) is xliff.UnknownXML

    from copy import copy
    node = copy(placeable.xml_node)
    for i in range(len(node)):
        del node[0]
    node.tail = None
    node.text = None

    return node

_placeable_dictionary = {
    xliff.Bpt: lambda placeable: placeable_as_dom_node(placeable, 'bpt'),
    xliff.Bx: lambda placeable: placeable_as_dom_node(placeable, 'bx'),
    xliff.Ept: lambda placeable: placeable_as_dom_node(placeable, 'ept'),
    xliff.Ex: lambda placeable: placeable_as_dom_node(placeable, 'ex'),
    xliff.G: lambda placeable: placeable_as_dom_node(placeable, 'g'),
    xliff.It: lambda placeable: placeable_as_dom_node(placeable, 'it'),
    xliff.Ph: lambda placeable: placeable_as_dom_node(placeable, 'ph'),
    xliff.Sub: lambda placeable: placeable_as_dom_node(placeable, 'sub'),
    xliff.X: lambda placeable: placeable_as_dom_node(placeable, 'x'),
    xliff.UnknownXML: unknown_placeable_as_dom_node,
    base.Bpt: lambda placeable: placeable_as_dom_node(placeable, 'bpt'),
    base.Bx: lambda placeable: placeable_as_dom_node(placeable, 'bx'),
    base.Ept: lambda placeable: placeable_as_dom_node(placeable, 'ept'),
    base.Ex: lambda placeable: placeable_as_dom_node(placeable, 'ex'),
    base.G: lambda placeable: placeable_as_dom_node(placeable, 'g'),
    base.It: lambda placeable: placeable_as_dom_node(placeable, 'it'),
    base.Ph: lambda placeable: placeable_as_dom_node(placeable, 'ph'),
    base.Sub: lambda placeable: placeable_as_dom_node(placeable, 'sub'),
    base.X: lambda placeable: placeable_as_dom_node(placeable, 'x'),
}


def xml_append_string(node, string):
    if not len(node):
        if not node.text:
            node.text = unicode(string)
        else:
            node.text += unicode(string)
    else:
        lastchild = node.getchildren()[-1]
        if lastchild.tail is None:
            lastchild.tail = ''
        lastchild.tail += unicode(string)
    return node


def strelem_to_xml(parent_node, elem):
    if isinstance(elem, unicode):
        return xml_append_string(parent_node, elem)
    if not isinstance(elem, StringElem):
        return parent_node

    if type(elem) is StringElem and elem.isleaf():
        return xml_append_string(parent_node, elem)

    if elem.__class__ in _placeable_dictionary:
        node = _placeable_dictionary[elem.__class__](elem)
        parent_node.append(node)
    else:
        node = parent_node

    for sub in elem.sub:
        strelem_to_xml(node, sub)

    return parent_node


def parse_xliff(pstr):
    try:
        return xml_to_strelem(etree.fromstring('<source>%s</source>' % (pstr)))
    except Exception as exc:
        raise
        return None
xliff.parsers = [parse_xliff]

########NEW FILE########
__FILENAME__ = parse
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Contains the ``parse`` function that parses normal strings into StringElem-
based "rich" string element trees.
"""

from translate.storage.placeables import StringElem, base


def parse(tree, parse_funcs):
    """Parse placeables from the given string or sub-tree by using the
    parsing functions provided.

    The output of this function is **heavily** dependent on the order of the
    parsing functions. This is because of the algorithm used.

    An over-simplification of the algorithm: the leaves in the ``StringElem``
    tree are expanded to the output of the first parsing function in
    ``parse_funcs``. The next level of recursion is then started on the new
    set of leaves with the used parsing function removed from
    ``parse_funcs``.

    :type  tree: unicode|StringElem
    :param tree: The string or string element sub-tree to parse.
    :type  parse_funcs: A list of parsing functions. It must take exactly
                        one argument (a ``unicode`` string to parse) and
                        return a list of ``StringElem``s which, together,
                        form the original string. If nothing could be
                        parsed, it should return ``None``.
    """
    if isinstance(tree, unicode):
        tree = StringElem(tree)
    if not parse_funcs:
        return tree

    parse_func = parse_funcs[0]

    for leaf in tree.flatten():
        #FIXME: we might rather want to test for editability, but for now this
        # works better
        if not leaf.istranslatable:
            continue

        unileaf = unicode(leaf)
        if not unileaf:
            continue

        subleaves = parse_func(unileaf)
        if subleaves is not None:
            if (len(subleaves) == 1 and type(leaf) is type(subleaves[0]) and
                leaf == subleaves[0]):
                pass
            elif isinstance(leaf, unicode):
                parent = tree.get_parent_elem(leaf)
                if parent is not None:
                    if len(parent.sub) == 1:
                        parent.sub = subleaves
                        leaf = parent
                    else:
                        leafindex = parent.sub.index(leaf)
                        parent.sub[leafindex] = StringElem(subleaves)
                        leaf = parent.sub[leafindex]
            else:
                leaf.sub = subleaves

        parse(leaf, parse_funcs[1:])

        if isinstance(leaf, StringElem):
            leaf.prune()
    return tree

########NEW FILE########
__FILENAME__ = strelem
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
# Copyright 2013-2014 F Wolff
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Contains the base :class:`StringElem` class that represents a node in a
parsed rich-string tree. It is the base class of all placeables.
"""

import logging
import sys


class ElementNotFoundError(ValueError):
    pass


class StringElem(object):
    """
    This class represents a sub-tree of a string parsed into a rich structure.
    It is also the base class of all placeables.
    """

    renderer = None
    """An optional function that returns the Unicode representation of
    the string."""
    sub = []
    """The sub-elements that make up this this string."""
    has_content = True
    """Whether this string can have sub-elements."""
    iseditable = True
    """Whether this string should be changable by the user. Not used at
    the moment."""
    isfragile = False
    """Whether this element should be deleted in its entirety when partially
        deleted. Only checked when ``iseditable = False``"""
    istranslatable = True
    """Whether this string is translatable into other languages."""
    isvisible = True
    """Whether this string should be visible to the user. Not used at
    the moment."""

    # INITIALIZERS #
    def __init__(self, sub=None, id=None, rid=None, xid=None, **kwargs):
        if sub is None:
            self.sub = []
        elif isinstance(sub, (unicode, StringElem)):
            self.sub = [sub]
        else:
            for elem in sub:
                if not isinstance(elem, (unicode, StringElem)):
                    raise ValueError(elem)
            self.sub = sub
            self.prune()

        self.id = id
        self.rid = rid
        self.xid = xid

        for key, value in kwargs.items():
            if hasattr(self, key):
                raise ValueError('attribute already exists: %s' % (key))
            setattr(self, key, value)

    # SPECIAL METHODS #
    def __add__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) + rhs

    def __contains__(self, item):
        """Emulate the ``unicode`` class."""
        return item in unicode(self)

    def __eq__(self, rhs):
        """:returns: ``True`` if (and only if) all members as well as sub-trees
            are equal. False otherwise."""
        if not isinstance(rhs, StringElem):
            return False

        return self.id == rhs.id and \
               self.iseditable == rhs.iseditable and \
               self.istranslatable == rhs.istranslatable and \
               self.isvisible == rhs.isvisible and \
               self.rid == rhs.rid and \
               self.xid == rhs.xid and \
               len(self.sub) == len(rhs.sub) and \
               not [i for i in range(len(self.sub)) if self.sub[i] != rhs.sub[i]]

    def __ge__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) >= rhs

    def __getitem__(self, i):
        """Emulate the ``unicode`` class."""
        return unicode(self)[i]

    def __getslice__(self, i, j):
        """Emulate the ``unicode`` class."""
        return unicode(self)[i:j]

    def __gt__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) > rhs

    def __iter__(self):
        """Create an iterator of this element's sub-elements."""
        for elem in self.sub:
            yield elem

    def __le__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) <= rhs

    def __len__(self):
        """Emulate the ``unicode`` class."""
        return len(unicode(self))

    def __lt__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) < rhs

    def __mul__(self, rhs):
        """Emulate the ``unicode`` class."""
        return unicode(self) * rhs

    def __ne__(self, rhs):
        return not self.__eq__(rhs)

    def __radd__(self, lhs):
        """Emulate the ``unicode`` class."""
        return self + lhs

    def __rmul__(self, lhs):
        """Emulate the ``unicode`` class."""
        return self * lhs

    def __repr__(self):
        elemstr = ', '.join([repr(elem) for elem in self.sub])
        return '<%(class)s(%(id)s%(rid)s%(xid)s[%(subs)s])>' % {
            'class': self.__class__.__name__,
            'id': self.id is not None and 'id="%s" ' % (self.id) or '',
            'rid': self.rid is not None and 'rid="%s" ' % (self.rid) or '',
            'xid': self.xid is not None and 'xid="%s" ' % (self.xid) or '',
            'subs': elemstr,
        }

    def __str__(self):
        if not self.isvisible:
            return ''
        return ''.join([unicode(elem).encode('utf-8') for elem in self.sub])

    def __unicode__(self):
        if callable(self.renderer):
            return self.renderer(self)
        if not self.isvisible:
            return u''
        return u''.join([unicode(elem) for elem in self.sub])

    # METHODS #
    def apply_to_strings(self, f):
        """Apply ``f`` to all actual strings in the tree.

        :param f: Must take one (str or unicode) argument and return a
                  string or unicode.
        """
        for elem in self.flatten():
            for i in range(len(elem.sub)):
                if isinstance(elem.sub[i], basestring):
                    elem.sub[i] = f(elem.sub[i])

    def copy(self):
        """Returns a copy of the sub-tree.  This should be overridden in
        sub-classes with more data.

        .. note:: ``self.renderer`` is **not** copied."""
        #logging.debug('Copying instance of class %s' % (self.__class__.__name__))
        cp = self.__class__(id=self.id, xid=self.xid, rid=self.rid)
        for sub in self.sub:
            if isinstance(sub, StringElem):
                cp.sub.append(sub.copy())
            else:
                cp.sub.append(sub.__class__(sub))
        return cp

    def delete_elem(self, elem):
        if elem is self:
            self.sub = []
            return
        parent = self.get_parent_elem(elem)
        if parent is None:
            raise ElementNotFoundError(repr(elem))
        subidx = -1
        for i in range(len(parent.sub)):
            if parent.sub[i] is elem:
                subidx = i
                break
        if subidx < 0:
            raise ElementNotFoundError(repr(elem))
        del parent.sub[subidx]

    def delete_range(self, start_index, end_index):
        """Delete the text in the range given by the string-indexes
        ``start_index`` and ``end_index``.

        Partial nodes will only be removed if they are editable.

        :returns: A ``StringElem`` representing the removed sub-string, the
                  parent node from which it was deleted as well as the offset at
                  which it was deleted from. ``None`` is returned for the parent
                  value if the root was deleted. If the parent and offset values
                  are not ``None``, ``parent.insert(offset, deleted)``
                  effectively undoes the delete."""
        if start_index == end_index:
            return StringElem(), self, 0
        if start_index > end_index:
            raise IndexError('start_index > end_index: %d > %d' %
                             (start_index, end_index))
        if start_index < 0 or start_index > len(self):
            raise IndexError('start_index: %d' % (start_index))
        if end_index < 1 or end_index > len(self) + 1:
            raise IndexError('end_index: %d' % (end_index))

        start = self.get_index_data(start_index)
        if isinstance(start['elem'], tuple):
            # If {start} is "between" elements, we use the one on the "right"
            start['elem'] = start['elem'][-1]
            start['offset'] = start['offset'][-1]
        end = self.get_index_data(end_index)
        if isinstance(end['elem'], tuple):
            # If {end} is "between" elements, we use the one on the "left"
            end['elem'] = end['elem'][0]
            end['offset'] = end['offset'][0]
        assert start['elem'].isleaf() and end['elem'].isleaf()

        #logging.debug('FROM %s TO %s' % (start, end))

        # Ranges can be one of 3 types:
        # 1) The entire string.
        # 2) An entire element.
        # 3) Restricted to a single element.
        # 4) Spans multiple elements (start- and ending elements are
        #    not the same).

        # Case 1: Entire string #
        if start_index == 0 and end_index == len(self):
            #logging.debug('Case 1: [%s]' % (unicode(self)))
            removed = self.copy()
            self.sub = []
            return removed, None, None

        # Case 2: An entire element #
        if (start['elem'] is end['elem'] and start['offset'] == 0 and
            end['offset'] == len(start['elem']) or
            (not start['elem'].iseditable and start['elem'].isfragile)):
            ##### FOR DEBUGGING #####
            #s = ''
            #for e in self.flatten():
            #    if e is start['elem']:
            #        s += '[' + unicode(e) + ']'
            #    else:
            #        s += unicode(e)
            #logging.debug('Case 2: %s' % (s))
            #########################

            if start['elem'] is self and self.__class__ is StringElem:
                removed = self.copy()
                self.sub = []
                return removed, None, None
            removed = start['elem'].copy()
            parent = self.get_parent_elem(start['elem'])
            offset = parent.elem_offset(start['elem'])
            # Filter out start['elem'] below with a list comprehension in stead
            # of using parent.sub.remove(), becase list.remove() tests value
            # and not identity, which is what we want here. This ensures that
            # start['elem'] is removed and not the first element that is equal
            # to it.
            parent.sub = [i for i in parent.sub if i is not start['elem']]
            return removed, parent, offset

        # Case 3: Within a single element #
        if start['elem'] is end['elem'] and start['elem'].iseditable:
            ##### FOR DEBUGGING #####
            #s = ''
            #for e in self.flatten():
            #    if e is start['elem']:
            #        s += '%s[%s]%s' % (
            #            e[:start['offset']],
            #            e[start['offset']:end['offset']],
            #            e[end['offset']:]
            #        )
            #    else:
            #        s += unicode(e)
            #logging.debug('Case 3: %s' % (s))
            #########################

            # XXX: This might not have the expected result if start['elem']
            # is a StringElem sub-class instance.
            newstr = u''.join(start['elem'].sub)
            removed = StringElem(newstr[start['offset']:end['offset']])
            newstr = newstr[:start['offset']] + newstr[end['offset']:]
            parent = self.get_parent_elem(start['elem'])
            if parent is None and start['elem'] is self:
                parent = self
            start['elem'].sub = [newstr]
            self.prune()
            return removed, start['elem'], start['offset']

        # Case 4: Across multiple elements #
        range_nodes = self.depth_first()
        startidx = 0
        endidx = -1
        for i in range(len(range_nodes)):
            if range_nodes[i] is start['elem']:
                startidx = i
            elif range_nodes[i] is end['elem']:
                endidx = i
                break
        range_nodes = range_nodes[startidx:endidx+1]
        #assert (range_nodes[0] is start['elem'] and
        #        range_nodes[-1] is end['elem'])
        #logging.debug("Nodes in delete range: %s" % (str(range_nodes)))

        marked_nodes = []  # Contains nodes that have been marked for deletion (directly or inderectly (via parent)).
        for node in range_nodes[1:-1]:
            if [n for n in marked_nodes if n is node]:
                continue
            subtree = node.depth_first()
            if not [e for e in subtree if e is end['elem']]:
                #logging.debug("Marking node: %s" % (subtree))
                marked_nodes.extend(subtree)  # "subtree" does not include "node"

        ##### FOR DEBUGGING #####
        #s = ''
        #for e in self.flatten():
        #    if e is start['elem']:
        #        s += '%s[%s' % (e[:start['offset']], e[start['offset']:])
        #    elif e is end['elem']:
        #        s += '%s]%s' % (e[:end['offset']], e[end['offset']:])
        #    else:
        #        s += unicode(e)
        #logging.debug('Case 4: %s' % (s))
        #########################

        removed = self.copy()

        # Save offsets before we start changing the tree
        start_offset = self.elem_offset(start['elem'])
        end_offset = self.elem_offset(end['elem'])

        for node in marked_nodes:
            try:
                self.delete_elem(node)
            except ElementNotFoundError as e:
                pass

        if start['elem'] is not end['elem']:
            if (start_offset == start['index'] or
                (not start['elem'].iseditable and start['elem'].isfragile)):
                self.delete_elem(start['elem'])
            elif start['elem'].iseditable:
                start['elem'].sub = [u''.join(start['elem'].sub)[:start['offset']]]

            if (end_offset + len(end['elem']) == end['index'] or
                (not end['elem'].iseditable and end['elem'].isfragile)):
                self.delete_elem(end['elem'])
            elif end['elem'].iseditable:
                end['elem'].sub = [u''.join(end['elem'].sub)[end['offset']:]]

        self.prune()
        return removed, None, None

    def depth_first(self, filter=None):
        """Returns a list of the nodes in the tree in depth-first order."""
        if filter is None or not callable(filter):
            filter = lambda e: True
        elems = []
        if filter(self):
            elems.append(self)

        for sub in self.sub:
            if not isinstance(sub, StringElem):
                continue
            if sub.isleaf() and filter(sub):
                elems.append(sub)
            else:
                elems.extend(sub.depth_first())
        return elems

    def encode(self, encoding=sys.getdefaultencoding()):
        """More ``unicode`` class emulation."""
        return unicode(self).encode(encoding)

    def elem_offset(self, elem):
        """Find the offset of ``elem`` in the current tree.

        This cannot be reliably used if ``self.renderer`` is used and even
        less so if the rendering function renders the string differently
        upon different calls. In Virtaal the ``StringElemGUI.index()`` method
        is used as replacement for this one.

        :returns: The string index where element ``e`` starts, or -1 if ``e``
                  was not found."""
        offset = 0
        for e in self.iter_depth_first():
            if e is elem:
                return offset
            if e.isleaf():
                offset += len(e)

        # If we can't find the same instance element, settle for one that
        # looks like it
        offset = 0
        for e in self.iter_depth_first():
            if e.isleaf():
                leafoffset = 0
                for s in e.sub:
                    if unicode(s) == unicode(elem):
                        return offset + leafoffset
                    else:
                        leafoffset += len(unicode(s))
                offset += len(e)
        return -1

    def elem_at_offset(self, offset):
        """Get the ``StringElem`` in the tree that contains the string rendered
            at the given offset."""
        if offset < 0 or offset > len(self):
            return None

        length = 0
        elem = None
        for elem in self.flatten():
            elem_len = len(elem)
            if length <= offset < length + elem_len:
                return elem
            length += elem_len
        return elem

    def find(self, x):
        """Find sub-string ``x`` in this string tree and return the position
            at which it starts."""
        if isinstance(x, basestring):
            return unicode(self).find(x)
        if isinstance(x, StringElem):
            return unicode(self).find(unicode(x))
        return None

    def find_elems_with(self, x):
        """Find all elements in the current sub-tree containing ``x``."""
        return [elem for elem in self.flatten() if x in unicode(elem)]

    def flatten(self, filter=None):
        """Flatten the tree by returning a depth-first search over the
        tree's leaves."""
        if filter is None or not callable(filter):
            filter = lambda e: True
        return [elem for elem in self.iter_depth_first(lambda e: e.isleaf() and filter(e))]

    def get_ancestor_where(self, child, criteria):
        parent = self.get_parent_elem(child)
        if parent is None or criteria(parent):
            return parent
        return self.get_ancestor_where(parent, criteria)

    def get_index_data(self, index):
        """Get info about the specified range in the tree.

        :returns: A dictionary with the following items:

            * *elem*: The element in which ``index`` resides.
            * *index*: Copy of the ``index`` parameter
            * *offset*: The offset of ``index`` into ``'elem'``.
         """
        info = {
            'elem': self.elem_at_offset(index),
            'index': index,
        }
        info['offset'] = info['index'] - self.elem_offset(info['elem'])

        # Check if there "index" is actually between elements
        leftelem = self.elem_at_offset(index - 1)
        if leftelem is not None and leftelem is not info['elem']:
            info['elem'] = (leftelem, info['elem'])
            info['offset'] = (len(leftelem), 0)

        return info

    def get_parent_elem(self, child):
        """Searches the current sub-tree for and returns the parent of the
            ``child`` element."""
        for elem in self.iter_depth_first():
            if not isinstance(elem, StringElem):
                continue
            for sub in elem.sub:
                if sub is child:
                    return elem
        return None

    def insert(self, offset, text, preferred_parent=None):
        """Insert the given text at the specified offset of this string-tree's
            string (Unicode) representation."""
        if offset < 0 or offset > len(self) + 1:
            raise IndexError('Index out of range: %d' % (offset))
        if isinstance(text, (str, unicode)):
            text = StringElem(text)
        if not isinstance(text, StringElem):
            raise ValueError('text must be of type StringElem')

        def checkleaf(elem, text):
            if elem.isleaf() and type(text) is StringElem and text.isleaf():
                return unicode(text)
            return text

        # There are 4 general cases (including specific cases) where text can
        # be inserted:
        # 1) At the beginning of the string (self)
        # 1.1) self.sub[0] is editable
        # 1.2) self.sub[0] is not editable
        # 2) At the end of the string (self)
        # 3) In the middle of a node
        # 4) Between two nodes
        # 4.1) Neither of the nodes are editable
        # 4.2) Both nodes are editable
        # 4.3) Node at offset-1 is editable, node at offset is not
        # 4.4) Node at offset is editable, node at offset-1 is not

        oelem = self.elem_at_offset(offset)

        # Case 1 #
        if offset == 0:
            # 1.1 #
            if oelem.iseditable:
                #logging.debug('Case 1.1')
                oelem.sub.insert(0, checkleaf(oelem, text))
                oelem.prune()
                return True
            # 1.2 #
            else:
                #logging.debug('Case 1.2')
                oparent = self.get_ancestor_where(oelem, lambda x: x.iseditable)
                if oparent is not None:
                    oparent.sub.insert(0, checkleaf(oparent, text))
                    return True
                else:
                    self.sub.insert(0, checkleaf(self, text))
                    return True
            return False

        # Case 2 #
        if offset >= len(self):
            #logging.debug('Case 2')
            last = self.flatten()[-1]
            parent = self.get_ancestor_where(last, lambda x: x.iseditable)
            if parent is None:
                parent = self
            parent.sub.append(checkleaf(parent, text))
            return True

        before = self.elem_at_offset(offset - 1)

        # Case 3 #
        if oelem is before:
            if oelem.iseditable:
                #logging.debug('Case 3')
                eoffset = offset - self.elem_offset(oelem)
                if oelem.isleaf():
                    s = unicode(oelem)  # Collapse all sibling strings into one
                    head = s[:eoffset]
                    tail = s[eoffset:]
                    if type(text) is StringElem and text.isleaf():
                        oelem.sub = [head + unicode(text) + tail]
                    else:
                        oelem.sub = [StringElem(head), text, StringElem(tail)]
                    return True
                else:
                    return oelem.insert(eoffset, text)
            return False

        # And the only case left: Case 4 #
        # 4.1 #
        if not before.iseditable and not oelem.iseditable:
            #logging.debug('Case 4.1')
            # Neither are editable, so we add it as a sibling (to the right)
            # of before
            bparent = self.get_parent_elem(before)
            # bparent cannot be a leaf (because it has before as a child), so
            # we insert the text as StringElem(text)
            # We need the index of `before`, but can't use .index(), since we
            # need to test identity, otherwise we might hit an earlier
            # occurrence of an identical string (likely with lots of newlines,
            # for example).
            bindex = 0
            for child in bparent.sub:
                if child is before:
                    break
                bindex += 1
            bparent.sub.insert(bindex + 1, text)
            return True

        # 4.2 #
        elif before.iseditable and oelem.iseditable:
            #logging.debug('Case 4.2')
            # We can add to either, but we try hard to add to the correct one
            # so that we avoid inserting text in the wrong place on undo, for
            # example.
            preferred_type = type(preferred_parent)
            before_type = type(before)
            oelem_type = type(oelem)
            if preferred_parent is oelem:
                # The preferred parent is still in this StringElem
                return oelem.insert(0, text)
            elif oelem_type == preferred_type and not before_type == preferred_type:
                # oelem has the right type and before has the wrong type
                return oelem.insert(0, text)
            elif oelem_type != preferred_type and before_type != preferred_type:
                # Both are the wrong type, so we add it as if neither were
                # editable
                bparent = self.get_parent_elem(before)
                # As above, we can't use .index()
                bindex = 0
                for child in bparent.sub:
                    if child is before:
                        break
                    bindex += 1
                bparent.sub.insert(bindex + 1, text)
                return True

            return before.insert(len(before), text)  # Reinterpret as a case 2

        # 4.3 #
        elif before.iseditable and not oelem.iseditable:
            #logging.debug('Case 4.3')
            return before.insert(len(before), text)  # Reinterpret as a case 2

        # 4.4 #
        elif not before.iseditable and oelem.iseditable:
            #logging.debug('Case 4.4')
            return oelem.insert(0, text)  # Reinterpret as a case 1

        return False

    def insert_between(self, left, right, text):
        """Insert the given text between the two parameter ``StringElem``\s."""
        if not isinstance(left, StringElem) and left is not None:
            raise ValueError('"left" is not a StringElem or None')
        if not isinstance(right, StringElem) and right is not None:
            raise ValueError('"right" is not a StringElem or None')
        if left is right:
            if left.sub:
                # This is an error because the cursor cannot be inside an
                # element ("left is right"), if it has any other content.
                # If an element has content, it will be at least directly
                # left or directly right of the current cursor position.
                raise ValueError('"left" and "right" refer to the same element and is not empty.')
            if not left.iseditable:
                return False
        if isinstance(text, unicode):
            text = StringElem(text)

        if left is right:
            #logging.debug('left%s.sub.append(%s)' % (repr(left), repr(text)))
            left.sub.append(text)
            return True
        # XXX: The "in" keyword is *not* used below, because the "in" tests
        # with __eq__ and not "is", as we do below. Testing for identity is
        # intentional and required.

        if left is None:
            if self is right:
                #logging.debug('self%s.sub.insert(0, %s)' %
                #              (repr(self), repr(text)))
                self.sub.insert(0, text)
                return True
            parent = self.get_parent_elem(right)
            if parent is not None:
                #logging.debug('parent%s.sub.insert(0, %s)' %
                #              (repr(parent), repr(text)))
                parent.sub.insert(0, text)
                return True
            return False

        if right is None:
            if self is left:
                #logging.debug('self%s.sub.append(%s)' %
                #              (repr(self), repr(text)))
                self.sub.append(text)
                return True
            parent = self.get_parent_elem(left)
            if parent is not None:
                #logging.debug('parent%s.sub.append(%s)' %
                #              (repr(parent), repr(text)))
                parent.sub.append(text)
                return True
            return False

        # The following two blocks handle the cases where one element
        # "surrounds" another as its parent. In that way the parent would be
        # "left" of its first child, like in the first case.
        ischild = False
        for sub in left.sub:
            if right is sub:
                ischild = True
                break
        if ischild:
            #logging.debug('left%s.sub.insert(0, %s)' %
            #              (repr(left), repr(text)))
            left.sub.insert(0, text)
            return True

        ischild = False
        for sub in right.sub:
            if left is sub:
                ischild = True
                break
        if ischild:
            #logging.debug('right%s.sub.append(%s)' %
            #              (repr(right), repr(text)))
            right.sub.append(text)
            return True

        parent = self.get_parent_elem(left)
        if parent.iseditable:
            idx = 1
            for child in parent.sub:
                if child is left:
                    break
                idx += 1
            #logging.debug('parent%s.sub.insert(%d, %s)' %
            #              (repr(parent), idx, repr(text)))
            parent.sub.insert(idx, text)
            return True

        parent = self.get_parent_elem(right)
        if parent.iseditable:
            idx = 0
            for child in parent.sub:
                if child is right:
                    break
                idx += 1
            #logging.debug('parent%s.sub.insert(%d, %s)' %
            #              (repr(parent), idx, repr(text)))
            parent.sub.insert(0, text)
            return True

        logging.debug('Could not insert between %s and %s... odd.' %
                      (repr(left), repr(right)))
        return False

    def isleaf(self):
        """
        Whether or not this instance is a leaf node in the ``StringElem`` tree.

        A node is a leaf node if it is a ``StringElem`` (not a sub-class) and
        contains only sub-elements of type ``str`` or ``unicode``.

        :rtype: bool
        """
        for e in self.sub:
            if not isinstance(e, (str, unicode)):
                return False
        return True

    def iter_depth_first(self, filter=None):
        """Iterate through the nodes in the tree in dept-first order."""
        if filter is None or not callable(filter):
            filter = lambda e: True
        if filter(self):
            yield self
        for sub in self.sub:
            if not isinstance(sub, StringElem):
                continue
            if sub.isleaf() and filter(sub):
                yield sub
            else:
                for node in sub.iter_depth_first(filter):
                    yield node

    def map(self, f, filter=None):
        """Apply ``f`` to all nodes for which ``filter`` returned ``True``
        (optional)."""
        if filter is not None and not callable(filter):
            raise ValueError('filter is not callable or None')
        if filter is None:
            filter = lambda e: True

        for elem in self.depth_first():
            if filter(elem):
                f(elem)

    @classmethod
    def parse(cls, pstr):
        """Parse an instance of this class from the start of the given string.
            This method should be implemented by any sub-class that wants to
            parseable by :mod:`translate.storage.placeables.parse`.

            :type  pstr: unicode
            :param pstr: The string to parse into an instance of this class.
            :returns: An instance of the current class, or ``None`` if the
                string not parseable by this class."""
        return cls(pstr)

    def print_tree(self, indent=0, verbose=False):
        """Print the tree from the current instance's point in an indented
            manner."""
        indent_prefix = " " * indent * 2
        out = (u"%s%s [%s]" % (indent_prefix, self.__class__.__name__,
                               unicode(self))).encode('utf-8')
        if verbose:
            out += u' ' + repr(self)

        print(out)

        for elem in self.sub:
            if isinstance(elem, StringElem):
                elem.print_tree(indent + 1, verbose=verbose)
            else:
                print((u'%s%s[%s]' % (indent_prefix, indent_prefix,
                                      elem)).encode('utf-8'))

    def prune(self):
        """Remove unnecessary nodes to make the tree optimal."""
        changed = False
        for elem in self.iter_depth_first():
            if len(elem.sub) == 1:
                child = elem.sub[0]
                # Symbolically: X->StringElem(leaf) => X(leaf)
                #   (where X is any sub-class of StringElem,
                #   but not StringElem)
                if type(child) is StringElem and child.isleaf():
                    elem.sub = child.sub

                # Symbolically:
                #   StringElem->StringElem2->(leaves) => StringElem->(leaves)
                if type(elem) is StringElem and type(child) is StringElem:
                    elem.sub = child.sub
                    changed = True

                # Symbolically: StringElem->X(leaf) => X(leaf)
                #   (where X is any sub-class of StringElem,
                #   but not StringElem)
                if (type(elem) is StringElem and
                    isinstance(child, StringElem) and
                    type(child) is not StringElem):
                    parent = self.get_parent_elem(elem)
                    if parent is not None:
                        parent.sub[parent.sub.index(elem)] = child
                        changed = True

            if type(elem) is StringElem and elem.isleaf():
                # Collapse all strings in this leaf into one string.
                elem.sub = [u''.join(elem.sub)]

            for i in reversed(range(len(elem.sub))):
                # Remove empty strings or StringElem nodes
                # (but not StringElem sub-class instances, because they
                # might contain important (non-rendered) data.
                if (type(elem.sub[i]) in (StringElem, str, unicode) and
                    len(elem.sub[i]) == 0):
                    del elem.sub[i]
                    continue

                if type(elem.sub[i]) in (str, unicode) and not elem.isleaf():
                    elem.sub[i] = StringElem(elem.sub[i])
                    changed = True

            # Merge sibling StringElem leaves
            if not elem.isleaf():
                leafchanged = True
                while leafchanged:
                    leafchanged = False

                    for i in range(len(elem.sub) - 1):
                        lsub = elem.sub[i]
                        rsub = elem.sub[i+1]

                        if (type(lsub) is StringElem and
                            type(rsub) is StringElem):
                            changed = True
                            lsub.sub.extend(rsub.sub)
                            del elem.sub[i+1]
                            leafchanged = True
                            break

        # If any changes were made, call prune() again to make sure that
        # changes made later does not create situations fixed by earlier
        # checks.
        if changed:
            self.prune()

    # TODO: Write unit test for this method
    def remove_type(self, ptype):
        """Replace nodes with type ``ptype`` with base ``StringElem``\s,
        containing the same sub-elements. This is only applicable to
        elements below the element tree root node."""
        for elem in self.iter_depth_first():
            if type(elem) is ptype:
                parent = self.get_parent_elem(elem)
                pindex = parent.sub.index(elem)
                parent.sub[pindex] = StringElem(
                    sub=elem.sub,
                    id=elem.id,
                    xid=elem.xid,
                    rid=elem.rid,
                )

    def translate(self):
        """Transform the sub-tree according to some class-specific needs.
            This method should be either overridden in implementing sub-classes
            or dynamically replaced by specific applications.

            :returns: The transformed Unicode string representing the sub-tree.
            """
        return self.copy()

########NEW FILE########
__FILENAME__ = terminology
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
Contains the placeable that represents a terminology term.
"""

from translate.storage.placeables import StringElem, base


__all__ = ['TerminologyPlaceable', 'parsers']


class TerminologyPlaceable(base.Ph):
    """Terminology distinguished from the rest of a string by being
    a placeable."""

    matchers = []
    """A list of matcher objects to use to identify terminology."""
    translations = []
    """The available translations for this placeable."""

    def __init__(self, *args, **kwargs):
        self.translations = []
        super(TerminologyPlaceable, self).__init__(*args, **kwargs)

    @classmethod
    def parse(cls, pstr):
        parts = []
        matches = []
        match_info = {}

        for matcher in cls.matchers:
            matches.extend(matcher.matches(pstr))
            match_info.update(matcher.match_info)

        lastend = 0

        def sort_matches(x, y):
            # This function will sort a list of matches according to the
            #  match's starting position, putting the one with the longer
            # source text first, if two are the same.
            c = cmp(match_info[x.source]['pos'], match_info[y.source]['pos'])
            return c and c or cmp(len(y.source), len(x.source))
        matches.sort(sort_matches)

        for match in matches:
            info = match_info[match.source]
            if info['pos'] < lastend:
                continue
            end = info['pos'] + len(match.source)
            if 'newtermlen' in info:
                end = info['pos'] + info['newtermlen']

            if lastend < info['pos']:
                parts.append(StringElem(pstr[lastend:info['pos']]))

            term_string = pstr[info['pos']:end]
            term_placeable = cls([term_string])
            parts.append(term_placeable)

            # Get translations for the placeable
            for m in matches:
                m_info = match_info[m.source]
                m_end = m_info['pos']
                if 'newtermlen' in m_info:
                    m_end += m_info['newtermlen']
                else:
                    m_end += len(m.source)
                if info['pos'] == m_info['pos'] and end == m_end:
                    term_placeable.translations.append(m.target)

            # remove duplicates:
            term_placeable.translations = list(set(term_placeable.translations))

            lastend = end
        if lastend != len(pstr) and parts:
            parts.append(StringElem(pstr[lastend:]))

        return parts or None

    def translate(self):
        return (self.translations and self.translations[0] or
                super(TerminologyPlaceable, self).translate())


parsers = [TerminologyPlaceable.parse]

########NEW FILE########
__FILENAME__ = test_base
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

from pytest import mark

from translate.storage.placeables import StringElem, base, general, parse, xliff


class TestStringElem:
    ORIGSTR = u't <a href="http://www.example.com" alt="t &brand;!">&brandLong;</a>'

    def setup_method(self, method):
        self.elem = parse(self.ORIGSTR, general.parsers)

    def test_parse(self):
        assert unicode(self.elem) == self.ORIGSTR

    def test_tree(self):
        assert len(self.elem.sub) == 4
        assert unicode(self.elem.sub[0]) == u't '
        assert unicode(self.elem.sub[1]) == u'<a href="http://www.example.com" alt="t &brand;!">'
        assert unicode(self.elem.sub[2]) == u'&brandLong;'
        assert unicode(self.elem.sub[3]) == u'</a>'

        assert len(self.elem.sub[0].sub) == 1 and self.elem.sub[0].sub[0] == u't '
        assert len(self.elem.sub[1].sub) == 1 and self.elem.sub[1].sub[0] == u'<a href="http://www.example.com" alt="t &brand;!">'
        assert len(self.elem.sub[2].sub) == 1 and self.elem.sub[2].sub[0] == u'&brandLong;'
        assert len(self.elem.sub[3].sub) == 1 and self.elem.sub[3].sub[0] == u'</a>'

    def test_add(self):
        assert self.elem + ' ' == self.ORIGSTR + ' '
        # ... and __radd__() ... doesn't work
        #assert ' ' + self.elem == ' ' + self.ORIGSTR

    def test_contains(self):
        assert 'href' in self.elem
        assert u'hrf' not in self.elem

    def test_getitem(self):
        assert self.elem[0] == u''
        assert self.elem[2] == 't'

    def test_getslice(self):
        assert self.elem[0:3] == u't'

    def test_iter(self):
        for chunk in self.elem:
            assert issubclass(chunk.__class__, StringElem)

    def test_len(self):
        assert len(self.elem) == len(self.ORIGSTR)

    def test_mul(self):
        assert self.elem * 2 == self.ORIGSTR * 2
        # ... and __rmul__()
        assert 2 * self.elem == 2 * self.ORIGSTR

    def test_elem_offset(self):
        assert self.elem.elem_offset(self.elem.sub[0]) == 0
        assert self.elem.elem_offset(self.elem.sub[1]) == 4

    def test_elem_at_offset(self):
        assert self.elem.elem_at_offset(0) is self.elem.sub[0]
        assert self.elem.elem_at_offset(self.elem.find('!')) is self.elem.sub[1]

    def test_find(self):
        assert self.elem.find('example') == 24
        assert self.elem.find(u'example') == 24
        searchelem = parse(u'&brand;', general.parsers)
        assert self.elem.find(searchelem) == 46

    def test_find_elems_with(self):
        assert self.elem.find_elems_with(u't') == [self.elem.sub[0], self.elem.sub[1]]
        assert len(self.elem.find_elems_with('a')) == 3

    def test_flatten(self):
        assert u''.join([unicode(i) for i in self.elem.flatten()]) == self.ORIGSTR

    def test_delete_range_case1(self):
        # Case 1: Entire string #
        elem = self.elem.copy()
        deleted, parent, offset = elem.delete_range(0, len(elem))
        assert deleted == self.elem
        assert parent is None and offset is None

    def test_delete_range_case2(self):
        # Case 2: An entire element #
        elem = self.elem.copy()
        offset = elem.elem_offset(elem.sub[2])
        deleted, parent, offset = elem.delete_range(offset, offset + len(elem.sub[2]))
        assert deleted == self.elem.sub[2]
        assert parent is elem
        assert offset == len(elem.sub[0]) + len(elem.sub[1])

    def test_delete_range_case3(self):
        # Case 3: Within a single element #
        elem = self.elem.copy()
        deleted, parent, offset = elem.delete_range(1, 2)
        assert deleted == StringElem(u'')
        assert parent is elem.sub[0]
        assert offset == 1

    def test_delete_range_case4(self):
        # Case 4: Across multiple elements #
        elem = self.elem.copy()
        # Delete the last two elements
        deleted, parent, offset = elem.delete_range(elem.elem_offset(elem.sub[2]), len(elem))
        assert deleted == self.elem
        assert parent is None
        assert offset is None
        assert len(elem.sub) == 2
        assert unicode(elem) == u't <a href="http://www.example.com" alt="t &brand;!">'

        # A separate test case where the delete range include elements between
        # the start- and end elements.
        origelem = parse(u'foo %s bar', general.parsers)
        elem = origelem.copy()
        assert len(elem.sub) == 3
        deleted, parent, offset = elem.delete_range(3, 7)
        assert deleted == origelem
        assert parent is None
        assert offset is None
        assert unicode(elem) == 'foobar'

    def test_insert(self):
        # Test inserting at the beginning
        elem = self.elem.copy()
        elem.insert(0, u'xxx')
        assert unicode(elem.sub[0]) == u'xxx' + unicode(self.elem.sub[0])

        # Test inserting at the end
        elem = self.elem.copy()
        elem.insert(len(elem) + 1, u'xxx')
        assert elem.flatten()[-1] == StringElem(u'xxx')

        # Test inserting in the middle of an existing string
        elem = self.elem.copy()
        elem.insert(2, u'xxx')
        assert unicode(elem.sub[0]) == u'xxxt '

        # Test inserting between elements
        elem = self.elem.copy()
        elem.insert(56, u'xxx')
        assert unicode(elem)[56:59] == u'xxx'

    def test_isleaf(self):
        for child in self.elem.sub:
            assert child.isleaf()

    def test_prune(self):
        elem = StringElem(u'foo')
        child = StringElem(u'bar')
        elem.sub.append(child)
        elem.prune()
        assert elem == StringElem(u'foobar')


class TestConverters:

    def setup_method(self, method):
        self.elem = parse(TestStringElem.ORIGSTR, general.parsers)

    def test_to_base_placeables(self):
        basetree = base.to_base_placeables(self.elem)
        # The following asserts say that, even though tree and newtree represent the same string
        # (the unicode() results are the same), they are composed of different classes (and so
        # their repr()s are different
        assert unicode(self.elem) == unicode(basetree)
        assert repr(self.elem) != repr(basetree)

    @mark.xfail(reason="Test needs fixing, disabled for now")
    def test_to_general_placeables(self):
        basetree = base.to_base_placeables(self.elem)
        gentree = general.to_general_placeables(basetree)
        assert gentree == self.elem

    @mark.xfail(reason="Test needs fixing, disabled for now")
    def test_to_xliff_placeables(self):
        basetree = base.to_base_placeables(self.elem)
        xliff_from_base = xliff.to_xliff_placeables(basetree)
        assert unicode(xliff_from_base) != unicode(self.elem)
        assert repr(xliff_from_base) != repr(self.elem)

        xliff_from_gen = xliff.to_xliff_placeables(self.elem)
        assert unicode(xliff_from_gen) != unicode(self.elem)
        assert repr(xliff_from_gen) != repr(self.elem)

        assert unicode(xliff_from_base) == unicode(xliff_from_gen)
        assert repr(xliff_from_base) == repr(xliff_from_gen)


if __name__ == '__main__':
    for test in [TestStringElem(), TestConverters()]:
        for method in dir(test):
            if method.startswith('test_') and callable(getattr(test, method)):
                getattr(test, method)()

    test.elem.print_tree()

########NEW FILE########
__FILENAME__ = test_general
# -*- coding: utf-8 -*-

from translate.storage.placeables import general


def test_placeable_numbers():
    """Check the correct functioning of number placeables"""
    assert general.NumberPlaceable([u"25"]) in general.NumberPlaceable.parse(u"Here is a 25 number")
    assert general.NumberPlaceable([u"-25"]) in general.NumberPlaceable.parse(u"Here is a -25 number")
    assert general.NumberPlaceable([u"+25"]) in general.NumberPlaceable.parse(u"Here is a +25 number")
    assert general.NumberPlaceable([u"25.00"]) in general.NumberPlaceable.parse(u"Here is a 25.00 number")
    assert general.NumberPlaceable([u"2,500.00"]) in general.NumberPlaceable.parse(u"Here is a 2,500.00 number")
    assert general.NumberPlaceable([u"1\u00a0000,99"]) in general.NumberPlaceable.parse(u"Here is a 1\u00a0000,99 number")


def test_placeable_newline():
    assert general.NewlinePlaceable.parse(u"A newline\n")[1] == general.NewlinePlaceable([u"\n"])
    assert general.NewlinePlaceable.parse(u"First\nSecond")[1] == general.NewlinePlaceable([u"\n"])


def test_placeable_alt_attr():
    assert general.AltAttrPlaceable.parse(u'Click on the <img src="image.jpg" alt="Image">')[1] == general.AltAttrPlaceable([u'alt="Image"'])


def test_placeable_qt_formatting():
    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[1] == general.QtFormattingPlaceable([u'%1'])
    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[3] == general.QtFormattingPlaceable([u'%99'])
    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[5] == general.QtFormattingPlaceable([u'%L1'])


def test_placeable_camelcase():
    assert general.CamelCasePlaceable.parse(u'CamelCase')[0] == general.CamelCasePlaceable([u'CamelCase'])
    assert general.CamelCasePlaceable.parse(u'iPod')[0] == general.CamelCasePlaceable([u'iPod'])
    assert general.CamelCasePlaceable.parse(u'DokuWiki')[0] == general.CamelCasePlaceable([u'DokuWiki'])
    assert general.CamelCasePlaceable.parse(u'KBabel')[0] == general.CamelCasePlaceable([u'KBabel'])
    assert general.CamelCasePlaceable.parse(u'_Bug') is None
    assert general.CamelCasePlaceable.parse(u'NOTCAMEL') is None


def test_placeable_space():
    assert general.SpacesPlaceable.parse(u' Space at start')[0] == general.SpacesPlaceable([u' '])
    assert general.SpacesPlaceable.parse(u'Space at end ')[1] == general.SpacesPlaceable([u' '])
    assert general.SpacesPlaceable.parse(u'Double  space')[1] == general.SpacesPlaceable([u'  '])


def test_placeable_punctuation():
    assert general.PunctuationPlaceable.parse(u'These, are not. Special: punctuation; marks! Or are "they"?') is None
    assert general.PunctuationPlaceable.parse(u'Downloading')[1] == general.PunctuationPlaceable([u''])


def test_placeable_xml_entity():
    assert general.XMLEntityPlaceable.parse(u'&brandShortName;')[0] == general.XMLEntityPlaceable([u'&brandShortName;'])
    assert general.XMLEntityPlaceable.parse(u'&#1234;')[0] == general.XMLEntityPlaceable([u'&#1234;'])
    assert general.XMLEntityPlaceable.parse(u'&xDEAD;')[0] == general.XMLEntityPlaceable([u'&xDEAD;'])


def test_placeable_xml_tag():
    assert general.XMLTagPlaceable.parse(u'<a>koei</a>')[0] == general.XMLTagPlaceable([u'<a>'])
    assert general.XMLTagPlaceable.parse(u'<a>koei</a>')[2] == general.XMLTagPlaceable([u'</a>'])
    assert general.XMLTagPlaceable.parse(u'<Exif.XResolution>')[0] == general.XMLTagPlaceable([u'<Exif.XResolution>'])
    assert general.XMLTagPlaceable.parse(u'<tag_a>')[0] == general.XMLTagPlaceable([u'<tag_a>'])
    assert general.XMLTagPlaceable.parse(u'<img src="koei.jpg" />')[0] == general.XMLTagPlaceable([u'<img src="koei.jpg" />'])
    # We don't want this to be recognised, so we test for None - not sure if that is a stable assumption
    assert general.XMLTagPlaceable.parse(u'<important word>') is None
    assert general.XMLTagPlaceable.parse(u'<img ="koei.jpg" />') is None
    assert general.XMLTagPlaceable.parse(u'<img "koei.jpg" />') is None
    assert general.XMLTagPlaceable.parse(u'<span xml:space="preserve">')[0] == general.XMLTagPlaceable([u'<span xml:space="preserve">'])
    assert general.XMLTagPlaceable.parse(u'<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />')[0] == general.XMLTagPlaceable([u'<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />'])
    # Bug 933
    assert general.XMLTagPlaceable.parse(u'This entry expires in %days% days. Would you like to <a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">Renew this Entry Now</a> ?')[1] == general.XMLTagPlaceable([u'<a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">'])
    assert general.XMLTagPlaceable.parse(u'''<span weight='bold' size='larger'>Your Google Account is locked</span>''')[0] == general.XMLTagPlaceable([u'''<span weight='bold' size='larger'>'''])


def test_placeable_option():
    assert general.OptionPlaceable.parse(u'Type --help for this help')[1] == general.OptionPlaceable([u'--help'])
    assert general.OptionPlaceable.parse(u'Short -S ones also')[1] == general.OptionPlaceable([u'-S'])


def test_placeable_file():
    assert general.FilePlaceable.parse(u'Store in /home/user')[1] == general.FilePlaceable([u'/home/user'])
    assert general.FilePlaceable.parse(u'Store in ~/Download directory')[1] == general.FilePlaceable([u'~/Download'])


def test_placeable_email():
    assert general.EmailPlaceable.parse(u'Send email to info@example.com')[1] == general.EmailPlaceable([u'info@example.com'])
    assert general.EmailPlaceable.parse(u'Send email to mailto:info@example.com')[1] == general.EmailPlaceable([u'mailto:info@example.com'])


def test_placeable_caps():
    assert general.CapsPlaceable.parse(u'Use the HTML page')[1] == general.CapsPlaceable([u'HTML'])
    assert general.CapsPlaceable.parse(u'I am') is None
    assert general.CapsPlaceable.parse(u'Use the A4 paper') is None
    assert general.CapsPlaceable.parse(u'In GTK+')[1] == general.CapsPlaceable([u'GTK+'])
#    assert general.CapsPlaceable.parse(u'GNOME-stuff')[0] == general.CapsPlaceable([u'GNOME'])
    assert general.CapsPlaceable.parse(u'with XDG_USER_DIRS')[1] == general.CapsPlaceable([u'XDG_USER_DIRS'])


def test_placeable_formatting():
    fp = general.FormattingPlaceable
    assert fp.parse(u'There were %d cows')[1] == fp([u'%d'])
    assert fp.parse(u'There were %Id cows')[1] == fp([u'%Id'])
    assert fp.parse(u'There were %d %s')[3] == fp([u'%s'])
    assert fp.parse(u'%1$s was kicked by %2$s')[0] == fp([u'%1$s'])
    assert fp.parse(u'There were %Id cows')[1] == fp([u'%Id'])
    assert fp.parse(u'There were % d cows')[1] == fp([u'% d'])
    # only a real space is allowed as formatting flag
    assert fp.parse(u'There were %\u00a0d cows') is None
    assert fp.parse(u"There were %'f cows")[1] == fp([u"%'f"])
    assert fp.parse(u"There were %#x cows")[1] == fp([u"%#x"])

    # field width
    assert fp.parse(u'There were %3d cows')[1] == fp([u'%3d'])
    assert fp.parse(u'There were %33d cows')[1] == fp([u'%33d'])
    assert fp.parse(u'There were %*d cows')[1] == fp([u'%*d'])

    # numbered variables
    assert fp.parse(u'There were %1$d cows')[1] == fp([u'%1$d'])


# TODO: PythonFormattingPlaceable, JavaMessageFormatPlaceable, UrlPlaceable, XMLTagPlaceable

########NEW FILE########
__FILENAME__ = test_lisa
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

from lxml import etree

from translate.storage.placeables import StringElem, lisa
from translate.storage.placeables.xliff import Bx, Ex, G, UnknownXML, X


def test_xml_to_strelem():
    source = etree.fromstring(u'<source>a</source>')
    elem = lisa.xml_to_strelem(source)
    assert elem == StringElem(u'a')

    source = etree.fromstring(u'<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
    elem = lisa.xml_to_strelem(source)
    assert elem.sub == [StringElem(u'a'), X(id=u'foo[1]/bar[1]/baz[1]')]

    source = etree.fromstring(u'<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
    elem = lisa.xml_to_strelem(source)
    assert elem.sub == [StringElem(u'a'), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u'')]

    source = etree.fromstring(u'<source>a<g id="foo[2]/bar[2]/baz[2]">b<x id="foo[1]/bar[1]/baz[1]"/>c</g></source>')
    elem = lisa.xml_to_strelem(source)
    assert elem.sub == [StringElem(u'a'), G(id=u'foo[2]/bar[2]/baz[2]', sub=[StringElem(u'b'), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u'c')]), StringElem(u'')]


def test_xml_space():
    source = etree.fromstring(u'<source xml:space="default"> a <x id="foo[1]/bar[1]/baz[1]"/> </source>')
    elem = lisa.xml_to_strelem(source)
    print(elem.sub)
    assert elem.sub == [StringElem(u'a '), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u' ')]


def test_chunk_list():
    left = StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u''])
    right = StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u''])
    assert left == right


def test_set_strelem_to_xml():
    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem(u'a'))
    assert etree.tostring(source, encoding='UTF-8') == '<source>a</source>'

    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem([u'a', u'']))
    assert etree.tostring(source, encoding='UTF-8') == '<source>a</source>'

    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem(X(id='foo[1]/bar[1]/baz[1]')))
    assert etree.tostring(source, encoding='UTF-8') == '<source><x id="foo[1]/bar[1]/baz[1]"/></source>'

    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem([u'a', X(id='foo[1]/bar[1]/baz[1]')]))
    assert etree.tostring(source, encoding='UTF-8') == '<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>'

    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem([u'a', X(id='foo[1]/bar[1]/baz[1]'), u'']))
    assert etree.tostring(source, encoding='UTF-8') == '<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>'

    source = etree.Element(u'source')
    lisa.strelem_to_xml(source, StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u'']))
    assert etree.tostring(source, encoding='UTF-8') == '<source>a<g id="foo[2]/bar[2]/baz[2]">b<x id="foo[1]/bar[1]/baz[1]"/>c</g></source>'


def test_unknown_xml_placeable():
    # The XML below is (modified) from the official XLIFF example file Sample_AlmostEverything_1.2_strict.xlf
    source = etree.fromstring(u"""<source xml:lang="en-us">Text <g id="_1_ski_040">g</g>TEXT<bpt id="_1_ski_139">bpt<sub>sub</sub>
               </bpt>TEXT<ept id="_1_ski_238">ept</ept>TEXT<ph id="_1_ski_337"/>TEXT<it id="_1_ski_436" pos="open">it</it>TEXT<mrk mtype="x-test">mrk</mrk>
               <x id="_1_ski_535"/>TEXT<bx id="_1_ski_634"/>TEXT<ex id="_1_ski_733"/>TEXT.</source>""")
    elem = lisa.xml_to_strelem(source)

    from copy import copy
    custom = StringElem([
        StringElem(u'Text '),
        G(u'g', id='_1_ski_040'),
        StringElem(u'TEXT'),
        UnknownXML(
            [
                StringElem(u'bpt'),
                UnknownXML(u'sub', xml_node=copy(source[1][0])),
                StringElem(u'\n               '),
            ],
            id='_1_ski_139',
            xml_node=copy(source[3])),
        StringElem(u'TEXT'),
        UnknownXML(u'ept', id=u'_1_ski_238', xml_node=copy(source[2])),
        StringElem(u'TEXT'),
        UnknownXML(id='_1_ski_337', xml_node=copy(source[3])),  # ph-tag
        StringElem(u'TEXT'),
        UnknownXML(u'it', id='_1_ski_436', xml_node=copy(source[4])),
        StringElem(u'TEXT'),
        UnknownXML(u'mrk', xml_node=copy(source[5])),
        StringElem(u'\n               '),
        X(id='_1_ski_535'),
        StringElem(u'TEXT'),
        Bx(id='_1_ski_634'),
        StringElem(u'TEXT'),
        Ex(id='_1_ski_733'),
        StringElem(u'TEXT.')
    ])
    assert elem == custom

    xml = copy(source)
    for i in range(len(xml)):
        del xml[0]
    xml.text = None
    xml.tail = None
    lisa.strelem_to_xml(xml, elem)
    assert etree.tostring(xml) == etree.tostring(source)


if __name__ == '__main__':
    test_chunk_list()
    test_xml_to_strelem()
    test_set_strelem_to_xml()
    test_unknown_xml_placeable()

########NEW FILE########
__FILENAME__ = test_terminology
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

from cStringIO import StringIO

from translate.search.match import terminologymatcher
from translate.storage.placeables import StringElem, base, general, parse
from translate.storage.placeables.terminology import (TerminologyPlaceable,
                                                      parsers as term_parsers)
from translate.storage.pypo import pofile


class TestTerminologyPlaceable:
    TERMINOLOGY = """
msgid "name"
msgstr "naam"

msgid "file"
msgstr "ler"

msgid "file name th"
msgstr "lernaam wat?"

msgid "file name"
msgstr "lernaam"
"""

    def setup_method(self, method):
        self.term_po = pofile(StringIO(self.TERMINOLOGY))
        self.matcher = terminologymatcher(self.term_po)
        self.test_string = u'<b>Inpt</b> file name thingy.'

    def test_simple_terminology(self):
        TerminologyPlaceable.matchers = [self.matcher]
        tree = parse(self.test_string, general.parsers + term_parsers)

        assert isinstance(tree.sub[0], general.XMLTagPlaceable)
        assert isinstance(tree.sub[2], general.XMLTagPlaceable)

        tree.print_tree()
        term = tree.sub[3].sub[1]

        assert isinstance(term, TerminologyPlaceable)
        assert unicode(term) == self.term_po.getunits()[2].source
        assert term.translate() == unicode(self.term_po.getunits()[2].target)


if __name__ == '__main__':
    for test in [TestTerminologyPlaceable()]:
        for method in dir(test):
            if method.startswith('test_') and callable(getattr(test, method)):
                if hasattr(test, 'setup_method'):
                    getattr(test, 'setup_method')(getattr(test, method))
                getattr(test, method)()

########NEW FILE########
__FILENAME__ = xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Contains XLIFF-specific placeables."""

from translate.storage.placeables import base
from translate.storage.placeables.strelem import StringElem


__all__ = [
    'Bpt', 'Ept', 'X', 'Bx', 'Ex', 'G', 'It', 'Sub', 'Ph', 'UnknownXML',
    'parsers', 'to_xliff_placeables'
]


class Bpt(base.Bpt):
    pass


class Ept(base.Ept):
    pass


class Ph(base.Ph):
    pass


class It(base.It):
    pass


class G(base.G):
    pass


class Bx(base.Bx):
    pass


class Ex(base.Ex):
    pass


class X(base.X):
    pass


class Sub(base.Sub):
    pass


class UnknownXML(StringElem):
    """Placeable for unrecognized or umimplemented XML nodes. It's main
        purpose is to preserve all associated XML data."""
    iseditable = True

    # INITIALIZERS #
    def __init__(self, sub=None, id=None, rid=None,
                 xid=None, xml_node=None, **kwargs):
        super(UnknownXML, self).__init__(sub=sub, id=id, rid=rid,
                                         xid=xid, **kwargs)
        if xml_node is None:
            raise ValueError('xml_node must be a lxml node')
        self.xml_node = xml_node

        if sub:
            self.has_content = True

    # SPECIAL METHODS #
    def __repr__(self):
        """String representation of the sub-tree with the current node as the
        root.

        Copied from :meth:`StringElem.__repr__`, but includes
        ``self.xml_node.tag``."""
        tag = self.xml_node.tag
        if tag.startswith('{'):
            tag = tag[tag.index('}')+1:]

        elemstr = ', '.join([repr(elem) for elem in self.sub])

        return '<%(class)s{%(tag)s}(%(id)s%(rid)s%(xid)s[%(subs)s])>' % {
            'class': self.__class__.__name__,
            'tag': tag,
            'id': self.id is not None and 'id="%s" ' % (self.id) or '',
            'rid': self.rid is not None and 'rid="%s" ' % (self.rid) or '',
            'xid': self.xid is not None and 'xid="%s" ' % (self.xid) or '',
            'subs': elemstr,
        }

    # METHODS #
    def copy(self):
        """Returns a copy of the sub-tree.  This should be overridden in
        sub-classes with more data.

        .. note:: ``self.renderer`` is **not** copied.
        """
        from copy import copy
        cp = self.__class__(id=self.id, rid=self.rid, xid=self.xid,
                            xml_node=copy(self.xml_node))
        for sub in self.sub:
            if isinstance(sub, StringElem):
                cp.sub.append(sub.copy())
            else:
                cp.sub.append(sub.__class__(sub))
        return cp


def to_xliff_placeables(tree):
    if not isinstance(tree, StringElem):
        return tree

    newtree = None

    classmap = {
        base.Bpt: Bpt,
        base.Ept: Ept,
        base.Ph: Ph,
        base.It: It,
        base.G: G,
        base.Bx: Bx,
        base.Ex: Ex,
        base.X: X,
        base.Sub: Sub,
    }
    for baseclass, xliffclass in classmap.items():
        if isinstance(tree, baseclass):
            newtree = xliffclass()

    if newtree is None:
        newtree = tree.__class__()

    newtree.id = tree.id
    newtree.rid = tree.rid
    newtree.xid = tree.xid
    newtree.sub = []

    for subtree in tree.sub:
        newtree.sub.append(to_xliff_placeables(subtree))

    return newtree


parsers = []

########NEW FILE########
__FILENAME__ = po
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""A class loader that will load C or Python implementations of the PO class
depending on the USECPO variable.

Use the environment variable USECPO=2 (or 1) to choose the C implementation which
uses Gettext's libgettextpo for high parsing speed.  Otherise the local
Python based parser is used (slower but very well tested)."""

import logging
import os
import platform


usecpo = os.getenv('USECPO')

if platform.python_implementation() == "CPython":
    if usecpo == "1":
        logging.info("Using cPO")
        from translate.storage.cpo import *  # pylint: disable=W0401,W0614
    elif usecpo == "2":
        logging.info("Using new fPO")
        from translate.storage.fpo import *  # pylint: disable=W0401,W0614
    else:
        logging.info("Using Python PO")
        from translate.storage.pypo import *  # pylint: disable=W0401,W0614
else:
    if usecpo:
        logging.error("cPO and fPO do not work on %s defaulting to PyPO" %
                      platform.python_implementation())
    from translate.storage.pypo import *  # pylint: disable=W0401

########NEW FILE########
__FILENAME__ = pocommon
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2011 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import re
import urllib

from translate.storage import base, poheader
from translate.storage.workflow import StateEnum as state


msgid_comment_re = re.compile("_: (.*?)\n")


def extract_msgid_comment(text):
    """The one definitive way to extract a msgid comment out of an unescaped
    unicode string that might contain it.

    :rtype: unicode"""
    msgidcomment = msgid_comment_re.match(text)
    if msgidcomment:
        return msgidcomment.group(1)
    return u""


def quote_plus(text):
    """Quote the query fragment of a URL; replacing ' ' with '+'"""
    return urllib.quote_plus(text.encode("utf-8"))


def unquote_plus(text):
    """unquote('%7e/abc+def') -> '~/abc def'"""
    try:
        if isinstance(text, unicode):
            text = text.encode('utf-8')
        return urllib.unquote_plus(text).decode('utf-8')
    except UnicodeEncodeError as e:
        # for some reason there is a non-ascii character here. Let's assume it
        # is already unicode (because of originally decoding the file)
        return text


class pounit(base.TranslationUnit):
    S_FUZZY_OBSOLETE = state.OBSOLETE - 1
    S_OBSOLETE = state.OBSOLETE
    S_UNTRANSLATED = state.EMPTY
    S_FUZZY = state.NEEDS_WORK
    S_TRANSLATED = state.UNREVIEWED

    STATE = {
        S_FUZZY_OBSOLETE: (S_FUZZY_OBSOLETE, state.OBSOLETE),
        S_OBSOLETE: (state.OBSOLETE, state.EMPTY),
        S_UNTRANSLATED: (state.EMPTY, state.NEEDS_WORK),
        S_FUZZY: (state.NEEDS_WORK, state.UNREVIEWED),
        S_TRANSLATED: (state.UNREVIEWED, state.MAX),
    }

    def adderror(self, errorname, errortext):
        """Adds an error message to this unit."""
        text = u'(pofilter) %s: %s' % (errorname, errortext)
        # Don't add the same error twice:
        if text not in self.getnotes(origin='translator'):
            self.addnote(text, origin="translator")

    def geterrors(self):
        """Get all error messages."""
        notes = self.getnotes(origin="translator").split('\n')
        errordict = {}
        for note in notes:
            if '(pofilter) ' in note:
                error = note.replace('(pofilter) ', '')
                errorname, errortext = error.split(': ', 1)
                errordict[errorname] = errortext
        return errordict

    def markreviewneeded(self, needsreview=True, explanation=None):
        """Marks the unit to indicate whether it needs review. Adds an optional explanation as a note."""
        if needsreview:
            reviewnote = "(review)"
            if explanation:
                reviewnote += " " + explanation
            self.addnote(reviewnote, origin="translator")
        else:
            # Strip (review) notes.
            notestring = self.getnotes(origin="translator")
            notes = notestring.split('\n')
            newnotes = []
            for note in notes:
                if not '(review)' in note:
                    newnotes.append(note)
            newnotes = '\n'.join(newnotes)
            self.removenotes()
            self.addnote(newnotes, origin="translator")

    def istranslated(self):
        return super(pounit, self).istranslated() and not self.isobsolete() and not self.isheader()

    def istranslatable(self):
        return not (self.isheader() or self.isblank() or self.isobsolete())

    def hasmarkedcomment(self, commentmarker):
        raise NotImplementedError

    def isreview(self):
        return self.hasmarkedcomment("review") or self.hasmarkedcomment("pofilter")

    def isobsolete(self):
        return self.STATE[self.S_FUZZY_OBSOLETE][0] <= self.get_state_n() < self.STATE[self.S_OBSOLETE][1]

    def isfuzzy(self):
        # implementation specific fuzzy detection, must not use get_state_n()
        raise NotImplementedError()

    def markfuzzy(self, present=True):
        if present:
            self.set_state_n(self.STATE[self.S_FUZZY][0])
        else:
            self.set_state_n(self.STATE[self.S_TRANSLATED][0])
        # set_state_n will check if target exists

    def makeobsolete(self):
        if self.isfuzzy():
            self.set_state_n(self.STATE[self.S_FUZZY_OBSOLETE][0])
        else:
            self.set_state_n(self.STATE[self.S_OBSOLETE][0])

    def resurrect(self):
        self.set_state_n(self.STATE[self.S_TRANSLATED][0])
        if not self.gettarget():
            self.set_state_n(self.STATE[self.S_UNTRANSLATED][0])

    def _domarkfuzzy(self, present=True):
        raise NotImplementedError()

    def get_state_n(self):
        value = super(pounit, self).get_state_n()
        if value <= self.S_OBSOLETE:
            return value
        if self.target:
            if self.isfuzzy():
                return self.S_FUZZY
            else:
                return self.S_TRANSLATED
        else:
            return self.S_UNTRANSLATED

    def set_state_n(self, value):
        super(pounit, self).set_state_n(value)
        has_target = False
        if self.hasplural():
            for string in self.target.strings:
                if string:
                    has_target = True
                    break
        else:
            has_target = bool(self.target)
        if has_target:
            isfuzzy = self.STATE[self.S_FUZZY][0] <= value < self.STATE[self.S_FUZZY][1] or \
                    self.STATE[self.S_FUZZY_OBSOLETE][0] <= value < self.STATE[self.S_FUZZY_OBSOLETE][1]
            self._domarkfuzzy(isfuzzy)  # Implementation specific fuzzy-marking
        else:
            super(pounit, self).set_state_n(self.S_UNTRANSLATED)
            self._domarkfuzzy(False)


def encodingToUse(encoding):
    """Tests whether the given encoding is known in the python runtime, or returns utf-8.
    This function is used to ensure that a valid encoding is always used."""
    if encoding == "CHARSET" or encoding is None:
        return 'utf-8'
    return encoding
#    if encoding is None: return False
#    return True
#    try:
#        tuple = codecs.lookup(encoding)
#    except LookupError:
#        return False
#    return True


class pofile(poheader.poheader, base.TranslationStore):
    Name = "Gettext PO file"  # pylint: disable=E0602
    Mimetypes = ["text/x-gettext-catalog", "text/x-gettext-translation", "text/x-po", "text/x-pot"]
    Extensions = ["po", "pot"]
    # We don't want windows line endings on Windows:
    _binary = True

    def __init__(self, inputfile=None, encoding=None):
        super(pofile, self).__init__(unitclass=self.UnitClass)
        self.units = []
        self.filename = ''
        self._encoding = encodingToUse(encoding)
        if inputfile is not None:
            self.parse(inputfile)
        else:
            self.init_headers()

    @property
    def merge_on(self):
        """The matching criterion to use when merging on."""
        return self.parseheader().get('X-Merge-On', 'id')

########NEW FILE########
__FILENAME__ = poheader
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""class that handles all header functions for a header in a po file"""

import re
import time

try:
    from collections import OrderedDict
except ImportError:
    # Python <= 2.6 fallback
    from translate.misc.dictutils import ordereddict as OrderedDict

from translate import __version__
from translate.misc.dictutils import cidict


author_re = re.compile(r".*<\S+@\S+>.*\d{4,4}")

default_header = {
    "Project-Id-Version": "PACKAGE VERSION",
    "PO-Revision-Date": "YEAR-MO-DA HO:MI+ZONE",
    "Last-Translator": "FULL NAME <EMAIL@ADDRESS>",
    "Language-Team": "LANGUAGE <LL@li.org>",
    "Plural-Forms": "nplurals=INTEGER; plural=EXPRESSION;",
}


def parseheaderstring(input):
    """Parses an input string with the definition of a PO header and returns
    the interpreted values as a dictionary."""
    headervalues = OrderedDict()
    for line in input.split("\n"):
        if not line or ":" not in line:
            continue
        key, value = line.split(":", 1)
        #We don't want unicode keys
        key = str(key.strip())
        headervalues[key] = value.strip()
    return headervalues


def tzstring():
    """Returns the timezone as a string in the format [+-]0000, eg +0200.

    :rtype: str"""
    if time.daylight:
        tzoffset = time.altzone
    else:
        tzoffset = time.timezone

    hours, minutes = time.gmtime(abs(tzoffset))[3:5]
    if tzoffset > 0:
        hours *= -1
    tz = str("%+d" % hours).zfill(3) + str(minutes).zfill(2)
    return tz


def update(existing, add=False, **kwargs):
    """Update an existing header dictionary with the values in kwargs, adding new values
    only if add is true.

    :return: Updated dictionary of header entries
    :rtype: dict
    """
    headerargs = OrderedDict()
    fixedargs = cidict()
    for key, value in kwargs.items():
        key = key.replace("_", "-")
        if key.islower():
            key = key.title()
        fixedargs[key] = value
    removed = []
    for key in poheader.header_order:
        if key in existing:
            if key in fixedargs:
                headerargs[key] = fixedargs.pop(key)
            else:
                headerargs[key] = existing[key]
            removed.append(key)
        elif add and key in fixedargs:
            headerargs[key] = fixedargs.pop(key)
    for key, value in existing.iteritems():
        if not key in removed:
            headerargs[key] = value
    if add:
        for key in fixedargs:
            headerargs[key] = fixedargs[key]
    return headerargs


class poheader(object):
    """This class implements functionality for manipulation of po file headers.
    This class is a mix-in class and useless on its own. It must be used from all
    classes which represent a po file"""

    x_generator = "Translate Toolkit %s" % __version__.sver

    header_order = [
        "Project-Id-Version",
        "Report-Msgid-Bugs-To",
        "POT-Creation-Date",
        "PO-Revision-Date",
        "Last-Translator",
        "Language-Team",
        "Language",
        "MIME-Version",
        "Content-Type",
        "Content-Transfer-Encoding",
        "Plural-Forms",
        "X-Generator",
    ]

    def init_headers(self, charset='UTF-8', encoding='8bit', **kwargs):
        """sets default values for po headers"""
        #FIXME: we need to allow at least setting target language, pluralforms and generator
        headerdict = self.makeheaderdict(charset=charset, encoding=encoding, **kwargs)
        self.updateheader(add=True, **headerdict)
        return self.header()

    def makeheaderdict(self,
            charset="CHARSET",
            encoding="ENCODING",
            project_id_version=None,
            pot_creation_date=None,
            po_revision_date=None,
            last_translator=None,
            language_team=None,
            mime_version=None,
            plural_forms=None,
            report_msgid_bugs_to=None,
            **kwargs):
        """Create a header dictionary with useful defaults.

        pot_creation_date can be None (current date) or a value (datetime or string)
        po_revision_date can be None (form), False (=pot_creation_date), True (=now),
        or a value (datetime or string)

        :return: Dictionary with the header items
        :rtype: dict
        """
        if project_id_version is None:
            project_id_version = "PACKAGE VERSION"
        if pot_creation_date is None or pot_creation_date == True:
            pot_creation_date = time.strftime("%Y-%m-%d %H:%M") + tzstring()
        if isinstance(pot_creation_date, time.struct_time):
            pot_creation_date = time.strftime("%Y-%m-%d %H:%M", pot_creation_date) + tzstring()
        if po_revision_date is None:
            po_revision_date = "YEAR-MO-DA HO:MI+ZONE"
        elif po_revision_date == False:
            po_revision_date = pot_creation_date
        elif po_revision_date == True:
            po_revision_date = time.strftime("%Y-%m-%d %H:%M") + tzstring()
        if isinstance(po_revision_date, time.struct_time):
            po_revision_date = time.strftime("%Y-%m-%d %H:%M", po_revision_date) + tzstring()
        if last_translator is None:
            last_translator = "FULL NAME <EMAIL@ADDRESS>"
        if language_team is None:
            language_team = "LANGUAGE <LL@li.org>"
        if mime_version is None:
            mime_version = "1.0"
        if report_msgid_bugs_to is None:
            report_msgid_bugs_to = ""

        defaultargs = OrderedDict()
        defaultargs["Project-Id-Version"] = project_id_version
        defaultargs["Report-Msgid-Bugs-To"] = report_msgid_bugs_to
        defaultargs["POT-Creation-Date"] = pot_creation_date
        defaultargs["PO-Revision-Date"] = po_revision_date
        defaultargs["Last-Translator"] = last_translator
        defaultargs["Language-Team"] = language_team
        defaultargs["MIME-Version"] = mime_version
        defaultargs["Content-Type"] = "text/plain; charset=%s" % charset
        defaultargs["Content-Transfer-Encoding"] = encoding
        if plural_forms:
            defaultargs["Plural-Forms"] = plural_forms
        defaultargs["X-Generator"] = self.x_generator

        return update(defaultargs, add=True, **kwargs)

    def header(self):
        """Returns the header element, or None. Only the first element is allowed
        to be a header. Note that this could still return an empty header element,
        if present."""
        if len(self.units) == 0:
            return None
        candidate = self.units[0]
        if candidate.isheader():
            return candidate
        else:
            return None

    def parseheader(self):
        """Parses the PO header and returns the interpreted values as a
        dictionary."""
        header = self.header()
        if not header:
            return {}
        return parseheaderstring(header.target)

    def updateheader(self, add=False, **kwargs):
        """Updates the fields in the PO style header.

        This will create a header if add == True."""
        header = self.header()
        if not header:
            if add:
                header = self.makeheader(**kwargs)
                self._insert_header(header)
        else:
            headeritems = update(self.parseheader(), add, **kwargs)
            keys = headeritems.keys()
            if not "Content-Type" in keys or "charset=CHARSET" in headeritems["Content-Type"]:
                headeritems["Content-Type"] = "text/plain; charset=UTF-8"
            if not "Content-Transfer-Encoding" in keys or "ENCODING" in headeritems["Content-Transfer-Encoding"]:
                headeritems["Content-Transfer-Encoding"] = "8bit"
            headerString = ""
            for key, value in headeritems.items():
                if value is not None:
                    headerString += "%s: %s\n" % (key, value)
            header.target = headerString
            header.markfuzzy(False)    # TODO: check why we do this?
        return header

    def _insert_header(self, header):
        # we should be using .addunit() or some equivalent in case the
        # unit needs to refer back to the store, etc. This might be
        # subtly broken for POXLIFF, since we don't dupliate the code
        # from lisa::addunit().
        header._store = self
        self.units.insert(0, header)

    def getheaderplural(self):
        """Returns the nplural and plural values from the header."""
        header = self.parseheader()
        pluralformvalue = header.get('Plural-Forms', None)
        if pluralformvalue is None:
            return None, None
        nplural = re.findall("nplurals=(.+?);", pluralformvalue)
        plural = re.findall("plural=(.+?);?$", pluralformvalue)
        if not nplural or nplural[0] == "INTEGER":
            nplural = None
        else:
            nplural = nplural[0]
        if not plural or plural[0] == "EXPRESSION":
            plural = None
        else:
            plural = plural[0]
        return nplural, plural

    def updateheaderplural(self, nplurals, plural):
        """Update the Plural-Form PO header."""
        if isinstance(nplurals, basestring):
            nplurals = int(nplurals)
        self.updateheader(add=True, Plural_Forms="nplurals=%d; plural=%s;" % (nplurals, plural))

    def gettargetlanguage(self):
        """Return the target language based on information in the header.

        The target language is determined in the following sequence:
          1. Use the 'Language' entry in the header.
          2. Poedit's custom headers.
          3. Analysing the 'Language-Team' entry.
        """
        header = self.parseheader()
        lang = header.get('Language', None)
        if lang is not None:
            from translate.lang.data import langcode_ire
            if langcode_ire.match(lang):
                return lang
            else:
                lang = None
        if 'X-Poedit-Language' in header:
            from translate.lang import poedit
            language = header.get('X-Poedit-Language')
            country = header.get('X-Poedit-Country')
            return poedit.isocode(language, country)
        if 'Language-Code' in header:  # Used in Plone files
            return header.get('Language-Code')
        if 'Language-Team' in header:
            from translate.lang.team import guess_language
            return guess_language(header.get('Language-Team'))
        return None

    def settargetlanguage(self, lang):
        """Set the target language in the header.

        This removes any custom Poedit headers if they exist.

        :param lang: the new target language code
        :type lang: str
        """
        if isinstance(lang, basestring) and len(lang) > 1:
            self.updateheader(add=True, Language=lang, X_Poedit_Language=None, X_Poedit_Country=None)

    def getprojectstyle(self):
        """Return the project based on information in the header.

        The project is determined in the following sequence:
          1. Use the 'X-Project-Style' entry in the header.
          2. Use 'Report-Msgid-Bug-To' entry
          3. Use the 'X-Accelerator' entry
          4. Use the Project ID
          5. Analyse the file itself (not yet implemented)
        """
        header = self.parseheader()
        project = header.get('X-Project-Style', None)
        if project is not None:
            return project
        bug_address = header.get('Report-Msgid-Bugs-To', None)
        if bug_address is not None:
            if 'bugzilla.gnome.org' in bug_address:
                return 'gnome'
            if 'bugs.kde.org' in bug_address:
                return 'kde'
        accelerator = header.get('X-Accelerator-Marker', None)
        if accelerator is not None:
            if accelerator == "~":
                return "openoffice"
            elif accelerator == "&":
                return "mozilla"
        project_id = header.get('Project-Id-Version', None)
        if project_id is not None:
            if 'gnome' in project_id.lower():
                return "gnome"
        # TODO Call some project guessing code and probably move all of the above there also
        return None

    def setprojectstyle(self, project_style):
        """Set the project in the header.

        :param project_style: the new project
        :type project_style: str
        """
        from translate.filters.checks import projectcheckers
        if project_style in projectcheckers:
            self.updateheader(add=True, X_Project_Style=project_style)

    def mergeheaders(self, otherstore):
        """Merges another header with this header.

        This header is assumed to be the template.

        :type otherstore: :class:`~translate.storage.base.TranslationStore`
        """

        newvalues = otherstore.parseheader()
        retain_list = ("Project-Id-Version", "PO-Revision-Date", "Last-Translator",
                       "Language-Team", "Plural-Forms")
        retain = dict((key, newvalues[key]) for key in retain_list if newvalues.get(key, None) and newvalues[key] != default_header.get(key, None))
        self.updateheader(**retain)

    def updatecontributor(self, name, email=None):
        """Add contribution comments if necessary."""
        header = self.header()
        if not header:
            return
        prelines = []
        contriblines = []
        postlines = []
        contribexists = False
        incontrib = False
        outcontrib = False
        for line in header.getnotes("translator").split('\n'):
            line = line.strip()
            if line == u"FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.":
                incontrib = True
                continue
            if author_re.match(line):
                incontrib = True
                contriblines.append(line)
                continue
            if line == "" and incontrib:
                incontrib = False
                outcontrib = True
            if incontrib:
                contriblines.append(line)
            elif not outcontrib:
                prelines.append(line)
            else:
                postlines.append(line)

        year = time.strftime("%Y")
        contribexists = False
        for i in range(len(contriblines)):
            line = contriblines[i]
            if name in line and (email is None or email in line):
                contribexists = True
                if year in line:
                    break
                else:
                    #The contributor is there, but not for this year
                    if line[-1] == '.':
                        line = line[:-1]
                    contriblines[i] = "%s, %s." % (line, year)

        if not contribexists:
            # Add a new contributor
            if email:
                contriblines.append("%s <%s>, %s." % (name, email, year))
            else:
                contriblines.append("%s, %s." % (name, year))

        header.removenotes()
        header.addnote("\n".join(prelines))
        header.addnote("\n".join(contriblines))
        header.addnote("\n".join(postlines))

    def makeheader(self, **kwargs):
        """Create a header for the given filename.

        Check .makeheaderdict() for information on parameters."""
        headerpo = self.UnitClass("", encoding=self._encoding)
        headerpo.markfuzzy()
        headeritems = self.makeheaderdict(**kwargs)
        headervalue = ""
        for (key, value) in headeritems.items():
            if value is None:
                continue
            headervalue += "%s: %s\n" % (key, value)
        headerpo.target = headervalue
        return headerpo

########NEW FILE########
__FILENAME__ = poparser
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import re


"""
From the GNU gettext manual:
     WHITE-SPACE
     #  TRANSLATOR-COMMENTS
     #. AUTOMATIC-COMMENTS
     #| PREVIOUS MSGID                 (Gettext 0.16 - check if this is the correct position - not yet implemented)
     #: REFERENCE...
     #, FLAG...
     msgctxt CONTEXT                   (Gettext 0.15)
     msgid UNTRANSLATED-STRING
     msgstr TRANSLATED-STRING
"""

isspace = str.isspace
find = str.find
rfind = str.rfind
startswith = str.startswith
append = list.append
decode = str.decode


class ParseState(object):

    def __init__(self, input_iterator, UnitClass, encoding=None):
        self._input_iterator = input_iterator
        self.next_line = ''
        self.eof = False
        self.encoding = encoding
        self.read_line()
        self.UnitClass = UnitClass

    def decode(self, string):
        if self.encoding is not None:
            return decode(string, self.encoding)
        else:
            return string

    def read_line(self):
        current = self.next_line
        if self.eof:
            return current
        try:
            self.next_line = self._input_iterator.next()
            while not self.eof and isspace(self.next_line):
                self.next_line = self._input_iterator.next()
        except StopIteration:
            self.next_line = ''
            self.eof = True
        return current

    def new_input(self, _input):
        return ParseState(_input, self.UnitClass, self.encoding)


def read_prevmsgid_lines(parse_state):
    """Read all the lines belonging starting with #|. These lines contain
    the previous msgid and msgctxt info. We strip away the leading '#| '
    and read until we stop seeing #|."""
    prevmsgid_lines = []
    next_line = parse_state.next_line
    while startswith(next_line, '#| ') or startswith(next_line, '| '):
        content = parse_state.read_line()
        prefix_len = content.index('| ')
        content = content[prefix_len+2:]
        append(prevmsgid_lines, content)
        next_line = parse_state.next_line
    return prevmsgid_lines


def parse_prev_msgctxt(parse_state, unit):
    parse_message(parse_state, 'msgctxt', 7, unit.prev_msgctxt)
    return len(unit.prev_msgctxt) > 0


def parse_prev_msgid(parse_state, unit):
    parse_message(parse_state, 'msgid', 5, unit.prev_msgid)
    return len(unit.prev_msgid) > 0


def parse_prev_msgid_plural(parse_state, unit):
    parse_message(parse_state, 'msgid_plural', 12, unit.prev_msgid_plural)
    return len(unit.prev_msgid_plural) > 0


def parse_comment(parse_state, unit):
    next_line = parse_state.next_line.lstrip()
    if len(next_line) > 0 and next_line[0] in ('#', '|'):
        next_char = next_line[1]
        if next_char == '.':
            append(unit.automaticcomments, parse_state.decode(next_line))
        elif next_line[0] == '|' or next_char == '|':
            # Read all the lines starting with #|
            prevmsgid_lines = read_prevmsgid_lines(parse_state)
            # Create a parse state object that holds these lines
            ps = parse_state.new_input(iter(prevmsgid_lines))
            # Parse the msgctxt if any
            parse_prev_msgctxt(ps, unit)
            # Parse the msgid if any
            parse_prev_msgid(ps, unit)
            # Parse the msgid_plural if any
            parse_prev_msgid_plural(ps, unit)
            return parse_state.next_line
        elif next_char == ':':
            append(unit.sourcecomments, parse_state.decode(next_line))
        elif next_char == ',':
            append(unit.typecomments, parse_state.decode(next_line))
        elif next_char == '~':
            # Special case: we refuse to parse obsoletes: they are done
            # elsewhere to ensure we reuse the normal unit parsing code
            return None
        else:
            append(unit.othercomments, parse_state.decode(next_line))
        return parse_state.read_line()
    else:
        return None


def parse_comments(parse_state, unit):
    if not parse_comment(parse_state, unit):
        return None
    else:
        while parse_comment(parse_state, unit):
            pass
        return True


def read_obsolete_lines(parse_state):
    """Read all the lines belonging to the current unit if obsolete."""
    obsolete_lines = []
    next_line = parse_state.next_line
    while startswith(next_line, '#~'):
        content = parse_state.read_line()[2:].lstrip()
        append(obsolete_lines, content)
        next_line = parse_state.next_line
        if startswith(content, 'msgstr'):
            # now we saw a msgstr, so we need to become more conservative to
            # avoid parsing into the following unit
            while startswith(next_line, '#~ "') or startswith(next_line, '#~ msgstr'):
                content = parse_state.read_line()[3:]
                append(obsolete_lines, content)
                next_line = parse_state.next_line
            break
    return obsolete_lines


def parse_obsolete(parse_state, unit):
    obsolete_lines = read_obsolete_lines(parse_state)
    if obsolete_lines == []:
        return None
    unit = parse_unit(parse_state.new_input(iter(obsolete_lines)), unit)
    if unit is not None:
        unit.makeobsolete()
    return unit


def parse_quoted(parse_state, start_pos=0):
    line = parse_state.next_line
    left = find(line, '"', start_pos)
    if left == start_pos or isspace(line[start_pos:left]):
        right = rfind(line, '"')
        if left != right:
            return parse_state.read_line()[left:right+1]
        else:
            # There is no terminating quote, so we append an extra quote, but
            # we also ignore the newline at the end (therefore the -1)
            return parse_state.read_line()[left:-1] + '"'
    return None


def parse_msg_comment(parse_state, msg_comment_list, string):
    while string is not None:
        append(msg_comment_list, parse_state.decode(string))
        if find(string, '\\n') > -1:
            return parse_quoted(parse_state)
        string = parse_quoted(parse_state)
    return None


def parse_multiple_quoted(parse_state, msg_list, msg_comment_list, first_start_pos=0):
    string = parse_quoted(parse_state, first_start_pos)
    while string is not None:
        if not startswith(string, '"_:'):
            append(msg_list, parse_state.decode(string))
            string = parse_quoted(parse_state)
        else:
            string = parse_msg_comment(parse_state, msg_comment_list, string)


def parse_message(parse_state, start_of_string, start_of_string_len, msg_list, msg_comment_list=None):
    if msg_comment_list is None:
        msg_comment_list = []
    if startswith(parse_state.next_line, start_of_string):
        return parse_multiple_quoted(parse_state, msg_list, msg_comment_list, start_of_string_len)


def parse_msgctxt(parse_state, unit):
    parse_message(parse_state, 'msgctxt', 7, unit.msgctxt)
    return len(unit.msgctxt) > 0


def parse_msgid(parse_state, unit):
    parse_message(parse_state, 'msgid', 5, unit.msgid, unit.msgidcomments)
    return len(unit.msgid) > 0 or len(unit.msgidcomments) > 0


def parse_msgstr(parse_state, unit):
    parse_message(parse_state, 'msgstr', 6, unit.msgstr)
    return len(unit.msgstr) > 0


def parse_msgid_plural(parse_state, unit):
    parse_message(parse_state, 'msgid_plural', 12, unit.msgid_plural, unit.msgid_pluralcomments)
    return len(unit.msgid_plural) > 0 or len(unit.msgid_pluralcomments) > 0

MSGSTR_ARRAY_ENTRY_LEN = len('msgstr[')


def add_to_dict(msgstr_dict, line, right_bracket_pos, entry):
    index = int(line[MSGSTR_ARRAY_ENTRY_LEN:right_bracket_pos])
    if index not in msgstr_dict:
        msgstr_dict[index] = []
    msgstr_dict[index].extend(entry)


def get_entry(parse_state, right_bracket_pos):
    entry = []
    parse_message(parse_state, 'msgstr[', right_bracket_pos + 1, entry)
    return entry


def parse_msgstr_array_entry(parse_state, msgstr_dict):
    line = parse_state.next_line
    right_bracket_pos = find(line, ']', MSGSTR_ARRAY_ENTRY_LEN)
    if right_bracket_pos >= 0:
        entry = get_entry(parse_state, right_bracket_pos)
        if len(entry) > 0:
            add_to_dict(msgstr_dict, line, right_bracket_pos, entry)
            return True
        else:
            return False
    else:
        return False


def parse_msgstr_array(parse_state, unit):
    msgstr_dict = {}
    result = parse_msgstr_array_entry(parse_state, msgstr_dict)
    if not result:  # We require at least one result
        return False
    while parse_msgstr_array_entry(parse_state, msgstr_dict):
        pass
    unit.msgstr = msgstr_dict
    return True


def parse_plural(parse_state, unit):
    if parse_msgid_plural(parse_state, unit) and \
       (parse_msgstr_array(parse_state, unit) or parse_msgstr(parse_state, unit)):
        return True
    else:
        return False


def parse_msg_entries(parse_state, unit):
    parse_msgctxt(parse_state, unit)
    if parse_msgid(parse_state, unit) and \
       (parse_msgstr(parse_state, unit) or parse_plural(parse_state, unit)):
        return True
    else:
        return False


def parse_unit(parse_state, unit=None):
    unit = unit or parse_state.UnitClass()
    parsed_comments = parse_comments(parse_state, unit)
    obsolete_unit = parse_obsolete(parse_state, unit)
    if obsolete_unit is not None:
        return obsolete_unit
    parsed_msg_entries = parse_msg_entries(parse_state, unit)
    if parsed_comments or parsed_msg_entries:
        return unit
    else:
        return None


def set_encoding(parse_state, store, unit):
    charset = None
    if isinstance(unit.msgstr, list) and len(unit.msgstr) > 0 and isinstance(unit.msgstr[0], str):
        charset = re.search("charset=([^\\s\\\\n]+)", "".join(unit.msgstr))
    if charset:
        encoding = charset.group(1)
        if encoding != 'CHARSET':
            store._encoding = encoding
        else:
            store._encoding = 'utf-8'
    else:
        store._encoding = 'utf-8'
    parse_state.encoding = store._encoding


def decode_list(lst, decode):
    return [decode(item) for item in lst]


def decode_header(unit, decode):
    for attr in ('msgctxt', 'msgid', 'msgid_pluralcomments',
                 'msgid_plural', 'msgstr',
                 'othercomments', 'automaticcomments', 'sourcecomments',
                 'typecomments', 'msgidcomments'):
        element = getattr(unit, attr)
        if isinstance(element, list):
            setattr(unit, attr, decode_list(element, decode))
        else:
            setattr(unit, attr, dict([(key, decode_list(value, decode)) for key, value in element.items()]))


def parse_header(parse_state, store):
    first_unit = parse_unit(parse_state)
    if first_unit is None:
        return None
    set_encoding(parse_state, store, first_unit)
    decode_header(first_unit, parse_state.decode)
    return first_unit


def parse_units(parse_state, store):
    unit = parse_header(parse_state, store)
    while unit:
        unit.infer_state()
        store.addunit(unit)
        unit = parse_unit(parse_state)
    return parse_state.eof

########NEW FILE########
__FILENAME__ = poxliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""XLIFF classes specifically suited for handling the PO representation in
XLIFF.

This way the API supports plurals as if it was a PO file, for example.
"""

import re

from lxml import etree

from translate.misc.multistring import multistring
from translate.storage import base, lisa, poheader, xliff
from translate.storage.placeables import general


def hasplurals(thing):
    if not isinstance(thing, multistring):
        return False
    return len(thing.strings) > 1


class PoXliffUnit(xliff.xliffunit):
    """A class to specifically handle the plural units created from a po file."""

    rich_parsers = general.parsers

    def __init__(self, source=None, empty=False, encoding="UTF-8"):
        self._rich_source = None
        self._rich_target = None
        self._state_n = 0
        self.units = []

        if empty:
            return

        if not hasplurals(source):
            super(PoXliffUnit, self).__init__(source)
            return

        self.xmlelement = etree.Element(self.namespaced("group"))
        self.xmlelement.set("restype", "x-gettext-plurals")
        self.setsource(source)

    def __eq__(self, other):
        if isinstance(other, PoXliffUnit):
            if len(self.units) != len(other.units):
                return False
            if not super(PoXliffUnit, self).__eq__(other):
                return False
            for i in range(len(self.units) - 1):
                if not self.units[i+1] == other.units[i+1]:
                    return False
            return True
        if len(self.units) <= 1:
            if isinstance(other, lisa.LISAunit):
                return super(PoXliffUnit, self).__eq__(other)
            else:
                return self.source == other.source and self.target == other.target
        return False

#XXX: We don't return language nodes correctly at the moment
#    def getlanguageNodes(self):
#        if not self.hasplural():
#            return super(PoXliffUnit, self).getlanguageNodes()
#        else:
#            return self.units[0].getlanguageNodes()

    def setsource(self, source, sourcelang="en"):
        # TODO: consider changing from plural to singular, etc.
        self._rich_source = None
        if not hasplurals(source):
            super(PoXliffUnit, self).setsource(source, sourcelang)
        else:
            target = self.target
            for unit in self.units:
                try:
                    self.xmlelement.remove(unit.xmlelement)
                except xml.dom.NotFoundErr:
                    pass
            self.units = []
            for s in source.strings:
                newunit = xliff.xliffunit(s)
#                newunit.namespace = self.namespace #XXX?necessary?
                self.units.append(newunit)
                self.xmlelement.append(newunit.xmlelement)
            self.target = target

    # We don't support any rich strings yet
    multistring_to_rich = base.TranslationUnit.multistring_to_rich
    rich_to_multistring = base.TranslationUnit.rich_to_multistring

    rich_source = base.TranslationUnit.rich_source
    rich_target = base.TranslationUnit.rich_target

    def getsource(self):
        if not self.hasplural():
            return super(PoXliffUnit, self).getsource()
        else:
            strings = []
            strings.extend([unit.source for unit in self.units])
            return multistring(strings)
    source = property(getsource, setsource)

    def settarget(self, text, lang='xx', append=False):
        self._rich_target = None
        if self.gettarget() == text:
            return
        if not self.hasplural():
            super(PoXliffUnit, self).settarget(text, lang, append)
            return
        if not isinstance(text, multistring):
            text = multistring(text)
        source = self.source
        sourcel = len(source.strings)
        targetl = len(text.strings)
        if sourcel < targetl:
            sources = source.strings + [source.strings[-1]] * (targetl - sourcel)
            targets = text.strings
            id = self.getid()
            self.source = multistring(sources)
            self.setid(id)
        elif targetl < sourcel:
            targets = text.strings + [""] * (sourcel - targetl)
        else:
            targets = text.strings

        for i in range(len(self.units)):
            self.units[i].target = targets[i]

    def gettarget(self):
        if self.hasplural():
            strings = [unit.target for unit in self.units]
            if strings:
                return multistring(strings)
            else:
                return None
        else:
            return super(PoXliffUnit, self).gettarget()

    target = property(gettarget, settarget)

    def addnote(self, text, origin=None, position="append"):
        """Add a note specifically in a "note" tag"""
        if isinstance(text, str):
            text = text.decode("utf-8")
        note = etree.SubElement(self.xmlelement, self.namespaced("note"))
        note.text = text
        if origin:
            note.set("from", origin)
        for unit in self.units[1:]:
            unit.addnote(text, origin)

    def getnotes(self, origin=None):
        #NOTE: We support both <context> and <note> tags in xliff files for comments
        if origin == "translator":
            notes = super(PoXliffUnit, self).getnotes("translator")
            trancomments = self.gettranslatorcomments()
            if notes == trancomments or trancomments.find(notes) >= 0:
                notes = ""
            elif notes.find(trancomments) >= 0:
                trancomments = notes
                notes = ""
            trancomments = trancomments + notes
            return trancomments
        elif origin in ["programmer", "developer", "source code"]:
            devcomments = super(PoXliffUnit, self).getnotes("developer")
            autocomments = self.getautomaticcomments()
            if devcomments == autocomments or autocomments.find(devcomments) >= 0:
                devcomments = ""
            elif devcomments.find(autocomments) >= 0:
                autocomments = devcomments
                devcomments = ""
            return autocomments
        else:
            return super(PoXliffUnit, self).getnotes(origin)

    def markfuzzy(self, value=True):
        super(PoXliffUnit, self).markfuzzy(value)
        for unit in self.units[1:]:
            unit.markfuzzy(value)

    def marktranslated(self):
        super(PoXliffUnit, self).marktranslated()
        for unit in self.units[1:]:
            unit.marktranslated()

    def setid(self, id):
        super(PoXliffUnit, self).setid(id)
        if len(self.units) > 1:
            for i in range(len(self.units)):
                self.units[i].setid("%s[%d]" % (id, i))

    def getlocations(self):
        """Returns all the references (source locations)"""
        groups = self.getcontextgroups("po-reference")
        references = []
        for group in groups:
            sourcefile = ""
            linenumber = ""
            for (type, text) in group:
                if type == "sourcefile":
                    sourcefile = text
                elif type == "linenumber":
                    linenumber = text
            assert sourcefile
            if linenumber:
                sourcefile = sourcefile + ":" + linenumber
            references.append(sourcefile)
        return references

    def getautomaticcomments(self):
        """Returns the automatic comments (x-po-autocomment), which corresponds
        to the #. style po comments."""

        def hasautocomment((type, text)):
            return type == "x-po-autocomment"
        groups = self.getcontextgroups("po-entry")
        comments = []
        for group in groups:
            commentpairs = filter(hasautocomment, group)
            for (type, text) in commentpairs:
                comments.append(text)
        return "\n".join(comments)

    def gettranslatorcomments(self):
        """Returns the translator comments (x-po-trancomment), which corresponds
        to the # style po comments."""

        def hastrancomment((type, text)):
            return type == "x-po-trancomment"
        groups = self.getcontextgroups("po-entry")
        comments = []
        for group in groups:
            commentpairs = filter(hastrancomment, group)
            for (type, text) in commentpairs:
                comments.append(text)
        return "\n".join(comments)

    def isheader(self):
        return "gettext-domain-header" in (self.getrestype() or "")

    def istranslatable(self):
        return super(PoXliffUnit, self).istranslatable() and not self.isheader()

    @classmethod
    def createfromxmlElement(cls, element, namespace=None):
        if element.tag.endswith("trans-unit"):
            object = cls(None, empty=True)
            object.xmlelement = element
            object.namespace = namespace
            return object
        assert element.tag.endswith("group")
        group = cls(None, empty=True)
        group.xmlelement = element
        group.namespace = namespace
        units = list(element.iterdescendants(group.namespaced('trans-unit')))
        for unit in units:
            subunit = xliff.xliffunit.createfromxmlElement(unit)
            subunit.namespace = namespace
            group.units.append(subunit)
        return group

    def hasplural(self):
        return self.xmlelement.tag == self.namespaced("group")


class PoXliffFile(xliff.xlifffile, poheader.poheader):
    """a file for the po variant of Xliff files"""
    UnitClass = PoXliffUnit

    def __init__(self, *args, **kwargs):
        if not "sourcelanguage" in kwargs:
            kwargs["sourcelanguage"] = "en-US"
        xliff.xlifffile.__init__(self, *args, **kwargs)

    def createfilenode(self, filename, sourcelanguage="en-US", datatype="po"):
        # Let's ignore the sourcelanguage parameter opting for the internal
        # one. PO files will probably be one language
        return super(PoXliffFile, self).createfilenode(filename, sourcelanguage=self.sourcelanguage, datatype="po")

    def _insert_header(self, header):
        header.xmlelement.set("restype", "x-gettext-domain-header")
        header.xmlelement.set("approved", "no")
        lisa.setXMLspace(header.xmlelement, "preserve")
        self.addunit(header)

    def addheaderunit(self, target, filename):
        unit = self.addsourceunit(target, filename, True)
        unit.target = target
        unit.xmlelement.set("restype", "x-gettext-domain-header")
        unit.xmlelement.set("approved", "no")
        lisa.setXMLspace(unit.xmlelement, "preserve")
        return unit

    def addplural(self, source, target, filename, createifmissing=False):
        """This method should now be unnecessary, but is left for reference"""
        assert isinstance(source, multistring)
        if not isinstance(target, multistring):
            target = multistring(target)
        sourcel = len(source.strings)
        targetl = len(target.strings)
        if sourcel < targetl:
            sources = source.strings + [source.strings[-1]] * targetl - sourcel
            targets = target.strings
        else:
            sources = source.strings
            targets = target.strings
        self._messagenum += 1
        pluralnum = 0
        group = self.creategroup(filename, True, restype="x-gettext-plural")
        for (src, tgt) in zip(sources, targets):
            unit = self.UnitClass(src)
            unit.target = tgt
            unit.setid("%d[%d]" % (self._messagenum, pluralnum))
            pluralnum += 1
            group.append(unit.xmlelement)
            self.units.append(unit)

        if pluralnum < sourcel:
            for string in sources[pluralnum:]:
                unit = self.UnitClass(src)
                unit.xmlelement.set("translate", "no")
                unit.setid("%d[%d]" % (self._messagenum, pluralnum))
                pluralnum += 1
                group.append(unit.xmlelement)
                self.units.append(unit)

        return self.units[-pluralnum]

    def parse(self, xml):
        """Populates this object from the given xml string"""
        #TODO: Make more robust

        def ispluralgroup(node):
            """determines whether the xml node refers to a getttext plural"""
            return node.get("restype") == "x-gettext-plurals"

        def isnonpluralunit(node):
            """determindes whether the xml node contains a plural like id.

            We want to filter out all the plural nodes, except the very first
            one in each group.
            """
            return re.match(r"\d+\[[123456]\]$", node.get("id") or "") is None

        def pluralunits(pluralgroups):
            for pluralgroup in pluralgroups:
                yield self.UnitClass.createfromxmlElement(pluralgroup, namespace=self.namespace)

        self.filename = getattr(xml, 'name', '')
        if hasattr(xml, "read"):
            xml.seek(0)
            xmlsrc = xml.read()
            xml = xmlsrc
        self.document = etree.fromstring(xml).getroottree()
        self.initbody()
        root_node = self.document.getroot()
        assert root_node.tag == self.namespaced(self.rootNode)
        groups = root_node.iterdescendants(self.namespaced("group"))
        pluralgroups = filter(ispluralgroup, groups)
        termEntries = root_node.iterdescendants(self.namespaced(self.UnitClass.rootNode))

        singularunits = filter(isnonpluralunit, termEntries)
        if len(singularunits) == 0:
            return
        pluralunit_iter = pluralunits(pluralgroups)
        try:
            nextplural = pluralunit_iter.next()
        except StopIteration:
            nextplural = None

        for entry in singularunits:
            term = self.UnitClass.createfromxmlElement(entry, namespace=self.namespace)
            if nextplural and unicode(term.getid()) == ("%s[0]" % nextplural.getid()):
                self.addunit(nextplural, new=False)
                try:
                    nextplural = pluralunit_iter.next()
                except StopIteration as i:
                    nextplural = None
            else:
                self.addunit(term, new=False)

########NEW FILE########
__FILENAME__ = project
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os

from translate.convert import factory as convert_factory
from translate.storage.projstore import ProjectStore


__all__ = ['Project']


def split_extensions(filename):
    """Split the given filename into a name and extensions part.
        The extensions part is defined by any sequence of extensions, where an
        extension is a 3-letter, .-separated string or one of "po" or
        "properties". If the file name consists entirely out of extensions, the
        first part is assumed to be the file name and the rest extensions."""
    # FIXME: Isn't there a better place for this function?
    # XXX: Make sure that all extensions supported in translate.convert.factory
    #      that are not 3 letters long are added to the first "if" statement in
    #      split_extensions() below.
    filename_parts = filename.split(os.extsep)
    extensions = []
    for part in reversed(filename_parts):
        if len(part) != 3 and part not in ('po', 'properties'):
            break
        extensions.append(part)
    if not extensions:
        return filename, ''
    extensions = [x for x in reversed(extensions)]

    if len(extensions) == len(filename_parts):
        extensions = extensions[1:]
    return os.extsep.join(filename_parts[:-len(extensions)]), os.extsep.join(extensions)


class Project(object):
    """Manages a project store as well as the processes involved in a project
        workflow."""

    # INITIALIZERS #
    def __init__(self, projstore=None):
        if projstore is None:
            projstore = ProjectStore()
        self.store = projstore

    def __del__(self):
        if self.store:
            del self.store

    # METHODS #
    def add_source(self, srcfile, src_fname=None):
        """Proxy for ``self.store.append_sourcefile()``."""
        return self.store.append_sourcefile(srcfile, src_fname)

    def add_source_convert(self, srcfile, src_fname=None, convert_options=None, extension=None):
        """Convenience method that calls :meth:`~Project.add_source` and
        :meth:`~Project.convert_forward` and returns the results from both."""
        srcfile, srcfname = self.add_source(srcfile, src_fname)
        transfile, transfname = self.convert_forward(srcfname, convert_options=convert_options)
        return srcfile, srcfname, transfile, transfname

    def close(self):
        """Proxy for ``self.store.close()``."""
        self.store.close()

    def convert_forward(self, input_fname, template=None, output_fname=None, **options):
        """Convert the given input file to the next type in the process:

        Source document (eg. ODT) -> Translation file (eg. XLIFF) ->
        Translated document (eg. ODT).

        :type  input_fname: basestring
        :param input_fname: The project name of the file to convert
        :type  convert_options: dict (optional)
        :param convert_options: Passed as-is to
                                :meth:`translate.convert.factory.convert`.
        :returns 2-tuple: the converted file object and its project name."""
        inputfile = self.get_file(input_fname)
        input_type = self.store.get_filename_type(input_fname)

        if input_type == 'tgt':
            raise ValueError('Cannot convert a target document further: %s' % (input_fname))

        templ_fname = None
        if isinstance(template, basestring):
            template, templ_fname = self.get_file(template)

        if template and not templ_fname:
            templ_fname = template.name

        # Check if we can determine a template from the conversion map
        if template is None:
            convert_map = self.store.convert_map
            if input_fname in convert_map:
                templ_fname = convert_map[input_fname][1]
                template = self.get_file(templ_fname)
            elif input_type == 'trans':
                # inputfile is a translatable file, so it needed to be converted
                # from some input document. Let's try and use that document as a
                # template for this conversion.
                for in_name, (out_name, tmpl_name) in self.store.convert_map.items():
                    if input_fname == out_name:
                        template, templ_fname = self.get_file(in_name), in_name
                        break

        # Populate the conv_options dict with the options we can detect
        conv_options = dict(in_fname=input_fname)

        if input_fname in self.store.convert_map:
            out_name, tmpl_name = self.store.convert_map[input_fname]
            if out_name in self.store._files and options.get('overwrite_output', True):
                self.remove_file(out_name)

        converted_file, converted_ext = convert_factory.convert(
            inputfile,
            template=template,
            options=conv_options,
            convert_options=options.get('convert_options', None))

        # Determine the file name and path where the output should be moved.
        if not output_fname:
            _dir, fname = os.path.split(input_fname)
            directory = ''
            if hasattr(inputfile, 'name'):
                # Prefer to put it in the same directory as the input file
                directory, _fn = os.path.split(inputfile.name)
            else:
                # Otherwise put it in the current working directory
                directory = os.getcwd()
            output_fname = os.path.join(directory, fname)
        output_fname, output_ext = split_extensions(output_fname)
        output_ext_parts = output_ext.split(os.extsep)

        # Add the output suffix, if supplied
        if 'output_suffix' in options:
            output_fname += options['output_suffix']

        # Check if we are in the situation where the output has an extension
        # of, for example, .odt.xlf.odt. If so, we want to change that to only
        # .odt.
        if len(output_ext_parts) >= 2 and output_ext_parts[-2] == converted_ext:
            output_ext_parts = output_ext_parts[:-1]
        else:
            output_ext_parts.append(converted_ext)
        output_fname += os.extsep.join([''] + output_ext_parts)

        if os.path.isfile(output_fname):
            # If the output file already exist, we can't assume that it's safe
            # to overwrite it.
            os.unlink(converted_file.name)
            raise IOError("Output file already exists: %s" % (output_fname))

        os.rename(converted_file.name, output_fname)

        output_type = self.store.TYPE_INFO['next_type'][input_type]
        outputfile, output_fname = self.store.append_file(
            output_fname, None, ftype=output_type, delete_orig=True)
        self.store.convert_map[input_fname] = (output_fname, templ_fname)

        return outputfile, output_fname

    def export_file(self, fname, destfname):
        """Export the file with the specified filename to the given destination.
            This method will raise
            :exc:`~translate.storage.projstore.FileNotInProjectError`
            via the call to
            :meth:`~translate.storage.projstore.ProjectStore.get_file`
            if *fname* is not found in the project."""
        open(destfname, 'w').write(self.store.get_file(fname).read())

    def get_file(self, fname):
        """Proxy for ``self.store.get_file()``."""
        return self.store.get_file(fname)

    def get_proj_filename(self, realfname):
        """Proxy for ``self.store.get_proj_filename()``."""
        return self.store.get_proj_filename(realfname)

    def get_real_filename(self, projfname):
        """Try and find a real file name for the given project file name."""
        projfile = self.get_file(projfname)
        rfname = getattr(projfile, 'name', getattr(projfile, 'filename', None))
        if rfname is None:
            raise ValueError('Project file has no real file: %s' % (projfname))
        return rfname

    def remove_file(self, projfname, ftype=None):
        """Proxy for ``self.store.remove_file()``."""
        self.store.remove_file(projfname, ftype)

    def save(self, filename=None):
        """Proxy for ``self.store.save()``."""
        self.store.save(filename)

    def update_file(self, proj_fname, infile):
        """Proxy for ``self.store.update_file()``."""
        self.store.update_file(proj_fname, infile)

########NEW FILE########
__FILENAME__ = projstore
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os

from lxml import etree


__all__ = ['FileExistsInProjectError', 'FileNotInProjectError', 'ProjectStore']


class FileExistsInProjectError(Exception):
    pass


class FileNotInProjectError(Exception):
    pass


class ProjectStore(object):
    """Basic project file container."""

    # INITIALIZERS #
    def __init__(self):
        self._files = {}
        self._sourcefiles = []
        self._targetfiles = []
        self._transfiles = []
        self.settings = {}
        self.convert_map = {}
        # The above map maps the conversion of input files (keys) to its output
        # file and template used (2-tuple). All values are project file names.
        # eg. convert_map = {
        #    'sources/doc.odt':   ('trans/doc.odt.xlf', None),
        #    'trans/doc.odt.xlf': ('targets/doc.odt', 'sources/doc.odt')
        #}

        # The following dict groups together sets of mappings from a file
        # "type" string ("src", "tgt" or "trans") to various other values
        # or objects.
        self.TYPE_INFO = {
            # type => prefix for new files
            'f_prefix': {
                'src': 'sources/',
                'tgt': 'targets/',
                'trans': 'trans/',
            },
            # type => list containing filenames for that type
            'lists': {
                'src': self._sourcefiles,
                'tgt': self._targetfiles,
                'trans': self._transfiles,
            },
            # type => next type in process: src => trans => tgt
            'next_type': {
                'src': 'trans',
                'trans': 'tgt',
                'tgt': None,
            },
            # type => name of the sub-section in the settings file/dict
            'settings': {
                'src': 'sources',
                'tgt': 'targets',
                'trans': 'transfiles',
            }
        }

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    # ACCESSORS #
    def _get_sourcefiles(self):
        """Read-only access to ``self._sourcefiles``."""
        return tuple(self._sourcefiles)
    sourcefiles = property(_get_sourcefiles)

    def _get_targetfiles(self):
        """Read-only access to ``self._targetfiles``."""
        return tuple(self._targetfiles)
    targetfiles = property(_get_targetfiles)

    def _get_transfiles(self):
        """Read-only access to ``self._transfiles``."""
        return tuple(self._transfiles)
    transfiles = property(_get_transfiles)

    # SPECIAL METHODS #
    def __in__(self, lhs):
        """@returns ``True`` if ``lhs`` is a file name or file object in the project store."""
        return lhs in self._sourcefiles or \
               lhs in self._targetfiles or \
               lhs in self._transfiles or \
               lhs in self._files or \
               lhs in self._files.values()

    # METHODS #
    def append_file(self, afile, fname, ftype='trans', delete_orig=False):
        """Append the given file to the project with the given filename, marked
            to be of type ``ftype`` ('src', 'trans', 'tgt').

            :type  delete_orig: bool
            :param delete_orig: Whether or not the original (given) file should
                                be deleted after being appended. This is set to
                                ``True`` by
                                :meth:`~translate.storage.project.convert_forward`
                                . Not used in this class."""
        if not ftype in self.TYPE_INFO['f_prefix']:
            raise ValueError('Invalid file type: %s' % (ftype))

        if isinstance(afile, basestring) and os.path.isfile(afile) and not fname:
            # Try and use afile as the file name
            fname, afile = afile, open(afile)

        # Check if we can get an real file name
        realfname = fname
        if realfname is None or not os.path.isfile(realfname):
            realfname = getattr(afile, 'name', None)
        if realfname is None or not os.path.isfile(realfname):
            realfname = getattr(afile, 'filename', None)
        if not realfname or not os.path.isfile(realfname):
            realfname = None

        # Try to get the file name from the file object, if it was not given:
        if not fname:
            fname = getattr(afile, 'name', None)
        if not fname:
            fname = getattr(afile, 'filename', None)

        fname = self._fix_type_filename(ftype, fname)

        if not fname:
            raise ValueError('Could not deduce file name and none given')
        if fname in self._files:
            raise FileExistsInProjectError(fname)

        if realfname is not None and os.path.isfile(realfname):
            self._files[fname] = realfname
        else:
            self._files[fname] = afile
        self.TYPE_INFO['lists'][ftype].append(fname)

        return afile, fname

    def append_sourcefile(self, afile, fname=None):
        return self.append_file(afile, fname, ftype='src')

    def append_targetfile(self, afile, fname=None):
        return self.append_file(afile, fname, ftype='tgt')

    def append_transfile(self, afile, fname=None):
        return self.append_file(afile, fname, ftype='trans')

    def remove_file(self, fname, ftype=None):
        """Remove the file with the given project name from the project.
            If the file type ('src', 'trans' or 'tgt') is not given, it is
            guessed."""
        if fname not in self._files:
            raise FileNotInProjectError(fname)
        if not ftype:
            # Guess file type (source/trans/target)
            for ft, prefix in self.TYPE_INFO['f_prefix'].items():
                if fname.startswith(prefix):
                    ftype = ft
                    break

        self.TYPE_INFO['lists'][ftype].remove(fname)
        if self._files[fname] and hasattr(self._files[fname], 'close'):
            self._files[fname].close()
        del self._files[fname]

    def remove_sourcefile(self, fname):
        self.remove_file(fname, ftype='src')

    def remove_targetfile(self, fname):
        self.remove_file(fname, ftype='tgt')

    def remove_transfile(self, fname):
        self.remove_file(fname, ftype='trans')

    def close(self):
        self.save()

    def get_file(self, fname, mode='rb'):
        """Retrieve the file with the given name from the project store.

        The file is looked up in the ``self._files`` dictionary. The values
        in this dictionary may be ``None``, to indicate that the file is not
        cacheable and needs to be retrieved in a special way. This special
        way must be defined in this method of sub-classes. The value may
        also be a string, which indicates that it is a real file accessible
        via ``open``.

        :type  mode: str
        :param mode: The mode in which to re-open the file (if it is closed).
        """
        if fname not in self._files:
            raise FileNotInProjectError(fname)

        rfile = self._files[fname]
        if isinstance(rfile, basestring):
            rfile = open(rfile, 'rb')
        # Check that the file is actually open
        if getattr(rfile, 'closed', False):
            rfname = fname
            if not os.path.isfile(rfname):
                rfname = getattr(rfile, 'name', None)
            if not rfile or not os.path.isfile(rfname):
                rfname = getattr(rfile, 'filename', None)
            if not rfile or not os.path.isfile(rfname):
                raise IOError('Could not locate file: %s (%s)' % (rfile, fname))
            rfile = open(rfname, mode)
            self._files[fname] = rfile

        return rfile

    def get_filename_type(self, fname):
        """Get the type of file ('src', 'trans', 'tgt') with the given name."""
        for ftype in self.TYPE_INFO['lists']:
            if fname in self.TYPE_INFO['lists'][ftype]:
                return ftype
        raise FileNotInProjectError(fname)

    def get_proj_filename(self, realfname):
        """Try and find a project file name for the given real file name."""
        for fname in self._files:
            if fname == realfname or self._files[fname] == realfname:
                return fname
        raise ValueError('Real file not in project store: %s' % (realfname))

    def load(self, *args, **kwargs):
        """Load the project in some way. Undefined for this (base) class."""
        pass

    def save(self, filename=None, *args, **kwargs):
        """Save the project in some way. Undefined for this (base) class."""
        pass

    def update_file(self, pfname, infile):
        """Remove the project file with name ``pfname`` and add the contents
            from ``infile`` to the project under the same file name.

            :returns: the results from :meth:`ProjectStore.append_file`."""
        ftype = self.get_filename_type(pfname)
        self.remove_file(pfname)
        self.append_file(infile, pfname, ftype)

    def _fix_type_filename(self, ftype, fname):
        """Strip the path from the filename and prepend the correct prefix."""
        path, fname = os.path.split(fname)
        return self.TYPE_INFO['f_prefix'][ftype] + fname

    def _generate_settings(self):
        """@returns A XML string that represents the current settings."""
        xml = etree.Element('translationproject')

        # Add file names to settings XML
        if self._sourcefiles:
            sources_el = etree.Element('sources')
            for fname in self._sourcefiles:
                src_el = etree.Element('filename')
                src_el.text = fname
                sources_el.append(src_el)
            xml.append(sources_el)
        if self._transfiles:
            transfiles_el = etree.Element('transfiles')
            for fname in self._transfiles:
                trans_el = etree.Element('filename')
                trans_el.text = fname
                transfiles_el.append(trans_el)
            xml.append(transfiles_el)
        if self._targetfiles:
            target_el = etree.Element('targets')
            for fname in self._targetfiles:
                tgt_el = etree.Element('filename')
                tgt_el.text = fname
                target_el.append(tgt_el)
            xml.append(target_el)

        # Add conversion mappings
        if self.convert_map:
            conversions_el = etree.Element('conversions')
            for in_fname, (out_fname, templ_fname) in self.convert_map.iteritems():
                if in_fname not in self._files or out_fname not in self._files:
                    continue
                conv_el = etree.Element('conv')

                input_el = etree.Element('input')
                input_el.text = in_fname
                conv_el.append(input_el)

                output_el = etree.Element('output')
                output_el.text = out_fname
                conv_el.append(output_el)

                if templ_fname:
                    templ_el = etree.Element('template')
                    templ_el.text = templ_fname
                    conv_el.append(templ_el)

                conversions_el.append(conv_el)
            xml.append(conversions_el)

        # Add options to settings
        if 'options' in self.settings:
            options_el = etree.Element('options')
            for option, value in self.settings['options'].items():
                opt_el = etree.Element('option')
                opt_el.attrib['name'] = option
                opt_el.text = value
                options_el.append(opt_el)
            xml.append(options_el)

        return etree.tostring(xml, pretty_print=True)

    def _load_settings(self, settingsxml):
        """Load project settings from the given XML string.
        ``settingsxml`` is parsed into a DOM tree (``lxml.etree.fromstring``)
        which is then inspected."""
        settings = {}
        xml = etree.fromstring(settingsxml)

        # Load files in project
        for section in ('sources', 'targets', 'transfiles'):
            groupnode = xml.find(section)
            if groupnode is None:
                continue

            settings[section] = []
            for fnode in groupnode.getchildren():
                settings[section].append(fnode.text)

        conversions_el = xml.find('conversions')
        if conversions_el is not None:
            self.convert_map = {}
            for conv_el in conversions_el.iterchildren():
                in_fname, out_fname, templ_fname = None, None, None
                for child_el in conv_el.iterchildren():
                    if child_el.tag == 'input':
                        in_fname = child_el.text
                    elif child_el.tag == 'output':
                        out_fname = child_el.text
                    elif child_el.tag == 'template':
                        templ_fname = child_el.text
                # Make sure that in_fname and out_fname exist in
                # settings['sources'], settings['targets'] or
                # settings['transfiles']
                in_found, out_found, templ_found = False, False, False
                for section in ('sources', 'transfiles', 'targets'):
                    if section not in settings:
                        continue
                    if in_fname in settings[section]:
                        in_found = True
                    if out_fname in settings[section]:
                        out_found = True
                    if templ_fname and templ_fname in settings[section]:
                        templ_found = True
                if in_found and out_found and (not templ_fname or templ_found):
                    self.convert_map[in_fname] = (out_fname, templ_fname)

        # Load options
        groupnode = xml.find('options')
        if groupnode is not None:
            settings['options'] = {}
            for opt in groupnode.iterchildren():
                settings['options'][opt.attrib['name']] = opt.text

        self.settings = settings

########NEW FILE########
__FILENAME__ = properties
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2014 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of .properties, and similar, files that are used in
translating Java, Mozilla, MacOS and other software.

The :class:`propfile` class is a monolingual class with :class:`propunit`
providing unit level access.

The .properties store has become a general key value pair class with
:class:`Dialect` providing the ability to change the behaviour of the
parsing and handling of the various dialects.

Currently we support:

- Java .properties
- Mozilla .properties
- Adobe Flex files
- MacOS X .strings files
- Skype .lang files

The following provides references and descriptions of the various
dialects supported:

Java
    Java .properties are supported completely except for the ability to drop
    pairs that are not translated.

    The following `.properties file description
    <http://docs.oracle.com/javase/1.4.2/docs/api/java/util/Properties.html#load(java.io.InputStream)>`_
    gives a good references to the .properties specification.

    Properties file may also hold Java `MessageFormat
    <http://docs.oracle.com/javase/1.4.2/docs/api/java/text/MessageFormat.html>`_
    messages.  No special handling is provided in this storage class for
    MessageFormat, but this may be implemented in future.

    All delimiter types, comments, line continuations and spaces handling in
    delimeters are supported.

Mozilla
    Mozilla files use '=' as a delimiter, are UTF-8 encoded and thus don't
    need \\u escaping.  Any \\U values will be converted to correct Unicode
    characters.

Strings
    Mac OS X strings files are implemented using
    `these <https://developer.apple.com/library/mac/#documentation/MacOSX/Conceptual/BPInternational/Articles/StringsFiles.html>`_
    `two <https://developer.apple.com/library/mac/#documentation/Cocoa/Conceptual/LoadingResources/Strings/Strings.html>`_
    articles as references.

Flex
    Adobe Flex files seem to be normal .properties files but in UTF-8 just like
    Mozilla files. This
    `page <http://livedocs.adobe.com/flex/3/html/help.html?content=l10n_3.html>`_
    provides the information used to implement the dialect.

Skype
    Skype .lang files seem to be UTF-16 encoded .properties files.

A simple summary of what is permissible follows.

Comments supported:

.. code-block:: properties

   # a comment
   ! a comment
   // a comment (only at the beginning of a line)
   /* a comment (not across multiple lines) */

Name and Value pairs:

.. code-block:: properties

   # Delimiters
   key = value
   key : value
   key value

   # Space in key and around value
   \ key\ = \ value

   # Note that the b and c are escaped for reST rendering
   b = a string with escape sequences \\t \\n \\r \\\\ \\" \\' \\ (space) \u0123
   c = a string with a continuation line \\
       continuation line

   # Special cases
   # key with no value
   key
   # value no key (extractable in prop2po but not mergeable in po2prop)
   =value

   # .strings specific
   "key" = "value";

"""

import re

from translate.lang import data
from translate.misc import quote
from translate.misc.deprecation import deprecated
from translate.storage import base


labelsuffixes = (".label", ".title")
"""Label suffixes: entries with this suffix are able to be comibed with accesskeys
found in in entries ending with :attr:`.accesskeysuffixes`"""
accesskeysuffixes = (".accesskey", ".accessKey", ".akey")
"""Accesskey Suffixes: entries with this suffix may be combined with labels
ending in :attr:`.labelsuffixes` into accelerator notation"""


# the rstripeols convert dos <-> unix nicely as well
# output will be appropriate for the platform

eol = "\n"


def _find_delimiter(line, delimiters):
    """Find the type and position of the delimiter in a property line.

    Property files can be delimited by "=", ":" or whitespace (space for now).
    We find the position of each delimiter, then find the one that appears
    first.

    :param line: A properties line
    :type line: str
    :param delimiters: valid delimiters
    :type delimiters: list
    :return: delimiter character and offset within *line*
    :rtype: Tuple (delimiter char, Offset Integer)
    """
    delimiter_dict = {}
    for delimiter in delimiters:
        delimiter_dict[delimiter] = -1
    delimiters = delimiter_dict
    # Find the position of each delimiter type
    for delimiter, pos in delimiters.iteritems():
        prewhitespace = len(line) - len(line.lstrip())
        pos = line.find(delimiter, prewhitespace)
        while pos != -1:
            if delimiters[delimiter] == -1 and line[pos-1] != u"\\":
                delimiters[delimiter] = pos
                break
            pos = line.find(delimiter, pos + 1)
    # Find the first delimiter
    mindelimiter = None
    minpos = -1
    for delimiter, pos in delimiters.iteritems():
        if pos == -1 or delimiter == u" ":
            continue
        if minpos == -1 or pos < minpos:
            minpos = pos
            mindelimiter = delimiter
    if mindelimiter is None and delimiters.get(u" ", -1) != -1:
        # Use space delimiter if we found nothing else
        return (u" ", delimiters[" "])
    if (mindelimiter is not None and
        u" " in delimiters and
        delimiters[u" "] < delimiters[mindelimiter]):
        # If space delimiter occurs earlier than ":" or "=" then it is the
        # delimiter only if there are non-whitespace characters between it and
        # the other detected delimiter.
        if len(line[delimiters[u" "]:delimiters[mindelimiter]].strip()) > 0:
            return (u" ", delimiters[u" "])
    return (mindelimiter, minpos)


@deprecated("Use Dialect.find_delimiter instead")
def find_delimeter(line):
    """Misspelled function that is kept around in case someone relies on it.

    .. deprecated:: 1.7.0
       Use :func:`find_delimiter` instead
    """
    return _find_delimiter(line, DialectJava.delimiters)


def is_line_continuation(line):
    """Determine whether *line* has a line continuation marker.

    .properties files can be terminated with a backslash (\\) indicating
    that the 'value' continues on the next line.  Continuation is only
    valid if there are an odd number of backslashses (an even number
    would result in a set of N/2 slashes not an escape)

    :param line: A properties line
    :type line: str
    :return: Does *line* end with a line continuation
    :rtype: Boolean
    """
    pos = -1
    count = 0
    if len(line) == 0:
        return False
    # Count the slashes from the end of the line. Ensure we don't
    # go into infinite loop.
    while len(line) >= -pos and line[pos:][0] == "\\":
        pos -= 1
        count += 1
    return (count % 2) == 1  # Odd is a line continuation, even is not


def is_comment_one_line(line):
    """Determine whether a *line* is a one-line comment.

    :param line: A properties line
    :type line: unicode
    :return: True if line is a one-line comment
    :rtype: bool
    """
    stripped = line.strip()
    line_starters = (u'#', u'!', u'//', )
    for starter in line_starters:
        if stripped.startswith(starter):
            return True
    if stripped.startswith(u'/*') and stripped.endswith(u'*/'):
        return True
    return False


def is_comment_start(line):
    """Determine whether a *line* starts a new multi-line comment.

    :param line: A properties line
    :type line: unicode
    :return: True if line starts a new multi-line comment
    :rtype: bool
    """
    stripped = line.strip()
    return stripped.startswith('/*') and not stripped.endswith('*/')


def is_comment_end(line):
    """Determine whether a *line* ends a new multi-line comment.

    :param line: A properties line
    :type line: unicode
    :return: True if line ends a new multi-line comment
    :rtype: bool
    """
    stripped = line.strip()
    return not stripped.startswith('/*') and stripped.endswith('*/')


def _key_strip(key):
    """Cleanup whitespace found around a key

    :param key: A properties key
    :type key: str
    :return: Key without any unneeded whitespace
    :rtype: str
    """
    newkey = key.rstrip()
    # If string now ends in \ we put back the whitespace that was escaped
    if newkey[-1:] == "\\":
        newkey += key[len(newkey):len(newkey)+1]
    return newkey.lstrip()

dialects = {}
default_dialect = "java"


def register_dialect(dialect):
    """Decorator that registers the dialect."""
    dialects[dialect.name] = dialect
    return dialect


def get_dialect(dialect=default_dialect):
    return dialects.get(dialect)


class Dialect(object):
    """Settings for the various behaviours in key=value files."""
    name = None
    default_encoding = 'iso-8859-1'
    delimiters = None
    pair_terminator = u""
    key_wrap_char = u""
    value_wrap_char = u""
    drop_comments = []

    @classmethod
    def encode(cls, string, encoding=None):
        """Encode the string"""
        # FIXME: dialects are a bad idea, not possible for subclasses
        # to override key methods
        if encoding != "utf-8":
            return quote.javapropertiesencode(string or u"")
        return string or u""

    @classmethod
    def find_delimiter(cls, line):
        """Find the delimiter"""
        return _find_delimiter(line, cls.delimiters)

    @classmethod
    def key_strip(cls, key):
        """Strip unneeded characters from the key"""
        return _key_strip(key)

    @classmethod
    def value_strip(cls, value):
        """Strip unneeded characters from the value"""
        return value.lstrip()


@register_dialect
class DialectJava(Dialect):
    name = "java"
    default_encoding = "iso-8859-1"
    delimiters = [u"=", u":", u" "]


@register_dialect
class DialectJavaUtf8(DialectJava):
    name = "java-utf8"
    default_encoding = "utf-8"
    delimiters = [u"=", u":", u" "]

    @classmethod
    def encode(cls, string, encoding=None):
        return quote.mozillapropertiesencode(string or u"")


@register_dialect
class DialectFlex(DialectJava):
    name = "flex"
    default_encoding = "utf-8"


@register_dialect
class DialectMozilla(DialectJavaUtf8):
    name = "mozilla"
    delimiters = [u"="]

    @classmethod
    def encode(cls, string, encoding=None):
        """Encode the string"""
        string = quote.mozillapropertiesencode(string or u"")
        string = quote.mozillaescapemarginspaces(string or u"")
        return string


@register_dialect
class DialectGaia(DialectMozilla):
    name = "gaia"
    delimiters = [u"="]


@register_dialect
class DialectSkype(Dialect):
    name = "skype"
    default_encoding = "utf-16"
    delimiters = [u"="]

    @classmethod
    def encode(cls, string, encoding=None):
        return quote.mozillapropertiesencode(string or u"")


@register_dialect
class DialectStrings(Dialect):
    name = "strings"
    default_encoding = "utf-16"
    delimiters = [u"="]
    pair_terminator = u";"
    key_wrap_char = u'"'
    value_wrap_char = u'"'
    out_ending = u';'
    out_delimiter_wrappers = u' '
    drop_comments = ["/* No comment provided by engineer. */"]

    @classmethod
    def key_strip(cls, key):
        """Strip unneeded characters from the key"""
        newkey = key.rstrip().rstrip('"')
        # If string now ends in \ we put back the char that was escaped
        if newkey[-1:] == "\\":
            newkey += key[len(newkey):len(newkey)+1]
        ret = newkey.lstrip().lstrip('"')
        return ret.replace('\\"', '"')

    @classmethod
    def value_strip(cls, value):
        """Strip unneeded characters from the value"""
        newvalue = value.rstrip().rstrip(';').rstrip('"')
        # If string now ends in \ we put back the char that was escaped
        if newvalue[-1:] == "\\":
            newvalue += value[len(newvalue):len(newvalue)+1]
        ret = newvalue.lstrip().lstrip('"')
        return ret.replace('\\"', '"')

    @classmethod
    def encode(cls, string, encoding=None):
        return string.replace("\n", r"\n").replace("\t", r"\t")


class propunit(base.TranslationUnit):
    """An element of a properties file i.e. a name and value, and any
    comments associated."""

    def __init__(self, source="", personality="java"):
        """Construct a blank propunit."""
        self.personality = get_dialect(personality)
        super(propunit, self).__init__(source)
        self.name = u""
        self.value = u""
        self.translation = u""
        self.delimiter = u"="
        self.comments = []
        self.source = source
        # a pair of symbols to enclose delimiter on the output
        # (a " " can be used for the sake of convenience)
        self.out_delimiter_wrappers = getattr(self.personality,
                                              'out_delimiter_wrappers', u'')
        # symbol that should end every property sentence
        # (e.g. ";" is required for Mac OS X strings)
        self.out_ending = getattr(self.personality, 'out_ending', u'')

    def getsource(self):
        value = quote.propertiesdecode(self.value)
        return value

    def setsource(self, source):
        self._rich_source = None
        source = data.forceunicode(source)
        self.value = self.personality.encode(source or u"", self.encoding)

    source = property(getsource, setsource)

    def gettarget(self):
        translation = quote.propertiesdecode(self.translation)
        translation = re.sub(u"\\\\ ", u" ", translation)
        return translation

    def settarget(self, target):
        self._rich_target = None
        target = data.forceunicode(target)
        self.translation = self.personality.encode(target or u"",
                                                   self.encoding)

    target = property(gettarget, settarget)

    @property
    def encoding(self):
        if self._store:
            return self._store.encoding
        else:
            return self.personality.default_encoding

    def __str__(self):
        """Convert to a string. Double check that unicode is handled
        somehow here."""
        source = self.getoutput()
        assert isinstance(source, unicode)
        return source.encode(self.encoding)

    def getoutput(self):
        """Convert the element back into formatted lines for a
        .properties file"""
        notes = self.getnotes()
        if notes:
            notes += u"\n"
        if self.isblank():
            return notes + u"\n"
        else:
            self.value = self.personality.encode(self.source, self.encoding)
            self.translation = self.personality.encode(self.target,
                                                       self.encoding)
            # encode key, if needed
            key = self.name
            kwc = self.personality.key_wrap_char
            if kwc:
                key = key.replace(kwc, '\\%s' % kwc)
                key = '%s%s%s' % (kwc, key, kwc)
            # encode value, if needed
            value = self.translation or self.value
            vwc = self.personality.value_wrap_char
            if vwc:
                value = value.replace(vwc, '\\%s' % vwc)
                value = '%s%s%s' % (vwc, value, vwc)
            wrappers = self.out_delimiter_wrappers
            delimiter = '%s%s%s' % (wrappers, self.delimiter, wrappers)
            ending = self.out_ending
            out_dict = {
                "notes": notes,
                "key": key,
                "del": delimiter,
                "value": value,
                "ending": ending,
            }
            return u"%(notes)s%(key)s%(del)s%(value)s%(ending)s\n" % out_dict

    def getlocations(self):
        return [self.name]

    def addnote(self, text, origin=None, position="append"):
        if origin in ['programmer', 'developer', 'source code', None]:
            text = data.forceunicode(text)
            self.comments.append(text)
        else:
            return super(propunit, self).addnote(text, origin=origin,
                                                 position=position)

    def getnotes(self, origin=None):
        if origin in ['programmer', 'developer', 'source code', None]:
            return u'\n'.join(self.comments)
        else:
            return super(propunit, self).getnotes(origin)

    def removenotes(self):
        self.comments = []

    def isblank(self):
        """returns whether this is a blank element, containing only
        comments."""
        return not (self.name or self.value)

    def istranslatable(self):
        return bool(self.name)

    def getid(self):
        return self.name

    def setid(self, value):
        self.name = value


class propfile(base.TranslationStore):
    """this class represents a .properties file, made up of propunits"""
    UnitClass = propunit

    def __init__(self, inputfile=None, personality="java", encoding=None):
        """construct a propfile, optionally reading in from inputfile"""
        super(propfile, self).__init__(unitclass=self.UnitClass)
        self.personality = get_dialect(personality)
        self.encoding = encoding or self.personality.default_encoding
        self.filename = getattr(inputfile, 'name', '')
        if inputfile is not None:
            propsrc = inputfile.read()
            inputfile.close()
            self.parse(propsrc)
            self.makeindex()

    def parse(self, propsrc):
        """Read the source of a properties file in and include them
        as units."""
        text, encoding = self.detect_encoding(propsrc,
            default_encodings=[self.personality.default_encoding, 'utf-8',
                               'utf-16'])
        if not text:
            raise IOError("Cannot detect encoding for %s." % (self.filename or
                                                              "given string"))
        self.encoding = encoding
        propsrc = text

        newunit = propunit("", self.personality.name)
        inmultilinevalue = False
        inmultilinecomment = False

        for line in propsrc.split(u"\n"):
            # handle multiline value if we're in one
            line = quote.rstripeol(line)
            if inmultilinevalue:
                newunit.value += line.lstrip()
                # see if there's more
                inmultilinevalue = is_line_continuation(newunit.value)
                # if we're still waiting for more...
                if inmultilinevalue:
                    # strip the backslash
                    newunit.value = newunit.value[:-1]
                if not inmultilinevalue:
                    # we're finished, add it to the list...
                    self.addunit(newunit)
                    newunit = propunit("", self.personality.name)
            # otherwise, this could be a comment
            # FIXME handle // inline comments
            elif (inmultilinecomment or is_comment_one_line(line) or
                  is_comment_start(line) or is_comment_end(line)):
                # add a comment
                if line not in self.personality.drop_comments:
                    newunit.comments.append(line)
                if is_comment_start(line):
                    inmultilinecomment = True
                elif is_comment_end(line):
                    inmultilinecomment = False
            elif not line.strip():
                # this is a blank line...
                if str(newunit).strip():
                    self.addunit(newunit)
                    newunit = propunit("", self.personality.name)
            else:
                newunit.delimiter, delimiter_pos = self.personality.find_delimiter(line)
                if delimiter_pos == -1:
                    newunit.name = self.personality.key_strip(line)
                    newunit.value = u""
                    self.addunit(newunit)
                    newunit = propunit("", self.personality.name)
                else:
                    newunit.name = self.personality.key_strip(line[:delimiter_pos])
                    if is_line_continuation(line[delimiter_pos+1:].lstrip()):
                        inmultilinevalue = True
                        newunit.value = line[delimiter_pos+1:].lstrip()[:-1]
                    else:
                        newunit.value = self.personality.value_strip(line[delimiter_pos+1:])
                        self.addunit(newunit)
                        newunit = propunit("", self.personality.name)
        # see if there is a leftover one...
        if inmultilinevalue or len(newunit.comments) > 0:
            self.addunit(newunit)

    def __str__(self):
        """Convert the units back to lines."""
        lines = []
        for unit in self.units:
            lines.append(unit.getoutput())
        uret = u"".join(lines)
        return uret.encode(self.encoding)

class javafile(propfile):
    Name = "Java Properties"
    Extensions = ['properties']

    def __init__(self, *args, **kwargs):
        kwargs['personality'] = "java"
        kwargs['encoding'] = "auto"
        super(javafile, self).__init__(*args, **kwargs)


class javautf8file(propfile):
    Name = "Java Properties (UTF-8)"
    Extensions = ['properties']

    def __init__(self, *args, **kwargs):
        kwargs['personality'] = "java-utf8"
        kwargs['encoding'] = "utf-8"
        super(javautf8file, self).__init__(*args, **kwargs)


class stringsfile(propfile):
    Name = "OS X Strings"
    Extensions = ['strings']

    def __init__(self, *args, **kwargs):
        kwargs['personality'] = "strings"
        super(stringsfile, self).__init__(*args, **kwargs)

########NEW FILE########
__FILENAME__ = pypo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2009 Zuza Software Foundation
# Copyright 2013 F Wolff
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of Gettext .po files (pounit) or entire
files (pofile).
"""

import copy
import re
import textwrap
from cStringIO import StringIO

from translate.lang import data
from translate.misc import quote
from translate.misc.deprecation import deprecated
from translate.misc.multistring import multistring
from translate.storage import base, pocommon, poparser
from translate.storage.pocommon import encodingToUse


lsep = "\n#: "
"""Separator for #: entries"""

# general functions for quoting / unquoting po strings

po_unescape_map = {"\\r": "\r", "\\t": "\t", '\\"': '"', '\\n': '\n', '\\\\': '\\'}
po_escape_map = dict([(value, key) for (key, value) in po_unescape_map.items()])


def escapeforpo(line):
    """Escapes a line for po format. assumes no \n occurs in the line.

    :param line: unescaped text
    """
    special_locations = []
    for special_key in po_escape_map:
        special_locations.extend(quote.find_all(line, special_key))
    special_locations = dict.fromkeys(special_locations).keys()
    special_locations.sort()
    escaped_line = ""
    last_location = 0
    for location in special_locations:
        escaped_line += line[last_location:location]
        escaped_line += po_escape_map[line[location:location+1]]
        last_location = location + 1
    escaped_line += line[last_location:]
    return escaped_line


def unescapehandler(escape):
    return po_unescape_map.get(escape, escape)


wrapper = textwrap.TextWrapper(
        width=77,
        replace_whitespace=False,
        expand_tabs=False,
        drop_whitespace=False
)
wrapper.wordsep_re = re.compile(
    r'(\s+|'                                  # any whitespace
    r'\w*\\.|'                                # any escape should not be split
    r'[\w\!\'\&\.\,\?]+\s+|'                  # space should go with a word
    r'[^\s\w]*\w+[a-zA-Z]-(?=\w+[a-zA-Z])|'   # hyphenated words
    r'(?<=[\w\!\"\'\&\.\,\?])-{2,}(?=\w))')   # em-dash
wrapper.wordsep_re_uni = re.compile(wrapper.wordsep_re.pattern, re.UNICODE)


def quoteforpo(text):
    """Quotes the given text for a PO file, returning quoted and
    escaped lines"""
    if text is None:
        return []
    text = escapeforpo(text)
    lines = text.split(u"\\n")
    for i, l in enumerate(lines[:-1]):
        lines[i] = l + u"\\n"

    polines = []
    len_lines = len(lines)
    if len_lines > 2 or (len_lines == 2 and lines[1]) or len(lines[0]) > 71:
        polines.append(u'""')
    for line in lines:
        lns = wrapper.wrap(line)
        for ln in lns:
            polines.append(u'"%s"' % ln)
    return polines


@deprecated("Use pypo.unescape() instead")
def extractpoline(line):
    """Remove quote and unescape line from po file.

    :param line: a quoted line from a po file (msgid or msgstr)

    .. deprecated:: 1.10
       Replaced by :func:`unescape`. :func:`extractpoline` is kept to allow
       tests of correctness, and in case of external users.
    """
    extracted = quote.extractwithoutquotes(line, '"', '"', '\\', includeescapes=unescapehandler)[0]
    return extracted


def unescape(line):
    """Unescape the given line.

    Quotes on either side should already have been removed.
    """
    escape_places = quote.find_all(line, u"\\")
    if not escape_places:
        return line

    # filter escaped escapes
    true_escape = False
    true_escape_places = []
    for escape_pos in escape_places:
        if escape_pos - 1 in escape_places:
            true_escape = not true_escape
        else:
            true_escape = True
        if true_escape:
            true_escape_places.append(escape_pos)

    extracted = u""
    lastpos = 0
    for pos in true_escape_places:
        # everything leading up to the escape
        extracted += line[lastpos:pos]
        # the escaped sequence (consuming 2 characters)
        extracted += unescapehandler(line[pos:pos+2])
        lastpos = pos+2

    extracted += line[lastpos:]
    return extracted


def unquotefrompo(postr):
    return u"".join([unescape(line[1:-1]) for line in postr])


def is_null(lst):
    return lst == [] or len(lst) == 1 and lst[0] == '""'


def extractstr(string):
    left = string.find('"')
    right = string.rfind('"')
    if right > -1:
        return string[left:right+1]
    else:
        return string[left:] + '"'


class pounit(pocommon.pounit):
    # othercomments = []      #   # this is another comment
    # automaticcomments = []  #   #. comment extracted from the source code
    # sourcecomments = []     #   #: sourcefile.xxx:35
    # prev_msgctxt = []       #   #| The previous values that msgctxt and msgid held
    # prev_msgid = []         #
    # prev_msgid_plural = []  #
    # typecomments = []       #   #, fuzzy
    # msgidcomments = []      #   _: within msgid
    # msgctxt
    # msgid = []
    # msgstr = []

    # Our homegrown way to indicate what must be copied in a shallow
    # fashion
    __shallow__ = ['_store']

    def __init__(self, source=None, encoding="UTF-8"):
        self._encoding = encodingToUse(encoding)
        self.obsolete = False
        self._initallcomments(blankall=True)
        self.prev_msgctxt = []
        self.prev_msgid = []
        self.prev_msgid_plural = []
        self.msgctxt = []
        self.msgid = []
        self.msgid_pluralcomments = []
        self.msgid_plural = []
        self.msgstr = []
        pocommon.pounit.__init__(self, source)

    def _initallcomments(self, blankall=False):
        """Initialises allcomments"""
        if blankall:
            self.othercomments = []
            self.automaticcomments = []
            self.sourcecomments = []
            self.typecomments = []
            self.msgidcomments = []

    def _get_all_comments(self):
        return [self.othercomments,
                self.automaticcomments,
                self.sourcecomments,
                self.typecomments,
                self.msgidcomments,
                ]

    allcomments = property(_get_all_comments)

    def _get_source_vars(self, msgid, msgid_plural):
        singular = unquotefrompo(msgid)
        if self.hasplural():
            pluralform = unquotefrompo(msgid_plural)
            return multistring([singular, pluralform], self._encoding)
        return singular

    def _set_source_vars(self, source):
        msgid = None
        msgid_plural = None
        if isinstance(source, str):
            source = source.decode(self._encoding)
        if isinstance(source, multistring):
            source = source.strings
        if isinstance(source, list):
            msgid = quoteforpo(source[0])
            if len(source) > 1:
                msgid_plural = quoteforpo(source[1])
            else:
                msgid_plural = []
        else:
            msgid = quoteforpo(source)
            msgid_plural = []
        return msgid, msgid_plural

    def getsource(self):
        """Returns the unescaped msgid"""
        return self._get_source_vars(self.msgid, self.msgid_plural)

    def setsource(self, source):
        """Sets the msgid to the given (unescaped) value.

        :param source: an unescaped source string.
        """
        self._rich_source = None
        self.msgid, self.msgid_plural = self._set_source_vars(source)
    source = property(getsource, setsource)

    def _get_prev_source(self):
        """Returns the unescaped msgid"""
        return self._get_source_vars(self.prev_msgid, self.prev_msgid_plural)

    def _set_prev_source(self, source):
        """Sets the msgid to the given (unescaped) value.

        :param source: an unescaped source string.
        """
        self.prev_msgid, self.prev_msgid_plural = self._set_source_vars(source)
    prev_source = property(_get_prev_source, _set_prev_source)

    def gettarget(self):
        """Returns the unescaped msgstr"""
        if isinstance(self.msgstr, dict):
            return multistring(map(unquotefrompo, self.msgstr.values()), self._encoding)
        else:
            return unquotefrompo(self.msgstr)

    def settarget(self, target):
        """Sets the msgstr to the given (unescaped) value"""
        self._rich_target = None
        if isinstance(target, str):
            target = target.decode(self._encoding)
        if self.hasplural():
            if isinstance(target, multistring):
                target = target.strings
            elif isinstance(target, basestring):
                target = [target]
        elif isinstance(target, (dict, list)):
            if len(target) == 1:
                target = target[0]
            else:
                raise ValueError("po msgid element has no plural but msgstr has %d elements (%s)" % (len(target), target))
        templates = self.msgstr
        if isinstance(templates, list):
            templates = {0: templates}
        if isinstance(target, list):
            self.msgstr = dict([(i, quoteforpo(target[i])) for i in range(len(target))])
        elif isinstance(target, dict):
            self.msgstr = dict([(i, quoteforpo(targetstring)) for i, targetstring in target.iteritems()])
        else:
            self.msgstr = quoteforpo(target)
    target = property(gettarget, settarget)

    def getalttrans(self):
        """Return a list of alternate units.

        Previous msgid and current msgstr is combined to form a single
        alternative unit."""
        prev_source = self.prev_source
        if prev_source and self.isfuzzy():
            unit = type(self)(prev_source)
            unit.target = self.target
            # Already released versions of Virtaal (0.6.x) only supported XLIFF
            # alternatives, and expect .xmlelement.get().
            # This can be removed soon:
            unit.xmlelement = dict()
            return [unit]
        return []

    def getnotes(self, origin=None):
        """Return comments based on origin value.

        :param origin: programmer, developer, source code, translator or None
        """
        if origin is None:
            comments = u"".join([comment[2:] for comment in self.othercomments])
            comments += u"".join([comment[3:] for comment in self.automaticcomments])
        elif origin == "translator":
            comments = u"".join([comment[2:] for comment in self.othercomments])
        elif origin in ["programmer", "developer", "source code"]:
            comments = u"".join([comment[3:] for comment in self.automaticcomments])
        else:
            raise ValueError("Comment type not valid")
        # Let's drop the last newline
        return comments[:-1]

    def addnote(self, text, origin=None, position="append"):
        """This is modeled on the XLIFF method.

        See :meth:`translate.storage.xliff.xliffunit.addnote`
        """
        # ignore empty strings and strings without non-space characters
        if not (text and text.strip()):
            return
        text = data.forceunicode(text)
        commentlist = self.othercomments
        linestart = "# "
        autocomments = False
        if origin in ["programmer", "developer", "source code"]:
            autocomments = True
            commentlist = self.automaticcomments
            linestart = "#. "
        text = text.split("\n")
        newcomments = [linestart + line + "\n" for line in text]
        if position == "append":
            newcomments = commentlist + newcomments
        elif position == "prepend":
            newcomments = newcomments + commentlist

        if autocomments:
            self.automaticcomments = newcomments
        else:
            self.othercomments = newcomments

    def removenotes(self):
        """Remove all the translator's notes (other comments)"""
        self.othercomments = []

    def __deepcopy__(self, memo={}):
        # Make an instance to serve as the copy
        new_unit = self.__class__()
        # We'll be testing membership frequently, so make a set from
        # self.__shallow__
        shallow = set(self.__shallow__)
        # Make deep copies of all members which are not in shallow
        for key, value in self.__dict__.iteritems():
            if key not in shallow:
                setattr(new_unit, key, copy.deepcopy(value))
        # Make shallow copies of all members which are in shallow
        for key in set(shallow):
            setattr(new_unit, key, getattr(self, key))
        # Mark memo with ourself, so that we won't get deep copied
        # again
        memo[id(self)] = self
        # Return our copied unit
        return new_unit

    def copy(self):
        return copy.deepcopy(self)

    def _msgidlen(self):
        if self.hasplural():
            return len(unquotefrompo(self.msgid)) + len(unquotefrompo(self.msgid_plural))
        else:
            return len(unquotefrompo(self.msgid))

    def _msgstrlen(self):
        if isinstance(self.msgstr, dict):
            combinedstr = "\n".join(filter(None, [unquotefrompo(msgstr) for msgstr in self.msgstr.itervalues()]))
            return len(combinedstr)
        else:
            return len(unquotefrompo(self.msgstr))

    def merge(self, otherpo, overwrite=False, comments=True, authoritative=False):
        """Merges the otherpo (with the same msgid) into this one.

        Overwrite non-blank self.msgstr only if overwrite is True
        merge comments only if comments is True
        """

        def mergelists(list1, list2, split=False):
            #decode where necessary
            if unicode in [type(item) for item in list2] + [type(item) for item in list1]:
                for position, item in enumerate(list1):
                    if isinstance(item, str):
                        list1[position] = item.decode("utf-8")
                for position, item in enumerate(list2):
                    if isinstance(item, str):
                        list2[position] = item.decode("utf-8")

            #Determine the newline style of list1
            lineend = ""
            if list1 and list1[0]:
                for candidate in ["\n", "\r", "\n\r"]:
                    if list1[0].endswith(candidate):
                        lineend = candidate
                if not lineend:
                    lineend = ""
            else:
                lineend = "\n"

            #Split if directed to do so:
            if split:
                splitlist1 = []
                splitlist2 = []
                prefix = "#"
                for item in list1:
                    splitlist1.extend(item.split()[1:])
                    prefix = item.split()[0]
                for item in list2:
                    splitlist2.extend(item.split()[1:])
                    prefix = item.split()[0]
                list1.extend(["%s %s%s" % (prefix, item, lineend) for item in splitlist2 if not item in splitlist1])
            else:
                #Normal merge, but conform to list1 newline style
                if list1 != list2:
                    for item in list2:
                        if lineend:
                            item = item.rstrip() + lineend
                        # avoid duplicate comment lines (this might cause some problems)
                        if item not in list1 or len(item) < 5:
                            list1.append(item)
        if not isinstance(otherpo, pounit):
            super(pounit, self).merge(otherpo, overwrite, comments)
            return
        if comments:
            mergelists(self.othercomments, otherpo.othercomments)
            mergelists(self.typecomments, otherpo.typecomments)
            if not authoritative:
                # We don't bring across otherpo.automaticcomments as we
                # consider ourself to be the the authority.  Same applies
                # to otherpo.msgidcomments
                mergelists(self.automaticcomments, otherpo.automaticcomments)
                mergelists(self.msgidcomments, otherpo.msgidcomments)
                mergelists(self.sourcecomments, otherpo.sourcecomments, split=True)
        if not self.istranslated() or overwrite:
            # Remove kde-style comments from the translation (if any).
            if self._extract_msgidcomments(otherpo.target):
                otherpo.target = otherpo.target.replace('_: ' + otherpo._extract_msgidcomments() + '\n', '')
            self.target = otherpo.target
            if self.source != otherpo.source or self.getcontext() != otherpo.getcontext():
                self.markfuzzy()
            else:
                self.markfuzzy(otherpo.isfuzzy())
        elif not otherpo.istranslated():
            if self.source != otherpo.source:
                self.markfuzzy()
        else:
            if self.target != otherpo.target:
                self.markfuzzy()

    def isheader(self):
        #return (self._msgidlen() == 0) and (self._msgstrlen() > 0) and (len(self.msgidcomments) == 0)
        #rewritten here for performance:
        return (is_null(self.msgid)
                        and not is_null(self.msgstr)
                        and self.msgidcomments == []
                        and is_null(self.msgctxt))

    def isblank(self):
        if self.isheader() or len(self.msgidcomments):
            return False
        if (self._msgidlen() == 0) and (self._msgstrlen() == 0) and (is_null(self.msgctxt)):
            return True
        return False
        # TODO: remove:
        # Before, the equivalent of the following was the final return statement:
        # return len(self.source.strip()) == 0

    def hastypecomment(self, typecomment):
        """Check whether the given type comment is present"""
        if not self.typecomments:
            return False
        for tc in self.typecomments:
            # check for word boundaries properly by using a regular expression
            if re.search("\\b%s\\b" % typecomment, tc):
                return True
        return False

    def hasmarkedcomment(self, commentmarker):
        """Check whether the given comment marker is present.

        These should appear as::

                # (commentmarker) ...
        """
        commentmarker = "(%s)" % commentmarker
        for comment in self.othercomments:
            if comment.replace("#", "", 1).strip().startswith(commentmarker):
                return True
        return False

    def settypecomment(self, typecomment, present=True):
        """Alters whether a given typecomment is present"""
        if self.hastypecomment(typecomment) != present:
            typecomments = re.findall(r"\b[-\w]+\b", "\n".join(self.typecomments))
            if present:
                typecomments.append(typecomment)
            else:
                typecomments.remove(typecomment)
            if typecomments:
                typecomments.sort()
                self.typecomments = ["#, %s\n" % ", ".join(typecomments)]
            else:
                self.typecomments = []

    def isfuzzy(self):
        return self.hastypecomment('fuzzy')

    def markfuzzy(self, present=True):
        if present:
            self.set_state_n(self.STATE[self.S_FUZZY][0])
        elif self.hasplural() and not self._msgstrlen() or is_null(self.msgstr):
            self.set_state_n(self.STATE[self.S_UNTRANSLATED][0])
        else:
            self.set_state_n(self.STATE[self.S_TRANSLATED][0])
        self._domarkfuzzy(present)

    def _domarkfuzzy(self, present=True):
        self.settypecomment("fuzzy", present)

    def infer_state(self):
        if self.obsolete:
            self.makeobsolete()
        else:
            self.markfuzzy(self.hastypecomment('fuzzy'))

    def isobsolete(self):
        return self.obsolete

    def makeobsolete(self):
        """Makes this unit obsolete"""
        super(pounit, self).makeobsolete()
        self.obsolete = True
        self.sourcecomments = []
        self.automaticcomments = []

    def resurrect(self):
        """Makes an obsolete unit normal"""
        super(pounit, self).resurrect()
        self.obsolete = False

    def hasplural(self):
        """returns whether this pounit contains plural strings..."""
        return len(self.msgid_plural) > 0

    def parse(self, src):
        return poparser.parse_unit(poparser.ParseState(StringIO(src), pounit), self)

    def _getmsgpartstr(self, partname, partlines, partcomments=""):
        if isinstance(partlines, dict):
            partkeys = partlines.keys()
            partkeys.sort()
            return "".join([self._getmsgpartstr("%s[%d]" % (partname, partkey), partlines[partkey], partcomments) for partkey in partkeys])
        partstr = partname + " "
        partstartline = 0
        if len(partlines) > 0 and len(partcomments) == 0:
            partstr += partlines[0]
            partstartline = 1
        elif len(partcomments) > 0:
            if len(partlines) > 0 and len(unquotefrompo(partlines[:1])) == 0:
                # if there is a blank leader line, it must come before the comment
                partstr += partlines[0] + '\n'
                # but if the whole string is blank, leave it in
                if len(partlines) > 1:
                    partstartline += 1
            else:
                # All partcomments should start on a newline
                partstr += '""\n'
            # combine comments into one if more than one
            if len(partcomments) > 1:
                combinedcomment = []
                for comment in partcomments:
                    comment = unquotefrompo([comment])
                    if comment.startswith("_:"):
                        comment = comment[len("_:"):]
                    if comment.endswith("\\n"):
                        comment = comment[:-len("\\n")]
                    #Before we used to strip. Necessary in some cases?
                    combinedcomment.append(comment)
                partcomments = quoteforpo("_:%s" % "".join(combinedcomment))
            # comments first, no blank leader line needed
            partstr += "\n".join(partcomments)
            partstr = quote.rstripeol(partstr)
        else:
            partstr += '""'
        partstr += '\n'
        # add the rest
        for partline in partlines[partstartline:]:
            partstr += partline + '\n'
        return partstr

    def _encodeifneccessary(self, output):
        """Encodes unicode strings and returns other strings unchanged"""
        if isinstance(output, unicode):
            encoding = encodingToUse(getattr(self, "_encoding", "UTF-8"))
            return output.encode(encoding)
        return output

    def __str__(self):
        """Convert to a string. Double check that unicode is handled
        somehow here"""
        output = self._getoutput()
        return self._encodeifneccessary(output)

    def _getoutput(self):
        """return this po element as a string"""

        def add_prev_msgid_lines(lines, prefix, header, var):
            if len(var) > 0:
                lines.append("%s %s %s\n" % (prefix, header, var[0]))
                lines.extend("%s %s\n" % (prefix, line) for line in var[1:])

        def add_prev_msgid_info(lines, prefix):
            add_prev_msgid_lines(lines, prefix, 'msgctxt', self.prev_msgctxt)
            add_prev_msgid_lines(lines, prefix, 'msgid', self.prev_msgid)
            add_prev_msgid_lines(lines, prefix, 'msgid_plural', self.prev_msgid_plural)

        lines = []
        lines.extend(self.othercomments)
        if self.isobsolete():
            lines.extend(self.typecomments)
            obsoletelines = []
            add_prev_msgid_info(obsoletelines, prefix="#~|")
            if self.msgctxt:
                obsoletelines.append(self._getmsgpartstr("#~ msgctxt", self.msgctxt))
            obsoletelines.append(self._getmsgpartstr("#~ msgid", self.msgid, self.msgidcomments))
            if self.msgid_plural or self.msgid_pluralcomments:
                obsoletelines.append(self._getmsgpartstr("#~ msgid_plural", self.msgid_plural, self.msgid_pluralcomments))
            obsoletelines.append(self._getmsgpartstr("#~ msgstr", self.msgstr))
            for index, obsoleteline in enumerate(obsoletelines):
                # We need to account for a multiline msgid or msgstr here
                obsoletelines[index] = obsoleteline.replace('\n"', '\n#~ "')
            lines.extend(obsoletelines)
            return u"".join(lines)
        # if there's no msgid don't do msgid and string, unless we're the
        # header this will also discard any comments other than plain
        # othercomments...
        if is_null(self.msgid):
            if not (self.isheader() or self.getcontext() or self.sourcecomments):
                return u"".join(lines)
        lines.extend(self.automaticcomments)
        lines.extend(self.sourcecomments)
        lines.extend(self.typecomments)
        add_prev_msgid_info(lines, prefix="#|")
        if self.msgctxt:
            lines.append(self._getmsgpartstr(u"msgctxt", self.msgctxt))
        lines.append(self._getmsgpartstr(u"msgid", self.msgid, self.msgidcomments))
        if self.msgid_plural or self.msgid_pluralcomments:
            lines.append(self._getmsgpartstr(u"msgid_plural", self.msgid_plural, self.msgid_pluralcomments))
        lines.append(self._getmsgpartstr(u"msgstr", self.msgstr))
        postr = u"".join(lines)
        return postr

    def getlocations(self):
        """Get a list of locations from sourcecomments in the PO unit

        rtype: List
        return: A list of the locations with '#: ' stripped

        """
        locations = []
        for sourcecomment in self.sourcecomments:
            locations += quote.rstripeol(sourcecomment)[3:].split()
        for i, loc in enumerate(locations):
            locations[i] = pocommon.unquote_plus(loc)
        return locations

    def addlocation(self, location):
        """Add a location to sourcecomments in the PO unit

        :param location: Text location e.g. 'file.c:23' does not include #:
        :type location: String

        """
        location = data.forceunicode(location)
        if location.find(" ") != -1:
            location = pocommon.quote_plus(location)
        self.sourcecomments.append("#: %s\n" % location)

    def _extract_msgidcomments(self, text=None):
        """Extract KDE style msgid comments from the unit.

        :rtype: String
        :return: Returns the extracted msgidcomments found in this
                 unit's msgid.
        """

        if not text:
            text = unquotefrompo(self.msgidcomments)
        return text.split('\n')[0].replace('_: ', '', 1)

    def setmsgidcomment(self, msgidcomment):
        if msgidcomment:
            self.msgidcomments = ['"_: %s\\n"' % msgidcomment]
        else:
            self.msgidcomments = []

    msgidcomment = property(_extract_msgidcomments, setmsgidcomment)

    def getcontext(self):
        """Get the message context."""
        return unquotefrompo(self.msgctxt) + self._extract_msgidcomments()

    def setcontext(self, context):
        context = data.forceunicode(context)
        self.msgctxt = quoteforpo(context)

    def getid(self):
        """Returns a unique identifier for this unit."""
        context = self.getcontext()
        # Gettext does not consider the plural to determine duplicates, only
        # the msgid. For generation of .mo files, we might want to use this
        # code to generate the entry for the hash table, but for now, it is
        # commented out for conformance to gettext.
#        id = '\0'.join(self.source.strings)
        id = self.source
        if self.msgidcomments:
            id = u"_: %s\n%s" % (context, id)
        elif context:
            id = u"%s\04%s" % (context, id)
        return id


class pofile(pocommon.pofile):
    """A .po file containing various units"""
    UnitClass = pounit

    def parse(self, input):
        """Parses the given file or file source string."""
        if True:
            if hasattr(input, 'name'):
                self.filename = input.name
            elif not getattr(self, 'filename', ''):
                self.filename = ''
            if isinstance(input, str):
                input = StringIO(input)
            # clear units to get rid of automatically generated headers before parsing
            self.units = []
            poparser.parse_units(poparser.ParseState(input, pounit), self)

    def removeduplicates(self, duplicatestyle="merge"):
        """Make sure each msgid is unique ; merge comments etc from
        duplicates into original"""
        # TODO: can we handle consecutive calls to removeduplicates()? What
        # about files already containing msgctxt? - test
        id_dict = {}
        uniqueunits = []
        # TODO: this is using a list as the pos aren't hashable, but this is slow.
        # probably not used frequently enough to worry about it, though.
        markedpos = []

        def addcomment(thepo):
            thepo.msgidcomments.append('"_: %s\\n"' % " ".join(thepo.getlocations()))
            markedpos.append(thepo)
        for thepo in self.units:
            id = thepo.getid()
            if thepo.isheader() and not thepo.getlocations():
                # header msgids shouldn't be merged...
                uniqueunits.append(thepo)
            elif id in id_dict:
                if duplicatestyle == "merge":
                    if id:
                        id_dict[id].merge(thepo)
                    else:
                        addcomment(thepo)
                        uniqueunits.append(thepo)
                elif duplicatestyle == "msgctxt":
                    origpo = id_dict[id]
                    if origpo not in markedpos and id:
                        # if it doesn't have an id, we already added msgctxt
                        origpo.msgctxt.append('"%s"' % escapeforpo(" ".join(origpo.getlocations())))
                        markedpos.append(thepo)
                    thepo.msgctxt.append('"%s"' % escapeforpo(" ".join(thepo.getlocations())))
                    uniqueunits.append(thepo)
            else:
                if not id:
                    if duplicatestyle == "merge":
                        addcomment(thepo)
                    else:
                        thepo.msgctxt.append('"%s"' % escapeforpo(" ".join(thepo.getlocations())))
                id_dict[id] = thepo
                uniqueunits.append(thepo)
        self.units = uniqueunits

    def __str__(self):
        """Convert to a string. Double check that unicode is handled somehow
        here"""
        output = self._getoutput()
        if isinstance(output, unicode):
            try:
                return output.encode(getattr(self, "_encoding", "UTF-8"))
            except UnicodeEncodeError as e:
                self.updateheader(add=True, Content_Type="text/plain; charset=UTF-8")
                self._encoding = "UTF-8"
                for unit in self.units:
                    unit._encoding = "UTF-8"
                return self._getoutput().encode("UTF-8")

        return output

    def _getoutput(self):
        """convert the units back to lines"""
        lines = []
        for unit in self.units:
            unitsrc = unit._getoutput() + u"\n"
            lines.append(unitsrc)
        lines = u"".join(lines).rstrip()
        #After the last pounit we will have \n\n and we only want to end in \n:
        if lines:
            lines += u"\n"
        return lines

    def encode(self, lines):
        """encode any unicode strings in lines in self._encoding"""
        newlines = []
        encoding = self._encoding
        if encoding is None or encoding.lower() == "charset":
            encoding = 'UTF-8'
        for line in lines:
            if isinstance(line, unicode):
                line = line.encode(encoding)
            newlines.append(line)
        return newlines

    def decode(self, lines):
        """decode any non-unicode strings in lines with self._encoding"""
        newlines = []
        for line in lines:
            if (isinstance(line, str) and self._encoding is not None and
                self._encoding.lower() != "charset"):
                try:
                    line = line.decode(self._encoding)
                except UnicodeError as e:
                    raise UnicodeError("Error decoding line with encoding %r: %s. Line is %r" %
                                       (self._encoding, e, line))
            newlines.append(line)
        return newlines

    def unit_iter(self):
        for unit in self.units:
            if not (unit.isheader() or unit.isobsolete()):
                yield unit

########NEW FILE########
__FILENAME__ = qm
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Module for parsing Qt .qm files.

.. note::

    Based on documentation from Gettext's .qm implementation
    (see *write-qt.c*) and on observation of the output of lrelease.

.. note::

    Certain deprecated section tags are not implemented.  These will break
    and print out the missing tag.  They are easy to implement and should
    follow the structure in 03 (Translation).  We could find no examples
    that use these so we'd rather leave it unimplemented until we
    actually have test data.

.. note::

    Many .qm files are unable to be parsed as they do not have the source
    text.  We assume that since they use a hash table to lookup the
    data there is actually no need for the source text.  It seems however
    that in Qt4's lrelease all data is included in the resultant .qm file.

.. note::

    We can only parse, not create, a .qm file.  The main issue is that we
    need to implement the hashing algorithm (which seems to be identical to the
    Gettext hash algorithm).  Unlike Gettext it seems that the hash is
    required, but that has not been validated.

.. note::

    The code can parse files correctly.  But it could be cleaned up to be
    more readable, especially the part that breaks the file into sections.

http://qt.gitorious.org/+kde-developers/qt/kde-qt/blobs/master/tools/linguist/shared/qm.cpp
`Plural information <http://qt.gitorious.org/+kde-developers/qt/kde-qt/blobs/master/tools/linguist/shared/numerus.cpp>`_
`QLocale languages <http://docs.huihoo.com/qt/4.5/qlocale.html#Language-enum>`_
"""

import codecs
import logging
import struct

from translate.misc.multistring import multistring
from translate.storage import base


logger = logging.getLogger(__name__)

QM_MAGIC_NUMBER = (0x3CB86418, 0xCAEF9C95, 0xCD211CBF, 0x60A1BDDD)


def qmunpack(file_='messages.qm'):
    """Helper to unpack Qt .qm files into a Python string"""
    f = open(file_)
    s = f.read()
    print "\\x%02x" * len(s) % tuple(map(ord, s))
    f.close()


class qmunit(base.TranslationUnit):
    """A class representing a .qm translation message."""

    def __init__(self, source=None):
        super(qmunit, self).__init__(source)


class qmfile(base.TranslationStore):
    """A class representing a .qm file."""
    UnitClass = qmunit
    Name = "Qt .qm file"
    Mimetypes = ["application/x-qm"]
    Extensions = ["qm"]
    _binary = True

    def __init__(self, inputfile=None, unitclass=qmunit):
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.units = []
        self.filename = ''
        if inputfile is not None:
            self.parsestring(inputfile)

    def __str__(self):
        """Output a string representation of the .qm data file"""
        raise Exception("Writing of .qm files is not supported yet")

    def parse(self, input):
        """Parses the given file or file source string."""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            qmsrc = input.read()
            input.close()
            input = qmsrc
        if len(input) < 16:
            raise ValueError("This is not a .qm file: file empty or too small")
        magic = struct.unpack(">4L", input[:16])
        if magic != QM_MAGIC_NUMBER:
            raise ValueError("This is not a .qm file: invalid magic number")
        startsection = 16
        sectionheader = 5

        def section_debug(name, section_type, startsection, length):
            print "Section: %s (type: %#x, offset: %#x, length: %d)" % (name, section_type, startsection, length)
            return

        while startsection < len(input):
            section_type, length = struct.unpack(">BL", input[startsection:startsection + sectionheader])
            if section_type == 0x42:
                #section_debug("Hash", section_type, startsection, length)
                hashash = True
                hash_start = startsection + sectionheader
                hash_data = struct.unpack(">%db" % length, input[startsection + sectionheader:startsection + sectionheader + length])
            elif section_type == 0x69:
                #section_debug("Messages", section_type, startsection, length)
                hasmessages = True
                messages_start = startsection + sectionheader
                messages_data = struct.unpack(">%db" % length, input[startsection + sectionheader:startsection + sectionheader + length])
            elif section_type == 0x2f:
                #section_debug("Contexts", section_type, startsection, length)
                hascontexts = True
                contexts_start = startsection + sectionheader
                contexts_data = struct.unpack(">%db" % length, input[startsection + sectionheader:startsection + sectionheader + length])
            elif section_type == 0x88:
                #section_debug("NumerusRules", section_type, startsection, length)
                hasnumerusrules = True
                numerusrules_start = startsection + sectionheader
                numerusrules_data = struct.unpack(">%db" % length, input[startsection + sectionheader:startsection + sectionheader + length])
            else:
                section_debug("Unkown", section_type, startsection, length)
            startsection = startsection + sectionheader + length
        pos = messages_start
        source = target = None
        while pos < messages_start + len(messages_data):
            subsection, = struct.unpack(">B", input[pos:pos + 1])
            if subsection == 0x01:  # End
                #print "End"
                pos = pos + 1
                if not source is None and not target is None:
                    newunit = self.addsourceunit(source)
                    newunit.target = target
                    source = target = None
                else:
                    raise ValueError("Old .qm format with no source defined")
                continue
            #print pos, subsection
            pos = pos + 1
            length, = struct.unpack(">l", input[pos:pos + 4])
            if subsection == 0x03:  # Translation
                if length != -1:
                    raw, = struct.unpack(">%ds" % length,
                                         input[pos + 4:pos + 4 + length])
                    string, templen = codecs.utf_16_be_decode(raw)
                    if target:
                        target.strings.append(string)
                    else:
                        target = multistring(string)
                    pos = pos + 4 + length
                else:
                    target = u""
                    pos = pos + 4
                #print "Translation: %s" % target.encode('utf-8')
            elif subsection == 0x06:  # SourceText
                source = input[pos + 4:pos + 4 + length].decode('iso-8859-1')
                #print "SourceText: %s" % source
                pos = pos + 4 + length
            elif subsection == 0x07:  # Context
                context = input[pos + 4:pos + 4 + length].decode('iso-8859-1')
                #print "Context: %s" % context
                pos = pos + 4 + length
            elif subsection == 0x08:  # Disambiguating-comment
                comment = input[pos + 4:pos + 4 + length]
                #print "Disambiguating-comment: %s" % comment
                pos = pos + 4 + length
            elif subsection == 0x05:  # hash
                hash = input[pos:pos + 4]
                #print "Hash: %s" % hash
                pos = pos + 4
            else:
                if subsection == 0x02:  # SourceText16
                    subsection_name = "SourceText16"
                elif subsection == 0x04:  # Context16
                    subsection_name = "Context16"
                else:
                    subsection_name = "Unknown"
                logger.warning("Unimplemented: 0x%x %s",
                               subsection, subsection_name)
                return

    def savefile(self, storefile):
        raise Exception("Writing of .qm files is not supported yet")

########NEW FILE########
__FILENAME__ = qph
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008, 2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Module for handling Qt Linguist Phrase Book (.qph) files.

Extract from the `Qt Linguist Manual: Translators
<http://doc.trolltech.com/4.3/linguist-translators.html>`_:
.qph Qt Phrase Book Files are human-readable XML files containing standard
phrases and their translations. These files are created and updated by Qt
Linguist and may be used by any number of projects and applications.

A DTD to define the format does not seem to exist, but the following `code
<http://qt.gitorious.org/qt/qt/blobs/4.7/tools/linguist/shared/qph.cpp>`_
provides the reference implementation for the Qt Linguist product.
"""

from lxml import etree

from translate.lang import data
from translate.storage import lisa


class QphUnit(lisa.LISAunit):
    """A single term in the qph file."""

    rootNode = "phrase"
    languageNode = "source"
    textNode = ""
    namespace = ''

    def createlanguageNode(self, lang, text, purpose):
        """Returns an xml Element setup with given parameters."""
        assert purpose
        langset = etree.Element(self.namespaced(purpose))
        langset.text = text
        return langset

    def _getsourcenode(self):
        return self.xmlelement.find(self.namespaced(self.languageNode))

    def _gettargetnode(self):
        return self.xmlelement.find(self.namespaced("target"))

    def getlanguageNodes(self):
        """We override this to get source and target nodes."""

        def not_none(node):
            return not node is None

        return filter(not_none, [self._getsourcenode(), self._gettargetnode()])

    def addnote(self, text, origin=None, position="append"):
        """Add a note specifically in a "definition" tag"""
        current_notes = self.getnotes(origin)
        self.removenotes()
        note = etree.SubElement(self.xmlelement, self.namespaced("definition"))
        note.text = "\n".join(filter(None, [current_notes, text.strip()]))

    def getnotes(self, origin=None):
        #TODO: consider only responding when origin has certain values
        notenode = self.xmlelement.find(self.namespaced("definition"))
        comment = ''
        if not notenode is None:
            comment = notenode.text
        return comment

    def removenotes(self):
        """Remove all the translator notes."""
        note = self.xmlelement.find(self.namespaced("definition"))
        if not note is None:
            self.xmlelement.remove(note)


class QphFile(lisa.LISAfile):
    """Class representing a QPH file store."""
    UnitClass = QphUnit
    Name = "Qt Phrase Book"
    Mimetypes = ["application/x-qph"]
    Extensions = ["qph"]
    rootNode = "QPH"
    bodyNode = "QPH"
    XMLskeleton = '''<!DOCTYPE QPH>
<QPH>
</QPH>
'''
    namespace = ''

    def initbody(self):
        """Initialises self.body so it never needs to be retrieved from the
        XML again."""
        self.namespace = self.document.getroot().nsmap.get(None, None)
        self.header = self.document.getroot()
        self.body = self.document.getroot()  # The root node contains the units

    def getsourcelanguage(self):
        """Get the source language for this .qph file.

        We don't implement setsourcelanguage as users really shouldn't be
        altering the source language in .qph files, it should be set correctly
        by the extraction tools.

        :return: ISO code e.g. af, fr, pt_BR
        :rtype: String
        """
        lang = data.normalize_code(self.header.get('sourcelanguage', "en"))
        if lang == 'en-us':
            return 'en'
        return lang

    def gettargetlanguage(self):
        """Get the target language for this .qph file.

        :return: ISO code e.g. af, fr, pt_BR
        :rtype: String
        """
        return data.normalize_code(self.header.get('language'))

    def settargetlanguage(self, targetlanguage):
        """Set the target language for this .qph file to *targetlanguage*.

        :param targetlanguage: ISO code e.g. af, fr, pt_BR
        :type targetlanguage: String
        """
        if targetlanguage:
            self.header.set('language', targetlanguage)

    def __str__(self):
        """Converts to a string containing the file's XML.

        We have to override this to ensure mimic the Qt convention:
            - no XML decleration
            - plain DOCTYPE that lxml seems to ignore
        """
        # A bug in lxml means we have to output the doctype ourselves. For
        # more information, see:
        # http://codespeak.net/pipermail/lxml-dev/2008-October/004112.html
        # The problem was fixed in lxml 2.1.3
        output = etree.tostring(self.document, pretty_print=True,
                                xml_declaration=False, encoding='utf-8')
        if not "<!DOCTYPE QPH>" in output[:30]:
            output = "<!DOCTYPE QPH>" + output
        return output

########NEW FILE########
__FILENAME__ = rc
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006,2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Classes that hold units of .rc files (:class:`rcunit`) or entire files
(:class:`rcfile`) used in translating Windows Resources.

.. note:::

   This implementation is based mostly on observing WINE .rc files,
   these should mimic other non-WINE .rc files.
"""

import re

from translate.storage import base


def escape_to_python(string):
    """Escape a given .rc string into a valid Python string."""
    pystring = re.sub('"\s*\\\\\n\s*"', "", string)   # xxx"\n"xxx line continuation
    pystring = re.sub("\\\\\\\n", "", pystring)       # backslash newline line continuation
    pystring = re.sub("\\\\n", "\n", pystring)        # Convert escaped newline to a real newline
    pystring = re.sub("\\\\t", "\t", pystring)        # Convert escape tab to a real tab
    pystring = re.sub("\\\\\\\\", "\\\\", pystring)   # Convert escape backslash to a real escaped backslash
    return pystring


def escape_to_rc(string):
    """Escape a given Python string into a valid .rc string."""
    rcstring = re.sub("\\\\", "\\\\\\\\", string)
    rcstring = re.sub("\t", "\\\\t", rcstring)
    rcstring = re.sub("\n", "\\\\n", rcstring)
    return rcstring


class rcunit(base.TranslationUnit):
    """A unit of an rc file"""

    def __init__(self, source="", encoding="cp1252"):
        """Construct a blank rcunit."""
        super(rcunit, self).__init__(source)
        self.name = ""
        self._value = ""
        self.comments = []
        self.source = source
        self.match = None
        self.encoding = encoding

    def setsource(self, source):
        """Sets the source AND the target to be equal"""
        self._rich_source = None
        self._value = source or ""

    def getsource(self):
        return self._value

    source = property(getsource, setsource)

    def settarget(self, target):
        """.. note:: This also sets the ``.source`` attribute!"""
        self._rich_target = None
        self.source = target

    def gettarget(self):
        return self.source
    target = property(gettarget, settarget)

    def __str__(self):
        """Convert to a string. Double check that unicode is handled somehow here."""
        source = self.getoutput()
        if isinstance(source, unicode):
            return source.encode(getattr(self, "encoding", "UTF-8"))
        return source

    def getoutput(self):
        """Convert the element back into formatted lines for a .rc file."""
        if self.isblank():
            return "".join(self.comments + ["\n"])
        else:
            return "".join(self.comments + ["%s=%s\n" % (self.name, self.value)])

    def getlocations(self):
        return [self.name]

    def addnote(self, text, origin=None, position="append"):
        self.comments.append(note)

    def getnotes(self, origin=None):
        return '\n'.join(self.comments)

    def removenotes(self):
        self.comments = []

    def isblank(self):
        """Returns whether this is a blank element, containing only comments."""
        return not (self.name or self.value)


class rcfile(base.TranslationStore):
    """This class represents a .rc file, made up of rcunits."""
    UnitClass = rcunit

    def __init__(self, inputfile=None, lang=None, sublang=None, encoding="cp1252"):
        """Construct an rcfile, optionally reading in from inputfile."""
        self.encoding = encoding
        super(rcfile, self).__init__(unitclass=self.UnitClass)
        self.filename = getattr(inputfile, 'name', '')
        self.lang = lang
        self.sublang = sublang
        if inputfile is not None:
            rcsrc = inputfile.read().decode(encoding)
            inputfile.close()
            self.parse(rcsrc)

    def parse(self, rcsrc):
        """Read the source of a .rc file in and include them as units."""
        BLOCKS_RE = re.compile("""
                         (?:
                         LANGUAGE\s+[^\n]*|                              # Language details
                         /\*.*?\*/[^\n]*|                                      # Comments
                         (?:[0-9A-Z_]+\s+(?:MENU|DIALOG|DIALOGEX)|STRINGTABLE)\s  # Translatable section
                         .*?
                         (?:
                         BEGIN(?:\s*?POPUP.*?BEGIN.*?END\s*?)+?END|BEGIN.*?END|  # FIXME Need a much better approach to nesting menus
                         {(?:\s*?POPUP.*?{.*?}\s*?)+?}|{.*?})+[\n]|
                         \s*[\n]         # Whitespace
                         )
                         """, re.DOTALL + re.VERBOSE)
        STRINGTABLE_RE = re.compile("""
                         (?P<name>[0-9A-Za-z_]+?),?\s*
                         L?"(?P<value>.*?)"\s*[\n]
                         """, re.DOTALL + re.VERBOSE)
        DIALOG_RE = re.compile("""
                         (?P<type>AUTOCHECKBOX|AUTORADIOBUTTON|CAPTION|Caption|CHECKBOX|CTEXT|CONTROL|DEFPUSHBUTTON|
                         GROUPBOX|LTEXT|PUSHBUTTON|RADIOBUTTON|RTEXT)  # Translatable types
                         \s+
                         L?                                    # Unkown prefix see ./dlls/shlwapi/shlwapi_En.rc
                         "(?P<value>.*?)"                                      # String value
                         (?:\s*,\s*|[\n])                          # FIXME ./dlls/mshtml/En.rc ID_DWL_DIALOG.LTEXT.ID_DWL_STATUS
                         (?P<name>.*?|)\s*(?:/[*].*?[*]/|),
                         """, re.DOTALL + re.VERBOSE)
        MENU_RE = re.compile("""
                         (?P<type>POPUP|MENUITEM)
                         \s+
                         "(?P<value>.*?)"                                      # String value
                         (?:\s*,?\s*)?
                         (?P<name>[^\s]+).*?[\n]
                         """, re.DOTALL + re.VERBOSE)

        processsection = False
        self.blocks = BLOCKS_RE.findall(rcsrc)
        for blocknum, block in enumerate(self.blocks):
            #print block.split("\n")[0]
            processblock = None
            if block.startswith("LANGUAGE"):
                if self.lang is None or self.sublang is None or re.match("LANGUAGE\s+%s,\s*%s\s*$" % (self.lang, self.sublang), block) is not None:
                    processsection = True
                else:
                    processsection = False
            else:
                if re.match(".+LANGUAGE\s+[0-9A-Za-z_]+,\s*[0-9A-Za-z_]+\s*[\n]", block, re.DOTALL) is not None:
                    if re.match(".+LANGUAGE\s+%s,\s*%s\s*[\n]" % (self.lang, self.sublang), block, re.DOTALL) is not None:
                        processblock = True
                    else:
                        processblock = False

            if not (processblock or (processsection and processblock)):
                continue

            if block.startswith("STRINGTABLE"):
                #print "stringtable:\n %s------\n" % block
                for match in STRINGTABLE_RE.finditer(block):
                    if not match.groupdict()['value']:
                        continue
                    newunit = rcunit(escape_to_python(match.groupdict()['value']))
                    newunit.name = "STRINGTABLE." + match.groupdict()['name']
                    newunit.match = match
                    self.addunit(newunit)
            if block.startswith("/*"):  # Comments
                #print "comment"
                pass
            if re.match("[0-9A-Z_]+\s+DIALOG", block) is not None:
                dialog = re.match("(?P<dialogname>[0-9A-Z_]+)\s+(?P<dialogtype>DIALOGEX|DIALOG)", block).groupdict()
                dialogname = dialog["dialogname"]
                dialogtype = dialog["dialogtype"]
                #print "dialog: %s" % dialogname
                for match in DIALOG_RE.finditer(block):
                    if not match.groupdict()['value']:
                        continue
                    type = match.groupdict()['type']
                    value = match.groupdict()['value']
                    name = match.groupdict()['name']
                    newunit = rcunit(escape_to_python(value))
                    if type == "CAPTION" or type == "Caption":
                        newunit.name = "%s.%s.%s" % (dialogtype, dialogname, type)
                    elif name == "-1":
                        newunit.name = "%s.%s.%s.%s" % (dialogtype, dialogname, type, value.replace(" ", "_"))
                    else:
                        newunit.name = "%s.%s.%s.%s" % (dialogtype, dialogname, type, name)
                    newunit.match = match
                    self.addunit(newunit)
            if re.match("[0-9A-Z_]+\s+MENU", block) is not None:
                menuname = re.match("(?P<menuname>[0-9A-Z_]+)\s+MENU", block).groupdict()["menuname"]
                #print "menu: %s" % menuname
                for match in MENU_RE.finditer(block):
                    if not match.groupdict()['value']:
                        continue
                    type = match.groupdict()['type']
                    value = match.groupdict()['value']
                    name = match.groupdict()['name']
                    newunit = rcunit(escape_to_python(value))
                    if type == "POPUP":
                        newunit.name = "MENU.%s.%s" % (menuname, type)
                    elif name == "-1":
                        newunit.name = "MENU.%s.%s.%s" % (menuname, type, value.replace(" ", "_"))
                    else:
                        newunit.name = "MENU.%s.%s.%s" % (menuname, type, name)
                    newunit.match = match
                    self.addunit(newunit)

    def __str__(self):
        """Convert the units back to lines."""
        return "".join(self.blocks)

########NEW FILE########
__FILENAME__ = statistics
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module to provide statistics and related functionality.

"""

from translate import lang
from translate.lang import factory


# calling classifyunits() in the constructor is probably not ideal.
# idea: have a property for .classification that calls it if necessary

# If we add units or change translations, statistics are out of date
# Compare with modules/Status.py in pootling that uses a bitmask to
# filter units

# Add support for reading and writing Pootle style .stats files

# Consider providing quickstats


class Statistics(object):
    """Manages statistics for storage objects."""

    def __init__(self, sourcelanguage='en', targetlanguage='en', checkerstyle=None):
        self.sourcelanguage = sourcelanguage
        self.targetlanguage = targetlanguage
        self.language = lang.factory.getlanguage(self.sourcelanguage)
#        self.init_checker(checkerstyle)

        self.classification = {}

    def init_checker(self, checkerstyle=None):
        from translate.filters import checks
        from translate.filters import pofilter
        checkerclasses = [checkerstyle or checks.StandardChecker, pofilter.StandardPOChecker]
        self.checker = pofilter.POTeeChecker(checkerclasses=checkerclasses)

    def fuzzy_units(self):
        """Return a list of fuzzy units."""
        if not self.classification:
            self.classifyunits()
        units = self.getunits()
        return [units[item] for item in self.classification["fuzzy"]]

    def fuzzy_unitcount(self):
        """Returns the number of fuzzy units."""
        return len(self.fuzzy_units())

    def translated_units(self):
        """Return a list of translated units."""
        if not self.classification:
            self.classifyunits()
        units = self.getunits()
        return [units[item] for item in self.classification["translated"]]

    def translated_unitcount(self):
        """Returns the number of translated units."""
        return len(self.translated_units())

    def untranslated_units(self):
        """Return a list of untranslated units."""
        if not self.classification:
            self.classifyunits()
        units = self.getunits()
        return [units[item] for item in self.classification["blank"]]

    def untranslated_unitcount(self):
        """Returns the number of untranslated units."""

        return len(self.untranslated_units())

    def getunits(self):
        """Returns a list of all units in this object."""
        return []

    def get_source_text(self, units):
        """Joins the unit source strings in a single string of text."""
        source_text = ""
        for unit in units:
            source_text += unit.source + "\n"
            plurals = getattr(unit.source, "strings", [])
            if plurals:
                source_text += "\n".join(plurals[1:])
        return source_text

    def wordcount(self, text):
        """Returns the number of words in the given text."""
        return len(self.language.words(text))

    def source_wordcount(self):
        """Returns the number of words in the source text."""
        source_text = self.get_source_text(self.getunits())
        return self.wordcount(source_text)

    def translated_wordcount(self):
        """Returns the number of translated words in this object."""

        text = self.get_source_text(self.translated_units())
        return self.wordcount(text)

    def untranslated_wordcount(self):
        """Returns the number of untranslated words in this object."""

        text = self.get_source_text(self.untranslated_units())
        return self.wordcount(text)

    def classifyunit(self, unit):
        """Returns a list of the classes that the unit belongs to.

        :param unit: the unit to classify
        """
        classes = ["total"]
        if unit.isfuzzy():
            classes.append("fuzzy")
        if unit.gettargetlen() == 0:
            classes.append("blank")
        if unit.istranslated():
            classes.append("translated")
        #TODO: we don't handle checking plurals at all yet, as this is tricky...
        source = unit.source
        target = unit.target
        if isinstance(source, str) and isinstance(target, unicode):
            source = source.decode(getattr(unit, "encoding", "utf-8"))
        #TODO: decoding should not be done here
#        checkresult = self.checker.run_filters(unit, source, target)
        checkresult = {}
        for checkname, checkmessage in checkresult.iteritems():
            classes.append("check-" + checkname)
        return classes

    def classifyunits(self):
        """Makes a dictionary of which units fall into which classifications.

        This method iterates over all units.
        """
        self.classification = {}
        self.classification["fuzzy"] = []
        self.classification["blank"] = []
        self.classification["translated"] = []
        self.classification["has-suggestion"] = []
        self.classification["total"] = []
#        for checkname in self.checker.getfilters().keys():
#            self.classification["check-" + checkname] = []
        for item, unit in enumerate(self.unit_iter()):
            classes = self.classifyunit(unit)
#            if self.basefile.getsuggestions(item):
#                classes.append("has-suggestion")
            for classname in classes:
                if classname in self.classification:
                    self.classification[classname].append(item)
                else:
                    self.classification[classname] = item
        self.countwords()

    def countwords(self):
        """Counts the source and target words in each of the units."""
        self.sourcewordcounts = []
        self.targetwordcounts = []
        for unit in self.unit_iter():
            self.sourcewordcounts.append([self.wordcount(text) for text in getattr(unit.source, "strings", [""])])
            self.targetwordcounts.append([self.wordcount(text) for text in getattr(unit.target, "strings", [""])])

    def reclassifyunit(self, item):
        """Updates the classification of a unit in self.classification.

        :param item: an integer that is an index in .getunits().
        """
        unit = self.getunits()[item]
        self.sourcewordcounts[item] = [self.wordcount(text) for text in unit.source.strings]
        self.targetwordcounts[item] = [self.wordcount(text) for text in unit.target.strings]
        classes = self.classifyunit(unit)
#        if self.basefile.getsuggestions(item):
#            classes.append("has-suggestion")
        for classname, matchingitems in self.classification.items():
            if (classname in classes) != (item in matchingitems):
                if classname in classes:
                    self.classification[classname].append(item)
                else:
                    self.classification[classname].remove(item)
                self.classification[classname].sort()
#        self.savestats()

########NEW FILE########
__FILENAME__ = statsdb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


"""Module to provide a cache of statistics in a database.

"""

import logging
import os.path
import re
import stat
import sys
import thread
from sqlite3 import dbapi2
from UserDict import UserDict

from translate import __version__ as toolkitversion
from translate.lang.common import Common
from translate.misc.multistring import multistring
from translate.storage import factory
from translate.storage.workflow import StateEnum


logger = logging.getLogger(__name__)

#kdepluralre = re.compile("^_n: ") #Restore this if you really need support for old kdeplurals
brtagre = re.compile("<br\s*?/?>")
# xmltagre is a direct copy of the from placeables/general.py
xmltagre = re.compile(r'''
        <                         # start of opening tag
        ([\w.:]+)                 # tag name, possibly namespaced
        (\s([\w.:]+=              # space and attribute name followed by =
            ((".*?")|('.*?'))     # attribute value, single or double quoted
        )?)*/?>                   # end of opening tag, possibly self closing
        |</([\w.]+)>              # or a closing tag
        ''', re.VERBOSE)
numberre = re.compile("\\D\\.\\D")

extended_state_strings = {
    StateEnum.EMPTY: "empty",
    StateEnum.NEEDS_WORK: "needs-work",
    StateEnum.REJECTED: "rejected",
    StateEnum.NEEDS_REVIEW: "needs-review",
    StateEnum.UNREVIEWED: "unreviewed",
    StateEnum.FINAL: "final",
}

UNTRANSLATED = StateEnum.EMPTY
FUZZY = StateEnum.NEEDS_WORK
TRANSLATED = StateEnum.UNREVIEWED

state_strings = {
    UNTRANSLATED: "untranslated",
    FUZZY: "fuzzy",
    TRANSLATED: "translated",
}


def wordcount(string):
    # TODO: po class should understand KDE style plurals ##
    #string = kdepluralre.sub("", string) #Restore this if you really need support for old kdeplurals
    string = brtagre.sub("\n", string)
    string = xmltagre.sub("", string)
    string = numberre.sub(" ", string)
    # TODO: This should still use the correct language to count in the target
    # language
    return len(Common.words(string))


def wordsinunit(unit):
    """Counts the words in the unit's source and target, taking plurals into
    account. The target words are only counted if the unit is translated."""
    (sourcewords, targetwords) = (0, 0)
    if isinstance(unit.source, multistring):
        sourcestrings = unit.source.strings
    else:
        sourcestrings = [unit.source or ""]
    for s in sourcestrings:
        sourcewords += wordcount(s)
    if not unit.istranslated():
        return sourcewords, targetwords
    if isinstance(unit.target, multistring):
        targetstrings = unit.target.strings
    else:
        targetstrings = [unit.target or ""]
    for s in targetstrings:
        targetwords += wordcount(s)
    return sourcewords, targetwords


class Record(UserDict):

    def __init__(self, record_keys, record_values=None, compute_derived_values=lambda x: x):
        if record_values is None:
            record_values = (0 for _i in record_keys)
        self.record_keys = record_keys
        self.data = dict(zip(record_keys, record_values))
        self._compute_derived_values = compute_derived_values
        self._compute_derived_values(self)

    def to_tuple(self):
        return tuple(self[key] for key in self.record_keys)

    def __add__(self, other):
        result = Record(self.record_keys)
        for key in self.keys():
            result[key] = self[key] + other[key]
        self._compute_derived_values(self)
        return result

    def __sub__(self, other):
        result = Record(self.record_keys)
        for key in self.keys():
            result[key] = self[key] - other[key]
        self._compute_derived_values(self)
        return result

    def as_string_for_db(self):
        return ",".join([repr(x) for x in self.to_tuple()])


def transaction(f):
    """Modifies f to commit database changes if it executes without exceptions.
    Otherwise it rolls back the database.

    ALL publicly accessible methods in StatsCache MUST be decorated with this
    decorator.
    """

    def decorated_f(self, *args, **kwargs):
        try:
            result = f(self, *args, **kwargs)
            self.con.commit()
            return result
        except:
            # If ANY exception is raised, we're left in an
            # uncertain state and we MUST roll back any changes to avoid getting
            # stuck in an inconsistent state.
            if self.con:
                self.con.rollback()
            raise
    return decorated_f


def statefordb(unit):
    """Returns the numeric database state for the unit."""
    if unit.istranslated():
        return TRANSLATED
    if unit.isfuzzy() and unit.target:
        return FUZZY
    return UNTRANSLATED


class FileTotals(object):
    keys = ['translatedsourcewords',
            'fuzzysourcewords',
            'untranslatedsourcewords',
            'translated',
            'fuzzy',
            'untranslated',
            'translatedtargetwords']

    def db_keys(self):
        return ",".join(self.keys)

    def __init__(self, cur):
        self.cur = cur
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS filetotals(
                fileid                  INTEGER PRIMARY KEY AUTOINCREMENT,
                translatedsourcewords   INTEGER NOT NULL,
                fuzzysourcewords        INTEGER NOT NULL,
                untranslatedsourcewords INTEGER NOT NULL,
                translated              INTEGER NOT NULL,
                fuzzy                   INTEGER NOT NULL,
                untranslated            INTEGER NOT NULL,
                translatedtargetwords   INTEGER NOT NULL);""")

    @classmethod
    def new_record(cls, state_for_db=None, sourcewords=None, targetwords=None):
        record = Record(cls.keys, compute_derived_values=cls._compute_derived_values)
        if state_for_db is not None:
            if state_for_db is UNTRANSLATED:
                record['untranslated'] = 1
                record['untranslatedsourcewords'] = sourcewords
            if state_for_db is TRANSLATED:
                record['translated'] = 1
                record['translatedsourcewords'] = sourcewords
                record['translatedtargetwords'] = targetwords
            elif state_for_db is FUZZY:
                record['fuzzy'] = 1
                record['fuzzysourcewords'] = sourcewords
        return record

    @classmethod
    def _compute_derived_values(cls, record):
        record["total"] = record["untranslated"] + \
                          record["translated"] + \
                          record["fuzzy"]
        record["totalsourcewords"] = record["untranslatedsourcewords"] + \
                                     record["translatedsourcewords"] + \
                                     record["fuzzysourcewords"]
        record["review"] = 0

    def __getitem__(self, fileid):
        result = self.cur.execute("""
            SELECT %(keys)s
            FROM   filetotals
            WHERE  fileid=?;""" % {'keys': self.db_keys()}, (fileid,))
        return Record(FileTotals.keys, result.fetchone(), self._compute_derived_values)

    def __setitem__(self, fileid, record):
        self.cur.execute("""
            INSERT OR REPLACE into filetotals
            VALUES (%(fileid)d, %(vals)s);
        """ % {'fileid': fileid, 'vals': record.as_string_for_db()})

    def __delitem__(self, fileid):
        self.cur.execute("""
            DELETE FROM filetotals
            WHERE fileid=?;
        """, (fileid,))


def emptyfiletotals():
    """Returns a dictionary with all statistics initalised to 0."""
    return FileTotals.new_record()


def emptyfilechecks():
    return {}


def emptyfilestats():
    return {"total": [], "translated": [], "fuzzy": [], "untranslated": []}


def emptyunitstats():
    return {"sourcewordcount": [], "targetwordcount": []}


# We allow the caller to specify which value to return when errors_return_empty
# is True. We do this, since Poolte wants None to be returned when it calls
# get_mod_info directly, whereas we want an integer to be returned for
# uses of get_mod_info within this module.
# TODO: Get rid of empty_return when Pootle code is improved to not require
#       this.


def get_mod_info(file_path):
    file_stat = os.stat(file_path)
    assert not stat.S_ISDIR(file_stat.st_mode)
    return file_stat.st_mtime, file_stat.st_size


def suggestion_extension():
    return os.path.extsep + 'pending'


def suggestion_filename(filename):
    return filename + suggestion_extension()


# ALL PUBLICLY ACCESSIBLE METHODS MUST BE DECORATED WITH THE transaction DECORATOR.
class StatsCache(object):
    """An object instantiated as a singleton for each statsfile that provides
    access to the database cache from a pool of StatsCache objects."""
    _caches = {}
    defaultfile = None
    con = None
    """This cache's connection"""
    cur = None
    """The current cursor"""

    def __new__(cls, statsfile=None):
        current_thread = thread.get_ident()

        def make_database(statsfile):

            def connect(cache):
                # sqlite needs to get the name in utf-8 on all platforms
                cache.con = dbapi2.connect(statsfile.encode('utf-8'))
                cache.cur = cache.con.cursor()

            def clear_old_data(cache):
                try:
                    cache.cur.execute("""SELECT min(toolkitbuild) FROM files""")
                    val = cache.cur.fetchone()
                    # If the database is empty, we have no idea whether its layout
                    # is correct, so we might as well delete it.
                    if val is None or val[0] < toolkitversion.build:
                        cache.con.close()
                        del cache
                        os.unlink(statsfile)
                        return True
                    return False
                except dbapi2.OperationalError:
                    return False

            cache = cls._caches.setdefault(current_thread, {})[statsfile] = object.__new__(cls)
            connect(cache)
            if clear_old_data(cache):
                connect(cache)
            cache.create()
            return cache

        if not statsfile:
            if not cls.defaultfile:
                userdir = os.path.expanduser("~")
                cachedir = None
                if os.name == "nt":
                    cachedir = os.path.join(userdir, "Translate Toolkit")
                else:
                    cachedir = os.path.join(userdir, ".translate_toolkit")
                if not os.path.exists(cachedir):
                    os.mkdir(cachedir)
                cachedir = cachedir.decode(sys.getfilesystemencoding())
                cls.defaultfile = os.path.realpath(os.path.join(cachedir, u"stats.db"))
            statsfile = cls.defaultfile
        else:
            statsfile = os.path.realpath(statsfile)
        # First see if a cache for this file already exists:
        if current_thread in cls._caches and statsfile in cls._caches[current_thread]:
            return cls._caches[current_thread][statsfile]
        # No existing cache. Let's build a new one and keep a copy
        return make_database(statsfile)

    @transaction
    def create(self):
        """Create all tables and indexes."""
        self.file_totals = FileTotals(self.cur)

        self.cur.execute("""CREATE TABLE IF NOT EXISTS files(
            fileid INTEGER PRIMARY KEY AUTOINCREMENT,
            path VARCHAR NOT NULL UNIQUE,
            st_mtime INTEGER NOT NULL,
            st_size INTEGER NOT NULL,
            toolkitbuild INTEGER NOT NULL);""")

        self.cur.execute("""CREATE UNIQUE INDEX IF NOT EXISTS filepathindex
            ON files (path);""")

        self.cur.execute("""CREATE TABLE IF NOT EXISTS units(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            unitid VARCHAR NOT NULL,
            fileid INTEGER NOT NULL,
            unitindex INTEGER NOT NULL,
            source VARCHAR NOT NULL,
            target VARCHAR,
            state INTEGER,
            e_state INTEGER,
            sourcewords INTEGER,
            targetwords INTEGER);""")

        self.cur.execute("""CREATE INDEX IF NOT EXISTS fileidindex
            ON units(fileid);""")

        self.cur.execute("""CREATE TABLE IF NOT EXISTS checkerconfigs(
            configid INTEGER PRIMARY KEY AUTOINCREMENT,
            config VARCHAR);""")

        self.cur.execute("""CREATE INDEX IF NOT EXISTS configindex
            ON checkerconfigs(config);""")

        self.cur.execute("""CREATE TABLE IF NOT EXISTS uniterrors(
            errorid INTEGER PRIMARY KEY AUTOINCREMENT,
            unitindex INTEGER NOT NULL,
            fileid INTEGER NOT NULL,
            configid INTEGER NOT NULL,
            name VARCHAR NOT NULL,
            message VARCHAR);""")

        self.cur.execute("""CREATE INDEX IF NOT EXISTS uniterrorindex
            ON uniterrors(fileid, configid);""")

    @transaction
    def _getfileid(self, filename, check_mod_info=True, store=None):
        """return fileid representing the given file in the statscache.

        if file not in cache or has been updated since last record
        update, recalculate stats.

        optional argument store can be used to avoid unnessecary
        reparsing of already loaded translation files.

        store can be a TranslationFile object or a callback that returns one.
        """
        if isinstance(filename, str):
            filename = unicode(filename, sys.getfilesystemencoding())
        realpath = os.path.realpath(filename)
        self.cur.execute("""SELECT fileid, st_mtime, st_size FROM files
                WHERE path=?;""", (realpath,))
        filerow = self.cur.fetchone()
        mod_info = get_mod_info(realpath)
        if filerow:
            fileid = filerow[0]
            if not check_mod_info:
                # Update the mod_info of the file
                self.cur.execute("""UPDATE files
                        SET st_mtime=?, st_size=?
                        WHERE fileid=?;""", (mod_info[0], mod_info[1], fileid))
                return fileid
            if (filerow[1], filerow[2]) == mod_info:
                return fileid

        # file wasn't in db at all, lets recache it
        if callable(store):
            store = store()
        else:
            store = store or factory.getobject(realpath)

        return self._cachestore(store, realpath, mod_info)

    def _getstoredcheckerconfig(self, checker):
        """See if this checker configuration has been used before."""
        config = str(checker.config.__dict__)
        self.cur.execute("""SELECT configid, config FROM checkerconfigs WHERE
            config=?;""", (config,))
        configrow = self.cur.fetchone()
        if not configrow or configrow[1] != config:
            return None
        else:
            return configrow[0]

    @transaction
    def _cacheunitstats(self, units, fileid, unitindex=None, file_totals_record=FileTotals.new_record()):
        """Cache the statistics for the supplied unit(s)."""
        unitvalues = []
        for index, unit in enumerate(units):
            if unit.istranslatable():
                sourcewords, targetwords = wordsinunit(unit)
                if unitindex:
                    index = unitindex
                # what about plurals in .source and .target?
                unit_state_for_db = statefordb(unit)
                unitvalues.append((unit.getid(), fileid, index,
                                   unit.source, unit.target,
                                   sourcewords, targetwords,
                                   unit_state_for_db,
                                   unit.get_state_id()))
                file_totals_record = file_totals_record + FileTotals.new_record(unit_state_for_db, sourcewords, targetwords)
        # XXX: executemany is non-standard
        self.cur.executemany("""INSERT INTO units
            (unitid, fileid, unitindex, source, target, sourcewords, targetwords, state, e_state)
            values (?, ?, ?, ?, ?, ?, ?, ?, ?);""",
            unitvalues)
        self.file_totals[fileid] = file_totals_record
        if unitindex:
            return state_strings[statefordb(units[0])]
        return ""

    @transaction
    def _cachestore(self, store, realpath, mod_info):
        """Calculates and caches the statistics of the given store
        unconditionally."""
        self.cur.execute("""DELETE FROM files WHERE
            path=?;""", (realpath,))
        self.cur.execute("""iNSERT INTO files
            (fileid, path, st_mtime, st_size, toolkitbuild) values (NULL, ?, ?, ?, ?);""",
            (realpath, mod_info[0], mod_info[1], toolkitversion.build))
        # Unusual capitalisation intended. See bug 2073.
        fileid = self.cur.lastrowid
        self.cur.execute("""DELETE FROM units WHERE
            fileid=?""", (fileid,))
        self._cacheunitstats(store.units, fileid)
        return fileid

    def file_extended_totals(self, filename, store=None):
        stats = {}
        fileid = self._getfileid(filename, store=store)

        self.cur.execute("""SELECT e_state, COUNT(id), SUM(sourcewords), SUM(targetwords)
            FROM units WHERE fileid=? GROUP BY e_state""", (fileid,))
        values = self.cur.fetchall()

        for value in values:
            stats[extended_state_strings[value[0]]] = {
                "units": value[1],
                "sourcewords": value[2],
                "targetwords": value[3],
            }
        return stats

    def filetotals(self, filename, store=None, extended=False):
        """Retrieves the statistics for the given file if possible, otherwise
        delegates to cachestore()."""
        stats = self.file_totals[self._getfileid(filename, store=store)]
        if extended:
            stats["extended"] = self.file_extended_totals(filename, store=store)
        return stats

    @transaction
    def _cacheunitschecks(self, units, fileid, configid, checker, unitindex=None):
        """Helper method for cachestorechecks() and recacheunit()"""
        # We always want to store one dummy error to know that we have actually
        # run the checks on this file with the current checker configuration
        dummy = (-1, fileid, configid, "noerror", "")
        unitvalues = [dummy]
        # if we are doing a single unit, we want to return the checknames
        errornames = []
        for index, unit in enumerate(units):
            if unit.istranslatable():
                # Correctly assign the unitindex
                if unitindex:
                    index = unitindex
                failures = checker.run_filters(unit)
                for checkname, checkmessage in failures.iteritems():
                    unitvalues.append((index, fileid, configid, checkname, checkmessage))
                    errornames.append("check-" + checkname)
        checker.setsuggestionstore(None)

        if unitindex:
            # We are only updating a single unit, so we don't want to add an
            # extra noerror-entry
            unitvalues.remove(dummy)
            errornames.append("total")

        # XXX: executemany is non-standard
        self.cur.executemany("""INSERT INTO uniterrors
            (unitindex, fileid, configid, name, message)
            values (?, ?, ?, ?, ?);""",
            unitvalues)
        return errornames

    @transaction
    def _cachestorechecks(self, fileid, store, checker, configid):
        """Calculates and caches the error statistics of the given store
        unconditionally."""
        # Let's purge all previous failures because they will probably just
        # fill up the database without much use.
        self.cur.execute("""DELETE FROM uniterrors WHERE
            fileid=?;""", (fileid,))
        self._cacheunitschecks(store.units, fileid, configid, checker)
        return fileid

    def get_unit_stats(self, fileid, unitid):
        values = self.cur.execute("""
            SELECT   state, sourcewords, targetwords
            FROM     units
            WHERE    fileid=? AND unitid=?
        """, (fileid, unitid))
        result = values.fetchone()
        if result is not None:
            return result
        else:
            logger.warning("Database in inconsistent state - fileid %d and "
                           "unitid %s have no entries in the table units.",
                           fileid, unitid)
            # If values.fetchone() is None, then we return an empty list,
            # to make FileTotals.new_record(*self.get_unit_stats(fileid, unitid))
            # do the right thing.
            return []

    @transaction
    def recacheunit(self, filename, checker, unit):
        """Recalculate all information for a specific unit. This is necessary
        for updating all statistics when a translation of a unit took place,
        for example.

        This method assumes that everything was up to date before (file totals,
        checks, checker config, etc."""
        fileid = self._getfileid(filename, check_mod_info=False)
        configid = self._get_config_id(fileid, checker)
        unitid = unit.getid()
        # get the unit index
        totals_without_unit = self.file_totals[fileid] - \
                                   FileTotals.new_record(*self.get_unit_stats(fileid, unitid))
        self.cur.execute("""SELECT unitindex FROM units WHERE
            fileid=? AND unitid=?;""", (fileid, unitid))
        unitindex = self.cur.fetchone()[0]
        self.cur.execute("""DELETE FROM units WHERE
            fileid=? AND unitid=?;""", (fileid, unitid))
        state = [self._cacheunitstats([unit], fileid, unitindex, totals_without_unit)]
        # remove the current errors
        self.cur.execute("""DELETE FROM uniterrors WHERE
            fileid=? AND unitindex=?;""", (fileid, unitindex))
        if os.path.exists(suggestion_filename(filename)):
            checker.setsuggestionstore(factory.getobject(suggestion_filename(filename), ignore=suggestion_extension()))
        state.extend(self._cacheunitschecks([unit], fileid, configid, checker, unitindex))
        return state

    def _checkerrors(self, filename, fileid, configid, checker, store):

        def geterrors():
            self.cur.execute("""SELECT
                name,
                unitindex
                FROM uniterrors WHERE fileid=? and configid=?
                ORDER BY unitindex;""", (fileid, configid))
            return self.cur.fetchone(), self.cur

        first, cur = geterrors()
        if first is not None:
            return first, cur

        # This could happen if we haven't done the checks before, or the
        # file changed, or we are using a different configuration
        if callable(store):
            store = store()
        else:
            store = store or factory.getobject(filename)

        if os.path.exists(suggestion_filename(filename)):
            checker.setsuggestionstore(factory.getobject(suggestion_filename(filename), ignore=suggestion_extension()))
        self._cachestorechecks(fileid, store, checker, configid)
        return geterrors()

    def _geterrors(self, filename, fileid, configid, checker, store):
        result = []
        first, cur = self._checkerrors(filename, fileid, configid, checker, store)
        result.append(first)
        result.extend(cur.fetchall())
        return result

    @transaction
    def _get_config_id(self, fileid, checker):
        configid = self._getstoredcheckerconfig(checker)
        if configid:
            return configid
        self.cur.execute("""iNSERT INTO checkerconfigs
            (configid, config) values (NULL, ?);""",
            (str(checker.config.__dict__),))
        # Unusual capitalisation intended. See bug 2073.
        return self.cur.lastrowid

    def filechecks(self, filename, checker, store=None):
        """Retrieves the error statistics for the given file if possible,
        otherwise delegates to cachestorechecks()."""
        fileid = self._getfileid(filename, store=store)
        configid = self._get_config_id(fileid, checker)
        values = self._geterrors(filename, fileid, configid, checker, store)

        errors = emptyfilechecks()
        for value in values:
            if value[1] == -1:
                continue
            checkkey = 'check-' + value[0]      # value[0] is the error name
            if not checkkey in errors:
                errors[checkkey] = []
            errors[checkkey].append(value[1])   # value[1] is the unitindex

        return errors

    def file_fails_test(self, filename, checker, name):
        fileid = self._getfileid(filename)
        configid = self._get_config_id(fileid, checker)
        self._checkerrors(filename, fileid, configid, checker, None)
        self.cur.execute("""SELECT
            name,
            unitindex
            FROM uniterrors
            WHERE fileid=? and configid=? and name=?;""", (fileid, configid, name))
        return self.cur.fetchone() is not None

    def filestatestats(self, filename, store=None, extended=False):
        """Return a dictionary of unit stats mapping sets of unit
        indices with those states"""
        stats = emptyfilestats()
        if extended:
            stats["extended"] = {}

        fileid = self._getfileid(filename, store=store)

        self.cur.execute("""SELECT state, e_state, unitindex
            FROM units WHERE fileid=? ORDER BY unitindex;""", (fileid,))
        values = self.cur.fetchall()

        for value in values:
            stats[state_strings[value[0]]].append(value[2])
            if extended:
                if value[1] not in stats["extended"]:
                    stats["extended"][value[1]] = []
                stats["extended"][value[1]].append(value[2])
            stats["total"].append(value[2])
        return stats

    def filestats(self, filename, checker, store=None, extended=False):
        """Return a dictionary of property names mapping sets of unit
        indices with those properties."""
        stats = emptyfilestats()
        stats.update(self.filechecks(filename, checker, store))
        stats.update(self.filestatestats(filename, store, extended=extended))
        return stats

    def unitstats(self, filename, _lang=None, store=None):
        # For now, lang and store are unused. lang will allow the user to
        # base stats information on the given language. See the commented
        # line containing stats.update below.
        """Return a dictionary of property names mapping to arrays which
        map unit indices to property values.

        Please note that this is different from filestats, since filestats
        supplies sets of unit indices with a given property, whereas this
        method supplies arrays which map unit indices to given values."""
        stats = emptyunitstats()

        #stats.update(self.unitchecks(filename, lang, store))
        fileid = self._getfileid(filename, store=store)

        self.cur.execute("""SELECT
          sourcewords, targetwords
          FROM units WHERE fileid=?
          ORDER BY unitindex;""", (fileid,))

        for sourcecount, targetcount in self.cur.fetchall():
            stats["sourcewordcount"].append(sourcecount)
            stats["targetwordcount"].append(targetcount)

        return stats

########NEW FILE########
__FILENAME__ = subtitles
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Class that manages subtitle files for translation.

   This class makes use of the subtitle functionality of ``gaupol``.

   .. seealso:: gaupol/agents/open.py::open_main

   A patch to gaupol is required to open utf-8 files successfully.
"""

import os
import tempfile
from cStringIO import StringIO

try:
    from aeidon import Subtitle, documents, newlines
    from aeidon.encodings import detect
    from aeidon.files import (AdvSubStationAlpha, MicroDVD, SubRip,
                              SubStationAlpha, new)
    from aeidon.util import detect_format as determine
except ImportError:
    from gaupol import FormatDeterminer, documents
    from gaupol.encodings import detect
    from gaupol.files import (AdvSubStationAlpha, MicroDVD, SubRip,
                              SubStationAlpha, new)
    from gaupol.newlines import newlines
    from gaupol.subtitle import Subtitle
    from translate.storage import base
    _determiner = FormatDeterminer()
    determine = _determiner.determine

from translate.storage import base


class SubtitleUnit(base.TranslationUnit):
    """A subtitle entry that is translatable"""

    def __init__(self, source=None, encoding="utf_8"):
        self._start = None
        self._end = None
        if source:
            self.source = source
        super(SubtitleUnit, self).__init__(source)

    def getnotes(self, origin=None):
        if origin in ['programmer', 'developer', 'source code', None]:
            return "visible for %d seconds" % self._duration
        else:
            return ''

    def getlocations(self):
        return ["%s-->%s" % (self._start, self._end)]

    def getid(self):
        return self.getlocations()[0]


class SubtitleFile(base.TranslationStore):
    """A subtitle file"""
    UnitClass = SubtitleUnit

    def __init__(self, inputfile=None, unitclass=UnitClass):
        """construct an Subtitle file, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.units = []
        self.filename = None
        self._subtitlefile = None
        self._encoding = 'utf_8'
        if inputfile is not None:
            self._parsefile(inputfile)

    def __str__(self):
        subtitles = []
        for unit in self.units:
            subtitle = Subtitle()
            subtitle.main_text = unit.target or unit.source
            subtitle.start = unit._start
            subtitle.end = unit._end
            subtitles.append(subtitle)
        output = StringIO()
        self._subtitlefile.write_to_file(subtitles, documents.MAIN, output)
        return output.getvalue().encode(self._subtitlefile.encoding)

    def _parse(self):
        try:
            self._encoding = detect(self.filename)
            if self._encoding == 'ascii':
                self._encoding = 'utf_8'
            self._format = determine(self.filename, self._encoding)
            self._subtitlefile = new(self._format, self.filename, self._encoding)
            for subtitle in self._subtitlefile.read():
                newunit = self.addsourceunit(subtitle.main_text)
                newunit._start = subtitle.start
                newunit._end = subtitle.end
                newunit._duration = subtitle.duration_seconds
        except Exception as e:
            raise base.ParseError(e)

    def _parsefile(self, storefile):
        if hasattr(storefile, 'name'):
            self.filename = storefile.name
            storefile.close()
        elif hasattr(storefile, 'filename'):
            self.filename = storefile.filename
            storefile.close()
        elif isinstance(storefile, basestring):
            self.filename = storefile

        if self.filename and os.path.exists(self.filename):
            self._parse()
        else:
            self.parse(storefile.read())

    @classmethod
    def parsefile(cls, storefile):
        """parse the given file"""
        newstore = cls()
        newstore._parsefile(storefile)
        return newstore

    def parse(self, input):
        if isinstance(input, basestring):
            # Gaupol does not allow parsing from strings
            if self.filename:
                tmpfile, tmpfilename = tempfile.mkstemp(suffix=self.filename)
            else:
                tmpfile, tmpfilename = tempfile.mkstemp()
            tmpfile = open(tmpfilename, 'w')
            tmpfile.write(input)
            tmpfile.close()
            self._parsefile(tmpfilename)
            os.remove(tmpfilename)
        else:
            self._parsefile(input)


############# format specific classes ###################

# the generic SubtitleFile can adapt to any format, but only the
# specilized classes can be used to construct a new file


class SubRipFile(SubtitleFile):
    """specialized class for SubRipFile's only"""
    Name = "SubRip subtitles file"
    Extensions = ['srt']

    def __init__(self, *args, **kwargs):
        super(SubRipFile, self).__init__(*args, **kwargs)
        if self._subtitlefile is None:
            self._subtitlefile = SubRip(self.filename or '', self._encoding)
        if self._subtitlefile.newline is None:
            self._subtitlefile.newline = newlines.UNIX


class MicroDVDFile(SubtitleFile):
    """specialized class for SubRipFile's only"""
    Name = "MicroDVD subtitles file"
    Extensions = ['sub']

    def __init__(self, *args, **kwargs):
        super(SubRipFile, self).__init__(*args, **kwargs)
        if self._subtitlefile is None:
            self._subtitlefile = MicroDVD(self.filename or '', self._encoding)
        if self._subtitlefile.newline is None:
            self._subtitlefile.newline = newlines.UNIX


class AdvSubStationAlphaFile(SubtitleFile):
    """specialized class for SubRipFile's only"""
    Name = "Advanced Substation Alpha subtitles file"
    Extensions = ['ass']

    def __init__(self, *args, **kwargs):
        super(SubRipFile, self).__init__(*args, **kwargs)
        if self._subtitlefile is None:
            self._subtitlefile = AdvSubStationAlpha(self.filename or '', self._encoding)
        if self._subtitlefile.newline is None:
            self._subtitlefile.newline = newlines.UNIX


class SubStationAlphaFile(SubtitleFile):
    """specialized class for SubRipFile's only"""
    Name = "Substation Alpha subtitles file"
    Extensions = ['ssa']

    def __init__(self, *args, **kwargs):
        super(SubRipFile, self).__init__(*args, **kwargs)
        if self._subtitlefile is None:
            self._subtitlefile = SubStationAlpha(self.filename or '', self._encoding)
        if self._subtitlefile.newline is None:
            self._subtitlefile.newline = newlines.UNIX

########NEW FILE########
__FILENAME__ = symbian
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import re


charset_re = re.compile('CHARACTER_SET[ ]+(?P<charset>.*)')
header_item_or_end_re = re.compile('(((?P<key>[^ ]+)(?P<space>[ ]*:[ ]*)(?P<value>.*))|(?P<end_comment>[*]/))')
header_item_re = re.compile('(?P<key>[^ ]+)(?P<space>[ ]*:[ ]*)(?P<value>.*)')
string_entry_re = re.compile('(?P<start>rls_string[ ]+)(?P<id>[^ ]+)(?P<space>[ ]+)(?P<str>.*)')


def identity(x):
    return x


class ParseState(object):

    def __init__(self, f, charset, read_hook=identity):
        self.f = f
        self.charset = charset
        self.current_line = u''
        self.read_hook = read_hook
        self.read_line()

    def read_line(self):
        current_line = self.current_line
        self.read_hook(current_line)
        self.current_line = self.f.next().decode(self.charset)
        return current_line


def read_while(ps, f, test):
    result = f(ps.current_line)
    while test(result):
        ps.read_line()
        result = f(ps.current_line)
    return result


def eat_whitespace(ps):
    read_while(ps, identity, lambda line: line.strip() == '')


def skip_no_translate(ps):
    if ps.current_line.startswith('// DO NOT TRANSLATE'):
        ps.read_line()
        read_while(ps, identity, lambda line: not line.startswith('// DO NOT TRANSLATE'))
        ps.read_line()
        eat_whitespace(ps)


def read_charset(lines):
    for line in lines:
        match = charset_re.match(line)
        if match is not None:
            return match.groupdict()['charset']
    return 'UTF-8'

########NEW FILE########
__FILENAME__ = tbx
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2006-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""module for handling TBX glossary files"""

from lxml import etree

from translate.storage import lisa


class tbxunit(lisa.LISAunit):
    """A single term in the TBX file.
Provisional work is done to make several languages possible."""
    rootNode = "termEntry"
    languageNode = "langSet"
    textNode = "term"

    def createlanguageNode(self, lang, text, purpose):
        """returns a langset xml Element setup with given parameters"""
        if isinstance(text, str):
            text = text.decode("utf-8")
        langset = etree.Element(self.languageNode)
        lisa.setXMLlang(langset, lang)
        tig = etree.SubElement(langset, "tig")  # or ntig with termGrp inside
        term = etree.SubElement(tig, self.textNode)
        # probably not what we want:
        # lisa.setXMLspace(term, "preserve")
        term.text = text
        return langset

    def getid(self):
        # The id attribute is optional
        return self.xmlelement.get("id") or self.source


class tbxfile(lisa.LISAfile):
    """Class representing a TBX file store."""
    UnitClass = tbxunit
    Name = "TBX Glossary"
    Mimetypes = ["application/x-tbx"]
    Extensions = ["tbx"]
    rootNode = "martif"
    bodyNode = "body"
    XMLskeleton = '''<?xml version="1.0"?>
<!DOCTYPE martif PUBLIC "ISO 12200:1999A//DTD MARTIF core (DXFcdV04)//EN" "TBXcdv04.dtd">
<martif type="TBX">
<martifHeader>
<fileDesc>
<sourceDesc><p>Translate Toolkit</p></sourceDesc>
</fileDesc>
</martifHeader>
<text><body></body></text>
</martif>'''

    def addheader(self):
        """Initialise headers with TBX specific things."""
        lisa.setXMLlang(self.document.getroot(), self.sourcelanguage)

########NEW FILE########
__FILENAME__ = test_aresource
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from lxml import etree

from translate.storage import aresource, test_monolingual


class TestAndroidResourceUnit(test_monolingual.TestMonolingualUnit):
    UnitClass = aresource.AndroidResourceUnit

    def __check_escape(self, string, xml):
        """Helper that checks that a string is output with the right escape."""
        unit = self.UnitClass("Test String")
        unit.target = string

        print("unit.target:", repr(unit.target))
        print("xml:", repr(xml))

        assert str(unit) == xml

    def __check_parse(self, string, xml):
        """Helper that checks that a string is parsed correctly."""
        parser = etree.XMLParser(strip_cdata=False)

        translatable = 'translatable="false"' not in xml
        et = etree.fromstring(xml, parser)
        unit = self.UnitClass.createfromxmlElement(et)

        print("unit.target:", repr(unit.target))
        print("string:", string)
        print("translatable:", repr(unit.istranslatable()))

        assert unit.target == string
        assert unit.istranslatable() == translatable

    ############################ Check string escape ##########################

    def test_escape_message_with_newline(self):
        string = 'message\nwith newline'
        xml = '<string name="Test String">message\\nwith newline</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_message_with_newline_in_xml(self):
        string = 'message \nwith newline in xml'
        xml = ('<string name="Test String">message\n\\nwith newline in xml'
               '</string>\n\n')
        self.__check_escape(string, xml)

    def test_escape_twitter(self):
        string = '@twitterescape'
        xml = '<string name="Test String">\\@twitterescape</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_quote(self):
        string = 'quote \'escape\''
        xml = '<string name="Test String">quote \\\'escape\\\'</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_double_space(self):
        string = 'double  space'
        xml = '<string name="Test String">"double  space"</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_leading_space(self):
        string = ' leading space'
        xml = '<string name="Test String">" leading space"</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_xml_entities(self):
        string = '>xml&entities'
        xml = '<string name="Test String">&gt;xml&amp;entities</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_html_code(self):
        string = 'some <b>html code</b> here'
        xml = ('<string name="Test String">some <b>html code</b> here'
               '</string>\n\n')
        self.__check_escape(string, xml)

    def test_escape_arrows(self):
        string = '<<< arrow'
        xml = '<string name="Test String">&lt;&lt;&lt; arrow</string>\n\n'
        self.__check_escape(string, xml)

    def test_escape_link(self):
        string = '<a href="http://example.net">link</a>'
        xml = ('<string name="Test String"><a href="http://example.net">link'
               '</a></string>\n\n')
        self.__check_escape(string, xml)

    def test_escape_link_and_text(self):
        string = '<a href="http://example.net">link</a> and text'
        xml = ('<string name="Test String"><a href="http://example.net">link'
               '</a> and text</string>\n\n')
        self.__check_escape(string, xml)

    def test_escape_blank_string(self):
        string = ''
        xml = '<string name="Test String"></string>\n\n'
        self.__check_escape(string, xml)

    ############################ Check string parse ###########################

    def test_parse_message_with_newline(self):
        string = 'message\nwith newline'
        xml = '<string name="Test String">message\\nwith newline</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_message_with_newline_in_xml(self):
        string = 'message \nwith newline in xml'
        xml = ('<string name="Test String">message\n\\nwith newline in xml'
               '</string>\n\n')
        self.__check_parse(string, xml)

    def test_parse_twitter(self):
        string = '@twitterescape'
        xml = '<string name="Test String">\\@twitterescape</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_quote(self):
        string = 'quote \'escape\''
        xml = '<string name="Test String">quote \\\'escape\\\'</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_double_space(self):
        string = 'double  space'
        xml = '<string name="Test String">"double  space"</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_leading_space(self):
        string = ' leading space'
        xml = '<string name="Test String">" leading space"</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_xml_entities(self):
        string = '>xml&entities'
        xml = '<string name="Test String">&gt;xml&amp;entities</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_html_code(self):
        string = 'some <b>html code</b> here'
        xml = ('<string name="Test String">some <b>html code</b> here'
               '</string>\n\n')
        self.__check_parse(string, xml)

    def test_parse_arrows(self):
        string = '<<< arrow'
        xml = '<string name="Test String">&lt;&lt;&lt; arrow</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_link(self):
        string = '<a href="http://example.net">link</a>'
        xml = ('<string name="Test String"><a href="http://example.net">link'
               '</a></string>\n\n')
        self.__check_parse(string, xml)

    def test_parse_link_and_text(self):
        string = '<a href="http://example.net">link</a> and text'
        xml = ('<string name="Test String"><a href="http://example.net">link'
               '</a> and text</string>\n\n')
        self.__check_parse(string, xml)

    def test_parse_blank_string(self):
        string = ''
        xml = '<string name="Test String"></string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_blank_string_again(self):
        string = ''
        xml = '<string name="Test String"/>\n\n'
        self.__check_parse(string, xml)

    def test_parse_double_quotes_string(self):
        """Check that double quotes got removed."""
        string = 'double quoted text'
        xml = '<string name="Test String">"double quoted text"</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_newline_in_string(self):
        """Check that newline is read as space.

        At least it seems to be what Android does.
        """
        string = 'newline in string'
        xml = '<string name="Test String">newline\nin string</string>\n\n'
        self.__check_parse(string, xml)

    def test_parse_not_translatable_string(self):
        string = 'string'
        xml = ('<string name="Test String" translatable="false">string'
               '</string>\n\n')
        self.__check_parse(string, xml)


class TestAndroidResourceFile(test_monolingual.TestMonolingualStore):
    StoreClass = aresource.AndroidResourceFile

########NEW FILE########
__FILENAME__ = test_base
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""tests for storage base classes"""

import os
import warnings

import pytest

from translate.misc.multistring import multistring
from translate.storage import base, factory
from translate.storage.placeables import general, parse as rich_parse


def headerless_len(units):
    """return count of translatable (non header) units"""
    return len(filter(lambda x: not x.isheader(), units))


def first_translatable(store):
    """returns first translatable unit, skipping header if present"""
    if store.units[0].isheader() and len(store.units) > 1:
        return store.units[1]
    else:
        return store.units[0]


def test_force_override():
    """Tests that derived classes are not allowed to call certain functions"""

    class BaseClass:

        def test(self):
            base.force_override(self.test, BaseClass)
            return True

        @classmethod
        def classtest(cls):
            base.force_override(cls.classtest, BaseClass)
            return True

    class DerivedClass(BaseClass):
        pass

    baseobject = BaseClass()
    assert baseobject.test()
    assert baseobject.classtest()
    derivedobject = DerivedClass()
    assert pytest.raises(NotImplementedError, derivedobject.test)
    assert pytest.raises(NotImplementedError, derivedobject.classtest)


class TestTranslationUnit:
    """Tests a TranslationUnit.
    Derived classes can reuse these tests by pointing UnitClass to a derived Unit"""
    UnitClass = base.TranslationUnit

    def setup_method(self, method):
        self.unit = self.UnitClass("Test String")

    def test_isfuzzy(self):
        """Test that we can call isfuzzy() on a unit.

        The default return value for isfuzzy() should be False.
        """
        assert not self.unit.isfuzzy()

    def test_create(self):
        """tests a simple creation with a source string"""
        unit = self.unit
        print('unit.source:', unit.source)
        assert unit.source == "Test String"

    def test_eq(self):
        """tests equality comparison"""
        unit1 = self.unit
        unit2 = self.UnitClass("Test String")
        unit3 = self.UnitClass("Test String")
        unit4 = self.UnitClass("Blessed String")
        unit5 = self.UnitClass("Blessed String")
        unit6 = self.UnitClass("Blessed String")
        assert unit1 == unit1
        assert unit1 == unit2
        assert unit1 != unit4
        unit1.target = "Stressed Ting"
        unit2.target = "Stressed Ting"
        unit5.target = "Stressed Bling"
        unit6.target = "Stressed Ting"
        assert unit1 == unit2
        assert unit1 != unit3
        assert unit4 != unit5
        assert unit1 != unit6

    def test_target(self):
        unit = self.unit
        assert not unit.target
        unit.target = "Stressed Ting"
        assert unit.target == "Stressed Ting"
        unit.target = "Stressed Bling"
        assert unit.target == "Stressed Bling"
        unit.target = ""
        assert unit.target == ""

    def test_escapes(self):
        """Test all sorts of characters that might go wrong in a quoting and
        escaping roundtrip."""
        unit = self.unit
        specials = ['Fish & chips', 'five < six', 'six > five', 'five &lt; six',
                    'Use &nbsp;', 'Use &amp;nbsp;', 'Use &amp;amp;nbsp;',
                    'A "solution"', "skop 'n bal", '"""', "'''", u'',
                    '\n', '\t', '\r', '\r\n', '\\r', '\\', '\\\r']
        for special in specials:
            unit.source = special
            print("unit.source:", repr(unit.source))
            print("special:", repr(special))
            assert unit.source == special

    def test_difficult_escapes(self):
        """Test difficult characters that might go wrong in a quoting and
        escaping roundtrip."""

        unit = self.unit
        specials = ['\\n', '\\t', '\\"', '\\ ',
                    '\\\n', '\\\t', '\\\\n', '\\\\t', '\\\\r', '\\\\"',
                    '\\r\\n', '\\\\r\\n', '\\r\\\\n', '\\\\n\\\\r']
        for special in specials:
            unit.source = special
            print("unit.source:", repr(unit.source) + '|')
            print("special:", repr(special) + '|')
            assert unit.source == special

    def test_note_sanity(self):
        """Tests that all subclasses of the base behaves consistently with regards to notes."""
        unit = self.unit

        unit.addnote(u"Test note 1", origin="translator")
        unit.addnote(u"Test note 2", origin="translator")
        unit.addnote(u"Test note 3", origin="translator")
        expected_notes = u"Test note 1\nTest note 2\nTest note 3"
        actual_notes = unit.getnotes(origin="translator")
        assert actual_notes == expected_notes

        # Test with no origin.
        unit.removenotes()
        assert not unit.getnotes()
        unit.addnote(u"Test note 1")
        unit.addnote(u"Test note 2")
        unit.addnote(u"Test note 3")
        expected_notes = u"Test note 1\nTest note 2\nTest note 3"
        actual_notes = unit.getnotes()
        assert actual_notes == expected_notes

    def test_rich_get(self):
        """Basic test for converting from multistrings to StringElem trees."""
        target_mstr = multistring([u'tst', u'<b>string</b>'])
        unit = self.UnitClass(multistring([u'a', u'b']))
        unit.rich_parsers = general.parsers
        unit.target = target_mstr
        elems = unit.rich_target

        if unit.hasplural():
            assert len(elems) == 2
            assert len(elems[0].sub) == 1
            assert len(elems[1].sub) == 3

            assert unicode(elems[0]) == target_mstr.strings[0]
            assert unicode(elems[1]) == target_mstr.strings[1]

            assert unicode(elems[1].sub[0]) == u'<b>'
            assert unicode(elems[1].sub[1]) == u'string'
            assert unicode(elems[1].sub[2]) == u'</b>'
        else:
            assert len(elems[0].sub) == 1
            assert unicode(elems[0]) == target_mstr.strings[0]

    def test_rich_set(self):
        """Basic test for converting from multistrings to StringElem trees."""
        elems = [
            rich_parse(u'Tst <x>string</x>', general.parsers),
            rich_parse(u'Another test string.', general.parsers),
        ]
        unit = self.UnitClass(multistring([u'a', u'b']))
        unit.rich_target = elems

        if unit.hasplural():
            assert unit.target.strings[0] == u'Tst <x>string</x>'
            assert unit.target.strings[1] == u'Another test string.'
        else:
            assert unit.target == u'Tst <x>string</x>'


class TestTranslationStore(object):
    """Tests a TranslationStore.
    Derived classes can reuse these tests by pointing StoreClass to a derived Store"""
    StoreClass = base.TranslationStore

    def setup_method(self, method):
        """Allocates a unique self.filename for the method, making sure it doesn't exist"""
        self.filename = "%s_%s.test" % (self.__class__.__name__, method.__name__)
        if os.path.exists(self.filename):
            os.remove(self.filename)
        warnings.resetwarnings()

    def teardown_method(self, method):
        """Makes sure that if self.filename was created by the method, it is cleaned up"""
        if os.path.exists(self.filename):
            os.remove(self.filename)
        warnings.resetwarnings()

    def test_create_blank(self):
        """Tests creating a new blank store"""
        store = self.StoreClass()
        assert headerless_len(store.units) == 0

    def test_add(self):
        """Tests adding a new unit with a source string"""
        store = self.StoreClass()
        unit = store.addsourceunit("Test String")
        print(str(unit))
        print(str(store))
        assert headerless_len(store.units) == 1
        assert unit.source == "Test String"

    def test_find(self):
        """Tests searching for a given source string"""
        store = self.StoreClass()
        unit1 = store.addsourceunit("Test String")
        unit2 = store.addsourceunit("Blessed String")
        assert store.findunit("Test String") == unit1
        assert store.findunit("Blessed String") == unit2
        assert store.findunit("Nest String") is None

    def test_translate(self):
        """Tests the translate method and non-ascii characters."""
        store = self.StoreClass()
        unit = store.addsourceunit("scissor")
        unit.target = u"skr"
        unit = store.addsourceunit(u"Bezir curve")
        unit.target = u"Bezir-kurwe"
        assert store.translate("scissor") == u"skr"
        assert store.translate(u"Bezir curve") == u"Bezir-kurwe"

    def reparse(self, store):
        """converts the store to a string and back to a store again"""
        storestring = str(store)
        newstore = self.StoreClass.parsestring(storestring)
        return newstore

    def check_equality(self, store1, store2):
        """asserts that store1 and store2 are the same"""
        assert headerless_len(store1.units) == headerless_len(store2.units)
        for n, store1unit in enumerate(store1.units):
            store2unit = store2.units[n]
            match = store1unit == store2unit
            if not match:
                print("match failed between elements %d of %d" % ((n + 1), headerless_len(store1.units)))
                print("store1:")
                print(str(store1))
                print("store2:")
                print(str(store2))
                print("store1.units[%d].__dict__:" % n, store1unit.__dict__)
                print("store2.units[%d].__dict__:" % n, store2unit.__dict__)
                assert store1unit == store2unit

    def test_parse(self):
        """Tests converting to a string and parsing the resulting string"""
        store = self.StoreClass()
        unit1 = store.addsourceunit("Test String")
        unit1.target = "Test String"
        unit2 = store.addsourceunit("Test String 2")
        unit2.target = "Test String 2"
        newstore = self.reparse(store)
        self.check_equality(store, newstore)

    def test_files(self):
        """Tests saving to and loading from files"""
        store = self.StoreClass()
        unit1 = store.addsourceunit("Test String")
        unit1.target = "Test String"
        unit2 = store.addsourceunit("Test String 2")
        unit2.target = "Test String 2"
        store.savefile(self.filename)
        newstore = self.StoreClass.parsefile(self.filename)
        self.check_equality(store, newstore)

    def test_save(self):
        """Tests that we can save directly back to the original file."""
        store = self.StoreClass()
        unit1 = store.addsourceunit("Test String")
        unit1.target = "Test String"
        unit2 = store.addsourceunit("Test String 2")
        unit2.target = "Test String 2"
        store.savefile(self.filename)
        store.save()
        newstore = self.StoreClass.parsefile(self.filename)
        self.check_equality(store, newstore)

    def test_markup(self):
        """Tests that markup survives the roundtrip. Most usefull for xml types."""
        store = self.StoreClass()
        unit = store.addsourceunit("<vark@hok.org> %d keer %2$s")
        assert unit.source == "<vark@hok.org> %d keer %2$s"
        unit.target = "bla"
        assert store.translate("<vark@hok.org> %d keer %2$s") == "bla"

    def test_nonascii(self):
        store = self.StoreClass()
        unit = store.addsourceunit(u"Bezir curve")
        string = u"Bezir-kurwe"
        unit.target = string.encode("utf-8")
        answer = store.translate(u"Bezir curve")
        if isinstance(answer, str):
            answer = answer.decode("utf-8")
        assert answer == u"Bezir-kurwe"
        #Just test that __str__ doesn't raise exception:
        src = str(store)

    def test_extensions(self):
        """Test that the factory knows the extensions for this class."""
        supported = factory.supported_files()
        supported_dict = dict([(name, (extensions, mimetypes)) for name, extensions, mimetypes in supported])
        if not (self.StoreClass.Name and self.StoreClass.Name in supported_dict):
            return
        detail = supported_dict[self.StoreClass.Name]  # will start to get problematic once translated
        print("Factory:", detail[0])
        print("StoreClass:", self.StoreClass.Extensions)
        for ext in detail[0]:
            assert ext in self.StoreClass.Extensions
        for ext in self.StoreClass.Extensions:
            assert ext in detail[0]

    def test_mimetypes(self):
        """Test that the factory knows the mimetypes for this class."""
        supported = factory.supported_files()
        supported_dict = dict([(name, (extensions, mimetypes)) for name, extensions, mimetypes in supported])
        if not (self.StoreClass.Name and self.StoreClass.Name in supported_dict):
            return
        detail = supported_dict[self.StoreClass.Name]  # will start to get problematic once translated
        print("Factory:", detail[1])
        print("StoreClass:", self.StoreClass.Mimetypes)
        for ext in detail[1]:
            assert ext in self.StoreClass.Mimetypes
        for ext in self.StoreClass.Mimetypes:
            assert ext in detail[1]

########NEW FILE########
__FILENAME__ = test_catkeys
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.storage import catkeys, test_base


class TestCatkeysUnit(test_base.TestTranslationUnit):
    UnitClass = catkeys.CatkeysUnit

    def test_difficult_escapes(self):
        r"""Wordfast files need to perform magic with escapes.

           Wordfast does not accept line breaks in its TM (even though they would be
           valid in CSV) thus we turn \\n into \n and reimplement the base class test but
           eliminate a few of the actual tests.
        """
        unit = self.unit
        specials = ['\\"', '\\ ',
                    '\\\n', '\\\t', '\\\\r', '\\\\"']
        for special in specials:
            unit.source = special
            print("unit.source:", repr(unit.source) + '|')
            print("special:", repr(special) + '|')
            assert unit.source == special

    def test_newlines(self):
        """Wordfast does not like real newlines"""
        unit = self.UnitClass("One\nTwo")
        assert unit.dict['source'] == "One\\nTwo"

    def test_istranslated(self):
        unit = self.UnitClass()
        assert not unit.istranslated()
        unit.source = "Test"
        assert not unit.istranslated()
        unit.target = "Rest"
        assert unit.istranslated()

    def test_note_sanity(self):
        """Override test, since the format doesn't support notes."""
        pass


class TestCatkeysFile(test_base.TestTranslationStore):
    StoreClass = catkeys.CatkeysFile

########NEW FILE########
__FILENAME__ = test_cpo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys

from pytest import importorskip, mark, raises
cpo = importorskip("not sys.platform.startswith('linux')")

from translate.misc import wStringIO
from translate.misc.multistring import multistring
from translate.storage import test_po


cpo = importorskip("translate.storage.cpo")


class TestCPOUnit(test_po.TestPOUnit):
    UnitClass = cpo.pounit

    def test_plurals(self):
        """Tests that plurals are handled correctly."""
        unit = self.UnitClass("Cow")
        unit.msgid_plural = ["Cows"]
        assert isinstance(unit.source, multistring)
        assert unit.source.strings == ["Cow", "Cows"]
        assert unit.source == "Cow"

        unit.target = ["Koei", "Koeie"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == ["Koei", "Koeie"]
        assert unit.target == "Koei"

        unit.target = {0: "Koei", 3: "Koeie"}
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == ["Koei", "Koeie"]
        assert unit.target == "Koei"

        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target == u"Sk\u00ear"

    def test_plural_reduction(self):
        """checks that reducing the number of plurals supplied works"""
        unit = self.UnitClass("Tree")
        unit.msgid_plural = ["Trees"]
        assert isinstance(unit.source, multistring)
        assert unit.source.strings == ["Tree", "Trees"]
        unit.target = multistring(["Boom", "Bome", "Baie Bome"])
        assert isinstance(unit.source, multistring)
        assert unit.target.strings == ["Boom", "Bome", "Baie Bome"]
        unit.target = multistring(["Boom", "Bome"])
        assert unit.target.strings == ["Boom", "Bome"]
        unit.target = "Boom"
        # FIXME: currently assigning the target to the same as the first string won't change anything
        # we need to verify that this is the desired behaviour...
        assert unit.target.strings[0] == "Boom"
        unit.target = "Een Boom"
        assert unit.target.strings == ["Een Boom"]

    def test_notes(self):
        """tests that the generic notes API works"""
        unit = self.UnitClass("File")
        assert unit.getnotes() == ""
        unit.addnote("Which meaning of file?")
        assert unit.getnotes("translator") == "Which meaning of file?"
        assert unit.getnotes("developer") == ""
        unit.addnote("Verb", origin="programmer")
        assert unit.getnotes("developer") == "Verb"
        unit.addnote("Thank you", origin="translator")
        assert unit.getnotes("translator") == "Which meaning of file?\nThank you"
        assert unit.getnotes() == "Which meaning of file?\nThank you\nVerb"
        assert raises(ValueError, unit.getnotes, "devteam")

    def test_notes_withcomments(self):
        """tests that when we add notes that look like comments that we treat them properly"""
        unit = self.UnitClass("File")
        unit.addnote("# Double commented comment")
        assert unit.getnotes() == "# Double commented comment"


class TestCPOFile(test_po.TestPOFile):
    StoreClass = cpo.pofile

    def test_msgidcomments(self):
        """checks that we handle msgid comments"""
        posource = 'msgid "test me"\nmsgstr ""'
        pofile = self.poparse(posource)
        thepo = pofile.units[0]
        thepo.msgidcomment = "first comment"
        print(pofile)
        print("Blah", thepo.source)
        assert thepo.source == "test me"
        thepo.msgidcomment = "second comment"
        assert str(pofile).count("_:") == 1

    @mark.xfail(reason="Were disabled during port of Pypo to cPO - they might work")
    def test_merge_duplicates_msgctxt(self):
        """checks that merging duplicates works for msgctxt"""
        posource = '#: source1\nmsgid "test me"\nmsgstr ""\n\n#: source2\nmsgid "test me"\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("msgctxt")
        print(pofile)
        assert len(pofile.units) == 2
        assert str(pofile.units[0]).count("source1") == 2
        assert str(pofile.units[1]).count("source2") == 2

    @mark.xfail(reason="Were disabled during port of Pypo to cPO - they might work")
    def test_merge_blanks(self):
        """checks that merging adds msgid_comments to blanks"""
        posource = '#: source1\nmsgid ""\nmsgstr ""\n\n#: source2\nmsgid ""\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("merge")
        assert len(pofile.units) == 2
        print(pofile.units[0].msgidcomments)
        print(pofile.units[1].msgidcomments)
        assert po.unquotefrompo(pofile.units[0].msgidcomments) == "_: source1\n"
        assert po.unquotefrompo(pofile.units[1].msgidcomments) == "_: source2\n"

    @mark.xfail(reason="Were disabled during port of Pypo to cPO - they might work")
    def test_msgid_comment(self):
        """checks that when adding msgid_comments we place them on a newline"""
        posource = '#: source0\nmsgid "Same"\nmsgstr ""\n\n#: source1\nmsgid "Same"\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("msgid_comment")
        assert len(pofile.units) == 2
        assert po.unquotefrompo(pofile.units[0].msgidcomments) == "_: source0\n"
        assert po.unquotefrompo(pofile.units[1].msgidcomments) == "_: source1\n"
        # Now lets check for formating
        for i in (0, 1):
            expected = '''#: source%d\nmsgid ""\n"_: source%d\\n"\n"Same"\nmsgstr ""\n''' % (i, i)
            assert pofile.units[i].__str__() == expected

    @mark.xfail(reason="Were disabled during port of Pypo to cPO - they might work")
    def test_keep_blanks(self):
        """checks that keeping keeps blanks and doesn't add msgid_comments"""
        posource = '#: source1\nmsgid ""\nmsgstr ""\n\n#: source2\nmsgid ""\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("keep")
        assert len(pofile.units) == 2
        # check we don't add msgidcomments
        assert po.unquotefrompo(pofile.units[0].msgidcomments) == ""
        assert po.unquotefrompo(pofile.units[1].msgidcomments) == ""

    def test_output_str_unicode(self):
        """checks that we can str(pofile) which is in unicode"""
        posource = u'''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
        pofile = self.StoreClass(wStringIO.StringIO(posource.encode("UTF-8")), encoding="UTF-8")
        assert len(pofile.units) == 1
        print(str(pofile))
        thepo = pofile.units[0]
#        assert str(pofile) == posource.encode("UTF-8")
        # extra test: what if we set the msgid to a unicode? this happens in prop2po etc
        thepo.source = u"Norwegian Bokm\xe5l"
#        assert str(thepo) == posource.encode("UTF-8")
        # Now if we set the msgstr to Unicode
        # this is an escaped half character (1/2)
        halfstr = "\xbd ...".decode("latin-1")
        thepo.target = halfstr
#        assert halfstr in str(pofile).decode("UTF-8")
        thepo.target = halfstr.encode("UTF-8")
#        assert halfstr.encode("UTF-8") in str(pofile)

    def test_posections(self):
        """checks the content of all the expected sections of a PO message"""
        posource = '# other comment\n#. automatic comment\n#: source comment\n#, fuzzy\nmsgid "One"\nmsgstr "Een"\n'
        pofile = self.poparse(posource)
        print(pofile)
        assert len(pofile.units) == 1
        assert str(pofile) == posource

    def test_multiline_obsolete(self):
        """Tests for correct output of mulitline obsolete messages"""
        posource = '#~ msgid ""\n#~ "Old thing\\n"\n#~ "Second old thing"\n#~ msgstr ""\n#~ "Ou ding\\n"\n#~ "Tweede ou ding"\n'
        pofile = self.poparse(posource)
        print("Source:\n%s" % posource)
        print("Output:\n%s" % str(pofile))
        assert len(pofile.units) == 1
        assert pofile.units[0].isobsolete()
        assert not pofile.units[0].istranslatable()
        assert str(pofile) == posource

    def test_unassociated_comments(self):
        """tests behaviour of unassociated comments."""
        oldsource = '# old lonesome comment\n\nmsgid "one"\nmsgstr "een"\n'
        oldfile = self.poparse(oldsource)
        print("__str__", str(oldfile))
        assert len(oldfile.units) == 1
        assert str(oldfile).find("# old lonesome comment\nmsgid") >= 0

########NEW FILE########
__FILENAME__ = test_csvl10n
#!/usr/bin/env python

from translate.storage import csvl10n, test_base


class TestCSVUnit(test_base.TestTranslationUnit):
    UnitClass = csvl10n.csvunit


class TestCSV(test_base.TestTranslationStore):
    StoreClass = csvl10n.csvfile

    def test_singlequoting(self):
        """Tests round trip on single quoting at start of string"""
        store = self.StoreClass()
        unit1 = store.addsourceunit("Test 'String'")
        unit2 = store.addsourceunit("'Blessed' String")
        unit3 = store.addsourceunit("'Quoted String'")
        assert unit3.source == "'Quoted String'"
        newstore = self.reparse(store)
        self.check_equality(store, newstore)
        assert store.units[2] == newstore.units[2]
        assert str(store) == str(newstore)

########NEW FILE########
__FILENAME__ = test_directory
#!/usr/bin/env python

"""Tests for the directory module"""

import os

from translate.storage import directory


class TestDirectory(object):
    """a test class to run tests on a test Pootle Server"""

    def setup_method(self, method):
        """sets up a test directory"""
        print("setup_method called on", self.__class__.__name__)
        self.testdir = "%s_testdir" % (self.__class__.__name__)
        self.cleardir(self.testdir)
        os.mkdir(self.testdir)

    def teardown_method(self, method):
        """removes the attributes set up by setup_method"""
        self.cleardir(self.testdir)

    def cleardir(self, dirname):
        """removes the given directory"""
        if os.path.exists(dirname):
            for dirpath, subdirs, filenames in os.walk(dirname, topdown=False):
                for name in filenames:
                    os.remove(os.path.join(dirpath, name))
                for name in subdirs:
                    os.rmdir(os.path.join(dirpath, name))
        if os.path.exists(dirname):
            os.rmdir(dirname)
        assert not os.path.exists(dirname)

    def touchfiles(self, dir, filenames, content=None):
        for filename in filenames:
            f = open(os.path.join(dir, filename), "w")
            if content:
                f.write(content)
            f.close()

    def mkdir(self, dir):
        """Makes a directory inside self.testdir."""
        os.mkdir(os.path.join(self.testdir, dir))

    def test_created(self):
        """test that the directory actually exists"""
        print(self.testdir)
        assert os.path.isdir(self.testdir)

    def test_basic(self):
        """Tests basic functionality."""
        files = ["a.po", "b.po", "c.po"]
        files.sort()
        self.touchfiles(self.testdir, files)

        d = directory.Directory(self.testdir)
        filenames = [name for dir, name in d.getfiles()]
        filenames.sort()
        assert filenames == files

    def test_structure(self):
        """Tests a small directory structure."""
        files = ["a.po", "b.po", "c.po"]
        self.touchfiles(self.testdir, files)
        self.mkdir("bla")
        self.touchfiles(os.path.join(self.testdir, "bla"), files)

        d = directory.Directory(self.testdir)
        filenames = [name for dirname, name in d.getfiles()]
        filenames.sort()
        files = files * 2
        files.sort()
        assert filenames == files

    def test_getunits(self):
        """Tests basic functionality."""
        files = ["a.po", "b.po", "c.po"]
        posource = '''msgid "bla"\nmsgstr "blabla"\n'''
        self.touchfiles(self.testdir, files, posource)

        d = directory.Directory(self.testdir)
        for unit in d.getunits():
            assert unit.target == "blabla"
        assert len(d.getunits()) == 3

########NEW FILE########
__FILENAME__ = test_dtd
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2013 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

from pytest import mark

from translate.misc import wStringIO
from translate.storage import dtd, test_monolingual


def test_roundtrip_quoting():
    specials = [
        'Fish & chips',
        'five < six',
        'six > five',
        'Use &nbsp;',
        'Use &amp;nbsp;A "solution"',
        "skop 'n bal",
        '"""',
        "'''",
        '\n',
        '\t',
        '\r',
        'Escape at end \\',
        '',
        '\\n',
        '\\t',
        '\\r',
        '\\"',
        '\r\n',
        '\\r\\n',
        '\\',
        "Completed %S",
        "&blockAttackSites;",
        "&#x00A0;",
        "&intro-point2-a;",
        "&basePBMenu.label;",
        #"Don't buy",
        #"Don't \"buy\"",
        "A \"thing\"",
        "<a href=\"http"
    ]
    for special in specials:
        quoted_special = dtd.quotefordtd(special)
        unquoted_special = dtd.unquotefromdtd(quoted_special)
        print("special: %r\nquoted: %r\nunquoted: %r\n" % (special,
                                                           quoted_special,
                                                           unquoted_special))
        assert special == unquoted_special


@mark.xfail(reason="Not Implemented")
def test_quotefordtd_unimplemented_cases():
    """Test unimplemented quoting DTD cases."""
    assert dtd.quotefordtd("Between <p> and </p>") == ('"Between &lt;p&gt; and'
                                                       ' &lt;/p&gt;"')


def test_quotefordtd():
    """Test quoting DTD definitions"""
    assert dtd.quotefordtd('') == '""'
    assert dtd.quotefordtd("") == '""'
    assert dtd.quotefordtd("Completed %S") == '"Completed &#037;S"'
    assert dtd.quotefordtd("&blockAttackSites;") == '"&blockAttackSites;"'
    assert dtd.quotefordtd("&#x00A0;") == '"&#x00A0;"'
    assert dtd.quotefordtd("&intro-point2-a;") == '"&intro-point2-a;"'
    assert dtd.quotefordtd("&basePBMenu.label;") == '"&basePBMenu.label;"'
    # The ' character isn't escaped as &apos; since the " char isn't present.
    assert dtd.quotefordtd("Don't buy") == '"Don\'t buy"'
    # The ' character is escaped as &apos; because the " character is present.
    assert dtd.quotefordtd("Don't \"buy\"") == '"Don&apos;t &quot;buy&quot;"'
    assert dtd.quotefordtd("A \"thing\"") == '"A &quot;thing&quot;"'
    # The " character is not escaped when it indicates an attribute value.
    assert dtd.quotefordtd("<a href=\"http") == "'<a href=\"http'"
    # &amp;
    assert dtd.quotefordtd("Color & Light") == '"Color &amp; Light"'
    assert dtd.quotefordtd("Color & &block;") == '"Color &amp; &block;"'
    assert dtd.quotefordtd("Color&Light &red;") == '"Color&amp;Light &red;"'
    assert dtd.quotefordtd("Color & Light; Yes") == '"Color &amp; Light; Yes"'


@mark.xfail(reason="Not Implemented")
def test_unquotefromdtd_unimplemented_cases():
    """Test unimplemented unquoting DTD cases."""
    assert dtd.unquotefromdtd('"&lt;p&gt; and &lt;/p&gt;"') == "<p> and </p>"


def test_unquotefromdtd():
    """Test unquoting DTD definitions"""
    # %
    assert dtd.unquotefromdtd('"Completed &#037;S"') == "Completed %S"
    assert dtd.unquotefromdtd('"Completed &#37;S"') == "Completed %S"
    assert dtd.unquotefromdtd('"Completed &#x25;S"') == "Completed %S"
    # &entity;
    assert dtd.unquotefromdtd('"Color&light &block;"') == "Color&light &block;"
    assert dtd.unquotefromdtd('"Color & Light; Red"') == "Color & Light; Red"
    assert dtd.unquotefromdtd('"&blockAttackSites;"') == "&blockAttackSites;"
    assert dtd.unquotefromdtd('"&intro-point2-a;"') == "&intro-point2-a;"
    assert dtd.unquotefromdtd('"&basePBMenu.label"') == "&basePBMenu.label"
    # &amp;
    assert dtd.unquotefromdtd('"Color &amp; Light"') == "Color & Light"
    assert dtd.unquotefromdtd('"Color &amp; &block;"') == "Color & &block;"
    # nbsp
    assert dtd.unquotefromdtd('"&#x00A0;"') == "&#x00A0;"
    # '
    assert dtd.unquotefromdtd("'Don&apos;t buy'") == "Don't buy"
    # "
    assert dtd.unquotefromdtd("'Don&apos;t &quot;buy&quot;'") == 'Don\'t "buy"'
    assert dtd.unquotefromdtd('"A &quot;thing&quot;"') == "A \"thing\""
    assert dtd.unquotefromdtd('"A &#x0022;thing&#x0022;"') == "A \"thing\""
    assert dtd.unquotefromdtd("'<a href=\"http'") == "<a href=\"http"
    # other chars
    assert dtd.unquotefromdtd('"&#187;"') == u""


def test_android_roundtrip_quoting():
    specials = [
        "don't",
        'the "thing"'
    ]
    for special in specials:
        quoted_special = dtd.quoteforandroid(special)
        unquoted_special = dtd.unquotefromandroid(quoted_special)
        print("special: %r\nquoted: %r\nunquoted: %r\n" % (special,
                                                           quoted_special,
                                                           unquoted_special))
        assert special == unquoted_special


def test_quoteforandroid():
    """Test quoting Android DTD definitions."""
    assert dtd.quoteforandroid("don't") == r'"don\u0027t"'
    assert dtd.quoteforandroid('the "thing"') == r'"the \&quot;thing\&quot;"'


def test_unquotefromandroid():
    """Test unquoting Android DTD definitions."""
    assert dtd.unquotefromandroid('"Don\\&apos;t show"') == "Don't show"
    assert dtd.unquotefromandroid('"Don\\\'t show"') == "Don't show"
    assert dtd.unquotefromandroid('"Don\\u0027t show"') == "Don't show"
    assert dtd.unquotefromandroid('"A \\&quot;thing\\&quot;"') == "A \"thing\""


def test_removeinvalidamp(recwarn):
    """tests the the removeinvalidamps function"""

    def tester(actual, expected=None):
        if expected is None:
            expected = actual
        assert dtd.removeinvalidamps("test.name", actual) == expected
    # No errors
    tester("Valid &entity; included")
    tester("Valid &entity.name; included")
    tester("Valid &#1234; included")
    tester("Valid &entity_name;")
    # Errors that require & removal
    tester("This &amp is broken", "This amp is broken")
    tester("Mad & &amp &amp;", "Mad  amp &amp;")
    dtd.removeinvalidamps("simple.warningtest", "Dimpled &Ring")
    assert recwarn.pop(UserWarning)


class TestDTDUnit(test_monolingual.TestMonolingualUnit):
    UnitClass = dtd.dtdunit

    def test_rich_get(self):
        pass

    def test_rich_set(self):
        pass


class TestDTD(test_monolingual.TestMonolingualStore):
    StoreClass = dtd.dtdfile

    def dtdparse(self, dtdsource):
        """helper that parses dtd source without requiring files"""
        dummyfile = wStringIO.StringIO(dtdsource)
        dtdfile = dtd.dtdfile(dummyfile)
        return dtdfile

    def dtdregen(self, dtdsource):
        """helper that converts dtd source to dtdfile object and back"""
        return str(self.dtdparse(dtdsource))

    def test_simpleentity(self):
        """checks that a simple dtd entity definition is parsed correctly"""
        dtdsource = '<!ENTITY test.me "bananas for sale">\n'
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 1
        dtdunit = dtdfile.units[0]
        assert dtdunit.entity == "test.me"
        assert dtdunit.definition == '"bananas for sale"'

    def test_blanklines(self):
        """checks that blank lines don't break the parsing or regeneration"""
        dtdsource = '<!ENTITY test.me "bananas for sale">\n\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_simpleentity_source(self):
        """checks that a simple dtd entity definition can be regenerated as source"""
        dtdsource = '<!ENTITY test.me "">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

        dtdsource = '<!ENTITY test.me "bananas for sale">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_hashcomment_source(self):
        """checks that a #expand comment is retained in the source"""
        dtdsource = '#expand <!ENTITY lang.version "__MOZILLA_LOCALE_VERSION__">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_commentclosing(self):
        """tests that comment closes with trailing space aren't duplicated"""
        dtdsource = '<!-- little comment --> \n<!ENTITY pane.title "Notifications">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_commententity(self):
        """check that we don't process messages in <!-- comments -->: bug 102"""
        dtdsource = '''<!-- commenting out until bug 38906 is fixed
<!ENTITY messagesHeader.label         "Messages"> -->'''
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 1
        dtdunit = dtdfile.units[0]
        print(dtdunit)
        assert dtdunit.isnull()

    def test_newlines_in_entity(self):
        """tests that we can handle newlines in the entity itself"""
        dtdsource = '''<!ENTITY fileNotFound.longDesc "
<ul>
  <li>Check the file name for capitalisation or other typing errors.</li>
  <li>Check to see if the file was moved, renamed or deleted.</li>
</ul>
">
'''
        dtdregen = self.dtdregen(dtdsource)
        print(dtdregen)
        print(dtdsource)
        assert dtdsource == dtdregen

    def test_conflate_comments(self):
        """Tests that comments don't run onto the same line"""
        dtdsource = '<!-- test comments -->\n<!-- getting conflated -->\n<!ENTITY sample.txt "hello">\n'
        dtdregen = self.dtdregen(dtdsource)
        print(dtdsource)
        print(dtdregen)
        assert dtdsource == dtdregen

    def test_localisation_notes(self):
        """test to ensure that we retain the localisation note correctly"""
        dtdsource = '''<!--LOCALIZATION NOTE (publishFtp.label): Edit box appears beside this label -->
<!ENTITY publishFtp.label "If publishing to a FTP site, enter the HTTP address to browse to:">
'''
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_entitityreference_in_source(self):
        """checks that an &entity; in the source is retained"""
        dtdsource = '<!ENTITY % realBrandDTD SYSTEM "chrome://branding/locale/brand.dtd">\n%realBrandDTD;\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    #test for bug #610
    def test_entitityreference_order_in_source(self):
        """checks that an &entity; in the source is retained"""
        dtdsource = '<!ENTITY % realBrandDTD SYSTEM "chrome://branding/locale/brand.dtd">\n%realBrandDTD;\n<!-- some comment -->\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

        # The following test is identical to the one above, except that the entity is split over two lines.
        # This is to ensure that a recent bug fixed in dtdunit.parse() is at least partly documented.
        # The essence of the bug was that after it had read "realBrandDTD", the line index is not reset
        # before starting to parse the next line. It would then read the next available word (sequence of
        # alphanum characters) in stead of SYSTEM and then get very confused by not finding an opening ' or
        # " in the entity, borking the parsing for threst of the file.
        dtdsource = '<!ENTITY % realBrandDTD\n SYSTEM "chrome://branding/locale/brand.dtd">\n%realBrandDTD;\n'
        # FIXME: The following line is necessary, because of dtdfile's inability to remember the spacing of
        # the source DTD file when converting back to DTD.
        dtdregen = self.dtdregen(dtdsource).replace('realBrandDTD SYSTEM', 'realBrandDTD\n SYSTEM')
        print(dtdsource)
        print(dtdregen)
        assert dtdsource == dtdregen

    @mark.xfail(reason="Not Implemented")
    def test_comment_following(self):
        """check that comments that appear after and entity are not pushed onto another line"""
        dtdsource = '<!ENTITY textZoomEnlargeCmd.commandkey2 "="> <!-- + is above this key on many keyboards -->'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_comment_newline_space_closing(self):
        """check that comments that are closed by a newline then space then --> don't break the following entries"""
        dtdsource = '<!-- Comment\n -->\n<!ENTITY searchFocus.commandkey "k">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    @mark.xfail(reason="Not Implemented")
    def test_invalid_quoting(self):
        """checks that invalid quoting doesn't work - quotes can't be reopened"""
        # TODO: we should rather raise an error
        dtdsource = '<!ENTITY test.me "bananas for sale""room">\n'
        assert dtd.unquotefromdtd(dtdsource[dtdsource.find('"'):]) == 'bananas for sale'
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 1
        dtdunit = dtdfile.units[0]
        assert dtdunit.definition == '"bananas for sale"'
        assert str(dtdfile) == '<!ENTITY test.me "bananas for sale">\n'

    def test_missing_quotes(self, recwarn):
        """test that we fail graacefully when a message without quotes is found (bug #161)"""
        dtdsource = '<!ENTITY bad no quotes">\n<!ENTITY good "correct quotes">\n'
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 1
        assert recwarn.pop(Warning)

    # Test for bug #68
    def test_entity_escaping(self):
        """Test entities escaping (&amp; &quot; &lt; &gt; &apos;) (bug #68)"""
        dtdsource = ('<!ENTITY securityView.privacy.header "Privacy &amp; '
                     'History">\n<!ENTITY rights.safebrowsing-term3 "Uncheck '
                     'the options to &quot;&blockAttackSites.label;&quot; and '
                     '&quot;&blockWebForgeries.label;&quot;">\n<!ENTITY '
                     'translate.test1 \'XML encodings don&apos;t work\'>\n'
                     '<!ENTITY translate.test2 "In HTML the text paragraphs '
                     'are enclosed between &lt;p&gt; and &lt;/p&gt; tags.">\n')
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 4
        #dtdunit = dtdfile.units[0]
        #assert dtdunit.definition == '"Privacy &amp; History"'
        #assert dtdunit.target == "Privacy & History"
        #assert dtdunit.source == "Privacy & History"
        dtdunit = dtdfile.units[1]
        assert dtdunit.definition == ('"Uncheck the options to &quot;'
                                      '&blockAttackSites.label;&quot; and '
                                      '&quot;&blockWebForgeries.label;&quot;"')
        assert dtdunit.target == ("Uncheck the options to \""
                                  "&blockAttackSites.label;\" and \""
                                  "&blockWebForgeries.label;\"")
        assert dtdunit.source == ("Uncheck the options to \""
                                  "&blockAttackSites.label;\" and \""
                                  "&blockWebForgeries.label;\"")
        dtdunit = dtdfile.units[2]
        assert dtdunit.definition == "'XML encodings don&apos;t work'"
        assert dtdunit.target == "XML encodings don\'t work"
        assert dtdunit.source == "XML encodings don\'t work"
        #dtdunit = dtdfile.units[3]
        #assert dtdunit.definition == ('"In HTML the text paragraphs are '
        #                              'enclosed between &lt;p&gt; and &lt;/p'
        #                              '&gt; tags."')
        #assert dtdunit.target == ("In HTML the text paragraphs are enclosed "
        #                          "between <p> and </p> tags.")
        #assert dtdunit.source == ("In HTML the text paragraphs are enclosed "
        #                          "between <p> and </p> tags.")

    # Test for bug #68
    def test_entity_escaping_roundtrip(self):
        """Test entities escaping roundtrip (&amp; &quot; ...) (bug #68)"""
        dtdsource = ('<!ENTITY securityView.privacy.header "Privacy &amp; '
                     'History">\n<!ENTITY rights.safebrowsing-term3 "Uncheck '
                     'the options to &quot;&blockAttackSites.label;&quot; and '
                     '&quot;&blockWebForgeries.label;&quot;">\n<!ENTITY '
                     'translate.test1 \'XML encodings don&apos;t work\'>\n'
                     '<!ENTITY translate.test2 "In HTML the text paragraphs '
                     'are enclosed between &lt;p&gt; and &lt;/p&gt; tags.">\n')
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen


class TestAndroidDTD(test_monolingual.TestMonolingualStore):
    StoreClass = dtd.dtdfile

    def dtdparse(self, dtdsource):
        """Parses an Android DTD source string and returns a DTD store.

        This allows to simulate reading from Android DTD files without really
        having real Android DTD files.
        """
        dummyfile = wStringIO.StringIO(dtdsource)
        dtdfile = dtd.dtdfile(dummyfile, android=True)
        return dtdfile

    def dtdregen(self, dtdsource):
        """Parses an Android DTD string to DTD store and then converts it back.

        This allows to simulate reading from an Android DTD file to an
        in-memory store and writing back to an Android DTD file without really
        having a real file.
        """
        return str(self.dtdparse(dtdsource))

    # Test for bug #2480
    def test_android_single_quote_escape(self):
        """Checks several single quote unescaping cases in Android DTD.

        See bug #2480.
        """
        dtdsource = ('<!ENTITY pref_char_encoding_off "Don\\\'t show menu">\n'
                     '<!ENTITY sync.nodevice.label \'Don\\&apos;t show\'>\n'
                     '<!ENTITY sync.nodevice.label "Don\\u0027t show">\n')
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 3
        dtdunit = dtdfile.units[0]
        assert dtdunit.definition == '"Don\\\'t show menu"'
        assert dtdunit.target == "Don't show menu"
        assert dtdunit.source == "Don't show menu"
        dtdunit = dtdfile.units[1]
        assert dtdunit.definition == "'Don\\&apos;t show'"
        assert dtdunit.target == "Don't show"
        assert dtdunit.source == "Don't show"
        dtdunit = dtdfile.units[2]
        assert dtdunit.definition == '"Don\\u0027t show"'
        assert dtdunit.target == "Don't show"
        assert dtdunit.source == "Don't show"

    # Test for bug #2480
    def test_android_single_quote_escape_parse_and_convert_back(self):
        """Checks that Android DTD don't change after parse and convert back.

        An Android DTD source string with several single quote escapes is used
        instead of real files.

        See bug #2480.
        """
        dtdsource = ('<!ENTITY pref_char_encoding_off "Don\\\'t show menu">\n'
                     '<!ENTITY sync.nodevice.label \'Don\\&apos;t show\'>\n'
                     '<!ENTITY sync.nodevice.label "Don\\u0027t show">\n')
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

    def test_android_double_quote_escape(self):
        """Checks double quote unescaping in Android DTD."""
        dtdsource = '<!ENTITY translate.test "A \\&quot;thing\\&quot;">\n'
        dtdfile = self.dtdparse(dtdsource)
        assert len(dtdfile.units) == 1
        dtdunit = dtdfile.units[0]
        assert dtdunit.definition == '"A \\&quot;thing\\&quot;"'
        assert dtdunit.target == "A \"thing\""
        assert dtdunit.source == "A \"thing\""

    def test_android_double_quote_escape_parse_and_convert_back(self):
        """Checks that Android DTD don't change after parse and convert back.

        An Android DTD source string with double quote escapes is used instead
        of real files.
        """
        dtdsource = '<!ENTITY translate.test "A \\&quot;thing\\&quot;">\n'
        dtdregen = self.dtdregen(dtdsource)
        assert dtdsource == dtdregen

########NEW FILE########
__FILENAME__ = test_factory
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
from bz2 import BZ2File
from gzip import GzipFile

from translate.misc import wStringIO
from translate.storage import factory
from translate.storage.directory import Directory


def classname(filename):
    """returns the classname to ease testing"""
    classinstance = factory.getclass(filename)
    return str(classinstance.__name__).lower()


def givefile(filename, content):
    """returns a file dummy object with the given content"""
    file = wStringIO.StringIO(content)
    file.name = filename
    return file


class BaseTestFactory:

    def setup_method(self, method):
        """sets up a test directory"""
        self.testdir = "%s_testdir" % (self.__class__.__name__)
        self.cleardir(self.testdir)
        os.mkdir(self.testdir)

    def teardown_method(self, method):
        """removes the attributes set up by setup_method"""
        self.cleardir(self.testdir)

    @classmethod
    def cleardir(self, dirname):
        """removes the given directory"""
        if os.path.exists(dirname):
            for dirpath, subdirs, filenames in os.walk(dirname, topdown=False):
                for name in filenames:
                    os.remove(os.path.join(dirpath, name))
                for name in subdirs:
                    os.rmdir(os.path.join(dirpath, name))
        if os.path.exists(dirname):
            os.rmdir(dirname)
        assert not os.path.exists(dirname)

    def test_getclass(self):
        assert classname("file.po") == "pofile"
        assert classname("file.pot") == "pofile"
        assert classname("file.dtd.po") == "pofile"

        assert classname("file.tmx") == "tmxfile"
        assert classname("file.af.tmx") == "tmxfile"
        assert classname("file.tbx") == "tbxfile"
        assert classname("file.po.xliff") == "xlifffile"

        assert not classname("file.po") == "tmxfile"
        assert not classname("file.po") == "xlifffile"

        assert classname("file.po.gz") == "pofile"
        assert classname("file.pot.gz") == "pofile"
        assert classname("file.dtd.po.gz") == "pofile"

        assert classname("file.tmx.gz") == "tmxfile"
        assert classname("file.af.tmx.gz") == "tmxfile"
        assert classname("file.tbx.gz") == "tbxfile"
        assert classname("file.po.xliff.gz") == "xlifffile"

        assert not classname("file.po.gz") == "tmxfile"
        assert not classname("file.po.gz") == "xlifffile"

        assert classname("file.po.bz2") == "pofile"
        assert classname("file.pot.bz2") == "pofile"
        assert classname("file.dtd.po.bz2") == "pofile"

        assert classname("file.tmx.bz2") == "tmxfile"
        assert classname("file.af.tmx.bz2") == "tmxfile"
        assert classname("file.tbx.bz2") == "tbxfile"
        assert classname("file.po.xliff.bz2") == "xlifffile"

        assert not classname("file.po.bz2") == "tmxfile"
        assert not classname("file.po.bz2") == "xlifffile"

    def test_getobject(self):
        """Tests that we get a valid object."""
        fileobj = givefile(self.filename, self.file_content)
        store = factory.getobject(fileobj)
        assert isinstance(store, self.expected_instance)

    def test_get_noname_object(self):
        """Tests that we get a valid object from a file object without a name."""
        fileobj = wStringIO.StringIO(self.file_content)
        assert not hasattr(fileobj, 'name')
        store = factory.getobject(fileobj)
        assert isinstance(store, self.expected_instance)

    def test_gzfile(self):
        """Test that we can open a gzip file correctly."""
        filename = os.path.join(self.testdir, self.filename + '.gz')
        gzfile = GzipFile(filename, mode="wb")
        gzfile.write(self.file_content)
        gzfile.close()
        store = factory.getobject(filename)
        assert isinstance(store, self.expected_instance)

    def test_bz2file(self):
        """Test that we can open a gzip file correctly."""
        if not BZ2File:
            return
        filename = os.path.join(self.testdir, self.filename + '.bz2')
        bz2file = BZ2File(filename, mode="wb")
        bz2file.write(self.file_content)
        bz2file.close()
        store = factory.getobject(filename)
        assert isinstance(store, self.expected_instance)

    def test_directory(self):
        """Test that a directory is correctly detected."""
        object = factory.getobject(self.testdir)
        assert isinstance(object, Directory)


class TestPOFactory(BaseTestFactory):
    from translate.storage import po
    expected_instance = po.pofile
    filename = 'dummy.po'
    file_content = '''#: test.c\nmsgid "test"\nmsgstr "rest"\n'''


class TestXliffFactory(BaseTestFactory):
    from translate.storage import xliff
    expected_instance = xliff.xlifffile
    filename = 'dummy.xliff'
    file_content = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
<file>
<body>
  <trans-unit>
    <source>test</source>
    <target>rest</target>
  </trans-unit>
</body>
</file>
</xliff>'''


class TestPOXliffFactory(BaseTestFactory):
    from translate.storage import poxliff
    expected_instance = poxliff.PoXliffFile
    filename = 'dummy.xliff'
    file_content = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
<file datatype="po" original="file.po" source-language="en-US"><body><trans-unit approved="no" id="1" restype="x-gettext-domain-header" xml:space="preserve">
<source>MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
</source>
<target>MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
</target>
</trans-unit></body></file></xliff>'''


class TestWordfastFactory(BaseTestFactory):
    from translate.storage import wordfast
    expected_instance = wordfast.WordfastTMFile
    filename = 'dummy.txt'
    file_content = ('''%20070801~103212	%User ID,S,S SMURRAY,SMS Samuel Murray-Smit,SM Samuel Murray-Smit,MW Mary White,DS Deepak Shota,MT! Machine translation (15),AL! Alignment (10),SM Samuel Murray,	%TU=00000075	%AF-ZA	%Wordfast TM v.5.51r/00	%EN-ZA	%---80597535	Subject (5),EL,EL Electronics,AC Accounting,LE Legal,ME Mechanics,MD Medical,LT Literary,AG Agriculture,CO Commercial	Client (5),LS,LS LionSoft Corp,ST SuperTron Inc,CA CompArt Ltd			'''
'''20070801~103248	SM	0	AF-ZA	Langeraad en duimpie	EN-ZA	Big Ben and Little John	EL	LS''')

########NEW FILE########
__FILENAME__ = test_html
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Tests for the HTML classes"""

from pytest import mark, raises

from translate.storage import base, html


def test_guess_encoding():
    """Read an encoding header to guess the encoding correctly"""
    h = html.htmlfile()
    assert h.guess_encoding('''<META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset=UTF-8">''') == "UTF-8"
    assert h.guess_encoding('''<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><!-- base href="http://home.online.no/~rut-aane/linux.html" --><link rel="shortcut icon" href="http://home.online.no/~rut-aane/peng16x16a.gif"><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"><meta name="Description" content="Linux newbie stuff and a little about Watching TV under Linux"><meta name="MSSmartTagsPreventParsing" content="TRUE"><meta name="GENERATOR" content="Mozilla/4.7 [en] (X11; I; Linux 2.2.5-15 i586) [Netscape]"><title>Some Linux for beginners</title><style type="text/css">''') == "iso-8859-1"


def test_strip_html():
    assert html.strip_html("<a>Something</a>") == "Something"
    assert html.strip_html("You are <a>Something</a>") == "You are <a>Something</a>"
    assert html.strip_html("<b>You</b> are <a>Something</a>") == "<b>You</b> are <a>Something</a>"
    assert html.strip_html('<strong><font class="headingwhite">Projects</font></strong>') == "Projects"
    assert html.strip_html("<strong>Something</strong> else.") == "<strong>Something</strong> else."
    assert html.strip_html("<h1><strong>Something</strong> else.</h1>") == "<strong>Something</strong> else."
    assert html.strip_html('<h1 id="moral"><strong>We believe</strong> that the internet should be public, open and accessible.</h1>') == "<strong>We believe</strong> that the internet should be public, open and accessible."
    #assert html.strip_html('<h3><a href="http://www.firefox.com/" class="producttitle"><img src="../images/product-firefox-50.png" width="50" height="50" alt="" class="featured" style="display: block; margin-bottom: 30px;" /><strong>Firefox for Desktop</strong></a></h3>') == 'Firefox for Desktop'


def test_strip_html_with_pi():
    h = html.htmlfile()
    assert html.strip_html(h.pi_escape('<a href="<?$var?>">Something</a>')) == "Something"
    assert html.strip_html(h.pi_escape('<a href="<?=($a < $b ? $foo : ($b > c ? $bar : $cat))?>">Something</a>')) == "Something"


def test_normalize_html():
    assert html.normalize_html("<p>Simple  double  spaced</p>") == "<p>Simple double spaced</p>"


def test_pi_escaping():
    h = html.htmlfile()
    assert h.pi_escape('<a href="<?=($a < $b ? $foo : ($b > c ? $bar : $cat))?>">') == '<a href="<?=($a %lt; $b ? $foo : ($b %gt; c ? $bar : $cat))?>">'


class TestHTMLParsing:

    h = html.htmlfile

    def test_mismatched_tags(self):
        assert raises(base.ParseError, self.h.parsestring, "<h3><p>Some text<p></h3>")
        # First <tr> is not closed
        assert raises(base.ParseError, self.h.parsestring, "<html><head></head><body><table><tr><th>Heading One</th><th>Heading Two</th><tr><td>One</td><td>Two</td></tr></table></body></html>")
        # <tr> is not closed in <thead>
        assert raises(base.ParseError, self.h.parsestring, """<table summary="This is the summary"><caption>A caption</caption><thead><tr><th abbr="Head 1">Heading One</th><th>Heading Two</th></thead><tfoot><tr><td>Foot One</td><td>Foot Two</td></tr></tfoot><tbody><tr><td>One</td><td>Two</td></tr></tbody></table>""")

    def test_self_closing_tags(self):
        h = html.htmlfile()
        store = h.parsestring("<h3>Some text <img><br><img></h3>")
        assert len(store.units) == 1

    @mark.xfail(reason="Not implemented")
    def test_escaping_script_and_pre(self):
        """<script> and <pre> can contain < and > and these should not be
        interpretted as tags"""
        h = html.htmlfile()
        store = h.parsestring("<p>We are here</p><script>Some </tag>like data<script></p>")
        print(store.units[0].source)
        assert len(store.units) == 1


class TestHTMLExtraction(object):

    h = html.htmlfile

    def test_extraction_tag_figcaption(self):
        """Check that we can extract figcaption"""
        h = html.htmlfile()
        # Example form http://www.w3schools.com/tags/tag_figcaption.asp
        store = h.parsestring("""
               <figure>
                   <img src="img_pulpit.jpg" alt="The Pulpit Rock" width="304" height="228">
                   <figcaption>Fig1. - A view of the pulpit rock in Norway.</figcaption>
               </figure>""")
        print(store.units[0].source)
        assert len(store.units) == 2
        assert store.units[0].source == "The Pulpit Rock"
        assert store.units[1].source == "Fig1. - A view of the pulpit rock in Norway."

    def test_extraction_tag_caption_td_th(self):
        """Check that we can extract table related translatable: th, td and caption"""
        h = html.htmlfile()
        # Example form http://www.w3schools.com/tags/tag_caption.asp
        store = h.parsestring("""
            <table>
                <caption>Monthly savings</caption>
                <tr>
                    <th>Month</th>
                    <th>Savings</th>
                </tr>
                <tr>
                    <td>January</td>
                    <td>$100</td>
                </tr>
            </table>""")
        print(store.units[0].source)
        assert len(store.units) == 5
        assert store.units[0].source == "Monthly savings"
        assert store.units[1].source == "Month"
        assert store.units[2].source == "Savings"
        assert store.units[3].source == "January"
        assert store.units[4].source == "$100"

    def test_extraction_attr_alt(self):
        """Check that we can extract title attribute"""
        h = html.htmlfile()
        # Example from http://www.netmechanic.com/news/vol6/html_no1.htm
        store = h.parsestring("""
            <img src="cafeteria.jpg" height="200" width="200" alt="UAHC campers enjoy a meal in the camp cafeteria">
        """)
        assert len(store.units) == 1
        assert store.units[0].source == "UAHC campers enjoy a meal in the camp cafeteria"


    def test_extraction_attr_title(self):
        """Check that we can extract title attribute"""
        h = html.htmlfile()

        # Example form http://www.w3schools.com/tags/att_global_title.asp
        store = h.parsestring("""
            <p><abbr title="World Health Organization">WHO</abbr> was founded in 1948.</p>
            <p title="Free Web tutorials">W3Schools.com</p>""")
        print(store.units[0].source)
        assert len(store.units) == 4
        assert store.units[0].source == "World Health Organization"
        # FIXME this is not ideal we need to either drop title= as we've
        # extracted it already or not extract it earlier
        assert store.units[1].source == '<abbr title="World Health Organization">WHO</abbr> was founded in 1948.'
        assert store.units[2].source == "Free Web tutorials"
        assert store.units[3].source == "W3Schools.com"

        # Example from http://www.netmechanic.com/news/vol6/html_no1.htm
        store = h.parsestring("""
            <table width="100" border="2" title="Henry Jacobs Camp summer 2003 schedule">
        """)
        assert len(store.units) == 1
        assert store.units[0].source == "Henry Jacobs Camp summer 2003 schedule"
        # FIXME this doesn't extract as I'd have expected
        #store = h.parsestring("""
        #    <a href="page1.html" title="HS Jacobs - a UAHC camp in Utica, MS">Henry S. Jacobs Camp</a>
        #""")
        #assert len(store.units) == 2
        #assert store.units[0].source == "HS Jacobs - a UAHC camp in Utica, MS"
        #assert store.units[1].source == "Henry S. Jacobs Camp"
        store = h.parsestring("""
            <form name="application" title="Henry Jacobs camper application" method="  " action="  ">
        """)
        assert len(store.units) == 1
        assert store.units[0].source == "Henry Jacobs camper application"

########NEW FILE########
__FILENAME__ = test_mo
#!/usr/bin/env python

import os
import subprocess
import sys
from cStringIO import StringIO

from translate.storage import factory, mo, test_base


# get directory of this test
dir = os.path.dirname(os.path.abspath(__file__))
# get top-level directory (moral equivalent of ../..)
dir = os.path.dirname(os.path.dirname(dir))
# load python modules from top-level
sys.path.insert(0, dir)
# add top-level to PYTHONPATH for subprocesses
os.environ["PYTHONPATH"] = os.pathsep.join(sys.path)
# add {top-level}/translate/tools to PATH for pocompile
os.environ["PATH"] = os.pathsep.join([os.path.join(dir, "translate", "tools"),
                                      os.environ["PATH"]])


class TestMOUnit(test_base.TestTranslationUnit):
    UnitClass = mo.mounit

posources = [
r'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"
''',
r'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "plant"
msgstr ""
''',
# The following test is commented out, because the hash-size is different
# compared to gettext, since we're not counting untranslated units.
#r'''
#msgid ""
#msgstr ""
#"PO-Revision-Date: 2006-02-09 23:33+0200\n"
#"MIME-Version: 1.0\n"
#"Content-Type: text/plain; charset=UTF-8\n"
#"Content-Transfer-Encoding: 8-bit\n"
#
#msgid "plant"
#msgstr ""
#
#msgid ""
#"_: Noun\n"
#"convert"
#msgstr "bekeerling"
#''',
r'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "plant"
msgstr ""

msgid ""
"_: Noun\n"
"convert"
msgstr "bekeerling"

msgctxt "verb"
msgid ""
"convert"
msgstr "omskakel"
''',
r'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "plant"
msgstr ""

msgid ""
"_: Noun\n"
"convert"
msgstr "bekeerling"

msgctxt "verb"
msgid ""
"convert"
msgstr "omskakel"

msgid "tree"
msgid_plural "trees"
msgstr[0] ""
''']


class TestMOFile(test_base.TestTranslationStore):
    StoreClass = mo.mofile

    def get_mo_and_po(self):
        return (os.path.abspath(self.filename + '.po'),
                os.path.abspath(self.filename + '.msgfmt.mo'),
                os.path.abspath(self.filename + '.pocompile.mo'))

    def remove_po_and_mo(self):
        for file in self.get_mo_and_po():
            if os.path.exists(file):
                os.remove(file)

    def setup_method(self, method):
        test_base.TestTranslationStore.setup_method(self, method)
        self.remove_po_and_mo()

    def teardown_method(self, method):
        test_base.TestTranslationStore.teardown_method(self, method)
        self.remove_po_and_mo()

    def test_language(self):
        """Test that we can return the target language correctly."""
        store = self.StoreClass()
        store.updateheader(add=True, Language="zu")
        assert store.gettargetlanguage() == "zu"

    def test_output(self):
        for posource in posources:
            print("PO source file")
            print(posource)
            PO_FILE, MO_MSGFMT, MO_POCOMPILE = self.get_mo_and_po()

            out_file = open(PO_FILE, 'w')
            out_file.write(posource)
            out_file.close()

            subprocess.call(['msgfmt', PO_FILE, '-o', MO_MSGFMT])
            subprocess.call(['pocompile', '--errorlevel=traceback', PO_FILE, MO_POCOMPILE])

            store = factory.getobject(StringIO(posource))
            if store.isempty() and not os.path.exists(MO_POCOMPILE):
                # pocompile doesn't create MO files for empty PO files, so we
                # can skip the checks here.
                continue

            mo_msgfmt_f = open(MO_MSGFMT)
            mo_pocompile_f = open(MO_POCOMPILE)

            try:
                mo_msgfmt = mo_msgfmt_f.read()
                print("msgfmt output:")
                print(repr(mo_msgfmt))
                mo_pocompile = mo_pocompile_f.read()
                print("pocompile output:")
                print(repr(mo_pocompile))
                assert mo_msgfmt == mo_pocompile
            finally:
                mo_msgfmt_f.close()
                mo_pocompile_f.close()

########NEW FILE########
__FILENAME__ = test_monolingual
#!/usr/bin/env python
#
# -*- coding: utf-8 -*-
#
# These test classes should be used as super class of test classes for the
# classes that doesn't support the target property

from translate.storage import base, test_base


class TestMonolingualUnit(test_base.TestTranslationUnit):
    UnitClass = base.TranslationUnit

    def test_target(self):
        pass

    def test_rich_get(self):
        pass

    def test_rich_set(self):
        pass


class TestMonolingualStore(test_base.TestTranslationStore):
    StoreClass = base.TranslationStore

    def test_translate(self):
        pass

    def test_markup(self):
        pass

    def test_nonascii(self):
        pass

    def check_equality(self, store1, store2):
        """Check that store1 and store2 are the same."""
        assert len(store1.units) == len(store2.units)

        for n, store1unit in enumerate(store1.units):
            store2unit = store2.units[n]

            if str(store1unit) != str(store2unit):
                print(("match failed between elements %d of %d" % ((n + 1), len(store1.units))))
                print("store1:")
                print((str(store1)))
                print("store2:")
                print((str(store2)))
                print(("store1.units[%d].__dict__:" % n, store1unit.__dict__))
                print(("store2.units[%d].__dict__:" % n, store2unit.__dict__))
                assert str(store1unit) == str(store2unit)

########NEW FILE########
__FILENAME__ = test_mozilla_lang
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.storage import mozilla_lang, test_base


class TestMozLangUnit(test_base.TestTranslationUnit):
    UnitClass = mozilla_lang.LangUnit

    def test_translate_but_same(self):
        """Mozilla allows {ok} to indicate a line that is the
        same in source and target on purpose"""
        unit = self.UnitClass("Open")
        unit.target = "Open"
        assert unit.target == "Open"
        assert str(unit).endswith(" {ok}")

    def test_untranslated(self):
        """The target is always written to files and is never blank. If it is
        truly untranslated then it won't end with '{ok}."""
        unit = self.UnitClass("Open")
        assert unit.target is None
        assert str(unit).find("Open") == 1
        assert str(unit).find("Open", 2) == 6
        assert not str(unit).endswith(" {ok}")

        unit = self.UnitClass("Closed")
        unit.target = ""
        assert unit.target == ""
        assert str(unit).find("Closed") == 1
        assert str(unit).find("Closed", 2) == 8
        assert not str(unit).endswith(" {ok}")

    def test_comments(self):
        """Comments start with #."""
        unit = self.UnitClass("One")
        unit.addnote("Hello")
        assert str(unit).find("Hello") == 2
        assert str(unit).find("# Hello") == 0


class TestMozLangFile(test_base.TestTranslationStore):
    StoreClass = mozilla_lang.LangStore

    def test_nonascii(self):
        # FIXME investigate why this doesn't pass or why we even do this
        # text with UTF-8 encoded strings
        pass

    def test_format_layout(self):
        """General test of layout of the format"""
        lang = ("# Comment\n"
                ";Source\n"
                "Target\n")
        store = self.StoreClass.parsestring(lang)
        store.mark_active = False
        unit = store.units[0]
        assert unit.source == "Source"
        assert unit.target == "Target"
        assert "Comment" in unit.getnotes()
        assert str(store) == lang

    def test_active_flag(self):
        """Test the ## active ## flag"""
        lang = ("## active ##\n"
                ";Source\n"
                "Target\n")
        store = self.StoreClass.parsestring(lang)
        assert store.is_active
        assert str(store) == lang

    def test_multiline_comments(self):
        """Ensure we can handle and preserve miltiline comments"""
        lang = ("## active ##\n"
                "# First comment\n"
                "# Second comment\n"
                "# Third comment\n"
                ";Source\n"
                "Target\n")
        store = self.StoreClass.parsestring(lang)
        assert str(store) == lang

    def test_template(self):
        """A template should have source == target, though it could be blank"""
        lang = (";Source\n"
                "Source\n")
        store = self.StoreClass.parsestring(lang)
        unit = store.units[0]
        assert unit.source == "Source"
        assert unit.target == ""
        assert str(store) == lang
        lang2 = (";Source\n"
                "\n"
                ";Source2\n")
        store2 = self.StoreClass.parsestring(lang2)
        assert store2.units[0].source == "Source"
        assert store2.units[0].target == ""
        assert store2.units[1].source == "Source2"
        assert store2.units[1].target == ""

########NEW FILE########
__FILENAME__ = test_omegat
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.storage import omegat as ot, test_base


class TestOtUnit(test_base.TestTranslationUnit):
    UnitClass = ot.OmegaTUnit


class TestOtFile(test_base.TestTranslationStore):
    StoreClass = ot.OmegaTFile

    @mark.xfail(reason="This doesn't work, due to two store classes handling different "
                       "extensions, but factory listing it as one supported file type")
    def test_extensions(self):
        assert False

########NEW FILE########
__FILENAME__ = test_oo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import warnings

from translate.misc import wStringIO
from translate.storage import oo


def test_makekey():
    """checks the makekey function for consistency"""
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', 'resourcetype', 'GROUP_ID', 'LOCAL_ID', 'platform'), False) == "sourcefile.src#GROUP_ID.LOCAL_ID.resourcetype"
    # Testwith long_key i.e. used in multifile options
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', 'resourcetype', 'GROUP_ID', 'LOCAL_ID', 'platform'), True) == "project/path/to/the/sourcefile.src#GROUP_ID.LOCAL_ID.resourcetype"
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', 'resourcetype', 'GROUP_ID', '', 'platform'), False) == "sourcefile.src#GROUP_ID.resourcetype"
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', 'resourcetype', '', 'LOCAL_ID', 'platform'), False) == "sourcefile.src#LOCAL_ID.resourcetype"
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', '', 'GROUP_ID', 'LOCAL_ID', 'platform'), False) == "sourcefile.src#GROUP_ID.LOCAL_ID"
    assert oo.makekey(('project', r'path\to\the\sourcefile.src', '', 'GROUP_ID', '', 'platform'), False) == "sourcefile.src#GROUP_ID"


def test_escape_help_text():
    """Check the help text escape function"""
    assert oo.escape_help_text("If we don't know <tag> we don't <br> escape it") == "If we don't know <tag> we don't <br> escape it"
    # Bug 694
    assert oo.escape_help_text("A sz: <nyelv>") == "A sz: <nyelv>"
    assert oo.escape_help_text("""...kvetkez: "<kiszolgl> <tmakr> <elem>", ahol...""") == """...kvetkez: "<kiszolgl> <tmakr> <elem>", ahol..."""
    # See bug 694 comments 8-10 not fully resolved.
    assert oo.escape_help_text(r"...trtjel (\) ltrehozshoz...") == r"...trtjel (\\) ltrehozshoz..."


class TestOO:

    def setup_method(self, method):
        warnings.resetwarnings()

    def teardown_method(self, method):
        warnings.resetwarnings()

    def ooparse(self, oosource):
        """helper that parses oo source without requiring files"""
        dummyfile = wStringIO.StringIO(oosource)
        oofile = oo.oofile(dummyfile)
        return oofile

    def ooregen(self, oosource):
        """helper that converts oo source to oofile object and back"""
        return str(self.ooparse(oosource))

    def test_simpleentry(self):
        """checks that a simple oo entry is parsed correctly"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	Character				20050924 09:13:58'
        oofile = self.ooparse(oosource)
        assert len(oofile.units) == 1
        oe = oofile.units[0]
        assert oe.languages.keys() == ["en-US"]
        ol = oofile.oolines[0]
        assert ol.getkey() == ('svx', r'source\dialog\numpages.src', 'string', 'RID_SVXPAGE_NUM_OPTIONS', 'STR_BULLET', '')
        assert ol.text == 'Character'
        assert str(ol) == oosource

    def test_simpleentry_quickhelptest(self):
        """checks that a simple entry with quickhelptext is parsed correctly"""
        oosource = r'sd	source\ui\dlg\sdobjpal.src	0	imagebutton	FLTWIN_SDOBJPALETTE	BTN_SYMSIZE			16	en-US	-		Toggle Symbol Size		20051017 21:40:56'
        oofile = self.ooparse(oosource)
        assert len(oofile.units) == 1
        oe = oofile.units[0]
        assert oe.languages.keys() == ["en-US"]
        ol = oofile.oolines[0]
        assert ol.getkey() == ('sd', r'source\ui\dlg\sdobjpal.src', 'imagebutton', 'FLTWIN_SDOBJPALETTE', 'BTN_SYMSIZE', '')
        assert ol.quickhelptext == 'Toggle Symbol Size'
        assert str(ol) == oosource

    def test_simpleentry_title(self):
        """checks that a simple entry with title text is parsed correctly"""
        oosource = r'dbaccess	source\ui\dlg\indexdialog.src	0	querybox	QUERY_SAVE_CURRENT_INDEX				0	en-US	Do you want to save the changes made to the current index?			Exit Index Design	20051017 21:40:56'
        oofile = self.ooparse(oosource)
        assert len(oofile.units) == 1
        oe = oofile.units[0]
        assert oe.languages.keys() == ["en-US"]
        ol = oofile.oolines[0]
        assert ol.getkey() == ('dbaccess', r'source\ui\dlg\indexdialog.src', 'querybox', 'QUERY_SAVE_CURRENT_INDEX', '', '')
        assert ol.title == 'Exit Index Design'
        assert str(ol) == oosource

    def test_blankline(self):
        """checks that a blank line is parsed correctly"""
        oosource = '\n'
        warnings.simplefilter("error")
        oofile = self.ooparse(oosource)
        assert len(oofile.units) == 0

    def test_fieldlength(self):
        """checks that we process the length field correctly"""
        # Since the actual field is 18 characters long and the field width in this example is 16 we're not sure if they even use this!
        oosource = r'sd	source\ui\dlg\sdobjpal.src	0	imagebutton	FLTWIN_SDOBJPALETTE	BTN_SYMSIZE			16	en-US	-		Toggle Symbol Size		20051017 21:40:56'
        oofile = self.ooparse(oosource)
        assert len(oofile.units) == 1
        oe = oofile.units[0]
        assert oe.languages.keys() == ["en-US"]
        ol = oofile.oolines[0]
        assert int(ol.width) == 16

    def test_escapes(self):
        """checks that we escape properly"""
        oosource = r'svx	source\dialog\numpages.src	0	string	RID_SVXPAGE_NUM_OPTIONS	STR_BULLET			0	en-US	size *2 \\langle x \\rangle				20050924 09:13:58'
        oofile = self.ooregen(oosource)
        assert r'size *2 \\langle x \\rangle' in oofile

########NEW FILE########
__FILENAME__ = test_php
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark

from translate.misc import wStringIO
from translate.storage import php, test_monolingual


def test_php_escaping_single_quote():
    """Test the helper escaping funtions for 'single quotes'

    The tests are built mostly from examples from the PHP
    `string type definition <http://www.php.net/manual/en/language.types.string.php#language.types.string.syntax.single>`_.
    """
    # Decoding - PHP -> Python
    assert php.phpdecode(r"\'") == r"'"     # To specify a literal single quote, escape it with a backslash (\).
    assert php.phpdecode(r'"') == r'"'
    assert php.phpdecode(r"\\'") == r"\'"   # To specify a literal backslash before a single quote, or at the end of the string, double it (\\)
    assert php.phpdecode(r"\x") == r"\x"    # Note that attempting to escape any other character will print the backslash too.
    assert php.phpdecode(r'\t') == r'\t'
    assert php.phpdecode(r'\n') == r'\n'
    assert php.phpdecode(r"this is a simple string") == r"this is a simple string"
    assert php.phpdecode("""You can also have embedded newlines in
strings this way as it is
okay to do""") == """You can also have embedded newlines in
strings this way as it is
okay to do"""
    assert php.phpdecode(r"This will not expand: \n a newline") == r"This will not expand: \n a newline"
    assert php.phpdecode(r'Arnold once said: "I\'ll be back"') == r'''Arnold once said: "I'll be back"'''
    assert php.phpdecode(r'You deleted C:\\*.*?') == r"You deleted C:\*.*?"
    assert php.phpdecode(r'You deleted C:\*.*?') == r"You deleted C:\*.*?"
    assert php.phpdecode(r'\117\143\164\141\154') == r'\117\143\164\141\154'       # We don't handle Octal like " does
    assert php.phpdecode(r'\x48\x65\x78') == r'\x48\x65\x78'                       # Don't handle Hex either
    # Should implement for false interpretation of double quoted data.
    # Encoding - Python -> PHP
    assert php.phpencode(r"'") == r"\'"     # To specify a literal single quote, escape it with a backslash (\).
    assert php.phpencode(r"\'") == r"\\'"   # To specify a literal backslash before a single quote, or at the end of the string, double it (\\)
    assert php.phpencode(r'"') == r'"'
    assert php.phpencode(r"\x") == r"\x"    # Note that attempting to escape any other character will print the backslash too.
    assert php.phpencode(r"\t") == r"\t"
    assert php.phpencode(r"\n") == r"\n"
    assert php.phpencode(r"""String with
newline""") == r"""String with
newline"""
    assert php.phpencode(r"This will not expand: \n a newline") == r"This will not expand: \n a newline"
    assert php.phpencode(r'''Arnold once said: "I'll be back"''') == r'''Arnold once said: "I\'ll be back"'''
    assert php.phpencode(r'You deleted C:\*.*?') == r"You deleted C:\*.*?"


def test_php_escaping_double_quote():
    """Test the helper escaping funtions for 'double quotes'"""
    # Decoding - PHP -> Python
    assert php.phpdecode("'", quotechar='"') == "'"         # we do nothing with single quotes
    assert php.phpdecode(r"\n", quotechar='"') == "\n"      # See table of escaped characters
    assert php.phpdecode(r"\r", quotechar='"') == "\r"      # See table of escaped characters
    assert php.phpdecode(r"\t", quotechar='"') == "\t"      # See table of escaped characters
    assert php.phpdecode(r"\v", quotechar='"') == "\v"      # See table of escaped characters
    assert php.phpdecode(r"\f", quotechar='"') == "\f"      # See table of escaped characters
    assert php.phpdecode(r"\\", quotechar='"') == "\\"      # See table of escaped characters
    #assert php.phpdecode(r"\$", quotechar='"') == "$"      # See table of escaped characters - this may cause confusion with actual variables in roundtripping
    assert php.phpdecode(r"\$", quotechar='"') == "\\$"     # Just to check that we don't unescape this
    assert php.phpdecode(r'\"', quotechar='"') == '"'       # See table of escaped characters
    assert php.phpdecode(r'\117\143\164\141\154', quotechar='"') == 'Octal'       # Octal: \[0-7]{1,3}
    assert php.phpdecode(r'\x48\x65\x78', quotechar='"') == 'Hex'                 # Hex: \x[0-9A-Fa-f]{1,2}
    assert php.phpdecode(r'\117\\c\164\141\154', quotechar='"') == 'O\ctal'  # Mixed
    # Decoding - special examples
    assert php.phpdecode(r"Don't escape me here\'s", quotechar='"') == r"Don't escape me here\'s"  # See bug #589
    assert php.phpdecode("Line1\nLine2") == "Line1\nLine2"      # Preserve newlines in multiline messages
    assert php.phpdecode("Line1\r\nLine2") == "Line1\r\nLine2"  # DOS PHP files
    # Encoding - Python -> PHP
    assert php.phpencode("'", quotechar='"') == "'"
    assert php.phpencode("\n", quotechar='"') == "\n"       # See table of escaped characters - we leave newlines unescaped so that we can try best to preserve pretty printing. See bug 588
    assert php.phpencode("\r", quotechar='"') == r"\r"      # See table of escaped characters
    assert php.phpencode("\t", quotechar='"') == r"\t"      # See table of escaped characters
    assert php.phpencode("\v", quotechar='"') == r"\v"      # See table of escaped characters
    assert php.phpencode("\f", quotechar='"') == r"\f"      # See table of escaped characters
    assert php.phpencode(r"\\", quotechar='"') == r"\\"      # See table of escaped characters
    #assert php.phpencode("\$", quotechar='"') == "$"      # See table of escaped characters - this may cause confusion with actual variables in roundtripping
    assert php.phpencode("\$", quotechar='"') == r"\$"     # Just to check that we don't unescape this
    assert php.phpencode('"', quotechar='"') == r'\"'
    assert php.phpencode(r"Don't escape me here\'s", quotechar='"') == r"Don't escape me here\'s"  # See bug #589


class TestPhpUnit(test_monolingual.TestMonolingualUnit):
    UnitClass = php.phpunit

    def test_difficult_escapes(self):
        pass


class TestPhpFile(test_monolingual.TestMonolingualStore):
    StoreClass = php.phpfile

    def phpparse(self, phpsource):
        """helper that parses php source without requiring files"""
        dummyfile = wStringIO.StringIO(phpsource)
        phpfile = php.phpfile(dummyfile)
        return phpfile

    def phpregen(self, phpsource):
        """helper that converts php source to phpfile object and back"""
        return str(self.phpparse(phpsource))

    def test_simpledefinition(self):
        """checks that a simple php definition is parsed correctly"""
        phpsource = """$lang['mediaselect'] = 'Bestand selectie';"""
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang['mediaselect']"
        assert phpunit.source == "Bestand selectie"

    def test_simpledefinition_source(self):
        """checks that a simple php definition can be regenerated as source"""
        phpsource = """$lang['mediaselect']='Bestand selectie';"""
        phpregen = self.phpregen(phpsource)
        assert phpsource + '\n' == phpregen

    def test_spaces_in_name(self):
        """check that spaces in the array name doesn't throw us off"""
        phpsource = """$lang[ 'mediaselect' ] = 'Bestand selectie';"""
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang[ 'mediaselect' ]"
        assert phpunit.source == "Bestand selectie"

    def test_comment_definition(self):
        """check that comments are fully preserved"""
        phpsource = """/*
 * Comment line 1
 * Comment line 2
 */
$foo = "bar";
"""
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert phpunit.name == "$foo"
        assert phpunit.source == "bar"
        assert phpunit._comments == ["""/*""",
                                     """ * Comment line 1""",
                                     """ * Comment line 2""",
                                     """ */"""]

    def test_comment_blocks(self):
        """check that we don't process name value pairs in comment blocks"""
        phpsource = """/*
 * $lang[0] = "Blah";
 * $lang[1] = "Bluh";
 */
$lang[2] = "Yeah";
"""
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang[2]"
        assert phpunit.source == "Yeah"

    def test_comment_output(self):
        """check that linebreaks and spacing is preserved when comments are output"""
        # php.py uses single quotes and doesn't add spaces before or after '='
        phpsource = """/*
 * Comment line 1
 * Comment line 2
 */
$foo='bar';
"""
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert str(phpunit) == phpsource

    def test_multiline(self):
        """check that we preserve newlines in a multiline message"""
        phpsource = """$lang['multiline'] = "Line1%sLine2";"""
        # Try DOS and Unix and make sure the output has the same
        for lineending in ("\n", "\r\n"):
            phpfile = self.phpparse(phpsource % lineending)
            assert len(phpfile.units) == 1
            phpunit = phpfile.units[0]
            assert phpunit.name == "$lang['multiline']"
            assert phpunit.source == "Line1%sLine2" % lineending

    def test_parsing_arrays(self):
        """parse the array syntax"""
        phpsource = '''$lang = %s(
         'item1' => 'value1',
         'item2' => 'value2',
      );'''
        for arrayfn in ['array', 'Array', 'ARRAY']:
            phpfile = self.phpparse(phpsource % arrayfn)
            assert len(phpfile.units) == 2
            phpunit = phpfile.units[0]
            assert phpunit.name == "$lang->'item1'"
            assert phpunit.source == "value1"

    def test_parsing_array_no_array_syntax(self):
        """parse the array syntax"""
        phpsource = '''global $_LANGPDF;
        $_LANGPDF = array();
        $_LANGPDF['PDF065ab3a28ca4f16f55f103adc7d0226f'] = 'Delivery';
        '''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 1
        phpunit = phpfile.units[0]
        assert phpunit.name == "$_LANGPDF['PDF065ab3a28ca4f16f55f103adc7d0226f']"
        assert phpunit.source == "Delivery"

    def test_parsing_arrays_keys_with_spaces(self):
        """Ensure that our identifiers can have spaces. Bug #1683"""
        phpsource = '''$lang = array(
         'item 1' => 'value1',
         'item 2' => 'value2',
      );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item 1'"
        assert phpunit.source == "value1"

    def test_parsing_arrays_non_textual(self):
        """Don't break on non-textual data. Bug #1684"""
        phpsource = '''$lang = array(
         'item 1' => 'value1',
         'item 2' => false,
         'item 3' => 'value3',
      );'''
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'item 3'"
        assert phpunit.source == "value3"

    def test_parsing_simple_define(self):
        """Parse simple define syntax"""
        phpsource = """define("_FINISH", "Rematar");
define('_POSTEDON', 'Enviado o');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_POSTEDON'"
        assert phpunit.source == "Enviado o"

    def test_parsing_simple_define_with_spaces_before_key(self):
        """Parse simple define syntax with spaces before key"""
        phpsource = """define( "_FINISH", "Rematar");
define( '_CM_POSTED', 'Enviado');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define( "_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define( '_CM_POSTED'"
        assert phpunit.source == "Enviado"

    def test_parsing_define_spaces_after_equal_delimiter(self):
        """Parse define syntax with spaces after the equal delimiter"""
        phpsource = """define("_RELOAD",       "Recargar");
define('_CM_POSTED',    'Enviado');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_RELOAD"'
        assert phpunit.source == "Recargar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_CM_POSTED'"
        assert phpunit.source == "Enviado"

    def test_parsing_define_spaces_after_equal_delimiter_and_before_key(self):
        """Parse define syntax with spaces after the equal delimiter as well
        before the key
        """
        phpsource = """define( "_FINISH",       "Rematar");
define(  '_UPGRADE_CHARSET',    'Upgrade charset');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define( "_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define(  '_UPGRADE_CHARSET'"
        assert phpunit.source == "Upgrade charset"

    def test_parsing_define_no_spaces_after_equal_delimiter(self):
        """Parse define syntax without spaces after the equal delimiter"""
        phpsource = """define("_POSTEDON","Enviado o");
define('_UPGRADE_CHARSET','Upgrade charset');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_POSTEDON"'
        assert phpunit.source == "Enviado o"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_UPGRADE_CHARSET'"
        assert phpunit.source == "Upgrade charset"

    def test_parsing_define_no_spaces_after_equaldel_but_before_key(self):
        """Parse define syntax without spaces after the equal delimiter but
        with spaces before the key
        """
        phpsource = """define( "_FINISH","Rematar");
define( '_CM_POSTED','Enviado');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define( "_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "define( '_CM_POSTED'"
        assert phpunit.source == "Enviado"

    def test_parsing_define_entries_with_quotes(self):
        """Parse define syntax for entries with quotes"""
        phpsource = """define('_SETTINGS_COOKIEPREFIX', 'Prefixo da "cookie"');
define('_YOUR_USERNAME', 'O seu nome de usuario: "cookie"');
define("_REGISTER", "Register <a href=\"register.php\">here</a>");"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == "define('_SETTINGS_COOKIEPREFIX'"
        assert phpunit.source == "Prefixo da \"cookie\""
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_YOUR_USERNAME'"
        assert phpunit.source == "O seu nome de usuario: \"cookie\""
        phpunit = phpfile.units[2]
        assert phpunit.name == 'define("_REGISTER"'
        assert phpunit.source == "Register <a href=\"register.php\">here</a>"

    def test_parsing_define_comments_at_entry_line_end(self):
        """Parse define syntax with comments at the end of the entry line"""
        phpsource = """define("_POSTEDON", "Enviado o");// Keep this short
define('_CM_POSTED', 'Enviado'); // Posted date"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_POSTEDON"'
        assert phpunit.source == "Enviado o"
        assert phpunit._comments == ["Keep this short"]
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_CM_POSTED'"
        assert phpunit.source == "Enviado"
        assert phpunit._comments == ["Posted date"]

    def test_parsing_define_double_slash_comments_before_entries(self):
        """Parse define syntax with double slash comments before the entries"""
        phpsource = """// Keep this short
define("_FINISH", "Rematar");

// This means it was published
// It appears besides posts
define('_CM_POSTED', 'Enviado');"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_FINISH"'
        assert phpunit.source == "Rematar"
        assert phpunit._comments == ["// Keep this short"]
        phpunit = phpfile.units[1]
        assert phpunit.name == "define('_CM_POSTED'"
        assert phpunit.source == "Enviado"
        assert phpunit._comments == ["// This means it was published",
                                     "// It appears besides posts"]

    def test_parsing_define_spaces_before_end_delimiter(self):
        """Parse define syntax with spaces before the end delimiter"""
        phpsource = """define("_POSTEDON", "Enviado o");
define("_FINISH", "Rematar"     );
define("_RELOAD", "Recargar");"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_POSTEDON"'
        assert phpunit.source == "Enviado o"
        phpunit = phpfile.units[1]
        assert phpunit.name == 'define("_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[2]
        assert phpunit.name == 'define("_RELOAD"'
        assert phpunit.source == "Recargar"

    def test_parsing_simpledefinition_spaces_before_end_delimiter(self):
        """Parse simple definition syntax with spaces before the end
        delimiter"""
        phpsource = """$month_jan = 'Jan';
$month_feb = 'Feb'  ;
$month_mar = 'Mar';"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == '$month_jan'
        assert phpunit.source == "Jan"
        phpunit = phpfile.units[1]
        assert phpunit.name == '$month_feb'
        assert phpunit.source == "Feb"
        phpunit = phpfile.units[2]
        assert phpunit.name == '$month_mar'
        assert phpunit.source == "Mar"

    @mark.xfail(reason="Bug #1685")
    def test_parsing_arrays_no_trailing_comma(self):
        """parse the array syntax where we don't have a trailing comma.
        Bug #1685"""
        phpsource = '''$lang = array(
         'item1' => 'value1',
         'item2' => 'value2'
      );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item1'"
        assert phpunit.source == "value1"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'item2'"
        assert phpunit.source == "value2"

    def test_parsing_arrays_space_before_comma(self):
        """parse the array syntax with spaces before the comma. Bug #1898"""
        phpsource = '''$lang = array(
         'item1' => 'value1',
         'item2' => 'value2' ,
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item1'"
        assert phpunit.source == "value1"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'item2'"
        assert phpunit.source == "value2"

    def test_parsing_arrays_with_space_before_array_declaration(self):
        """parse the array syntax with spaces before the array declaration.
        Bug #2646"""
        phpsource = '''$lang = array   (
         'item1' => 'value1',
         'item2' => 'value2',
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item1'"
        assert phpunit.source == "value1"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'item2'"
        assert phpunit.source == "value2"

    def test_parsing_nested_arrays(self):
        """parse the nested array syntax. Bug #2240"""
        phpsource = '''$app_list_strings = array(
            'Mailbox' => 'Mailbox',
            'moduleList' => array(
                'Home' => 'Home',
                'Contacts' => 'Contacts',
                'Accounts' => 'Accounts',
            ),
            'FAQ' => 'FAQ',
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 5
        phpunit = phpfile.units[0]
        assert phpunit.name == "$app_list_strings->'Mailbox'"
        assert phpunit.source == "Mailbox"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Home'"
        assert phpunit.source == "Home"
        phpunit = phpfile.units[2]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Contacts'"
        assert phpunit.source == "Contacts"
        phpunit = phpfile.units[3]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Accounts'"
        assert phpunit.source == "Accounts"
        phpunit = phpfile.units[4]
        assert phpunit.name == "$app_list_strings->'FAQ'"
        assert phpunit.source == "FAQ"

    def test_parsing_nested_arrays_with_space_before_array_declaration(self):
        """parse the nested array syntax with whitespace before the array
        declaration."""
        phpsource = '''$app_list_strings = array  (
            'Mailbox' => 'Mailbox',
            'moduleList' => array  (
                'Home' => 'Home',
                'Contacts' => 'Contacts',
                'Accounts' => 'Accounts',
            ),
            'FAQ' => 'FAQ',
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 5
        phpunit = phpfile.units[0]
        assert phpunit.name == "$app_list_strings->'Mailbox'"
        assert phpunit.source == "Mailbox"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Home'"
        assert phpunit.source == "Home"
        phpunit = phpfile.units[2]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Contacts'"
        assert phpunit.source == "Contacts"
        phpunit = phpfile.units[3]
        assert phpunit.name == "$app_list_strings->'moduleList'->'Accounts'"
        assert phpunit.source == "Accounts"
        phpunit = phpfile.units[4]
        assert phpunit.name == "$app_list_strings->'FAQ'"
        assert phpunit.source == "FAQ"

    @mark.xfail(reason="Bug #2647")
    def test_parsing_nested_arrays_with_array_declaration_in_next_line(self):
        """parse the nested array syntax with array declaration in the next
        line. Bug #2647"""
        phpsource = '''$lang = array(
            'item1' => 'value1',
            'newsletter_frequency_dom' =>
                array(
                    'Weekly' => 'Weekly',
                ),
            'item2' => 'value2',
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item1'"
        assert phpunit.source == "value1"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'newsletter_frequency_dom'->'Weekly'"
        assert phpunit.source == "Weekly"
        phpunit = phpfile.units[2]
        assert phpunit.name == "$lang->'item2'"
        assert phpunit.source == "value2"

    @mark.xfail(reason="Bug #2648")
    def test_parsing_nested_arrays_with_blank_entries(self):
        """parse the nested array syntax with blank entries. Bug #2648"""
        phpsource = '''$lang = array(
            'item1' => 'value1',
            'newsletter_frequency_dom' =>
                array(
                    '' => '',
                    'Weekly' => 'Weekly',
                ),
            'item2' => 'value2',
        );'''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == "$lang->'item1'"
        assert phpunit.source == "value1"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang->'newsletter_frequency_dom'->'Weekly'"
        assert phpunit.source == "Weekly"
        phpunit = phpfile.units[2]
        assert phpunit.name == "$lang->'item2'"
        assert phpunit.source == "value2"

    @mark.xfail(reason="Bug #2611")
    def test_parsing_simple_heredoc_syntax(self):
        """parse the heredoc syntax. Bug #2611"""
        phpsource = '''$month_jan = 'Jan';
$lang_register_approve_email = <<<EOT
A new user with the username "{USER_NAME}" has registered in your gallery.

In order to activate the account, you need to click on the link below.

<a href="{ACT_LINK}">{ACT_LINK}</a>
EOT;

$foobar = <<<FOOBAR
Simple example
FOOBAR;

$month_mar = 'Mar';
        '''
        phpfile = self.phpparse(phpsource)
        assert len(phpfile.units) == 3
        phpunit = phpfile.units[0]
        assert phpunit.name == '$month_jan'
        assert phpunit.source == "Jan"
        phpunit = phpfile.units[1]
        assert phpunit.name == '$lang_register_approve_email'
        assert phpunit.source == "A new user with the username \"{USER_NAME}\" has registered in your gallery.\n\nIn order to activate the account, you need to click on the link below.\n\n<a href=\"{ACT_LINK}\">{ACT_LINK}</a>"
        phpunit = phpfile.units[2]
        assert phpunit.name == '$foobar'
        assert phpunit.source == "Simple example"
        phpunit = phpfile.units[3]
        assert phpunit.name == '$month_mar'
        assert phpunit.source == "Mar"

    def test_simpledefinition_after_define(self):
        """Check that a simple definition after define is parsed correctly."""
        phpsource = """define("_FINISH", "Rematar");
$lang['mediaselect'] = 'Bestand selectie';"""
        phpfile = self.phpparse(phpsource)
        print(len(phpfile.units))
        assert len(phpfile.units) == 2
        phpunit = phpfile.units[0]
        assert phpunit.name == 'define("_FINISH"'
        assert phpunit.source == "Rematar"
        phpunit = phpfile.units[1]
        assert phpunit.name == "$lang['mediaselect']"
        assert phpunit.source == "Bestand selectie"

########NEW FILE########
__FILENAME__ = test_po
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import mark, raises

from translate.misc import wStringIO
from translate.misc.multistring import multistring
from translate.storage import po, pypo, test_base


def test_roundtrip_quoting():
    specials = ['Fish & chips', 'five < six', 'six > five',
                'Use &nbsp;', 'Use &amp;nbsp;'
                'A "solution"', "skop 'n bal", '"""', "'''",
                '\n', '\t', '\r',
                '\\n', '\\t', '\\r', '\\"', '\r\n', '\\r\\n', '\\']
    for special in specials:
        quoted_special = pypo.quoteforpo(special)
        unquoted_special = pypo.unquotefrompo(quoted_special)
        print("special: %r\nquoted: %r\nunquoted: %r\n" % (special, quoted_special, unquoted_special))
        assert special == unquoted_special


class TestPOUnit(test_base.TestTranslationUnit):
    UnitClass = po.pounit

    def test_istranslatable(self):
        """Tests for the correct behaviour of istranslatable()."""
        unit = self.UnitClass("Message")
        assert unit.istranslatable()

        unit.source = " "
        assert unit.istranslatable()

        unit.source = ""
        assert not unit.istranslatable()
        # simulate a header
        unit.target = "PO-Revision-Date: 2006-02-09 23:33+0200\n"
        assert unit.isheader()
        assert not unit.istranslatable()

        unit.source = "Message"
        unit.target = "Boodskap"
        unit.makeobsolete()
        assert not unit.istranslatable()

    def test_locations(self):
        """Tests that we can add and retrieve error messages for a unit."""

        def locations_helper(location):
            unit = self.UnitClass()
            assert len(unit.getlocations()) == 0
            unit.addlocation(location)
            assert len(unit.getlocations()) == 1
            assert unit.getlocations() == [location]
        locations_helper("key")
        locations_helper("file.c:100")
        locations_helper("I am a key")
        locations_helper(u"unicoe key")

    def test_nongettext_location(self):
        """test that we correctly handle a non-gettext (file:linenumber) location"""
        u = self.UnitClass(u"")
        u.addlocation(u"programming/C/programming.xml:44(para)")
        assert "programming/C/programming.xml:44(para)" in str(u)
        assert u"programming/C/programming.xml:44(para)" in u.getlocations()

    def test_adding_empty_note(self):
        unit = self.UnitClass("bla")
        print(str(unit))
        assert not '#' in str(unit)
        for empty_string in ["", " ", "\t", "\n"]:
            unit.addnote(empty_string)
            assert not '#' in str(unit)

    def test_markreview(self):
        """Tests if we can mark the unit to need review."""
        unit = self.unit
        # We have to explicitly set the target to nothing, otherwise xliff
        # tests will fail.
        # Can we make it default behavior for the UnitClass?
        unit.target = ""

        unit.addnote("Test note 1", origin="translator")
        unit.addnote("Test note 2", origin="translator")
        original_notes = unit.getnotes(origin="translator")

        assert not unit.isreview()
        unit.markreviewneeded()
        print(unit.getnotes())
        assert unit.isreview()
        unit.markreviewneeded(False)
        assert not unit.isreview()
        assert unit.getnotes(origin="translator") == original_notes
        unit.markreviewneeded(explanation="Double check spelling.")
        assert unit.isreview()
        notes = unit.getnotes(origin="translator")
        assert notes.count("Double check spelling.") == 1

    def test_errors(self):
        """Tests that we can add and retrieve error messages for a unit."""
        unit = self.unit

        assert len(unit.geterrors()) == 0
        unit.adderror(errorname='test1', errortext='Test error message 1.')
        unit.adderror(errorname='test2', errortext='Test error message 2.')
        unit.adderror(errorname='test3', errortext='Test error message 3.')
        assert len(unit.geterrors()) == 3
        assert unit.geterrors()['test1'] == 'Test error message 1.'
        assert unit.geterrors()['test2'] == 'Test error message 2.'
        assert unit.geterrors()['test3'] == 'Test error message 3.'
        unit.adderror(errorname='test1', errortext='New error 1.')
        assert unit.geterrors()['test1'] == 'New error 1.'

    def test_no_plural_settarget(self):
        """tests that target handling of file with no plural is correct"""
        # plain text, no plural test
        unit = self.UnitClass("Tree")
        unit.target = "ki"
        assert not unit.hasplural()

        # plural test with multistring
        unit.setsource(["Tree", "Trees"])
        assert unit.source.strings == ["Tree", "Trees"]
        assert unit.hasplural()
        unit.target = multistring(["ki", "ni ki"])
        assert unit.target.strings == ["ki", "ni ki"]

        # test of msgid with no plural and msgstr with plural
        unit = self.UnitClass("Tree")
        assert raises(ValueError, unit.settarget, [u"ki", u"ni ki"])
        assert not unit.hasplural()

    def test_wrapping_bug(self):
        """This tests for a wrapping bug that existed at some stage."""
        unit = self.UnitClass("")
        message = 'Projeke ya Pootle ka boyona e ho <a href="http://translate.sourceforge.net/">translate.sourceforge.net</a> moo o ka fumanang dintlha ka source code, di mailing list jwalo jwalo.'
        unit.target = message
        print(unit.target)
        assert unit.target == message

    def test_extract_msgidcomments_from_text(self):
        """Test that KDE style comments are extracted correctly."""
        unit = self.UnitClass("test source")

        kdetext = "_: Simple comment\nsimple text"
        assert unit._extract_msgidcomments(kdetext) == "Simple comment"

    def test_isheader(self):
        """checks that we deal correctly with headers."""
        unit = self.UnitClass()
        unit.target = "PO-Revision-Date: 2006-02-09 23:33+0200\n"
        assert unit.isheader()
        unit.source = "Some English string"
        assert not unit.isheader()
        unit.source = u"Goeiemre"
        assert not unit.isheader()

#     def test_rich_source(self):
#         unit = self.unit
#         unit.rich_source = [['a', X('42'), 'c']]
#         assert unit.rich_source == [[u'a\ufffcc']]

#     def test_rich_target(self):
#         unit = self.unit
#         unit.rich_target = [['a', G('42', ['b']), 'c']]
#         assert unit.rich_target == [['abc']]


class TestPOFile(test_base.TestTranslationStore):
    StoreClass = po.pofile

    def poparse(self, posource):
        """helper that parses po source without requiring files"""
        dummyfile = wStringIO.StringIO(posource)
        pofile = self.StoreClass(dummyfile)
        return pofile

    def poregen(self, posource):
        """helper that converts po source to pofile object and back"""
        return str(self.poparse(posource))

    def pomerge(self, oldmessage, newmessage, authoritative):
        """helper that merges two messages"""
        oldpofile = self.poparse(oldmessage)
        oldunit = oldpofile.units[0]
        if newmessage:
            newpofile = self.poparse(newmessage)
            newunit = newpofile.units[0]
        else:
            newunit = oldpofile.UnitClass()
        oldunit.merge(newunit, authoritative=authoritative)
        print(oldunit)
        return str(oldunit)

    def poreflow(self, posource):
        """Helper to parse and reflow all text according to our code."""
        pofile = self.poparse(posource)
        for u in pofile.units:
            # force rewrapping:
            u.source = u.source
            u.target = u.target
        return str(pofile)

    def test_context_only(self):
        """Checks that an empty msgid with msgctxt is handled correctly."""
        posource = '''msgctxt "CONTEXT"
msgid ""
msgstr ""
'''
        pofile = self.poparse(posource)
        assert pofile.units[0].istranslatable()
        assert not pofile.units[0].isheader()
        # we were not generating output for thse at some stage
        assert str(pofile)

    def test_simpleentry(self):
        """checks that a simple po entry is parsed correctly"""
        posource = '#: test.c:100 test.c:101\nmsgid "test"\nmsgstr "rest"\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1
        thepo = pofile.units[0]
        assert thepo.getlocations() == ["test.c:100", "test.c:101"]
        assert thepo.source == "test"
        assert thepo.target == "rest"

    def test_copy(self):
        """checks that we can copy all the needed PO fields"""
        posource = '''# TRANSLATOR-COMMENTS
#. AUTOMATIC-COMMENTS
#: REFERENCE...
#, fuzzy
msgctxt "CONTEXT"
msgid "UNTRANSLATED-STRING"
msgstr "TRANSLATED-STRING"'''
        pofile = self.poparse(posource)
        oldunit = pofile.units[0]
        newunit = oldunit.copy()
        assert newunit == oldunit

    def test_parse_source_string(self):
        """parse a string"""
        posource = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1

    def test_parse_file(self):
        """test parsing a real file"""
        posource = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1

    def test_unicode(self):
        """check that the po class can handle Unicode characters"""
        posource = 'msgid ""\nmsgstr ""\n"Content-Type: text/plain; charset=UTF-8\\n"\n\n#: test.c\nmsgid "test"\nmsgstr "rest\xe2\x80\xa6"\n'
        pofile = self.poparse(posource)
        print(pofile)
        assert len(pofile.units) == 2

    def test_plurals(self):
        posource = r'''msgid "Cow"
msgid_plural "Cows"
msgstr[0] "Koei"
msgstr[1] "Koeie"
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1
        unit = pofile.units[0]
        assert isinstance(unit.target, multistring)
        print(unit.target.strings)
        assert unit.target == "Koei"
        assert unit.target.strings == ["Koei", "Koeie"]

        posource = r'''msgid "Skaap"
msgid_plural "Skape"
msgstr[0] "Sheep"
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1
        unit = pofile.units[0]
        assert isinstance(unit.target, multistring)
        print(unit.target.strings)
        assert unit.target == "Sheep"
        assert unit.target.strings == ["Sheep"]

    def test_plural_unicode(self):
        """tests that all parts of the multistring are unicode."""
        posource = r'''msgid "Cw"
msgid_plural "Cws"
msgstr[0] "Kei"
msgstr[1] "Keie"
'''
        pofile = self.poparse(posource)
        unit = pofile.units[0]
        assert isinstance(unit.source, multistring)
        assert isinstance(unit.source.strings[1], unicode)

    def test_nongettext_location(self):
        """test that we correctly handle a non-gettext (file:linenumber) location"""
        posource = '#: programming/C/programming.xml:44(para)\nmsgid "test"\nmsgstr "rest"\n'
        pofile = self.poparse(posource)
        u = pofile.units[-1]

        locations = u.getlocations()
        print(locations)
        assert len(locations) == 1
        assert locations[0] == u"programming/C/programming.xml:44(para)"
        assert isinstance(locations[0], unicode)

    @mark.xfail(reason="Not Implemented")
    def test_kde_plurals(self):
        """Tests kde-style plurals. (Bug: 191)"""
        posource = '''msgid "_n Singular\\n"
"Plural"
msgstr "Een\\n"
"Twee\\n"
"Drie"
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 1
        unit = pofile.units[0]
        assert unit.hasplural()
        assert isinstance(unit.source, multistring)
        print(unit.source.strings)
        assert unit.source == "Singular"
        assert unit.source.strings == ["Singular", "Plural"]
        assert isinstance(unit.target, multistring)
        print(unit.target.strings)
        assert unit.target == "Een"
        assert unit.target.strings == ["Een", "Twee", "Drie"]

    def test_empty_lines_notes(self):
        """Tests that empty comment lines are preserved"""
        posource = r'''# License name
#
# license line 1
# license line 2
# license line 3
msgid ""
msgstr "POT-Creation-Date: 2006-03-08 17:30+0200\n"
'''
        pofile = self.poparse(posource)
        assert str(pofile) == posource

    def test_fuzzy(self):
        """checks that fuzzy functionality works as expected"""
        posource = '#, fuzzy\nmsgid "ball"\nmsgstr "bal"\n'
        expectednonfuzzy = 'msgid "ball"\nmsgstr "bal"\n'
        pofile = self.poparse(posource)
        print(pofile)
        assert pofile.units[0].isfuzzy()
        pofile.units[0].markfuzzy(False)
        assert not pofile.units[0].isfuzzy()
        assert str(pofile) == expectednonfuzzy

        posource = '#, fuzzy, python-format\nmsgid "ball"\nmsgstr "bal"\n'
        expectednonfuzzy = '#, python-format\nmsgid "ball"\nmsgstr "bal"\n'
        expectedfuzzyagain = '#, fuzzy, python-format\nmsgid "ball"\nmsgstr "bal"\n'  # must be sorted
        pofile = self.poparse(posource)
        print(pofile)
        assert pofile.units[0].isfuzzy()
        pofile.units[0].markfuzzy(False)
        assert not pofile.units[0].isfuzzy()
        assert str(pofile) == expectednonfuzzy
        pofile.units[0].markfuzzy()
        print(str(pofile))
        assert str(pofile) == expectedfuzzyagain

        # test the same, but with flags in a different order
        posource = '#, python-format, fuzzy\nmsgid "ball"\nmsgstr "bal"\n'
        expectednonfuzzy = '#, python-format\nmsgid "ball"\nmsgstr "bal"\n'
        expectedfuzzyagain = '#, fuzzy, python-format\nmsgid "ball"\nmsgstr "bal"\n'  # must be sorted
        pofile = self.poparse(posource)
        print(pofile)
        assert pofile.units[0].isfuzzy()
        pofile.units[0].markfuzzy(False)
        assert not pofile.units[0].isfuzzy()
        print(str(pofile))
        assert str(pofile) == expectednonfuzzy
        pofile.units[0].markfuzzy()
        print(str(pofile))
        assert str(pofile) == expectedfuzzyagain

    @mark.xfail(reason="Check differing behaviours between pypo and cpo")
    def test_makeobsolete_untranslated(self):
        """Tests making an untranslated unit obsolete"""
        posource = '#. The automatic one\n#: test.c\nmsgid "test"\nmsgstr ""\n'
        pofile = self.poparse(posource)
        unit = pofile.units[0]
        print(str(pofile))
        assert not unit.isobsolete()
        unit.makeobsolete()
        assert str(unit) == ""
        # a better way might be for pomerge/pot2po to remove the unit

    def test_merging_automaticcomments(self):
        """checks that new automatic comments override old ones"""
        oldsource = '#. old comment\n#: line:10\nmsgid "One"\nmsgstr "Een"\n'
        newsource = '#. new comment\n#: line:10\nmsgid "One"\nmsgstr ""\n'
        expected = '#. new comment\n#: line:10\nmsgid "One"\nmsgstr "Een"\n'
        assert self.pomerge(newsource, oldsource, authoritative=True) == expected

    def test_malformed_units(self):
        """Test that we handle malformed units reasonably."""
        posource = 'msgid "thing\nmsgstr "ding"\nmsgid "Second thing"\nmsgstr "Tweede ding"\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        print(repr(pofile.units[0].source))
        assert pofile.units[0].source == u"thing"

    def test_malformed_obsolete_units(self):
        """Test that we handle malformed obsolete units reasonably."""
        posource = '''msgid "thing
msgstr "ding"

#~ msgid "Second thing"
#~ msgstr "Tweede ding"
#~ msgid "Third thing"
#~ msgstr "Derde ding"
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 3

    def test_uniforum_po(self):
        """Test that we handle Uniforum PO files."""
        posource = '''# File: ../somefile.cpp, line: 33
msgid "thing"
msgstr "ding"
#
# File: anotherfile.cpp, line: 34
msgid "second"
msgstr "tweede"
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        # FIXME we still need to handle this correctly for proper Uniforum support if required
        #assert pofile.units[0].getlocations() == "File: somefile, line: 300"
        #assert pofile.units[1].getlocations() == "File: anotherfile, line: 200"

    def test_obsolete(self):
        """Tests that obsolete messages work"""
        posource = '#~ msgid "Old thing"\n#~ msgstr "Ou ding"\n'
        pofile = self.poparse(posource)
        assert pofile.isempty()
        assert len(pofile.units) == 1
        unit = pofile.units[0]
        assert unit.isobsolete()
        assert str(pofile) == posource

        posource = '''msgid "one"
msgstr "een"

#, fuzzy
#~ msgid "File not found."
#~ msgid_plural "Files not found."
#~ msgstr[0] "Leer(s) nie gevind nie."
#~ msgstr[1] "Leer(s) nie gevind nie."
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        unit = pofile.units[1]
        assert unit.isobsolete()

        print(str(pofile))
        # Doesn't work with CPO if obsolete units are mixed with non-obsolete units
        assert str(pofile) == posource
        unit.resurrect()
        assert unit.hasplural()

    def test_obsolete_with_prev_msgid(self):
        """Tests that obsolete messages work"""
        # Bug 1429
        posource = r'''msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "one"
msgstr "een"

#, fuzzy
#~| msgid ""
#~| "You cannot read anything except web pages with\n"
#~| "this plugin, sorry."
#~ msgid "You cannot read anything except web pages with this plugin, sorry."
#~ msgstr ""
#~ "Mit diesem Modul knnen leider ausschlielich Webseiten vorgelesen werden."
'''
        pofile = self.poparse(posource)
        assert len(pofile.units) == 3
        unit = pofile.units[2]
        print(str(unit))
        assert unit.isobsolete()
        assert unit.isfuzzy()
        assert not unit.istranslatable()

        print(posource)
        print(str(pofile))
        assert str(pofile) == posource

    def test_header_escapes(self):
        pofile = self.StoreClass()
        pofile.updateheader(add=True, **{"Report-Msgid-Bugs-To": r"http://qa.openoffice.org/issues/enter_bug.cgi?subcomponent=ui&comment=&short_desc=Localization%20issue%20in%20file%3A%20dbaccess\source\core\resource.oo&component=l10n&form_name=enter_issue"})
        filecontents = str(pofile)
        print(filecontents)
        # We need to make sure that the \r didn't get misrepresented as a
        # carriage return, but as a slash (escaped) followed by a normal 'r'
        assert r'\source\core\resource' in pofile.header().target
        assert r're\\resource' in filecontents

    def test_makeobsolete(self):
        """Tests making a unit obsolete"""
        posource = '#. The automatic one\n#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poexpected = '#~ msgid "test"\n#~ msgstr "rest"\n'
        pofile = self.poparse(posource)
        print(pofile)
        unit = pofile.units[0]
        assert not unit.isobsolete()
        unit.makeobsolete()
        assert unit.isobsolete()
        print(pofile)
        assert str(unit) == poexpected

    def test_makeobsolete_plural(self):
        """Tests making a plural unit obsolete"""
        posource = r'''msgid "Cow"
msgid_plural "Cows"
msgstr[0] "Koei"
msgstr[1] "Koeie"
'''
        poexpected = '''#~ msgid "Cow"
#~ msgid_plural "Cows"
#~ msgstr[0] "Koei"
#~ msgstr[1] "Koeie"
'''
        pofile = self.poparse(posource)
        print(pofile)
        unit = pofile.units[0]
        assert not unit.isobsolete()
        unit.makeobsolete()
        assert unit.isobsolete()
        print(pofile)
        assert str(unit) == poexpected

    def test_makeobsolete_msgctxt(self):
        """Tests making a unit with msgctxt obsolete"""
        posource = '#: test.c\nmsgctxt "Context"\nmsgid "test"\nmsgstr "rest"\n'
        poexpected = '#~ msgctxt "Context"\n#~ msgid "test"\n#~ msgstr "rest"\n'
        pofile = self.poparse(posource)
        print(pofile)
        unit = pofile.units[0]
        assert not unit.isobsolete()
        assert unit.istranslatable()
        unit.makeobsolete()
        assert unit.isobsolete()
        assert not unit.istranslatable()
        print(pofile)
        assert str(unit) == poexpected

    def test_makeobsolete_msgidcomments(self):
        """Tests making a unit with msgidcomments obsolete"""
        posource = '#: first.c\nmsgid ""\n"_: first.c\\n"\n"test"\nmsgstr "rest"\n\n#: second.c\nmsgid ""\n"_: second.c\\n"\n"test"\nmsgstr "rest"'
        poexpected = '#~ msgid ""\n#~ "_: first.c\\n"\n#~ "test"\n#~ msgstr "rest"\n'
        print("Source:\n%s" % posource)
        print("Expected:\n%s" % poexpected)
        pofile = self.poparse(posource)
        unit = pofile.units[0]
        assert not unit.isobsolete()
        unit.makeobsolete()
        assert unit.isobsolete()
        print("Result:\n%s" % pofile)
        assert str(unit) == poexpected

    def test_multiline_obsolete(self):
        """Tests for correct output of mulitline obsolete messages"""
        posource = '#~ msgid ""\n#~ "Old thing\\n"\n#~ "Second old thing"\n#~ msgstr ""\n#~ "Ou ding\\n"\n#~ "Tweede ou ding"\n'
        pofile = self.poparse(posource)
        assert pofile.isempty()
        assert len(pofile.units) == 1
        unit = pofile.units[0]
        assert unit.isobsolete()
        print(str(pofile))
        print(posource)
        assert str(pofile) == posource

    def test_merge_duplicates(self):
        """checks that merging duplicates works"""
        posource = '#: source1\nmsgid "test me"\nmsgstr ""\n\n#: source2\nmsgid "test me"\nmsgstr ""\n'
        pofile = self.poparse(posource)
        #assert len(pofile.units) == 2
        pofile.removeduplicates("merge")
        assert len(pofile.units) == 1
        assert pofile.units[0].getlocations() == ["source1", "source2"]
        print(pofile)

    def test_merge_mixed_sources(self):
        """checks that merging works with different source location styles"""
        posource = '''
#: source1
#: source2
msgid "test"
msgstr ""

#: source1 source2
msgid "test"
msgstr ""
'''
        pofile = self.poparse(posource)
        print(str(pofile))
        pofile.removeduplicates("merge")
        print(str(pofile))
        assert len(pofile.units) == 1
        assert pofile.units[0].getlocations() == ["source1", "source2"]

    def test_parse_context(self):
        """Tests that msgctxt is parsed correctly and that it is accessible via the api methods."""
        posource = '''# Test comment
#: source1
msgctxt "noun"
msgid "convert"
msgstr "bekeerling"

# Test comment 2
#: source2
msgctxt "verb"
msgid "convert"
msgstr "omskakel"
'''
        pofile = self.poparse(posource)
        unit = pofile.units[0]

        assert unit.getcontext() == 'noun'
        assert unit.getnotes() == 'Test comment'

        unit = pofile.units[1]
        assert unit.getcontext() == 'verb'
        assert unit.getnotes() == 'Test comment 2'

    def test_parse_advanced_context(self):
        """Tests that some weird possible msgctxt scenarios are parsed correctly."""
        posource = r'''# Test multiline context
#: source1
msgctxt "Noun."
" A person that changes his or her ways."
msgid "convert"
msgstr "bekeerling"

# Test quotes
#: source2
msgctxt "Verb. Converting from \"something\" to \"something else\"."
msgid "convert"
msgstr "omskakel"

# Test quotes, newlines and multiline.
#: source3
msgctxt "Verb.\nConverting from \"something\""
" to \"something else\"."
msgid "convert"
msgstr "omskakel"
'''
        pofile = self.poparse(posource)
        unit = pofile.units[0]

        assert unit.getcontext() == 'Noun. A person that changes his or her ways.'
        assert unit.getnotes() == 'Test multiline context'

        unit = pofile.units[1]
        assert unit.getcontext() == 'Verb. Converting from "something" to "something else".'
        assert unit.getnotes() == 'Test quotes'

        unit = pofile.units[2]
        assert unit.getcontext() == 'Verb.\nConverting from "something" to "something else".'
        assert unit.getnotes() == 'Test quotes, newlines and multiline.'

    def test_kde_context(self):
        """Tests that kde-style msgid comments can be retrieved via getcontext()."""
        posource = '''# Test comment
#: source1
msgid ""
"_: Noun\\n"
"convert"
msgstr "bekeerling"

# Test comment 2
#: source2
msgid ""
"_: Verb. _: "
"The action of changing.\\n"
"convert"
msgstr "omskakel"
'''
        pofile = self.poparse(posource)
        unit = pofile.units[0]

        assert unit.getcontext() == 'Noun'
        assert unit.getnotes() == 'Test comment'

        unit = pofile.units[1]
        assert unit.getcontext() == 'Verb. _: The action of changing.'
        assert unit.getnotes() == 'Test comment 2'

    def test_broken_kde_context(self):
        posource = '''msgid "Broken _: here"
msgstr "Broken _: here"
'''
        pofile = self.poparse(posource)
        unit = pofile.units[0]
        assert unit.source == "Broken _: here"
        assert unit.target == "Broken _: here"

    def test_id(self):
        """checks that ids work correctly"""
        posource = r'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "plant"
msgstr ""

msgid ""
"_: Noun\n"
"convert"
msgstr "bekeerling"

msgctxt "verb"
msgid ""
"convert"
msgstr "omskakel"

msgid "tree"
msgid_plural "trees"
msgstr[0] ""
'''
        pofile = self.poparse(posource)
        assert pofile.units[0].getid() == ""
        assert pofile.units[1].getid() == "plant"
        assert pofile.units[2].getid() == "_: Noun\nconvert"
        assert pofile.units[3].getid() == "verb\04convert"
        # Gettext does not consider the plural to determine duplicates, only
        # the msgid. For generation of .mo files, we might want to use this
        # code to generate the entry for the hash table, but for now, it is
        # commented out for conformance to gettext.
#        assert pofile.units[4].getid() == "tree\0trees"

    def test_non_ascii_header_comments(self):
        posource = r'''
# Tt is.
# H H H.
#. Lkkr.
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "a"
msgstr "b"
'''
        pofile = self.poparse(posource)
        for line in pofile.units[0].getnotes():
            assert isinstance(line, unicode)

    def test_non_ascii_header_comments(self):
        posource = r'''
# Copyright bla.
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Last-Translator: Trnsltr\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "a"
msgstr "b"
'''
        pofile = self.poparse(posource)
        assert u"Trnsltr" in pofile.units[0].target
        header_dict = pofile.parseheader()
        assert u"Last-Translator" in header_dict
        assert header_dict[u"Last-Translator"] == u"Trnsltr"


        # let's test the same with latin-1:
        posource = r'''
# Copyright bla.
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Last-Translator: Trnsltr\n"
"Content-Type: text/plain; charset=ISO-8859-1\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "a"
msgstr "b"
'''.decode('utf-8').encode('ISO-8859-1')

        pofile = self.poparse(posource)
        assert u"Trnsltr" in pofile.units[0].target
        header_dict = pofile.parseheader()
        assert u"Last-Translator" in header_dict
        assert header_dict[u"Last-Translator"] == u"Trnsltr"

    def test_final_slash(self):
        """Test that \ as last character is correcly interpreted (bug 960)."""
        posource = r'''
msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"

#: System-Support,Project>>decideAboutCreatingBlank:
msgid "I cannot locate the project\\"
msgstr ""
'''
        pofile1 = self.poparse(posource)
        print(pofile1.units[1].source)
        assert pofile1.units[1].source == u"I cannot locate the project\\"
        pofile2 = self.poparse(str(pofile1))
        print(str(pofile2))
        assert str(pofile1) == str(pofile2)

    def test_unfinished_lines(self):
        """Test that we reasonably handle lines with a single quote."""
        posource = r'''
msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"

msgid "I cannot locate the project\\"
msgstr "start thing dingis fish"
"
"
'''
        pofile1 = self.poparse(posource)
        print(repr(pofile1.units[1].target))
        assert pofile1.units[1].target == u"start thing dingis fish"
        pofile2 = self.poparse(str(pofile1))
        assert pofile2.units[1].target == u"start thing dingis fish"
        print(str(pofile2))
        assert str(pofile1) == str(pofile2)

    def test_encoding_change(self):
        posource = ur'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=ISO-8859-1\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "a"
msgstr "d"
'''.encode('iso-8859-1')
        pofile = self.poparse(posource)
        unit = pofile.units[1]
        unit.target = u""
        contents = str(pofile)
        assert 'msgstr "\341\270\223"' in contents
        assert 'charset=UTF-8' in contents

    def test_istranslated(self):
        """checks that istranslated works ok."""
        posource = ur'''
msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=ISO-8859-1\n"
"Content-Transfer-Encoding: 8-bit\n"

msgid "a"
msgid_plural "aa"
msgstr[0] ""
'''
        pofile = self.poparse(posource)
        unit = pofile.units[1]
        print(str(unit))
        assert "msgid_plural" in str(unit)
        assert not unit.istranslated()
        assert unit.get_state_n() == 0

    def test_wrapping(self):
        """This tests that we wrap like gettext."""
        posource = r'''#: file.h:1
msgid "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345"
msgstr "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345"
'''
        # should be unchanged:
        assert self.poreflow(posource) == posource

        posource = r'''#: 2
msgid "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
msgstr "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
'''
        posource_wanted = r'''#: 2
msgid ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
msgstr ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
'''
        assert self.poreflow(posource) == posource_wanted

        posource = r'''#: 7
msgid "bla\t12345 12345 12345 12345 12345 12 12345 12345 12345 12345 12345 12345 123"
msgstr "bla\t12345 12345 12345 12345 12345 15 12345 12345 12345 12345 12345 12345 123"
'''
        posource_wanted = r'''#: 7
msgid ""
"bla\t12345 12345 12345 12345 12345 12 12345 12345 12345 12345 12345 12345 123"
msgstr ""
"bla\t12345 12345 12345 12345 12345 15 12345 12345 12345 12345 12345 12345 123"
'''
        assert self.poreflow(posource) == posource_wanted

        posource = r'''#: 7
msgid "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
msgstr "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1"
'''
        posource_wanted = r'''#: 7
msgid ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 "
"1"
msgstr ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 "
"1"
'''
        assert self.poreflow(posource) == posource_wanted

        posource = r'''#: 8
msgid "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1234\n1234"
msgstr "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1234\n1234"

#: 9
msgid "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345\n12345"
msgstr "bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345\n12345"
'''
        posource_wanted = r'''#: 8
msgid ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1234\n"
"1234"
msgstr ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 1234\n"
"1234"

#: 9
msgid ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 "
"12345\n"
"12345"
msgstr ""
"bla\t12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 12345 "
"12345\n"
"12345"
'''
        assert self.poreflow(posource) == posource_wanted

        posource =r'''#: 10
msgid "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
msgstr "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
'''
        posource_wanted = r'''#: 10
msgid ""
"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
"\\"
msgstr ""
"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
"\\"
'''

########NEW FILE########
__FILENAME__ = test_pocommon
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.storage import pocommon


def test_roundtrip_quote_plus():
    "Test that what we put in is what we get out"""
    def roundtrip_quote_plus(text, quoted):
        quote = pocommon.quote_plus(text)
        assert quote == quoted
        unquote = pocommon.unquote_plus(quoted)
        assert unquote == text
    roundtrip_quote_plus(u"abc", u"abc")
    roundtrip_quote_plus(u"key space", u"key+space")
    roundtrip_quote_plus(u"key ey", u"key+%E1%B8%93ey")

########NEW FILE########
__FILENAME__ = test_poheader
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time

try:
    from collections import OrderedDict
except ImportError:
    # Python <= 2.6 fallback
    from translate.misc.dictutils import ordereddict as OrderedDict

from translate.lang.team import guess_language
from translate.misc import wStringIO
from translate.storage import po, poheader, poxliff


def test_parseheaderstring():
    """ test for the header parsing function"""
    source = r'''item1: one
item2: two:two
this item must get ignored because there is no colon sign in it
item3: three
'''
    d = poheader.parseheaderstring(source)
    print(type(d))
    assert len(d) == 3
    assert d['item1'] == 'one'
    assert d['item2'] == 'two:two'
    assert d['item3'] == 'three'


def test_update():
    '''test the update function'''
    # do we really add nothing if add==False ?
    d = poheader.update({}, test='hello')
    assert len(d) == 0
    # do we add if add==True ?
    d = poheader.update({}, add=True, Test='hello')
    assert len(d) == 1
    assert d['Test'] == 'hello'
    # do we really update ?
    d = poheader.update({'Test': 'hello'}, add=True, Test='World')
    assert len(d) == 1
    assert d['Test'] == 'World'
    # does key rewrite work ?
    d = poheader.update({}, add=True, test_me='hello')
    assert d['Test-Me'] == 'hello'
    # is the order correct ?
    d = OrderedDict()
    d['Project-Id-Version'] = 'abc'
    d['POT-Creation-Date'] = 'now'
    d = poheader.update(d, add=True, Test='hello', Report_Msgid_Bugs_To='bugs@list.org')
    assert d.keys()[0] == "Project-Id-Version"
    assert d.keys()[1] == "Report-Msgid-Bugs-To"
    assert d.keys()[2] == "POT-Creation-Date"
    assert d.keys()[3] == "Test"


def poparse(posource):
    """helper that parses po source without requiring files"""
    dummyfile = wStringIO.StringIO(posource)
    return po.pofile(dummyfile)


def poxliffparse(posource):
    """helper that parses po source into poxliffFile"""
    poxli = poxliff.PoXliffFile()
    poxli.parse(posource)
    return poxli


def check_po_date(datestring):
    """Check the validity of a PO date.

    The datestring must be in the format: 2007-06-08 10:08+0200
    """

    # We don't include the timezone offset as part of our format,
    # because time.strptime() does not recognize %z
    # The use of %z is deprecated in any case.
    date_format = "%Y-%m-%d %H:%M"

    # Get the timezone offset (last 4 digits):
    tz = datestring[-4:]
    assert type(int(tz)) == int

    # Strip the timezone from the string, typically something like "+0200".
    # This is to make the datestring conform to the specified format,
    # we can't add %z to the format.
    datestring = datestring[0:-5]

    # Check that the date can be parsed
    assert type(time.strptime(datestring, date_format)) == time.struct_time


def test_po_dates():
    pofile = po.pofile()
    headerdict = pofile.makeheaderdict(po_revision_date=True)
    check_po_date(headerdict["POT-Creation-Date"])
    check_po_date(headerdict["PO-Revision-Date"])

    headerdict = pofile.makeheaderdict(pot_creation_date=time.localtime(),
        po_revision_date=time.localtime())
    check_po_date(headerdict["POT-Creation-Date"])
    check_po_date(headerdict["PO-Revision-Date"])


def test_timezones():
    pofile = po.pofile()

    # The following will only work on Unix because of tzset() and %z
    if 'tzset' in time.__dict__:
        os.environ['TZ'] = 'Asia/Kabul'
        time.tzset()
        assert time.timezone == -16200
        # Typically "+0430"
        assert poheader.tzstring() == time.strftime("%z")

        os.environ['TZ'] = 'Asia/Seoul'
        time.tzset()
        assert time.timezone == -32400
        # Typically "+0900"
        assert poheader.tzstring() == time.strftime("%z")

        os.environ['TZ'] = 'Africa/Johannesburg'
        time.tzset()
        assert time.timezone == -7200
        # Typically "+0200"
        assert poheader.tzstring() == time.strftime("%z")

        os.environ['TZ'] = 'Africa/Windhoek'
        time.tzset()
        assert time.timezone == -3600
        # Typically "+0100"
        # For some reason python's %z doesn't know about Windhoek DST
        #assert poheader.tzstring() == time.strftime("%z")

        os.environ['TZ'] = 'UTC'
        time.tzset()
        assert time.timezone == 0
        # Typically "+0000"
        assert poheader.tzstring() == time.strftime("%z")


def test_header_blank():

    def compare(pofile):
        print(pofile)
        assert len(pofile.units) == 1
        header = pofile.header()
        assert header.isheader()
        assert not header.isblank()

        headeritems = pofile.parseheader()
        assert headeritems["Project-Id-Version"] == "PACKAGE VERSION"
        assert headeritems["Report-Msgid-Bugs-To"] == ""
        check_po_date(headeritems["POT-Creation-Date"])
        assert headeritems["PO-Revision-Date"] == "YEAR-MO-DA HO:MI+ZONE"
        assert headeritems["Last-Translator"] == "FULL NAME <EMAIL@ADDRESS>"
        assert headeritems["Language-Team"] == "LANGUAGE <LL@li.org>"
        assert headeritems["MIME-Version"] == "1.0"
        assert headeritems["Content-Type"] == "text/plain; charset=UTF-8"
        assert headeritems["Content-Transfer-Encoding"] == "8bit"
        assert headeritems["Plural-Forms"] == "nplurals=INTEGER; plural=EXPRESSION;"

    """test header functionality"""
    posource = r'''# other comment\n
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2006-03-08 17:30+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;\n"
'''
    pofile = poparse(posource)
    compare(pofile)

## TODO: enable this code if PoXliffFile is able to parse a header
##
##    poxliffsource = r'''<?xml version="1.0" encoding="utf-8"?>
##<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
##
##<file datatype="po" original="test.po" source-language="en-US"><body><trans-unit approved="no" id="1" restype="x-gettext-domain-header" xml:space="preserve"><source>Project-Id-Version: PACKAGE VERSION
##Report-Msgid-Bugs-To:
##POT-Creation-Date: 2006-03-08 17:30+0200
##PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE
##Last-Translator: FULL NAME <ph id="1">&lt;EMAIL@ADDRESS&gt;</ph>
##Language-Team: LANGUAGE <ph id="2">&lt;LL@li.org&gt;</ph>
##MIME-Version: 1.0
##Content-Type: text/plain; charset=UTF-8
##Content-Transfer-Encoding: 8bit
##Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;
##</source><target>Project-Id-Version: PACKAGE VERSION
##Report-Msgid-Bugs-To:
##POT-Creation-Date: 2006-03-08 17:30+0200
##PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE
##Last-Translator: FULL NAME <ph id="1">&lt;EMAIL@ADDRESS&gt;</ph>
##Language-Team: LANGUAGE <ph id="2">&lt;LL@li.org&gt;</ph>
##MIME-Version: 1.0
##Content-Type: text/plain; charset=UTF-8
##Content-Transfer-Encoding: 8bit
##Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;
##</target><context-group name="po-entry" purpose="information"><context context-type="x-po-trancomment">other comment\n</context></context-group><note from="po-translator">other comment\n</note></trans-unit></body></file></xliff>
##'''
##    pofile = poparse(poxliffsource)
##    compare(pofile)


def test_plural_equation():
    """test that we work with the equation even is the last semicolon is left out, since gettext
    tools don't seem to mind"""
    posource = r'''msgid ""
msgstr ""
"Plural-Forms: nplurals=2; plural=(n != 1)%s\n"
'''
    for colon in ("", ";"):
        pofile = poparse(posource % colon)
        print(pofile)
        assert len(pofile.units) == 1
        header = pofile.units[0]
        assert header.isheader()
        assert not header.isblank()

        headeritems = pofile.parseheader()
        nplural, plural = pofile.getheaderplural()
        assert nplural == "2"
        assert plural == "(n != 1)"
##    TODO: add the same test for PoXliffFile


def test_plural_equation_across_lines():
    """test that we work if the plural equation spans more than one line"""
    posource = r'''msgid ""
msgstr ""
"Plural-Forms:  nplurals=3; plural=(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%"
"10<=4 && (n%100<10 || n%100>=20) ? 1 : 2);\n"
'''
    pofile = poparse(posource)
    print(pofile)
    assert len(pofile.units) == 1
    header = pofile.units[0]
    assert header.isheader()
    assert not header.isblank()

    headeritems = pofile.parseheader()
    nplural, plural = pofile.getheaderplural()
    assert nplural == "3"
    assert plural == "(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)"
##    TODO: add the same test for PoXliffFile


def test_updatecontributor():
    """Test that we can update contributor information in the header comments."""
    posource = r'''msgid ""
msgstr ""
"MIME-Version: 1.0"
'''
    pofile = poparse(posource)
    pofile.updatecontributor("Grasvreter")
    assert "# Grasvreter, 20" in str(pofile)

    pofile.updatecontributor("Koeivreter", "monster@grasveld.moe")
    assert "# Koeivreter <monster@grasveld.moe>, 20" in str(pofile)

    pofile.header().addnote("Khaled Hosny <khaledhosny@domain.org>, 2006, 2007, 2008.")
    pofile.updatecontributor("Khaled Hosny", "khaledhosny@domain.org")
    print(str(pofile))
    assert "# Khaled Hosny <khaledhosny@domain.org>, 2006, 2007, 2008, %s." % time.strftime("%Y") in str(pofile)


def test_language():
    """Test that we can get a language from the relevant headers."""
    posource = r'''msgid ""
msgstr ""
"MIME-Version: 1.0\n"
'''

    pofile = poparse(posource)
    assert pofile.gettargetlanguage() is None

    posource += '"Language-Team: translate-discuss-af@lists.sourceforge.net\\n"\n'
    pofile = poparse(posource)
    assert pofile.gettargetlanguage() == 'af'

    posource += '"X-Poedit-Language: German\\n"\n'
    pofile = poparse(posource)
    assert pofile.gettargetlanguage() == 'de'

    posource += '"Language: fr_CA\\n"\n'
    pofile = poparse(posource)
    assert pofile.gettargetlanguage() == 'fr_CA'


def test_project():
    """Test that we can get a project from the relevant headers."""
    posource = r'''msgid ""
msgstr ""
"MIME-Version: 1.0\n"
'''

    pofile = poparse(posource)
    assert pofile.getprojectstyle() is None

    posource += '"X-Accelerator-Marker: ~\\n"\n'
    pofile = poparse(posource)
    assert pofile.getprojectstyle() == 'openoffice'

    posource += '"Report-Msgid-Bugs-To: http://bugzilla.gnome.org/enter_bug.cgi?product=system-\\n"\n'
    pofile = poparse(posource)
    assert pofile.getprojectstyle() == 'gnome'

    posource += '"X-Project-Style: drupal\\n"\n'
    pofile = poparse(posource)
    assert pofile.getprojectstyle() == 'drupal'

    pofile.setprojectstyle('kde')
    assert pofile.getprojectstyle() == 'kde'

    pofile.setprojectstyle('complete-rubbish')
    assert pofile.getprojectstyle() == 'kde'

########NEW FILE########
__FILENAME__ = test_poxliff
#!/usr/bin/env python

from translate.misc.multistring import multistring
from translate.storage import poxliff, test_xliff


class TestPOXLIFFUnit(test_xliff.TestXLIFFUnit):
    UnitClass = poxliff.PoXliffUnit

    def test_plurals(self):
        """Tests that plurals are handled correctly."""
        unit = self.UnitClass(multistring(["Cow", "Cows"]))
        print(type(unit.source))
        print(repr(unit.source))
        assert isinstance(unit.source, multistring)
        assert unit.source.strings == ["Cow", "Cows"]
        assert unit.source == "Cow"

        unit.target = ["Koei", "Koeie"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == ["Koei", "Koeie"]
        assert unit.target == "Koei"

        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target == u"Sk\u00ear"

    def test_ids(self):
        """Tests that ids are assigned correctly, especially for plurals"""
        unit = self.UnitClass("gras")
        assert not unit.getid()
        unit.setid("4")
        assert unit.getid() == "4"

        unit = self.UnitClass(multistring(["shoe", "shoes"]))
        assert not unit.getid()
        unit.setid("20")
        assert unit.getid() == "20"
        assert unit.units[1].getid() == "20[1]"

        unit.target = ["utshani", "uutshani", "uuutshani"]
        assert unit.getid() == "20"
        assert unit.units[1].getid() == "20[1]"


class TestPOXLIFFfile(test_xliff.TestXLIFFfile):
    StoreClass = poxliff.PoXliffFile
    xliffskeleton = '''<?xml version="1.0" ?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
  <file original="filename.po" source-language="en-US" datatype="po">
    <body>
        %s
    </body>
  </file>
</xliff>'''

    def test_parse(self):
        minixlf = self.xliffskeleton % '''<group restype="x-gettext-plurals">
        <trans-unit id="1[0]" xml:space="preserve">
            <source>cow</source>
            <target>inkomo</target>
        </trans-unit>
        <trans-unit id="1[1]" xml:space="preserve">
            <source>cows</source>
            <target>iinkomo</target>
        </trans-unit>
</group>'''
        xlifffile = self.StoreClass.parsestring(minixlf)
        assert len(xlifffile.units) == 1
        assert xlifffile.translate("cow") == "inkomo"
        assert xlifffile.units[0].source == "cow"
        assert xlifffile.units[0].source == multistring(["cow", "cows"])

    def test_notes(self):
        minixlf = self.xliffskeleton % '''<group restype="x-gettext-plurals">
        <trans-unit id="1[0]" xml:space="preserve">
            <source>cow</source>
            <target>inkomo</target>
<note from="po-translator">Zulu translation of program ABC</note>
<note from="developer">azoozoo come back!</note>
        </trans-unit>
        <trans-unit id="1[1]" xml:space="preserve">
            <source>cows</source>
            <target>iinkomo</target>
<note from="po-translator">Zulu translation of program ABC</note>
<note from="developer">azoozoo come back!</note>
        </trans-unit>
</group>'''
        xlifffile = self.StoreClass.parsestring(minixlf)
        assert xlifffile.units[0].getnotes() == "Zulu translation of program ABC\nazoozoo come back!"
        assert xlifffile.units[0].getnotes("developer") == "azoozoo come back!"
        assert xlifffile.units[0].getnotes("po-translator") == "Zulu translation of program ABC"

########NEW FILE########
__FILENAME__ = test_properties
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import deprecated_call, raises

from translate.misc import wStringIO
from translate.storage import properties, test_monolingual


def test_find_delimiter_pos_simple():
    """Simple tests to find the various delimiters"""
    assert properties._find_delimiter(u"key=value", [u"=", u":", u" "]) == ('=', 3)
    assert properties._find_delimiter(u"key:value", [u"=", u":", u" "]) == (':', 3)
    assert properties._find_delimiter(u"key value", [u"=", u":", u" "]) == (' ', 3)
    # NOTE this is valid in Java properties, the key is then the empty string
    assert properties._find_delimiter(u"= value", [u"=", u":", u" "]) == ('=', 0)


def test_find_delimiter_pos_multiple():
    """Find delimiters when multiple potential delimiters are involved"""
    assert properties._find_delimiter(u"key=value:value", [u"=", u":", u" "]) == ('=', 3)
    assert properties._find_delimiter(u"key:value=value", [u"=", u":", u" "]) == (':', 3)
    assert properties._find_delimiter(u"key value=value", [u"=", u":", u" "]) == (' ', 3)


def test_find_delimiter_pos_none():
    """Find delimiters when there isn't one"""
    assert properties._find_delimiter(u"key", [u"=", u":", u" "]) == (None, -1)
    assert properties._find_delimiter(u"key\=\:\ ", [u"=", u":", u" "]) == (None, -1)


def test_find_delimiter_pos_whitespace():
    """Find delimiters when whitespace is involved"""
    assert properties._find_delimiter(u"key = value", [u"=", u":", u" "]) == ('=', 4)
    assert properties._find_delimiter(u"key : value", [u"=", u":", u" "]) == (':', 4)
    assert properties._find_delimiter(u"key   value", [u"=", u":", u" "]) == (' ', 3)
    assert properties._find_delimiter(u"key value = value", [u"=", u":", u" "]) == (' ', 3)
    assert properties._find_delimiter(u"key value value", [u"=", u":", u" "]) == (' ', 3)
    assert properties._find_delimiter(u" key = value", [u"=", u":", u" "]) == ('=', 5)


def test_find_delimiter_pos_escapes():
    """Find delimiters when potential earlier delimiters are escaped"""
    assert properties._find_delimiter(u"key\:=value", [u"=", u":", u" "]) == ('=', 5)
    assert properties._find_delimiter(u"key\=: value", [u"=", u":", u" "]) == (':', 5)
    assert properties._find_delimiter(u"key\   value", [u"=", u":", u" "]) == (' ', 5)
    assert properties._find_delimiter(u"key\ key\ key\: = value", [u"=", u":", u" "]) == ('=', 16)


def test_find_delimiter_deprecated_fn():
    """Test that the deprecated function still actually works"""
    assert properties.find_delimeter(u"key=value") == ('=', 3)
    deprecated_call(properties.find_delimeter, u"key=value")


def test_is_line_continuation():
    assert not properties.is_line_continuation(u"")
    assert not properties.is_line_continuation(u"some text")
    assert properties.is_line_continuation(u"""some text\\""")
    assert not properties.is_line_continuation(u"""some text\\\\""")  # Escaped \
    assert properties.is_line_continuation(u"""some text\\\\\\""")  # Odd num. \ is line continuation
    assert properties.is_line_continuation(u"""\\\\\\""")


def test_key_strip():
    assert properties._key_strip(u"key") == "key"
    assert properties._key_strip(u" key") == "key"
    assert properties._key_strip(u"\ key") == "\ key"
    assert properties._key_strip(u"key ") == "key"
    assert properties._key_strip(u"key\ ") == "key\ "


def test_is_comment_one_line():
    assert properties.is_comment_one_line("# comment")
    assert properties.is_comment_one_line("! comment")
    assert properties.is_comment_one_line("// comment")
    assert properties.is_comment_one_line("  # comment")
    assert properties.is_comment_one_line("/* comment */")
    assert not properties.is_comment_one_line("not = comment_line /* comment */")
    assert not properties.is_comment_one_line("/* comment ")


def test_is_comment_start():
    assert properties.is_comment_start("/* comment")
    assert not properties.is_comment_start("/* comment */")


def test_is_comment_end():
    assert properties.is_comment_end(" comment */")
    assert not properties.is_comment_end("/* comment */")


class TestPropUnit(test_monolingual.TestMonolingualUnit):
    UnitClass = properties.propunit

    def test_rich_get(self):
        pass

    def test_rich_set(self):
        pass


class TestProp(test_monolingual.TestMonolingualStore):
    StoreClass = properties.propfile

    def propparse(self, propsource, personality="java", encoding=None):
        """helper that parses properties source without requiring files"""
        dummyfile = wStringIO.StringIO(propsource)
        propfile = properties.propfile(dummyfile, personality, encoding)
        return propfile

    def propregen(self, propsource):
        """helper that converts properties source to propfile object and back"""
        return str(self.propparse(propsource))

    def test_simpledefinition(self):
        """checks that a simple properties definition is parsed correctly"""
        propsource = 'test_me=I can code!'
        propfile = self.propparse(propsource)
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == "test_me"
        assert propunit.source == "I can code!"

    def test_simpledefinition_source(self):
        """checks that a simple properties definition can be regenerated as source"""
        propsource = 'test_me=I can code!'
        propregen = self.propregen(propsource)
        assert propsource + '\n' == propregen

    def test_unicode_escaping(self):
        """check that escaped unicode is converted properly"""
        propsource = "unicode=\u0411\u0416\u0419\u0428"
        messagevalue = u'\u0411\u0416\u0419\u0428'.encode("UTF-8")
        propfile = self.propparse(propsource, personality="mozilla")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == "unicode"
        assert propunit.source.encode("UTF-8") == ""
        regensource = str(propfile)
        assert messagevalue in regensource
        assert "\\u" not in regensource

    def test_newlines_startend(self):
        """check that we preserve \n that appear at start and end of properties"""
        propsource = "newlines=\\ntext\\n"
        propregen = self.propregen(propsource)
        assert propsource + '\n' == propregen

    def test_whitespace_handling(self):
        """check that we remove extra whitespace around property"""
        whitespaces = (
            ('key = value', 'key', 'value'),      # Standard for baseline
            (' key =  value', 'key', 'value'),    # Extra \s before key and value
            ('\ key\ = value', '\ key\ ', 'value'),  # extra space at start and end of key
            ('key = \ value ', 'key', ' value '),  # extra space at start end end of value
        )
        for propsource, key, value in whitespaces:
            propfile = self.propparse(propsource)
            propunit = propfile.units[0]
            print(repr(propsource), repr(propunit.name), repr(propunit.source))
            assert propunit.name == key
            assert propunit.source == value
            # let's reparse the output to ensure good serialisation->parsing roundtrip:
            propfile = self.propparse(str(propunit))
            propunit = propfile.units[0]
            assert propunit.name == key
            assert propunit.source == value

    def test_key_value_delimiters_simple(self):
        """test that we can handle colon, equals and space delimiter
        between key and value.  We don't test any space removal or escaping"""
        delimiters = [":", "=", " "]
        for delimiter in delimiters:
            propsource = "key%svalue" % delimiter
            print("source: '%s'\ndelimiter: '%s'" % (propsource, delimiter))
            propfile = self.propparse(propsource)
            assert len(propfile.units) == 1
            propunit = propfile.units[0]
            assert propunit.name == "key"
            assert propunit.source == "value"

    def test_comments(self):
        """checks that we handle # and ! comments"""
        markers = ['#', '!']
        for comment_marker in markers:
            propsource = '''%s A comment
key=value
''' % comment_marker
            propfile = self.propparse(propsource)
            print(repr(propsource))
            print("Comment marker: '%s'" % comment_marker)
            assert len(propfile.units) == 1
            propunit = propfile.units[0]
            assert propunit.comments == ['%s A comment' % comment_marker]

    def test_latin1(self):
        """checks that we handle non-escaped latin1 text"""
        prop_source = u"key=val".encode('latin1')
        prop_store = self.propparse(prop_source)
        assert len(prop_store.units) == 1
        unit = prop_store.units[0]
        assert unit.source == u"val"

    def test_fullspec_delimiters(self):
        """test the full definiation as found in Java docs"""
        proplist = ['Truth = Beauty\n', '       Truth:Beauty', 'Truth                  :Beauty', 'Truth        Beauty']
        for propsource in proplist:
            propfile = self.propparse(propsource)
            propunit = propfile.units[0]
            print(propunit)
            assert propunit.name == "Truth"
            assert propunit.source == "Beauty"

    def test_fullspec_escaped_key(self):
        """Escaped delimeters can be in the key"""
        prop_source = u"\:\="
        prop_store = self.propparse(prop_source)
        assert len(prop_store.units) == 1
        unit = prop_store.units[0]
        print(unit)
        assert unit.name == u"\:\="

    def test_fullspec_line_continuation(self):
        """Whitespace delimiter and pre whitespace in line continuation are dropped"""
        prop_source = ur"""fruits                           apple, banana, pear, \
                                  cantaloupe, watermelon, \
                                  kiwi, mango
"""
        prop_store = self.propparse(prop_source)
        print(prop_store)
        assert len(prop_store.units) == 1
        unit = prop_store.units[0]
        print(unit)
        assert properties._find_delimiter(prop_source, [u"=", u":", u" "]) == (' ', 6)
        assert unit.name == u"fruits"
        assert unit.source == u"apple, banana, pear, cantaloupe, watermelon, kiwi, mango"

    def test_fullspec_key_without_value(self):
        """A key can have no value in which case the value is the empty string"""
        prop_source = u"cheeses"
        prop_store = self.propparse(prop_source)
        assert len(prop_store.units) == 1
        unit = prop_store.units[0]
        print(unit)
        assert unit.name == u"cheeses"
        assert unit.source == u""

    def test_mac_strings(self):
        """test various items used in Mac OS X strings files"""
        propsource = ur'''"I am a \"key\"" = "I am a \"value\"";'''.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == ur'I am a "key"'
        assert propunit.source.encode('utf-8') == u'I am a "value"'

    def test_mac_strings_unicode(self):
        """Ensure we can handle Unicode"""
        propsource = ur'''"I am a key" = "I am a value";'''.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == ur'I am a key'
        assert propfile.personality.encode(propunit.source) == u'I am a value'

    def test_mac_strings_newlines(self):
        """test newlines \n within a strings files"""
        propsource = ur'''"key" = "value\nvalue";'''.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == u'key'
        assert propunit.source.encode('utf-8') == u'value\nvalue'
        assert propfile.personality.encode(propunit.source) == ur'value\nvalue'

    def test_mac_strings_comments(self):
        """test .string comment types"""
        propsource = ur'''/* Comment */
// Comment
"key" = "value";'''.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == u'key'
        assert propunit.source.encode('utf-8') == u'value'
        assert propunit.getnotes() == u"/* Comment */\n// Comment"

    def test_mac_strings_multilines_comments(self):
        """test .string multiline comments"""
        propsource = (u'/* Foo\n'
                      u'Bar\n'
                      u'Baz */\n'
                      u'"key" = "value"').encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == u'key'
        assert propunit.source.encode('utf-8') == u'value'
        assert propunit.getnotes() == u"/* Foo\nBar\nBaz */"

    def test_mac_strings_comments_dropping(self):
        """.string generic (and unuseful) comments should be dropped"""
        propsource = ur'''/* No comment provided by engineer. */
"key" = "value";'''.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == u'key'
        assert propunit.source.encode('utf-8') == u'value'
        assert propunit.getnotes() == u""

    def test_mac_strings_quotes(self):
        """test that parser unescapes characters used as wrappers"""
        propsource = ur'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        propunit = propfile.units[0]
        assert propunit.name == ur'key with "quotes"'
        assert propunit.value == ur'value with "quotes"'

    def test_mac_strings_serialization(self):
        """test that serializer quotes mac strings properly"""
        propsource = ur'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
        propfile = self.propparse(propsource, personality="strings")
        # we don't care about leading and trailing newlines and zero bytes
        # in the assert, we just want to make sure that
        # - all quotes are in place
        # - quotes inside are escaped
        # - for the sake of beauty a pair of spaces encloses the equal mark
        # - every line ends with ";"
        assert str(propfile.units[0]).strip('\n\x00') == propsource.strip('\n\x00')
        assert str(propfile).strip('\n\x00') == propsource.strip('\n\x00')

    def test_override_encoding(self):
        """test that we can override the encoding of a properties file"""
        propsource = u"key = value".encode("cp1252")
        propfile = self.propparse(propsource, personality="strings", encoding="cp1252")
        assert len(propfile.units) == 1
        propunit = propfile.units[0]
        assert propunit.name == u'key'
        assert propunit.source == u'value'

    def test_trailing_comments(self):
        """test that we handle non-unit data at the end of a file"""
        propsource = u"key = value\n# END"
        propfile = self.propparse(propsource)
        assert len(propfile.units) == 2
        propunit = propfile.units[1]
        assert propunit.name == u''
        assert propunit.source == u''
        assert propunit.getnotes() == u"# END"

    def test_utf16_byte_order_mark(self):
        """test that BOM appears in the resulting text once only"""
        propsource = u"key1 = value1\nkey2 = value2\n".encode('utf-16')
        propfile = self.propparse(propsource, encoding='utf-16')
        result = str(propfile)
        bom = propsource[:2]
        assert result.startswith(bom)
        assert bom not in result[2:]

    def test_raise_ioerror_if_cannot_detect_encoding(self):
        """Test that IOError is thrown if file encoding cannot be detected."""
        propsource = u"key = ".encode("cp1250")
        with raises(IOError):
            self.propparse(propsource, personality="strings")

########NEW FILE########
__FILENAME__ = test_pypo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from pytest import raises

from translate.misc import wStringIO
from translate.misc.multistring import multistring
from translate.storage import pypo, test_po


class TestHelpers():
    def test_unescape(self):
        assert pypo.unescape(r"koei") == "koei"
        assert pypo.unescape(r"koei\n") == "koei\n"
        assert pypo.unescape(r"koei\\") == "koei\\"
        assert pypo.unescape(r"koei\"") == "koei\""
        assert pypo.unescape(r"koei\r") == "koei\r"

        assert pypo.unescape(r"\nkoei\n") == "\nkoei\n"
        assert pypo.unescape(r"\\koei\\") == "\\koei\\"
        assert pypo.unescape(r"\"koei\"") == "\"koei\""
        assert pypo.unescape(r"\rkoei\r") == "\rkoei\r"

        assert pypo.unescape(r"\n\nkoei\n") == "\n\nkoei\n"
        assert pypo.unescape(r"\\\nkoei\\\n") == "\\\nkoei\\\n"
        assert pypo.unescape(r"\"\\koei\"\\") == "\"\\koei\"\\"
        assert pypo.unescape(r"\\\rkoei\r\\") == "\\\rkoei\r\\"

    def test_quoteforpo(self):
        """Special escaping routine to manage newlines and linewrap in PO"""
        # Simple case
        assert pypo.quoteforpo("Some test") == ['"Some test"']
        # Newline handling
        assert pypo.quoteforpo("One\nTwo\n") == ['""', '"One\\n"', '"Two\\n"']
        # First line wrapping
        assert pypo.quoteforpo("A very long sentence. A very long sentence. A very long sentence. A ver") == \
                             ['"A very long sentence. A very long sentence. A very long sentence. A ver"']
        assert pypo.quoteforpo("A very long sentence. A very long sentence. A very long sentence. A very") == \
                              ['""',
                               '"A very long sentence. A very long sentence. A very long sentence. A very"']
        # Long line with a newline
        assert pypo.quoteforpo("A very long sentence. A very long sentence. A very long sentence. A very lon\n") == \
                             ['""', '"A very long sentence. A very long sentence. A very long sentence. A very "', '"lon\\n"']
        assert pypo.quoteforpo("A very long sentence. A very long sentence. A very long sentence. A very 123\n") == \
                             ['""', '"A very long sentence. A very long sentence. A very long sentence. A very "', '"123\\n"']
        # Special 77 char failure.
        assert pypo.quoteforpo("Ukuba uyayiqonda into eyenzekayo, \nungaxelela i-&brandShortName; ukuba iqalise ukuthemba ufaniso lwale sayithi. \n<b>Nokuba uyayithemba isayithi, le mposiso isenokuthetha ukuba   kukho umntu \nobhucabhuca ukudibanisa kwakho.</b>") == \
                             ['""',
                              '"Ukuba uyayiqonda into eyenzekayo, \\n"',
                              '"ungaxelela i-&brandShortName; ukuba iqalise ukuthemba ufaniso lwale sayithi. "',
                              '"\\n"',
                              '"<b>Nokuba uyayithemba isayithi, le mposiso isenokuthetha ukuba   kukho umntu "',
                              '"\\n"',
                              '"obhucabhuca ukudibanisa kwakho.</b>"']

    def test_quoteforpo_escaped_quotes(self):
        """Ensure that we don't break \" in two when wrapping

        See :bug:`3140`
        """
        assert pypo.quoteforpo('''You can get a copy of your Recovery Key by going to &syncBrand.shortName.label; Options on your other device, and selecting  "My Recovery Key" under "Manage Account".''') == [u'""', u'"You can get a copy of your Recovery Key by going to "', u'"&syncBrand.shortName.label; Options on your other device, and selecting  \\""', u'"My Recovery Key\\" under \\"Manage Account\\"."']


class TestPYPOUnit(test_po.TestPOUnit):
    UnitClass = pypo.pounit

    def test_plurals(self):
        """Tests that plurals are handled correctly."""
        unit = self.UnitClass("Cow")
        unit.msgid_plural = ['"Cows"']
        assert isinstance(unit.source, multistring)
        assert unit.source.strings == ["Cow", "Cows"]
        assert unit.source == "Cow"

        unit.target = ["Koei", "Koeie"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == ["Koei", "Koeie"]
        assert unit.target == "Koei"

        unit.target = {0: "Koei", 3: "Koeie"}
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == ["Koei", "Koeie"]
        assert unit.target == "Koei"

        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
        assert isinstance(unit.target, multistring)
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
        assert unit.target == u"Sk\u00ear"

    def test_plural_reduction(self):
        """checks that reducing the number of plurals supplied works"""
        unit = self.UnitClass("Tree")
        unit.msgid_plural = ['"Trees"']
        assert isinstance(unit.source, multistring)
        assert unit.source.strings == ["Tree", "Trees"]
        unit.target = multistring(["Boom", "Bome", "Baie Bome"])
        assert isinstance(unit.source, multistring)
        assert unit.target.strings == ["Boom", "Bome", "Baie Bome"]
        unit.target = multistring(["Boom", "Bome"])
        assert unit.target.strings == ["Boom", "Bome"]
        unit.target = "Boom"
        # FIXME: currently assigning the target to the same as the first string won't change anything
        # we need to verify that this is the desired behaviour...
        assert unit.target.strings == ["Boom"]
        unit.target = "Een Boom"
        assert unit.target.strings == ["Een Boom"]

    def test_notes(self):
        """tests that the generic notes API works"""
        unit = self.UnitClass("File")
        unit.addnote("Which meaning of file?")
        assert str(unit) == '# Which meaning of file?\nmsgid "File"\nmsgstr ""\n'
        unit.addnote("Verb", origin="programmer")
        assert str(unit) == '# Which meaning of file?\n#. Verb\nmsgid "File"\nmsgstr ""\n'
        unit.addnote("Thank you", origin="translator")
        assert str(unit) == '# Which meaning of file?\n# Thank you\n#. Verb\nmsgid "File"\nmsgstr ""\n'

        assert unit.getnotes("developer") == "Verb"
        assert unit.getnotes("translator") == "Which meaning of file?\nThank you"
        assert unit.getnotes() == "Which meaning of file?\nThank you\nVerb"
        assert raises(ValueError, unit.getnotes, "devteam")

    def test_notes_withcomments(self):
        """tests that when we add notes that look like comments that we treat them properly"""
        unit = self.UnitClass("File")
        unit.addnote("# Double commented comment")
        assert str(unit) == '# # Double commented comment\nmsgid "File"\nmsgstr ""\n'
        assert unit.getnotes() == "# Double commented comment"

    def test_wrap_firstlines(self):
        '''tests that we wrap the first line correctly a first line if longer then 71 chars
        as at 71 chars we should align the text on the left and preceed with with a msgid ""'''
        # longest before we wrap text
        str_max = "123456789 123456789 123456789 123456789 123456789 123456789 123456789 1"
        unit = self.UnitClass(str_max)
        expected = 'msgid "%s"\nmsgstr ""\n' % str_max
        print(expected, str(unit))
        assert str(unit) == expected
        # at this length we wrap
        str_wrap = str_max + '2'
        unit = self.UnitClass(str_wrap)
        expected = 'msgid ""\n"%s"\nmsgstr ""\n' % str_wrap
        print(expected, str(unit))
        assert str(unit) == expected

    def test_wrap_on_newlines(self):
        """test that we wrap newlines on a real \n"""
        string = "123456789\n" * 3
        postring = ('"123456789\\n"\n' * 3)[:-1]
        unit = self.UnitClass(string)
        expected = 'msgid ""\n%s\nmsgstr ""\n' % postring
        print(expected, str(unit))
        assert str(unit) == expected

        # Now check for long newlines segments
        longstring = ("123456789 " * 10 + "\n") * 3
        expected = r'''msgid ""
"123456789 123456789 123456789 123456789 123456789 123456789 123456789 "
"123456789 123456789 123456789 \n"
"123456789 123456789 123456789 123456789 123456789 123456789 123456789 "
"123456789 123456789 123456789 \n"
"123456789 123456789 123456789 123456789 123456789 123456789 123456789 "
"123456789 123456789 123456789 \n"
msgstr ""
'''
        unit = self.UnitClass(longstring)
        print(expected, str(unit))
        assert str(unit) == expected

    def test_wrap_on_max_line_length(self):
        """test that we wrap all lines on the maximum line length"""
        string = "1 3 5 7 N " * 11
        expected = 'msgid ""\n%s\nmsgstr ""\n' % '"1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 "\n"7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N "'
        unit = self.UnitClass(string)
        print("Expected:")
        print(expected)
        print("Actual:")
        print(str(unit))
        assert str(unit) == expected

    def test_spacing_max_line(self):
        """Test that the spacing of text is done the same as msgcat."""
        idstring = "Creates a new document using an existing template iiiiiiiiiiiiiiiiiiiiiii or "
        idstring += "opens a sample document."
        expected = '''msgid ""
"Creates a new document using an existing template iiiiiiiiiiiiiiiiiiiiiii or "
"opens a sample document."
msgstr ""
'''
        unit = self.UnitClass(idstring)
        print("Expected:")
        print(expected)
        print("Actual:")
        print(str(unit))
        assert str(unit) == expected


class TestPYPOFile(test_po.TestPOFile):
    StoreClass = pypo.pofile

    def test_combine_msgidcomments(self):
        """checks that we don't get duplicate msgid comments"""
        posource = 'msgid "test me"\nmsgstr ""'
        pofile = self.poparse(posource)
        thepo = pofile.units[0]
        thepo.msgidcomments.append('"_: first comment\\n"')
        thepo.msgidcomments.append('"_: second comment\\n"')
        regenposource = str(pofile)
        assert regenposource.count("_:") == 1

    def test_merge_duplicates_msgctxt(self):
        """checks that merging duplicates works for msgctxt"""
        posource = '#: source1\nmsgid "test me"\nmsgstr ""\n\n#: source2\nmsgid "test me"\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("msgctxt")
        print(pofile)
        assert len(pofile.units) == 2
        assert str(pofile.units[0]).count("source1") == 2
        assert str(pofile.units[1]).count("source2") == 2

    def test_merge_blanks(self):
        """checks that merging adds msgid_comments to blanks"""
        posource = '#: source1\nmsgid ""\nmsgstr ""\n\n#: source2\nmsgid ""\nmsgstr ""\n'
        pofile = self.poparse(posource)
        assert len(pofile.units) == 2
        pofile.removeduplicates("merge")
        assert len(pofile.units) == 2
        print(pofile.units[0].msgidcomments)
        print(pofile.units[1].msgidcomments)
        assert pypo.unquotefrompo(pofile.units[0].msgidcomments) == "_: source1\n"
        assert pypo.unquotefrompo(pofile.units[1].msgidcomments) == "_: source2\n"

    def test_output_str_unicode(self):
        """checks that we can str(element) which is in unicode"""
        posource = u'''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
        pofile = self.StoreClass(wStringIO.StringIO(posource.encode("UTF-8")), encoding="UTF-8")
        assert len(pofile.units) == 1
        print(str(pofile))
        thepo = pofile.units[0]
        assert str(thepo) == posource.encode("UTF-8")
        # extra test: what if we set the msgid to a unicode? this happens in prop2po etc
        thepo.source = u"Norwegian Bokm\xe5l"
        assert str(thepo) == posource.encode("UTF-8")
        # Now if we set the msgstr to Unicode
        # this is an escaped half character (1/2)
        halfstr = "\xbd ...".decode("latin-1")
        thepo.target = halfstr
        assert halfstr in str(thepo).decode("UTF-8")
        thepo.target = halfstr.encode("UTF-8")
        assert halfstr.encode("UTF-8") in str(thepo)

    def test_posections(self):
        """checks the content of all the expected sections of a PO message"""
        posource = '# other comment\n#. automatic comment\n#: source comment\n#, fuzzy\nmsgid "One"\nmsgstr "Een"\n'
        pofile = self.poparse(posource)
        print(pofile)
        assert len(pofile.units) == 1
        assert str(pofile) == posource
        assert pofile.units[0].othercomments == ["# other comment\n"]
        assert pofile.units[0].automaticcomments == ["#. automatic comment\n"]
        assert pofile.units[0].sourcecomments == ["#: source comment\n"]
        assert pofile.units[0].typecomments == ["#, fuzzy\n"]

    def test_unassociated_comments(self):
        """tests behaviour of unassociated comments."""
        oldsource = '# old lonesome comment\n\nmsgid "one"\nmsgstr "een"\n'
        oldfile = self.poparse(oldsource)
        print(str(oldfile))
        assert len(oldfile.units) == 1

    def test_prevmsgid_parse(self):
        """checks that prevmsgid (i.e. #|) is parsed and saved correctly"""
        posource = r'''msgid ""
msgstr ""
"PO-Revision-Date: 2006-02-09 23:33+0200\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8-bit\n"

#, fuzzy
#| msgid "trea"
msgid "tree"
msgstr "boom"

#| msgid "trea"
#| msgid_plural "treas"
msgid "tree"
msgid_plural "trees"
msgstr[0] "boom"
msgstr[1] "bome"

#| msgctxt "context 1"
#| msgid "tast"
msgctxt "context 1a"
msgid "test"
msgstr "toets"

#| msgctxt "context 2"
#| msgid "tast"
#| msgid_plural "tasts"
msgctxt "context 2a"
msgid "test"
msgid_plural "tests"
msgstr[0] "toet"
msgstr[1] "toetse"
'''

        pofile = self.poparse(posource)

        assert pofile.units[1].prev_msgctxt == []
        assert pofile.units[1].prev_source == multistring([u"trea"])

        assert pofile.units[2].prev_msgctxt == []
        assert pofile.units[2].prev_source == multistring([u"trea", u"treas"])

        assert pofile.units[3].prev_msgctxt == [u'"context 1"']
        assert pofile.units[3].prev_source == multistring([u"tast"])

        assert pofile.units[4].prev_msgctxt == [u'"context 2"']
        assert pofile.units[4].prev_source == multistring([u"tast", u"tasts"])

        assert str(pofile) == posource

########NEW FILE########
__FILENAME__ = test_qm
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pytest

from translate.storage import qm, test_base


class TestQtUnit(test_base.TestTranslationUnit):
    UnitClass = qm.qmunit


class TestQtFile(test_base.TestTranslationStore):
    StoreClass = qm.qmfile

    def test_parse(self):
        # self.reparse relies on __str__ to be output and then parsed
        # qm.py does not implement __str__ but returns u''
        pass

    def test_save(self):
        # QM does not implement saving
        assert pytest.raises(Exception, self.StoreClass.savefile,
                           self.StoreClass())

    def test_files(self):
        # QM does not implement saving
        assert pytest.raises(Exception, self.StoreClass.savefile,
                           self.StoreClass())

    def test_nonascii(self):
        # QM does not implement serialising
        assert pytest.raises(Exception, self.StoreClass.__str__,
                           self.StoreClass())

    def test_add(self):
        # QM does not implement serialising
        assert pytest.raises(Exception, self.StoreClass.__str__,
                           self.StoreClass())

########NEW FILE########
__FILENAME__ = test_qph
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Tests for Qt Linguist phase book storage class"""

from translate.storage import qph, test_base
from translate.storage.placeables import parse, xliff


xliffparsers = []
for attrname in dir(xliff):
    attr = getattr(xliff, attrname)
    if type(attr) is type and \
       attrname not in ('XLIFFPlaceable') and \
       hasattr(attr, 'parse') and \
       attr.parse is not None:
        xliffparsers.append(attr.parse)


def rich_parse(s):
    return parse(s, xliffparsers)


class TestQphUnit(test_base.TestTranslationUnit):
    UnitClass = qph.QphUnit


class TestQphFile(test_base.TestTranslationStore):
    StoreClass = qph.QphFile

    def test_basic(self):
        qphfile = self.StoreClass()
        assert qphfile.units == []
        qphfile.addsourceunit("Bla")
        assert len(qphfile.units) == 1
        newfile = qph.QphFile.parsestring(str(qphfile))
        print(str(qphfile))
        assert len(newfile.units) == 1
        assert newfile.units[0].source == "Bla"
        assert newfile.findunit("Bla").source == "Bla"
        assert newfile.findunit("dit") is None

    def test_source(self):
        qphfile = qph.QphFile()
        qphunit = qphfile.addsourceunit("Concept")
        qphunit.source = "Term"
        newfile = qph.QphFile.parsestring(str(qphfile))
        print(str(qphfile))
        assert newfile.findunit("Concept") is None
        assert newfile.findunit("Term") is not None

    def test_target(self):
        qphfile = qph.QphFile()
        qphunit = qphfile.addsourceunit("Concept")
        qphunit.target = "Konsep"
        newfile = qph.QphFile.parsestring(str(qphfile))
        print(str(qphfile))
        assert newfile.findunit("Concept").target == "Konsep"

    def test_language(self):
        """Check that we can get and set language and sourcelanguage
        in the header"""
        qphstr = '''<!DOCTYPE QPH>
<QPH language="fr" sourcelanguage="de">
</QPH>
'''
        qphfile = qph.QphFile.parsestring(qphstr)
        assert qphfile.gettargetlanguage() == 'fr'
        assert qphfile.getsourcelanguage() == 'de'
        qphfile.settargetlanguage('pt_BR')
        assert 'pt_BR' in str(qphfile)
        assert qphfile.gettargetlanguage() == 'pt-br'
        # We convert en_US to en
        qphstr = '''<!DOCTYPE QPH>
<QPH language="fr" sourcelanguage="en_US">
</QPH>
'''
        qphfile = qph.QphFile.parsestring(qphstr)
        assert qphfile.getsourcelanguage() == 'en'

########NEW FILE########
__FILENAME__ = test_rc
from translate.storage import rc


def test_escaping():
    """test escaping Windows Resource files to Python strings"""
    assert rc.escape_to_python('''First line \
second line''') == "First line second line"
    assert rc.escape_to_python("A newline \\n in a string") == "A newline \n in a string"
    assert rc.escape_to_python("A tab \\t in a string") == "A tab \t in a string"
    assert rc.escape_to_python("A backslash \\\\ in a string") == "A backslash \\ in a string"
    assert rc.escape_to_python(r'''First line " \
 "second line''') == "First line second line"

########NEW FILE########
__FILENAME__ = test_statsdb
#!/usr/bin/env python

import os
import os.path

from translate.filters import checks
from translate.storage import factory, statsdb


fr_terminology_extract = r"""
msgid ""
msgstr ""
"Project-Id-Version: GnomeGlossary\n"
"POT-Creation-Date: 2002-05-22 23:40+0200\n"
"PO-Revision-Date: 2002-05-22 23:38+0200\n"
"Last-Translator: Christophe Merlet (RedFox) <christophe@merlet.net>\n"
"Language-Team: GNOME French Team <gnomefr@traduc.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=ISO-8859-1\n"
"Content-Transfer-Encoding: 8bit\n"

#. "English Definition"
msgid "Term"
msgstr "Terme"

#. "To terminate abruptly a processing activity in a computer system because it is impossible or undesirable for the activity to procees."
msgid "abort"
msgstr "annuler"
"""

jtoolkit_extract = r"""
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2005-06-13 14:54-0500\n"
"PO-Revision-Date: 2007-05-04 19:54+0200\n"
"Last-Translator: F Wolff <friedel@translate.org.za>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Pootle 1.0rc1\n"
"Generated-By: pygettext.py 1.5\n"

#: web/server.py:57
#, python-format
#, fuzzy
msgid "Login for %s"
msgstr "Meld aan vir %s"

#: web/server.py:91
msgid "Cancel this action and start a new session"
msgstr "Kanselleer hierdie aksie en begin 'n nuwe sessie"

#: web/server.py:92
msgid "Instead of confirming this action, log out and start from scratch"
msgstr "Meld af en begin op nuut eerder as om hierdie aksie te bevestig."

#: web/server.py:97
#, fuzzy
msgid "Exit application"
msgstr "Verlaat toepassing"

#: web/server.py:98
msgid "Exit this application and return to the parent application"
msgstr "Verlaat hierdie toepassing en gaan terug na die ouertoepassing"

#: web/server.py:105
msgid ", please confirm login"
msgstr ""
"""


def rm_rf(path):
    for dirpath, _, filenames in os.walk(path):
        for filename in filenames:
            os.remove(os.path.join(dirpath, filename))
    os.rmdir(dirpath)


class TestStatsDb:

    def remove_dirs(self, path):
        if os.path.exists(path):
            rm_rf(path)

    def get_test_path(self, method):
        return os.path.realpath("%s_%s" % (self.__class__.__name__, method.__name__))

    def setup_method(self, method):
        """Allocates a unique self.filename for the method, making sure it doesn't exist"""
        self.path = self.get_test_path(method)
        self.remove_dirs(self.path)
        os.makedirs(self.path)

    def teardown_method(self, method):
        """Makes sure that if self.filename was created by the method, it is cleaned up"""
        self.remove_dirs(self.path)

    def setup_file_and_db(self, file_contents=fr_terminology_extract):
        cache = statsdb.StatsCache(os.path.join(self.path, "stats.db"))
        filename = os.path.join(self.path, "test.po")
        open(filename, "w").write(file_contents)
        f = factory.getobject(filename)
        return f, cache

    def test_getfileid_recache_cached_unit(self):
        """checks that a simple oo entry is parsed correctly"""
        checker = checks.UnitChecker()
        f, cache = self.setup_file_and_db()
        cache.filestats(f.filename, checker)
        state = cache.recacheunit(f.filename, checker, f.units[1])
        assert state == ['translated', 'total']

    def test_unitstats(self):
        f, cache = self.setup_file_and_db(jtoolkit_extract)
        u = cache.unitstats(f.filename)
        assert u['sourcewordcount'] == [3, 8, 11, 2, 9, 3]

    def test_filestats(self):
        f, cache = self.setup_file_and_db(jtoolkit_extract)
        s = cache.filestats(f.filename, checks.UnitChecker())
        assert s['translated'] == [2, 3, 5]
        assert s['fuzzy'] == [1, 4]
        assert s['untranslated'] == [6]
        assert s['total'] == [1, 2, 3, 4, 5, 6]

    def make_file_and_return_id(self, cache, filename):
        cache.cur.execute("""
            SELECT fileid, st_mtime, st_size FROM files
            WHERE path=?;""", (os.path.realpath(filename),))
        return cache.cur.fetchone()

    def test_if_cached_after_filestats(self):
        f, cache = self.setup_file_and_db(jtoolkit_extract)
        cache.filestats(f.filename, checks.UnitChecker())
        assert self.make_file_and_return_id(cache, f.filename) is not None

    def test_if_cached_after_unitstats(self):
        f, cache = self.setup_file_and_db(jtoolkit_extract)
        cache.unitstats(f.filename, checks.UnitChecker())
        assert self.make_file_and_return_id(cache, f.filename) is not None

    def test_singletonness(self):
        f1, cache1 = self.setup_file_and_db(jtoolkit_extract)
        f2, cache2 = self.setup_file_and_db(fr_terminology_extract)
        assert cache1 == cache2

########NEW FILE########
__FILENAME__ = test_tbx
#!/usr/bin/env python

from translate.storage import tbx, test_base


class TestTBXUnit(test_base.TestTranslationUnit):
    UnitClass = tbx.tbxunit


class TestTBXfile(test_base.TestTranslationStore):
    StoreClass = tbx.tbxfile

    def test_basic(self):
        tbxfile = tbx.tbxfile()
        assert tbxfile.units == []
        tbxfile.addsourceunit("Bla")
        assert len(tbxfile.units) == 1
        newfile = tbx.tbxfile.parsestring(str(tbxfile))
        print(str(tbxfile))
        assert len(newfile.units) == 1
        assert newfile.units[0].source == "Bla"
        assert newfile.findunit("Bla").source == "Bla"
        assert newfile.findunit("dit") is None

    def test_source(self):
        tbxfile = tbx.tbxfile()
        tbxunit = tbxfile.addsourceunit("Concept")
        tbxunit.source = "Term"
        newfile = tbx.tbxfile.parsestring(str(tbxfile))
        print(str(tbxfile))
        assert newfile.findunit("Concept") is None
        assert newfile.findunit("Term") is not None

    def test_target(self):
        tbxfile = tbx.tbxfile()
        tbxunit = tbxfile.addsourceunit("Concept")
        tbxunit.target = "Konsep"
        newfile = tbx.tbxfile.parsestring(str(tbxfile))
        print(str(tbxfile))
        assert newfile.findunit("Concept").target == "Konsep"

########NEW FILE########
__FILENAME__ = test_tiki
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# tiki unit tests
# Author: Wil Clouser <wclouser@mozilla.com>
# Date: 2008-12-01
from translate.storage import tiki


class TestTikiUnit:

    def test_locations(self):
        unit = tiki.TikiUnit("one")
        unit.addlocation('blah')
        assert unit.getlocations() == []
        unit.addlocation('unused')
        assert unit.getlocations() == ['unused']

    def test_to_unicode(self):
        unit = tiki.TikiUnit("one")
        unit.settarget('two')
        assert unicode(unit) == '"one" => "two",\n'

        unit2 = tiki.TikiUnit("one")
        unit2.settarget('two')
        unit2.addlocation('untranslated')
        assert unicode(unit2) == '// "one" => "two",\n'


class TestTikiStore:

    def test_parse_simple(self):
        tikisource = r'"Top authors" => "Top autoren",'
        tikifile = tiki.TikiStore(tikisource)
        assert len(tikifile.units) == 1
        assert tikifile.units[0].source == "Top authors"
        assert tikifile.units[0].target == "Top autoren"

    def test_parse_encode(self):
        """Make sure these tiki special symbols come through correctly"""
        tikisource = r'"test: |\n \r \t \\ \$ \"|" => "test: |\n \r \t \\ \$ \"|",'
        tikifile = tiki.TikiStore(tikisource)
        assert tikifile.units[0].source == r"test: |\n \r \t \\ \$ \"|"
        assert tikifile.units[0].target == r"test: |\n \r \t \\ \$ \"|"

    def test_parse_locations(self):
        """This function will test to make sure the location matching is working.  It
        tests that locations are detected, the default "translated" case, and that
        "unused" lines can start with //"""
        tikisource = """
"zero_source" => "zero_target",
// ### Start of unused words
"one_source" => "one_target",
// ### end of unused words
"two_source" => "two_target",
// ### start of untranslated words
// "three_source" => "three_target",
// ### end of untranslated words
"four_source" => "four_target",
// ### start of possibly untranslated words
"five_source" => "five_target",
// ### end of possibly untranslated words
"six_source" => "six_target",
        """
        tikifile = tiki.TikiStore(tikisource)
        assert len(tikifile.units) == 7
        assert tikifile.units[0].location == ["translated"]
        assert tikifile.units[1].location == ["unused"]
        assert tikifile.units[2].location == ["translated"]
        assert tikifile.units[3].location == ["untranslated"]
        assert tikifile.units[4].location == ["translated"]
        assert tikifile.units[5].location == ["possiblyuntranslated"]
        assert tikifile.units[6].location == ["translated"]

    def test_parse_ignore_extras(self):
        """Tests that we ignore extraneous lines"""
        tikisource = """<?php
$lang = Array(
"zero_source" => "zero_target",
// ###
// this is a blank line:

"###end###"=>"###end###");
        """
        tikifile = tiki.TikiStore(tikisource)
        assert len(tikifile.units) == 1
        assert tikifile.units[0].source == "zero_source"
        assert tikifile.units[0].target == "zero_target"

########NEW FILE########
__FILENAME__ = test_tmx
#!/usr/bin/env python

from translate.misc import wStringIO
from translate.storage import test_base, tmx


class TestTMXUnit(test_base.TestTranslationUnit):
    UnitClass = tmx.tmxunit


class TestTMXUnitFromParsedString(TestTMXUnit):
    tmxsource = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE tmx
  SYSTEM 'tmx14.dtd'>
<tmx version="1.4">
        <header adminlang="en" creationtool="Translate Toolkit - po2tmx" creationtoolversion="1.0beta" datatype="PlainText" o-tmf="UTF-8" segtype="sentence" srclang="en"/>
        <body>
                <tu>
                        <tuv xml:lang="en">
                                <seg>Test String</seg>
                        </tuv>
                </tu>
        </body>
</tmx>'''

    def setup_method(self, method):
        store = tmx.tmxfile.parsestring(self.tmxsource)
        self.unit = store.units[0]


class TestTMXfile(test_base.TestTranslationStore):
    StoreClass = tmx.tmxfile

    def tmxparse(self, tmxsource):
        """helper that parses tmx source without requiring files"""
        dummyfile = wStringIO.StringIO(tmxsource)
        print(tmxsource)
        tmxfile = tmx.tmxfile(dummyfile)
        return tmxfile

    def test_translate(self):
        tmxfile = tmx.tmxfile()
        assert tmxfile.translate("Anything") is None
        tmxfile.addtranslation("A string of characters", "en", "'n String karakters", "af")
        assert tmxfile.translate("A string of characters") == "'n String karakters"

    def test_addtranslation(self):
        """tests that addtranslation() stores strings correctly"""
        tmxfile = tmx.tmxfile()
        tmxfile.addtranslation("A string of characters", "en", "'n String karakters", "af")
        newfile = self.tmxparse(str(tmxfile))
        print(str(tmxfile))
        assert newfile.translate("A string of characters") == "'n String karakters"

    def test_withcomment(self):
        """tests that addtranslation() stores string's comments correctly"""
        tmxfile = tmx.tmxfile()
        tmxfile.addtranslation("A string of chars",
                               "en", "'n String karakters", "af", "comment")
        newfile = self.tmxparse(str(tmxfile))
        print(str(tmxfile))
        assert newfile.findunit("A string of chars").getnotes() == "comment"

    def test_withnewlines(self):
        """test addtranslation() with newlines"""
        tmxfile = tmx.tmxfile()
        tmxfile.addtranslation("First line\nSecond line", "en", "Eerste lyn\nTweede lyn", "af")
        newfile = self.tmxparse(str(tmxfile))
        print(str(tmxfile))
        assert newfile.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"

    def test_xmlentities(self):
        """Test that the xml entities '&' and '<'  are escaped correctly"""
        tmxfile = tmx.tmxfile()
        tmxfile.addtranslation("Mail & News", "en", "Nuus & pos", "af")
        tmxfile.addtranslation("Five < ten", "en", "Vyf < tien", "af")
        xmltext = str(tmxfile)
        print("The generated xml:")
        print(xmltext)
        assert tmxfile.translate('Mail & News') == 'Nuus & pos'
        assert xmltext.index('Mail &amp; News')
        assert xmltext.find('Mail & News') == -1
        assert tmxfile.translate('Five < ten') == 'Vyf < tien'
        assert xmltext.index('Five &lt; ten')
        assert xmltext.find('Five < ten') == -1

########NEW FILE########
__FILENAME__ = test_trados
# -*- coding: utf-8 -*-

from pytest import importorskip
importorskip("bs4")

from translate.storage import trados


def test_unescape():
    # NBSP
    assert trados.unescape(u"Ordre du jour\\~:") == u"Ordre du jour\u00a0:"
    assert trados.unescape(u"Association for Road Safety \\endash  Conference") == u"Association for Road Safety   Conference"


def test_escape():
    # NBSP
    assert trados.escape(u"Ordre du jour\u00a0:") == u"Ordre du jour\\~:"
    assert trados.escape(u"Association for Road Safety   Conference") == u"Association for Road Safety \\endash  Conference"

#@mark.xfail(reason="Lots to implement")
#class TestTradosTxtTmUnit(test_base.TestTranslationUnit):
#    UnitClass = trados.TradosUnit
#
#@mark.xfail(reason="Lots to implement")
#class TestTrodosTxtTmFile(test_base.TestTranslationStore):
#    StoreClass = trados.TradosTxtTmFile

########NEW FILE########
__FILENAME__ = test_ts
#!/usr/bin/env python

from translate.storage import ts


class TestTS:

    def test_construct(self):
        tsfile = ts.QtTsParser()
        tsfile.addtranslation("ryan", "Bread", "Brood", "Wit", createifmissing=True)

########NEW FILE########
__FILENAME__ = test_ts2
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Tests for Qt Linguist storage class

Reference implementation & tests:
gitorious:qt5-tools/src/qttools/tests/auto/linguist/lconvert/data
"""

from lxml import etree

from translate.misc.multistring import multistring
from translate.storage import test_base, ts2 as ts
from translate.storage.placeables import parse, xliff
from translate.storage.placeables.lisa import xml_to_strelem


xliffparsers = []
for attrname in dir(xliff):
    attr = getattr(xliff, attrname)
    if type(attr) is type and \
       attrname not in ('XLIFFPlaceable') and \
       hasattr(attr, 'parse') and \
       attr.parse is not None:
        xliffparsers.append(attr.parse)


def rich_parse(s):
    return parse(s, xliffparsers)


class TestTSUnit(test_base.TestTranslationUnit):
    UnitClass = ts.tsunit


class TestTSfile(test_base.TestTranslationStore):
    StoreClass = ts.tsfile

    def test_basic(self):
        tsfile = ts.tsfile()
        assert tsfile.units == []
        tsfile.addsourceunit("Bla")
        assert len(tsfile.units) == 1
        newfile = ts.tsfile.parsestring(str(tsfile))
        print(str(tsfile))
        assert len(newfile.units) == 1
        assert newfile.units[0].source == "Bla"
        assert newfile.findunit("Bla").source == "Bla"
        assert newfile.findunit("dit") is None

    def test_source(self):
        tsfile = ts.tsfile()
        tsunit = tsfile.addsourceunit("Concept")
        tsunit.source = "Term"
        newfile = ts.tsfile.parsestring(str(tsfile))
        print(str(tsfile))
        assert newfile.findunit("Concept") is None
        assert newfile.findunit("Term") is not None

    def test_target(self):
        tsfile = ts.tsfile()
        tsunit = tsfile.addsourceunit("Concept")
        tsunit.target = "Konsep"
        newfile = ts.tsfile.parsestring(str(tsfile))
        print(str(tsfile))
        assert newfile.findunit("Concept").target == "Konsep"

    def test_plurals(self):
        """Test basic plurals"""
        tsfile = ts.tsfile()
        tsunit = tsfile.addsourceunit("File(s)")
        tsunit.target = [u"Ler", u"Lers"]
        newfile = ts.tsfile.parsestring(str(tsfile))
        print(str(tsfile))
        checkunit = newfile.findunit("File(s)")
        assert checkunit.target == [u"Ler", u"Lers"]
        assert checkunit.hasplural()

    def test_language(self):
        """Check that we can get and set language and sourcelanguage
        in the header"""
        tsstr = '''<!DOCTYPE TS>
<TS version="2.0" language="fr" sourcelanguage="de">
</TS>
'''
        tsfile = ts.tsfile.parsestring(tsstr)
        assert tsfile.gettargetlanguage() == 'fr'
        assert tsfile.getsourcelanguage() == 'de'
        tsfile.settargetlanguage('pt_BR')
        assert 'pt_BR' in str(tsfile)
        assert tsfile.gettargetlanguage() == 'pt-br'
        # We convert en_US to en
        tsstr = '''<!DOCTYPE TS>
<TS version="2.0" language="fr" sourcelanguage="en_US">
</TS>
'''
        tsfile = ts.tsfile.parsestring(tsstr)
        assert tsfile.getsourcelanguage() == 'en'

    def test_locations(self):
        """test that locations work well"""
        tsstr = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TS>
<TS version="2.0" language="hu">
<context>
    <name>MainWindow</name>
    <message>
        <location filename="../tools/qtconfig/mainwindow.cpp" line="+202"/>
        <source>Desktop Settings (Default)</source>
        <translation>Asztali belltsok (Alaprtelmezett)</translation>
    </message>
    <message>
        <location line="+5"/>
        <source>Choose style and palette based on your desktop settings.</source>
        <translation>Stlus s paletta alap kivlasztsa az asztali belltsokban.</translation>
    </message>
</context>
</TS>
'''
        tsfile = ts.tsfile.parsestring(tsstr)
        assert len(tsfile.units) == 2
        assert tsfile.units[0].getlocations() == ['../tools/qtconfig/mainwindow.cpp:+202']
        assert tsfile.units[1].getlocations() == ['+5']

    def test_merge_with_fuzzies(self):
        """test that merge with fuzzy works well"""
        tsstr1 = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TS>
<TS version="2.0" language="hu">
<context>
    <name>MainWindow</name>
    <message>
        <location filename="../tools/qtconfig/mainwindow.cpp" line="+202"/>
        <source>Desktop Settings (Default)</source>
        <translation type="unfinished">Asztali belltsok (Alaprtelmezett)</translation>
    </message>
    <message>
        <location line="+5"/>
        <source>Choose style and palette based on your desktop settings.</source>
        <translation>Stlus s paletta alap kivlasztsa az asztali belltsokban.</translation>
    </message>
</context>
</TS>
'''

        tsstr2 = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TS>
<TS version="2.0" language="hu">
<context>
    <name>MainWindow</name>
    <message>
        <location filename="../tools/qtconfig/mainwindow.cpp" line="+202"/>
        <source>Desktop Settings (Default)</source>
        <translation type="unfinished"/>
    </message>
    <message>
        <location line="+5"/>
        <source>Choose style and palette based on your desktop settings.</source>
        <translation type="unfinished"/>
    </message>
</context>
</TS>
'''
        tsfile = ts.tsfile.parsestring(tsstr1)
        tsfile2 = ts.tsfile.parsestring(tsstr2)
        assert len(tsfile.units) == 2
        assert len(tsfile2.units) == 2

        tsfile2.units[0].merge(tsfile.units[0])  # fuzzy
        tsfile2.units[1].merge(tsfile.units[1])  # not fuzzy
        assert tsfile2.units[0].isfuzzy()
        assert not tsfile2.units[1].isfuzzy()

    def test_getid(self):
        """test that getid works well"""
        tsstr = """<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TS>
<TS version="2.1">
<context>
    <name>Dialog2</name>
    <message numerus="yes">
        <source>%n files</source>
        <translation type="unfinished">
            <numerusform></numerusform>
        </translation>
    </message>
    <message id="this_is_some_id" numerus="yes">
        <source>%n cars</source>
        <translation type="unfinished">
            <numerusform></numerusform>
        </translation>
    </message>
    <message>
        <source>Age: %1</source>
        <translation type="unfinished"></translation>
    </message>
    <message id="this_is_another_id">
        <source>func3</source>
        <translation type="unfinished"></translation>
    </message>
</context>
</TS>"""

        tsfile = ts.tsfile.parsestring(tsstr)
        assert tsfile.units[0].getid() == "Dialog2%n files"
        assert tsfile.units[1].getid() == "Dialog2\nthis_is_some_id%n cars"
        assert tsfile.units[3].getid() == "Dialog2\nthis_is_another_idfunc3"

########NEW FILE########
__FILENAME__ = test_txt
#!/usr/bin/env python

from translate.misc import wStringIO
from translate.storage import test_monolingual, txt


class TestTxtUnit(test_monolingual.TestMonolingualUnit):
    UnitClass = txt.TxtUnit


class TestTxtFile(test_monolingual.TestMonolingualStore):
    StoreClass = txt.TxtFile

    def txtparse(self, txtsource):
        """helper that parses txt source without requiring files"""
        dummyfile = wStringIO.StringIO(txtsource)
        txtfile = self.StoreClass(dummyfile)
        return txtfile

    def txtregen(self, txtsource):
        """helper that converts txt source to txtfile object and back"""
        return str(self.txtparse(txtsource))

    def test_simpleblock(self):
        """checks that a simple txt block is parsed correctly"""
        txtsource = 'bananas for sale'
        txtfile = self.txtparse(txtsource)
        assert len(txtfile.units) == 1
        assert txtfile.units[0].source == txtsource
        assert self.txtregen(txtsource) == txtsource

    def test_multipleblocks(self):
        """ check that multiple blocks are parsed correctly"""
        txtsource = '''One\nOne\n\nTwo\n---\n\nThree'''
        txtfile = self.txtparse(txtsource)
        assert len(txtfile.units) == 3
        print(txtsource)
        print(str(txtfile))
        print("*%s*" % txtfile.units[0])
        assert str(txtfile) == txtsource
        assert self.txtregen(txtsource) == txtsource

########NEW FILE########
__FILENAME__ = test_utx
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.storage import test_base, utx


class TestUtxUnit(test_base.TestTranslationUnit):
    UnitClass = utx.UtxUnit


class TestUtxFile(test_base.TestTranslationStore):
    StoreClass = utx.UtxFile

########NEW FILE########
__FILENAME__ = test_wordfast
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.storage import test_base, wordfast as wf


class TestWFTime(object):

    def test_timestring(self):
        """Setting and getting times set using a timestring"""
        wftime = wf.WordfastTime()
        assert wftime.timestring is None
        wftime.timestring = "19710820~050000"
        assert wftime.time[:6] == (1971, 8, 20, 5, 0, 0)

    def test_time(self):
        """Setting and getting times set using time tuple"""
        wftime = wf.WordfastTime()
        assert wftime.time is None
        wftime.time = (1999, 3, 27)
        wftime.timestring = "19990327~000000"


class TestWFUnit(test_base.TestTranslationUnit):
    UnitClass = wf.WordfastUnit

    def test_difficult_escapes(self):
        r"""Wordfast files need to perform magic with escapes.

           Wordfast does not accept line breaks in its TM (even though they would be
           valid in CSV) thus we turn \\n into \n and reimplement the base class test but
           eliminate a few of the actual tests.
        """
        unit = self.unit
        specials = ['\\"', '\\ ',
                    '\\\n', '\\\t', '\\\\r', '\\\\"']
        for special in specials:
            unit.source = special
            print("unit.source:", repr(unit.source) + '|')
            print("special:", repr(special) + '|')
            assert unit.source == special

    def test_wordfast_escaping(self):
        """Check handling of &'NN; style escaping"""

        def compare(real, escaped):
            unit = self.UnitClass(real)
            print(real.encode('utf-8'), unit.source.encode('utf-8'))
            assert unit.source == real
            assert unit.dict['source'] == escaped
            unit.target = real
            assert unit.target == real
            assert unit.dict['target'] == escaped
        for escaped, real in wf.WF_ESCAPE_MAP[:16]:  # Only common and Windows, not testing Mac
            compare(real, escaped)
        # Real world cases
        unit = self.UnitClass("Open &File. n Probleem.")
        assert unit.dict['source'] == "Open &'26;File. &'92;n Probleem."

    def test_newlines(self):
        """Wordfast does not like real newlines"""
        unit = self.UnitClass("One\nTwo")
        assert unit.dict['source'] == "One\\nTwo"

    def test_language_setting(self):
        """Check that we can set the target language"""
        unit = self.UnitClass("Test")
        unit.targetlang = "AF"
        assert unit.dict['target-lang'] == 'AF'

    def test_istranslated(self):
        unit = self.UnitClass()
        assert not unit.istranslated()
        unit.source = "Test"
        assert not unit.istranslated()
        unit.target = "Rest"
        assert unit.istranslated()


class TestWFFile(test_base.TestTranslationStore):
    StoreClass = wf.WordfastTMFile

########NEW FILE########
__FILENAME__ = test_xliff
#!/usr/bin/env python

from lxml import etree

from translate.storage import lisa, test_base, xliff
from translate.storage.placeables import StringElem
from translate.storage.placeables.xliff import G, X


class TestXLIFFUnit(test_base.TestTranslationUnit):
    UnitClass = xliff.xliffunit

    def test_markreview(self):
        """Tests if we can mark the unit to need review."""
        unit = self.unit
        # We have to explicitly set the target to nothing, otherwise xliff
        # tests will fail.
        # Can we make it default behavior for the UnitClass?
        unit.target = ""

        unit.addnote("Test note 1", origin="translator")
        unit.addnote("Test note 2", origin="translator")
        original_notes = unit.getnotes(origin="translator")

        assert not unit.isreview()
        unit.markreviewneeded()
        assert unit.isreview()
        unit.markreviewneeded(False)
        assert not unit.isreview()
        assert unit.getnotes(origin="translator") == original_notes
        unit.markreviewneeded(explanation="Double check spelling.")
        assert unit.isreview()
        notes = unit.getnotes(origin="translator")
        assert notes.count("Double check spelling.") == 1

    def test_errors(self):
        """Tests that we can add and retrieve error messages for a unit."""
        unit = self.unit

        assert len(unit.geterrors()) == 0
        unit.adderror(errorname='test1', errortext='Test error message 1.')
        unit.adderror(errorname='test2', errortext='Test error message 2.')
        unit.adderror(errorname='test3', errortext='Test error message 3.')
        assert len(unit.geterrors()) == 3
        assert unit.geterrors()['test1'] == 'Test error message 1.'
        assert unit.geterrors()['test2'] == 'Test error message 2.'
        assert unit.geterrors()['test3'] == 'Test error message 3.'
        unit.adderror(errorname='test1', errortext='New error 1.')
        assert unit.geterrors()['test1'] == 'New error 1.'


class TestXLIFFfile(test_base.TestTranslationStore):
    StoreClass = xliff.xlifffile
    skeleton = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
        <file original="doc.txt" source-language="en-US">
                <body>
                        %s
                </body>
        </file>
</xliff>'''

    def test_basic(self):
        xlifffile = xliff.xlifffile()
        assert xlifffile.units == []
        xlifffile.addsourceunit("Bla")
        assert len(xlifffile.units) == 1
        newfile = xliff.xlifffile.parsestring(str(xlifffile))
        print(str(xlifffile))
        assert len(newfile.units) == 1
        assert newfile.units[0].source == "Bla"
        assert newfile.findunit("Bla").source == "Bla"
        assert newfile.findunit("dit") is None

    def test_namespace(self):
        """Check that we handle namespaces other than the default correctly."""
        xlfsource = '''<?xml version="1.0" encoding="utf-8"?>
<xliff:xliff version="1.2" xmlns:xliff="urn:oasis:names:tc:xliff:document:1.2">
    <xliff:file original="doc.txt" source-language="en-US">
        <xliff:body>
            <xliff:trans-unit id="1">
                <xliff:source>File 1</xliff:source>
            </xliff:trans-unit>
        </xliff:body>
    </xliff:file>
</xliff:xliff>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        print(str(xlifffile))
        assert xlifffile.units[0].source == "File 1"

    def test_rich_source(self):
        xlifffile = xliff.xlifffile()
        xliffunit = xlifffile.addsourceunit(u'')

        # Test 1
        xliffunit.rich_source = [StringElem([u'foo', X(id='bar'), u'baz'])]
        source_dom_node = xliffunit.getlanguageNode(None, 0)
        x_placeable = source_dom_node[0]

        assert source_dom_node.text == 'foo'

        assert x_placeable.tag == u'x'
        assert x_placeable.attrib['id'] == 'bar'
        assert x_placeable.tail == 'baz'

        xliffunit.rich_source[0].print_tree(2)
        print(xliffunit.rich_source)
        assert xliffunit.rich_source == [StringElem([StringElem(u'foo'), X(id='bar'), StringElem(u'baz')])]

        # Test 2
        xliffunit.rich_source = [StringElem([u'foo', u'baz', G(id='oof', sub=[G(id='zab', sub=[u'bar', u'rab'])])])]
        source_dom_node = xliffunit.getlanguageNode(None, 0)
        g_placeable = source_dom_node[0]
        nested_g_placeable = g_placeable[0]

        assert source_dom_node.text == u'foobaz'

        assert g_placeable.tag == u'g'
        assert g_placeable.text is None
        assert g_placeable.attrib[u'id'] == u'oof'
        assert g_placeable.tail is None

        assert nested_g_placeable.tag == u'g'
        assert nested_g_placeable.text == u'barrab'
        assert nested_g_placeable.attrib[u'id'] == u'zab'
        assert nested_g_placeable.tail is None

        rich_source = xliffunit.rich_source
        rich_source[0].print_tree(2)
        assert rich_source == [StringElem([u'foobaz', G(id='oof', sub=[G(id='zab', sub=[u'barrab'])])])]

    def test_rich_target(self):
        xlifffile = xliff.xlifffile()
        xliffunit = xlifffile.addsourceunit(u'')

        # Test 1
        xliffunit.set_rich_target([StringElem([u'foo', X(id='bar'), u'baz'])], u'fr')
        target_dom_node = xliffunit.getlanguageNode(None, 1)
        x_placeable = target_dom_node[0]

        assert target_dom_node.text == 'foo'
        assert x_placeable.tag == u'x'
        assert x_placeable.attrib['id'] == 'bar'
        assert x_placeable.tail == 'baz'

        # Test 2
        xliffunit.set_rich_target([StringElem([u'foo', u'baz', G(id='oof', sub=[G(id='zab', sub=[u'bar', u'rab'])])])], u'fr')
        target_dom_node = xliffunit.getlanguageNode(None, 1)
        g_placeable = target_dom_node[0]
        nested_g_placeable = g_placeable[0]

        assert target_dom_node.text == u'foobaz'

        assert g_placeable.tag == u'g'
        print('g_placeable.text: %s (%s)' % (g_placeable.text, type(g_placeable.text)))
        assert g_placeable.text is None
        assert g_placeable.attrib[u'id'] == u'oof'
        assert g_placeable.tail is None

        assert nested_g_placeable.tag == u'g'
        assert nested_g_placeable.text == u'barrab'
        assert nested_g_placeable.attrib[u'id'] == u'zab'
        assert nested_g_placeable.tail is None

        xliffunit.rich_target[0].print_tree(2)
        assert xliffunit.rich_target == [StringElem([u'foobaz', G(id='oof', sub=[G(id='zab', sub=[u'barrab'])])])]

    def test_source(self):
        xlifffile = xliff.xlifffile()
        xliffunit = xlifffile.addsourceunit("Concept")
        xliffunit.source = "Term"
        newfile = xliff.xlifffile.parsestring(str(xlifffile))
        print(str(xlifffile))
        assert newfile.findunit("Concept") is None
        assert newfile.findunit("Term") is not None

    def test_target(self):
        xlifffile = xliff.xlifffile()
        xliffunit = xlifffile.addsourceunit("Concept")
        xliffunit.target = "Konsep"
        newfile = xliff.xlifffile.parsestring(str(xlifffile))
        print(str(xlifffile))
        assert newfile.findunit("Concept").target == "Konsep"

    def test_sourcelanguage(self):
        xlifffile = xliff.xlifffile(sourcelanguage="xh")
        xmltext = str(xlifffile)
        print(xmltext)
        assert xmltext.find('source-language="xh"') > 0
        #TODO: test that it also works for new files.

    def test_targetlanguage(self):
        xlifffile = xliff.xlifffile(sourcelanguage="zu", targetlanguage="af")
        xmltext = str(xlifffile)
        print(xmltext)
        assert xmltext.find('source-language="zu"') > 0
        assert xmltext.find('target-language="af"') > 0

    def test_notes(self):
        xlifffile = xliff.xlifffile()
        unit = xlifffile.addsourceunit("Concept")
        # We don't want to add unnecessary notes
        assert not "note" in str(xlifffile)
        unit.addnote(None)
        assert not "note" in str(xlifffile)
        unit.addnote("")
        assert not "note" in str(xlifffile)

        unit.addnote("Please buy bread")
        assert unit.getnotes() == "Please buy bread"
        notenodes = unit.xmlelement.findall(".//%s" % unit.namespaced("note"))
        assert len(notenodes) == 1

        unit.addnote("Please buy milk", origin="Mom")
        notenodes = unit.xmlelement.findall(".//%s" % unit.namespaced("note"))
        assert len(notenodes) == 2
        assert not "from" in notenodes[0].attrib
        assert notenodes[1].get("from") == "Mom"
        assert unit.getnotes(origin="Mom") == "Please buy milk"

        unit.addnote("Don't forget the beer", origin="Dad")
        notenodes = unit.xmlelement.findall(".//%s" % unit.namespaced("note"))
        assert len(notenodes) == 3
        assert notenodes[1].get("from") == "Mom"
        assert notenodes[2].get("from") == "Dad"
        assert unit.getnotes(origin="Dad") == "Don't forget the beer"

        assert not unit.getnotes(origin="Bob") == "Please buy bread\nPlease buy milk\nDon't forget the beer"
        assert not notenodes[2].get("from") == "Mom"
        assert not "from" in notenodes[0].attrib
        assert unit.getnotes() == "Please buy bread\nPlease buy milk\nDon't forget the beer"
        assert unit.correctorigin(notenodes[2], "ad")
        assert not unit.correctorigin(notenodes[2], "om")

    def test_alttrans(self):
        """Test xliff <alt-trans> accessors"""
        xlifffile = xliff.xlifffile()
        unit = xlifffile.addsourceunit("Testing")

        unit.addalttrans("ginmi")
        unit.addalttrans("shikenki")
        alternatives = unit.getalttrans()
        assert alternatives[0].source == "Testing"
        assert alternatives[0].target == "ginmi"
        assert alternatives[1].target == "shikenki"

        assert not unit.target

        unit.addalttrans("Tasting", origin="bob", lang="eng")
        alternatives = unit.getalttrans()
        assert alternatives[2].target == "Tasting"

        alternatives = unit.getalttrans(origin="bob")
        assert alternatives[0].target == "Tasting"

        unit.delalttrans(alternatives[0])
        assert len(unit.getalttrans(origin="bob")) == 0
        alternatives = unit.getalttrans()
        assert len(alternatives) == 2
        assert alternatives[0].target == "ginmi"
        assert alternatives[1].target == "shikenki"

        #clean up:
        alternatives = unit.getalttrans()
        for alt in alternatives:
            unit.delalttrans(alt)
        unit.addalttrans("targetx", sourcetxt="sourcex")
        # test that the source node is before the target node:
        alt = unit.getalttrans()[0]
        altformat = etree.tostring(alt.xmlelement)
        print(altformat)
        assert altformat.find("<source") < altformat.find("<target")

        # test that a new target is still before alt-trans (bug 1098)
        unit.target = u"newester target"
        unitformat = str(unit)
        print(unitformat)
        assert unitformat.find("<source") < unitformat.find("<target") < unitformat.find("<alt-trans")

    def test_fuzzy(self):
        xlifffile = xliff.xlifffile()
        unit = xlifffile.addsourceunit("Concept")
        unit.markfuzzy()
        assert not unit.isfuzzy()  # untranslated
        unit.target = "Konsep"
        assert unit.isfuzzy()
        unit.markfuzzy()
        assert unit.isfuzzy()
        unit.markfuzzy(False)
        assert not unit.isfuzzy()
        unit.markfuzzy(True)
        assert unit.isfuzzy()

        #If there is no target, we can't really indicate fuzzyness, so we set
        #approved to "no". If we want isfuzzy() to reflect that, the line can
        #be uncommented
        unit.target = None
        assert unit.target is None
        print(unit)
        unit.markfuzzy(True)
        assert 'approved="no"' in str(unit)
        #assert unit.isfuzzy()

    def test_xml_space(self):
        """Test for the correct handling of xml:space attributes."""
        xlfsource = self.skeleton \
          % '''<trans-unit id="1" xml:space="preserve">
                   <source> File  1 </source>
               </trans-unit>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].source == " File  1 "
        root_node = xlifffile.document.getroot()
        lisa.setXMLspace(root_node, "preserve")
        assert xlifffile.units[0].source == " File  1 "
        lisa.setXMLspace(root_node, "default")
        assert xlifffile.units[0].source == " File  1 "

        xlfsource = self.skeleton \
          % '''<trans-unit id="1" xml:space="default">
                   <source> File  1 </source>
               </trans-unit>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].source == "File 1"
        root_node = xlifffile.document.getroot()
        lisa.setXMLspace(root_node, "preserve")
        assert xlifffile.units[0].source == "File 1"
        lisa.setXMLspace(root_node, "default")
        assert xlifffile.units[0].source == "File 1"

        xlfsource = self.skeleton \
          % '''<trans-unit id="1">
                   <source> File  1 </source>
               </trans-unit>'''
        # we currently always normalize as default behaviour for xliff
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].source == "File 1"
        root_node = xlifffile.document.getroot()
        lisa.setXMLspace(root_node, "preserve")
        assert xlifffile.units[0].source == "File 1"
        lisa.setXMLspace(root_node, "default")
        assert xlifffile.units[0].source == "File 1"

        xlfsource = self.skeleton \
          % '''<trans-unit id="1">
                   <source> File  1
</source>
               </trans-unit>'''
        # we currently always normalize as default behaviour for xliff
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].source == "File 1"
        root_node = xlifffile.document.getroot()
        lisa.setXMLspace(root_node, "preserve")
        assert xlifffile.units[0].source == "File 1"
        lisa.setXMLspace(root_node, "default")
        assert xlifffile.units[0].source == "File 1"

    def test_parsing(self):
        xlfsource = self.skeleton \
          % '''<trans-unit id="1" xml:space="preserve">
                   <source>File</source>
                   <target/>
               </trans-unit>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].istranslatable()

        xlfsource = self.skeleton \
          % '''<trans-unit id="1" xml:space="preserve" translate="no">
                   <source>File</source>
                   <target/>
               </trans-unit>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert not xlifffile.units[0].istranslatable()

        xlfsource = self.skeleton \
          % '''<trans-unit id="1" xml:space="preserve" translate="yes">
                   <source>File</source>
                   <target/>
               </trans-unit>'''
        xlifffile = xliff.xlifffile.parsestring(xlfsource)
        assert xlifffile.units[0].istranslatable()

########NEW FILE########
__FILENAME__ = test_zip
#!/usr/bin/env python

"""Tests for the zip storage module"""

import os
from zipfile import ZipFile

from translate.storage import directory, zip


class TestZIPFile(object):
    """A test class to test the zip class that provides the directory interface."""

    def setup_method(self, method):
        """sets up a test directory"""
        print("setup_method called on", self.__class__.__name__)
        self.testzip = "%s_testzip.zip" % (self.__class__.__name__)
        self.cleardir(self.testzip)
        self.zip = ZipFile(self.testzip, mode="w")

    def teardown_method(self, method):
        """removes the attributes set up by setup_method"""
        self.cleardir(self.testzip)

    def cleardir(self, dirname):
        """removes the given directory"""
        if os.path.exists(self.testzip):
            os.remove(self.testzip)
        assert not os.path.exists(self.testzip)

    def touchfiles(self, dir, filenames, content="", last=False):
        for filename in filenames:
            if dir:
                self.zip.writestr(os.path.join(dir, filename), content)
            else:
                self.zip.writestr(filename, content)
        if last:
            self.zip.close()

    def mkdir(self, dir):
        """Makes a directory inside self.testzip."""
        pass

    def test_created(self):
        """test that the directory actually exists"""
        print(self.testzip)
        assert os.path.isfile(self.testzip)

    def test_basic(self):
        """Tests basic functionality."""
        files = ["a.po", "b.po", "c.po"]
        self.touchfiles(None, files, last=True)

        d = zip.ZIPFile(self.testzip)
        filenames = [name for dir, name in d.getfiles()]
        assert filenames == files

    def test_structure(self):
        """Tests a small directory structure."""
        files = ["a.po", "b.po", "c.po"]
        self.touchfiles(self.testzip, files)
        self.mkdir("bla")
        self.touchfiles(os.path.join(self.testzip, "bla"), files, last=True)

        d = zip.ZIPFile(self.testzip)
        filenames = [name for dir, name in d.getfiles()]
        assert filenames == files * 2

    def test_getunits(self):
        """Tests basic functionality."""
        files = ["a.po", "b.po", "c.po"]
        posource = '''msgid "bla"\nmsgstr "blabla"\n'''
        self.touchfiles(self.testzip, files, posource, last=True)

        d = zip.ZIPFile(self.testzip)
        for unit in d.getunits():
            assert unit.target == "blabla"
        assert len(d.getunits()) == 3

########NEW FILE########
__FILENAME__ = tiki
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Mozilla Corporation, Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Class that manages TikiWiki files for translation.  Tiki files are <strike>ugly and
inconsistent</strike> formatted as a single large PHP array with several special
sections identified by comments.  Example current as of 2008-12-01:

.. code-block:: php

  <?php
    // Many comments at the top
    $lang=Array(
    // ### Start of unused words
    "aaa" => "zzz",
    // ### end of unused words

    // ### start of untranslated words
    // "bbb" => "yyy",
    // ### end of untranslated words

    // ### start of possibly untranslated words
    "ccc" => "xxx",
    // ### end of possibly untranslated words

    "ddd" => "www",
    "###end###"=>"###end###");
  ?>

In addition there are several auto-generated //-style comments scattered through the
page and array, some of which matter when being parsed.

This has all been gleaned from the
`TikiWiki source <http://tikiwiki.svn.sourceforge.net/viewvc/tikiwiki/trunk/get_strings.php?view=markup>`_.
As far as I know no detailed documentation exists for the tiki language.php files.

"""

import datetime
import re

from translate.misc import wStringIO
from translate.storage import base


class TikiUnit(base.TranslationUnit):
    """A tiki unit entry."""

    def __init__(self, source=None, encoding="UTF-8"):
        self.location = []
        super(TikiUnit, self).__init__(source)

    def __unicode__(self):
        """Returns a string formatted to be inserted into a tiki language.php file."""
        ret = u'"%s" => "%s",' % (self.source, self.target)
        if self.location == ["untranslated"]:
            ret = u'// ' + ret
        return ret + "\n"

    def addlocation(self, location):
        """Location is defined by the comments in the file. This function will only
        set valid locations.

        :param location: Where the string is located in the file.  Must be a valid location.
        """
        if location in ['unused', 'untranslated', 'possiblyuntranslated', 'translated']:
            self.location.append(location)

    def getlocations(self):
        """Returns the a list of the location(s) of the string."""
        return self.location


class TikiStore(base.TranslationStore):
    """Represents a tiki language.php file."""

    def __init__(self, inputfile=None):
        """If an inputfile is specified it will be parsed.

        :param inputfile: Either a string or a filehandle of the source file
        """
        base.TranslationStore.__init__(self, TikiUnit)
        self.units = []
        self.filename = getattr(inputfile, 'name', '')
        if inputfile is not None:
            self.parse(inputfile)

    def __str__(self):
        """Will return a formatted tiki-style language.php file."""
        _unused = []
        _untranslated = []
        _possiblyuntranslated = []
        _translated = []

        output = self._tiki_header()

        # Reorder all the units into their groups
        for unit in self.units:
            if unit.getlocations() == ["unused"]:
                _unused.append(unit)
            elif unit.getlocations() == ["untranslated"]:
                _untranslated.append(unit)
            elif unit.getlocations() == ["possiblyuntranslated"]:
                _possiblyuntranslated.append(unit)
            else:
                _translated.append(unit)

        output += "// ### Start of unused words\n"
        for unit in _unused:
            output += unicode(unit)
        output += "// ### end of unused words\n\n"
        output += "// ### start of untranslated words\n"
        for unit in _untranslated:
            output += unicode(unit)
        output += "// ### end of untranslated words\n\n"
        output += "// ### start of possibly untranslated words\n"
        for unit in _possiblyuntranslated:
            output += unicode(unit)
        output += "// ### end of possibly untranslated words\n\n"
        for unit in _translated:
            output += unicode(unit)

        output += self._tiki_footer()
        return output.encode('UTF-8')

    def _tiki_header(self):
        """Returns a tiki-file header string."""
        return u"<?php // -*- coding:utf-8 -*-\n// Generated from po2tiki on %s\n\n$lang=Array(\n" % datetime.datetime.now()

    def _tiki_footer(self):
        """Returns a tiki-file footer string."""
        return u'"###end###"=>"###end###");\n?>'

    def parse(self, input):
        """Parse the given input into source units.

        :param input: the source, either a string or filehandle
        """
        if hasattr(input, "name"):
            self.filename = input.name

        if isinstance(input, str):
            input = wStringIO.StringIO(input)

        _split_regex = re.compile(r"^(?:// )?\"(.*)\" => \"(.*)\",$", re.UNICODE)

        try:
            _location = "translated"

            for line in input:
                # The tiki file fails to identify each section so we have to look for start and end
                # points and if we're outside of them we assume the string is translated
                if line.count("### Start of unused words"):
                    _location = "unused"
                elif line.count("### start of untranslated words"):
                    _location = "untranslated"
                elif line.count("### start of possibly untranslated words"):
                    _location = "possiblyuntranslated"
                elif line.count("### end of unused words"):
                    _location = "translated"
                elif line.count("### end of untranslated words"):
                    _location = "translated"
                elif line.count("### end of possibly untranslated words"):
                    _location = "translated"

                match = _split_regex.match(line)

                if match:
                    unit = self.addsourceunit("".join(match.group(1)))
                    # Untranslated words get an empty msgstr
                    if not _location == "untranslated":
                        unit.settarget(match.group(2))
                    unit.addlocation(_location)
        finally:
            input.close()

########NEW FILE########
__FILENAME__ = tmdb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009, 2013 Zuza Software Foundation
# Copyright 2013 F Wolff
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module to provide a translation memory database."""

import logging
import math
import re
import threading
import time
from sqlite3 import dbapi2

from translate.lang import data
from translate.search.lshtein import LevenshteinComparer


STRIP_REGEXP = re.compile("\W", re.UNICODE)


class LanguageError(Exception):

    def __init__(self, value):
        self.value = value

    def __str__(self):
        return str(self.value)


class TMDB(object):
    _tm_dbs = {}

    def __init__(self, db_file, max_candidates=3, min_similarity=75,
                 max_length=1000):

        self.max_candidates = max_candidates
        self.min_similarity = min_similarity
        self.max_length = max_length

        if not isinstance(db_file, unicode):
            db_file = unicode(db_file)  # don't know which encoding
        self.db_file = db_file
        # share connections to same database file between different instances
        if db_file not in self._tm_dbs:
            self._tm_dbs[db_file] = {}
        self._tm_db = self._tm_dbs[db_file]

        # FIXME: do we want to do any checks before we initialize the DB?
        self.init_database()
        self.fulltext = False
        self.init_fulltext()

        self.comparer = LevenshteinComparer(self.max_length)

        self.preload_db()

    def _get_connection(self, index):
        current_thread = threading.currentThread()
        if current_thread not in self._tm_db:
            connection = dbapi2.connect(self.db_file.encode('utf-8'))
            cursor = connection.cursor()
            self._tm_db[current_thread] = (connection, cursor)
        return self._tm_db[current_thread][index]

    connection = property(lambda self: self._get_connection(0))
    cursor = property(lambda self: self._get_connection(1))

    def init_database(self):
        """creates database tables and indices"""

        script = """
CREATE TABLE IF NOT EXISTS sources (
       sid INTEGER PRIMARY KEY AUTOINCREMENT,
       text VARCHAR NOT NULL,
       context VARCHAR DEFAULT NULL,
       lang VARCHAR NOT NULL,
       length INTEGER NOT NULL
);
CREATE INDEX IF NOT EXISTS sources_context_idx ON sources (context);
CREATE INDEX IF NOT EXISTS sources_lang_idx ON sources (lang);
CREATE INDEX IF NOT EXISTS sources_length_idx ON sources (length);
CREATE UNIQUE INDEX IF NOT EXISTS sources_uniq_idx ON sources (text, context, lang);

CREATE TABLE IF NOT EXISTS targets (
       tid INTEGER PRIMARY KEY AUTOINCREMENT,
       sid INTEGER NOT NULL,
       text VARCHAR NOT NULL,
       lang VARCHAR NOT NULL,
       time INTEGER DEFAULT NULL,
       FOREIGN KEY (sid) references sources(sid)
);
CREATE INDEX IF NOT EXISTS targets_sid_idx ON targets (sid);
CREATE INDEX IF NOT EXISTS targets_lang_idx ON targets (lang);
CREATE INDEX IF NOT EXISTS targets_time_idx ON targets (time);
CREATE UNIQUE INDEX IF NOT EXISTS targets_uniq_idx ON targets (sid, text, lang);
"""

        try:
            self.cursor.executescript(script)
            self.connection.commit()
        except:
            self.connection.rollback()
            raise

    def init_fulltext(self):
        """detects if fts3 fulltext indexing module exists, initializes fulltext table if it does"""

        # HACKISH: no better way to detect fts3 support except trying to
        # construct a dummy table?!
        try:
            script = """
DROP TABLE IF EXISTS test_for_fts3;
CREATE VIRTUAL TABLE test_for_fts3 USING fts3;
DROP TABLE test_for_fts3;
"""
            self.cursor.executescript(script)
            logging.debug("fts3 supported")
            # for some reason CREATE VIRTUAL TABLE doesn't support IF NOT
            # EXISTS syntax check if fulltext index table exists manually
            self.cursor.execute("SELECT name FROM sqlite_master WHERE name = 'fulltext'")
            if not self.cursor.fetchone():
                # create fulltext index table, and index all strings in sources
                script = """
CREATE VIRTUAL TABLE fulltext USING fts3(text);
"""
                logging.debug("fulltext table not exists, creating")
                self.cursor.executescript(script)
                logging.debug("created fulltext table")
            else:
                logging.debug("fulltext table already exists")

            # create triggers that would sync sources table with fulltext index
            script = """
INSERT INTO fulltext (rowid, text) SELECT sid, text FROM sources WHERE sid NOT IN (SELECT rowid FROM fulltext);
CREATE TRIGGER IF NOT EXISTS sources_insert_trig AFTER INSERT ON sources FOR EACH ROW
BEGIN
    INSERT INTO fulltext (docid, text) VALUES (NEW.sid, NEW.text);
END;
CREATE TRIGGER IF NOT EXISTS sources_update_trig AFTER UPDATE OF text ON sources FOR EACH ROW
BEGIN
    UPDATE fulltext SET text = NEW.text WHERE docid = NEW.sid;
END;
CREATE TRIGGER IF NOT EXISTS sources_delete_trig AFTER DELETE ON sources FOR EACH ROW
BEGIN
    DELETE FROM fulltext WHERE docid = OLD.sid;
END;
"""
            self.cursor.executescript(script)
            self.connection.commit()
            logging.debug("created fulltext triggers")
            self.fulltext = True

        except dbapi2.OperationalError as e:
            self.fulltext = False
            logging.debug("failed to initialize fts3 support: " + str(e))
            script = """
DROP TRIGGER IF EXISTS sources_insert_trig;
DROP TRIGGER IF EXISTS sources_update_trig;
DROP TRIGGER IF EXISTS sources_delete_trig;
"""
            self.cursor.executescript(script)

    def preload_db(self):
        """ugly hack to force caching of sqlite db file in memory for
        improved performance"""
        if self.fulltext:
            query = """SELECT COUNT(*) FROM sources s JOIN fulltext f ON s.sid = f.docid JOIN targets t on s.sid = t.sid"""
        else:
            query = """SELECT COUNT(*) FROM sources s JOIN targets t on s.sid = t.sid"""
        self.cursor.execute(query)
        (numrows,) = self.cursor.fetchone()
        logging.debug("tmdb has %d records" % numrows)
        return numrows

    def add_unit(self, unit, source_lang=None, target_lang=None, commit=True):
        """inserts unit in the database"""
        # TODO: is that really the best way to handle unspecified
        # source and target languages? what about conflicts between
        # unit attributes and passed arguments
        if unit.getsourcelanguage():
            source_lang = unit.getsourcelanguage()
        if unit.gettargetlanguage():
            target_lang = unit.gettargetlanguage()

        if not source_lang:
            raise LanguageError("undefined source language")
        if not target_lang:
            raise LanguageError("undefined target language")

        unitdict = {
            "source": unit.source,
            "target": unit.target,
            "context": unit.getcontext(),
        }
        self.add_dict(unitdict, source_lang, target_lang, commit)

    def add_dict(self, unit, source_lang, target_lang, commit=True):
        """inserts units represented as dictionaries in database"""
        source_lang = data.normalize_code(source_lang)
        target_lang = data.normalize_code(target_lang)
        try:
            try:
                self.cursor.execute("INSERT INTO sources (text, context, lang, length) VALUES(?, ?, ?, ?)",
                                    (unit["source"],
                                     unit["context"],
                                     source_lang,
                                     len(unit["source"])))
                sid = self.cursor.lastrowid
            except dbapi2.IntegrityError:
                # source string already exists in db, run query to find sid
                self.cursor.execute("SELECT sid FROM sources WHERE text=? AND context=? and lang=?",
                                    (unit["source"],
                                     unit["context"],
                                     source_lang))
                sid = self.cursor.fetchone()
                (sid,) = sid
            try:
                # FIXME: get time info from translation store
                # FIXME: do we need so store target length?
                self.cursor.execute("INSERT INTO targets (sid, text, lang, time) VALUES (?, ?, ?, ?)",
                                    (sid,
                                     unit["target"],
                                     target_lang,
                                     int(time.time())))
            except dbapi2.IntegrityError:
                # target string already exists in db, do nothing
                pass

            if commit:
                self.connection.commit()
        except:
            if commit:
                self.connection.rollback()
            raise

    def add_store(self, store, source_lang, target_lang, commit=True):
        """insert all units in store in database"""
        count = 0
        for unit in store.units:
            if unit.istranslatable() and unit.istranslated():
                self.add_unit(unit, source_lang, target_lang, commit=False)
                count += 1
        if commit:
            self.connection.commit()
        return count

    def add_list(self, units, source_lang, target_lang, commit=True):
        """insert all units in list into the database, units are
        represented as dictionaries"""
        count = 0
        for unit in units:
            self.add_dict(unit, source_lang, target_lang, commit=False)
            count += 1
        if commit:
            self.connection.commit()
        return count

    def translate_unit(self, unit_source, source_langs, target_langs):
        """return TM suggestions for unit_source"""
        if isinstance(unit_source, str):
            unit_source = unicode(unit_source, "utf-8")
        if isinstance(source_langs, list):
            source_langs = [data.normalize_code(lang) for lang in source_langs]
            source_langs = ','.join(source_langs)
        else:
            source_langs = data.normalize_code(source_langs)
        if isinstance(target_langs, list):
            target_langs = [data.normalize_code(lang) for lang in target_langs]
            target_langs = ','.join(target_langs)
        else:
            target_langs = data.normalize_code(target_langs)

        minlen = min_levenshtein_length(len(unit_source), self.min_similarity)
        maxlen = max_levenshtein_length(len(unit_source), self.min_similarity,
                                        self.max_length)

        # split source into words, remove punctuation and special
        # chars, keep words that are at least 3 chars long
        unit_words = STRIP_REGEXP.sub(' ', unit_source).split()
        unit_words = filter(lambda word: len(word) > 2, unit_words)

        if self.fulltext and len(unit_words) > 3:
            logging.debug("fulltext matching")
            query = """SELECT s.text, t.text, s.context, s.lang, t.lang FROM sources s JOIN targets t ON s.sid = t.sid JOIN fulltext f ON s.sid = f.docid
                       WHERE s.lang IN (?) AND t.lang IN (?) AND s.length BETWEEN ? AND ?
                       AND fulltext MATCH ?"""
            search_str = " OR ".join(unit_words)
            self.cursor.execute(query, (source_langs, target_langs, minlen,
                                maxlen, search_str))
        else:
            logging.debug("nonfulltext matching")
            query = """SELECT s.text, t.text, s.context, s.lang, t.lang FROM sources s JOIN targets t ON s.sid = t.sid
            WHERE s.lang IN (?) AND t.lang IN (?)
            AND s.length >= ? AND s.length <= ?"""
            self.cursor.execute(query, (source_langs, target_langs, minlen,
                                        maxlen))

        results = []
        for row in self.cursor:
            quality = self.comparer.similarity(unit_source, row[0],
                                               self.min_similarity)
            if quality >= self.min_similarity:
                results.append({
                    'source': row[0],
                    'target': row[1],
                    'context': row[2],
                    'quality': quality,
                })
        results.sort(key=lambda match: match['quality'], reverse=True)
        results = results[:self.max_candidates]
        logging.debug("results: %s", unicode(results))
        return results


def min_levenshtein_length(length, min_similarity):
    return math.ceil(max(length * (min_similarity / 100.0), 2))


def max_levenshtein_length(length, min_similarity, max_length):
    return math.floor(min(length / (min_similarity / 100.0), max_length))

########NEW FILE########
__FILENAME__ = tmx
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""module for parsing TMX translation memeory files"""

from lxml import etree

from translate import __version__
from translate.storage import lisa


class tmxunit(lisa.LISAunit):
    """A single unit in the TMX file."""
    rootNode = "tu"
    languageNode = "tuv"
    textNode = "seg"

    def createlanguageNode(self, lang, text, purpose):
        """returns a langset xml Element setup with given parameters"""
        if isinstance(text, str):
            text = text.decode("utf-8")
        langset = etree.Element(self.languageNode)
        lisa.setXMLlang(langset, lang)
        seg = etree.SubElement(langset, self.textNode)
        # implied by the standard:
        # lisa.setXMLspace(seg, "preserve")
        seg.text = text
        return langset

    def getid(self):
        """Returns the identifier for this unit. The optional tuid property is
        used if available, otherwise we inherit .getid(). Note that the tuid
        property is only mandated to be unique from TMX 2.0."""
        id = self.xmlelement.get("tuid", "")
        return id or super(tmxunit, self).getid()

    def istranslatable(self):
        return bool(self.source)

    def addnote(self, text, origin=None, position="append"):
        """Add a note specifically in a "note" tag.

        The origin parameter is ignored"""
        if isinstance(text, str):
            text = text.decode("utf-8")
        note = etree.SubElement(self.xmlelement, self.namespaced("note"))
        note.text = text.strip()

    def _getnotelist(self, origin=None):
        """Returns the text from notes.

        :param origin: Ignored
        :return: The text from notes
        :rtype: List
        """
        note_nodes = self.xmlelement.iterdescendants(self.namespaced("note"))
        note_list = [lisa.getText(note) for note in note_nodes]

        return note_list

    def getnotes(self, origin=None):
        return '\n'.join(self._getnotelist(origin=origin))

    def removenotes(self):
        """Remove all the translator notes."""
        notes = self.xmlelement.iterdescendants(self.namespaced("note"))
        for note in notes:
            self.xmlelement.remove(note)

    def adderror(self, errorname, errortext):
        """Adds an error message to this unit."""
        # TODO: consider factoring out: some duplication between XLIFF and TMX
        text = errorname
        if errortext:
            text += ': ' + errortext
        self.addnote(text, origin="pofilter")

    def geterrors(self):
        """Get all error messages."""
        # TODO: consider factoring out: some duplication between XLIFF and TMX
        notelist = self._getnotelist(origin="pofilter")
        errordict = {}
        for note in notelist:
            errorname, errortext = note.split(': ')
            errordict[errorname] = errortext
        return errordict

    def copy(self):
        """Make a copy of the translation unit.

        We don't want to make a deep copy - this could duplicate the whole XML
        tree. For now we just serialise and reparse the unit's XML."""
        # TODO: check performance
        new_unit = self.__class__(None, empty=True)
        new_unit.xmlelement = etree.fromstring(etree.tostring(self.xmlelement))
        return new_unit


class tmxfile(lisa.LISAfile):
    """Class representing a TMX file store."""
    UnitClass = tmxunit
    Name = "TMX Translation Memory"
    Mimetypes = ["application/x-tmx"]
    Extensions = ["tmx"]
    rootNode = "tmx"
    bodyNode = "body"
    XMLskeleton = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE tmx SYSTEM "tmx14.dtd">
<tmx version="1.4">
<header></header>
<body></body>
</tmx>'''

    def addheader(self):
        headernode = self.document.getroot().iterchildren(self.namespaced("header")).next()
        headernode.set("creationtool", "Translate Toolkit - po2tmx")
        headernode.set("creationtoolversion", __version__.sver)
        headernode.set("segtype", "sentence")
        headernode.set("o-tmf", "UTF-8")
        headernode.set("adminlang", "en")
        # TODO: consider adminlang. Used for notes, etc. Possibly same as
        # targetlanguage
        headernode.set("srclang", self.sourcelanguage)
        headernode.set("datatype", "PlainText")
        #headernode.set("creationdate", "YYYYMMDDTHHMMSSZ"
        #headernode.set("creationid", "CodeSyntax"

    def addtranslation(self, source, srclang, translation, translang,
                       comment=None):
        """addtranslation method for testing old unit tests"""
        unit = self.addsourceunit(source)
        unit.target = translation
        if comment is not None and len(comment) > 0:
            unit.addnote(comment)

        tuvs = unit.xmlelement.iterdescendants(self.namespaced('tuv'))
        lisa.setXMLlang(tuvs.next(), srclang)
        lisa.setXMLlang(tuvs.next(), translang)

    def translate(self, sourcetext, sourcelang=None, targetlang=None):
        """method to test old unit tests"""
        return getattr(self.findunit(sourcetext), "target", None)

########NEW FILE########
__FILENAME__ = trados
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Manage the Trados .txt Translation Memory format

A Trados file looks like this:

.. code-block:: xml

    <TrU>
    <CrD>18012000, 13:18:35
    <CrU>CAROL-ANN
    <UsC>0
    <Seg L=EN_GB>Association for Road Safety \endash  Conference
    <Seg L=DE_DE>Tagung der Gesellschaft fr Verkehrssicherheit
    </TrU>
    <TrU>
    <CrD>18012000, 13:19:14
    <CrU>CAROL-ANN
    <UsC>0
    <Seg L=EN_GB>Road Safety Education in our Schools
    <Seg L=DE_DE>Verkehrserziehung an Schulen
    </TrU>

"""

import re
import time

try:
    # FIXME see if we can't use lxml
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError("BeautifulSoup 4 is not installed. Support for Trados txt is disabled.")

from translate.storage import base


TRADOS_TIMEFORMAT = "%d%m%Y, %H:%M:%S"
"""Time format used by Trados .txt"""

RTF_ESCAPES = {
    u"\\emdash": u"",
    u"\\endash": u"",
    # Nonbreaking space equal to width of character "m" in current font.
    u"\\emspace": u"\u2003",
    # Nonbreaking space equal to width of character "n" in current font.
    u"\\enspace": u"\u2002",
    #u"\\qmspace": "",    # One-quarter em space.
    u"\\bullet": u"",     # Bullet character.
    u"\\lquote": u"",     # Left single quotation mark. \u2018
    u"\\rquote": u"",     # Right single quotation mark. \u2019
    u"\\ldblquote": u"",  # Left double quotation mark. \u201C
    u"\\rdblquote": u"",  # Right double quotation mark. \u201D
    u"\\~": u"\u00a0",  # Nonbreaking space
    u"\\-": u"\u00ad",  # Optional hyphen.
    u"\\_": u"",  # Nonbreaking hyphen \U2011
    # A hexadecimal value, based on the specified character set (may be used to
    # identify 8-bit values).
    #u"\\'hh": "",
}
"""RTF control to Unicode map. See
http://msdn.microsoft.com/en-us/library/aa140283(v=office.10).aspx
"""


def unescape(text):
    """Convert Trados text to normal Unicode string"""
    for trados_escape, char in RTF_ESCAPES.iteritems():
        text = text.replace(trados_escape, char)
    return text


def escape(text):
    """Convert Unicode string to Trodas escapes"""
    for trados_escape, char in RTF_ESCAPES.iteritems():
        text = text.replace(char, trados_escape)
    return text


class TradosTxtDate(object):
    """Manages the timestamps in the Trados .txt format of DDMMYYY, hh:mm:ss"""

    def __init__(self, newtime=None):
        self._time = None
        if newtime:
            if isinstance(newtime, basestring):
                self.timestring = newtime
            elif isinstance(newtime, time.struct_time):
                self.time = newtime

    def get_timestring(self):
        """Get the time in the Trados time format"""
        if not self._time:
            return None
        else:
            return time.strftime(TRADOS_TIMEFORMAT, self._time)

    def set_timestring(self, timestring):
        """Set the time_struct object using a Trados time formated string

        :param timestring: A Trados time string (DDMMYYYY, hh:mm:ss)
        :type timestring: String
        """
        self._time = time.strptime(timestring, TRADOS_TIMEFORMAT)
    timestring = property(get_timestring, set_timestring)

    def get_time(self):
        """Get the time_struct object"""
        return self._time

    def set_time(self, newtime):
        """Set the time_struct object

        :param newtime: a new time object
        :type newtime: time.time_struct
        """
        if newtime and isinstance(newtime, time.struct_time):
            self._time = newtime
        else:
            self._time = None
    time = property(get_time, set_time)

    def __str__(self):
        if not self.timestring:
            return ""
        else:
            return self.timestring


class TradosUnit(base.TranslationUnit):

    def __init__(self, source=None):
        self._soup = None
        super(TradosUnit, self).__init__(source)

    def getsource(self):
        return unescape(self._soup.findAll('seg')[0].contents[0])
    source = property(getsource, None)

    def gettarget(self):
        return unescape(self._soup.findAll('seg')[1].contents[0])
    target = property(gettarget, None)


class TradosSoup(BeautifulSoup):

    MARKUP_MASSAGE = [
        (re.compile('<(?P<fulltag>(?P<tag>[^\s\/]+).*?)>(?P<content>.+)\r'),
         lambda x: '<%(fulltag)s>%(content)s</%(tag)s>' % x.groupdict()),
    ]


class TradosTxtTmFile(base.TranslationStore):
    """A Trados translation memory file"""
    Name = "Trados Translation Memory"
    Mimetypes = ["application/x-trados-tm"]
    Extensions = ["txt"]

    def __init__(self, inputfile=None, unitclass=TradosUnit):
        """construct a Wordfast TM, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self._encoding = 'iso-8859-1'
        if inputfile is not None:
            self.parse(inputfile)

    def parse(self, input):
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            tmsrc = input.read()
            input.close()
            input = tmsrc
        self._soup = TradosSoup(input)
        for tu in self._soup.findAll('tru'):
            unit = TradosUnit()
            unit._soup = TradosSoup(str(tu))
            self.addunit(unit)

    def __str__(self):
        # FIXME turn the lowercased tags back into mixed case
        return self._soup.prettify()

########NEW FILE########
__FILENAME__ = ts
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

"""Module for parsing Qt .ts files for translation.

Currently this module supports the old format of .ts files. Some applictaions
use the newer .ts format which are documented here:
`TS file format 4.3 <http://doc.qt.digia.com/4.3/linguist-ts-file-format.html>`_,
`Example <http://svn.ez.no/svn/ezcomponents/trunk/Translation/docs/linguist-format.txt>`_

`Specification of the valid variable entries <http://qt-project.org/doc/qt-5.0/qtcore/qstring.html#arg>`_,
`2 <http://qt-project.org/doc/qt-5.0/qtcore/qstring.html#arg-2>`_
"""

from translate.misc import ourdom


class QtTsParser:
    contextancestors = dict.fromkeys(["TS"])
    messageancestors = dict.fromkeys(["TS", "context"])

    def __init__(self, inputfile=None):
        """make a new QtTsParser, reading from the given inputfile if required"""
        self.filename = getattr(inputfile, "filename", None)
        self.knowncontextnodes = {}
        self.indexcontextnodes = {}
        if inputfile is None:
            self.document = ourdom.parseString("<!DOCTYPE TS><TS></TS>")
        else:
            self.document = ourdom.parse(inputfile)
            assert self.document.documentElement.tagName == "TS"

    def addtranslation(self, contextname, source, translation, comment=None, transtype=None, createifmissing=False):
        """adds the given translation (will create the nodes required if asked). Returns success"""
        contextnode = self.getcontextnode(contextname)
        if contextnode is None:
            if not createifmissing:
                return False
            # construct a context node with the given name
            contextnode = self.document.createElement("context")
            namenode = self.document.createElement("name")
            nametext = self.document.createTextNode(contextname)
            namenode.appendChild(nametext)
            contextnode.appendChild(namenode)
            self.document.documentElement.appendChild(contextnode)
        if not createifmissing:
            return False
        messagenode = self.document.createElement("message")
        sourcenode = self.document.createElement("source")
        sourcetext = self.document.createTextNode(source)
        sourcenode.appendChild(sourcetext)
        messagenode.appendChild(sourcenode)
        if comment:
            commentnode = self.document.createElement("comment")
            commenttext = self.document.createTextNode(comment)
            commentnode.appendChild(commenttext)
            messagenode.appendChild(commentnode)
        translationnode = self.document.createElement("translation")
        translationtext = self.document.createTextNode(translation)
        translationnode.appendChild(translationtext)
        if transtype:
            translationnode.setAttribute("type", transtype)
        messagenode.appendChild(translationnode)
        contextnode.appendChild(messagenode)
        return True

    def getxml(self):
        """return the ts file as xml"""
        xml = self.document.toprettyxml(indent="    ", encoding="utf-8")
        # This line causes empty lines in the translation text to be removed
        # (when there are two newlines)
        xml = "\n".join([line for line in xml.split("\n") if line.strip()])
        return xml

    def getcontextname(self, contextnode):
        """returns the name of the given context"""
        namenode = ourdom.getFirstElementByTagName(contextnode, "name")
        return ourdom.getnodetext(namenode)

    def getcontextnode(self, contextname):
        """finds the contextnode with the given name"""
        contextnode = self.knowncontextnodes.get(contextname, None)
        if contextnode is not None:
            return contextnode
        contextnodes = self.document.searchElementsByTagName("context", self.contextancestors)
        for contextnode in contextnodes:
            if self.getcontextname(contextnode) == contextname:
                self.knowncontextnodes[contextname] = contextnode
                return contextnode
        return None

    def getmessagenodes(self, context=None):
        """returns all the messagenodes, limiting to the given context (name or node) if given"""
        if context is None:
            return self.document.searchElementsByTagName("message", self.messageancestors)
        else:
            if isinstance(context, (str, unicode)):
                # look up the context node by name
                context = self.getcontextnode(context)
                if context is None:
                    return []
            return context.searchElementsByTagName("message", self.messageancestors)

    def getmessagesource(self, message):
        """returns the message source for a given node"""
        sourcenode = ourdom.getFirstElementByTagName(message, "source")
        return ourdom.getnodetext(sourcenode)

    def getmessagetranslation(self, message):
        """returns the message translation for a given node"""
        translationnode = ourdom.getFirstElementByTagName(message, "translation")
        return ourdom.getnodetext(translationnode)

    def getmessagetype(self, message):
        """returns the message translation attributes for a given node"""
        translationnode = ourdom.getFirstElementByTagName(message, "translation")
        return translationnode.getAttribute("type")

    def getmessagecomment(self, message):
        """returns the message comment for a given node"""
        commentnode = ourdom.getFirstElementByTagName(message, "comment")
        # NOTE: handles only one comment per msgid (OK)
        # and only one-line comments (can be VERY wrong) TODO!!!
        return ourdom.getnodetext(commentnode)

    def iteritems(self):
        """iterates through (contextname, messages)"""
        for contextnode in self.document.searchElementsByTagName("context", self.contextancestors):
            yield self.getcontextname(contextnode), self.getmessagenodes(contextnode)

    def __del__(self):
        """clean up the document if required"""
        if hasattr(self, "document"):
            self.document.unlink()

########NEW FILE########
__FILENAME__ = ts2
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module for handling Qt linguist (.ts) files.

This will eventually replace the older ts.py which only supports the older
format. While converters haven't been updated to use this module, we retain
both.

`TS file format 4.3 <http://doc.qt.digia.com/4.3/linguist-ts-file-format.html>`_,
`4.8 <http://qt-project.org/doc/qt-4.8/linguist-ts-file-format.html>`_,
`5.0 <http://qt-project.org/doc/qt-5.0/qtlinguist/linguist-ts-file-format.html>`_.
`Example <http://svn.ez.no/svn/ezcomponents/trunk/Translation/docs/linguist-format.txt>`_.

`Specification of the valid variable entries <http://qt-project.org/doc/qt-5.0/qtcore/qstring.html#arg>`_,
`2 <http://qt-project.org/doc/qt-5.0/qtcore/qstring.html#arg-2>`_
"""

from lxml import etree

from translate.lang import data
from translate.misc.multistring import multistring
from translate.storage import base, lisa
from translate.storage.placeables import general
from translate.storage.workflow import StateEnum as state


# TODO: handle translation types

NPLURALS = {
    'jp': 1,
    'en': 2,
    'fr': 2,
    'lv': 3,
    'ga': 3,
    'cs': 3,
    'sk': 3,
    'mk': 3,
    'lt': 3,
    'ru': 3,
    'pl': 3,
    'ro': 3,
    'sl': 4,
    'mt': 4,
    'cy': 5,
    'ar': 6,
}


class tsunit(lisa.LISAunit):
    """A single term in the TS file."""

    rootNode = "message"
    languageNode = "source"
    textNode = ""
    namespace = ''
    rich_parsers = general.parsers

    S_OBSOLETE = state.OBSOLETE
    S_UNTRANSLATED = state.EMPTY
    S_FUZZY = state.NEEDS_WORK
    S_TRANSLATED = state.UNREVIEWED

    statemap = {
                "obsolete": S_OBSOLETE,
                "unfinished": S_FUZZY,
                "": S_TRANSLATED,
                None: S_TRANSLATED,
    }
    """This maps the unit "type" attribute to state."""

    STATE = {
        S_OBSOLETE: (state.OBSOLETE, state.EMPTY),
        S_UNTRANSLATED: (state.EMPTY, state.NEEDS_WORK),
        S_FUZZY: (state.NEEDS_WORK, state.UNREVIEWED),
        S_TRANSLATED: (state.UNREVIEWED, state.MAX),
    }

    statemap_r = dict((i[1], i[0]) for i in statemap.iteritems())

    def createlanguageNode(self, lang, text, purpose):
        """Returns an xml Element setup with given parameters."""

        assert purpose
        if purpose == "target":
            purpose = "translation"
        langset = etree.Element(self.namespaced(purpose))
        # TODO: check language
        #lisa.setXMLlang(langset, lang)

        langset.text = text
        return langset

    def _getsourcenode(self):
        return self.xmlelement.find(self.namespaced(self.languageNode))

    def _gettargetnode(self):
        return self.xmlelement.find(self.namespaced("translation"))

    def getlanguageNodes(self):
        """We override this to get source and target nodes."""

        def not_none(node):
            return not node is None
        return filter(not_none, [self._getsourcenode(), self._gettargetnode()])

    def getsource(self):
        # TODO: support <byte>. See bug 528.
        sourcenode = self._getsourcenode()
        if self.hasplural():
            return multistring([sourcenode.text])
        else:
            return data.forceunicode(sourcenode.text)
    source = property(getsource, lisa.LISAunit.setsource)
    rich_source = property(base.TranslationUnit._get_rich_source, base.TranslationUnit._set_rich_source)

    def settarget(self, text):
        # This is a fairly destructive implementation. Don't assume that this
        # is necessarily correct in all regards, but it does deal with a lot of
        # cases. It is hard to deal with plurals.
        #
        # Firstly deal with reinitialising to None or setting to identical
        # string.
        self._rich_target = None
        if self.gettarget() == text:
            return
        strings = []
        if isinstance(text, multistring):
            strings = text.strings
        elif isinstance(text, list):
            strings = text
        else:
            strings = [text]
        targetnode = self._gettargetnode()
        type = targetnode.get("type")
        targetnode.clear()
        if type:
            targetnode.set("type", type)
        if self.hasplural() or len(strings) > 1:
            self.xmlelement.set("numerus", "yes")
            for string in strings:
                numerus = etree.SubElement(targetnode, self.namespaced("numerusform"))
                numerus.text = data.forceunicode(string) or u""
                # manual, nasty pretty printing. See bug 1420.
                numerus.tail = u"\n        "
        else:
            targetnode.text = data.forceunicode(text) or u""
            targetnode.tail = u"\n    "

    def gettarget(self):
        targetnode = self._gettargetnode()
        if targetnode is None:
            etree.SubElement(self.xmlelement, self.namespaced("translation"))
            return None
        if self.hasplural():
            numerus_nodes = targetnode.findall(self.namespaced("numerusform"))
            return multistring([node.text or u"" for node in numerus_nodes])
        else:
            return data.forceunicode(targetnode.text) or u""
    target = property(gettarget, settarget)
    rich_target = property(base.TranslationUnit._get_rich_target, base.TranslationUnit._set_rich_target)

    def hasplural(self):
        return self.xmlelement.get("numerus") == "yes"

    def addnote(self, text, origin=None, position="append"):
        """Add a note specifically in the appropriate *comment* tag"""
        if isinstance(text, str):
            text = text.decode("utf-8")
        current_notes = self.getnotes(origin)
        self.removenotes(origin)
        if origin in ["programmer", "developer", "source code"]:
            note = etree.SubElement(self.xmlelement, self.namespaced("extracomment"))
        else:
            note = etree.SubElement(self.xmlelement, self.namespaced("translatorcomment"))
        if position == "append":
            note.text = "\n".join(filter(None, [current_notes, text.strip()]))
        else:
            note.text = text.strip()

    def getnotes(self, origin=None):
        # TODO: consider only responding when origin has certain values
        comments = []
        if origin in ["programmer", "developer", "source code", None]:
            notenode = self.xmlelement.find(self.namespaced("extracomment"))
            if notenode is not None and notenode.text is not None:
                comments.append(notenode.text)
        if origin in ["translator", None]:
            notenode = self.xmlelement.find(self.namespaced("translatorcomment"))
            if notenode is not None and notenode.text is not None:
                comments.append(notenode.text)
        return '\n'.join(comments)

    def removenotes(self, origin=None):
        """Remove all the translator notes."""
        if origin in ["programmer", "developer", "source code", None]:
            note = self.xmlelement.find(self.namespaced("extracomment"))
            if not note is None:
                self.xmlelement.remove(note)
        if origin in ["translator", None]:
            note = self.xmlelement.find(self.namespaced("translatorcomment"))
            if not note is None:
                self.xmlelement.remove(note)

    def _gettype(self):
        """Returns the type of this translation."""
        targetnode = self._gettargetnode()
        if targetnode is not None:
            return targetnode.get("type")
        return None

    def _settype(self, value=None):
        """Set the type of this translation."""
        if value:
            self._gettargetnode().set("type", value)
        elif self._gettype():
            # lxml recommends against using .attrib, but there seems to be no
            # other way
            self._gettargetnode().attrib.pop("type")

    def isreview(self):
        """States whether this unit needs to be reviewed"""
        return self._gettype() == "unfinished"

    def isfuzzy(self):
        return self._gettype() == "unfinished" and bool(self.target)

    def markfuzzy(self, value=True):
        if value:
            self._settype("unfinished")
        else:
            self._settype(None)

    def getid(self):
        context_name = self.getcontext()
        if self.source is None and context_name is None:
            return None

        # XXX: context_name is not supposed to be able to be None (the <name>
        # tag is compulsary in the <context> tag)
        if context_name is not None:
            if self.source:
                return context_name + self.source
            else:
                return context_name
        else:
            return self.source

    def istranslatable(self):
        # Found a file in the wild with no context and an empty source. This
        # served as a header, so let's classify this as not translatable.
        # http://bibletime.svn.sourceforge.net/viewvc/bibletime/trunk/bibletime/i18n/messages/bibletime_ui.ts
        # Furthermore, let's decide to handle obsolete units as untranslatable
        # like we do with PO.
        return bool(self.getid()) and not self.isobsolete()

    def getcontextname(self):
        parent = self.xmlelement.getparent()
        if parent is None:
            return None
        context = parent.find("name")
        if context is None:
            return None
        return context.text

    def getcontext(self):
        contexts = [self.getcontextname()]
        commentnode = self.xmlelement.find(self.namespaced("comment"))
        if commentnode is not None and commentnode.text is not None:
            contexts.append(commentnode.text)
        message_id = self.xmlelement.get('id')
        if message_id is not None:
            contexts.append(message_id)
        contexts = filter(None, contexts)
        return '\n'.join(contexts)

    def addlocation(self, location):
        if isinstance(location, str):
            location = location.decode("utf-8")
        newlocation = etree.SubElement(self.xmlelement, self.namespaced("location"))
        try:
            filename, line = location.split(':', 1)
        except ValueError:
            filename = location
            line = None
        newlocation.set("filename", filename)
        if line is not None:
            newlocation.set("line", line)

    def getlocations(self):
        location_tags = self.xmlelement.iterfind(self.namespaced("location"))
        locations = []
        for location_tag in location_tags:
            location = location_tag.get("filename")
            line = location_tag.get("line")
            if line:
                if location:
                    location += ':' + line
                else:
                    location = line
            locations.append(location)
        return locations

    def merge(self, otherunit, overwrite=False, comments=True, authoritative=False):
        super(tsunit, self).merge(otherunit, overwrite, comments)
        # TODO: check if this is necessary:
        if otherunit.isfuzzy():
            self.markfuzzy()
        else:
            self.markfuzzy(False)

    def isobsolete(self):
        return self._gettype() == "obsolete"

    def get_state_n(self):
        type = self._gettype()
        if type == "unfinished":
            # We want to distinguish between fuzzy and untranslated, which the
            # format doesn't really do
            if self.target:
                return self.S_FUZZY
            else:
                return self.S_UNTRANSLATED
        return self.statemap[type]

    def set_state_n(self, value):
        if value not in self.statemap_r:
            value = self.get_state_id(value)

        if value == self.S_UNTRANSLATED:
            # No real way of representing that in the format, so we just
            # handle it the same as unfinished
            value = self.S_FUZZY
        self._settype(self.statemap_r[value])


class tsfile(lisa.LISAfile):
    """Class representing a TS file store."""
    UnitClass = tsunit
    Name = "Qt Linguist Translation File"
    Mimetypes = ["application/x-linguist"]
    Extensions = ["ts"]
    rootNode = "TS"
    # We will switch out .body to fit with the context we are working on
    bodyNode = "context"
    XMLskeleton = '''<!DOCTYPE TS>
<TS>
</TS>
'''
    namespace = ''

    def __init__(self, *args, **kwargs):
        self._contextname = None
        lisa.LISAfile.__init__(self, *args, **kwargs)

    def initbody(self):
        """Initialises self.body."""
        self.namespace = self.document.getroot().nsmap.get(None, None)
        self.header = self.document.getroot()
        if self._contextname:
            self.body = self._getcontextnode(self._contextname)
        else:
            self.body = self.document.getroot()

    def getsourcelanguage(self):
        """Get the source language for this .ts file.

        The 'sourcelanguage' attribute was only added to the TS format in
        Qt v4.5. We return 'en' if there is no sourcelanguage set.

        We don't implement setsourcelanguage as users really shouldn't be
        altering the source language in .ts files, it should be set correctly
        by the extraction tools.

        :return: ISO code e.g. af, fr, pt_BR
        :rtype: String
        """
        lang = data.normalize_code(self.header.get('sourcelanguage', "en"))
        if lang == 'en-us':
            return 'en'
        return lang

    def gettargetlanguage(self):
        """Get the target language for this .ts file.

        :return: ISO code e.g. af, fr, pt_BR
        :rtype: String
        """
        return data.normalize_code(self.header.get('language'))

    def settargetlanguage(self, targetlanguage):
        """Set the target language for this .ts file to *targetlanguage*.

        :param targetlanguage: ISO code e.g. af, fr, pt_BR
        :type targetlanguage: String
        """
        if targetlanguage:
            self.header.set('language', targetlanguage)

    def _createcontext(self, contextname, comment=None):
        """Creates a context node with an optional comment"""
        context = etree.SubElement(self.document.getroot(), self.namespaced(self.bodyNode))
        name = etree.SubElement(context, self.namespaced("name"))
        name.text = contextname
        if comment:
            comment_node = context.SubElement(context, "comment")
            comment_node.text = comment
        return context

    def _getcontextname(self, contextnode):
        """Returns the name of the given context node."""
        return contextnode.find(self.namespaced("name")).text

    def _getcontextnames(self):
        """Returns all contextnames in this TS file."""
        contextnodes = self.document.findall(self.namespaced("context"))
        contextnames = [self.getcontextname(contextnode) for contextnode in contextnodes]
        return contextnames

    def _getcontextnode(self, contextname):
        """Returns the context node with the given name."""
        contextnodes = self.document.findall(self.namespaced("context"))
        for contextnode in contextnodes:
            if self._getcontextname(contextnode) == contextname:
                return contextnode
        return None

    def addunit(self, unit, new=True, contextname=None, createifmissing=True):
        """Adds the given unit to the last used body node (current context).

        If the contextname is specified, switch to that context (creating it
        if allowed by createifmissing)."""
        if contextname is None:
            contextname = unit.getcontextname()

        if self._contextname != contextname:
            if not self._switchcontext(contextname, createifmissing):
                return None
        super(tsfile, self).addunit(unit, new)
#        lisa.setXMLspace(unit.xmlelement, "preserve")
        return unit

    def _switchcontext(self, contextname, createifmissing=False):
        """Switch the current context to the one named contextname, optionally
        creating it if it doesn't exist."""
        self._contextname = contextname
        contextnode = self._getcontextnode(contextname)
        if contextnode is None:
            if not createifmissing:
                return False
            contextnode = self._createcontext(contextname)

        self.body = contextnode
        if self.body is None:
            return False
        return True

    def nplural(self):
        lang = self.header.get("language")
        if lang in NPLURALS:
            return NPLURALS[lang]
        else:
            return 1

    def __str__(self):
        """Converts to a string containing the file's XML.

        We have to override this to ensure mimic the Qt convention:
            - no XML decleration
            - plain DOCTYPE that lxml seems to ignore
        """
        # A bug in lxml means we have to output the doctype ourselves. For
        # more information, see:
        # http://codespeak.net/pipermail/lxml-dev/2008-October/004112.html
        # The problem was fixed in lxml 2.1.3
        output = etree.tostring(self.document, pretty_print=True,
                                xml_declaration=False, encoding='utf-8')
        if not "<!DOCTYPE TS>" in output[:30]:
            output = "<!DOCTYPE TS>" + output
        return output

########NEW FILE########
__FILENAME__ = txt
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This class implements the functionality for handling plain text files, or
similar wiki type files.

Supported formats are
  - Plain text
  - dokuwiki
  - MediaWiki
"""

import re

from translate.storage import base


dokuwiki = []
dokuwiki.append(("Dokuwiki heading", re.compile(r"( ?={2,6}[\s]*)(.+)"), re.compile("([\s]*={2,6}[\s]*)$")))
dokuwiki.append(("Dokuwiki bullet", re.compile(r"([\s]{2,}\*[\s]*)(.+)"), re.compile("[\s]+$")))
dokuwiki.append(("Dokuwiki numbered item", re.compile(r"([\s]{2,}-[\s]*)(.+)"), re.compile("[\s]+$")))

mediawiki = []
mediawiki.append(("MediaWiki heading", re.compile(r"(={1,5}[\s]*)(.+)"), re.compile("([\s]*={1,5}[\s]*)$")))
mediawiki.append(("MediaWiki bullet", re.compile(r"(\*+[\s]*)(.+)"), re.compile("[\s]+$")))
mediawiki.append(("MediaWiki numbered item", re.compile(r"(#+[\s]*)(.+)"), re.compile("[\s]+$")))

flavours = {
    "dokuwiki": dokuwiki,
    "mediawiki": mediawiki,
    None: [],
    "plain": [],
}


class TxtUnit(base.TranslationUnit):
    """This class represents a block of text from a text file"""

    def __init__(self, source="", encoding="utf-8"):
        """Construct the txtunit"""
        self.encoding = encoding
        super(TxtUnit, self).__init__(source)
        self.source = source
        self.pretext = ""
        self.posttext = ""
        self.location = []

    def __str__(self):
        """Convert a txt unit to a string"""
        string = u"".join([self.pretext, self.source, self.posttext])
        if isinstance(string, unicode):
            return string.encode(self.encoding)
        return string

    # Note that source and target are equivalent for monolingual units
    def setsource(self, source):
        """Sets the definition to the quoted value of source"""
        if isinstance(source, str):
            source = source.decode(self.encoding)
        self._rich_source = None
        self._source = source

    def getsource(self):
        """gets the unquoted source string"""
        return self._source
    source = property(getsource, setsource)

    def settarget(self, target):
        """Sets the definition to the quoted value of target"""
        self._rich_target = None
        self.source = target

    def gettarget(self):
        """gets the unquoted target string"""
        return self.source
    target = property(gettarget, settarget)

    def addlocation(self, location):
        self.location.append(location)

    def getlocations(self):
        return self.location


class TxtFile(base.TranslationStore):
    """This class represents a text file, made up of txtunits"""
    UnitClass = TxtUnit

    def __init__(self, inputfile=None, flavour=None, encoding="utf-8"):
        base.TranslationStore.__init__(self, unitclass=self.UnitClass)
        self.filename = getattr(inputfile, 'name', '')
        self.flavour = flavours.get(flavour, [])
        self.encoding = encoding
        if inputfile is not None:
            txtsrc = inputfile.readlines()
            self.parse(txtsrc)

    def parse(self, lines):
        """Read in text lines and create txtunits from the blocks of text"""
        block = []
        startline = 0
        pretext = ""
        posttext = ""
        if not isinstance(lines, list):
            lines = lines.split("\n")
        for linenum in range(len(lines)):
            line = lines[linenum].rstrip("\r\n")
            for rule, prere, postre in self.flavour:
                match = prere.match(line)
                if match:
                    pretext, source = match.groups()
                    postmatch = postre.search(source)
                    if postmatch:
                        posttext = postmatch.group()
                        source = source[:postmatch.start()]
                    block.append(source)
                    isbreak = True
                    break
            else:
                isbreak = not line.strip()
            if isbreak and block:
                unit = self.addsourceunit("\n".join(block))
                unit.addlocation("%s:%d" % (self.filename, startline + 1))
                unit.pretext = pretext
                unit.posttext = posttext
                pretext = ""
                posttext = ""
                block = []
            elif not isbreak:
                if not block:
                    startline = linenum
                block.append(line)
        if block:
            unit = self.addsourceunit("\n".join(block))
            unit.addlocation("%s:%d" % (self.filename, startline + 1))

    def __str__(self):
        source = self.getoutput()
        if isinstance(source, unicode):
            return source.encode(getattr(self, "encoding", "UTF-8"))
        return source

    def getoutput(self):
        """Convert the units back to blocks"""
        blocks = [str(unit) for unit in self.units]
        string = "\n\n".join(blocks)
        return string

########NEW FILE########
__FILENAME__ = utx
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Manage the Universal Terminology eXchange (UTX) format

UTX is a format for terminology exchange, designed it seems with Machine
Translation (MT) as it's primary consumer.  The format is created by
the Asia-Pacific Association for Machine Translation (AAMT).

It is a bilingual base class derived format with :class:`UtxFile`
and :class:`UtxUnit` providing file and unit level access.

The format can manage monolingual dictionaries but these classes don't
implement that.

Specification
    The format is implemented according to UTX v1.0 (No longer available from
    their website. The current `UTX version
    <http://www.aamt.info/english/utx/#Download>`_ may be downloaded instead).

Format Implementation
    The UTX format is a Tab Seperated Value (TSV) file in UTF-8.  The
    first two lines are headers with subsequent lines containing a
    single source target definition.

Encoding
    The files are UTF-8 encoded with no BOM and CR+LF line terminators.
"""

import csv
import time

from translate.storage import base


class UtxDialect(csv.Dialect):
    """Describe the properties of an UTX generated TAB-delimited dictionary
    file."""
    delimiter = "\t"
    # The spec says \r\n but there are older version < 1.0 with just \n
    # FIXME if we find older specs then lets see if we can support these
    # differences
    lineterminator = "\r\n"
    quoting = csv.QUOTE_NONE
csv.register_dialect("utx", UtxDialect)


class UtxHeader:
    """A UTX header entry

    A UTX header is a single line that looks like this::
        #UTX-S <version>; < source language >/< target language>;
        <date created>; <optional fields (creator, license, etc.)>

    Where::
        - UTX-S version is currently 1.00.
        - Source language/target language: ISO 639, 3166 formats.
          In the case of monolingual dictionary, target language should be
          omitted.
        - Date created: ISO 8601 format
        - Optional fields (creator, license, etc.)
    """


class UtxUnit(base.TranslationUnit):
    """A UTX dictionary unit"""

    def __init__(self, source=None):
        self._dict = {}
        if source:
            self.source = source
        super(UtxUnit, self).__init__(source)

    def getdict(self):
        """Get the dictionary of values for a UTX line"""
        return self._dict

    def setdict(self, newdict):
        """Set the dictionary of values for a UTX line

        :param newdict: a new dictionary with UTX line elements
        :type newdict: Dict
        """
        # TODO First check that the values are OK
        self._dict = newdict
    dict = property(getdict, setdict)

    def _get_field(self, key):
        if key not in self._dict:
            return None
        elif self._dict[key]:
            return self._dict[key].decode('utf-8')
        else:
            return ""

    def _set_field(self, key, newvalue):
        # FIXME update the header date
        if newvalue is None:
            self._dict[key] = None
        if isinstance(newvalue, unicode):
            newvalue = newvalue.encode('utf-8')
        if not key in self._dict or newvalue != self._dict[key]:
            self._dict[key] = newvalue

    def getnotes(self, origin=None):
        return self._get_field('comment')

    def addnote(self, text, origin=None, position="append"):
        currentnote = self._get_field('comment')
        if (position == "append" and
            currentnote is not None and
            currentnote != u''):
            self._set_field('comment', currentnote + '\n' + text)
        else:
            self._set_field('comment', text)

    def removenotes(self):
        self._set_field('comment', u'')

    def getsource(self):
        return self._get_field('src')

    def setsource(self, newsource):
        self._rich_source = None
        return self._set_field('src', newsource)
    source = property(getsource, setsource)

    def gettarget(self):
        return self._get_field('tgt')

    def settarget(self, newtarget):
        self._rich_target = None
        return self._set_field('tgt', newtarget)
    target = property(gettarget, settarget)

    def settargetlang(self, newlang):
        self._dict['target-lang'] = newlang
    targetlang = property(None, settargetlang)

    def __str__(self):
        return str(self._dict)

    def istranslated(self):
        return bool(self._dict.get('tgt', None))


class UtxFile(base.TranslationStore):
    """A UTX dictionary file"""
    Name = "UTX Dictionary"
    Mimetypes = ["text/x-utx"]
    Extensions = ["utx"]

    def __init__(self, inputfile=None, unitclass=UtxUnit):
        """Construct an UTX dictionary, optionally reading in from
        inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self.extension = ''
        self._fieldnames = ['src', 'tgt', 'src:pos']
        self._header = {
            "version": "1.00",
             "source_language": "en",
             "date_created": time.strftime("%FT%TZ%z",
                                           time.localtime(time.time()))
        }
        if inputfile is not None:
            self.parse(inputfile)

    def _read_header(self, header=None):
        """Read a UTX header"""
        if header is None:
            self._fieldnames = ['src', 'tgt', 'src:pos']
            # FIXME make the header properly
            self._header = {"version": "1.00"}
            return
        header_lines = []
        for line in header.split(UtxDialect.lineterminator):
            if line.startswith("#"):
                header_lines.append(line)
            else:
                break
        self._header = {}
        header_components = []
        for line in header_lines[:-1]:
            header_components += line[1:].split(";")
        self._header["version"] = header_components[0].replace("UTX-S ", "")
        languages = header_components[1].strip().split("/")
        self._header["source_language"] = languages[0]
        self._header["target_language"] = languages[1] or None
        self._header["date_created"] = header_components[2].strip()
        for data in header_components[3:]:
            key, value = data.strip().split(":")
            self._header[key] = value.strip()
        self._fieldnames = header_lines[-1:][0].replace("#", ""). split('\t')
        return len(header_lines)

    def _write_header(self):
        """Create a UTX header"""
        header = "#UTX-S %(version)s; %(src)s/%(tgt)s; %(date)s" % {
                    "version": self._header["version"],
                    "src": self._header["source_language"],
                    "tgt": self._header.get("target_language", ""),
                    "date": self._header["date_created"],
                 }
        items = []
        for key, value in self._header.iteritems():
            if key in ["version", "source_language",
                       "target_language", "date_created"]:
                continue
            items.append("%s: %s" % (key, value))
        if len(items):
            items = "; ".join(items)
            header += "; " + items
        header += UtxDialect.lineterminator
        header += "#" + "\t".join(self._fieldnames) + UtxDialect.lineterminator
        return header

    def getsourcelanguage(self):
        return self._header.get("source_language", None)

    def setsourcelanguage(self, sourcelanguage):
        self._header["source_language"] = sourcelanguage

    def gettargetlanguage(self):
        return self._header.get("target_language", None)

    def settargetlanguage(self, targetlanguage):
        self._header["target_language"] = targetlanguage

    def parse(self, input):
        """parsese the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            tmsrc = input.read()
            input.close()
            input = tmsrc
        try:
            header_length = self._read_header(input)
        except:
            raise base.ParseError("Cannot parse header")
        lines = csv.DictReader(
                    input.split(UtxDialect.lineterminator)[header_length:],
                    fieldnames=self._fieldnames,
                    dialect="utx")
        for line in lines:
            newunit = UtxUnit()
            newunit.dict = line
            self.addunit(newunit)

    def __str__(self):
        output = csv.StringIO()
        writer = csv.DictWriter(output, fieldnames=self._fieldnames,
                                dialect="utx")
        unit_count = 0
        for unit in self.units:
            if unit.istranslated():
                unit_count += 1
                writer.writerow(unit.dict)
        if unit_count == 0:
            return ""
        output.reset()
        return self._write_header() + "".join(output.readlines())

########NEW FILE########
__FILENAME__ = bzr
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command,
                                              youngest_ancestor)


def is_available():
    """check if bzr is installed"""
    exitcode, output, error = run_command(["bzr", "version"])
    return exitcode == 0


_version = None


def get_version():
    """return a tuple of (major, minor) for the installed bazaar client"""
    global _version
    if _version:
        return _version

    import re
    command = ["bzr", "--version"]
    exitcode, output, error = run_command(command)
    if exitcode == 0:
        version_line = output.splitlines()[0]
        version_match = re.search(r"\d+\.\d+", version_line)
        if version_match:
            major, minor = version_match.group().split(".")
            if (major.isdigit() and minor.isdigit()):
                _version = (int(major), int(minor))
                return _version
    # if anything broke before, then we return the invalid version number
    return (0, 0)


class bzr(GenericRevisionControlSystem):
    """Class to manage items under revision control of bzr."""

    RCS_METADIR = ".bzr"
    SCAN_PARENTS = True

    def update(self, revision=None, needs_revert=True):
        """Does a clean update of the given path"""
        output_revert = ""
        if needs_revert:
            # bzr revert
            command = ["bzr", "revert", self.location_abs]
            exitcode, output_revert, error = run_command(command)
            if exitcode != 0:
                raise IOError("[BZR] revert of '%s' failed: %s" % (
                              self.location_abs, error))

        # bzr pull
        command = ["bzr", "pull"]
        exitcode, output_pull, error = run_command(command)
        if exitcode != 0:
            raise IOError("[BZR] pull of '%s' failed: %s" % (
                          self.location_abs, error))
        return output_revert + output_pull

    def add(self, files, message=None, author=None):
        """Add and commit files."""
        files = prepare_filelist(files)
        command = ["bzr", "add"] + files
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[BZR] add in '%s' failed: %s" % (
                          self.location_abs, error))

        # go down as deep as possible in the tree to avoid accidental commits
        # TODO: explicitly commit files by name
        ancestor = youngest_ancestor(files)
        return output + type(self)(ancestor).commit(message, author)

    def commit(self, message=None, author=None):
        """Commits the file and supplies the given commit message if present"""
        # bzr commit
        command = ["bzr", "commit"]
        if message:
            command.extend(["-m", message])
        # the "--author" argument is supported since bzr v0.91rc1
        if author and (get_version() >= (0, 91)):
            command.extend(["--author", author])
        # the filename is the last argument
        command.append(self.location_abs)
        exitcode, output_commit, error = run_command(command)
        if exitcode != 0:
            raise IOError("[BZR] commit of '%s' failed: %s" % (
                          self.location_abs, error))
        # bzr push
        command = ["bzr", "push"]
        exitcode, output_push, error = run_command(command)
        if exitcode != 0:
            raise IOError("[BZR] push of '%s' failed: %s" % (
                          self.location_abs, error))
        return output_commit + output_push

    def getcleanfile(self, revision=None):
        """Get a clean version of a file from the bzr repository"""
        # bzr cat
        command = ["bzr", "cat", self.location_abs]
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[BZR] cat failed for '%s': %s" % (
                          self.location_abs, error))
        return output

########NEW FILE########
__FILENAME__ = cvs
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import os

from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command,
                                              youngest_ancestor)


def is_available():
    """check if cvs is installed"""
    exitcode, output, error = run_command(["cvs", "--version"])
    return exitcode == 0


class cvs(GenericRevisionControlSystem):
    """Class to manage items under revision control of CVS."""

    RCS_METADIR = "CVS"
    SCAN_PARENTS = False

    def _readfile(self, cvsroot, path, revision=None):
        """
        Read a single file from the CVS repository without checking out a full
        working directory.

        :param cvsroot: the CVSROOT for the repository
        :param path: path to the file relative to cvs root
        :param revision: revision or tag to get (retrieves from HEAD if None)
        """
        command = ["cvs", "-d", cvsroot, "-Q", "co", "-p"]
        if revision:
            command.extend(["-r", revision])
        # the path is the last argument
        command.append(path)
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[CVS] Could not read '%s' from '%s': %s / %s" % (
                          path, cvsroot, output, error))
        return output

    def getcleanfile(self, revision=None):
        """Get the content of the file for the given revision"""
        parentdir = os.path.dirname(self.location_abs)
        cvsdir = os.path.join(parentdir, "CVS")
        cvsroot = open(os.path.join(cvsdir, "Root"), "r").read().strip()
        cvspath = open(os.path.join(cvsdir, "Repository"), "r").read().strip()
        cvsfilename = os.path.join(cvspath, os.path.basename(self.location_abs))
        if revision is None:
            cvsentries = open(os.path.join(cvsdir, "Entries"), "r").readlines()
            revision = self._getcvstag(cvsentries)
        if revision == "BASE":
            cvsentries = open(os.path.join(cvsdir, "Entries"), "r").readlines()
            revision = self._getcvsrevision(cvsentries)
        return self._readfile(cvsroot, cvsfilename, revision)

    def update(self, revision=None, needs_revert=True):
        """Does a clean update of the given path"""
        # TODO: take needs_revert parameter into account
        working_dir = os.path.dirname(self.location_abs)
        filename = self.location_abs
        filename_backup = filename + os.path.extsep + "bak"
        # rename the file to be updated
        try:
            os.rename(filename, filename_backup)
        except OSError as error:
            raise IOError("[CVS] could not move the file '%s' to '%s': %s" % (
                          filename, filename_backup, error))
        command = ["cvs", "-Q", "update", "-C"]
        if revision:
            command.extend(["-r", revision])
        # the filename is the last argument
        command.append(os.path.basename(filename))
        # run the command within the given working_dir
        exitcode, output, error = run_command(command, working_dir)
        # restore backup in case of an error - remove backup for success
        try:
            if exitcode != 0:
                os.rename(filename_backup, filename)
            else:
                os.remove(filename_backup)
        except OSError:
            pass
        # raise an error or return successfully - depending on the CVS command
        if exitcode != 0:
            raise IOError("[CVS] Error running CVS command '%s': %s" %
                          (command, error))
        else:
            return output

    def add(self, files, message=None, author=None):
        """Add and commit the new files."""
        working_dir = os.path.dirname(self.location_abs)
        command = ["cvs", "-Q", "add"]
        if message:
            command.extend(["-m", message])
        files = prepare_filelist(files)
        command.extend(files)
        exitcode, output, error = run_command(command, working_dir)
        # raise an error or return successfully - depending on the CVS command
        if exitcode != 0:
            raise IOError("[CVS] Error running CVS command '%s': %s" %
                          (command, error))

        # go down as deep as possible in the tree to avoid accidental commits
        # TODO: explicitly commit files by name
        ancestor = youngest_ancestor(files)
        return output + type(self)(ancestor).commit(message, author)

    def commit(self, message=None, author=None):
        """Commits the file and supplies the given commit message if present

        the 'author' parameter is not suitable for CVS, thus it is ignored
        """
        working_dir = os.path.dirname(self.location_abs)
        filename = os.path.basename(self.location_abs)
        command = ["cvs", "-Q", "commit"]
        if message:
            command.extend(["-m", message])
        # the filename is the last argument
        command.append(filename)
        exitcode, output, error = run_command(command, working_dir)
        # raise an error or return successfully - depending on the CVS command
        if exitcode != 0:
            raise IOError("[CVS] Error running CVS command '%s': %s" %
                          (command, error))
        else:
            return output

    def _getcvsrevision(self, cvsentries):
        """returns the revision number the file was checked out with by looking
        in the lines of cvsentries
        """
        filename = os.path.basename(self.location_abs)
        for cvsentry in cvsentries:
            # an entries line looks like the following:
            #  /README.TXT/1.19/Sun Dec 16 06:00:12 2001//
            cvsentryparts = cvsentry.split("/")
            if len(cvsentryparts) < 6:
                continue
            if os.path.normcase(cvsentryparts[1]) == os.path.normcase(filename):
                return cvsentryparts[2].strip()
        return None

    def _getcvstag(self, cvsentries):
        """Returns the sticky tag the file was checked out with by looking in
        the lines of cvsentries.
        """
        filename = os.path.basename(self.location_abs)
        for cvsentry in cvsentries:
            # an entries line looks like the following:
            #  /README.TXT/1.19/Sun Dec 16 06:00:12 2001//
            cvsentryparts = cvsentry.split("/")
            if len(cvsentryparts) < 6:
                continue
            if os.path.normcase(cvsentryparts[1]) == os.path.normcase(filename):
                if cvsentryparts[5].startswith("T"):
                    return cvsentryparts[5][1:].strip()
        return None

########NEW FILE########
__FILENAME__ = darcs
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command,
                                              youngest_ancestor)


def is_available():
    """check if darcs is installed"""
    exitcode, output, error = run_command(["darcs", "--version"])
    return exitcode == 0


class darcs(GenericRevisionControlSystem):
    """Class to manage items under revision control of darcs."""

    RCS_METADIR = "_darcs"
    SCAN_PARENTS = True

    def update(self, revision=None, needs_revert=True):
        """Does a clean update of the given path

        :param revision: ignored for darcs
        """
        output_revert = ""
        if needs_revert:
            # revert local changes (avoids conflicts)
            command = ["darcs", "revert", "--repodir", self.root_dir,
                    "-a", self.location_rel]
            exitcode, output_revert, error = run_command(command)
            if exitcode != 0:
                raise IOError("[Darcs] error running '%s': %s" % (command, error))

        # pull new patches
        command = ["darcs", "pull", "--repodir", self.root_dir, "-a"]
        exitcode, output_pull, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Darcs] error running '%s': %s" % (command, error))
        return output_revert + output_pull

    def add(self, files, message=None, author=None):
        """Add and commit files."""
        files = prepare_filelist(files)
        command = ["darcs", "add", "--repodir", self.root_dir] + files
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Darcs] Error running darcs command '%s': %s" % (
                          command, error))

        # go down as deep as possible in the tree to avoid accidental commits
        # TODO: explicitly commit files by name
        ancestor = youngest_ancestor(files)
        return output + type(self)(ancestor).commit(message, author)

    def commit(self, message=None, author=None):
        """Commits the file and supplies the given commit message if present"""
        if message is None:
            message = ""
        # set change message
        command = ["darcs", "record", "-a", "--repodir", self.root_dir,
                "--skip-long-comment", "-m", message]
        # add the 'author' to the list of arguments if it was given
        if author:
            command.extend(["--author", author])
        # the location of the file is the last argument
        command.append(self.location_rel)
        exitcode, output_record, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Darcs] Error running darcs command '%s': %s" % (
                          command, error))
        # push changes
        command = ["darcs", "push", "-a", "--repodir", self.root_dir]
        exitcode, output_push, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Darcs] Error running darcs command '%s': %s" % (
                          command, error))
        return output_record + output_push

    def getcleanfile(self, revision=None):
        """Get a clean version of a file from the darcs repository

        :param revision: ignored for darcs
        """
        import os
        filename = os.path.join(self.root_dir, self.RCS_METADIR, 'pristine',
                self.location_rel)
        try:
            darcs_file = open(filename)
            output = darcs_file.read()
            darcs_file.close()
        except IOError as error:
            raise IOError("[Darcs] error reading original file '%s': %s" % (
                          filename, error))
        return output

########NEW FILE########
__FILENAME__ = git
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#
#
# Requires: git
#

import os

from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command)


def is_available():
    """check if git is installed"""
    exitcode, output, error = run_command(["git", "--version"])
    return exitcode == 0


class git(GenericRevisionControlSystem):
    """Class to manage items under revision control of git."""

    RCS_METADIR = ".git"
    SCAN_PARENTS = True

    def _get_git_dir(self):
        """git requires the git metadata directory for every operation
        """
        return os.path.join(self.root_dir, self.RCS_METADIR)

    def _get_git_command(self, args):
        """prepends generic git arguments to default ones
        """
        command = ["git", "--git-dir", self._get_git_dir()]
        command.extend(args)
        return command

    def _has_changes(self):
        command = self._get_git_command(["diff", "--cached", "--exit-code"])
        exitcode, output_checkout, error = run_command(command, self.root_dir)
        return bool(exitcode)

    def update(self, revision=None, needs_revert=True):
        """Does a clean update of the given path"""
        output_checkout = ""
        if needs_revert:
            # git checkout
            command = self._get_git_command(["checkout", self.location_rel])
            exitcode, output_checkout, error = run_command(command, self.root_dir)
            if exitcode != 0:
                raise IOError("[GIT] checkout failed (%s): %s" % (command, error))

        # pull changes
        command = self._get_git_command(["pull"])
        exitcode, output_pull, error = run_command(command, self.root_dir)
        if exitcode != 0:
            raise IOError("[GIT] pull failed (%s): %s" % (command, error))
        return output_checkout + output_pull

    def add(self, files, message=None, author=None):
        """Add and commit the new files."""
        args = ["add"] + prepare_filelist(files)
        command = self._get_git_command(args)
        exitcode, output, error = run_command(command, self.root_dir)
        if exitcode != 0:
            raise IOError("[GIT] add of files in '%s') failed: %s" % (
                          self.root_dir, error))

        return output + self.commit(message, author, add=False)

    def commit(self, message=None, author=None, add=True):
        """Commits the file and supplies the given commit message if present"""
        # add the file
        output_add = ""
        if add:
            command = self._get_git_command(["add", self.location_rel])
            exitcode, output_add, error = run_command(command, self.root_dir)
            if exitcode != 0:
                raise IOError("[GIT] add of ('%s', '%s') failed: %s" % (
                              self.root_dir, self.location_rel, error))

        if not self._has_changes():
            raise IOError("[GIT] no changes to commit")

        # commit file
        command = self._get_git_command(["commit"])
        if message:
            command.extend(["-m", message])
        if author:
            command.extend(["--author", author])
        exitcode, output_commit, error = run_command(command, self.root_dir)
        if exitcode != 0:
            if len(error):
                msg = error
            else:
                msg = output_commit
            raise IOError("[GIT] commit of ('%s', '%s') failed: %s" % (
                          self.root_dir, self.location_rel, msg))
        # push changes
        command = self._get_git_command(["push"])
        exitcode, output_push, error = run_command(command, self.root_dir)
        if exitcode != 0:
            raise IOError("[GIT] push of ('%s', '%s') failed: %s" % (
                          self.root_dir, self.location_rel, error))
        return output_add + output_commit + output_push

    def getcleanfile(self, revision=None):
        """Get a clean version of a file from the git repository"""
        # run git-show
        command = self._get_git_command(["show", "HEAD:%s" % self.location_rel])
        exitcode, output, error = run_command(command, self.root_dir)
        if exitcode != 0:
            raise IOError("[GIT] 'show' failed for ('%s', %s): %s" % (
                          self.root_dir, self.location_rel, error))
        return output

########NEW FILE########
__FILENAME__ = hg
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command,
                                              youngest_ancestor)


def is_available():
    """check if hg is installed"""
    exitcode, output, error = run_command(["hg", "--version"])
    return exitcode == 0


_version = None


def get_version():
    """Return a tuple of (major, minor) for the installed mercurial client."""
    global _version
    if _version:
        return _version

    import re
    command = ["hg", "--version"]
    exitcode, output, error = run_command(command)
    if exitcode == 0:
        version_line = output.splitlines()[0]
        version_match = re.search(r"\d+\.\d+", version_line)
        if version_match:
            major, minor = version_match.group().split(".")
            if (major.isdigit() and minor.isdigit()):
                _version = (int(major), int(minor))
                return _version
    # if anything broke before, then we return the invalid version number
    return (0, 0)


class hg(GenericRevisionControlSystem):
    """Class to manage items under revision control of mercurial."""

    RCS_METADIR = ".hg"
    SCAN_PARENTS = True

    def update(self, revision=None, needs_revert=True):
        """Does a clean update of the given path

        :param revision: ignored for hg
        """
        output_revert = ""
        if needs_revert:
            # revert local changes (avoids conflicts)
            command = ["hg", "-R", self.root_dir, "revert",
                    "--all", self.location_abs]
            exitcode, output_revert, error = run_command(command)
            if exitcode != 0:
                raise IOError("[Mercurial] error running '%s': %s" %
                              (command, error))

        # pull new patches
        command = ["hg", "-R", self.root_dir, "pull"]
        exitcode, output_pull, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] error running '%s': %s" %
                          (command, error))
        # update working directory
        command = ["hg", "-R", self.root_dir, "update"]
        exitcode, output_update, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] error running '%s': %s" %
                          (command, error))
        return output_revert + output_pull + output_update

    def add(self, files, message=None, author=None):
        """Add and commit the new files."""
        files = prepare_filelist(files)
        command = ["hg", "add", "-q"] + files
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] Error running '%s': %s" %
                          (command, error))

        # go down as deep as possible in the tree to avoid accidental commits
        # TODO: explicitly commit files by name
        ancestor = youngest_ancestor(files)
        return output + type(self)(ancestor).commit(message, author)

    def commit(self, message=None, author=None):
        """Commits the file and supplies the given commit message if present"""
        if message is None:
            message = ""
        # commit changes
        command = ["hg", "-R", self.root_dir, "commit", "-m", message]
        # add the 'author' argument, if it was given (only supported
        # since v1.0)
        if author and (get_version() >= (1, 0)):
            command.extend(["--user", author])
        # the location is the last argument
        command.append(self.location_abs)
        exitcode, output_commit, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] Error running '%s': %s" % (
                          command, error))
        # push changes
        command = ["hg", "-R", self.root_dir, "push"]
        exitcode, output_push, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] Error running '%s': %s" % (
                          command, error))
        return output_commit + output_push

    def getcleanfile(self, revision=None):
        """Get a clean version of a file from the hg repository"""
        # run hg cat
        command = ["hg", "-R", self.root_dir, "cat",
                self.location_abs]
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[Mercurial] Error running '%s': %s" % (
                          command, error))
        return output

########NEW FILE########
__FILENAME__ = svn
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2008,2012 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.


from translate.storage.versioncontrol import (GenericRevisionControlSystem,
                                              prepare_filelist, run_command,
                                              youngest_ancestor)


def is_available():
    """check if svn is installed"""
    exitcode, output, error = run_command(["svn", "--version"])
    return exitcode == 0

_version = None


def get_version():
    """return a tuple of (major, minor) for the installed subversion client"""
    global _version
    if _version:
        return _version

    command = ["svn", "--version", "--quiet"]
    exitcode, output, error = run_command(command)
    if exitcode == 0:
        major, minor = output.strip().split(".")[0:2]
        if (major.isdigit() and minor.isdigit()):
            _version = (int(major), int(minor))
            return _version
    # something went wrong above
    return (0, 0)


class svn(GenericRevisionControlSystem):
    """Class to manage items under revision control of Subversion."""

    RCS_METADIR = ".svn"
    SCAN_PARENTS = False

    def update(self, revision=None, needs_revert=True):
        """update the working copy - remove local modifications if necessary"""
        output_revert = ""
        if needs_revert:
            # revert the local copy (remove local changes)
            command = ["svn", "revert", self.location_abs]
            exitcode, output_revert, error = run_command(command)
            # any errors?
            if exitcode != 0:
                raise IOError("[SVN] Subversion error running '%s': %s" %
                              (command, error))

        # update the working copy to the given revision
        command = ["svn", "update"]
        if not revision is None:
            command.extend(["-r", revision])
        # the filename is the last argument
        command.append(self.location_abs)
        exitcode, output_update, error = run_command(command)
        if exitcode != 0:
            raise IOError("[SVN] Subversion error running '%s': %s" %
                          (command, error))
        return output_revert + output_update

    def add(self, files, message=None, author=None):
        """Add and commit the new files."""
        files = prepare_filelist(files)
        command = ["svn", "add", "-q", "--non-interactive", "--parents"] + files
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[SVN] Error running SVN command '%s': %s" %
                          (command, error))

        # go down as deep as possible in the tree to avoid accidental commits
        # TODO: explicitly commit files by name
        ancestor = youngest_ancestor(files)
        return output + type(self)(ancestor).commit(message, author)

    def commit(self, message=None, author=None):
        """commit the file and return the given message if present

        the 'author' parameter is used for revision property 'translate:author'
        """
        command = ["svn", "-q", "--non-interactive", "commit", "-m", message or ""]
        # the "--with-revprop" argument is support since svn v1.5
        if author and (get_version() >= (1, 5)):
            command.extend(["--with-revprop", "translate:author=%s" % author])
        # the location is the last argument
        command.append(self.location_abs)
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[SVN] Error running SVN command '%s': %s" %
                          (command, error))
        return output

    def getcleanfile(self, revision=None):
        """return the content of the 'head' revision of the file"""
        command = ["svn", "cat"]
        if not revision is None:
            command.extend(["-r", revision])
        # the filename is the last argument
        command.append(self.location_abs)
        exitcode, output, error = run_command(command)
        if exitcode != 0:
            raise IOError("[SVN] Subversion error running '%s': %s" %
                          (command, error))
        return output

########NEW FILE########
__FILENAME__ = test_helper
# -*- coding: utf-8 -*-

import os.path
import shutil

from translate.storage.versioncontrol import get_versioned_object, run_command


class HelperTest(object):

    def remove_dirs(self, path):
        if os.path.exists(path):
            shutil.rmtree(path)

    def get_test_path(self, method):
        return os.path.realpath("%s_%s" % (self.__class__.__name__, method.__name__))

    def setup_method(self, method):
        """Allocates a unique self.filename for the method, making sure it doesn't exist"""
        self.path = self.get_test_path(method)
        self.co_path = os.path.join(self.path, "checkout")
        self.remove_dirs(self.path)
        os.makedirs(self.path)
        self.setup_repo_and_checkout()

    def setup_repo_and_checkout(self):
        """Implementations should override this to create a repository and a
        clone/checkout.

        The repository should be in 'repo'.
        The checkout/clone should be in 'checkout'.
        """
        pass

    def teardown_method(self, method):
        """Makes sure that if self.filename was created by the method, it is cleaned up"""
        self.remove_dirs(self.path)

    def create_files(self, files_dict):
        """Creates file(s) named after the keys, with contents from the values
        of the dictionary."""
        for name, content in files_dict.items():
            assert not os.path.isabs(name)
            dirs = os.path.dirname(name)
            if dirs:
                os.path.makedirs(os.path.join(self.co_path, dirs))
            f = open(os.path.join(self.co_path, dirs, name), 'w')
            f.write(content)
            f.close()

########NEW FILE########
__FILENAME__ = test_svn
# -*- coding: utf-8 -*-

import os.path

from translate.storage.versioncontrol import (get_versioned_object, run_command,
                                              svn)
from translate.storage.versioncontrol.test_helper import HelperTest


class TestSVN(HelperTest):

    def setup_repo_and_checkout(self):
        run_command(["svnadmin", "create", "repo"], cwd=self.path)
        run_command(["svn", "co", "file:///%s/repo" % self.path, "checkout"], cwd=self.path)

    def test_detection(self):
        print(self.co_path)
        o = get_versioned_object(self.co_path)
        assert isinstance(o, svn.svn)
        assert o.location_abs == self.co_path

    def test_add(self):
        o = get_versioned_object(self.co_path)
        self.create_files({
            "test1.txt": "First file\n",
            "test2.txt": "Second file\n",
        })
        file_path = os.path.join(self.co_path, "test1.txt")
        o.add(os.path.join(file_path))
        o = get_versioned_object(file_path)
        assert os.path.samefile(o.location_abs, file_path)

        assert o.getcleanfile() == "First file\n"

########NEW FILE########
__FILENAME__ = wordfast
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Manage the Wordfast Translation Memory format

Wordfast TM format is the Translation Memory format used by the
`Wordfast <http://www.wordfast.net/>`_ computer aided translation tool.

It is a bilingual base class derived format with :class:`WordfastTMFile`
and :class:`WordfastUnit` providing file and unit level access.

Wordfast is a computer aided translation tool.  It is an application
built on top of Microsoft Word and is implemented as a rather
sophisticated set of macros.  Understanding that helps us understand
many of the seemingly strange choices around this format including:
encoding, escaping and file naming.

Implementation
    The implementation covers the full requirements of a Wordfast TM file.
    The files are simple Tab Separated Value (TSV) files that can be read
    by Microsoft Excel and other spreadsheet programs.  They use the .txt
    extension which does make it more difficult to automatically identify
    such files.

    The dialect of the TSV files is specified by :class:`WordfastDialect`.

Encoding
    The files are UTF-16 or ISO-8859-1 (Latin1) encoded.  These choices
    are most likely because Microsoft Word is the base editing tool for
    Wordfast.

    The format is tab separated so we are able to detect UTF-16 vs Latin-1
    by searching for the occurance of a UTF-16 tab character and then
    continuing with the parsing.

Timestamps
    :class:`WordfastTime` allows for the correct management of the Wordfast
    YYYYMMDD~HHMMSS timestamps.  However, timestamps on individual units are
    not updated when edited.

Header
    :class:`WordfastHeader` provides header management support.  The header
    functionality is fully implemented through observing the behaviour of the
    files in real use cases, input from the Wordfast programmers and
    public documentation.

Escaping
    Wordfast TM implements a form of escaping that covers two aspects:

    1. Placeable: bold, formating, etc.  These are left as is and ignored.  It
       is up to the editor and future placeable implementation to manage these.

    2. Escapes: items that may confuse Excel or translators are escaped as
       ``&'XX;``. These are fully implemented and are converted to and from
       Unicode.  By observing behaviour and reading documentation we where able
       to observe all possible escapes. Unfortunately the escaping differs
       slightly between Windows and Mac version.  This might cause errors in
       future.  Functions allow for ``<_wf_to_char>`` and back to Wordfast
       escape (``<_char_to_wf>``).

Extended Attributes
    The last 4 columns allow users to define and manage extended attributes.
    These are left as is and are not directly managed byour implemenation.
"""

import csv
import time

from translate.storage import base


WF_TIMEFORMAT = "%Y%m%d~%H%M%S"
"""Time format used by Wordfast"""

WF_FIELDNAMES_HEADER = ["date", "userlist", "tucount", "src-lang", "version",
                        "target-lang", "license", "attr1list", "attr2list",
                        "attr3list", "attr4list", "attr5list"]
"""Field names for the Wordfast header"""

WF_FIELDNAMES = ["date", "user", "reuse", "src-lang", "source", "target-lang",
                 "target", "attr1", "attr2", "attr3", "attr4"]
"""Field names for a Wordfast TU"""

WF_FIELDNAMES_HEADER_DEFAULTS = {
    "date": "%19000101~121212",
    "userlist": "%User ID,TT,TT Translate-Toolkit",
    "tucount": "%TU=00000001",
    "src-lang": "%EN-US",
    "version": "%Wordfast TM v.5.51w9/00",
    "target-lang": "",
    "license": "%---00000001",
    "attr1list": "",
    "attr2list": "",
    "attr3list": "",
    "attr4list": "",
}
"""Default or minimum header entries for a Wordfast file"""

# TODO Needs validation.  The following need to be checked against a WF TM file
# to ensure that the correct Unicode values have been chosen for the characters.
# For now these look correct and have been taken from Windows CP1252 and
# Macintosh code points found for the respective character sets on Linux.
WF_ESCAPE_MAP = (
              ("&'26;", u"\u0026"),  # & - Ampersand (must be first to prevent
                                     #     escaping of escapes)
              ("&'82;", u"\u201A"),  #  - Single low-9 quotation mark
              ("&'85;", u"\u2026"),  #  - Elippsis
              ("&'91;", u"\u2018"),  #  - left single quotation mark
              ("&'92;", u"\u2019"),  #  - right single quotation mark
              ("&'93;", u"\u201C"),  #  - left double quotation mark
              ("&'94;", u"\u201D"),  #  - right double quotation mark
              ("&'96;", u"\u2013"),  #  - en dash (validate)
              ("&'97;", u"\u2014"),  #  - em dash (validate)
              ("&'99;", u"\u2122"),  #  - Trade mark
              # Windows only
              ("&'A0;", u"\u00A0"),  #  -Non breaking space
              ("&'A9;", u"\u00A9"),  #  - Copyright
              ("&'AE;", u"\u00AE"),  #  - Registered
              ("&'BC;", u"\u00BC"),  # 
              ("&'BD;", u"\u00BD"),  # 
              ("&'BE;", u"\u00BE"),  # 
              # Mac only
              ("&'A8;", u"\u00AE"),  #  - Registered
              ("&'AA;", u"\u2122"),  #  - Trade mark
              ("&'C7;", u"\u00AB"),  #  - Left-pointing double angle quotation mark
              ("&'C8;", u"\u00BB"),  #  - Right-pointing double angle quotation mark
              ("&'C9;", u"\u2026"),  #  - Horizontal Elippsis
              ("&'CA;", u"\u00A0"),  #  -Non breaking space
              ("&'D0;", u"\u2013"),  #  - en dash (validate)
              ("&'D1;", u"\u2014"),  #  - em dash (validate)
              ("&'D2;", u"\u201C"),  #  - left double quotation mark
              ("&'D3;", u"\u201D"),  #  - right double quotation mark
              ("&'D4;", u"\u2018"),  #  - left single quotation mark
              ("&'D5;", u"\u2019"),  #  - right single quotation mark
              ("&'E2;", u"\u201A"),  #  - Single low-9 quotation mark
              ("&'E3;", u"\u201E"),  #  - Double low-9 quotation mark
              # Other markers
              #("&'B;", u"\n"), # Soft-break - XXX creates a problem with
                                # roundtripping could also be represented
                                # by \u2028
             )
"""Mapping of Wordfast &'XX; escapes to correct Unicode characters"""

TAB_UTF16 = "\x00\x09"
"""The tab \\t character as it would appear in UTF-16 encoding"""


def _char_to_wf(string):
    """Char -> Wordfast &'XX; escapes

       Full roundtripping is not possible because of the escaping of
       NEWLINE \\n and TAB \\t"""
    # FIXME there is no platform check to ensure that we use Mac encodings
    # when running on a Mac
    if string:
        for code, char in WF_ESCAPE_MAP:
            string = string.replace(char.encode('utf-8'), code)
        string = string.replace("\n", "\\n").replace("\t", "\\t")
    return string


def _wf_to_char(string):
    """Wordfast &'XX; escapes -> Char"""
    if string:
        for code, char in WF_ESCAPE_MAP:
            string = string.replace(code, char.encode('utf-8'))
        string = string.replace("\\n", "\n").replace("\\t", "\t")
    return string


class WordfastDialect(csv.Dialect):
    """Describe the properties of a Wordfast generated TAB-delimited file."""
    delimiter = "\t"
    lineterminator = "\r\n"
    quoting = csv.QUOTE_NONE
csv.register_dialect("wordfast", WordfastDialect)


class WordfastTime(object):
    """Manages time stamps in the Wordfast format of YYYYMMDD~hhmmss"""

    def __init__(self, newtime=None):
        self._time = None
        if not newtime:
            self.time = None
        elif isinstance(newtime, basestring):
            self.timestring = newtime
        elif isinstance(newtime, time.struct_time):
            self.time = newtime

    def get_timestring(self):
        """Get the time in the Wordfast time format"""
        if not self._time:
            return None
        else:
            return time.strftime(WF_TIMEFORMAT, self._time)

    def set_timestring(self, timestring):
        """Set the time_sturct object using a Wordfast time formated string

        :param timestring: A Wordfast time string (YYYMMDD~hhmmss)
        :type timestring: String
        """
        self._time = time.strptime(timestring, WF_TIMEFORMAT)
    timestring = property(get_timestring, set_timestring)

    def get_time(self):
        """Get the time_struct object"""
        return self._time

    def set_time(self, newtime):
        """Set the time_struct object

        :param newtime: a new time object
        :type newtime: time.time_struct
        """
        if newtime and isinstance(newtime, time.struct_time):
            self._time = newtime
        else:
            self._time = None
    time = property(get_time, set_time)

    def __str__(self):
        if not self.timestring:
            return ""
        else:
            return self.timestring


class WordfastHeader(object):
    """A wordfast translation memory header"""

    def __init__(self, header=None):
        self._header_dict = []
        if not header:
            self.header = self._create_default_header()
        elif isinstance(header, dict):
            self.header = header

    def _create_default_header(self):
        """Create a default Wordfast header with the date set to the current
        time"""
        defaultheader = WF_FIELDNAMES_HEADER_DEFAULTS
        defaultheader['date'] = '%%%s' % WordfastTime(time.localtime()).timestring
        return defaultheader

    def getheader(self):
        """Get the header dictionary"""
        return self._header_dict

    def setheader(self, newheader):
        self._header_dict = newheader
    header = property(getheader, setheader)

    def settargetlang(self, newlang):
        self._header_dict['target-lang'] = '%%%s' % newlang
    targetlang = property(None, settargetlang)

    def settucount(self, count):
        self._header_dict['tucount'] = '%%TU=%08d' % count
    tucount = property(None, settucount)


class WordfastUnit(base.TranslationUnit):
    """A Wordfast translation memory unit"""

    def __init__(self, source=None):
        self._dict = {}
        if source:
            self.source = source
        super(WordfastUnit, self).__init__(source)

    def _update_timestamp(self):
        """Refresh the timestamp for the unit"""
        self._dict['date'] = WordfastTime(time.localtime()).timestring

    def getdict(self):
        """Get the dictionary of values for a Wordfast line"""
        return self._dict

    def setdict(self, newdict):
        """Set the dictionary of values for a Wordfast line

        :param newdict: a new dictionary with Wordfast line elements
        :type newdict: Dict
        """
        # TODO First check that the values are OK
        self._dict = newdict
    dict = property(getdict, setdict)

    def _get_source_or_target(self, key):
        if self._dict.get(key, None) is None:
            return None
        elif self._dict[key]:
            return _wf_to_char(self._dict[key]).decode('utf-8')
        else:
            return ""

    def _set_source_or_target(self, key, newvalue):
        if newvalue is None:
            self._dict[key] = None
        if isinstance(newvalue, unicode):
            newvalue = newvalue.encode('utf-8')
        newvalue = _char_to_wf(newvalue)
        if not key in self._dict or newvalue != self._dict[key]:
            self._dict[key] = newvalue
            self._update_timestamp()

    def getsource(self):
        return self._get_source_or_target('source')

    def setsource(self, newsource):
        self._rich_source = None
        return self._set_source_or_target('source', newsource)
    source = property(getsource, setsource)

    def gettarget(self):
        return self._get_source_or_target('target')

    def settarget(self, newtarget):
        self._rich_target = None
        return self._set_source_or_target('target', newtarget)
    target = property(gettarget, settarget)

    def settargetlang(self, newlang):
        self._dict['target-lang'] = newlang
    targetlang = property(None, settargetlang)

    def __str__(self):
        return str(self._dict)

    def istranslated(self):
        if not self._dict.get('source', None):
            return False
        return bool(self._dict.get('target', None))


class WordfastTMFile(base.TranslationStore):
    """A Wordfast translation memory file"""
    Name = "Wordfast Translation Memory"
    Mimetypes = ["application/x-wordfast"]
    Extensions = ["txt"]

    def __init__(self, inputfile=None, unitclass=WordfastUnit):
        """construct a Wordfast TM, optionally reading in from inputfile."""
        self.UnitClass = unitclass
        base.TranslationStore.__init__(self, unitclass=unitclass)
        self.filename = ''
        self.header = WordfastHeader()
        self._encoding = 'iso-8859-1'
        if inputfile is not None:
            self.parse(inputfile)

    def parse(self, input):
        """parsese the given file or file source string"""
        if hasattr(input, 'name'):
            self.filename = input.name
        elif not getattr(self, 'filename', ''):
            self.filename = ''
        if hasattr(input, "read"):
            tmsrc = input.read()
            input.close()
            input = tmsrc
        if TAB_UTF16 in input.split("\n")[0]:
            self._encoding = 'utf-16'
        else:
            self._encoding = 'iso-8859-1'
        try:
            input = input.decode(self._encoding).encode('utf-8')
        except:
            raise ValueError("Wordfast files are either UTF-16 (UCS2) or ISO-8859-1 encoded")
        for header in csv.DictReader(input.split("\n")[:1],
                                    fieldnames=WF_FIELDNAMES_HEADER,
                                    dialect="wordfast"):
            self.header = WordfastHeader(header)
        lines = csv.DictReader(input.split("\n")[1:],
                                fieldnames=WF_FIELDNAMES,
                                dialect="wordfast")
        for line in lines:
            newunit = WordfastUnit()
            newunit.dict = line
            self.addunit(newunit)

    def __str__(self):
        output = csv.StringIO()
        header_output = csv.StringIO()
        writer = csv.DictWriter(output, fieldnames=WF_FIELDNAMES,
                                dialect="wordfast")
        unit_count = 0
        for unit in self.units:
            if unit.istranslated():
                unit_count += 1
                writer.writerow(unit.dict)
        if unit_count == 0:
            return ""
        output.reset()
        self.header.tucount = unit_count
        outheader = csv.DictWriter(header_output,
                                    fieldnames=WF_FIELDNAMES_HEADER,
                                    dialect="wordfast")
        outheader.writerow(self.header.header)
        header_output.reset()
        decoded = "".join(header_output.readlines() + output.readlines()).decode('utf-8')
        try:
            return decoded.encode(self._encoding)
        except UnicodeEncodeError:
            return decoded.encode('utf-16')

########NEW FILE########
__FILENAME__ = workflow
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""
A workflow is defined by a set of states that a translation unit can be in and
the (allowed) transitions between these states. A state is defined by a range
between -128 and 127, indicating its level of "completeness". The range is
closed at the beginning and open at the end. That is, if a workflow contains
states A, B and C where A < B < C, a unit with state number n is in state A if
A <= n < B, state B if B <= n < C or state C if C <= n < MAX.

A value of 0 is typically the "empty" or "new" state with negative values
reserved for states like "obsolete" or "do not use".

Format specific workflows should be defined in such a way that the numeric
state values correspond to similar states. For example state 0 should be
"untranslated" in PO and "new" or "empty" in XLIFF, state 100 should be
"translated" in PO and "final" in XLIFF. This allows formats to implicitly
define similar states.
"""


class StateEnum:
    """Only contains the constants for default states."""
    MIN = -128
    OBSOLETE = -100
    EMPTY = 0
    NEEDS_WORK = 30
    REJECTED = 60
    NEEDS_REVIEW = 80
    UNREVIEWED = 100
    FINAL = 120
    MAX = 127


class State(object):

    def __init__(self, name, enter_action=None, leave_action=None):
        self.name = name
        self.enter_action = enter_action
        self.leave_action = leave_action

    def __eq__(self, rhs):
        return self.name == rhs.name

    def __repr__(self):
        return '<State "%s">' % (self.name)

    def enter(self, obj):
        if not self.enter_action or not callable(self.enter_action):
            return
        self.enter_action(obj)

    def leave(self, obj):
        if not self.leave_action or not callable(self.leave_action):
            return
        self.leave_action(obj)


class UnitState(State):

    def __init__(self, name, state_value):
        self.state_value = state_value
        super(UnitState, self).__init__(name, self._enter)

    def __repr__(self):
        return '<UnitState name=%s value=%d>' % (self.name, self.state_value)

    def _enter(self, unit):
        unit.set_state_n(self.state_value)


class WorkflowError(Exception):
    pass


class NoInitialStateError(WorkflowError):
    pass


class TransitionError(WorkflowError):
    pass


class InvalidStateObjectError(WorkflowError):

    def __init__(self, obj):
        super(InvalidStateObjectError, self).__init__('Invalid state object: %s' % (obj))


class StateNotInWorkflowError(Exception):

    def __init__(self, state):
        super(StateNotInWorkflowError, self).__init__(
            'State not in workflow: %s' % (state))


class Workflow(object):

    # INITIALISERS #
    def __init__(self, wf_obj=None):
        self._current_state = None
        self._edges = []
        self._initial_state = None
        self._states = []
        self._workflow_obj = wf_obj

    # ACCESSORS #
    def _get_edges(self):
        return list(self._edges)
    edges = property(_get_edges)

    def _get_states(self):
        return list(self._states)
    states = property(_get_states)

    # METHODS #
    def add_edge(self, from_state, to_state):
        if isinstance(from_state, basestring):
            from_state = self.get_state_by_name(from_state)
        if isinstance(to_state, basestring):
            to_state = self.get_state_by_name(to_state)
        for s in (from_state, to_state):
            if s not in self.states:
                raise StateNotInWorkflowError(s)
        if (from_state, to_state) in self.edges:
            return  # Edge already exists. Return quietly

        self._edges.append((from_state, to_state))

    def add_state(self, state):
        if not isinstance(state, State):
            raise InvalidStateObjectError(state)
        if state in self.states:
            raise ValueError('State already in workflow: %s' % (state))
        self._states.append(state)
        if self._initial_state is None:
            self._initial_state = state

    def get_from_states(self):
        """Returns a list of states that can be transitioned from to the
            current state."""
        return [e[0] for e in self.edges if e[1] is self._current_state]

    def get_to_states(self):
        """Returns a list of states that can be transitioned to from the
            current state."""
        return [e[1] for e in self.edges if e[0] is self._current_state]

    def get_state_by_name(self, state_name):
        """Get the ``State`` object for the given name."""
        for s in self.states:
            if s.name == state_name:
                return s
        else:
            raise StateNotInWorkflowError(state_name)

    def set_current_state(self, state):
        """Set the current state. This is absolute and not subject to edge
            constraints. The current state's ``leave`` and the new state's
            ``enter`` method is still called. For edge transitions, see the
            ``trans`` method."""
        if isinstance(state, basestring):
            state = self.get_state_by_name(state)
        if state not in self.states:
            raise StateNotInWorkflowError(state)

        if self._current_state:
            self._current_state.leave(self._workflow_obj)
        self._current_state = state
        self._current_state.enter(self._workflow_obj)

    def set_initial_state(self, state):
        """Sets the initial state, used by the :meth:`.reset` method."""
        if isinstance(state, basestring):
            state = self.get_state_by_name(state)
        if not isinstance(state, State):
            raise InvalidStateObjectError(state)
        if state not in self.states:
            raise StateNotInWorkflowError(state)
        self._initial_state = state

    def reset(self, wf_obj, init_state=None):
        """Reset the work flow to the initial state using the given object."""
        self._workflow_obj = wf_obj
        if init_state is not None:
            if isinstance(init_state, basestring):
                init_state = self.get_state_by_name(init_state)
            if init_state not in self.states:
                raise StateNotInWorkflowError()
            self._initial_state = init_state
            self._current_state = init_state
            return
        if self._initial_state is None:
            raise NoInitialStateError()
        self._current_state = None
        self.set_current_state(self._initial_state)

    def trans(self, to_state=None):
        """Transition to the given state. If no state is given, the first one
            returned by ``get_to_states`` is used."""
        if self._current_state is None:
            raise ValueError('No current state set')
        if isinstance(to_state, basestring):
            to_state = self.get_state_by_name(to_state)
        if to_state is None:
            to_state = self.get_to_states()
            if not to_state:
                raise TransitionError('No state to transition to')
            to_state = to_state[0]
        if to_state not in self.states:
            raise StateNotInWorkflowError(to_state)
        if (self._current_state, to_state) not in self.edges:
            raise TransitionError('No edge between edges %s and %s' % (
                                  self._current_state, to_state))
        self._current_state.leave(self._workflow_obj)
        self._current_state = to_state
        self._current_state.enter(self._workflow_obj)


def create_unit_workflow(unit, state_names):
    wf = Workflow(unit)

    state_info = unit.STATE.items()
    state_info.sort(key=lambda x: x[0])

    init_state, prev_state = None, None
    for state_id, state_range in state_info:
        if state_range[0] < 0:
            continue
        state_name = state_names[state_id]
        # We use the low end value below, because the range is closed there
        state = UnitState(state_name, state_range[0])
        wf.add_state(state)

        # Use the first non-negative state as the initial state...
        if init_state is None and state_range[0] >= 0:
            init_state = state

        if prev_state:
            wf.add_edge(prev_state, state_name)
        prev_state = state_name

    if init_state:
        wf.set_initial_state(init_state)

    return wf

########NEW FILE########
__FILENAME__ = xliff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005-2011 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Module for handling XLIFF files for translation.

The official recommendation is to use the extention .xlf for XLIFF files.
"""

from lxml import etree

from translate.misc.multistring import multistring
from translate.storage import base, lisa
from translate.storage.lisa import getXMLspace
from translate.storage.placeables.lisa import strelem_to_xml, xml_to_strelem
from translate.storage.workflow import StateEnum as state


# TODO: handle translation types

ID_SEPARATOR = u"\04"
# ID_SEPARATOR is commonly used through toolkit to generate compound
# unit ids (for instance to concatenate msgctxt and msgid in po), but
# \04 is an illegal char in XML 1.0, ID_SEPARATOR_SAFE will be used
# instead when converting between xliff and other toolkit supported
# formats
ID_SEPARATOR_SAFE = u"__%04__"


class xliffunit(lisa.LISAunit):
    """A single term in the xliff file."""

    rootNode = "trans-unit"
    languageNode = "source"
    textNode = ""
    namespace = 'urn:oasis:names:tc:xliff:document:1.1'

    _default_xml_space = "default"

    # TODO: id and all the trans-unit level stuff

    S_UNTRANSLATED = state.EMPTY
    S_NEEDS_TRANSLATION = state.NEEDS_WORK
    S_NEEDS_REVIEW = state.NEEDS_REVIEW
    S_TRANSLATED = state.UNREVIEWED
    S_SIGNED_OFF = state.FINAL

    statemap = {
                "new": S_UNTRANSLATED + 1,
                "needs-translation": S_NEEDS_TRANSLATION,
                "needs-adaptation": S_NEEDS_TRANSLATION + 1,
                "needs-l10n": S_NEEDS_TRANSLATION + 2,
                "needs-review-translation": S_NEEDS_REVIEW,
                "needs-review-adaptation": S_NEEDS_REVIEW + 1,
                "needs-review-l10n": S_NEEDS_REVIEW + 2,
                "translated": S_TRANSLATED,
                "signed-off": S_SIGNED_OFF,
                "final": S_SIGNED_OFF + 1,
                }

    statemap_r = dict((i[1], i[0]) for i in statemap.iteritems())

    STATE = {
        S_UNTRANSLATED: (state.EMPTY, state.NEEDS_WORK),
        S_NEEDS_TRANSLATION: (state.NEEDS_WORK, state.NEEDS_REVIEW),
        S_NEEDS_REVIEW: (state.NEEDS_REVIEW, state.UNREVIEWED),
        S_TRANSLATED: (state.UNREVIEWED, state.FINAL),
        S_SIGNED_OFF: (state.FINAL, state.MAX),
    }

    def __init__(self, source, empty=False, **kwargs):
        """Override the constructor to set xml:space="preserve"."""
        super(xliffunit, self).__init__(source, empty, **kwargs)
        if empty:
            return
        lisa.setXMLspace(self.xmlelement, "preserve")

    def createlanguageNode(self, lang, text, purpose):
        """Returns an xml Element setup with given parameters."""

        # TODO: for now we do source, but we have to test if it is target,
        # perhaps with parameter. Alternatively, we can use lang, if
        # supplied, since an xliff file has to conform to the bilingual
        # nature promised by the header.
        assert purpose
        langset = etree.Element(self.namespaced(purpose))
        # TODO: check language
        #lisa.setXMLlang(langset, lang)

        langset.text = text
        return langset

    def getlanguageNodes(self):
        """We override this to get source and target nodes."""
        source = None
        target = None
        nodes = []
        try:
            source = self.xmlelement.iterchildren(self.namespaced(self.languageNode)).next()
            target = self.xmlelement.iterchildren(self.namespaced('target')).next()
            nodes = [source, target]
        except StopIteration:
            if source is not None:
                nodes.append(source)
            if not target is None:
                nodes.append(target)
        return nodes

    def set_rich_source(self, value, sourcelang='en'):
        sourcelanguageNode = self.get_source_dom()
        if sourcelanguageNode is None:
            sourcelanguageNode = self.createlanguageNode(sourcelang, u'', "source")
            self.set_source_dom(sourcelanguageNode)

        # Clear sourcelanguageNode first
        for i in range(len(sourcelanguageNode)):
            del sourcelanguageNode[0]
        sourcelanguageNode.text = None

        strelem_to_xml(sourcelanguageNode, value[0])

    def get_rich_source(self):
        #rsrc = xml_to_strelem(self.source_dom)
        #logging.debug('rich source: %s' % (repr(rsrc)))
        #from dubulib.debug.misc import print_stack_funcs
        #print_stack_funcs()
        return [
            xml_to_strelem(self.source_dom,
                           getXMLspace(self.xmlelement,
                                       self._default_xml_space))
        ]
    rich_source = property(get_rich_source, set_rich_source)

    def set_rich_target(self, value, lang='xx', append=False):
        self._rich_target = None
        if value is None:
            self.set_target_dom(self.createlanguageNode(lang, u'', "target"))
            return

        languageNode = self.get_target_dom()
        if languageNode is None:
            languageNode = self.createlanguageNode(lang, u'', "target")
            self.set_target_dom(languageNode, append)

        # Clear languageNode first
        for i in range(len(languageNode)):
            del languageNode[0]
        languageNode.text = None

        strelem_to_xml(languageNode, value[0])
        ### currently giving some issues in Virtaal: self._rich_target = value

    def get_rich_target(self, lang=None):
        """retrieves the "target" text (second entry), or the entry in the
        specified language, if it exists"""
        if self._rich_target is None:
            self._rich_target = [
                xml_to_strelem(self.get_target_dom(lang),
                getXMLspace(self.xmlelement, self._default_xml_space))
            ]
        return self._rich_target
    rich_target = property(get_rich_target, set_rich_target)

    def addalttrans(self, txt, origin=None, lang=None, sourcetxt=None,
                    matchquality=None):
        """Adds an alt-trans tag and alt-trans components to the unit.

        :type txt: String
        :param txt: Alternative translation of the source text.
        """

        # TODO: support adding a source tag ad match quality attribute.  At the
        # source tag is needed to inject fuzzy matches from a TM.
        if isinstance(txt, str):
            txt = txt.decode("utf-8")
        alttrans = etree.SubElement(self.xmlelement, self.namespaced("alt-trans"))
        lisa.setXMLspace(alttrans, "preserve")
        if sourcetxt:
            if isinstance(sourcetxt, str):
                sourcetxt = sourcetxt.decode("utf-8")
            altsource = etree.SubElement(alttrans, self.namespaced("source"))
            altsource.text = sourcetxt
        alttarget = etree.SubElement(alttrans, self.namespaced("target"))
        alttarget.text = txt
        if matchquality:
            alttrans.set("match-quality", matchquality)
        if origin:
            alttrans.set("origin", origin)
        if lang:
            lisa.setXMLlang(alttrans, lang)

    def getalttrans(self, origin=None):
        """Returns <alt-trans> for the given origin as a list of units. No
        origin means all alternatives."""
        translist = []
        for node in self.xmlelement.iterdescendants(self.namespaced("alt-trans")):
            if self.correctorigin(node, origin):
                # We build some mini units that keep the xmlelement. This
                # makes it easier to delete it if it is passed back to us.
                newunit = base.TranslationUnit(self.source)

                # the source tag is optional
                sourcenode = node.iterdescendants(self.namespaced("source"))
                try:
                    newunit.source = lisa.getText(sourcenode.next(),
                                                  getXMLspace(node, self._default_xml_space))
                except StopIteration:
                    pass

                # must have one or more targets
                targetnode = node.iterdescendants(self.namespaced("target"))
                newunit.target = lisa.getText(targetnode.next(),
                                              getXMLspace(node, self._default_xml_space))
                # TODO: support multiple targets better
                # TODO: support notes in alt-trans
                newunit.xmlelement = node

                translist.append(newunit)
        return translist

    def delalttrans(self, alternative):
        """Removes the supplied alternative from the list of alt-trans tags"""
        self.xmlelement.remove(alternative.xmlelement)

    def addnote(self, text, origin=None, position="append"):
        """Add a note specifically in a "note" tag"""
        if position != "append":
            self.removenotes(origin=origin)

        if text:
            text = text.strip()
        if not text:
            return
        if isinstance(text, str):
            text = text.decode("utf-8")
        note = etree.SubElement(self.xmlelement, self.namespaced("note"))
        note.text = text
        if origin:
            note.set("from", origin)

    def _getnotelist(self, origin=None):
        """Returns the text from notes matching ``origin`` or all notes.

        :param origin: The origin of the note (or note type)
        :type origin: String
        :return: The text from notes matching ``origin``
        :rtype: List
        """
        note_nodes = self.xmlelement.iterdescendants(self.namespaced("note"))
        # TODO: consider using xpath to construct initial_list directly
        # or to simply get the correct text from the outset (just remember to
        # check for duplication.
        initial_list = [lisa.getText(note, getXMLspace(self.xmlelement, self._default_xml_space)) for note in note_nodes if self.correctorigin(note, origin)]

        # Remove duplicate entries from list:
        dictset = {}
        note_list = [dictset.setdefault(note, note) for note in initial_list if note not in dictset]

        return note_list

    def getnotes(self, origin=None):
        return '\n'.join(self._getnotelist(origin=origin))

    def removenotes(self, origin="translator"):
        """Remove all the translator notes."""
        notes = self.xmlelement.iterdescendants(self.namespaced("note"))
        for note in notes:
            if self.correctorigin(note, origin=origin):
                self.xmlelement.remove(note)

    def adderror(self, errorname, errortext):
        """Adds an error message to this unit."""
        # TODO: consider factoring out: some duplication between XLIFF and TMX
        text = errorname
        if errortext:
            text += ': ' + errortext
        self.addnote(text, origin="pofilter")

    def geterrors(self):
        """Get all error messages."""
        # TODO: consider factoring out: some duplication between XLIFF and TMX
        notelist = self._getnotelist(origin="pofilter")
        errordict = {}
        for note in notelist:
            errorname, errortext = note.split(': ')
            errordict[errorname] = errortext
        return errordict

    def get_state_n(self):
        targetnode = self.getlanguageNode(lang=None, index=1)
        if targetnode is None:
            if self.isapproved():
                return self.S_UNREVIEWED
            else:
                return self.S_UNTRANSLATED

        xmlstate = targetnode.get("state", None)
        state_n = self.statemap.get(xmlstate, self.S_UNTRANSLATED)

        if state_n < self.S_NEEDS_TRANSLATION and self.target:
            state_n = self.S_NEEDS_TRANSLATION

        if self.isapproved() and state_n < self.S_UNREVIEWED:
            state_n = self.S_UNREVIEWED

        if not self.isapproved() and state_n > self.S_UNREVIEWED:
            state_n = self.S_UNREVIEWED

        return state_n

    def set_state_n(self, value):
        if value not in self.statemap_r:
            value = self.get_state_id(value)

        targetnode = self.getlanguageNode(lang=None, index=1)

        # FIXME: handle state qualifiers
        if value == self.S_UNTRANSLATED:
            if targetnode is not None and "state" in targetnode.attrib:
                del targetnode.attrib["state"]
        else:
            if targetnode is not None:
                xmlstate = self.statemap_r.get(value)
                targetnode.set("state", xmlstate)

        self.markapproved(value > self.S_NEEDS_REVIEW)

    def isapproved(self):
        """States whether this unit is approved."""
        return self.xmlelement.get("approved") == "yes"

    def markapproved(self, value=True):
        """Mark this unit as approved."""
        if value:
            self.xmlelement.set("approved", "yes")
        elif self.isapproved():
            self.xmlelement.set("approved", "no")

    def isreview(self):
        """States whether this unit needs to be reviewed"""
        return self.get_state_id() == self.S_NEEDS_REVIEW

    def markreviewneeded(self, needsreview=True, explanation=None):
        """Marks the unit to indicate whether it needs review.

        Adds an optional explanation as a note."""
        state_id = self.get_state_id()
        if needsreview and state_id != self.S_NEEDS_REVIEW:
            self.set_state_n(self.S_NEEDS_REVIEW)
            if explanation:
                self.addnote(explanation, origin="translator")
        elif not needsreview and state_id < self.S_UNREVIEWED:
            self.set_state_n(self.S_UNREVIEWED)

    def isfuzzy(self):
        # targetnode = self.getlanguageNode(lang=None, index=1)
        # return not targetnode is None and \
        #         (targetnode.get("state-qualifier") == "fuzzy-match" or \
        #         targetnode.get("state") == "needs-review-translation")
        return not self.isapproved() and bool(self.target)

    def markfuzzy(self, value=True):
        state_id = self.get_state_id()
        if value:
            self.markapproved(False)
            if state_id != self.S_NEEDS_TRANSLATION:
                self.set_state_n(self.S_NEEDS_TRANSLATION)
        else:
            self.markapproved(True)
            if state_id < self.S_UNREVIEWED:
                self.set_state_n(self.S_UNREVIEWED)

    def settarget(self, text, lang='xx', append=False):
        """Sets the target string to the given value."""
        super(xliffunit, self).settarget(text, lang, append)
        if text:
            self.marktranslated()

# This code is commented while this will almost always return false.
# This way pocount, etc. works well.
#    def istranslated(self):
#        targetnode = self.getlanguageNode(lang=None, index=1)
#        return not targetnode is None and \
#                (targetnode.get("state") == "translated")

    def istranslatable(self):
        value = self.xmlelement.get("translate")
        if value and value.lower() == 'no':
            return False
        return True

    def marktranslated(self):
        state_id = self.get_state_id()
        if state_id < self.S_UNREVIEWED:
            self.set_state_n(self.S_UNREVIEWED)

    def setid(self, id):
        # sanitize id in case ID_SEPERATOR is present
        self.xmlelement.set("id", id.replace(ID_SEPARATOR, ID_SEPARATOR_SAFE))

    def getid(self):
        uid = u""
        try:
            filename = self.xmlelement.iterancestors(self.namespaced('file')).next().get('original')
            if filename:
                uid = filename + ID_SEPARATOR
        except StopIteration:
            # unit has no proper file ancestor, probably newly created
            pass
        # hide the fact that we sanitize ID_SEPERATOR
        uid += unicode(self.xmlelement.get("id") or u"").replace(ID_SEPARATOR_SAFE, ID_SEPARATOR)
        return uid

    def addlocation(self, location):
        self.setid(location)

    def getlocations(self):
        id_attr = unicode(self.xmlelement.get("id") or u"")
        # XLIFF files downloaded from PO projects in Pootle
        # might have id equal to .source, so let's avoid
        # that:
        if id_attr and id_attr != self.source:
            return [id_attr]
        return []

    def createcontextgroup(self, name, contexts=None, purpose=None):
        """Add the context group to the trans-unit with contexts a list with
        (type, text) tuples describing each context."""
        assert contexts
        group = etree.Element(self.namespaced("context-group"))
        # context-group tags must appear at the start within <group>
        # tags. Otherwise it must be appended to the end of a group
        # of tags.
        if self.xmlelement.tag == self.namespaced("group"):
            self.xmlelement.insert(0, group)
        else:
            self.xmlelement.append(group)
        group.set("name", name)
        if purpose:
            group.set("purpose", purpose)
        for type, text in contexts:
            if isinstance(text, str):
                text = text.decode("utf-8")
            context = etree.SubElement(group, self.namespaced("context"))
            context.text = text
            context.set("context-type", type)

    def getcontextgroups(self, name):
        """Returns the contexts in the context groups with the specified name"""
        groups = []
        grouptags = self.xmlelement.iterdescendants(self.namespaced("context-group"))
        # TODO: conbine name in query
        for group in grouptags:
            if group.get("name") == name:
                contexts = group.iterdescendants(self.namespaced("context"))
                pairs = []
                for context in contexts:
                    pairs.append((context.get("context-type"), lisa.getText(context, getXMLspace(self.xmlelement, self._default_xml_space))))
                groups.append(pairs)  # not extend
        return groups

    def getrestype(self):
        """returns the restype attribute in the trans-unit tag"""
        return self.xmlelement.get("restype")

    def merge(self, otherunit, overwrite=False, comments=True, authoritative=False):
        # TODO: consider other attributes like "approved"
        super(xliffunit, self).merge(otherunit, overwrite, comments)
        if self.target:
            self.marktranslated()
            if otherunit.isfuzzy():
                self.markfuzzy()
            elif otherunit.source == self.source:
                self.markfuzzy(False)
        if comments:
            self.addnote(otherunit.getnotes())

    def correctorigin(self, node, origin):
        """Check against node tag's origin (e.g note or alt-trans)"""
        if origin is None:
            return True
        elif origin in node.get("from", ""):
            return True
        elif origin in node.get("origin", ""):
            return True
        else:
            return False

    @classmethod
    def multistring_to_rich(cls, mstr):
        """Override :meth:`TranslationUnit.multistring_to_rich` which is used
        by the ``rich_source`` and ``rich_target`` properties."""
        strings = mstr
        if isinstance(mstr, multistring):
            strings = mstr.strings
        elif isinstance(mstr, basestring):
            strings = [mstr]

        return [xml_to_strelem(s) for s in strings]

    @classmethod
    def rich_to_multistring(cls, elem_list):
        """Override :meth:`TranslationUnit.rich_to_multistring` which is used
        by the ``rich_source`` and ``rich_target`` properties."""
        return multistring([unicode(elem) for elem in elem_list])


class xlifffile(lisa.LISAfile):
    """Class representing a XLIFF file store."""
    UnitClass = xliffunit
    Name = "XLIFF Translation File"
    Mimetypes = ["application/x-xliff", "application/x-xliff+xml"]
    Extensions = ["xlf", "xliff", "sdlxliff"]
    rootNode = "xliff"
    bodyNode = "body"
    XMLskeleton = '''<?xml version="1.0" ?>
<xliff version='1.1' xmlns='urn:oasis:names:tc:xliff:document:1.1'>
<file original='NoName' source-language='en' datatype='plaintext'>
<body>
</body>
</file>
</xliff>'''
    namespace = 'urn:oasis:names:tc:xliff:document:1.1'
    unversioned_namespace = 'urn:oasis:names:tc:xliff:document:'

    suggestions_in_format = True
    """xliff units have alttrans tags which can be used to store suggestions"""

    def __init__(self, *args, **kwargs):
        self._filename = None
        lisa.LISAfile.__init__(self, *args, **kwargs)
        self._messagenum = 0

    def initbody(self):
        # detect the xliff namespace, handle both 1.1 and 1.2
        for prefix, ns in self.document.getroot().nsmap.items():
            if ns and ns.startswith(self.unversioned_namespace):
                self.namespace = ns
                break
        else:
            # handle crappy xliff docs without proper namespace declaration
            # by simply using the xmlns default namespace
            self.namespace = self.document.getroot().nsmap.get(None, None)

        if self._filename:
            filenode = self.getfilenode(self._filename, createifmissing=True)
        else:
            filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
        self.body = self.getbodynode(filenode, createifmissing=True)

    def addheader(self):
        """Initialise the file header."""
        pass

    def createfilenode(self, filename, sourcelanguage=None,
                       targetlanguage=None, datatype='plaintext'):
        """creates a filenode with the given filename. All parameters
        are needed for XLIFF compliance."""
        if sourcelanguage is None:
            sourcelanguage = self.sourcelanguage
        if targetlanguage is None:
            targetlanguage = self.targetlanguage

        # find the default NoName file tag and use it instead of creating a new one
        for filenode in self.document.getroot().iterchildren(self.namespaced("file")):
            if filenode.get("original") == "NoName":
                filenode.set("original", filename)
                filenode.set("source-language", sourcelanguage)
                if targetlanguage:
                    filenode.set("target-language", targetlanguage)
                return filenode

        filenode = etree.Element(self.namespaced("file"))
        filenode.set("original", filename)
        filenode.set("source-language", sourcelanguage)
        if targetlanguage:
            filenode.set("target-language", targetlanguage)
        filenode.set("datatype", datatype)
        bodyNode = etree.SubElement(filenode, self.namespaced(self.bodyNode))
        return filenode

    def getfilename(self, filenode):
        """returns the name of the given file"""
        return filenode.get("original")

    def setfilename(self, filenode, filename):
        """set the name of the given file"""
        return filenode.set("original", filename)

    def getfilenames(self):
        """returns all filenames in this XLIFF file"""
        filenodes = self.document.getroot().iterchildren(self.namespaced("file"))
        filenames = [self.getfilename(filenode) for filenode in filenodes]
        filenames = filter(None, filenames)
        if len(filenames) == 1 and filenames[0] == '':
            filenames = []
        return filenames

    def getfilenode(self, filename, createifmissing=False):
        """finds the filenode with the given name"""
        filenodes = self.document.getroot().iterchildren(self.namespaced("file"))
        for filenode in filenodes:
            if self.getfilename(filenode) == filename:
                return filenode
        if createifmissing:
            filenode = self.createfilenode(filename)
            return filenode
        return None

    def getids(self, filename=None):
        if not filename:
            return super(xlifffile, self).getids()

        self.id_index = {}
        prefix = filename + ID_SEPARATOR
        units = (unit for unit in self.units if unit.getid().startswith(prefix))
        for index, unit in enumerate(units):
            self.id_index[unit.getid()[len(prefix):]] = unit
        return self.id_index.keys()

    def setsourcelanguage(self, language):
        if not language:
            return
        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
        filenode.set("source-language", language)

    def getsourcelanguage(self):
        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
        return filenode.get("source-language")
    sourcelanguage = property(getsourcelanguage, setsourcelanguage)

    def settargetlanguage(self, language):
        if not language:
            return
        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
        filenode.set("target-language", language)

    def gettargetlanguage(self):
        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
        return filenode.get("target-language")
    targetlanguage = property(gettargetlanguage, settargetlanguage)

    def getdatatype(self, filename=None):
        """Returns the datatype of the stored file. If no filename is given,
        the datatype of the first file is given."""
        if filename:
            node = self.getfilenode(filename)
            if not node is None:
                return node.get("datatype")
        else:
            filenames = self.getfilenames()
            if len(filenames) > 0 and filenames[0] != "NoName":
                return self.getdatatype(filenames[0])
        return ""

    def getdate(self, filename=None):
        """Returns the date attribute for the file.

        If no filename is given, the date of the first file is given.
        If the date attribute is not specified, None is returned.

        :returns: Date attribute of file
        :rtype: Date or None
        """
        if filename:
            node = self.getfilenode(filename)
            if not node is None:
                return node.get("date")
        else:
            filenames = self.getfilenames()
            if len(filenames) > 0 and filenames[0] != "NoName":
                return self.getdate(filenames[0])
        return None

    def removedefaultfile(self):
        """We want to remove the default file-tag as soon as possible if we
        know if still present and empty."""
        filenodes = list(self.document.getroot().iterchildren(self.namespaced("file")))
        if len(filenodes) > 1:
            for filenode in filenodes:
                if (filenode.get("original") == "NoName" and
                    not list(filenode.iterdescendants(self.namespaced(self.UnitClass.rootNode)))):
                    self.document.getroot().remove(filenode)
                break

    def getheadernode(self, filenode, createifmissing=False):
        """finds the header node for the given filenode"""
        # TODO: Deprecated?
        headernode = filenode.iterchildren(self.namespaced("header"))
        try:
            return headernode.next()
        except StopIteration:
            pass
        if not createifmissing:
            return None
        headernode = etree.SubElement(filenode, self.namespaced("header"))
        return headernode

    def getbodynode(self, filenode, createifmissing=False):
        """finds the body node for the given filenode"""
        bodynode = filenode.iterchildren(self.namespaced("body"))
        try:
            return bodynode.next()
        except StopIteration:
            pass
        if not createifmissing:
            return None
        bodynode = etree.SubElement(filenode, self.namespaced("body"))
        return bodynode

    def addsourceunit(self, source, filename="NoName", createifmissing=False):
        """adds the given trans-unit to the last used body node if the
        filename has changed it uses the slow method instead (will
        create the nodes required if asked). Returns success"""
        if self._filename != filename:
            if not self.switchfile(filename, createifmissing):
                return None
        unit = super(xlifffile, self).addsourceunit(source)
        self._messagenum += 1
        unit.setid("%d" % self._messagenum)
        return unit

    def switchfile(self, filename, createifmissing=False):
        """Adds the given trans-unit (will create the nodes required if asked).

        :returns: Success
        :rtype: Boolean
        """
        self._filename = filename
        filenode = self.getfilenode(filename)
        if filenode is None:
            if not createifmissing:
                return False
            filenode = self.createfilenode(filename)
            self.document.getroot().append(filenode)

        self.body = self.getbodynode(filenode, createifmissing=createifmissing)
        if self.body is None:
            return False
        self._messagenum = len(list(self.body.iterdescendants(self.namespaced("trans-unit"))))
        # TODO: was 0 based before - consider
    #    messagenum = len(self.units)
        # TODO: we want to number them consecutively inside a body/file tag
        # instead of globally in the whole XLIFF file, but using
        # len(self.units) will be much faster
        return True

    def creategroup(self, filename="NoName", createifmissing=False, restype=None):
        """adds a group tag into the specified file"""
        if self._filename != filename:
            if not self.switchfile(filename, createifmissing):
                return None
        group = etree.SubElement(self.body, self.namespaced("group"))
        if restype:
            group.set("restype", restype)
        return group

    def __str__(self):
        self.removedefaultfile()
        return super(xlifffile, self).__str__()

    @classmethod
    def parsestring(cls, storestring):
        """Parses the string to return the correct file object"""
        xliff = super(xlifffile, cls).parsestring(storestring)
        if xliff.units:
            header = xliff.units[0]
            if (("gettext-domain-header" in (header.getrestype() or "") or
                 xliff.getdatatype() == "po") and
                 cls.__name__.lower() != "poxlifffile"):
                from translate.storage import poxliff
                xliff = poxliff.PoXliffFile.parsestring(storestring)
        return xliff

########NEW FILE########
__FILENAME__ = extract
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

from contextlib import contextmanager, nested

from lxml import etree

from translate.storage import base
from translate.storage.placeables import StringElem, xliff
from translate.storage.xml_extract import misc, xpath_breadcrumb


class Translatable(object):
    """A node corresponds to a translatable element. A node may
       have children, which correspond to placeables."""

    def __init__(self, placeable_name, xpath, dom_node, source):
        self.placeable_name = placeable_name
        self.source = source
        self.xpath = xpath
        self.is_inline = False
        self.dom_node = dom_node

    def _get_placeables(self):
        return [placeable for placeable in self.source if isinstance(placeable, Translatable)]

    placeables = property(_get_placeables)


def reduce_unit_tree(f, unit_node, *state):
    return misc.reduce_tree(f, unit_node, unit_node, lambda unit_node: unit_node.placeables, *state)


class ParseState(object):
    """Maintain constants and variables used during the walking of a
    DOM tree (via the function apply)."""

    def __init__(self, no_translate_content_elements, inline_elements={}, nsmap={}):
        self.no_translate_content_elements = no_translate_content_elements
        self.inline_elements = inline_elements
        self.is_inline = False
        self.xpath_breadcrumb = xpath_breadcrumb.XPathBreadcrumb()
        self.placeable_name = u"<top-level>"
        self.nsmap = nsmap


def _process_placeable(dom_node, state):
    """Run find_translatable_dom_nodes on the current dom_node"""
    placeable = find_translatable_dom_nodes(dom_node, state)
    # This happens if there were no recognized child tags and thus
    # no translatable is returned. Make a placeable with the name
    # "placeable"
    if len(placeable) == 0:
        return Translatable(u"placeable", state.xpath_breadcrumb.xpath, dom_node, [])
    # The ideal situation: we got exactly one translateable back
    # when processing this tree.
    elif len(placeable) == 1:
        return placeable[0]
    else:
        raise Exception("BUG: find_translatable_dom_nodes should never return more than a single translatable")


def _process_placeables(dom_node, state):
    """Return a list of placeables and list with
    alternating string-placeable objects. The former is
    useful for directly working with placeables and the latter
    is what will be used to build the final translatable string."""

    source = []
    for child in dom_node:
        source.extend([_process_placeable(child, state), unicode(child.tail or u"")])
    return source


def _process_translatable(dom_node, state):
    source = [unicode(dom_node.text or u"")] + _process_placeables(dom_node, state)
    translatable = Translatable(state.placeable_name, state.xpath_breadcrumb.xpath, dom_node, source)
    translatable.is_inline = state.is_inline
    return [translatable]


def _process_children(dom_node, state):
    _namespace, tag = misc.parse_tag(dom_node.tag)
    children = [find_translatable_dom_nodes(child, state) for child in dom_node]
    # Flatten a list of lists into a list of elements
    children = [child for child_list in children for child in child_list]
    if len(children) > 1:
        intermediate_translatable = Translatable(tag, state.xpath_breadcrumb.xpath, dom_node, children)
        return [intermediate_translatable]
    else:
        return children


def compact_tag(nsmap, namespace, tag):
    if namespace in nsmap:
        return u'%s:%s' % (nsmap[namespace], tag)
    else:
        return u'{%s}%s' % (namespace, tag)


def find_translatable_dom_nodes(dom_node, state):
    # For now, we only want to deal with XML elements.
    # And we want to avoid processing instructions, which
    # are XML elements (in the inheritance hierarchy).
    if not isinstance(dom_node, etree._Element) or \
           isinstance(dom_node, etree._ProcessingInstruction):
        return []

    namespace, tag = misc.parse_tag(dom_node.tag)

    @contextmanager
    def xpath_set():
        state.xpath_breadcrumb.start_tag(compact_tag(state.nsmap, namespace, tag))
        yield state.xpath_breadcrumb
        state.xpath_breadcrumb.end_tag()

    @contextmanager
    def placeable_set():
        old_placeable_name = state.placeable_name
        state.placeable_name = tag
        yield state.placeable_name
        state.placeable_name = old_placeable_name

    @contextmanager
    def inline_set():
        old_inline = state.is_inline
        if (namespace, tag) in state.inline_elements:
            state.is_inline = True
        else:
            state.is_inline = False
        yield state.is_inline
        state.is_inline = old_inline

    with nested(xpath_set(), placeable_set(), inline_set()):
        if (namespace, tag) not in state.no_translate_content_elements:
            return _process_translatable(dom_node, state)
        else:
            return _process_children(dom_node, state)


class IdMaker(object):

    def __init__(self):
        self._max_id = 0
        self._obj_id_map = {}

    def get_id(self, obj):
        if not self.has_id(obj):
            self._obj_id_map[obj] = self._max_id
            self._max_id += 1
        return self._obj_id_map[obj]

    def has_id(self, obj):
        return obj in self._obj_id_map


def _to_placeables(parent_translatable, translatable, id_maker):
    result = []
    for chunk in translatable.source:
        if isinstance(chunk, unicode):
            result.append(chunk)
        else:
            id = unicode(id_maker.get_id(chunk))
            if chunk.is_inline:
                result.append(xliff.G(sub=_to_placeables(parent_translatable, chunk, id_maker), id=id))
            else:
                result.append(xliff.X(id=id, xid=chunk.xpath))
    return result


def _add_translatable_to_store(store, parent_translatable, translatable, id_maker):
    """Construct a new translation unit, set its source and location
    information and add it to 'store'.
    """
    unit = store.UnitClass(u'')
    unit.rich_source = [StringElem(_to_placeables(parent_translatable, translatable, id_maker))]
    unit.addlocation(translatable.xpath)
    store.addunit(unit)


def _contains_translatable_text(translatable):
    """Checks whether translatable contains any chunks of text which contain
    more than whitespace.

    If not, then there's nothing to translate."""
    for chunk in translatable.source:
        if isinstance(chunk, unicode):
            if chunk.strip() != u"":
                return True
    return False


def _make_store_adder(store):
    """Return a function which, when called with a Translatable will add
    a unit to 'store'. The placeables will represented as strings according
    to 'placeable_quoter'."""
    id_maker = IdMaker()

    def add_to_store(parent_translatable, translatable, rid):
        _add_translatable_to_store(store, parent_translatable, translatable, id_maker)

    return add_to_store


def _walk_translatable_tree(translatables, f, parent_translatable, rid):
    for translatable in translatables:
        if _contains_translatable_text(translatable) and not translatable.is_inline:
            rid = rid + 1
            new_parent_translatable = translatable
            f(parent_translatable, translatable, rid)
        else:
            new_parent_translatable = parent_translatable

        _walk_translatable_tree(translatable.placeables, f, new_parent_translatable, rid)


def reverse_map(a_map):
    return dict((value, key) for key, value in a_map.iteritems())


def build_store(odf_file, store, parse_state, store_adder=None):
    """Utility function for loading xml_filename"""
    store_adder = store_adder or _make_store_adder(store)
    tree = etree.parse(odf_file)
    root = tree.getroot()
    parse_state.nsmap = reverse_map(root.nsmap)
    translatables = find_translatable_dom_nodes(root, parse_state)
    _walk_translatable_tree(translatables, store_adder, None, 0)
    return tree

########NEW FILE########
__FILENAME__ = generate
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

import lxml.etree as etree

from translate.storage import base
from translate.storage.xml_extract import extract, misc, unit_tree
from translate.storage.xml_name import XmlNamer


def _get_tag_arrays(dom_node):
    """Return a dictionary indexed by child tag names, where each tag is associated with an array
    of all the child nodes with matching the tag name, in the order in which they appear as children
    of dom_node.

    >>> xml = '<a><b></b><c></c><b></b><d/></a>'
    >>> element = etree.fromstring(xml)
    >>> get_tag_arrays(element)
    {'b': [<Element a at 84df144>, <Element a at 84df148>], 'c': [<Element a at 84df120>], 'd': [<Element a at 84df152>]}
    """
    child_dict = {}
    for child in dom_node:
        if child.tag not in child_dict:
            child_dict[child.tag] = []
        child_dict[child.tag].append(child)
    return child_dict


def apply_translations(dom_node, unit_node, do_translate):
    tag_array = _get_tag_arrays(dom_node)
    for unit_child_index, unit_child in unit_node.children.iteritems():
        tag, index = unit_child_index
        try:
            dom_child = tag_array[XmlNamer(dom_node).name(tag)][index]
            apply_translations(dom_child, unit_child, do_translate)
        # Raised if tag is not in tag_array. We might want to complain to the
        # user in the future.
        except KeyError:
            pass
        # Raised if index is not in tag_array[tag]. We might want to complain to
        # the user in the future
        except IndexError:
            pass
    # If there is a translation unit associated with this unit_node...
    if unit_node.unit is not None:
        # The invoke do_translate on the dom_node and the unit; do_translate
        # should replace the text in dom_node with the text in unit_node.
        do_translate(dom_node, unit_node.unit)


def reduce_dom_tree(f, dom_node, *state):
    return misc.reduce_tree(f, dom_node, dom_node, lambda dom_node: dom_node, *state)


def find_dom_root(parent_dom_node, dom_node):
    """
    .. seealso:: :meth:`find_placeable_dom_tree_roots`
    """
    if dom_node is None or parent_dom_node is None:
        return None
    if dom_node.getparent() == parent_dom_node:
        return dom_node
    elif dom_node.getparent() is None:
        return None
    else:
        return find_dom_root(parent_dom_node, dom_node.getparent())


def find_placeable_dom_tree_roots(unit_node):
    """For an inline placeable, find the root DOM node for the placeable in its
    parent.

    Consider the diagram. In this pseudo-ODF example, there is an inline span
    element. However, the span is contained in other tags (which we never process).
    When splicing the template DOM tree (that is, the DOM which comes from
    the XML document we're using to generate a translated XML document), we'll
    need to move DOM sub-trees around and we need the roots of these sub-trees::

        <p> This is text \/                <- Paragraph containing an inline placeable
                         <blah>            <- Inline placeable's root (which we want to find)
                         ...               <- Any number of intermediate DOM nodes
                         <span> bold text  <- The inline placeable's Translatable
                                              holds a reference to this DOM node
    """

    def set_dom_root_for_unit_node(parent_unit_node, unit_node, dom_tree_roots):
        dom_tree_roots[unit_node] = find_dom_root(parent_unit_node.dom_node, unit_node.dom_node)
        return dom_tree_roots
    return extract.reduce_unit_tree(set_dom_root_for_unit_node, unit_node, {})


def _map_source_dom_to_doc_dom(unit_node, source_dom_node):
    """Creating a mapping from the DOM nodes in source_dom_node which correspond to
    placeables, with DOM nodes in the XML document template (this information is obtained
    from unit_node). We are interested in DOM nodes in the XML document template which
    are the roots of placeables. See the diagram below, as well as
    :meth:`find_placeable_dom_tree_roots`.

    XLIFF Source (below)::
        <source>This is text <g> bold text</g> and a footnote<x/></source>
                             /                                 \________
                            /                                           \
        <p>This is text<blah>...<span> bold text</span>...</blah> and <note>...</note></p>
    Input XML document used as a template (above)

    In the above diagram, the XLIFF source DOM node <g> is associated with the XML
    document DOM node <blah>, whereas the XLIFF source DOM node <x> is associated with
    the XML document DOM node <note>.
    """
    dom_tree_roots = find_placeable_dom_tree_roots(unit_node)
    source_dom_to_doc_dom = {}

    def loop(unit_node, source_dom_node):
        for child_unit_node, child_source_dom in zip(unit_node.placeables, source_dom_node):
            source_dom_to_doc_dom[child_source_dom] = dom_tree_roots[child_unit_node]
            loop(child_unit_node, child_source_dom)

    loop(unit_node, source_dom_node)
    return source_dom_to_doc_dom


def _map_target_dom_to_source_dom(source_dom_node, target_dom_node):
    """Associate placeables in source_dom_node and target_dom_node which
    have the same 'id' attributes.

    We're using XLIFF placeables. The XLIFF standard requires that
    placeables have unique ids. The id of a placeable is never modified,
    which means that even if placeables are moved around in a translation,
    we can easily associate placeables from the source text with placeables
    in the target text.

    This function does exactly that.
    """

    def map_id_to_dom_node(parent_node, node, id_to_dom_node):
        # If this DOM node has an 'id' attribute, then add an id -> node
        # mapping to 'id_to_dom_node'.
        if u'id' in node.attrib:
            id_to_dom_node[node.attrib[u'id']] = node
        return id_to_dom_node

    # Build a mapping of id attributes to the DOM nodes which have these ids.
    id_to_dom_node = reduce_dom_tree(map_id_to_dom_node, target_dom_node, {})

    def map_target_dom_to_source_dom_aux(parent_node, node, target_dom_to_source_dom):
        #
        if u'id' in node.attrib and node.attrib[u'id'] in id_to_dom_node:
            target_dom_to_source_dom[id_to_dom_node[node.attrib[u'id']]] = node
        return target_dom_to_source_dom

    # For each node in the DOM tree rooted at source_dom_node:
    # 1. Check whether the node has an 'id' attribute.
    # 2. If so, check whether there is a mapping of this id to a target DOM node
    #    in id_to_dom_node.
    # 3. If so, associate this source DOM node with the target DOM node.
    return reduce_dom_tree(map_target_dom_to_source_dom_aux, source_dom_node, {})


def _build_target_dom_to_doc_dom(unit_node, source_dom, target_dom):
    source_dom_to_doc_dom = _map_source_dom_to_doc_dom(unit_node, source_dom)
    target_dom_to_source_dom = _map_target_dom_to_source_dom(source_dom, target_dom)
    return misc.compose_mappings(target_dom_to_source_dom, source_dom_to_doc_dom)


def _get_translated_node(target_node, target_dom_to_doc_dom):
    """Convenience function to get node corresponding to 'target_node'
    and to assign the tail text of 'target_node' to this node."""
    dom_node = target_dom_to_doc_dom[target_node]
    dom_node.tail = target_node.tail
    return dom_node


def _build_translated_dom(dom_node, target_node, target_dom_to_doc_dom):
    """Use the "shape" of 'target_node' (which is a DOM tree) to insert nodes
    into the DOM tree rooted at 'dom_node'.

    The mapping 'target_dom_to_doc_dom' is used to map nodes from 'target_node'
    to nodes which much be inserted into dom_node.
    """
    dom_node.text = target_node.text
    # 1. Find all child nodes of target_node.
    # 2. Filter out the children which map to None.
    # 3. Call _get_translated_node on the remaining children; this maps a node in
    #    'target_node' to a node in 'dom_node' and assigns the tail text of 'target_node'
    #    to the mapped node.
    # 4. Add all of these mapped nodes to 'dom_node'
    dom_node.extend(_get_translated_node(child, target_dom_to_doc_dom) for child in target_node
                    if target_dom_to_doc_dom[child] is not None)
    # Recursively call this function on pairs of matched children in
    # dom_node and target_node.
    for dom_child, target_child in zip(dom_node, target_node):
        _build_translated_dom(dom_child, target_child, target_dom_to_doc_dom)


def replace_dom_text(make_parse_state):
    """Return a function::

          action: etree_Element x base.TranslationUnit -> None

      which takes a dom_node and a translation unit. The dom_node is rearranged
      according to rearrangement of placeables in unit.target (relative to their
      positions in unit.source).
    """

    def action(dom_node, unit):
        """Use the unit's target (or source in the case where there is no translation)
        to update the text in the dom_node and at the tails of its children."""
        source_dom = unit.source_dom
        if unit.target_dom is not None:
            target_dom = unit.target_dom
        else:
            target_dom = unit.source_dom
        # Build a tree of (non-DOM) nodes which correspond to the translatable DOM nodes in 'dom_node'.
        # Pass in a fresh parse_state every time, so as avoid working with stale parse state info.
        unit_node = extract.find_translatable_dom_nodes(dom_node, make_parse_state())[0]
        target_dom_to_doc_dom = _build_target_dom_to_doc_dom(unit_node, source_dom, target_dom)
        # Before we start reconstructing the sub-tree rooted at dom_node, we must clear out its children
        dom_node[:] = []
        _build_translated_dom(dom_node, target_dom, target_dom_to_doc_dom)

    return action

########NEW FILE########
__FILENAME__ = misc
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

import re


# Python 3 compatibility
try:
    unicode
except NameError:
    unicode = str


def reduce_tree(f, parent_unit_node, unit_node, get_children, *state):
    """Enumerate a tree, applying f to in a pre-order fashion to each node.

    parent_unit_node contains the parent of unit_node. For the root of the tree,
    parent_unit_node == unit_node.

    get_children is a single argument function applied to a unit_node to
    get a list/iterator to its children.

    state is used by f to modify state information relating to whatever f does
    to the tree.
    """

    def as_tuple(x):
        if isinstance(x, tuple):
            return x
        else:
            return (x,)

    state = f(parent_unit_node, unit_node, *state)
    for child_unit_node in get_children(unit_node):
        state = reduce_tree(f, unit_node, child_unit_node, get_children, *as_tuple(state))
    return state


def compose_mappings(left, right):
    """Given two mappings left: A -> B and right: B -> C, create a
    hash result_map: A -> C. Only values in left (i.e. things from B)
    which have corresponding keys in right will have their keys mapped
    to values in right. """
    result_map = {}
    for left_key, left_val in left.items():
        try:
            result_map[left_key] = right[left_val]
        except KeyError:
            pass
    return result_map

tag_pattern = re.compile('({(?P<namespace>(\w|[-:./])*)})?(?P<tag>(\w|[-])*)')


def parse_tag(full_tag):
    """
    >>> parse_tag('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content')
    ('urn:oasis:names:tc:opendocument:xmlns:office:1.0', 'document-content')
    """
    match = tag_pattern.match(full_tag)
    if match is not None:
        # Slightly hacky way of supporting 2+3
        ret = []
        for k in ("namespace", "tag"):
            value = match.groupdict()[k] or ""
            if not isinstance(value, unicode):
                value = unicode(value, encoding="utf-8")
            ret.append(value)
        return ret[0], ret[1]
    else:
        raise Exception('Passed an invalid tag')

########NEW FILE########
__FILENAME__ = test_misc
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

from translate.storage.xml_extract import misc


# reduce_tree

test_tree_1 = (u'a',
               [(u'b', []),
                (u'c', [(u'd', []), (u'e', [])]),
                (u'f', [(u'g', [(u'h', [])])])])

test_tree_2 = (1,
               [(2, []),
                (3, [(4, []), (5, [])]),
                (6, [(7, [(8, [])])])])


def get_children(node):
    return node[1]


def test_reduce_tree():

    def concatenate(parent_node, node, string):
        return string + node[0]

    assert u'abcdefgh' == misc.reduce_tree(concatenate, test_tree_1, test_tree_1, get_children, u'')

    def get_even_and_total(parent_node, node, even_lst, total):
        num = node[0]
        if num % 2 == 0:
            even_lst.append(num)
        return even_lst, total + num

    assert ([2, 4, 6, 8], 36) == misc.reduce_tree(get_even_and_total, test_tree_2, test_tree_2, get_children, [], 0)

# compose_mappings

left_mapping = {1: u'a', 2: u'b', 3: u'c', 4: u'd', 5: u'e'}
right_mapping = {u'a': -1, u'b': -2, u'd': -4, u'e': -5, u'f': -6}

composed_mapping = {1: -1, 2: -2, 4: -4, 5: -5}


def test_compose_mappings():
    assert composed_mapping == misc.compose_mappings(left_mapping, right_mapping)

# parse_tag


def test_parse_tag():
    assert (u'some-urn', u'some-tag') == \
        misc.parse_tag(u'{some-urn}some-tag')

    assert (u'urn:oasis:names:tc:opendocument:xmlns:office:1.0', u'document-content') == \
        misc.parse_tag(u'{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content')

########NEW FILE########
__FILENAME__ = test_unit_tree
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

from translate.storage import factory
from translate.storage.xml_extract import unit_tree


# _split_xpath_component


def test__split_xpath_component():
    assert (u'some-tag', 0) == unit_tree._split_xpath_component(u'some-tag[0]')

# _split_xpath


def test__split_xpath():
    assert [(u'p', 4), (u'text', 3), (u'body', 2), (u'document-content', 1)] == \
        unit_tree._split_xpath(u'document-content[1]/body[2]/text[3]/p[4]')

# _add_unit_to_tree


def make_tree_1(unit):
    root = unit_tree.XPathTree()
    node = root

    node.children[u'document-content', 1] = unit_tree.XPathTree()
    node = node.children[u'document-content', 1]

    node.children[u'body', 1] = unit_tree.XPathTree()
    node = node.children[u'body', 1]

    node.children[u'text', 1] = unit_tree.XPathTree()
    node = node.children[u'text', 1]

    node.children[u'p', 1] = unit_tree.XPathTree()
    node = node.children[u'p', 1]

    node.unit = unit

    return root


def make_tree_2(unit_1, unit_2):
    root = make_tree_1(unit_1)
    node = root.children[u'document-content', 1]

    node.children[u'body', 2] = unit_tree.XPathTree()
    node = node.children[u'body', 2]

    node.children[u'text', 3] = unit_tree.XPathTree()
    node = node.children[u'text', 3]

    node.children[u'p', 4] = unit_tree.XPathTree()
    node = node.children[u'p', 4]

    node.unit = unit_2

    return root


def test__add_unit_to_tree():
    from translate.storage import xliff
    xliff_file = xliff.xlifffile
#    xliff_file = factory.classes[u'xlf']()

    # Add the first unit

    unit_1 = xliff_file.UnitClass(u'Hello')
    xpath_1 = u'document-content[1]/body[1]/text[1]/p[1]'

    constructed_tree_1 = unit_tree.XPathTree()
    unit_tree._add_unit_to_tree(constructed_tree_1,
                                unit_tree._split_xpath(xpath_1),
                                unit_1)
    test_tree_1 = make_tree_1(unit_1)
    assert test_tree_1 == constructed_tree_1

    # Add another unit

    unit_2 = xliff_file.UnitClass(u'World')
    xpath_2 = u'document-content[1]/body[2]/text[3]/p[4]'

    constructed_tree_2 = make_tree_1(unit_1)
    unit_tree._add_unit_to_tree(constructed_tree_2,
                                unit_tree._split_xpath(xpath_2),
                                unit_2)
    test_tree_2 = make_tree_2(unit_1, unit_2)
    assert test_tree_2 == constructed_tree_2

########NEW FILE########
__FILENAME__ = test_xpath_breadcrumb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

import xpath_breadcrumb


def test_breadcrumb():
    xb = xpath_breadcrumb.XPathBreadcrumb()
    assert xb.xpath == u''

    xb.start_tag(u'a')
    assert xb.xpath == u'a[0]'

    xb.start_tag(u'b')
    assert xb.xpath == u'a[0]/b[0]'
    xb.end_tag()

    assert xb.xpath == u'a[0]'

    xb.start_tag(u'b')
    assert xb.xpath == u'a[0]/b[1]'
    xb.end_tag()

    assert xb.xpath == u'a[0]'
    xb.end_tag()

    assert xb.xpath == u''

    xb.start_tag(u'a')
    assert xb.xpath == u'a[1]'

########NEW FILE########
__FILENAME__ = unit_tree
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#

from lxml import etree

from translate.storage import base, xliff


class XPathTree(object):

    def __init__(self, unit=None):
        self.unit = unit
        self.children = {}

    def __eq__(self, other):
        return isinstance(other, XPathTree) and \
            self.unit == other.unit and \
            self.children == other.children


def _split_xpath_component(xpath_component):
    """Split an xpath component into a tag-index tuple.

    >>> split_xpath_component('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content[0]')
    ('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content', 0).
    """
    lbrac = xpath_component.rfind(u'[')
    rbrac = xpath_component.rfind(u']')
    tag = xpath_component[:lbrac]
    index = int(xpath_component[lbrac+1:rbrac])
    return tag, index


def _split_xpath(xpath):
    """Split an 'xpath' string separated by / into a reversed list of its components. Thus:

    >>> split_xpath('document-content[1]/body[2]/text[3]/p[4]')
    [('p', 4), ('text', 3), ('body', 2), ('document-content', 1)]

    The list is reversed so that it can be used as a stack, where the top of the stack is
    the first component.
    """
    if xliff.ID_SEPARATOR in xpath:
        xpath = xpath.split(xliff.ID_SEPARATOR)[-1]
    components = xpath.split(u'/')
    components = [_split_xpath_component(component) for component in components]
    return list(reversed(components))


def _add_unit_to_tree(node, xpath_components, unit):
    """Walk down the tree rooted a node, and follow nodes which correspond to the
    components of xpath_components. When reaching the end of xpath_components,
    set the reference of the node to unit.

    With reference to the tree diagram in build_unit_tree::

      add_unit_to_tree(node, [('p', 2), ('text', 3), ('body', 2), ('document-content', 1)], unit)

    would begin by popping ('document-content', 1) from the path and
    following the node marked ``('document-content', 1)`` in the tree.
    Likewise, will descend down the nodes marked ``('body', 2)``
    and ``('text', 3)``.

    Since the node marked ``('text', 3)`` has no child node marked
    ``('p', 2)``, this node is created. Then the ``add_unit_to_tree``
    descends down this node. When this happens, there are no xpath
    components left to pop. Thus, ``node.unit = unit`` is executed.
    """
    if len(xpath_components) > 0:
        component = xpath_components.pop()  # pop the stack; is a component such as ('p', 4)
        # if the current node does not have any children indexed by
        # the current component, add such a child
        if component not in node.children:
            node.children[component] = XPathTree()
        _add_unit_to_tree(node.children[component], xpath_components, unit)
    else:
        node.unit = unit


def build_unit_tree(store):
    """Enumerate a translation store and build a tree with XPath components as nodes
    and where a node contains a unit if a path from the root of the tree to the node
    containing the unit, is equal to the XPath of the unit.

    The tree looks something like this::

        root
           `- ('document-content', 1)
              `- ('body', 2)
                 |- ('text', 1)
                 |  `- ('p', 1)
                 |     `- <reference to a unit>
                 |- ('text', 2)
                 |  `- ('p', 1)
                 |     `- <reference to a unit>
                 `- ('text', 3)
                    `- ('p', 1)
                       `- <reference to a unit>
    """
    tree = XPathTree()
    for unit in store.units:
        if not unit.isfuzzy():
            location = _split_xpath(unit.getlocations()[0])
            _add_unit_to_tree(tree, location, unit)
    return tree

########NEW FILE########
__FILENAME__ = xpath_breadcrumb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


class XPathBreadcrumb(object):
    """A class which is used to build XPath-like paths as a DOM tree is
    walked. It keeps track of the number of times which it has seen
    a certain tag, so that it will correctly create indices for tags.

    Initially, the path is empty. Thus
    >>> xb = XPathBreadcrumb()
    >>> xb.xpath
    ""

    Suppose we walk down a DOM node for the tag <foo> and we want to
    record this, we simply do
    >>> xb.start_tag('foo')

    Now, the path is no longer empty. Thus
    >>> xb.xpath
    foo[0]

    Now suppose there are two <bar> tags under the tag <foo> (that is
    <foo><bar></bar><bar></bar><foo>), then the breadcrumb will keep
    track of the number of times it sees <bar>. Thus

    >>> xb.start_tag('bar')
    >>> xb.xpath
    foo[0]/bar[0]
    >>> xb.end_tag()
    >>> xb.xpath
    foo[0]
    >>> xb.start_tag('bar')
    >>> xb.xpath
    foo[0]/bar[1]
    """

    def __init__(self):
        self._xpath = []
        self._tagtally = [{}]

    def start_tag(self, tag):
        tally_dict = self._tagtally[-1]
        tally = tally_dict.get(tag, -1) + 1
        tally_dict[tag] = tally
        self._xpath.append((tag, tally))
        self._tagtally.append({})

    def end_tag(self):
        self._xpath.pop()
        self._tagtally.pop()

    def _get_xpath(self):

        def str_component(component):
            tag, pos = component
            return u"%s[%d]" % (tag, pos)
        return u"/".join(str_component(component) for component in self._xpath)

    xpath = property(_get_xpath)

########NEW FILE########
__FILENAME__ = xml_name
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.
#


class XmlNamespace(object):

    def __init__(self, namespace):
        self._namespace = namespace

    def name(self, tag):
        return "{%s}%s" % (self._namespace, tag)


class XmlNamer(object):
    """Initialize me with a DOM node or a DOM document node (the
    toplevel node you get when parsing an XML file). Then use me
    to generate fully qualified XML names.

    >>> xml = '<office:document-styles xmlns:office="urn:oasis:names:tc:opendocument:xmlns:office:1.0"></office>'
    >>> from lxml import etree
    >>> namer = XmlNamer(etree.fromstring(xml))
    >>> namer.name('office', 'blah')
    {urn:oasis:names:tc:opendocument:xmlns:office:1.0}blah
    >>> namer.name('office:blah')
    {urn:oasis:names:tc:opendocument:xmlns:office:1.0}blah

    I can also give you XmlNamespace objects if you give me the abbreviated
    namespace name. These are useful if you need to reference a namespace
    continuously.

    >>> office_ns = name.namespace('office')
    >>> office_ns.name('foo')
    {urn:oasis:names:tc:opendocument:xmlns:office:1.0}foo
    """

    def __init__(self, dom_node):
        # Allow the user to pass a dom node of the
        # XML document nodle
        if hasattr(dom_node, 'nsmap'):
            self.nsmap = dom_node.nsmap
        else:
            self.nsmap = dom_node.getroot().nsmap

    def name(self, namespace_shortcut, tag=None):
        # If the user doesn't pass an argument into 'tag'
        # then namespace_shortcut contains a tag of the form
        # 'short-namespace:tag'
        if tag is None:
            namespace_shortcut, tag = namespace_shortcut.split(':')
        return "{%s}%s" % (self.nsmap[namespace_shortcut], tag)

    def namespace(self, namespace_shortcut):
        return XmlNamespace(self.nsmap[namespace_shortcut])

########NEW FILE########
__FILENAME__ = zip
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This module provides functionality to work with zip files."""

# Perhaps all methods should work with a wildcard to limit searches in some
# way (examples: *.po, base.xlf, pootle-terminology.tbx)

# TODO: consider also providing directories as we currently provide files

from os import path
from zipfile import ZipFile

from translate.misc import wStringIO
from translate.storage import directory, factory


class ZIPFile(directory.Directory):
    """This class represents a ZIP file like a directory."""

    def __init__(self, filename=None):
        self.filename = filename
        self.filedata = []

    def unit_iter(self):
        """Iterator over all the units in all the files in this zip file."""
        for dirname, filename in self.file_iter():
            strfile = wStringIO.StringIO(self.archive.read(path.join(dirname, filename)))
            strfile.filename = filename
            store = factory.getobject(strfile)
            # TODO: don't regenerate all the storage objects
            for unit in store.unit_iter():
                yield unit

    def scanfiles(self):
        """Populate the internal file data."""
        self.filedata = []
        self.archive = ZipFile(self.filename)
        for completename in self.archive.namelist():
            dir, name = path.split(completename)
            self.filedata.append((dir, name))

########NEW FILE########
__FILENAME__ = _factory_classes
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2010 Zuza Software Foundation
#
# This file is part of Virtaal.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Py2exe can't find stuff that we import dynamically, so we have this file
just for the sake of the Windows installer to easily pick up all the stuff
that we need and ensure they make it into the installer."""

import catkeys
import csvl10n
import mo
import omegat
import po
import qm
import qph
import tbx
import tmx
import ts2
import utx
import wordfast
import xliff

try:
    import trados
except ImportError as e:
    pass

########NEW FILE########
__FILENAME__ = build_tmdb
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Import units from translations files into tmdb."""

import logging
import os
from argparse import ArgumentParser

from translate.storage import factory, tmdb


logger = logging.getLogger(__name__)


class Builder:

    def __init__(self, tmdbfile, source_lang, target_lang, filenames):
        self.tmdb = tmdb.TMDB(tmdbfile)
        self.source_lang = source_lang
        self.target_lang = target_lang

        for filename in filenames:
            if not os.path.exists(filename):
                logger.error("cannot process %s: does not exist", filename)
                continue
            elif os.path.isdir(filename):
                self.handledir(filename)
            else:
                self.handlefile(filename)
        self.tmdb.connection.commit()

    def handlefile(self, filename):
        try:
            store = factory.getobject(filename)
        except Exception as e:
            logger.error(str(e))
            return
        # do something useful with the store and db
        try:
            self.tmdb.add_store(store, self.source_lang, self.target_lang, commit=False)
        except Exception as e:
            print(e)
        print("File added:", filename)

    def handlefiles(self, dirname, filenames):
        for filename in filenames:
            pathname = os.path.join(dirname, filename)
            if os.path.isdir(pathname):
                self.handledir(pathname)
            else:
                self.handlefile(pathname)

    def handledir(self, dirname):
        path, name = os.path.split(dirname)
        if name in ["CVS", ".svn", "_darcs", ".git", ".hg", ".bzr"]:
            return
        entries = os.listdir(dirname)
        self.handlefiles(dirname, entries)


def main():
    parser = ArgumentParser()
    parser.add_argument(
        "-d", "--tmdb", dest="tmdb_file", default="tm.db",
        help="translation memory database file (default: tm.db)")
    parser.add_argument(
        "-s", "--import-source-lang", dest="source_lang", default="en",
        help="source language of translation files (default: en)")
    parser.add_argument(
        "-t", "--import-target-lang", dest="target_lang",
        help="target language of translation files", required=True)
    parser.add_argument(
        "files", metavar="input files", nargs="+"
    )
    args = parser.parse_args()

    logging.basicConfig(format="%(name)s: %(levelname)s: %(message)s")

    Builder(args.tmdb_file, args.source_lang, args.target_lang, args.files)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = phppo2pypo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Mozilla Corporation, Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert PHP format .po files to Python format .po files.
"""

import re

from translate.misc.multistring import multistring
from translate.storage import po


class phppo2pypo:

    def convertstore(self, inputstore):
        """Converts a given .po file (PHP Format) to a Python format .po file, the difference being
            how variable substitutions work.  PHP uses a %1$s format, and Python uses
            a {0} format (zero indexed).  This method will convert, e.g.:
                I have %2$s apples and %1$s oranges
                    to
                I have {1} apples and {0} oranges
            This method ignores strings with %s as both languages will recognize that.
        """
        thetargetfile = po.pofile(inputfile="")

        for unit in inputstore.units:
            newunit = self.convertunit(unit)
            thetargetfile.addunit(newunit)
        return thetargetfile

    def convertunit(self, unit):
        developer_notes = unit.getnotes(origin="developer")
        translator_notes = unit.getnotes(origin="translator")
        unit.removenotes()
        unit.addnote(self.convertstrings(developer_notes))
        unit.addnote(self.convertstrings(translator_notes))
        unit.source = self.convertstrings(unit.source)
        unit.target = self.convertstrings(unit.target)
        return unit

    def convertstring(self, input):
        return re.sub('%(\d)\$s', lambda x: "{%d}" % (int(x.group(1)) - 1), input)

    def convertstrings(self, input):
        if isinstance(input, multistring):
            strings = input.strings
        elif isinstance(input, list):
            strings = input
        else:
            return self.convertstring(input)
        for index, string in enumerate(strings):
            strings[index] = re.sub('%(\d)\$s', lambda x: "{%d}" % (int(x.group(1)) - 1), string)
        return multistring(strings)


def convertphp2py(inputfile, outputfile, template=None):
    """Converts from PHP .po format to Python .po format

    :param inputfile: file handle of the source
    :param outputfile: file handle to write to
    :param template: unused
    """
    convertor = phppo2pypo()
    inputstore = po.pofile(inputfile)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return False
    outputfile.write(str(outputstore))
    return True


def main(argv=None):
    """Converts PHP .po files to Python .po files."""
    from translate.convert import convert

    formats = {"po": ("po", convertphp2py)}
    parser = convert.ConvertOptionParser(formats, description=__doc__)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = poclean
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Produces a clean file from an unclean file (Trados/Wordfast) by stripping
out the tw4win indicators.

This does not convert an RTF file to PO/XLIFF, but produces the target file
with only the target text in from a text version of the RTF.
"""

import re

from translate.misc.multistring import multistring
from translate.storage import factory


tw4winre = re.compile(r"\{0>.*?<\}\d{1,3}\{>(.*?)<0\}", re.M | re.S)


def cleanunit(unit):
    """cleans the targets in the given unit"""
    if isinstance(unit.target, multistring):
        strings = unit.target.strings
    else:
        strings = [unit.target]
    for index, string in enumerate(strings):
        string = string.replace("\par", "")
        strings[index] = tw4winre.sub(r"\1", string)
    if len(strings) == 1:
        unit.target = strings[0]
    else:
        unit.target = strings


def cleanfile(thefile):
    """cleans the given file"""
    for unit in thefile.units:
        cleanunit(unit)
    return thefile


def runclean(inputfile, outputfile, templatefile):
    """reads in inputfile, cleans, writes to outputfile"""
    fromfile = factory.getobject(inputfile)

    cleanfile(fromfile)
#    if fromfile.isempty():
#        return False
    outputfile.write(str(fromfile))
    return True


def main():
    from translate.convert import convert
    formats = {"po": ("po", runclean), "xlf": ("xlf", runclean), None: ("po", runclean)}
    parser = convert.ConvertOptionParser(formats, usetemplates=False, description=__doc__)
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pocompile
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006,2010 Zuza Software Foundation
#
# This file is part of the translate-toolkit
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Compile XLIFF and Gettext PO localization files into Gettext MO (Machine Object) files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pocompile.html
for examples and usage instructions.
"""

from translate.misc.multistring import multistring
from translate.storage import factory, mo


def _do_msgidcomment(string):
    return u"_: %s\n" % string


class POCompile:

    def convertstore(self, inputfile, includefuzzy=False):
        outputfile = mo.mofile()
        for unit in inputfile.units:
            if unit.istranslated() or (unit.isfuzzy() and includefuzzy and unit.target) or unit.isheader():
                mounit = mo.mounit()
                if unit.isheader():
                    mounit.source = ""
                else:
                    mounit.source = unit.source
                    context = unit.getcontext()
                    if unit.msgidcomment:
                        if mounit.hasplural():
                            mounit.source = multistring(_do_msgidcomment(unit.msgidcomment) + mounit.source, *mounit.source.strings[1:])
                        else:
                            mounit.source = _do_msgidcomment(unit.msgidcomment) + mounit.source
                    elif context:
                        mounit.msgctxt = [context]
                mounit.target = unit.target
                outputfile.addunit(mounit)
        return str(outputfile)


def convertmo(inputfile, outputfile, templatefile, includefuzzy=False):
    """reads in a base class derived inputfile, converts using pocompile, writes to outputfile"""
    # note that templatefile is not used, but it is required by the converter...
    inputstore = factory.getobject(inputfile)
    if inputstore.isempty():
        return 0
    convertor = POCompile()
    outputmo = convertor.convertstore(inputstore, includefuzzy)
    # We have to make sure that we write the files in binary mode, therefore we
    # reopen the file accordingly
    outputfile.close()
    outputfile = open(outputfile.name, 'wb')
    outputfile.write(outputmo)
    return 1


def main():
    from translate.convert import convert
    formats = {"po": ("mo", convertmo), "xlf": ("mo", convertmo)}
    parser = convert.ConvertOptionParser(formats, usepots=False, description=__doc__)
    parser.add_fuzzy_option()
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = poconflicts
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005-2008,2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Conflict finder for Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/poconflicts.html
for examples and usage instructions.
"""

import os
import sys

from translate.misc import optrecurse
from translate.storage import factory, po


class ConflictOptionParser(optrecurse.RecursiveOptionParser):
    """a specialized Option Parser for the conflict tool..."""

    def parse_args(self, args=None, values=None):
        """parses the command line options, handling implicit input/output args"""
        (options, args) = optrecurse.optparse.OptionParser.parse_args(self, args, values)
        # some intelligence as to what reasonable people might give on the command line
        if args and not options.input:
            if not options.output:
                options.input = args[:-1]
                args = args[-1:]
            else:
                options.input = args
                args = []
        if args and not options.output:
            options.output = args[-1]
            args = args[:-1]
        if not options.output:
            self.error("output file is required")
        if args:
            self.error("You have used an invalid combination of --input, --output and freestanding args")
        if isinstance(options.input, list) and len(options.input) == 1:
            options.input = options.input[0]
        return (options, args)

    def set_usage(self, usage=None):
        """sets the usage string - if usage not given, uses getusagestring for each option"""
        if usage is None:
            self.usage = "%prog " + " ".join([self.getusagestring(option) for option in self.option_list]) + \
                    "\n  input directory is searched for PO files, PO files with name of conflicting string are output in output directory"
        else:
            super(ConflictOptionParser, self).set_usage(usage)

    def run(self):
        """parses the arguments, and runs recursiveprocess with the resulting options"""
        (options, args) = self.parse_args()
        options.inputformats = self.inputformats
        options.outputoptions = self.outputoptions
        self.recursiveprocess(options)

    def recursiveprocess(self, options):
        """recurse through directories and process files"""
        if self.isrecursive(options.input, 'input') and getattr(options, "allowrecursiveinput", True):
            if not self.isrecursive(options.output, 'output'):
                try:
                    self.warning("Output directory does not exist. Attempting to create")
                    os.mkdir(options.output)
                except:
                    self.error(optrecurse.optparse.OptionValueError("Output directory does not exist, attempt to create failed"))
            if isinstance(options.input, list):
                inputfiles = self.recurseinputfilelist(options)
            else:
                inputfiles = self.recurseinputfiles(options)
        else:
            if options.input:
                inputfiles = [os.path.basename(options.input)]
                options.input = os.path.dirname(options.input)
            else:
                inputfiles = [options.input]
        self.textmap = {}
        self.initprogressbar(inputfiles, options)
        for inputpath in inputfiles:
            fullinputpath = self.getfullinputpath(options, inputpath)
            try:
                success = self.processfile(None, options, fullinputpath)
            except Exception as error:
                if isinstance(error, KeyboardInterrupt):
                    raise
                self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
                success = False
            self.reportprogress(inputpath, success)
        del self.progressbar
        self.buildconflictmap()
        self.outputconflicts(options)

    def clean(self, string, options):
        """returns the cleaned string that contains the text to be matched"""
        if options.ignorecase:
            string = string.lower()
        for accelerator in options.accelchars:
            string = string.replace(accelerator, "")
        string = string.strip()
        return string

    def processfile(self, fileprocessor, options, fullinputpath):
        """process an individual file"""
        inputfile = self.openinputfile(options, fullinputpath)
        inputfile = factory.getobject(inputfile)
        for unit in inputfile.units:
            if unit.isheader() or not unit.istranslated():
                continue
            if unit.hasplural():
                continue
            if not options.invert:
                source = self.clean(unit.source, options)
                target = self.clean(unit.target, options)
            else:
                target = self.clean(unit.source, options)
                source = self.clean(unit.target, options)
            self.textmap.setdefault(source, []).append((target, unit, fullinputpath))

    def flatten(self, text, joinchar):
        """flattens text to just be words"""
        flattext = ""
        for c in text:
            if c.isalnum():
                flattext += c
            elif flattext[-1:].isalnum():
                flattext += joinchar
        return flattext.rstrip(joinchar)

    def buildconflictmap(self):
        """work out which strings are conflicting"""
        self.conflictmap = {}
        for source, translations in self.textmap.iteritems():
            source = self.flatten(source, " ")
            if len(source) <= 1:
                continue
            if len(translations) > 1:
                uniquetranslations = dict.fromkeys([target for target, unit, filename in translations])
                if len(uniquetranslations) > 1:
                    self.conflictmap[source] = translations

    def outputconflicts(self, options):
        """saves the result of the conflict match"""
        print("%d/%d different strings have conflicts" % (len(self.conflictmap), len(self.textmap)))
        reducedmap = {}

        def str_len(x):
            return len(x)

        for source, translations in self.conflictmap.iteritems():
            words = source.split()
            words.sort(key=str_len)
            source = words[-1]
            reducedmap.setdefault(source, []).extend(translations)
        # reduce plurals
        plurals = {}
        for word in reducedmap:
            if word + "s" in reducedmap:
                plurals[word] = word + "s"
        for word, pluralword in plurals.iteritems():
            reducedmap[word].extend(reducedmap.pop(pluralword))
        for source, translations in reducedmap.iteritems():
            flatsource = self.flatten(source, "-")
            fulloutputpath = os.path.join(options.output, flatsource + os.extsep + "po")
            conflictfile = po.pofile()
            for target, unit, filename in translations:
                unit.othercomments.append("# (poconflicts) %s\n" % filename)
                conflictfile.units.append(unit)
            open(fulloutputpath, "w").write(str(conflictfile))


def main():
    formats = {"po": ("po", None), None: ("po", None)}
    parser = ConflictOptionParser(formats)
    parser.add_option("-I", "--ignore-case", dest="ignorecase",
        action="store_true", default=False, help="ignore case distinctions")
    parser.add_option("-v", "--invert", dest="invert",
        action="store_true", default=False, help="invert the conflicts thus extracting conflicting destination words")
    parser.add_option("", "--accelerator", dest="accelchars", default="",
        metavar="ACCELERATORS", help="ignores the given accelerator characters when matching")
    parser.set_usage()
    parser.description = __doc__
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pocount
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2003-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Count strings and words for supported localization files.

These include: XLIFF, TMX, Gettex PO and MO, Qt .ts and .qm, Wordfast TM, etc

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pocount.html
for examples and usage instructions.
"""

from __future__ import print_function

import logging
import os
import sys
from argparse import ArgumentParser

from translate.storage import factory, statsdb


logger = logging.getLogger(__name__)

# define style constants
style_full, style_csv, style_short_strings, style_short_words = range(4)

# default output style
default_style = style_full


def calcstats_old(filename):
    """This is the previous implementation of calcstats() and is left for
    comparison and debuging purposes."""
    # ignore totally blank or header units
    try:
        store = factory.getobject(filename)
    except ValueError as e:
        logger.warning(e)
        return {}
    units = filter(lambda unit: unit.istranslatable(), store.units)
    translated = translatedmessages(units)
    fuzzy = fuzzymessages(units)
    review = filter(lambda unit: unit.isreview(), units)
    untranslated = untranslatedmessages(units)
    wordcounts = dict(map(lambda unit: (unit, statsdb.wordsinunit(unit)), units))
    sourcewords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][0], elementlist))
    targetwords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][1], elementlist))
    stats = {}

    # units
    stats["translated"] = len(translated)
    stats["fuzzy"] = len(fuzzy)
    stats["untranslated"] = len(untranslated)
    stats["review"] = len(review)
    stats["total"] = stats["translated"] + \
                     stats["fuzzy"] + \
                     stats["untranslated"]

    # words
    stats["translatedsourcewords"] = sourcewords(translated)
    stats["translatedtargetwords"] = targetwords(translated)
    stats["fuzzysourcewords"] = sourcewords(fuzzy)
    stats["untranslatedsourcewords"] = sourcewords(untranslated)
    stats["reviewsourcewords"] = sourcewords(review)
    stats["totalsourcewords"] = stats["translatedsourcewords"] + \
                                stats["fuzzysourcewords"] + \
                                stats["untranslatedsourcewords"]
    return stats


def calcstats(filename):
    statscache = statsdb.StatsCache()
    return statscache.filetotals(filename, extended=True)


def summarize(title, stats, style=style_full, indent=8, incomplete_only=False):
    """Print summary for a .po file in specified format.

    :param title: name of .po file
    :param stats: array with translation statistics for the file specified
    :param indent: indentation of the 2nd column (length of longest filename)
    :param incomplete_only: omit fully translated files
    :type incomplete_only: Boolean
    :rtype: Boolean
    :return: 1 if counting incomplete files (incomplete_only=True) and the
             file is completely translated, 0 otherwise
    """

    def percent(denominator, devisor):
        if devisor == 0:
            return 0
        else:
            return denominator * 100 / devisor

    if incomplete_only and (stats["total"] == stats["translated"]):
        return 1

    if (style == style_csv):
        print("%s, " % title, end=' ')
        print("%d, %d, %d," % (stats["translated"],
                               stats["translatedsourcewords"],
                               stats["translatedtargetwords"]), end=' ')
        print("%d, %d," % (stats["fuzzy"], stats["fuzzysourcewords"]), end=' ')
        print("%d, %d," % (stats["untranslated"],
                           stats["untranslatedsourcewords"]), end=' ')
        print("%d, %d" % (stats["total"], stats["totalsourcewords"]), end=' ')
        if stats["review"] > 0:
            print(", %d, %d" % (stats["review"], stats["reviewsourdcewords"]), end=' ')
        print()
    elif (style == style_short_strings):
        spaces = " " * (indent - len(title))
        print("%s%s strings: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (
              title, spaces,
              stats["total"], stats["translated"], stats["fuzzy"], stats["untranslated"],
              percent(stats["translated"], stats["total"]),
              percent(stats["fuzzy"], stats["total"]),
              percent(stats["untranslated"], stats["total"])))
    elif (style == style_short_words):
        spaces = " " * (indent - len(title))
        print("%s%s source words: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (
              title, spaces,
              stats["totalsourcewords"], stats["translatedsourcewords"], stats["fuzzysourcewords"], stats["untranslatedsourcewords"],
              percent(stats["translatedsourcewords"], stats["totalsourcewords"]),
              percent(stats["fuzzysourcewords"], stats["totalsourcewords"]),
              percent(stats["untranslatedsourcewords"], stats["totalsourcewords"])))
    else:  # style == style_full
        print(title)
        print("type              strings      words (source)    words (translation)")
        print("translated:   %5d (%3d%%) %10d (%3d%%) %15d" % (
              stats["translated"],
              percent(stats["translated"], stats["total"]),
              stats["translatedsourcewords"],
              percent(stats["translatedsourcewords"], stats["totalsourcewords"]),
              stats["translatedtargetwords"]))
        print("fuzzy:        %5d (%3d%%) %10d (%3d%%)             n/a" % (
              stats["fuzzy"],
              percent(stats["fuzzy"], stats["total"]),
              stats["fuzzysourcewords"],
              percent(stats["fuzzysourcewords"], stats["totalsourcewords"])))
        print("untranslated: %5d (%3d%%) %10d (%3d%%)             n/a" % (
              stats["untranslated"],
              percent(stats["untranslated"], stats["total"]),
              stats["untranslatedsourcewords"],
              percent(stats["untranslatedsourcewords"], stats["totalsourcewords"])))
        print("Total:        %5d %17d %22d" % (
              stats["total"],
              stats["totalsourcewords"],
              stats["translatedtargetwords"]))
        if "extended" in stats:
            print("")
            for state, e_stats in stats["extended"].iteritems():
                print("%s:    %5d (%3d%%) %10d (%3d%%) %15d" % (
                      state, e_stats["units"], percent(e_stats["units"], stats["total"]),
                      e_stats["sourcewords"], percent(e_stats["sourcewords"], stats["totalsourcewords"]),
                      e_stats["targetwords"]))

        if stats["review"] > 0:
            print("review:       %5d %17d                    n/a" % (
                  stats["review"], stats["reviewsourcewords"]))
        print()
    return 0


def fuzzymessages(units):
    return filter(lambda unit: unit.isfuzzy() and unit.target, units)


def translatedmessages(units):
    return filter(lambda unit: unit.istranslated(), units)


def untranslatedmessages(units):
    return filter(lambda unit: not (unit.istranslated() or unit.isfuzzy()) and unit.source, units)


class summarizer:

    def __init__(self, filenames, style=default_style, incomplete_only=False):
        self.totals = {}
        self.filecount = 0
        self.longestfilename = 0
        self.style = style
        self.incomplete_only = incomplete_only
        self.complete_count = 0

        if (self.style == style_csv):
            print("""Filename, Translated Messages, Translated Source Words, Translated
Target Words, Fuzzy Messages, Fuzzy Source Words, Untranslated Messages,
Untranslated Source Words, Total Message, Total Source Words,
Review Messages, Review Source Words""")
        if (self.style == style_short_strings or self.style == style_short_words):
            for filename in filenames:  # find longest filename
                if (len(filename) > self.longestfilename):
                    self.longestfilename = len(filename)
        for filename in filenames:
            if not os.path.exists(filename):
                logger.error("cannot process %s: does not exist", filename)
                continue
            elif os.path.isdir(filename):
                self.handledir(filename)
            else:
                self.handlefile(filename)
        if self.filecount > 1 and (self.style == style_full):
            if self.incomplete_only:
                summarize("TOTAL (incomplete only):", self.totals,
                incomplete_only=True)
                print("File count (incomplete):   %5d" % (self.filecount - self.complete_count))
            else:
                summarize("TOTAL:", self.totals, incomplete_only=False)
            print("File count:   %5d" % (self.filecount))
            print()

    def updatetotals(self, stats):
        """Update self.totals with the statistics in stats."""
        for key in stats.keys():
            if key == "extended":
                # FIXME: calculate extended totals
                continue
            if not key in self.totals:
                self.totals[key] = 0
            self.totals[key] += stats[key]

    def handlefile(self, filename):
        try:
            stats = calcstats(filename)
            self.updatetotals(stats)
            self.complete_count += summarize(filename, stats, self.style,
                                             self.longestfilename,
                                             self.incomplete_only)
            self.filecount += 1
        except Exception:  # This happens if we have a broken file.
            logger.error(sys.exc_info()[1])

    def handlefiles(self, dirname, filenames):
        for filename in filenames:
            pathname = os.path.join(dirname, filename)
            if os.path.isdir(pathname):
                self.handledir(pathname)
            else:
                self.handlefile(pathname)

    def handledir(self, dirname):
        path, name = os.path.split(dirname)
        if name in ["CVS", ".svn", "_darcs", ".git", ".hg", ".bzr"]:
            return
        entries = os.listdir(dirname)
        self.handlefiles(dirname, entries)


def main():
    parser = ArgumentParser()
    parser.add_argument("--incomplete", action="store_true", default=False,
                        dest="incomplete_only",
                        help="skip 100%% translated files.")
    if sys.version_info[:2] <= (2, 6):
        # Python 2.6 using argparse from PyPI cannot define a mutually
        # exclusive group as a child of a group, but it works if it is a child
        # of the parser.  We lose the group title but the functionality works.
        # See https://code.google.com/p/argparse/issues/detail?id=90
        megroup = parser.add_mutually_exclusive_group()
    else:
        output_group = parser.add_argument_group("Output format")
        megroup = output_group.add_mutually_exclusive_group()
    megroup.add_argument("--full", action="store_const", const=style_full,
                        dest="style", default=style_full,
                        help="(default) statistics in full, verbose format")
    megroup.add_argument("--csv", action="store_const", const=style_csv,
                        dest="style",
                        help="statistics in CSV format")
    megroup.add_argument("--short", action="store_const", const=style_short_strings,
                        dest="style",
                        help="same as --short-strings")
    megroup.add_argument("--short-strings", action="store_const",
                        const=style_short_strings, dest="style",
                        help="statistics of strings in short format - one line per file")
    megroup.add_argument("--short-words", action="store_const",
                        const=style_short_words, dest="style",
                        help="statistics of words in short format - one line per file")

    parser.add_argument("files", nargs="+")

    args = parser.parse_args()

    logging.basicConfig(format="%(name)s: %(levelname)s: %(message)s")

    summarizer(args.files, args.style, args.incomplete_only)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = podebug
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2004-2006,2008-2010 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Insert debug messages into XLIFF and Gettext PO localization files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/podebug.html
for examples and usage instructions.
"""

import os
import re
from hashlib import md5

from translate.convert import dtd2po
from translate.storage import factory
from translate.storage.placeables import (StringElem, general,
                                          parse as rich_parse)


def add_prefix(prefix, stringelems):
    for stringelem in stringelems:
        for string in stringelem.flatten():
            if len(string.sub) > 0:
                string.sub[0] = prefix + string.sub[0]
                break
    return stringelems

podebug_parsers = general.parsers
podebug_parsers.remove(general.CapsPlaceable.parse)
podebug_parsers.remove(general.CamelCasePlaceable.parse)


class podebug:

    def __init__(self, format=None, rewritestyle=None, ignoreoption=None):
        if format is None:
            self.format = ""
        else:
            self.format = format
        self.rewritefunc = getattr(self, "rewrite_%s" % rewritestyle, None)
        self.ignorefunc = getattr(self, "ignore_%s" % ignoreoption, None)

    def apply_to_translatables(self, string, func):
        """Applies func to all translatable strings in string."""
        string.map(
            lambda e: e.apply_to_strings(func),
            lambda e: e.isleaf() and e.istranslatable)

    @classmethod
    def rewritelist(cls):
        return [rewrite.replace("rewrite_", "") for rewrite in dir(cls) if rewrite.startswith("rewrite_")]

    def _rewrite_prepend_append(self, string, prepend, append=None):
        if append is None:
            append = prepend
        if not isinstance(string, StringElem):
            string = StringElem(string)
        string.sub.insert(0, prepend)
        if unicode(string).endswith(u'\n'):
            # Try and remove the last character from the tree
            try:
                lastnode = string.flatten()[-1]
                if isinstance(lastnode.sub[-1], unicode):
                    lastnode.sub[-1] = lastnode.sub[-1].rstrip(u'\n')
            except IndexError:
                pass
            string.sub.append(append + u'\n')
        else:
            string.sub.append(append)
        return string

    def rewrite_xxx(self, string):
        return self._rewrite_prepend_append(string, u"xxx")

    def rewrite_bracket(self, string):
        return self._rewrite_prepend_append(string, u"[", u"]")

    def rewrite_en(self, string):
        if not isinstance(string, StringElem):
            string = StringElem(string)
        return string

    def rewrite_blank(self, string):
        return StringElem(u"")

    def rewrite_chef(self, string):
        """Rewrite using Mock Swedish as made famous by Monty Python"""
        if not isinstance(string, StringElem):
            string = StringElem(string)
        # From Dive into Python which itself got it elsewhere
        # http://www.renderx.com/demos/examples/diveintopython.pdf
        subs = (
               (r'a([nu])', r'u\1'),
               (r'A([nu])', r'U\1'),
               (r'a\B', r'e'),
               (r'A\B', r'E'),
               (r'en\b', r'ee'),
               (r'\Bew', r'oo'),
               (r'\Be\b', r'e-a'),
               (r'\be', r'i'),
               (r'\bE', r'I'),
               (r'\Bf', r'ff'),
               (r'\Bir', r'ur'),
               (r'(\w*?)i(\w*?)$', r'\1ee\2'),
               (r'\bow', r'oo'),
               (r'\bo', r'oo'),
               (r'\bO', r'Oo'),
               (r'the', r'zee'),
               (r'The', r'Zee'),
               (r'th\b', r't'),
               (r'\Btion', r'shun'),
               (r'\Bu', r'oo'),
               (r'\BU', r'Oo'),
               (r'v', r'f'),
               (r'V', r'F'),
               (r'w', r'w'),
               (r'W', r'W'),
               (r'([a-z])[.]', r'\1. Bork Bork Bork!'))
        for a, b in subs:
            self.apply_to_translatables(string, lambda s: re.sub(a, b, s))
        return string

    REWRITE_UNICODE_MAP = u"" + u"[\\]^_`" + u""

    def rewrite_unicode(self, string):
        """Convert to Unicode characters that look like the source string"""
        if not isinstance(string, StringElem):
            string = StringElem(string)

        def transpose(char):
            loc = ord(char) - 65
            if loc < 0 or loc > 56:
                return char
            return self.REWRITE_UNICODE_MAP[loc]

        def transformer(s):
            return ''.join([transpose(c) for c in s])
        self.apply_to_translatables(string, transformer)
        return string

    REWRITE_FLIPPED_MAP = u"#$%,()+-/0125986:;<=>@" + \
            u"HIWNOSXZ" + u"[\\]_," + \
            u"qpuodbsnxz"
    # Brackets should be swapped if the string will be reversed in memory.
    # If a right-to-left override is used, the brackets should be
    # unchanged.
    # Some alternatives:
    #  D: 
    #  K: 
    #  @:  - Seems only related in Dejavu Sans
    #  Q:        
    #  _:  - left out for now for the sake of GTK accelerators

    def rewrite_flipped(self, string):
        """Convert the string to look flipped upside down."""
        if not isinstance(string, StringElem):
            string = StringElem(string)

        def transpose(char):
            loc = ord(char) - 33
            if loc < 0 or loc > 89:
                return char
            return self.REWRITE_FLIPPED_MAP[loc]

        def transformer(s):
            return u"\u202e" + u''.join([transpose(c) for c in s])
            # To reverse instead of using the RTL override:
            #return u''.join(reversed([transpose(c) for c in s]))
        self.apply_to_translatables(string, transformer)
        return string

    @classmethod
    def ignorelist(cls):
        return [ignore.replace("ignore_", "") for ignore in dir(cls) if ignore.startswith("ignore_")]

    def ignore_openoffice(self, unit):
        for location in unit.getlocations():
            if location.startswith("Common.xcu#..Common.View.Localisation"):
                return True
            elif location.startswith("profile.lng#STR_DIR_MENU_NEW_"):
                return True
            elif location.startswith("profile.lng#STR_DIR_MENU_WIZARD_"):
                return True
        return False

    def ignore_libreoffice(self, unit):
        return ignore_openoffice(unit)

    def ignore_mozilla(self, unit):
        locations = unit.getlocations()
        if len(locations) == 1 and locations[0].lower().endswith(".accesskey"):
            return True
        for location in locations:
            if dtd2po.is_css_entity(location):
                return True
            if location in ["brandShortName", "brandFullName", "vendorShortName"]:
                return True
            if location.lower().endswith(".commandkey") or location.endswith(".key"):
                return True
        return False

    def ignore_gtk(self, unit):
        if unit.source == "default:LTR":
            return True
        return False

    def ignore_kde(self, unit):
        if unit.source == "LTR":
            return True
        return False

    def convertunit(self, unit, prefix):
        if self.ignorefunc:
            if self.ignorefunc(unit):
                return unit
        if prefix.find("@hash_placeholder@") != -1:
            if unit.getlocations():
                hashable = unit.getlocations()[0]
            else:
                hashable = unit.source
            prefix = prefix.replace("@hash_placeholder@", md5(hashable).hexdigest()[:self.hash_len])
        if unit.istranslated():
            rich_string = unit.rich_target
        else:
            rich_string = unit.rich_source
        if not isinstance(rich_string, StringElem):
            rich_string = [rich_parse(string, podebug_parsers) for string in rich_string]
        if self.rewritefunc:
            rewritten = [self.rewritefunc(string) for string in rich_string]
            if rewritten:
                rich_string = rewritten
        unit.rich_target = add_prefix(prefix, rich_string)
        return unit

    def convertstore(self, store):
        filename = self.shrinkfilename(store.filename)
        prefix = self.format
        for formatstr in re.findall("%[0-9c]*[sfFbBdh]", self.format):
            if formatstr.endswith("s"):
                formatted = self.shrinkfilename(store.filename)
            elif formatstr.endswith("f"):
                formatted = store.filename
                formatted = os.path.splitext(formatted)[0]
            elif formatstr.endswith("F"):
                formatted = store.filename
            elif formatstr.endswith("b"):
                formatted = os.path.basename(store.filename)
                formatted = os.path.splitext(formatted)[0]
            elif formatstr.endswith("B"):
                formatted = os.path.basename(store.filename)
            elif formatstr.endswith("d"):
                formatted = os.path.dirname(store.filename)
            elif formatstr.endswith("h"):
                try:
                    self.hash_len = int(filter(str.isdigit, formatstr[1:-1]))
                except ValueError:
                    self.hash_len = 4
                formatted = "@hash_placeholder@"
            else:
                continue
            formatoptions = formatstr[1:-1]
            if formatoptions and not formatstr.endswith("h"):
                if "c" in formatoptions and formatted:
                    formatted = formatted[0] + filter(lambda x: x.lower() not in "aeiou", formatted[1:])
                length = filter(str.isdigit, formatoptions)
                if length:
                    formatted = formatted[:int(length)]
            prefix = prefix.replace(formatstr, formatted)
        for unit in store.units:
            if not unit.istranslatable():
                continue
            unit = self.convertunit(unit, prefix)
        return store

    def shrinkfilename(self, filename):
        if filename.startswith("." + os.sep):
            filename = filename.replace("." + os.sep, "", 1)
        dirname = os.path.dirname(filename)
        dirparts = dirname.split(os.sep)
        if not dirparts:
            dirshrunk = ""
        else:
            dirshrunk = dirparts[0][:4] + "-"
            if len(dirparts) > 1:
                dirshrunk += "".join([dirpart[0] for dirpart in dirparts[1:]]) + "-"
        baseshrunk = os.path.basename(filename)[:4]
        if "." in baseshrunk:
            baseshrunk = baseshrunk[:baseshrunk.find(".")]
        return dirshrunk + baseshrunk


def convertpo(inputfile, outputfile, templatefile, format=None, rewritestyle=None, ignoreoption=None):
    """Reads in inputfile, changes it to have debug strings, writes to outputfile."""
    # note that templatefile is not used, but it is required by the converter...
    inputstore = factory.getobject(inputfile)
    if inputstore.isempty():
        return 0
    convertor = podebug(format=format, rewritestyle=rewritestyle, ignoreoption=ignoreoption)
    outputstore = convertor.convertstore(inputstore)
    outputfile.write(str(outputstore))
    return 1


def main():
    from translate.convert import convert
    formats = {
            "po": ("po", convertpo), "pot": ("po", convertpo),
            "xlf": ("xlf", convertpo),
            "tmx": ("tmx", convertpo),
    }
    parser = convert.ConvertOptionParser(formats, description=__doc__)
    # TODO: add documentation on format strings...
    parser.add_option("-f", "--format", dest="format", default="",
        help="specify format string")
    parser.add_option("", "--rewrite", dest="rewritestyle",
        type="choice", choices=podebug.rewritelist(), metavar="STYLE",
        help="the translation rewrite style: %s" % ", ".join(podebug.rewritelist()))
    parser.add_option("", "--ignore", dest="ignoreoption",
        type="choice", choices=podebug.ignorelist(), metavar="APPLICATION",
        help="apply tagging ignore rules for the given application: %s" % ", ".join(podebug.ignorelist()))
    parser.passthrough.append("format")
    parser.passthrough.append("rewritestyle")
    parser.passthrough.append("ignoreoption")
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pogrep
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2011 Zuza Software Foundation
# Copyright 2013 F Wolff
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Grep XLIFF, Gettext PO and TMX localization files.

Matches are output to snippet files of the same type which can then be reviewed
and later merged using :doc:`pomerge </commands/pomerge>`.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pogrep.html
for examples and usage instructions.
"""

import locale
import re

from translate.lang import data
from translate.misc import optrecurse
from translate.misc.multistring import multistring
from translate.storage import factory
from translate.storage.poheader import poheader


class GrepMatch(object):
    """Just a small data structure that represents a search match."""

    # INITIALIZERS #
    def __init__(self, unit, part='target', part_n=0, start=0, end=0):
        self.unit = unit
        self.part = part
        self.part_n = part_n
        self.start = start
        self.end = end

    # ACCESSORS #
    def get_getter(self):
        if self.part == 'target':
            if self.unit.hasplural():
                getter = lambda: self.unit.target.strings[self.part_n]
            else:
                getter = lambda: self.unit.target
            return getter
        elif self.part == 'source':
            if self.unit.hasplural():
                getter = lambda: self.unit.source.strings[self.part_n]
            else:
                getter = lambda: self.unit.source
            return getter
        elif self.part == 'notes':

            def getter():
                return self.unit.getnotes()[self.part_n]
            return getter
        elif self.part == 'locations':

            def getter():
                return self.unit.getlocations()[self.part_n]
            return getter

    def get_setter(self):
        if self.part == 'target':
            if self.unit.hasplural():

                def setter(value):
                    strings = self.unit.target.strings
                    strings[self.part_n] = value
                    self.unit.target = strings
            else:

                def setter(value):
                    self.unit.target = value
            return setter

    # SPECIAL METHODS #
    def __str__(self):
        start, end = self.start, self.end
        if start < 3:
            start = 3
        if end > len(self.get_getter()()) - 3:
            end = len(self.get_getter()()) - 3
        matchpart = self.get_getter()()[start-2:end+2]
        return '<GrepMatch "%s" part=%s[%d] start=%d end=%d>' % (matchpart, self.part, self.part_n, self.start, self.end)

    def __repr__(self):
        return str(self)


def real_index(string, nfc_index):
    """Calculate the real index in the unnormalized string that corresponds to
    the index nfc_index in the normalized string."""
    length = nfc_index
    max_length = len(string)
    while len(data.normalize(string[:length])) <= nfc_index:
        if length == max_length:
            return length
        length += 1
    return length - 1


def find_matches(unit, part, strings, re_search):
    """Return the GrepFilter objects where re_search matches in strings."""
    matches = []
    for n, string in enumerate(strings):
        if not string:
            continue
        normalized = data.normalize(string)
        if normalized == string:
            index_func = lambda s, i: i
        else:
            index_func = real_index
        for matchobj in re_search.finditer(normalized):
            start = index_func(string, matchobj.start())
            end = index_func(string, matchobj.end())
            matches.append(GrepMatch(unit, part=part, part_n=n, start=start, end=end))
    return matches


class GrepFilter:

    def __init__(self, searchstring, searchparts, ignorecase=False, useregexp=False,
            invertmatch=False, keeptranslations=False, accelchar=None, encoding='utf-8',
            max_matches=0):
        """builds a checkfilter using the given checker"""
        if isinstance(searchstring, unicode):
            self.searchstring = searchstring
        else:
            self.searchstring = searchstring.decode(encoding)
        self.searchstring = data.normalize(self.searchstring)
        if searchparts:
            # For now we still support the old terminology, except for the old 'source'
            # which has a new meaning now.
            self.search_source = ('source' in searchparts) or ('msgid' in searchparts)
            self.search_target = ('target' in searchparts) or ('msgstr' in searchparts)
            self.search_notes = ('notes' in searchparts) or ('comment' in searchparts)
            self.search_locations = 'locations' in searchparts
        else:
            self.search_source = True
            self.search_target = True
            self.search_notes = False
            self.search_locations = False
        self.ignorecase = ignorecase
        if self.ignorecase:
            self.searchstring = self.searchstring.lower()
        self.useregexp = useregexp
        if self.useregexp:
            self.searchpattern = re.compile(self.searchstring)
        self.invertmatch = invertmatch
        self.keeptranslations = keeptranslations
        self.accelchar = accelchar
        self.max_matches = max_matches

    def matches(self, teststr):
        if teststr is None:
            return False
        teststr = data.normalize(teststr)
        if self.ignorecase:
            teststr = teststr.lower()
        if self.accelchar:
            teststr = re.sub(self.accelchar + self.accelchar, "#", teststr)
            teststr = re.sub(self.accelchar, "", teststr)
        if self.useregexp:
            found = self.searchpattern.search(teststr)
        else:
            found = teststr.find(self.searchstring) != -1
        if self.invertmatch:
            found = not found
        return found

    def filterunit(self, unit):
        """runs filters on an element"""
        if unit.isheader():
            return True

        if self.keeptranslations and unit.target:
            return True

        if self.search_source:
            if isinstance(unit.source, multistring):
                strings = unit.source.strings
            else:
                strings = [unit.source]
            for string in strings:
                if self.matches(string):
                    return True

        if self.search_target:
            if isinstance(unit.target, multistring):
                strings = unit.target.strings
            else:
                strings = [unit.target]
            for string in strings:
                if self.matches(string):
                    return True

        if self.search_notes:
            if self.matches(unit.getnotes()):
                return True
        if self.search_locations:
            if self.matches(u" ".join(unit.getlocations())):
                return True
        return False

    def filterfile(self, thefile):
        """runs filters on a translation file object"""
        thenewfile = type(thefile)()
        thenewfile.setsourcelanguage(thefile.sourcelanguage)
        thenewfile.settargetlanguage(thefile.targetlanguage)
        for unit in thefile.units:
            if self.filterunit(unit):
                thenewfile.addunit(unit)

        if isinstance(thenewfile, poheader):
            thenewfile.updateheader(add=True, **thefile.parseheader())
        return thenewfile

    def getmatches(self, units):
        if not self.searchstring:
            return [], []

        searchstring = self.searchstring
        flags = re.LOCALE | re.MULTILINE | re.UNICODE

        if self.ignorecase:
            flags |= re.IGNORECASE
        if not self.useregexp:
            searchstring = re.escape(searchstring)
        self.re_search = re.compile(u'(%s)' % (searchstring), flags)

        matches = []
        indexes = []

        for index, unit in enumerate(units):
            old_length = len(matches)

            if self.search_target:
                if unit.hasplural():
                    targets = unit.target.strings
                else:
                    targets = [unit.target]
                matches.extend(find_matches(unit, 'target', targets, self.re_search))
            if self.search_source:
                if unit.hasplural():
                    sources = unit.source.strings
                else:
                    sources = [unit.source]
                matches.extend(find_matches(unit, 'source', sources, self.re_search))
            if self.search_notes:
                matches.extend(find_matches(unit, 'notes', unit.getnotes(), self.re_search))

            if self.search_locations:
                matches.extend(find_matches(unit, 'locations', unit.getlocations(), self.re_search))

            # A search for a single letter or an all-inclusive regular
            # expression could give enough results to cause performance
            # problems. The answer is probably not very useful at this scale.
            if self.max_matches and len(matches) > self.max_matches:
                raise Exception("Too many matches found")

            if len(matches) > old_length:
                old_length = len(matches)
                indexes.append(index)

        return matches, indexes


class GrepOptionParser(optrecurse.RecursiveOptionParser):
    """a specialized Option Parser for the grep tool..."""

    def parse_args(self, args=None, values=None):
        """parses the command line options, handling implicit input/output args"""
        (options, args) = optrecurse.optparse.OptionParser.parse_args(self, args, values)
        # some intelligence as to what reasonable people might give on the command line
        if args:
            options.searchstring = args[0]
            args = args[1:]
        else:
            self.error("At least one argument must be given for the search string")
        if args and not options.input:
            if not options.output:
                options.input = args[:-1]
                args = args[-1:]
            else:
                options.input = args
                args = []
        if args and not options.output:
            options.output = args[-1]
            args = args[:-1]
        if args:
            self.error("You have used an invalid combination of --input, --output and freestanding args")
        if isinstance(options.input, list) and len(options.input) == 1:
            options.input = options.input[0]
        return (options, args)

    def set_usage(self, usage=None):
        """sets the usage string - if usage not given, uses getusagestring for each option"""
        if usage is None:
            self.usage = "%prog searchstring " + " ".join([self.getusagestring(option) for option in self.option_list])
        else:
            super(GrepOptionParser, self).set_usage(usage)

    def run(self):
        """parses the arguments, and runs recursiveprocess with the resulting options"""
        (options, args) = self.parse_args()
        options.inputformats = self.inputformats
        options.outputoptions = self.outputoptions
        options.checkfilter = GrepFilter(options.searchstring,
                                         options.searchparts,
                                         options.ignorecase,
                                         options.useregexp,
                                         options.invertmatch,
                                         options.keeptranslations,
                                         options.accelchar,
                                         locale.getpreferredencoding())
        self.recursiveprocess(options)


def rungrep(inputfile, outputfile, templatefile, checkfilter):
    """reads in inputfile, filters using checkfilter, writes to outputfile"""
    fromfile = factory.getobject(inputfile)
    tofile = checkfilter.filterfile(fromfile)
    if tofile.isempty():
        return False
    outputfile.write(str(tofile))
    return True


def cmdlineparser():
    formats = {"po": ("po", rungrep), "pot": ("pot", rungrep),
            "mo": ("mo", rungrep), "gmo": ("gmo", rungrep),
            "tmx": ("tmx", rungrep),
            "xliff": ("xliff", rungrep), "xlf": ("xlf", rungrep), "xlff": ("xlff", rungrep),
            None: ("po", rungrep)}
    parser = GrepOptionParser(formats)
    parser.add_option("", "--search", dest="searchparts",
        action="append", type="choice", choices=["source", "target", "notes", "locations", "msgid", "msgstr", "comment"],
        metavar="SEARCHPARTS", help="searches the given parts (source, target, notes and locations)")
    parser.add_option("-I", "--ignore-case", dest="ignorecase",
        action="store_true", default=False, help="ignore case distinctions")
    parser.add_option("-e", "--regexp", dest="useregexp",
        action="store_true", default=False, help="use regular expression matching")
    parser.add_option("-v", "--invert-match", dest="invertmatch",
        action="store_true", default=False, help="select non-matching lines")
    parser.add_option("", "--accelerator", dest="accelchar",
        action="store", type="choice", choices=["&", "_", "~"],
        metavar="ACCELERATOR", help="ignores the given accelerator when matching")
    parser.add_option("-k", "--keep-translations", dest="keeptranslations",
        action="store_true", default=False, help="always extract units with translations")
    parser.set_usage()
    parser.passthrough.append('checkfilter')
    parser.description = __doc__
    return parser


def main():
    parser = cmdlineparser()
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pomerge
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2002-2010 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Merges XLIFF and Gettext PO localization files.

Snippet file produced by e.g. :doc:`pogrep </commands/pogrep>` and updated by a
translator can be merged back into the original files.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pomerge.html
for examples and usage instructions.
"""

import logging

from translate.storage import factory
from translate.storage.poheader import poheader


def mergestores(store1, store2, mergeblanks, mergefuzzy, mergecomments):
    """Take any new translations in store2 and write them into store1."""

    for unit2 in store2.units:
        if unit2.isheader():
            if isinstance(store1, poheader):
                store1.mergeheaders(store2)
            continue
        unit1 = store1.findid(unit2.getid())
        if unit1 is None:
            unit1 = store1.findunit(unit2.source)
        if unit1 is None:
            logging.error("The template does not contain the following unit:\n%s",
                          str(unit2))
        else:
            if not mergeblanks:
                if len(unit2.target.strip()) == 0:
                    continue
            if not mergefuzzy:
                if unit2.isfuzzy():
                    continue
            unit1.merge(unit2, overwrite=True, comments=mergecomments)
    return store1


def str2bool(option):
    """Convert a string value to boolean

    :param option: yes, true, 1, no, false, 0
    :type option: String
    :rtype: Boolean

    """
    option = option.lower()
    if option in ("yes", "true", "1"):
        return True
    elif option in ("no", "false", "0"):
        return False
    else:
        raise ValueError("invalid boolean value: %r" % option)


def mergestore(inputfile, outputfile, templatefile, mergeblanks="no", mergefuzzy="no",
               mergecomments="yes"):
    try:
        mergecomments = str2bool(mergecomments)
    except ValueError:
        raise ValueError("invalid mergecomments value: %r" % mergecomments)
    try:
        mergeblanks = str2bool(mergeblanks)
    except ValueError:
        raise ValueError("invalid mergeblanks value: %r" % mergeblanks)
    try:
        mergefuzzy = str2bool(mergefuzzy)
    except ValueError:
        raise ValueError("invalid mergefuzzy value: %r" % mergefuzzy)
    inputstore = factory.getobject(inputfile)
    if templatefile is None:
        # just merge nothing
        templatestore = type(inputstore)()
    else:
        templatestore = factory.getobject(templatefile)
    outputstore = mergestores(templatestore, inputstore, mergeblanks,
                    mergefuzzy, mergecomments)
    if outputstore.isempty():
        return 0
    outputfile.write(str(outputstore))
    return 1


def main():
    from translate.convert import convert
    pooutput = ("po", mergestore)
    potoutput = ("pot", mergestore)
    xliffoutput = ("xlf", mergestore)
    formats = {
        ("po", "po"): pooutput, ("po", "pot"): pooutput,
        ("pot", "po"): pooutput, ("pot", "pot"): potoutput,
        "po": pooutput, "pot": pooutput,
        ("xlf", "po"): pooutput, ("xlf", "pot"): pooutput,
        ("xlf", "xlf"): xliffoutput, ("po", "xlf"): xliffoutput,
    }
    mergeblanksoption = convert.optparse.Option("", "--mergeblanks",
        dest="mergeblanks", action="store", default="yes",
        help="whether to overwrite existing translations with blank translations (yes/no). Default is yes.")
    mergefuzzyoption = convert.optparse.Option("", "--mergefuzzy",
        dest="mergefuzzy", action="store", default="yes",
        help="whether to consider fuzzy translations from input (yes/no). Default is yes.")
    mergecommentsoption = convert.optparse.Option("", "--mergecomments",
        dest="mergecomments", action="store", default="yes",
        help="whether to merge comments as well as translations (yes/no). Default is yes.")
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         description=__doc__)
    parser.add_option(mergeblanksoption)
    parser.passthrough.append("mergeblanks")
    parser.add_option(mergefuzzyoption)
    parser.passthrough.append("mergefuzzy")
    parser.add_option(mergecommentsoption)
    parser.passthrough.append("mergecomments")
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = porestructure
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Restructure Gettxt PO files produced by
:doc:`poconflicts </commands/poconflicts>` into the original directory tree
for merging using :doc:`pomerge </commands/pomerge>`.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pomerge.html
for examples and usage instructions.
"""

import os
import sys

from translate.misc import optrecurse
from translate.storage import po


class SplitOptionParser(optrecurse.RecursiveOptionParser):
    """a specialized Option Parser for posplit"""

    def parse_args(self, args=None, values=None):
        """parses the command line options, handling implicit input/output args"""
        (options, args) = optrecurse.RecursiveOptionParser.parse_args(self, args, values)
        if not options.output:
            self.error("Output file is rquired")
        return (options, args)

    def set_usage(self, usage=None):
        """sets the usage string - if usage not given, uses getusagestring for each option"""
        if usage is None:
            self.usage = "%prog " + " ".join([self.getusagestring(option) for option in self.option_list]) + \
                         "\n  " + \
                         "input directory is searched for PO files with (poconflicts) comments, all entries are written to files in a directory structure for pomerge"
        else:
            super(SplitOptionParser, self).set_usage(usage)

    def recursiveprocess(self, options):
        """recurse through directories and process files"""
        if not self.isrecursive(options.output, 'output'):
            try:
                self.warning("Output directory does not exist. Attempting to create")
                # TODO: maybe we should only allow it to be created, otherwise
                # we mess up an existing tree.
                os.mkdir(options.output)
            except:
                self.error(optrecurse.optparse.OptionValueError("Output directory does not exist, attempt to create failed"))
        if self.isrecursive(options.input, 'input') and getattr(options, "allowrecursiveinput", True):
            if isinstance(options.input, list):
                inputfiles = self.recurseinputfilelist(options)
            else:
                inputfiles = self.recurseinputfiles(options)
        else:
            if options.input:
                inputfiles = [os.path.basename(options.input)]
                options.input = os.path.dirname(options.input)
            else:
                inputfiles = [options.input]
        self.textmap = {}
        self.initprogressbar(inputfiles, options)
        for inputpath in inputfiles:
            fullinputpath = self.getfullinputpath(options, inputpath)
            try:
                success = self.processfile(options, fullinputpath)
            except Exception as error:
                if isinstance(error, KeyboardInterrupt):
                    raise self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
                success = False
            self.reportprogress(inputpath, success)
        del self.progressbar

    def processfile(self, options, fullinputpath):
        """process an individual file"""
        inputfile = self.openinputfile(options, fullinputpath)
        inputpofile = po.pofile(inputfile)
        for pounit in inputpofile.units:
            if not (pounit.isheader() or pounit.hasplural()):  # XXX
                if pounit.hasmarkedcomment("poconflicts"):
                    for comment in pounit.othercomments:
                        if comment.find("# (poconflicts)") == 0:
                            pounit.othercomments.remove(comment)
                            break
                    # TODO: refactor writing out
                    outputpath = comment[comment.find(")") + 2:].strip()
                    self.checkoutputsubdir(options, os.path.dirname(outputpath))
                    fulloutputpath = os.path.join(options.output, outputpath)
                    if os.path.isfile(fulloutputpath):
                        outputfile = open(fulloutputpath, 'r')
                        outputpofile = po.pofile(outputfile)
                    else:
                        outputpofile = po.pofile()
                    outputpofile.units.append(pounit)   # TODO:perhaps check to see if it's already there...
                    outputfile = open(fulloutputpath, 'w')
                    outputfile.write(str(outputpofile))


def main():
    # outputfile extentions will actually be determined by the comments in the
    # po files
    pooutput = ("po", None)
    formats = {(None, None): pooutput, ("po", "po"): pooutput, "po": pooutput}
    parser = SplitOptionParser(formats, description=__doc__)
    parser.set_usage()
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = posegment
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Segment Gettext PO, XLIFF and TMX localization files at the sentence level.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/posegment.html
for examples and usage instructions.
"""

from translate.lang import factory as lang_factory
from translate.storage import factory


class segment:

    def __init__(self, sourcelang, targetlang, stripspaces=True, onlyaligned=False):
        self.sourcelang = sourcelang
        self.targetlang = targetlang
        self.stripspaces = stripspaces
        self.onlyaligned = onlyaligned

    def segmentunit(self, unit):
        if unit.isheader() or unit.hasplural():
            return [unit]
        sourcesegments = self.sourcelang.sentences(unit.source, strip=self.stripspaces)
        targetsegments = self.targetlang.sentences(unit.target, strip=self.stripspaces)
        if unit.istranslated() and (len(sourcesegments) != len(targetsegments)):
            if not self.onlyaligned:
                return [unit]
            else:
                return None
        # We could do more here to check if the lengths correspond more or less,
        # certain quality checks are passed, etc.  But for now this is a good
        # start.
        units = []
        for i in range(len(sourcesegments)):
            newunit = unit.copy()
            newunit.source = sourcesegments[i]
            if not unit.istranslated():
                newunit.target = ""
            else:
                newunit.target = targetsegments[i]
            units.append(newunit)
        return units

    def convertstore(self, fromstore):
        tostore = type(fromstore)()
        for unit in fromstore.units:
            newunits = self.segmentunit(unit)
            if newunits:
                for newunit in newunits:
                    tostore.addunit(newunit)
        return tostore


def segmentfile(inputfile, outputfile, templatefile, sourcelanguage="en", targetlanguage=None, stripspaces=True, onlyaligned=False):
    """reads in inputfile, segments it then, writes to outputfile"""
    # note that templatefile is not used, but it is required by the converter...
    inputstore = factory.getobject(inputfile)
    if inputstore.isempty():
        return 0
    sourcelang = lang_factory.getlanguage(sourcelanguage)
    targetlang = lang_factory.getlanguage(targetlanguage)
    convertor = segment(sourcelang, targetlang, stripspaces=stripspaces, onlyaligned=onlyaligned)
    outputstore = convertor.convertstore(inputstore)
    outputfile.write(str(outputstore))
    return 1


def main():
    from translate.convert import convert
    formats = {"po": ("po", segmentfile), "xlf": ("xlf", segmentfile), "tmx": ("tmx", segmentfile)}
    parser = convert.ConvertOptionParser(formats, usepots=True, description=__doc__)
    parser.add_option("-l", "--language", dest="targetlanguage", default=None,
            help="the target language code", metavar="LANG")
    parser.add_option("", "--source-language", dest="sourcelanguage", default=None,
            help="the source language code (default 'en')", metavar="LANG")
    parser.passthrough.append("sourcelanguage")
    parser.passthrough.append("targetlanguage")
    parser.add_option("", "--keepspaces", dest="stripspaces", action="store_false",
            default=True, help="Disable automatic stripping of whitespace")
    parser.passthrough.append("stripspaces")
    parser.add_option("", "--only-aligned", dest="onlyaligned", action="store_true",
            default=False, help="Removes units where sentence number does not correspond")
    parser.passthrough.append("onlyaligned")
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = poswap
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2007 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Builds a new translation file with the target of the input language as
source language.

.. note:: Ensure that the two po files correspond 100% to the same pot file before using
   this.

To translate Kurdish (ku) through French::

    poswap -i fr/ -t ku -o fr-ku

To convert the fr-ku files back to en-ku::

    poswap --reverse -i fr/ -t fr-ku -o en-ku

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/poswap.html
for examples and usage instructions.
"""

from translate.convert import convert
from translate.storage import po


def swapdir(store):
    """Swap the source and target of each unit."""
    for unit in store.units:
        if unit.isheader():
            continue
        if not unit.target or unit.isfuzzy():
            unit.target = unit.source
        else:
            unit.source, unit.target = unit.target, unit.source


def convertpo(inputpofile, outputpotfile, template, reverse=False):
    """reads in inputpofile, removes the header, writes to outputpotfile."""
    inputpo = po.pofile(inputpofile)
    templatepo = po.pofile(template)
    if reverse:
        swapdir(inputpo)
    templatepo.makeindex()
    header = inputpo.header()
    if header:
        inputpo.units = inputpo.units[1:]

    for i, unit in enumerate(inputpo.units):
        for location in unit.getlocations():
            templateunit = templatepo.locationindex.get(location, None)
            if templateunit and templateunit.source == unit.source:
                break
        else:
            templateunit = templatepo.findunit(unit.source)

        unit.othercomments = []
        if unit.target and not unit.isfuzzy():
            unit.source = unit.target
        elif not reverse:
            if inputpo.filename:
                unit.addnote("No translation found in %s" % inputpo.filename, origin="programmer")
            else:
                unit.addnote("No translation found in the supplied source language", origin="programmer")
        unit.target = ""
        unit.markfuzzy(False)
        if templateunit:
            unit.addnote(templateunit.getnotes(origin="translator"))
            unit.markfuzzy(templateunit.isfuzzy())
            unit.target = templateunit.target
        if unit.isobsolete():
            del inputpo.units[i]
    outputpotfile.write(str(inputpo))
    return 1


def main(argv=None):
    formats = {("po", "po"): ("po", convertpo), ("po", "pot"): ("po", convertpo), "po": ("po", convertpo)}
    parser = convert.ConvertOptionParser(formats, usetemplates=True, description=__doc__)
    parser.add_option("", "--reverse", dest="reverse", default=False, action="store_true",
                    help="reverse the process of intermediate language conversion")
    parser.passthrough.append("reverse")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = poterminology
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Create a terminology file by reading a set of .po or .pot files to produce a pootle-terminology.pot.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/poterminology.html
for examples and usage instructions.
"""
import logging
import os
import re
import sys

from translate.lang import factory as lang_factory
from translate.misc import file_discovery, optrecurse
from translate.storage import factory, po


logger = logging.getLogger(__name__)


def create_termunit(term, unit, targets, locations, sourcenotes, transnotes, filecounts):
    termunit = po.pounit(term)
    if unit is not None:
        termunit.merge(unit, overwrite=False, comments=False)
    if len(targets.keys()) > 1:
        txt = '; '.join(["%s {%s}" % (target, ', '.join(files))
                         for target, files in targets.iteritems()])
        if termunit.target.find('};') < 0:
            termunit.target = txt
            termunit.markfuzzy()
        else:
            # if annotated multiple terms already present, keep as-is
            termunit.addnote(txt, "translator")
    for location in locations:
        termunit.addlocation(location)
    for sourcenote in sourcenotes:
        termunit.addnote(sourcenote, "developer")
    for transnote in transnotes:
        termunit.addnote(transnote, "translator")
    for filename, count in filecounts.iteritems():
        termunit.addnote("(poterminology) %s (%d)\n" % (filename, count), 'translator')
    return termunit


class TerminologyExtractor(object):

    def __init__(self, foldtitle=True, ignorecase=False, accelchars="", termlength=3,
                 sourcelanguage="en", invert=False, stopfile=None):
        self.foldtitle = foldtitle
        self.ignorecase = ignorecase
        self.accelchars = accelchars
        self.termlength = termlength

        self.sourcelanguage = sourcelanguage
        self.invert = invert

        self.stopwords = {}
        self.stoprelist = []
        self.stopfoldtitle = True
        self.stopignorecase = False

        if stopfile is None:
            try:
                stopfile = file_discovery.get_abs_data_filename('stoplist-%s' % self.sourcelanguage)
            except:
                pass
        self.stopfile = stopfile
        self.parse_stopword_file()

        # handles c-format and python-format
        self.formatpat = re.compile(r"%(?:\([^)]+\)|[0-9]+\$)?[-+#0]*[0-9.*]*(?:[hlLzjt][hl])?[EFGXc-ginoprsux]")
        # handles XML/HTML elements (<foo>text</foo> => text)
        self.xmlelpat = re.compile(r"<(?:![[-]|[/?]?[A-Za-z_:])[^>]*>")
        # handles XML/HTML entities (&#32; &#x20; &amp; &my_entity;)
        self.xmlentpat = re.compile(r"&(?:#(?:[0-9]+|x[0-9a-f]+)|[a-z_:][\w.-:]*);",
                               flags=re.UNICODE | re.IGNORECASE)

        self.units = 0
        self.glossary = {}

    def parse_stopword_file(self):

        actions = {'+': frozenset(), ':': frozenset(['skip']),
                   '<': frozenset(['phrase']), '=': frozenset(['word']),
                   '>': frozenset(['word', 'skip']),
                   '@': frozenset(['word', 'phrase'])}

        stopfile = open(self.stopfile, "r")
        line = 0
        try:
            for stopline in stopfile:
                line += 1
                stoptype = stopline[0]
                if stoptype == '#' or stoptype == "\n":
                    continue
                elif stoptype == '!':
                    if stopline[1] == 'C':
                        self.stopfoldtitle = False
                        self.stopignorecase = False
                    elif stopline[1] == 'F':
                        self.stopfoldtitle = True
                        self.stopignorecase = False
                    elif stopline[1] == 'I':
                        self.stopignorecase = True
                    else:
                        logger.warning("%s:%d - bad case mapping directive",
                                       self.stopfile, line)
                elif stoptype == '/':
                    self.stoprelist.append(re.compile(stopline[1:-1] + '$'))
                else:
                    self.stopwords[stopline[1:-1]] = actions[stoptype]
        except KeyError as character:
            logger.warning("%s:%d - bad stopword entry starts with '%s'",
                           self.stopfile, line, str(character))
            logger.warning("%s:%d all lines after error ignored",
                           self.stopfile, line + 1)
        stopfile.close()

    def clean(self, string):
        """returns the cleaned string that contains the text to be matched"""
        for accelerator in self.accelchars:
            string = string.replace(accelerator, "")
        string = self.formatpat.sub(" ", string)
        string = self.xmlelpat.sub(" ", string)
        string = self.xmlentpat.sub(" ", string)
        string = string.strip()
        return string

    def stopmap(self, word):
        """return case-mapped stopword for input word"""
        if self.stopignorecase or (self.stopfoldtitle and word.istitle()):
            word = word.lower()
        return word

    def stopword(self, word, defaultset=frozenset()):
        """return stoplist frozenset for input word"""
        return self.stopwords.get(self.stopmap(word), defaultset)

    def addphrases(self, words, skips, translation, partials=True):
        """adds (sub)phrases with non-skipwords and more than one word"""
        if (len(words) > skips + 1 and
            'skip' not in self.stopword(words[0]) and
            'skip' not in self.stopword(words[-1])):
            self.glossary.setdefault(' '.join(words), []).append(translation)
        if partials:
            part = list(words)
            while len(part) > 2:
                if 'skip' in self.stopword(part.pop()):
                    skips -= 1
                if (len(part) > skips + 1 and
                    'skip' not in self.stopword(part[0]) and
                    'skip' not in self.stopword(part[-1])):
                    self.glossary.setdefault(' '.join(part), []).append(translation)

    def processunits(self, units, fullinputpath):
        sourcelang = lang_factory.getlanguage(self.sourcelanguage)
        rematchignore = frozenset(('word', 'phrase'))
        defaultignore = frozenset()
        for unit in units:
            self.units += 1
            if unit.isheader():
                continue
            if not self.invert:
                source = self.clean(unit.source)
                target = self.clean(unit.target)
            else:
                target = self.clean(unit.source)
                source = self.clean(unit.target)
            if len(source) <= 1:
                continue
            for sentence in sourcelang.sentences(source):
                words = []
                skips = 0
                for word in sourcelang.words(sentence):
                    stword = self.stopmap(word)
                    if self.ignorecase or (self.foldtitle and word.istitle()):
                        word = word.lower()
                    ignore = defaultignore
                    if stword in self.stopwords:
                        ignore = self.stopwords[stword]
                    else:
                        for stopre in self.stoprelist:
                            if stopre.match(stword) is not None:
                                ignore = rematchignore
                                break
                    translation = (source, target, unit, fullinputpath)
                    if 'word' not in ignore:
                        # reduce plurals
                        root = word
                        if len(word) > 3 and word[-1] == 's' and word[0:-1] in self.glossary:
                            root = word[0:-1]
                        elif len(root) > 2 and root + 's' in self.glossary:
                            self.glossary[root] = self.glossary.pop(root + 's')
                        self.glossary.setdefault(root, []).append(translation)
                    if self.termlength > 1:
                        if 'phrase' in ignore:
                            # add trailing phrases in previous words
                            while len(words) > 2:
                                if 'skip' in self.stopword(words.pop(0)):
                                    skips -= 1
                                self.addphrases(words, skips, translation)
                            words = []
                            skips = 0
                        else:
                            words.append(word)
                            if 'skip' in ignore:
                                skips += 1
                            if len(words) > self.termlength + skips:
                                while len(words) > self.termlength + skips:
                                    if 'skip' in self.stopword(words.pop(0)):
                                        skips -= 1
                                self.addphrases(words, skips, translation)
                            else:
                                self.addphrases(words, skips, translation, partials=False)
                if self.termlength > 1:
                    # add trailing phrases in sentence after reaching end
                    while self.termlength > 1 and len(words) > 2:

                        if 'skip' in self.stopword(words.pop(0)):
                            skips -= 1
                        self.addphrases(words, skips, translation)

    def extract_terms(self, create_termunit=create_termunit, inputmin=1, fullmsgmin=1, substrmin=2, locmin=2):
        terms = {}
        locre = re.compile(r":[0-9]+$")
        logger.info("%d terms from %d units", len(self.glossary), self.units)
        for term, translations in self.glossary.iteritems():
            if len(translations) <= 1:
                continue
            filecounts = {}
            sources = set()
            locations = set()
            sourcenotes = set()
            transnotes = set()
            targets = {}
            fullmsg = False
            bestunit = None
            for source, target, unit, filename in translations:
                sources.add(source)
                filecounts[filename] = filecounts.setdefault(filename, 0) + 1
                # FIXME: why reclean source and target?!
                if term.lower() == self.clean(unit.source).lower():
                    fullmsg = True
                    target = self.clean(unit.target)
                    if self.ignorecase or (self.foldtitle and target.istitle()):
                        target = target.lower()
                    unit.target = target
                    if target != "":
                        targets.setdefault(target, []).append(filename)
                    if term.lower() == unit.source.strip().lower():
                        sourcenotes.add(unit.getnotes("source code"))
                        transnotes.add(unit.getnotes("translator"))
                    unit.source = term
                    bestunit = unit
                # FIXME: figure out why we did a merge to begin with
                # termunit.merge(unit, overwrite=False, comments=False)
                for loc in unit.getlocations():
                    locations.add(locre.sub("", loc))

            numsources = len(sources)
            numfiles = len(filecounts)
            numlocs = len(locations)
            if numfiles < inputmin or 0 < numlocs < locmin:
                continue
            if fullmsg:
                if numsources < fullmsgmin:
                    continue
            elif numsources < substrmin:
                continue

            locmax = 2 * locmin
            if numlocs > locmax:
                locations = list(locations)[0:locmax]
                locations.append("(poterminology) %d more locations"
                                     % (numlocs - locmax))

            termunit = create_termunit(term, bestunit, targets, locations, sourcenotes, transnotes, filecounts)
            terms[term] = ((10 * numfiles) + numsources, termunit)
        return terms

    sortorders_default = ["frequency", "dictionary", "length"]

    def filter_terms(self, terms, nonstopmin=1, sortorders=sortorders_default):
        """reduce subphrases from extracted terms"""
        # reduce subphrase
        termlist = terms.keys()
        logger.info("%d terms after thresholding", len(termlist))
        termlist.sort(lambda x, y: cmp(len(x), len(y)))
        for term in termlist:
            words = term.split()
            nonstop = [word for word in words if not self.stopword(word)]
            if len(nonstop) < nonstopmin and len(nonstop) != len(words):
                del terms[term]
                continue
            if len(words) <= 2:
                continue
            while len(words) > 2:
                words.pop()
                if terms[term][0] == terms.get(' '.join(words), [0])[0]:
                    del terms[' '.join(words)]
            words = term.split()
            while len(words) > 2:
                words.pop(0)
                if terms[term][0] == terms.get(' '.join(words), [0])[0]:
                    del terms[' '.join(words)]
        logger.info("%d terms after subphrase reduction", len(terms.keys()))
        termitems = terms.values()
        if sortorders is None:
            sortorders = self.sortorders_default
        while len(sortorders) > 0:
            order = sortorders.pop()
            if order == "frequency":
                termitems.sort(lambda x, y: cmp(y[0], x[0]))
            elif order == "dictionary":
                termitems.sort(lambda x, y: cmp(x[1].source.lower(), y[1].source.lower()))
            elif order == "length":
                termitems.sort(lambda x, y: cmp(len(x[1].source), len(y[1].source)))
            else:
                logger.warning("unknown sort order %s", order)
        return termitems


class TerminologyOptionParser(optrecurse.RecursiveOptionParser):
    """a specialized Option Parser for the terminology tool..."""

    def parse_args(self, args=None, values=None):
        """parses the command line options, handling implicit input/output args"""
        (options, args) = optrecurse.optparse.OptionParser.parse_args(self, args, values)
        # some intelligence as to what reasonable people might give on the command line
        if args and not options.input:
            if not options.output and not options.update and len(args) > 1:
                options.input = args[:-1]
                args = args[-1:]
            else:
                options.input = args
                args = []
        # don't overwrite last freestanding argument file, to avoid accidents
        # due to shell wildcard expansion
        if args and not options.output and not options.update:
            if os.path.lexists(args[-1]) and not os.path.isdir(args[-1]):
                self.error("To overwrite %s, specify it with -o/--output or -u/--update" % (args[-1]))
            options.output = args[-1]
            args = args[:-1]
        if options.output and options.update:
            self.error("You cannot use both -u/--update and -o/--output")
        if args:
            self.error("You have used an invalid combination of -i/--input, -o/--output, -u/--update and freestanding args")
        if not options.input:
            self.error("No input file or directory was specified")
        if isinstance(options.input, list) and len(options.input) == 1:
            options.input = options.input[0]
            if options.inputmin is None:
                options.inputmin = 1
        elif not isinstance(options.input, list) and not os.path.isdir(options.input):
            if options.inputmin is None:
                options.inputmin = 1
        elif options.inputmin is None:
            options.inputmin = 2
        if options.update:
            options.output = options.update
            if isinstance(options.input, list):
                options.input.append(options.update)
            elif options.input:
                options.input = [options.input, options.update]
            else:
                options.input = options.update
        if not options.output:
            options.output = "pootle-terminology.pot"
        return (options, args)

    def set_usage(self, usage=None):
        """sets the usage string - if usage not given, uses getusagestring for each option"""
        if usage is None:
            self.usage = "%prog " + " ".join([self.getusagestring(option) for option in self.option_list]) + \
                    "\n  input directory is searched for PO files, terminology PO file is output file"
        else:
            super(TerminologyOptionParser, self).set_usage(usage)

    def run(self):
        """parses the arguments, and runs recursiveprocess with the resulting options"""
        self.files = 0
        (options, args) = self.parse_args()
        options.inputformats = self.inputformats
        options.outputoptions = self.outputoptions
        self.extractor = TerminologyExtractor(foldtitle=options.foldtitle, ignorecase=options.ignorecase,
                                              accelchars=options.accelchars, termlength=options.termlength,
                                              sourcelanguage=options.sourcelanguage,
                                              invert=options.invert, stopfile=options.stopfile)
        self.recursiveprocess(options)

    def recursiveprocess(self, options):
        """recurse through directories and process files"""
        if self.isrecursive(options.input, 'input') and getattr(options, "allowrecursiveinput", True):
            if isinstance(options.input, list):
                inputfiles = self.recurseinputfilelist(options)
            else:
                inputfiles = self.recurseinputfiles(options)
        else:
            if options.input:
                inputfiles = [os.path.basename(options.input)]
                options.input = os.path.dirname(options.input)
            else:
                inputfiles = [options.input]
        if os.path.isdir(options.output):
            options.output = os.path.join(options.output, "pootle-terminology.pot")

        self.initprogressbar(inputfiles, options)
        for inputpath in inputfiles:
            self.files += 1
            fullinputpath = self.getfullinputpath(options, inputpath)
            success = True
            try:
                self.processfile(None, options, fullinputpath)
            except Exception as error:
                if isinstance(error, KeyboardInterrupt):
                    raise
                self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
                success = False
            self.reportprogress(inputpath, success)
        del self.progressbar
        self.outputterminology(options)

    def processfile(self, fileprocessor, options, fullinputpath):
        """process an individual file"""
        inputfile = self.openinputfile(options, fullinputpath)
        inputfile = factory.getobject(inputfile)
        self.extractor.processunits(inputfile.units, fullinputpath)

    def outputterminology(self, options):
        """saves the generated terminology glossary"""
        termfile = po.pofile()
        logger.info("scanned %d files", self.files)
        terms = self.extractor.extract_terms(inputmin=options.inputmin, fullmsgmin=options.fullmsgmin,
                                   substrmin=options.substrmin, locmin=options.locmin)
        termitems = self.extractor.filter_terms(terms, nonstopmin=options.nonstopmin, sortorders=options.sortorders)
        for count, unit in termitems:
            termfile.units.append(unit)
        open(options.output, "w").write(str(termfile))


def fold_case_option(option, opt_str, value, parser):
    parser.values.ignorecase = False
    parser.values.foldtitle = True


def preserve_case_option(option, opt_str, value, parser):
    parser.values.ignorecase = parser.values.foldtitle = False


def main():
    formats = {"po": ("po", None), "pot": ("pot", None), None: ("po", None)}
    parser = TerminologyOptionParser(formats)

    parser.add_option("-u", "--update", type="string", dest="update",
        metavar="UPDATEFILE", help="update terminology in UPDATEFILE")

    parser.add_option("-S", "--stopword-list", type="string", metavar="STOPFILE", dest="stopfile",
                      help="read stopword (term exclusion) list from STOPFILE (default %s)" %
                      file_discovery.get_abs_data_filename('stoplist-en'))

    parser.set_defaults(foldtitle=True, ignorecase=False)
    parser.add_option("-F", "--fold-titlecase", callback=fold_case_option,
        action="callback", help="fold \"Title Case\" to lowercase (default)")
    parser.add_option("-C", "--preserve-case", callback=preserve_case_option,
        action="callback", help="preserve all uppercase/lowercase")
    parser.add_option("-I", "--ignore-case", dest="ignorecase",
        action="store_true", help="make all terms lowercase")

    parser.add_option("", "--accelerator", dest="accelchars", default="",
        metavar="ACCELERATORS", help="ignore the given accelerator characters when matching")

    parser.add_option("-t", "--term-words", type="int", dest="termlength", default="3",
        help="generate terms of up to LENGTH words (default 3)", metavar="LENGTH")
    parser.add_option("", "--nonstop-needed", type="int", dest="nonstopmin", default="1",
        help="omit terms with less than MIN nonstop words (default 1)", metavar="MIN")
    parser.add_option("", "--inputs-needed", type="int", dest="inputmin",
        help="omit terms appearing in less than MIN input files (default 2, or 1 if only one input file)", metavar="MIN")
    parser.add_option("", "--fullmsg-needed", type="int", dest="fullmsgmin", default="1",
        help="omit full message terms appearing in less than MIN different messages (default 1)", metavar="MIN")
    parser.add_option("", "--substr-needed", type="int", dest="substrmin", default="2",
        help="omit substring-only terms appearing in less than MIN different messages (default 2)", metavar="MIN")
    parser.add_option("", "--locs-needed", type="int", dest="locmin", default="2",
        help="omit terms appearing in less than MIN different original source files (default 2)", metavar="MIN")

    parser.add_option("", "--sort", dest="sortorders", action="append",
        type="choice", choices=TerminologyExtractor.sortorders_default, metavar="ORDER",
        help="output sort order(s): %s (may repeat option, default is all in above order)" % ', '.join(TerminologyExtractor.sortorders_default))

    parser.add_option("", "--source-language", dest="sourcelanguage", default="en",
        help="the source language code (default 'en')", metavar="LANG")
    parser.add_option("-v", "--invert", dest="invert",
        action="store_true", default=False, help="invert the source and target languages for terminology")
    parser.set_usage()
    parser.description = __doc__
    parser.run()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pretranslate
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008 Zuza Software Foundation
#
# This file is part of translate.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Fill localization files with suggested translations based on
translation memory and existing translations.

See: http://docs.translatehouse.org/projects/translate-toolkit/en/latest/commands/pretranslate.html
for examples and usage instructions.
"""

from translate.search import match
from translate.storage import factory, xliff


# We don't want to reinitialise the TM each time, so let's store it here.
tmmatcher = None


def memory(tmfiles, max_candidates=1, min_similarity=75, max_length=1000):
    """Returns the TM store to use. Only initialises on first call."""
    global tmmatcher
    # Only initialise first time
    if tmmatcher is None:
        if isinstance(tmfiles, list):
            tmstore = [factory.getobject(tmfile) for tmfile in tmfiles]
        else:
            tmstore = factory.getobject(tmfiles)
        tmmatcher = match.matcher(tmstore, max_candidates=max_candidates,
                                  min_similarity=min_similarity,
                                  max_length=max_length)
    return tmmatcher


def pretranslate_file(input_file, output_file, template_file, tm=None,
                      min_similarity=75, fuzzymatching=True):
    """Pretranslate any factory supported file with old translations and
    translation memory."""
    input_store = factory.getobject(input_file)
    template_store = None
    if template_file is not None:
        template_store = factory.getobject(template_file)

    output = pretranslate_store(input_store, template_store, tm,
                                min_similarity, fuzzymatching)
    output_file.write(str(output))
    return 1


def match_template_location(input_unit, template_store):
    """Returns a matching unit from a template. matching based on locations"""
    # we want to use slightly different matching strategies for PO files
    # generated by our own moz2po and oo2po. Let's take a cheap shot at
    # detecting them from the presence of a ':' in the first location.
    locations = input_unit.getlocations()
    if not locations or ":" in locations[0]:
        return match_template_id(input_unit, template_store)

    # since oo2po and moz2po use location as unique identifiers for strings
    # we match against location first, then check for matching source strings
    # this makes no sense for normal gettext files
    for location in locations:
        matching_unit = template_store.locationindex.get(location, None)
        if (matching_unit is not None and
            matching_unit.source == input_unit.source and
            matching_unit.gettargetlen() > 0):
            return matching_unit


def match_template_id(input_unit, template_store):
    """Returns a matching unit from a template. matching based on unit id"""
    matching_unit = template_store.findid(input_unit.getid())
    return matching_unit


def match_source(input_unit, template_store):
    """Returns a matching unit from a template. matching based on unit id"""
    # hack for weird mozilla single letter strings, we don't want to
    # match them by anything but locations
    if len(input_unit.source) > 1:
        matching_unit = template_store.findunit(input_unit.source)
        return matching_unit


def match_fuzzy(input_unit, matchers):
    """Return a fuzzy match from a queue of matchers."""
    for matcher in matchers:
        fuzzycandidates = matcher.matches(input_unit.source)
        if fuzzycandidates:
            return fuzzycandidates[0]


def pretranslate_unit(input_unit, template_store, matchers=None,
                      mark_reused=False, merge_on='id'):
    """Pretranslate a unit or return unchanged if no translation was found.

    :param input_unit: Unit that will be pretranslated.
    :param template_store: Fill input unit with units matching in this store.
    :param matchers: List of fuzzy :class:`~translate.search.match.matcher`
        objects.
    :param mark_reused: Whether to mark old translations as reused or not.
    :param merge_on: Where will the merge matching happen on.
    """
    matching_unit = None

    # Do template matching
    if template_store:
        # :param:`merge_on` supports `location` and `id` for now
        if merge_on == 'location':
            matching_unit = match_template_location(input_unit, template_store)
        else:
            matching_unit = match_template_id(input_unit, template_store)

    if matching_unit and matching_unit.gettargetlen() > 0:
        input_unit.merge(matching_unit, authoritative=True)
    elif matchers:
        # quickly try exact match by source
        matching_unit = match_source(input_unit, template_store)

        if not matching_unit or not matching_unit.gettargetlen():
            # do fuzzy matching
            matching_unit = match_fuzzy(input_unit, matchers)

        if matching_unit and matching_unit.gettargetlen() > 0:
            # FIXME: should we dispatch here instead of this crude type check
            if isinstance(input_unit, xliff.xliffunit):
                # FIXME: what about origin, lang and matchquality
                input_unit.addalttrans(matching_unit.target, origin="fish",
                                       sourcetxt=matching_unit.source)
            else:
                input_unit.merge(matching_unit, authoritative=True)

    # FIXME: ugly hack required by pot2po to mark old
    # translations reused for new file. loops over
    if mark_reused and matching_unit and template_store:
        original_unit = template_store.findunit(matching_unit.source)
        if original_unit is not None:
            original_unit.reused = True

    return input_unit


def pretranslate_store(input_store, template_store, tm=None,
                       min_similarity=75, fuzzymatching=True):
    """Do the actual pretranslation of a whole store."""
    # preperation
    matchers = []
    # prepare template
    if template_store is not None:
        template_store.makeindex()
        # template preparation based on type
        prepare_template = "prepare_template_%s" % template_store.__class__.__name__
        if prepare_template in globals():
            globals()[prepare_template](template_store)

        if fuzzymatching:
            # create template matcher
            # FIXME: max_length hardcoded
            matcher = match.matcher(template_store, max_candidates=1,
                                    min_similarity=min_similarity,
                                    max_length=3000, usefuzzy=True)
            matcher.addpercentage = False
            matchers.append(matcher)

    # prepare tm
    # create tm matcher
    if tm and fuzzymatching:
        # FIXME: max_length hardcoded
        matcher = memory(tm, max_candidates=1, min_similarity=min_similarity,
                         max_length=1000)
        matcher.addpercentage = False
        matchers.append(matcher)

    # Main loop
    for input_unit in input_store.units:
        if input_unit.istranslatable():
            input_unit = pretranslate_unit(input_unit, template_store,
                                           matchers,
                                           merge_on=input_store.merge_on)

    return input_store


def main(argv=None):
    from translate.convert import convert
    formats = {
        "pot": ("po", pretranslate_file),
                ("pot", "po"): ("po", pretranslate_file),
        "po": ("po", pretranslate_file),
               ("po", "po"): ("po", pretranslate_file),
        "xlf": ("xlf", pretranslate_file),
                ("xlf", "xlf"): ("xlf", pretranslate_file),
    }
    parser = convert.ConvertOptionParser(formats, usetemplates=True,
                                         allowmissingtemplate=True,
                                         description=__doc__)
    parser.add_option("", "--tm", dest="tm", default=None,
                      help="The file to use as translation memory when fuzzy matching")
    parser.passthrough.append("tm")
    defaultsimilarity = 75
    parser.add_option("-s", "--similarity", dest="min_similarity",
                      default=defaultsimilarity, type="float",
                      help="The minimum similarity for inclusion (default: %d%%)" % defaultsimilarity)
    parser.passthrough.append("min_similarity")
    parser.add_option("--nofuzzymatching", dest="fuzzymatching",
                      action="store_false", default=True,
                      help="Disable fuzzy matching")
    parser.passthrough.append("fuzzymatching")
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = pydiff
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2005, 2006 Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""diff tool like GNU diff, but lets you have special options
that are useful in dealing with PO files"""

import difflib
import fnmatch
import os
import sys
import time
from argparse import ArgumentParser


lineterm = "\n"


def main():
    """main program for pydiff"""
    parser = ArgumentParser()
    # GNU diff like options
    parser.add_argument("-i", "--ignore-case", default=False, action="store_true",
                        help='Ignore case differences in file contents.')
    parser.add_argument("-U", "--unified", type=int, metavar="NUM", default=3,
                        dest="unified_lines",
                        help='Output NUM (default 3) lines of unified context')
    parser.add_argument("-r", "--recursive", default=False, action="store_true",
                        help='Recursively compare any subdirectories found.')
    parser.add_argument("-N", "--new-file", default=False, action="store_true",
                        help='Treat absent files as empty.')
    parser.add_argument("--unidirectional-new-file", default=False,
                        action="store_true",
                        help='Treat absent first files as empty.')
    parser.add_argument("-s", "--report-identical-files", default=False,
                        action="store_true",
                        help='Report when two files are the same.')
    parser.add_argument("-x", "--exclude", default=["CVS", "*.po~"],
                        action="append", metavar="PAT",
                        help='Exclude files that match PAT.')
    # our own options
    parser.add_argument("--fromcontains", type=str, default=None,
                        metavar="TEXT",
                        help='Only show changes where fromfile contains TEXT')
    parser.add_argument("--tocontains", type=str, default=None,
                        metavar="TEXT",
                        help='Only show changes where tofile contains TEXT')
    parser.add_argument("--contains", type=str, default=None,
                        metavar="TEXT",
                        help='Only show changes where fromfile or tofile contains TEXT')
    parser.add_argument("-I", "--ignore-case-contains", default=False, action="store_true",
                        help='Ignore case differences when matching any of the changes')
    parser.add_argument("--accelerator", dest="accelchars", default="",
                        metavar="ACCELERATORS",
                        help="ignores the given accelerator characters when matching")
    parser.add_argument("fromfile", nargs=1)
    parser.add_argument("tofile", nargs=1)
    args = parser.parse_args()

    fromfile, tofile = args.fromfile[0], args.tofile[0]
    if fromfile == "-" and tofile == "-":
        parser.error("Only one of fromfile and tofile can be read from stdin")

    if os.path.isdir(fromfile):
        if os.path.isdir(tofile):
            differ = DirDiffer(fromfile, tofile, args)
        else:
            parser.error("File %s is a directory while file %s is a regular file" %
                         (fromfile, tofile))
    else:
        if os.path.isdir(tofile):
            parser.error("File %s is a regular file while file %s is a directory" %
                         (fromfile, tofile))
        else:
            differ = FileDiffer(fromfile, tofile, args)
    differ.writediff(sys.stdout)


class DirDiffer:
    """generates diffs between directories"""

    def __init__(self, fromdir, todir, options):
        """Constructs a comparison between the two dirs using the
        given options"""
        self.fromdir = fromdir
        self.todir = todir
        self.options = options

    def isexcluded(self, difffile):
        """checks if the given filename has been excluded from the diff"""
        for exclude_pat in self.options.exclude:
            if fnmatch.fnmatch(difffile, exclude_pat):
                return True
        return False

    def writediff(self, outfile):
        """writes the actual diff to the given file"""
        fromfiles = os.listdir(self.fromdir)
        tofiles = os.listdir(self.todir)
        difffiles = dict.fromkeys(fromfiles + tofiles).keys()
        difffiles.sort()
        for difffile in difffiles:
            if self.isexcluded(difffile):
                continue
            from_ok = (difffile in fromfiles or self.options.new_file or
                       self.options.unidirectional_new_file)
            to_ok = (difffile in tofiles or self.options.new_file)
            if from_ok and to_ok:
                fromfile = os.path.join(self.fromdir, difffile)
                tofile = os.path.join(self.todir, difffile)
                if os.path.isdir(fromfile):
                    if os.path.isdir(tofile):
                        if self.options.recursive:
                            differ = DirDiffer(fromfile, tofile, self.options)
                            differ.writediff(outfile)
                        else:
                            outfile.write("Common subdirectories: %s and %s\n" %
                                          (fromfile, tofile))
                    else:
                        outfile.write("File %s is a directory while file %s is a regular file\n" %
                                      (fromfile, tofile))
                else:
                    if os.path.isdir(tofile):
                        parser.error("File %s is a regular file while file %s is a directory\n" %
                                     (fromfile, tofile))
                    else:
                        filediffer = FileDiffer(fromfile, tofile, self.options)
                        filediffer.writediff(outfile)
            elif from_ok:
                outfile.write("Only in %s: %s\n" % (self.fromdir, difffile))
            elif to_ok:
                outfile.write("Only in %s: %s\n" % (self.todir, difffile))


class FileDiffer:
    """generates diffs between files"""

    def __init__(self, fromfile, tofile, options):
        """Constructs a comparison between the two files using the given
        options"""
        self.fromfile = fromfile
        self.tofile = tofile
        self.options = options

    def writediff(self, outfile):
        """writes the actual diff to the given file"""
        validfiles = True
        if os.path.exists(self.fromfile):
            self.from_lines = open(self.fromfile, 'U').readlines()
            fromfiledate = os.stat(self.fromfile).st_mtime
        elif self.fromfile == "-":
            self.from_lines = sys.stdin.readlines()
            fromfiledate = time.time()
        elif self.options.new_file or self.options.unidirectional_new_file:
            self.from_lines = []
            fromfiledate = 0
        else:
            outfile.write("%s: No such file or directory\n" % self.fromfile)
            validfiles = False
        if os.path.exists(self.tofile):
            self.to_lines = open(self.tofile, 'U').readlines()
            tofiledate = os.stat(self.tofile).st_mtime
        elif self.tofile == "-":
            self.to_lines = sys.stdin.readlines()
            tofiledate = time.time()
        elif self.options.new_file:
            self.to_lines = []
            tofiledate = 0
        else:
            outfile.write("%s: No such file or directory\n" % self.tofile)
            validfiles = False
        if not validfiles:
            return
        fromfiledate = time.ctime(fromfiledate)
        tofiledate = time.ctime(tofiledate)
        compare_from_lines = self.from_lines
        compare_to_lines = self.to_lines
        if self.options.ignore_case:
            compare_from_lines = [line.lower() for line in compare_from_lines]
            compare_to_lines = [line.lower() for line in compare_to_lines]
        matcher = difflib.SequenceMatcher(None, compare_from_lines, compare_to_lines)
        groups = matcher.get_grouped_opcodes(self.options.unified_lines)
        started = False
        fromstring = '--- %s\t%s%s' % (self.fromfile, fromfiledate, lineterm)
        tostring = '+++ %s\t%s%s' % (self.tofile, tofiledate, lineterm)

        for group in groups:
            hunk = "".join([line for line in self.unified_diff(group)])
            if self.options.fromcontains:
                if self.options.ignore_case_contains:
                    hunk_from_lines = "".join([line.lower() for line in self.get_from_lines(group)])
                else:
                    hunk_from_lines = "".join(self.get_from_lines(group))
                for accelerator in self.options.accelchars:
                    hunk_from_lines = hunk_from_lines.replace(accelerator, "")
                if self.options.fromcontains not in hunk_from_lines:
                    continue
            if self.options.tocontains:
                if self.options.ignore_case_contains:
                    hunk_to_lines = "".join([line.lower() for line in self.get_to_lines(group)])
                else:
                    hunk_to_lines = "".join(self.get_to_lines(group))
                for accelerator in self.options.accelchars:
                    hunk_to_lines = hunk_to_lines.replace(accelerator, "")
                if self.options.tocontains not in hunk_to_lines:
                    continue
            if self.options.contains:
                if self.options.ignore_case_contains:
                    hunk_lines = "".join([line.lower() for line in self.get_from_lines(group) + self.get_to_lines(group)])
                else:
                    hunk_lines = "".join(self.get_from_lines(group) + self.get_to_lines(group))
                for accelerator in self.options.accelchars:
                    hunk_lines = hunk_lines.replace(accelerator, "")
                if self.options.contains not in hunk_lines:
                    continue
            if not started:
                outfile.write(fromstring)
                outfile.write(tostring)
                started = True
            outfile.write(hunk)
        if not started and self.options.report_identical_files:
            outfile.write("Files %s and %s are identical\n" %
                          (self.fromfile, self.tofile))

    def get_from_lines(self, group):
        """returns the lines referred to by group, from the fromfile"""
        from_lines = []
        for tag, i1, i2, j1, j2 in group:
            from_lines.extend(self.from_lines[i1:i2])
        return from_lines

    def get_to_lines(self, group):
        """returns the lines referred to by group, from the tofile"""
        to_lines = []
        for tag, i1, i2, j1, j2 in group:
            to_lines.extend(self.to_lines[j1:j2])
        return to_lines

    def unified_diff(self, group):
        """takes the group of opcodes and generates a unified diff line
        by line"""
        i1, i2, j1, j2 = group[0][1], group[-1][2], group[0][3], group[-1][4]
        yield "@@ -%d,%d +%d,%d @@%s" % (i1 + 1, i2 - i1, j1 + 1, j2 - j1, lineterm)
        for tag, i1, i2, j1, j2 in group:
            if tag == 'equal':
                for line in self.from_lines[i1:i2]:
                    yield ' ' + line
                continue
            if tag == 'replace' or tag == 'delete':
                for line in self.from_lines[i1:i2]:
                    yield '-' + line
            if tag == 'replace' or tag == 'insert':
                for line in self.to_lines[j1:j2]:
                    yield '+' + line


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = pypo2phppo
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2009 Mozilla Corporation, Zuza Software Foundation
#
# This file is part of translate.
#
# translate is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# translate is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""Convert Python format .po files to PHP format .po files.
"""

import re

from translate.misc.multistring import multistring
from translate.storage import po


class pypo2phppo:

    def convertstore(self, inputstore):
        """Converts a given .po file (Python Format) to a PHP format .po file.

       The difference being how variable substitutions work.  PHP uses a %1$s
       format, and Python uses a {0} format (zero indexed).  This method will
       convert::

                I have {1} apples and {0} oranges

       To::

                I have %2$s apples and %1$s oranges

        This method ignores strings with %s as both languages will recognize
        that.
        """
        thetargetfile = po.pofile(inputfile="")

        for unit in inputstore.units:
            newunit = self.convertunit(unit)
            thetargetfile.addunit(newunit)
        return thetargetfile

    def convertunit(self, unit):
        developer_notes = unit.getnotes(origin="developer")
        translator_notes = unit.getnotes(origin="translator")
        unit.removenotes()
        unit.addnote(self.convertstrings(developer_notes))
        unit.addnote(self.convertstrings(translator_notes))
        unit.source = self.convertstrings(unit.source)
        unit.target = self.convertstrings(unit.target)
        return unit

    def convertstring(self, string):
        return re.sub('\{(\d)\}',
                      lambda x: "%%%d$s" % (int(x.group(1)) + 1), string)

    def convertstrings(self, input):
        if isinstance(input, multistring):
            strings = input.strings
        elif isinstance(input, list):
            strings = input
        else:
            return self.convertstring(input)

        for index, string in enumerate(strings):
            strings[index] = re.sub('\{(\d)\}',
                                    lambda x: "%%%d$s" % (int(x.group(1)) + 1),
                                                          string)
        return strings


def convertpy2php(inputfile, outputfile, template=None):
    """Converts from Python .po to PHP .po

    :param inputfile: file handle of the source
    :param outputfile: file handle to write to
    :param template: unused
    """
    convertor = pypo2phppo()
    inputstore = po.pofile(inputfile)
    outputstore = convertor.convertstore(inputstore)
    if outputstore.isempty():
        return False
    outputfile.write(str(outputstore))
    return True


def main(argv=None):
    """Converts from Python .po to PHP .po"""
    from translate.convert import convert

    formats = {"po": ("po", convertpy2php)}
    parser = convert.ConvertOptionParser(formats, description=__doc__)
    parser.run(argv)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_phppo2pypo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# phppo2pypo unit tests
# Author: Wil Clouser <wclouser@mozilla.com>
# Date: 2009-12-03

from translate.convert import test_convert
from translate.misc import wStringIO
from translate.tools import phppo2pypo


class TestPhpPo2PyPo:

    def test_single_po(self):
        inputfile = """
# This user comment refers to: %1$s
#. This developer comment does too: %1$s
#: some/path.php:111
#, php-format
msgid "I have %2$s apples and %1$s oranges"
msgstr "I have %2$s apples and %1$s oranges"
        """
        outputfile = wStringIO.StringIO()
        phppo2pypo.convertphp2py(inputfile, outputfile)

        output = outputfile.getvalue()

        assert "refers to: {0}" in output
        assert "does too: {0}" in output
        assert 'msgid "I have {1} apples and {0} oranges"' in output
        assert 'msgstr "I have {1} apples and {0} oranges"' in output

    def test_plural_po(self):
        inputfile = """
#. This developer comment refers to %1$s
#: some/path.php:111
#, php-format
msgid "I have %1$s apple"
msgid_plural "I have %1$s apples"
msgstr[0] "I have %1$s apple"
msgstr[1] "I have %1$s apples"
        """
        outputfile = wStringIO.StringIO()
        phppo2pypo.convertphp2py(inputfile, outputfile)
        output = outputfile.getvalue()

        assert 'msgid "I have {0} apple"' in output
        assert 'msgid_plural "I have {0} apples"' in output
        assert 'msgstr[0] "I have {0} apple"' in output
        assert 'msgstr[1] "I have {0} apples"' in output


class TestPhpPo2PyPoCommand(test_convert.TestConvertCommand, TestPhpPo2PyPo):
    """Tests running actual phppo2pypo commands on files"""
    convertmodule = phppo2pypo
    defaultoptions = {}

########NEW FILE########
__FILENAME__ = test_pocount
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from cStringIO import StringIO

from pytest import mark

from translate.storage import po, statsdb
from translate.tools import pocount


class TestCount:

    def count(self, source, expectedsource, target=None, expectedtarget=None):
        """simple helper to check the respective word counts"""
        poelement = po.pounit(source)
        if target is not None:
            poelement.target = target
        wordssource, wordstarget = statsdb.wordsinunit(poelement)
        print('Source (expected=%d; actual=%d): "%s"' % (expectedsource, wordssource, source))
        assert wordssource == expectedsource
        if target is not None:
            print('Target (expected=%d; actual=%d): "%s"' % (expectedtarget, wordstarget, target))
            assert wordstarget == expectedtarget

    def test_simple_count_zero(self):
        """no content"""
        self.count("", 0)

    def test_simple_count_one(self):
        """simplest one word count"""
        self.count("One", 1)

    def test_simple_count_two(self):
        """simplest one word count"""
        self.count("One two", 2)

    def test_punctuation_divides_words(self):
        """test that we break words when there is punctuation"""
        self.count("One. Two", 2)
        self.count("One.Two", 2)

    def test_xml_tags(self):
        """test that we do not count XML tags as words"""
        # <br> is a word break
        self.count("A word<br>Another word", 4)
        self.count("A word<br/>Another word", 4)
        self.count("A word<br />Another word", 4)
        # \n is a word break
        self.count("<p>A word</p>\n<p>Another word</p>", 4)
        # Not really an XML tag
        self.count("<no label>", 2)

    def test_newlines(self):
        """test to see that newlines divide words"""
        # newlines break words
        self.count("A word.\nAnother word", 4)
        self.count(r"A word.\\n\nAnother word", 4)

    def test_variables_are_words(self):
        """test that we count variables as words"""
        self.count("%PROGRAMNAME %PROGRAM% %s $file $1", 5)

    def test_plurals(self):
        """test that we can handle plural PO elements"""
        # #: gdk-pixbuf/gdk-pixdata.c:430
        # #, c-format
        # msgid "failed to allocate image buffer of %u byte"
        # msgid_plural "failed to allocate image buffer of %u bytes"
        # msgstr[0] "e paletwe go hweta seireleti sa seswantho sa paete ya %u"
        # msgstr[1] "e paletwe go hweta seireleti sa seswantho sa dipaete ta %u"

    @mark.xfail(reason="Support commented out pending removal")
    def test_plurals_kde(self):
        """test that we correcly count old style KDE plurals"""
        self.count("_n: Singular\\n\nPlural", 2, "Een\\n\ntwee\\n\ndrie", 3)

    def test_msgid_blank(self):
        """counts a message id"""
        self.count("   ", 0)

    # Counting strings
    #  We need to check how we count strings also and if we call it translated or untranslated
    # ie an all spaces msgid should be translated if there are spaces in the msgstr

    # Make sure we don't count obsolete messages

    # Do we correctly identify a translated yet blank message?

    # Need to test that we can differentiate between fuzzy, translated and untranslated


class TestPOCount:
    """This only tests the old (memory-based) pocount method, not the current
    code based on statsdb."""

    inputdata = r'''
msgid "translated unit"
msgstr "translated unit"

#, fuzzy
msgid "fuzzy unit"
msgstr "fuzzy unit"

# untranslated
msgid "untranslated unit"
msgstr ""

# obsolete
#~ msgid "obsolete translated unit"
#~ msgstr "obsolete translated unit"

#, fuzzy
#~ msgid "obsolete fuzzy unit"
#~ msgstr "obsolete fuzzy unit"

# untranslated
#~ msgid "obsolete untranslated unit"
#~ msgstr ""
'''

    def test_translated(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['translated'] == 1

    def test_fuzzy(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['fuzzy'] == 1

    def test_untranslated(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['untranslated'] == 1

    def test_total(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['total'] == 3

    def test_translatedsourcewords(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['translatedsourcewords'] == 2

    def test_fuzzysourcewords(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['fuzzysourcewords'] == 2

    def test_untranslatedsourcewords(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['untranslatedsourcewords'] == 2

    def test_totalsourcewords(self):
        pofile = StringIO(self.inputdata)
        stats = pocount.calcstats_old(pofile)
        assert stats['totalsourcewords'] == 6

########NEW FILE########
__FILENAME__ = test_podebug
# -*- coding: utf-8 -*-

from translate.storage import base, po, xliff
from translate.tools import podebug


PO_DOC = """
msgid "This is a %s test, hooray."
msgstr ""
"""

XLIFF_DOC = """<?xml version='1.0' encoding='utf-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.1" version="1.1">
  <file original="NoName" source-language="en" datatype="plaintext">
    <body>
      <trans-unit id="office:document-content[0]/office:body[0]/office:text[0]/text:p[0]">
        <source>This <g id="0">is a</g> test <x id="1" xid="office:document-content[0]/office:body[0]/office:text[0]/text:p[0]/text:note[0]"/>, hooray.</source>
      </trans-unit>
    </body>
  </file>
</xliff>
"""


class TestPODebug:
    debug = podebug.podebug()

    def setup_method(self, method):
        self.postore = po.pofile(PO_DOC)
        self.xliffstore = xliff.xlifffile(XLIFF_DOC)

    def test_ignore_gtk(self):
        """Test operation of GTK message ignoring"""
        unit = base.TranslationUnit("default:LTR")
        assert self.debug.ignore_gtk(unit)

    def test_keep_target(self):
        """Test that we use the target for rewriting if it exists."""
        unit = base.TranslationUnit(u"blie")

        unit.target = u"bla"
        debugger = podebug.podebug(rewritestyle="xxx")
        unit = debugger.convertunit(unit, "")
        assert unit.target == u"xxxblaxxx"

        unit.target = u"d%d"
        debugger = podebug.podebug(rewritestyle="flipped")
        unit = debugger.convertunit(unit, "")
        assert unit.target == u"\u202ep%d"

    def test_rewrite_blank(self):
        """Test the blank rewrite function"""
        assert str(self.debug.rewrite_blank(u"Test")) == u""

    def test_rewrite_en(self):
        """Test the en rewrite function"""
        assert str(self.debug.rewrite_en(u"Test")) == u"Test"

    def test_rewrite_xxx(self):
        """Test the xxx rewrite function"""
        assert str(self.debug.rewrite_xxx(u"Test")) == u"xxxTestxxx"
        assert str(self.debug.rewrite_xxx(u"Newline\n")) == u"xxxNewlinexxx\n"

    def test_rewrite_bracket(self):
        """Test the bracket rewrite function"""
        assert str(self.debug.rewrite_bracket(u"Test")) == u"[Test]"
        assert str(self.debug.rewrite_bracket(u"Newline\n")) == u"[Newline]\n"

    def test_rewrite_unicode(self):
        """Test the unicode rewrite function"""
        assert unicode(self.debug.rewrite_unicode(u"Test")) == u""

    def test_rewrite_flipped(self):
        """Test the unicode rewrite function"""
        assert unicode(self.debug.rewrite_flipped(u"Test")) == u"\u202es"
        # alternative with reversed string and no RTL override:
        #assert unicode(self.debug.rewrite_flipped("Test")) == u"s"
        # Chars < ! and > z are returned as is
        assert unicode(self.debug.rewrite_flipped(u" ")) == u"\u202e "
        assert unicode(self.debug.rewrite_flipped(u"")) == u"\u202e"

    def test_rewrite_chef(self):
        """Test the chef rewrite function

        This is not realy critical to test but a simple tests ensures
        that it stays working.
        """
        assert str(self.debug.rewrite_chef(u"Mock Swedish test you muppet")) == u"Mock Swedish test yooo mooppet"

    def test_po_variables(self):
        debug = podebug.podebug(rewritestyle='unicode')
        po_out = debug.convertstore(self.postore)

        in_unit = self.postore.units[0]
        out_unit = po_out.units[0]

        assert in_unit.source == out_unit.source
        print(out_unit.target)
        print(str(po_out))
        rewrite_func = self.debug.rewrite_unicode
        assert out_unit.target == u"%s%%s%s" % (rewrite_func(u'This is a '), rewrite_func(u' test, hooray.'))

    def test_xliff_rewrite(self):
        debug = podebug.podebug(rewritestyle='xxx')
        xliff_out = debug.convertstore(self.xliffstore)

        in_unit = self.xliffstore.units[0]
        out_unit = xliff_out.units[0]

        assert in_unit.source == out_unit.source
        print(out_unit.target)
        print(str(xliff_out))
        assert out_unit.target == u'xxx%sxxx' % (in_unit.source)

    def test_hash(self):
        po_docs = ("""
msgid "Test msgid 1"
msgstr "Test msgstr 1"
""",
"""
msgctxt "test context"
msgid "Test msgid 2"
msgstr "Test msgstr 2"
""",
"""
# Test comment 3
msgctxt "test context 3"
msgid "Test msgid 3"
msgstr "Test msgstr 3"
""")
        debugs = (podebug.podebug(format="%h "),
                  podebug.podebug(format="%6h."),
                  podebug.podebug(format="zzz%7h.zzz"),
                  podebug.podebug(format="%f %F %b %B %d %s "),
                  podebug.podebug(format="%3f %4F %5b %6B %7d %8s "),
                  podebug.podebug(format="%cf %cF %cb %cB %cd %cs "),
                  podebug.podebug(format="%3cf %4cF %5cb %6cB %7cd %8cs "),)
        results = ["85a9 Test msgstr 1", "a15d Test msgstr 2", "6398 Test msgstr 3",
                   "85a917.Test msgstr 1", "a15d71.Test msgstr 2", "639898.Test msgstr 3",
                   "zzz85a9170.zzzTest msgstr 1", "zzza15d718.zzzTest msgstr 2", "zzz639898c.zzzTest msgstr 3",
                   "fullpath/to/fakefile fullpath/to/fakefile.po fakefile fakefile.po fullpath/to full-t-fake Test msgstr 1",
                   "fullpath/to/fakefile fullpath/to/fakefile.po fakefile fakefile.po fullpath/to full-t-fake Test msgstr 2",
                   "fullpath/to/fakefile fullpath/to/fakefile.po fakefile fakefile.po fullpath/to full-t-fake Test msgstr 3",
                   "ful full fakef fakefi fullpat full-t-f Test msgstr 1",
                   "ful full fakef fakefi fullpat full-t-f Test msgstr 2",
                   "ful full fakef fakefi fullpat full-t-f Test msgstr 3",
                   "fllpth/t/fkfl fllpth/t/fkfl.p fkfl fkfl.p fllpth/t fll-t-fk Test msgstr 1",
                   "fllpth/t/fkfl fllpth/t/fkfl.p fkfl fkfl.p fllpth/t fll-t-fk Test msgstr 2",
                   "fllpth/t/fkfl fllpth/t/fkfl.p fkfl fkfl.p fllpth/t fll-t-fk Test msgstr 3",
                   "fll fllp fkfl fkfl.p fllpth/ fll-t-fk Test msgstr 1",
                   "fll fllp fkfl fkfl.p fllpth/ fll-t-fk Test msgstr 2",
                   "fll fllp fkfl fkfl.p fllpth/ fll-t-fk Test msgstr 3"]

        for debug in debugs:
            for po_doc in po_docs:
                postore = po.pofile(po_doc)
                postore.filename = "fullpath/to/fakefile.po"
                po_out = debug.convertstore(postore)
                in_unit = postore.units[0]
                out_unit = po_out.units[0]
                assert in_unit.source == out_unit.source
                assert out_unit.target == results.pop(0)

########NEW FILE########
__FILENAME__ = test_pogrep
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from translate.misc import wStringIO
from translate.storage import po, xliff
from translate.storage.test_base import first_translatable, headerless_len
from translate.tools import pogrep


class TestPOGrep:

    def poparse(self, posource):
        """helper that parses po source without requiring files"""
        dummyfile = wStringIO.StringIO(posource)
        pofile = po.pofile(dummyfile)
        return pofile

    def pogrep(self, posource, searchstring, cmdlineoptions=None):
        """helper that parses po source and passes it through a filter"""
        if cmdlineoptions is None:
            cmdlineoptions = []
        options, args = pogrep.cmdlineparser().parse_args(["xxx.po"] + cmdlineoptions)
        grepfilter = pogrep.GrepFilter(searchstring, options.searchparts, options.ignorecase, options.useregexp, options.invertmatch, options.keeptranslations, options.accelchar)
        tofile = grepfilter.filterfile(self.poparse(posource))
        print(str(tofile))
        return str(tofile)

    def test_simplegrep_msgid(self):
        """grep for a string in the source"""
        posource = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "test", ["--search=msgid"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "rest", ["--search=msgid"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_simplegrep_msgstr(self):
        """grep for a string in the target"""
        posource = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "rest", ["--search=msgstr"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "test", ["--search=msgstr"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_simplegrep_locations(self):
        """grep for a string in the location comments"""
        posource = '#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "test.c", ["--search=locations"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "rest.c", ["--search=locations"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_simplegrep_comments(self):
        """grep for a string in the comments"""
        posource = '# (review) comment\n#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "review", ["--search=comment"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "test", ["--search=comment"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_simplegrep_locations_with_comment_enabled(self):
        """grep for a string in "locations", while also "comment" is checked
        see http://bugs.locamotion.org/show_bug.cgi?id=1036
        """
        posource = '# (review) comment\n#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "test", ["--search=comment", "--search=locations"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "rest", ["--search=comment", "--search=locations"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_unicode_message_searchstring(self):
        """check that we can grep unicode messages and use unicode search strings"""
        poascii = '# comment\n#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        pounicode = '# comment\n#: test.c\nmsgid "test"\nmsgstr "re"\n'
        queryascii = 'rest'
        queryunicode = 're'
        for source, search, expected in [(poascii, queryascii, poascii),
                                         (poascii, queryunicode, ''),
                                         (pounicode, queryascii, ''),
                                         (pounicode, queryunicode, pounicode)]:
            print("Source:\n%s\nSearch: %s\n" % (source, search))
            poresult = self.pogrep(source, search)
            assert poresult.index(expected) >= 0

    def test_unicode_message_regex_searchstring(self):
        """check that we can grep unicode messages and use unicode regex search strings"""
        poascii = '# comment\n#: test.c\nmsgid "test"\nmsgstr "rest"\n'
        pounicode = '# comment\n#: test.c\nmsgid "test"\nmsgstr "re"\n'
        queryascii = 'rest'
        queryunicode = 're'
        for source, search, expected in [(poascii, queryascii, poascii),
                                         (poascii, queryunicode, ''),
                                         (pounicode, queryascii, ''),
                                         (pounicode, queryunicode, pounicode)]:
            print("Source:\n%s\nSearch: %s\n" % (source, search))
            poresult = self.pogrep(source, search, ["--regexp"])
            assert poresult.index(expected) >= 0

    def test_keep_translations(self):
        """check that we can grep unicode messages and use unicode regex search strings"""
        posource = '#: schemas.in\nmsgid "test"\nmsgstr "rest"\n'
        poresult = self.pogrep(posource, "schemas.in", ["--invert-match", "--keep-translations", "--search=locations"])
        assert poresult.index(posource) >= 0
        poresult = self.pogrep(posource, "schemas.in", ["--invert-match", "--search=locations"])
        assert headerless_len(po.pofile(poresult).units) == 0

    def test_unicode_normalise(self):
        """check that we normlise unicode strings before comparing"""
        source_template = u'# comment\n#: test.c\nmsgid "test"\nmsgstr "t%sst"\n'
        # , e + '
        # , L + ^
        # 
        groups = [
            (u"\u00e9", u"\u0065\u0301"),
            (u"\u1e3c", u"\u004c\u032d"),
            (u"\u1e4e", u"\u004f\u0303\u0308", u"\u00d5\u0308")
        ]
        for letters in groups:
            for source_letter in letters:
                source = source_template % source_letter
                for search_letter in letters:
                    print(search_letter.encode('utf-8'))
                    poresult = self.pogrep(source, search_letter)
                    assert poresult.index(source.encode('utf-8')) >= 0


class TestXLiffGrep:
    xliff_skeleton = '''<?xml version="1.0" ?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
  <file original="filename.po" source-language="en-US" datatype="po">
    <body>
        %s
    </body>
  </file>
</xliff>'''

    xliff_text = xliff_skeleton % '''<trans-unit>
  <source>rd</source>
  <target>rooi</target>
</trans-unit>'''

    def xliff_parse(self, xliff_text):
        """helper that parses po source without requiring files"""
        dummyfile = wStringIO.StringIO(xliff_text)
        xliff_file = xliff.xlifffile(dummyfile)
        return xliff_file

    def xliff_grep(self, xliff_text, searchstring, cmdlineoptions=None):
        """helper that parses xliff text and passes it through a filter"""
        if cmdlineoptions is None:
            cmdlineoptions = []
        options, args = pogrep.cmdlineparser().parse_args(["xxx.xliff"] + cmdlineoptions)
        grepfilter = pogrep.GrepFilter(searchstring, options.searchparts, options.ignorecase, options.useregexp, options.invertmatch, options.accelchar)
        tofile = grepfilter.filterfile(self.xliff_parse(xliff_text))
        return str(tofile)

    def test_simplegrep(self):
        """grep for a simple string."""
        xliff_text = self.xliff_text
        xliff_file = self.xliff_parse(xliff_text)
        xliff_result = self.xliff_parse(self.xliff_grep(xliff_text, "rd"))
        assert first_translatable(xliff_result).getsource() == u"rd"
        assert first_translatable(xliff_result).gettarget() == u"rooi"

        xliff_result = self.xliff_parse(self.xliff_grep(xliff_text, "unavailable string"))
        assert xliff_result.isempty()

########NEW FILE########
__FILENAME__ = test_pomerge
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pytest
from pytest import mark

from translate.misc import wStringIO
from translate.storage import po, xliff
from translate.tools import pomerge


def test_str2bool():
    """test the str2bool function"""
    assert pomerge.str2bool("yes")
    assert pomerge.str2bool("true")
    assert pomerge.str2bool("1")
    assert not pomerge.str2bool("no")
    assert not pomerge.str2bool("false")
    assert not pomerge.str2bool("0")
    pytest.raises(ValueError, pomerge.str2bool, "2")


class TestPOMerge:
    xliffskeleton = '''<?xml version="1.0" ?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
  <file original="filename.po" source-language="en-US" datatype="po">
    <body>
        %s
    </body>
  </file>
</xliff>'''

    def mergestore(self, templatesource, inputsource, mergeblanks="yes",
                   mergefuzzy="yes",
                   mergecomments="yes"):
        """merges the sources of the given files and returns a new pofile
        object"""
        templatefile = wStringIO.StringIO(templatesource)
        inputfile = wStringIO.StringIO(inputsource)
        outputfile = wStringIO.StringIO()
        assert pomerge.mergestore(inputfile, outputfile, templatefile,
                                  mergeblanks=mergeblanks,
                                  mergefuzzy=mergefuzzy,
                                  mergecomments=mergecomments,)
        outputpostring = outputfile.getvalue()
        outputpofile = po.pofile(outputpostring)
        return outputpofile

    def mergexliff(self, templatesource, inputsource, mergeblanks="yes",
                   mergefuzzy="yes",
                   mergecomments="yes"):
        """merges the sources of the given files and returns a new xlifffile
        object"""
        templatefile = wStringIO.StringIO(templatesource)
        inputfile = wStringIO.StringIO(inputsource)
        outputfile = wStringIO.StringIO()
        assert pomerge.mergestore(inputfile, outputfile, templatefile,
                                  mergeblanks=mergeblanks,
                                  mergefuzzy=mergefuzzy,
                                  mergecomments=mergecomments)
        outputxliffstring = outputfile.getvalue()
        print("Generated XML:")
        print(outputxliffstring)
        outputxlifffile = xliff.xlifffile(outputxliffstring)
        return outputxlifffile

    def countunits(self, pofile):
        """returns the number of non-header items"""
        if pofile.units[0].isheader():
            return len(pofile.units) - 1
        else:
            return len(pofile.units)

    def singleunit(self, pofile):
        """checks that the pofile contains a single non-header unit, and
        returns it"""
        assert self.countunits(pofile) == 1
        return pofile.units[-1]

    def test_mergesore_bad_data(self):
        """Test that we catch bad options sent to mergestore"""
        templatefile = wStringIO.StringIO("")
        inputfile = wStringIO.StringIO("")
        outputfile = wStringIO.StringIO()
        pytest.raises(ValueError, pomerge.mergestore, inputfile, outputfile,
                    templatefile, mergeblanks="yay")
        pytest.raises(ValueError, pomerge.mergestore, inputfile, outputfile,
                    templatefile, mergecomments="yay")

    def test_simplemerge(self):
        """checks that a simple po entry merges OK"""
        templatepo = '''#: simple.test\nmsgid "Simple String"\nmsgstr ""\n'''
        inputpo = '''#: simple.test\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        pofile = self.mergestore(templatepo, inputpo)
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "Dimpled Ring"

    def test_simplemerge_no_locations(self):
        """checks that a simple po entry merges OK"""
        templatepo = '''msgid "Simple String"
msgstr ""'''
        inputpo = '''msgid "Simple String"
msgstr "Dimpled Ring"'''
        pofile = self.mergestore(templatepo, inputpo)
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "Dimpled Ring"

    def test_replacemerge(self):
        """checks that a simple po entry merges OK"""
        templatepo = '''#: simple.test\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        inputpo = '''#: simple.test\nmsgid "Simple String"\nmsgstr "Dimpled King"\n'''
        pofile = self.mergestore(templatepo, inputpo)
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "Dimpled King"

    def test_merging_blanks(self):
        """By default we will merge blanks, but we can also override that"""
        templatepo = '''#: simple.test\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        inputpo = '''#: simple.test\nmsgid "Simple String"\nmsgstr ""\n'''
        pofile = self.mergestore(templatepo, inputpo)
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == ""
        pofile = self.mergestore(templatepo, inputpo, mergeblanks="no")
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "Dimpled Ring"

    def test_merging_fuzzies(self):
        """By default we will merge fuzzies, but can can also override that."""
        templatepo = '''#: simple.test\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        inputpo = '''#: simple.test\n#, fuzzy\nmsgid "Simple String"\nmsgstr "changed fish"\n'''
        pofile = self.mergestore(templatepo, inputpo)
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "changed fish"
        assert pounit.isfuzzy()
        pofile = self.mergestore(templatepo, inputpo, mergefuzzy="no")
        pounit = self.singleunit(pofile)
        assert pounit.source == "Simple String"
        assert pounit.target == "Dimpled Ring"
        assert not pounit.isfuzzy()

    def test_merging_locations(self):
        """check that locations on separate lines are output in Gettext form
        of all on one line"""
        templatepo = '''#: location.c:1\n#: location.c:2\nmsgid "Simple String"\nmsgstr ""\n'''
        inputpo = '''#: location.c:1\n#: location.c:2\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
        expectedpo = '''#: location.c:1%slocation.c:2\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n''' % po.lsep
        pofile = self.mergestore(templatepo, inputpo)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_unit_missing_in_template_with_locations(self):
        """If the unit is missing in the template we should raise an error"""
        templatepo = '''#: location.c:1
msgid "Simple String"
msgstr ""'''
        inputpo = '''#: location.c:1
msgid "Simple String"
msgstr "Dimpled Ring"

#: location.c:1
msgid "Extra string"
msgstr "Perplexa ring"'''
        expectedpo = '''#: location.c:1
msgid "Simple String"
msgstr "Dimpled Ring"
'''
        pofile = self.mergestore(templatepo, inputpo)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_unit_missing_in_template_no_locations(self):
        """If the unit is missing in the template we should raise an error"""
        templatepo = '''msgid "Simple String"
msgstr ""'''
        inputpo = '''msgid "Simple String"
msgstr "Dimpled Ring"

msgid "Extra string"
msgstr "Perplexa ring"'''
        expectedpo = '''msgid "Simple String"
msgstr "Dimpled Ring"
'''
        pofile = self.mergestore(templatepo, inputpo)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_reflowed_source_comments(self):
        """ensure that we don't duplicate source comments (locations) if they
        have been reflowed"""
        templatepo = '''#: newMenu.label\n#: newMenu.accesskey\nmsgid "&New"\nmsgstr ""\n'''
        newpo = '''#: newMenu.label newMenu.accesskey\nmsgid "&New"\nmsgstr "&Nuwe"\n'''
        expectedpo = '''#: newMenu.label%snewMenu.accesskey\nmsgid "&New"\nmsgstr "&Nuwe"\n''' % po.lsep
        pofile = self.mergestore(templatepo, newpo)
        pounit = self.singleunit(pofile)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_comments_with_blank_lines(self):
        """ensure that we don't loose empty newlines in comments"""
        templatepo = '''# # ***** BEGIN LICENSE BLOCK *****
# Version: MPL 1.1/GPL 2.0/LGPL 2.1
#
# bla bla
msgid "bla"
msgstr "blabla"
'''
        newpo = templatepo
        expectedpo = templatepo
        pofile = self.mergestore(templatepo, newpo)
        pounit = self.singleunit(pofile)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_merge_dont_delete_unassociated_comments(self):
        """ensure that we do not delete comments in the PO file that are not
        assocaited with a message block"""
        templatepo = '''# Lonely comment\n\n# Translation comment\nmsgid "Bob"\nmsgstr "Toolmaker"\n'''
        mergepo = '''# Translation comment\nmsgid "Bob"\nmsgstr "Builder"\n'''
        expectedpo = '''# Lonely comment\n# Translation comment\nmsgid "Bob"\nmsgstr "Builder"\n'''
        pofile = self.mergestore(templatepo, mergepo)
#        pounit = self.singleunit(pofile)
        print(pofile)
        assert str(pofile) == expectedpo

    def test_preserve_format_trailing_newlines(self):
        """Test that we can merge messages correctly that end with a newline"""
        templatepo = '''msgid "Simple string\\n"\nmsgstr ""\n'''
        mergepo = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
        expectedpo = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        templatepo = '''msgid ""\n"Simple string\\n"\nmsgstr ""\n'''
        mergepo = '''msgid ""\n"Simple string\\n"\nmsgstr ""\n"Dimpled ring\\n"\n'''
        expectedpo = '''msgid ""\n"Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
        expectedpo2 = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo or str(pofile) == expectedpo2

    def test_preserve_format_minor_start_and_end_of_sentence_changes(self):
        """Test that we are not too fussy about large diffs for simple
        changes at the start or end of a sentence"""
        templatepo = '''msgid "Target type:"\nmsgstr "Doelsoort"\n\n'''
        mergepo = '''msgid "Target type:"\nmsgstr "Doelsoort:"\n'''
        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        templatepo = '''msgid "&Select"\nmsgstr "Kies"\n\n'''
        mergepo = '''msgid "&Select"\nmsgstr "&Kies"\n'''
        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        templatepo = '''msgid "en-us, en"\nmsgstr "en-us, en"\n'''
        mergepo = '''msgid "en-us, en"\nmsgstr "af-za, af, en-za, en-gb, en-us, en"\n'''
        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_preserve_format_last_entry_in_a_file(self):
        """The last entry in a PO file is usualy not followed by an empty
        line.  Test that we preserve this"""
        templatepo = '''msgid "First"\nmsgstr ""\n\nmsgid "Second"\nmsgstr ""\n'''
        mergepo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
        expectedpo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        templatepo = '''msgid "First"\nmsgstr ""\n\nmsgid "Second"\nmsgstr ""\n\n'''
        mergepo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
        expectedpo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    @mark.xfail(reason="Not Implemented")
    def test_escape_tabs(self):
        """Ensure that input tabs are escaped in the output, like
        gettext does."""

        # The strings below contains the tab character, not spaces.
        templatepo = '''msgid "First	Second"\nmsgstr ""\n\n'''
        mergepo = '''msgid "First	Second"\nmsgstr "Eerste	Tweede"\n'''
        expectedpo = r'''msgid "First\tSecond"
msgstr "Eerste\tTweede"
'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_preserve_comments_layout(self):
        """Ensure that when we merge with new '# (poconflict)' or other
        comments that we don't mess formating"""
        templatepo = '''#: filename\nmsgid "Desktop Background.bmp"\nmsgstr "Desktop Background.bmp"\n\n'''
        mergepo = '''# (pofilter) unchanged: please translate\n#: filename\nmsgid "Desktop Background.bmp"\nmsgstr "Desktop Background.bmp"\n'''
        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_merge_dos2unix(self):
        """Test that merging a comment line with dos newlines doesn't add a
        new line"""
        templatepo = '''# User comment\n# (pofilter) Translate Toolkit comment\n#. Automatic comment\n#: location_comment.c:110\nmsgid "File"\nmsgstr "File"\n\n'''
        mergepo = '''# User comment\r\n# (pofilter) Translate Toolkit comment\r\n#. Automatic comment\r\n#: location_comment.c:110\r\nmsgid "File"\r\nmsgstr "Ifayile"\r\n\r\n'''
        expectedpo = '''# User comment\n# (pofilter) Translate Toolkit comment\n#. Automatic comment\n#: location_comment.c:110\nmsgid "File"\nmsgstr "Ifayile"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        assert str(pofile) == expectedpo

        # Unassociated comment
        templatepo = '''# Lonely comment\n\n#: location_comment.c:110\nmsgid "Bob"\nmsgstr "Toolmaker"\n'''
        mergepo = '''# Lonely comment\r\n\r\n#: location_comment.c:110\r\nmsgid "Bob"\r\nmsgstr "Builder"\r\n\r\n'''
        expectedpo = '''# Lonely comment\n#: location_comment.c:110\nmsgid "Bob"\nmsgstr "Builder"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        assert str(pofile) == expectedpo

        # New comment
        templatepo = '''#: location_comment.c:110\nmsgid "File"\nmsgstr "File"\n\n'''
        mergepo = '''# User comment\r\n# (pofilter) Translate Toolkit comment\r\n#: location_comment.c:110\r\nmsgid "File"\r\nmsgstr "Ifayile"\r\n\r\n'''
        expectedpo = '''# User comment\n# (pofilter) Translate Toolkit comment\n#: location_comment.c:110\nmsgid "File"\nmsgstr "Ifayile"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        assert str(pofile) == expectedpo

    def test_xliff_into_xliff(self):
        templatexliff = self.xliffskeleton % '''<trans-unit>
        <source>red</source>
        <target></target>
</trans-unit>'''
        mergexliff = self.xliffskeleton % '''<trans-unit>
        <source>red</source>
        <target>rooi</target>
</trans-unit>'''
        xlifffile = self.mergexliff(templatexliff, mergexliff)
        assert len(xlifffile.units) == 1
        unit = xlifffile.units[0]
        assert unit.source == "red"
        assert unit.target == "rooi"

    def test_po_into_xliff(self):
        templatexliff = self.xliffskeleton % '''<trans-unit>
        <source>red</source>
        <target></target>
</trans-unit>'''
        mergepo = 'msgid "red"\nmsgstr "rooi"'
        xlifffile = self.mergexliff(templatexliff, mergepo)
        assert len(xlifffile.units) == 1
        unit = xlifffile.units[0]
        assert unit.source == "red"
        assert unit.target == "rooi"

    def test_xliff_into_po(self):
        templatepo = '# my comment\nmsgid "red"\nmsgstr ""'
        mergexliff = self.xliffskeleton % '''<trans-unit>
        <source>red</source>
        <target>rooi</target>
</trans-unit>'''
        expectedpo = '# my comment\nmsgid "red"\nmsgstr "rooi"\n'
        pofile = self.mergestore(templatepo, mergexliff)
        assert str(pofile) == expectedpo

    def test_merging_dont_merge_kde_comments_found_in_translation(self):
        """If we find a KDE comment in the translation (target) then do not
        merge it."""

        templatepo = '''msgid "_: KDE comment\\n"\n"File"\nmsgstr "File"\n\n'''
        mergepo = '''msgid "_: KDE comment\\n"\n"File"\nmsgstr "_: KDE comment\\n"\n"Ifayile"\n\n'''
        expectedpo = '''msgid ""\n"_: KDE comment\\n"\n"File"\nmsgstr "Ifayile"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        # Translated kde comment.
        mergepo = '''msgid "_: KDE comment\\n"\n"File"\nmsgstr "_: KDE kommentaar\\n"\n"Ifayile"\n\n'''
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

        # multiline KDE comment
        templatepo = '''msgid "_: KDE "\n"comment\\n"\n"File"\nmsgstr "File"\n\n'''
        mergepo = '''msgid "_: KDE "\n"comment\\n"\n"File"\nmsgstr "_: KDE "\n"comment\\n"\n"Ifayile"\n\n'''
        expectedpo = '''msgid ""\n"_: KDE comment\\n"\n"File"\nmsgstr "Ifayile"\n'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_merging_untranslated_with_kde_disambiguation(self):
        """test merging untranslated messages that are the same except for
        KDE disambiguation"""
        templatepo = r'''#: sendMsgTitle
#: sendMsgTitle.accesskey
msgid "_: sendMsgTitle sendMsgTitle.accesskey\n"
"Send Message"
msgstr ""

#: sendMessageCheckWindowTitle
#: sendMessageCheckWindowTitle.accesskey
msgid "_: sendMessageCheckWindowTitle sendMessageCheckWindowTitle.accesskey\n"
"Send Message"
msgstr ""
'''
        mergepo = r'''#: sendMsgTitle%ssendMsgTitle.accesskey
msgid ""
"_: sendMsgTitle sendMsgTitle.accesskey\n"
"Send Message"
msgstr "Stuur"

#: sendMessageCheckWindowTitle%ssendMessageCheckWindowTitle.accesskey
msgid ""
"_: sendMessageCheckWindowTitle sendMessageCheckWindowTitle.accesskey\n"
"Send Message"
msgstr "Stuur"
''' % (po.lsep, po.lsep)
        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_merging_header_entries(self):
        """Check that we do the right thing if we have header entries in the
        input PO."""

        templatepo = r'''#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=INTEGER; plural=EXPRESSION;\n"
"X-Generator: Translate Toolkit 0.10rc2\n"

#: simple.test
msgid "Simple String"
msgstr ""
'''
        mergepo = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: old@example.com\n"
"POT-Creation-Date: 2006-01-01 01:01+0100\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.9\n"

#: simple.test
msgid "Simple String"
msgstr "Dimpled Ring"
'''
        expectedpo = r'''msgid ""
msgstr ""
"Project-Id-Version: Pootle 0.10\n"
"Report-Msgid-Bugs-To: new@example.com\n"
"POT-Creation-Date: 2006-11-11 11:11+0000\n"
"PO-Revision-Date: 2006-09-09 09:09+0900\n"
"Last-Translator: Joe Translate <joe@example.com>\n"
"Language-Team: Pig Latin <piglatin@example.com>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Generator: Translate Toolkit 0.10rc2\n"

#: simple.test
msgid "Simple String"
msgstr "Dimpled Ring"
'''
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo

    def test_merging_different_locations(self):
        """Test when merging units that are unchanged except for changed
        locations that we don't go fuzzy (bug 1583)"""

        templatepo = r'''#, fuzzy
msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: sentinelheadline
msgctxt "sentinelheadline"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"

#: sentinelheadline1
msgctxt "sentinelheadline1"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"
'''
        mergepo = r'''msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: sentinelheadline
#: sentinelheadline1
msgctxt "sentinelheadline"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"

#: sentinelheadline1
msgctxt "sentinelheadline1"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"
'''
        expectedpo2 = r'''msgid ""
msgstr ""
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: sentinelheadline sentinelheadline1
msgctxt "sentinelheadline"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"

#: sentinelheadline1
msgctxt "sentinelheadline1"
msgid "DESTROY SENTINELS"
msgstr "ZERSTRE WACHPOSTEN"
'''

        expectedpo = mergepo
        pofile = self.mergestore(templatepo, mergepo)
        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
        assert str(pofile) == expectedpo or str(pofile) == expectedpo2

########NEW FILE########
__FILENAME__ = test_pretranslate
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import warnings

from pytest import mark

from translate.convert import test_convert
from translate.misc import wStringIO
from translate.storage import po, xliff
from translate.tools import pretranslate


class TestPretranslate:
    xliff_skeleton = '''<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.1" xmlns="urn:oasis:names:tc:xliff:document:1.1">
        <file original="doc.txt" source-language="en-US">
                <body>
                        %s
                </body>
        </file>
</xliff>'''

    def setup_method(self, method):
        warnings.resetwarnings()

    def teardown_method(self, method):
        warnings.resetwarnings()

    def pretranslatepo(self, input_source, template_source=None):
        """helper that converts strings to po source without requiring files"""
        input_file = wStringIO.StringIO(input_source)
        if template_source:
            template_file = wStringIO.StringIO(template_source)
        else:
            template_file = None
        output_file = wStringIO.StringIO()

        pretranslate.pretranslate_file(input_file, output_file, template_file)
        output_file.seek(0)
        return po.pofile(output_file.read())

    def pretranslatexliff(self, input_source, template_source=None):
        """helper that converts strings to po source without requiring files"""
        input_file = wStringIO.StringIO(input_source)
        if template_source:
            template_file = wStringIO.StringIO(template_source)
        else:
            template_file = None
        output_file = wStringIO.StringIO()

        pretranslate.pretranslate_file(input_file, output_file, template_file)
        output_file.seek(0)
        return xliff.xlifffile(output_file.read())

    def singleunit(self, pofile):
        """checks that the pofile contains a single non-header unit, and
        returns it"""
        if len(pofile.units) == 2 and pofile.units[0].isheader():
            print(pofile.units[1])
            return pofile.units[1]
        else:
            print(pofile.units[0])
            return pofile.units[0]

    def test_pretranslatepo_blank(self):
        """checks that the pretranslatepo function is working for a simple file
        initialisation"""
        input_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        newpo = self.pretranslatepo(input_source)
        assert str(self.singleunit(newpo)) == input_source

    def test_merging_simple(self):
        """checks that the pretranslatepo function is working for a simple
        merge"""
        input_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        template_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.pretranslatepo(input_source, template_source)
        assert str(self.singleunit(newpo)) == template_source

    def test_merging_messages_marked_fuzzy(self):
        """test that when we merge PO files with a fuzzy message that it
        remains fuzzy"""
        input_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        template_source = '''#: simple.label%ssimple.accesskey\n#, fuzzy\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.pretranslatepo(input_source, template_source)
        assert str(self.singleunit(newpo)) == template_source

    def test_merging_plurals_with_fuzzy_matching(self):
        """test that when we merge PO files with a fuzzy message that it
        remains fuzzy"""
        input_source = r'''#: file.cpp:2
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] ""
msgstr[1] ""
'''
        template_source = r'''#: file.cpp:3
#, fuzzy
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] "%d handleiding."
msgstr[1] "%d handleidings."
'''
        # The #: comment and msgid's are different between the pot and the po
        poexpected = r'''#: file.cpp:2
#, fuzzy
msgid "%d manual"
msgid_plural "%d manuals"
msgstr[0] "%d handleiding."
msgstr[1] "%d handleidings."
'''
        newpo = self.pretranslatepo(input_source, template_source)
        assert str(self.singleunit(newpo)) == poexpected

    @mark.xfail(reason="Not Implemented")
    def test_merging_msgid_change(self):
        """tests that if the msgid changes but the location stays the same that
        we merge"""
        input_source = '''#: simple.label\n#: simple.accesskey\nmsgid "Its &hard coding a newline.\\n"\nmsgstr ""\n'''
        template_source = '''#: simple.label\n#: simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
        poexpected = '''#: simple.label\n#: simple.accesskey\n#, fuzzy\nmsgid "Its &hard coding a newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        assert str(newpo) == poexpected

    def test_merging_location_change(self):
        """tests that if the location changes but the msgid stays the same that
        we merge"""
        input_source = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr ""\n''' % po.lsep
        template_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        poexpected = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        assert str(newpo) == poexpected

    def test_merging_location_and_whitespace_change(self):
        """test that even if the location changes that if the msgid only has
        whitespace changes we can still merge"""
        input_source = '''#: singlespace.label%ssinglespace.accesskey\nmsgid "&We have spaces"\nmsgstr ""\n''' % po.lsep
        template_source = '''#: doublespace.label%sdoublespace.accesskey\nmsgid "&We  have  spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
        poexpected = '''#: singlespace.label%ssinglespace.accesskey\n#, fuzzy\nmsgid "&We have spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        assert str(newpo) == poexpected

    @mark.xfail(reason="Not Implemented")
    def test_merging_accelerator_changes(self):
        """test that a change in the accelerator localtion still allows
        merging"""
        input_source = '''#: someline.c\nmsgid "A&bout"\nmsgstr ""\n'''
        template_source = '''#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        poexpected = '''#: someline.c\nmsgid "A&bout"\nmsgstr "&Info"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        assert str(newpo) == poexpected

    @mark.xfail(reason="Not Implemented")
    def test_lines_cut_differently(self):
        """Checks that the correct formatting is preserved when pot an po lines
        differ."""
        input_source = '''#: simple.label\nmsgid "Line split "\n"differently"\nmsgstr ""\n'''
        template_source = '''#: simple.label\nmsgid "Line"\n" split differently"\nmsgstr "Lyne verskillend gesny"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == template_source

    def test_merging_automatic_comments_dont_duplicate(self):
        """ensure that we can merge #. comments correctly"""
        input_source = '''#. Row 35\nmsgid "&About"\nmsgstr ""\n'''
        template_source = '''#. Row 35\nmsgid "&About"\nmsgstr "&Info"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == template_source

    def test_merging_automatic_comments_new_overides_old(self):
        """ensure that new #. comments override the old comments"""
        input_source = '''#. new comment\n#: someline.c\nmsgid "&About"\nmsgstr ""\n'''
        template_source = '''#. old comment\n#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        poexpected = '''#. new comment\n#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_merging_comments_with_blank_comment_lines(self):
        """test that when we merge a comment that has a blank line we keep the
        blank line"""
        input_source = '''#: someline.c\nmsgid "About"\nmsgstr ""\n'''
        template_source = '''# comment1\n#\n# comment2\n#: someline.c\nmsgid "About"\nmsgstr "Omtrent"\n'''
        poexpected = template_source
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_empty_commentlines(self):
        input_source = '''#: paneSecurity.title
msgid "Security"
msgstr ""
'''
        template_source = '''# - Contributor(s):
# -
# - Alternatively, the
# -
#: paneSecurity.title
msgid "Security"
msgstr "Sekuriteit"
'''
        poexpected = template_source
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        print("expected")
        print(poexpected)
        print("got:")
        print(str(newpounit))
        assert str(newpounit) == poexpected

    def test_merging_msgidcomments(self):
        """ensure that we can merge msgidcomments messages"""
        input_source = r'''#: window.width
msgid ""
"_: Do not translate this.\n"
"36em"
msgstr ""
'''
        template_source = r'''#: window.width
msgid ""
"_: Do not translate this.\n"
"36em"
msgstr "36em"
'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == template_source

    def test_merging_plurals(self):
        """ensure that we can merge plural messages"""
        input_source = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] ""\nmsgstr[1] ""\n'''
        template_source = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] "Een"\nmsgstr[1] "Twee"\nmsgstr[2] "Drie"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == template_source

    def test_merging_resurect_obsolete_messages(self):
        """check that we can reuse old obsolete messages if the message comes
        back"""
        input_source = '''#: resurect.c\nmsgid "&About"\nmsgstr ""\n'''
        template_source = '''#~ msgid "&About"\n#~ msgstr "&Omtrent"\n'''
        expected = '''#: resurect.c\nmsgid "&About"\nmsgstr "&Omtrent"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        assert str(newpo) == expected

    def test_merging_comments(self):
        """Test that we can merge comments correctly"""
        input_source = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr ""\n'''
        template_source = '''#. Don't do it!\n#: file.py:2\nmsgid "One"\nmsgstr "Een"\n'''
        poexpected = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr "Een"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        print(newpo)
        newpounit = self.singleunit(newpo)
        assert str(newpounit) == poexpected

    def test_merging_typecomments(self):
        """Test that we can merge with typecomments"""
        input_source = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr ""\n'''
        template_source = '''#: file.c:2\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
        poexpected = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        print(newpounit)
        assert str(newpounit) == poexpected

        input_source = '''#: file.c:1\n#, c-format\nmsgid "%d computers"\nmsgstr ""\n'''
        template_source = '''#: file.c:2\n#, c-format\nmsgid "%s computers "\nmsgstr "%s-rekenaars"\n'''
        poexpected = '''#: file.c:1\n#, fuzzy, c-format\nmsgid "%d computers"\nmsgstr "%s-rekenaars"\n'''
        newpo = self.pretranslatepo(input_source, template_source)
        newpounit = self.singleunit(newpo)
        assert newpounit.isfuzzy()
        assert newpounit.hastypecomment("c-format")

    def test_xliff_states(self):
        """Test correct maintenance of XLIFF states."""
        xlf_template = self.xliff_skeleton \
          % '''<trans-unit id="1" xml:space="preserve">
                   <source> File  1 </source>
               </trans-unit>'''
        xlf_old = self.xliff_skeleton \
          % '''<trans-unit id="1" xml:space="preserve" approved="yes">
                   <source> File  1 </source>
                   <target> Ler 1 </target>
               </trans-unit>'''

        template = xliff.xlifffile.parsestring(xlf_template)
        old = xliff.xlifffile.parsestring(xlf_old)
        new = self.pretranslatexliff(template, old)
        print(str(old))
        print('---')
        print(str(new))
        assert new.units[0].isapproved()
        # Layout might have changed, so we won't compare the serialised
        # versions


class TestPretranslateCommand(test_convert.TestConvertCommand, TestPretranslate):
    """Tests running actual pretranslate commands on files"""
    convertmodule = pretranslate

    def test_help(self):
        """tests getting help"""
        options = test_convert.TestConvertCommand.test_help(self)
        options = self.help_check(options, "-t TEMPLATE, --template=TEMPLATE")
        options = self.help_check(options, "--tm")
        options = self.help_check(options, "-s MIN_SIMILARITY, --similarity=MIN_SIMILARITY")
        options = self.help_check(options, "--nofuzzymatching", last=True)

########NEW FILE########
__FILENAME__ = test_pypo2phppo
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# pypo2phppo unit tests
# Author: Wil Clouser <wclouser@mozilla.com>
# Date: 2009-12-03

from translate.convert import test_convert
from translate.misc import wStringIO
from translate.tools import pypo2phppo


class TestPyPo2PhpPo:

    def test_single_po(self):
        inputfile = """
# This user comment refers to: {0}
#. This developer comment does too: {0}
#: some/path.php:111
#, php-format
msgid "I have {1} apples and {0} oranges"
msgstr "I have {1} apples and {0} oranges"
        """
        outputfile = wStringIO.StringIO()
        pypo2phppo.convertpy2php(inputfile, outputfile)

        output = outputfile.getvalue()

        assert "refers to: %1$s" in output
        assert "does too: %1$s" in output
        assert 'msgid "I have %2$s apples and %1$s oranges"' in output
        assert 'msgstr "I have %2$s apples and %1$s oranges"' in output

    def test_plural_po(self):
        inputfile = """
#. This developer comment refers to {0}
#: some/path.php:111
#, php-format
msgid "I have {0} apple"
msgid_plural "I have {0} apples"
msgstr[0] "I have {0} apple"
msgstr[1] "I have {0} apples"
        """
        outputfile = wStringIO.StringIO()
        pypo2phppo.convertpy2php(inputfile, outputfile)
        output = outputfile.getvalue()

        assert 'msgid "I have %1$s apple"' in output
        assert 'msgid_plural "I have %1$s apples"' in output
        assert 'msgstr[0] "I have %1$s apple"' in output
        assert 'msgstr[1] "I have %1$s apples"' in output


class TestPyPo2PhpPoCommand(test_convert.TestConvertCommand, TestPyPo2PhpPo):
    """Tests running actual pypo2phppo commands on files"""
    convertmodule = pypo2phppo
    defaultoptions = {}

########NEW FILE########
__FILENAME__ = __version__
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright 2008-2009 Zuza Software Foundation
#
# This file is part of the Translate Toolkit.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses/>.

"""This file contains the version of the Translate Toolkit."""

build = 12018
"""The build number is used by external users of the Translate Toolkit to
trigger refreshes.  Thus increase the build number whenever changes are made to
code touching stats or quality checks.  An increased build number will force a
toolkit user, like Pootle, to regenerate it's stored stats and check
results."""

sver = "1.12.0-alpha1"
"""Human readable version number. Used for version number display."""

ver = (1, 12, 0)
"""Machine readable version number. Used by tools that need to adjust code
paths based on a Translate Toolkit release number."""

########NEW FILE########

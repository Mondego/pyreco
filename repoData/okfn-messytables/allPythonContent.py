__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# messytables documentation build configuration file, created by
# sphinx-quickstart on Sun Aug 14 17:09:50 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'messytables'
copyright = u'2011, Friedrich Lindenberg'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.3'
# The full version, including alpha/beta/rc tags.
release = '0.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
sys.path.append(os.path.abspath('_themes'))
html_theme_path = ['_themes']
html_theme = 'flask_small'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'messytablesdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'messytables.tex', u'messytables Documentation',
   u'Friedrich Lindenberg', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'messytables', u'messytables Documentation',
     [u'Friedrich Lindenberg'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'messytables', u'messytables Documentation', u'Friedrich Lindenberg',
   'messytables', 'One line description of project.', 'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = any
from messytables import (ZIPTableSet, PDFTableSet, CSVTableSet, XLSTableSet,
                         HTMLTableSet, ODSTableSet)
import messytables
import re


def TABTableSet(fileobj):
    return CSVTableSet(fileobj, delimiter='\t')

priorities = [ZIPTableSet, XLSTableSet,
              HTMLTableSet, TABTableSet, CSVTableSet,
              ODSTableSet]


def clean_ext(filename):
    """Takes a filename (or URL, or extension) and returns a better guess at
    the extension in question.
    >>> clean_ext("")
    ''
    >>> clean_ext("tsv")
    'tsv'
    >>> clean_ext("FILE.ZIP")
    'zip'
    >>> clean_ext("http://myserver.info/file.xlsx?download=True")
    'xlsx'
    """
    dot_ext = '.' + filename
    matches = re.findall('\.(\w*)', dot_ext)
    return matches[-1].lower()


def get_mime(fileobj):
    import magic
    # Since we need to peek the start of the stream, make sure we can
    # seek back later. If not, slurp in the contents into a StringIO.
    fileobj = messytables.seekable_stream(fileobj)
    header = fileobj.read(4096)
    mimetype = magic.from_buffer(header, mime=True)
    fileobj.seek(0)
    # There's an issue with vnd.ms-excel being returned from XLSX files, too.
    if mimetype == 'application/vnd.ms-excel' and header[:2] == 'PK':
        return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    return mimetype


def guess_mime(mimetype):
    lookup = {'application/x-zip-compressed': ZIPTableSet,
              'application/zip': ZIPTableSet,
              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheetapplication/zip': XLSTableSet,
              'text/comma-separated-values': CSVTableSet,
              'application/csv': CSVTableSet,
              'text/csv': CSVTableSet,
              'text/tab-separated-values': TABTableSet,
              'application/tsv': TABTableSet,
              'text/tsv': TABTableSet,
              'application/ms-excel': XLSTableSet,
              'application/xls': XLSTableSet,
              'application/vnd.ms-excel': XLSTableSet,
              'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': XLSTableSet,
              'text/html': HTMLTableSet,
              'application/xml': XLSTableSet,
              'application/pdf': PDFTableSet,
              'text/plain': CSVTableSet,  # could be TAB.
              'application/CDFV2-corrupt': XLSTableSet,
              'application/vnd.oasis.opendocument.spreadsheet': ODSTableSet,
              'application/x-vnd.oasis.opendocument.spreadsheet': ODSTableSet,
              }
    found = lookup.get(mimetype)
    if found:
        return found

    # But some aren't mimetyped due to being buggy but load fine!
    fuzzy_lookup = {'Composite Document File V2 Document': XLSTableSet}
    for candidate in fuzzy_lookup:
        if candidate in mimetype:
            return fuzzy_lookup[candidate]


def guess_ext(ext):
    lookup = {'zip': ZIPTableSet,
              'csv': CSVTableSet,
              'tsv': TABTableSet,
              'xls': XLSTableSet,
              'xlsx': XLSTableSet,
              'htm': HTMLTableSet,
              'html': HTMLTableSet,
              'pdf': PDFTableSet,
              'xlt': XLSTableSet,
                # obscure Excel extensions taken from
                # http://en.wikipedia.org/wiki/List_of_Microsoft_Office_filename_extensions
              'xlm': XLSTableSet,
              'xlsm': XLSTableSet,
              'xltx': XLSTableSet,
              'xltm': XLSTableSet,
              'ods': ODSTableSet}
    if ext in lookup:
        return lookup.get(ext, None)


def any_tableset(fileobj, mimetype=None, extension='', auto_detect=True):
    """Reads any supported table type according to a specified
    MIME type or file extension or automatically detecting the
    type.

    Best matching TableSet loaded with the fileobject is returned.
    Matching is done by looking at the type (e.g mimetype='text/csv'), then
    the file extension (e.g. extension='tsv'), then autodetecting the
    file format by using the magic library which looks at the first few
    bytes of the file BUT is often wrong. Consult the source for recognized
    MIME types and file extensions.

    On error it raises messytables.ReadError
    """

    short_ext = clean_ext(extension)
    # Auto-detect if the caller has offered no clue. (Because the
    # auto-detection routine is pretty poor.)
    error = []

    if mimetype is not None:
        attempt = guess_mime(mimetype)
        if attempt:
            return attempt(fileobj)
        else:
            error.append(
                'Did not recognise MIME type given: "{mimetype}".'.format(
                    mimetype=mimetype))

    if short_ext is not '':
        attempt = guess_ext(short_ext)
        if attempt:
            return attempt(fileobj)
        else:
            error.append(
                'Did not recognise extension "{ext}" (given "{full}".'.format(
                    ext=short_ext, full=extension))

    if auto_detect:
        magic_mime = get_mime(fileobj)
        attempt = guess_mime(magic_mime)
        if attempt:
            return attempt(fileobj)
        else:
            error.append(
                'Did not recognise detected MIME type: "{mimetype}".'.format(
                    mimetype=magic_mime))

    if error:
        raise messytables.ReadError('any: \n'.join(error))
    else:
        raise messytables.ReadError("any: Did not attempt any detection.")


class AnyTableSet:
    '''Deprecated - use any_tableset instead.'''
    @staticmethod
    def from_fileobj(fileobj, mimetype=None, extension=None):
        return any_tableset(fileobj, mimetype=mimetype, extension=extension)

########NEW FILE########
__FILENAME__ = commas
from ilines import ilines
import csv
import codecs
import chardet

from messytables.core import RowSet, TableSet, Cell
import messytables


class UTF8Recoder:
    """
    Iterator that reads an encoded stream and re-encodes the input to UTF-8
    """
    def __init__(self, f, encoding):
        sample = f.read(2000)
        if not encoding:
            results = chardet.detect(sample)
            encoding = results['encoding']
            if not encoding:
                # Don't break, just try and load the data with
                # a semi-sane encoding
                encoding = 'utf-8'
        f.seek(0)
        self.reader = codecs.getreader(encoding)(f, 'ignore')

        # The reader only skips a BOM if the encoding isn't explicit about its
        # endianness (i.e. if encoding is UTF-16 a BOM is handled properly
        # and taken out, but if encoding is UTF-16LE a BOM is ignored).
        # However, if chardet sees a BOM it returns an encoding with the
        # endianness explicit, which results in the codecs stream leaving the
        # BOM in the stream. This is ridiculously dumb. For UTF-{16,32}{LE,BE}
        # encodings, check for a BOM and remove it if it's there.
        if encoding in ("UTF-16LE", "UTF-16BE", "UTF-32LE", "UTF-32BE"):
            bom = getattr(codecs, "BOM_UTF" + encoding[4:6] +
                          "_" + encoding[-2:], None)
            if bom:
                # Try to read the BOM, which is a byte sequence, from
                # the underlying stream. If all characters match, then
                # go on. Otherwise when a character doesn't match, seek
                # the stream back to the beginning and go on.
                for c in bom:
                    if f.read(1) != c:
                        f.seek(0)
                        break

    def __iter__(self):
        return self

    def next(self):
        line = self.reader.readline()
        if not line or line == '\0':
            raise StopIteration
        result = line.encode("utf-8")
        return result


def to_unicode_or_bust(obj, encoding='utf-8'):
    if isinstance(obj, basestring):
        if not isinstance(obj, unicode):
            obj = unicode(obj, encoding)
    return obj


class CSVTableSet(TableSet):
    """ A CSV table set. Since CSV is always just a single table,
    this is just a pass-through for the row set. """

    def __init__(self, fileobj, delimiter=None, quotechar=None, name=None,
                 encoding=None, window=None, doublequote=None,
                 lineterminator=None, skipinitialspace=None):
        self.fileobj = messytables.seekable_stream(fileobj)
        self.name = name or 'table'
        self.delimiter = delimiter
        self.quotechar = quotechar
        self.encoding = encoding
        self.window = window
        self.doublequote = doublequote
        self.lineterminator = lineterminator
        self.skipinitialspace = skipinitialspace

    @property
    def tables(self):
        """ Return the actual CSV table. """
        return [CSVRowSet(self.name, self.fileobj,
                          delimiter=self.delimiter,
                          quotechar=self.quotechar,
                          encoding=self.encoding,
                          window=self.window,
                          doublequote=self.doublequote,
                          lineterminator=self.lineterminator,
                          skipinitialspace=self.skipinitialspace)]


class CSVRowSet(RowSet):
    """ A CSV row set is an iterator on a CSV file-like object
    (which can potentially be infinetly large). When loading,
    a sample is read and cached so you can run analysis on the
    fragment. """

    def __init__(self, name, fileobj, delimiter=None, quotechar=None,
                 encoding='utf-8', window=None, doublequote=None,
                 lineterminator=None, skipinitialspace=None):
        self.name = name
        seekable_fileobj = messytables.seekable_stream(fileobj)
        self.fileobj = UTF8Recoder(seekable_fileobj, encoding)
        self.lines = ilines(self.fileobj)
        self._sample = []
        self.delimiter = delimiter
        self.quotechar = quotechar
        self.window = window or 1000
        self.doublequote = doublequote
        self.lineterminator = lineterminator
        self.skipinitialspace = skipinitialspace
        try:
            for i in xrange(self.window):
                self._sample.append(self.lines.next())
        except StopIteration:
            pass
        super(CSVRowSet, self).__init__()

    @property
    def _dialect(self):
        delim = '\n'
        sample = delim.join(self._sample)
        try:
            dialect = csv.Sniffer().sniff(sample,
                delimiters=['\t', ',', ';'])
            dialect.lineterminator = delim
            dialect.doublequote = True
            return dialect
        except csv.Error:
            return csv.excel

    @property
    def _overrides(self):
        # some variables in the dialect can be overridden
        d = {}
        if self.delimiter:
            d['delimiter'] = self.delimiter
        if self.quotechar:
            d['quotechar'] = self.quotechar
        if self.doublequote:
            d['doublequote'] = self.doublequote
        if self.lineterminator:
            d['lineterminator'] = self.lineterminator
        if self.skipinitialspace is not None:
            d['skipinitialspace'] = self.skipinitialspace
        return d

    def raw(self, sample=False):
        def rows():
            for line in self._sample:
                yield line
            if not sample:
                for line in self.lines:
                    yield line

        # Fix the maximum field size to something a little larger
        csv.field_size_limit(256000)

        try:
            for row in csv.reader(rows(),
                    dialect=self._dialect, **self._overrides):
                yield [Cell(to_unicode_or_bust(c)) for c in row]
        except csv.Error, err:
            if 'newline inside string' in unicode(err) and sample:
                pass
            elif 'line contains NULL byte' in unicode(err):
                pass
            else:
                raise messytables.ReadError('Error reading CSV: %r', err)

########NEW FILE########
__FILENAME__ = core
from messytables.util import OrderedDict
from collections import Mapping
import cStringIO


def seekable_stream(fileobj):
    try:
        fileobj.seek(0)
        # if we got here, the stream is seekable
    except:
        # otherwise seek failed, so slurp in stream and wrap
        # it in a BytesIO
        fileobj = BufferedFile(fileobj)
    return fileobj


class BufferedFile(object):
    ''' A buffered file that preserves the beginning of
    a stream up to buffer_size
    '''
    def __init__(self, fp, buffer_size=2048):
        self.data = cStringIO.StringIO()
        self.fp = fp
        self.offset = 0
        self.len = 0
        self.fp_offset = 0
        self.buffer_size = buffer_size

    def _next_line(self):
        try:
            return self.fp.readline()
        except AttributeError:
            return self.fp.next()

    def _read(self, n):
        return self.fp.read(n)

    @property
    def _buffer_full(self):
        return self.len >= self.buffer_size

    def readline(self):
        if self.len < self.offset < self.fp_offset:
            raise BufferError('Line is not available anymore')
        if self.offset >= self.len:
            line = self._next_line()
            self.fp_offset += len(line)

            self.offset += len(line)

            if not self._buffer_full:
                self.data.write(line)
                self.len += len(line)
        else:
            line = self.data.readline()
            self.offset += len(line)
        return line

    def read(self, n=-1):
        if n == -1:
            # if the request is to do a complete read, then do a complete
            # read.
            self.data.seek(self.offset)
            return self.data.read(-1) + self.fp.read(-1)

        if self.len < self.offset < self.fp_offset:
            raise BufferError('Data is not available anymore')
        if self.offset >= self.len:
            byte = self._read(n)
            self.fp_offset += len(byte)

            self.offset += len(byte)

            if not self._buffer_full:
                self.data.write(byte)
                self.len += len(byte)
        else:
            byte = self.data.read(n)
            self.offset += len(byte)
        return byte

    def tell(self):
        return self.offset

    def seek(self, offset):
        if self.len < offset < self.fp_offset:
            raise BufferError('Cannot seek because data is not buffered here')
        self.offset = offset
        if offset < self.len:
            self.data.seek(offset)


class CoreProperties(Mapping):
    KEYS = []

    def __getitem__(self, key):
        if key in self.KEYS:
            return getattr(self, 'get_' + key)()
        else:
            raise KeyError("%r" % key)

    def __iter__(self):
        return self.KEYS.__iter__()

    def __len__(self):
        return len(self.KEYS)


class Cell(object):
    """ A cell is the basic value type. It always has a ``value`` (that
    may be ``None`` and may optionally also have a type and column name
    associated with it. If no ``type`` is set, the String type is set
    but no type conversion is set. """

    def __init__(self, value, column=None, type=None):
        if type is None:
            from messytables.types import StringType
            type = StringType()
        self.value = value
        self.column = column
        self.column_autogenerated = False
        self.type = type

    def __repr__(self):
        if self.column is not None:
            return "<Cell(%r=%r:%r>" % (self.column,
                                        self.type, self.value)
        return "<Cell(%r:%r>" % (self.type, self.value)

    @property
    def empty(self):
        """ Stringify the value and check that it has a length. """
        if self.value is None:
            return True
        value = self.value
        if not isinstance(value, basestring):
            value = unicode(value)
        if len(value.strip()):
            return False
        return True

    @property
    def properties(self):
        """ Source-specific information. Only a placeholder here. """
        return CoreProperties()


class TableSet(object):
    """ A table set is used for data formats in which multiple tabular
    objects are bundled. This might include relational databases and
    workbooks used in spreadsheet software (Excel, LibreOffice).

    For each format, we derive from this abstract base class, providing a
    constructor that takes a file object and tables() that returns each table.
    This means you can stream a table set directly off a web site or some
    similar source.

    On any fatal errors, it should raise messytables.ReadError
    """
    def __init__(self, fileobj):
        """ Store the fileobj, and perhaps all or part of the file. """
        pass

    @property
    def tables(self):
        """ Return a listing of tables (i.e. RowSets) in the ``TableSet``.
        Each table has a name. """
        pass

    def __getitem__(self, name):
        """ Return a RowSet based on the name given """
        matching = [table for table in self.tables if table.name == name]
        if not matching:
            raise KeyError("No RowSet called '%s'" % name)
        elif len(matching) > 1:
            raise LookupError("Multiple RowSets match '%s'" % name)
        return matching[0]

    @classmethod
    def from_fileobj(cls, fileobj, *args, **kwargs):
        """ Deprecated, only for compatibility reasons """
        return cls(fileobj, *args, **kwargs)


class RowSet(object):
    """ A row set (aka: table) is a simple wrapper for an iterator of
    rows (which in turn is a list of ``Cell`` objects). The main table
    iterable can only be traversed once, so on order to allow analytics
    like type and header guessing on the data, a sample of ``window``
    rows is read, cached, and made available.

    On any fatal errors, it should raise messytables.ReadError
    """

    def __init__(self, typed=False):
        self.typed = typed
        self._processors = []
        self._types = None

    def set_types(self, types):
        self.typed = True
        self._types = types

    def get_types(self):
        return self._types

    types = property(get_types, set_types)

    def register_processor(self, processor):
        """ Register a stream processor to be used on each row. A
        processor is a function called with the ``RowSet`` as its
        first argument and the row to be processed as the second
        argument. """
        self._processors.append(processor)

    def __iter__(self, sample=False):
        """ Apply processors to the row data. """
        for row in self.raw(sample=sample):
            for processor in self._processors:
                row = processor(self, row)
                if row is None:
                    break
            if row is not None:
                yield row

        # this is a bit dirty but required for the offset processor:
        self._offset = 0

    @property
    def sample(self):
        return self.__iter__(sample=True)

    def dicts(self, sample=False):
        """ Return a representation of the data as an iterator of
        ordered dictionaries. This is less specific than the cell
        format returned by the generic iterator but only gives a
        subset of the information. """
        generator = self.sample if sample else self
        for row in generator:
            yield OrderedDict([(c.column, c.value) for c in row])

    def __repr__(self):
        return "RowSet(%r)" % self.name

########NEW FILE########
__FILENAME__ = dateparser
import re

date_regex = re.compile(r'''^\d{1,4}[-\/\.\s]\S+[-\/\.\s]\S+''')


def is_date(value):
    return date_regex.match(value)


def create_date_formats(day_first=True):
    """generate combinations of time and date
    formats with different delimeters
    """

    if day_first:
        date_formats = ['dd/mm/yyyy', 'dd/mm/yy', 'yyyy/mm/dd']
        python_date_formats = ['%d/%m/%Y', '%d/%m/%y', '%Y/%m/%d']
    else:
        date_formats = ['mm/dd/yyyy', 'mm/dd/yy', 'yyyy/mm/dd']
        python_date_formats = ['%m/%d/%Y', '%m/%d/%y', '%Y/%m/%d']

    date_formats += [
        # Things with words in
        'dd/bb/yyyy', 'dd/bbb/yyyy'
    ]
    python_date_formats += [
        # Things with words in
        '%d/%b/%Y', '%d/%B/%Y'
    ]

    both_date_formats = zip(date_formats, python_date_formats)

    #time_formats = "hh:mmz hh:mm:ssz hh:mmtzd hh:mm:sstzd".split()
    time_formats = "hh:mm:ssz hh:mm:ss hh:mm:sstzd".split()
    python_time_formats = "%H:%M%Z %H:%M:%S %H:%M:%S%Z %H:%M%z %H:%M:%S%z".split()
    both_time_fromats = zip(time_formats, python_time_formats)

    #date_seperators = ["-","."," ","","/","\\"]
    date_seperators = ["-", ".", "/", " "]

    all_date_formats = []

    for seperator in date_seperators:
        for date_format, python_date_format in both_date_formats:
            all_date_formats.append(
                (date_format.replace("/", seperator),
                 python_date_format.replace("/", seperator))
            )

    all_formats = {}

    for date_format, python_date_format in all_date_formats:
        all_formats[date_format] = python_date_format
        for time_format, python_time_format in both_time_fromats:

            all_formats[date_format + time_format] = \
                python_date_format + python_time_format

            all_formats[date_format + "T" + time_format] =\
                python_date_format + "T" + python_time_format

            all_formats[date_format + " " + time_format] =\
                python_date_format + " " + python_time_format
    return all_formats.values()

DATE_FORMATS = create_date_formats()

########NEW FILE########
__FILENAME__ = error
class ReadError(Exception):
    '''Error reading the file/stream in terms of the expected format.'''
    pass

########NEW FILE########
__FILENAME__ = excel
from datetime import datetime
import xlrd
from xlrd.biffh import XLRDError

from messytables.core import RowSet, TableSet, Cell
from messytables.types import (StringType, IntegerType,
                               DateType, FloatType)
from messytables.error import ReadError


XLS_TYPES = {
    1: StringType(),
    # NB: Excel does not distinguish floats from integers so we use floats
    # We could try actual type detection between floats and ints later
    # or use the excel format string info - see
    # https://groups.google.com/forum/?fromgroups=#!topic/
    #  python-excel/cAQ1ndsCVxk
    2: FloatType(),
    3: DateType(None),
    # this is actually boolean but we do not have a boolean type yet
    4: IntegerType()
}


class XLSTableSet(TableSet):
    """An excel workbook wrapper object.
    """

    def __init__(self, fileobj=None, filename=None,
                 window=None, encoding=None):
        '''Initilize the tableset.

        :param encoding: passed on to xlrd.open_workbook function
            as encoding_override
        '''
        self.window = window
        try:
            if filename:
                self.workbook = xlrd.open_workbook(filename,
                                                   encoding_override=encoding)
            elif fileobj:
                self.workbook = xlrd.open_workbook(
                    file_contents=fileobj.read(),
                    encoding_override=encoding)
            else:
                raise Exception('You must provide one of filename or fileobj')
        except XLRDError:
            raise ReadError("Unsupported Excel format, or corrupt file")

    @property
    def tables(self):
        """ Return the sheets in the workbook. """
        return [XLSRowSet(name, self.workbook.sheet_by_name(name), self.window)
                for name in self.workbook.sheet_names()]


class XLSRowSet(RowSet):
    """ Excel support for a single sheet in the excel workbook. Unlike
    the CSV row set this is not a streaming operation. """

    def __init__(self, name, sheet, window=None):
        self.name = name
        self.sheet = sheet
        self.window = window or 1000
        super(XLSRowSet, self).__init__(typed=True)

    def raw(self, sample=False):
        """ Iterate over all rows in this sheet. Types are automatically
        converted according to the excel data types specified, including
        conversion of excel dates, which are notoriously buggy. """
        num_rows = self.sheet.nrows
        for i in xrange(min(self.window, num_rows) if sample else num_rows):
            row = []
            for j, cell in enumerate(self.sheet.row(i)):
                value = cell.value
                type = XLS_TYPES.get(cell.ctype, StringType())
                if type == DateType(None):
                    if value == 0:
                        raise ValueError('Invalid date at "%s":%d,%d' % (
                            self.sheet.name, j + 1, i + 1))
                    year, month, day, hour, minute, second = \
                        xlrd.xldate_as_tuple(value, self.sheet.book.datemode)
                    value = datetime(year, month, day, hour,
                                     minute, second)
                row.append(Cell(value, type=type))
            yield row

########NEW FILE########
__FILENAME__ = headers
from collections import defaultdict
from itertools import izip_longest

from messytables.core import Cell


def column_count_modal(rows):
    """ Return the modal value of columns in the row_set's
    sample. This can be assumed to be the number of columns
    of the table. """
    counts = defaultdict(int)
    for row in rows:
        length = len([c for c in row if not c.empty])
        if length > 1:
            counts[length] += 1
    if not len(counts):
        return 0
    return max(counts.items(), key=lambda (k, v): v)[0]


def headers_guess(rows, tolerance=1):
    """ Guess the offset and names of the headers of the row set.
    This will attempt to locate the first row within ``tolerance``
    of the mode of the number of rows in the row set sample.

    The return value is a tuple of the offset of the header row
    and the names of the columns.
    """
    rows = list(rows)
    modal = column_count_modal(rows)
    for i, row in enumerate(rows):
        length = len([c for c in row if not c.empty])
        if length >= modal - tolerance:
            # TODO: use type guessing to check that this row has
            # strings and does not conform to the type schema of
            # the table.
            return i, [c.value for c in row]
    return 0, []


def headers_processor(headers):
    """ Add column names to the cells in a row_set. If no header is
    defined, use an autogenerated name. """

    def apply_headers(row_set, row):
        _row = []
        pairs = izip_longest(row, headers)
        for i, (cell, header) in enumerate(pairs):
            if cell is None:
                cell = Cell(None)
            cell.column = header
            if not cell.column:
                cell.column = "column_%d" % i
                cell.column_autogenerated = True
            _row.append(cell)
        return _row
    return apply_headers


def headers_make_unique(headers, max_length=None):
    """Make sure the header names are unique. For non-unique
    columns, append 1, 2, 3, ... after the name. If max_length
    is set, truncate the original string so that the headers are
    unique up to that length."""

    headers = [h.strip() for h in headers]

    new_digits_length = 0
    while not max_length or new_digits_length <= max_length:
        # If maxlength is 63 and we expect 1 digit for appending
        # numerals to make the headers unique, then truncate the
        # column names to 62 characters.
        _headers = headers
        if max_length:
            _headers = [h[0:max_length - new_digits_length].strip()
                        for h in headers]

        # For headers that are not unique, add a number to the end.
        header_counter = {}
        new_headers = list(_headers)  # clone before using
        for i, h in enumerate(new_headers):
            if _headers.count(h) > 1:
                header_counter[h] = header_counter.get(h, 0) + 1
                new_headers[i] += "_%d" % header_counter[h]

        # If there is a max_length but adding a counter made a header longer
        # than max_length, we have to truncate more up front (which may
        # change which headers are nonunique) and try again.
        if max_length and (True in [len(h) > max_length for h in new_headers]):
            # Adding this counter made the new header longer than max_length.
            # We have to truncate the original headers more.
            new_digits_length += 1
            continue

        # Otherwise, the new headers are unique.
        return new_headers

    raise ValueError('''max_length is so small that the column names cannot
        be made unique.''')

########NEW FILE########
__FILENAME__ = html
from messytables.core import RowSet, TableSet, Cell, CoreProperties
import lxml.html
from collections import defaultdict


class HTMLTableSet(TableSet):
    """
    A TableSet from a HTML document.
    """
    def __init__(self, fileobj=None, filename=None, window=None):

        if filename:
            fh = open(filename, 'r')
        else:
            fh = fileobj
        if not fh:
            raise TypeError('You must provide one of filename or fileobj')

        self.htmltables = []
        root = lxml.html.fromstring(fh.read())

        # Grab tables that don't contain tables, remove from root, repeat.
        while True:
            dropped = False
            tables = root.xpath('//table[not(@messytable)]')
            if not tables:
                break
            for t in tables:
                if not t.xpath(".//table[not(@messytable)]"):
                    self.htmltables.append(t)
                    t.attrib['messytable'] = 'done'
                    dropped = True
            assert dropped, "Didn't find any tables not containing " + \
                "other tables. This is a bug."  # avoid infinite loops

    @property
    def tables(self):
        """
        Return a listing of tables (as HTMLRowSets) in the table set.
        """
        def rowset_name(rowset, table_index):
            return "Table {0} of {1}".format(table_index + 1,
                                             len(self.htmltables))

        return [HTMLRowSet(rowset_name(rowset, index), rowset)
                for index, rowset in enumerate(self.htmltables)]


def insert_blank_cells(row, blanks):
    """
    Given a list of values, insert blank cells at the indexes given by blanks
    The letters in these examples should really be cells.
    >>> insert_blank_cells(["a","e","f"],[1,2,3])
    ['a', <Cell(String:>, <Cell(String:>, <Cell(String:>, 'e', 'f']
    """
    # DISCUSS: option to repeat top-left of col/rowspan.
    # or to identify that areas are a single cell, originally.
    for i in blanks:
        row.insert(i, FakeHTMLCell())
    return row


class HTMLRowSet(RowSet):
    """
    A RowSet representing a HTML table.
    """
    def __init__(self, name, sheet, window=None):
        self.name = name
        self.sheet = sheet
        self.window = window or 1000
        super(HTMLRowSet, self).__init__()

    def in_table(self, els):
        """
        takes a list of xpath elements and returns only those
        whose parent table is this one
        """

        return [e for e in els
                if self.sheet in e.xpath("./ancestor::table[1]")]

    def raw(self, sample=False):
        def identify_anatomy(tag):
            # 0: thead, 1: tbody, 2: tfoot
            parts = ['.//ancestor::thead',
                     './/ancestor::tbody',
                     './/ancestor::tfoot']
            for i, part in enumerate(parts):
                if self.in_table(tag.xpath(part)):
                    return i
            return 2  # default to body

        blank_cells = defaultdict(list)  # ie row 2, cols 3,4,6: {2: [3,4,6]}
        allrows = sorted(self.in_table(self.sheet.xpath(".//tr")),
                         key=lambda tag: identify_anatomy(tag))
        # http://stackoverflow.com/questions/1915376/ - sorted() is stable.

        for r, row in enumerate(allrows):
            # TODO: handle header nicer - preserve the fact it's a header!
            html_cells = self.in_table(
                row.xpath('.//*[name()="td" or name()="th"]'))

            """ at the end of this chunk, you have accurate blank_cells."""
            output_column = 0
            for html_cell in html_cells:
                assert type(r) == int
                while output_column in blank_cells[r]:
                    output_column += 1  # pass over col, doesn't exist in src
                rowspan = int(html_cell.attrib.get('rowspan', "1"))
                colspan = int(html_cell.attrib.get('colspan', "1"))
                x_range = range(output_column, output_column + colspan)
                y_range = range(r, r + rowspan)
                for x in x_range:
                    for y in y_range:
                        if (output_column, r) != (x, y):
                            # don't skip current cell
                            blank_cells[y].append(x)
                output_column += 1

            cells = [HTMLCell(source=cell) for cell in html_cells]
            yield insert_blank_cells(cells, blank_cells[r])
            if sample and r == self.window:
                return
            del blank_cells[r]


class FakeHTMLCell(Cell):
    def __init__(self):
        super(FakeHTMLCell, self).__init__("")


class HTMLCell(Cell):
    """ The Cell __init__ signature is:
    def __init__(self, value=None, column=None, type=None):
    where 'value' is the primary input, 'column' is a column name, and
    type is messytables.types.StringType() or better."""

    def __init__(self, value=None, column=None, type=None, source=None):
        assert value is None
        assert isinstance(source, lxml.html.HtmlElement)
        self._lxml = source
        if type is None:
            from messytables.types import StringType
            type = StringType()
        self.type = type
        self.column = column
        self.column_autogenerated = False

    @property
    def value(self):
        return self._lxml.text_content()  # TODO improve?

    @property
    def properties(self):
        return HTMLProperties(self._lxml)


class HTMLProperties(CoreProperties):
    KEYS = ['_lxml', 'html']

    def __init__(self, lxml_element):
        if not isinstance(lxml_element, lxml.html.HtmlElement):
            raise TypeError("%r" % lxml_element)
        super(HTMLProperties, self).__init__()
        self.lxml_element = lxml_element

    def get_html(self):
        return lxml.html.tostring(self.lxml_element)

    def get__lxml(self):
        return self.lxml_element

########NEW FILE########
__FILENAME__ = ilines
# Created by Scott David Daniels on Wed, 23 Jun 2004, licensed under the PSF
# http://code.activestate.com/recipes/286165-ilines-universal
#   -newlines-from-any-data-source/


def ilines(source_iterable):
    '''yield lines as in universal-newlines from a stream of data blocks'''
    tail = ''
    for block in source_iterable:
        if not block:
            continue
        if tail.endswith('\015'):
            yield tail[:-1] + '\012'
            if block.startswith('\012'):
                pos = 1
            else:
                tail = ''
        else:
            pos = 0
        try:
            while True:  # While we are finding LF.
                npos = block.index('\012', pos) + 1
                try:
                    rend = npos - 2
                    rpos = block.index('\015', pos, rend)
                    if pos:
                        yield block[pos: rpos] + '\n'
                    else:
                        yield tail + block[:rpos] + '\n'
                    pos = rpos + 1
                    while True:  # While CRs 'inside' the LF
                        rpos = block.index('\015', pos, rend)
                        yield block[pos: rpos] + '\n'
                        pos = rpos + 1
                except ValueError:
                    pass
                if '\015' == block[rend]:
                    if pos:
                        yield block[pos: rend] + '\n'
                    else:
                        yield tail + block[:rend] + '\n'
                elif pos:
                    yield block[pos: npos]
                else:
                    yield tail + block[:npos]
                pos = npos
        except ValueError:
            pass
        # No LFs left in block.  Do all but final CR (in case LF)
        try:
            while True:
                rpos = block.index('\015', pos, -1)
                if pos:
                    yield block[pos: rpos] + '\n'
                else:
                    yield tail + block[:rpos] + '\n'
                pos = rpos + 1
        except ValueError:
            pass

        if pos:
            tail = block[pos:]
        else:
            tail += block
    if tail:
        yield tail

########NEW FILE########
__FILENAME__ = jts
'''
Convert a rowset to the json table schema
(http://www.dataprotocols.org/en/latest/json-table-schema.html)
'''

import messytables
import jsontableschema

MESSYTABLES_TO_JTS_MAPPING = {
    messytables.StringType: 'string',
    messytables.IntegerType: 'integer',
    messytables.FloatType: 'number',
    messytables.DecimalType: 'number',
    messytables.DateType: 'date',
    messytables.DateUtilType: 'date'
}


def celltype_as_string(celltype):
    return MESSYTABLES_TO_JTS_MAPPING[celltype.__class__]


def rowset_as_jts(rowset, headers=None, types=None):
    ''' Create a json table schema from a rowset
    '''
    _, headers = messytables.headers_guess(rowset.sample)
    types = map(celltype_as_string, messytables.type_guess(rowset.sample))

    return headers_and_typed_as_jts(headers, types)


def headers_and_typed_as_jts(headers, types):
    ''' Create a json table schema from headers and types as
    returned from :meth:`~messytables.headers.headers_guess`
    and :meth:`~messytables.types.type_guess`.
    '''
    j = jsontableschema.JSONTableSchema()

    for field_id, field_type in zip(headers, types):
        j.add_field(field_id=field_id,
                    label=field_id,
                    field_type=field_type)

    return j

########NEW FILE########
__FILENAME__ = ods
import cStringIO
import re
import zipfile

from lxml import etree

from messytables.core import RowSet, TableSet, Cell
from messytables.types import (StringType, DecimalType,
                               DateType)


ODS_TABLE_MATCH = re.compile(".*?(<table:table.*?<\/.*?:table>).*?", re.MULTILINE)
ODS_TABLE_NAME = re.compile('.*?table:name=\"(.*?)\".*?')
ODS_ROW_MATCH = re.compile(".*?(<table:table-row.*?<\/.*?:table-row>).*?", re.MULTILINE)

ODS_TYPES = {
    'float': DecimalType(),
    'date': DateType(None),
}

NAMESPACES = {
    "dc": u"http://purl.org/dc/elements/1.1/",
    "draw": u"urn:oasis:names:tc:opendocument:xmlns:drawing:1.0",
    "number": u"urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0",
    "office": u"urn:oasis:names:tc:opendocument:xmlns:office:1.0",
    "svg": u"urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0",
    "table": u"urn:oasis:names:tc:opendocument:xmlns:table:1.0",
    "text": u"urn:oasis:names:tc:opendocument:xmlns:text:1.0",
}

# We must wrap the XML fragments in a valid header otherwise iterparse will
# explode with certain (undefined) versions of libxml2.

ODS_HEADER = u"<wrapper {0}>"\
    .format(" ".join( 'xmlns:{0}="{1}"'.format(k,v)
            for k,v in NAMESPACES.iteritems()))
ODS_FOOTER = u"</wrapper>"


class ODSTableSet(TableSet):
    """
    A wrapper around ODS files. Because they are zipped and the info we want
    is in the zipped file as content.xml we must ensure that we either have
    a seekable object (local file) or that we retrieve all of the content from
    the remote URL.
    """

    def __init__(self, fileobj, window=None):
        '''Initialize the object.

        :param fileobj: may be a file path or a file-like object. Note the
        file-like object *must* be in binary mode and must be seekable (it will
        get passed to zipfile).

        As a specific tip: urllib2.urlopen returns a file-like object that is
        not in file-like mode while urllib.urlopen *does*!

        To get a seekable file you *cannot* use
        messytables.core.seekable_stream as it does not support the full seek
        functionality.
        '''
        if hasattr(fileobj, 'read'):
            # wrap in a StringIO so we do not have hassle with seeks and
            # binary etc (see notes to __init__ above)
            # TODO: rather wasteful if in fact fileobj comes from disk
            fileobj = cStringIO.StringIO(fileobj.read())

        self.window = window

        zf = zipfile.ZipFile(fileobj).open("content.xml")
        self.content = zf.read()
        zf.close()

    @property
    def tables(self):
        """
            Return the sheets in the workbook.

            A regex is used for this to avoid having to:

            1. load large the entire file into memory, or
            2. SAX parse the file more than once
        """
        sheets = [m.groups(0)[0]
                  for m in ODS_TABLE_MATCH.finditer(self.content)]
        return [ODSRowSet(sheet, self.window) for sheet in sheets]


class ODSRowSet(RowSet):
    """ ODS support for a single sheet in the ODS workbook. Unlike
    the CSV row set this is not a streaming operation. """

    def __init__(self, sheet, window=None):
        self.sheet = sheet

        self.name = "Unknown"
        m = ODS_TABLE_NAME.match(self.sheet)
        if m:
            self.name = m.groups(0)[0]

        self.window = window or 1000
        super(ODSRowSet, self).__init__(typed=True)

    def raw(self, sample=False):
        """ Iterate over all rows in this sheet. """
        rows = ODS_ROW_MATCH.findall(self.sheet)

        for row in rows:
            row_data = []

            block = "{0}{1}{2}".format(ODS_HEADER, row, ODS_FOOTER)
            partial = cStringIO.StringIO(block)

            for action, elem in etree.iterparse(partial, ('end',)):
                if elem.tag == '{urn:oasis:names:tc:opendocument:xmlns:table:1.0}table-cell':
                    cell_type = elem.attrib.get('urn:oasis:names:tc:opendocument:xmlns:office:1.0:value-type')
                    children = elem.getchildren()
                    if children:
                        c = Cell(children[0].text,
                                 type=ODS_TYPES.get(cell_type, StringType()))
                        row_data.append(c)

            if not row_data:
                raise StopIteration()

            del partial
            yield row_data
        del rows

########NEW FILE########
__FILENAME__ = pdf
from messytables.core import RowSet, TableSet, Cell
try:
    from pdftables import get_tables
except ImportError:
    get_tables = None


class PDFTableSet(TableSet):
    """
    A TableSet from a PDF document.
    """
    def __init__(self, fileobj=None, filename=None):
        if get_tables is None:
            raise ImportError("pdftables is not installed")
        if filename is not None:
            self.fh = open(filename, 'r')
        elif fileobj is not None:
            self.fh = fileobj
        else:
            raise TypeError('You must provide one of filename or fileobj')
        self.raw_tables = get_tables(self.fh)

    @property
    def tables(self):
        """
        Return a listing of tables (as PDFRowSets) in the table set.
        """
        def table_name(table):
            return "Table {0} of {1} on page {2} of {3}".format(
                table.table_number_on_page,
                table.total_tables_on_page,
                table.page_number,
                table.total_pages)
        return [PDFRowSet(table_name(table), table)
                for table in self.raw_tables]


class PDFRowSet(RowSet):
    """
    A RowSet representing a PDF table.
    """
    def __init__(self, name, table):
        if get_tables is None:
            raise ImportError("pdftables is not installed")
        super(PDFRowSet, self).__init__()
        self.name = name
        self.table = table

    def raw(self, sample=False):
        """
        Yield one row of cells at a time
        """
        for row in self.table:
            yield [Cell(pdf_cell) for pdf_cell in row]

########NEW FILE########
__FILENAME__ = types
import decimal
import datetime
from collections import defaultdict
from itertools import izip_longest
import locale
import sys

import dateutil.parser as parser

from messytables.dateparser import DATE_FORMATS, is_date


class CellType(object):
    """ A cell type maintains information about the format
    of the cell, providing methods to check if a type is
    applicable to a given value and to convert a value to the
    type. """

    guessing_weight = 1
    # the type that the result will have
    result_type = None

    def test(self, value):
        """ Test if the value is of the given type. The
        default implementation calls ``cast`` and checks if
        that throws an exception. True or False"""
        if isinstance(value, self.result_type):
            return True
        try:
            self.cast(value)
            return True
        except:
            return False

    @classmethod
    def instances(cls):
        return [cls()]

    def cast(self, value):
        """ Convert the value to the type. This may throw
        a quasi-random exception if conversion fails. """
        return value

    def __eq__(self, other):
        return self.__class__ == other.__class__

    def __hash__(self):
        return hash(self.__class__)

    def __repr__(self):
        return self.__class__.__name__.rsplit('Type', 1)[0]


class StringType(CellType):
    """ A string or other unconverted type. """
    result_type = basestring

    def cast(self, value):
        if value is None:
            return None
        if isinstance(value, self.result_type):
            return value
        try:
            return unicode(value)
        except UnicodeEncodeError:
            return str(value)


class IntegerType(CellType):
    """ An integer field. """
    guessing_weight = 6
    result_type = int

    def cast(self, value):
        if value in ('', None):
            return None
        try:
            return int(value)
        except:
            return locale.atoi(value)


class DecimalType(CellType):
    """ Decimal number, ``decimal.Decimal`` or float numbers. """
    guessing_weight = 4
    result_type = decimal.Decimal

    def cast(self, value):
        if value in ('', None):
            return None
        try:
            return decimal.Decimal(value)
        except:
            value = locale.atof(value)
            if sys.version_info < (2, 7):
                value = str(value)
            return decimal.Decimal(value)


class FloatType(DecimalType):
    """ FloatType is deprecated """
    pass


class DateType(CellType):
    """ The date type is special in that it also includes a specific
    date format that is used to parse the date, additionally to the
    basic type information. """
    guessing_weight = 3
    formats = DATE_FORMATS
    result_type = datetime.datetime

    def __init__(self, format):
        self.format = format

    @classmethod
    def instances(cls):
        return [cls(v) for v in cls.formats]

    def test(self, value):
        if isinstance(value, basestring) and not is_date(value):
            return False
        return CellType.test(self, value)

    def cast(self, value):
        if isinstance(value, self.result_type):
            return value
        if value in ('', None):
            return None
        if self.format is None:
            return value
        return datetime.datetime.strptime(value, self.format)

    def __eq__(self, other):
        return (isinstance(other, DateType) and
                self.format == other.format)

    def __repr__(self):
        return "Date(%s)" % self.format

    def __hash__(self):
        return hash(self.__class__) + hash(self.format)


class DateUtilType(CellType):
    """ The date util type uses the dateutil library to
    parse the dates. The advantage of this type over
    DateType is the speed and better date detection. However,
    it does not offer format detection.

    Do not use this together with the DateType"""
    guessing_weight = 3
    result_type = datetime.datetime

    def cast(self, value):
        if value in ('', None):
            return None
        return parser.parse(value)


TYPES = [StringType, DecimalType, IntegerType, DateType]


def type_guess(rows, types=TYPES, strict=False):
    """ The type guesser aggregates the number of successful
    conversions of each column to each type, weights them by a
    fixed type priority and select the most probable type for
    each column based on that figure. It returns a list of
    ``CellType``. Empty cells are ignored.

    Strict means that a type will not be guessed
    if parsing fails for a single cell in the column."""
    guesses = []
    type_instances = [i for t in types for i in t.instances()]
    if strict:
        at_least_one_value = []
        for ri, row in enumerate(rows):
            diff = len(row) - len(guesses)
            for _ in xrange(diff):
                typesdict = {}
                for type in type_instances:
                    typesdict[type] = 0
                guesses.append(typesdict)
                at_least_one_value.append(False)
            for ci, cell in enumerate(row):
                if not cell.value:
                    continue
                at_least_one_value[ci] = True
                for type in guesses[ci].keys():
                    if not type.test(cell.value):
                        guesses[ci].pop(type)
        # no need to set guessing weights before this
        # because we only accept a type if it never fails
        for i, guess in enumerate(guesses):
            for type in guess:
                guesses[i][type] = type.guessing_weight
        # in case there were no values at all in the column,
        # we just set the guessed type to string
        for i, v in enumerate(at_least_one_value):
            if not v:
                guesses[i] = {StringType(): 0}
    else:
        for i, row in enumerate(rows):
            diff = len(row) - len(guesses)
            for _ in xrange(diff):
                guesses.append(defaultdict(int))
            for i, cell in enumerate(row):
                # add string guess so that we have at least one guess
                guesses[i][StringType()] = guesses[i].get(StringType(), 0)
                if not cell.value:
                    continue
                for type in type_instances:
                    if type.test(cell.value):
                        guesses[i][type] += type.guessing_weight
        _columns = []
    _columns = []
    for guess in guesses:
        # this first creates an array of tuples because we want the types to be
        # sorted. Even though it is not specified, python chooses the first
        # element in case of a tie
        # See: http://stackoverflow.com/a/6783101/214950
        guesses_tuples = [(t, guess[t]) for t in type_instances if t in guess]
        _columns.append(max(guesses_tuples, key=lambda (t, n): n)[0])
    return _columns


def types_processor(types, strict=False):
    """ Apply the column types set on the instance to the
    current row, attempting to cast each cell to the specified
    type.

    Strict means that casting errors are not ignored"""
    def apply_types(row_set, row):
        if types is None:
            return row
        for cell, type in izip_longest(row, types):
            try:
                cell.value = type.cast(cell.value)
                cell.type = type
            except:
                if strict and type:
                    raise
        return row
    return apply_types

########NEW FILE########
__FILENAME__ = util
try:
    # python 2.7:
    from collections import OrderedDict
except ImportError:
    ## {{{ http://code.activestate.com/recipes/576669/ (r18)
    ## Raymond Hettingers proporsal to go in 2.7
    from collections import MutableMapping

    class OrderedDict(dict, MutableMapping):

        # Methods with direct access to underlying attributes

        def __init__(self, *args, **kwds):
            if len(args) > 1:
                raise TypeError('expected at 1 argument, got %d', len(args))
            if not hasattr(self, '_keys'):
                self._keys = []
            self.update(*args, **kwds)

        def clear(self):
            del self._keys[:]
            dict.clear(self)

        def __setitem__(self, key, value):
            if key not in self:
                self._keys.append(key)
            dict.__setitem__(self, key, value)

        def __delitem__(self, key):
            dict.__delitem__(self, key)
            self._keys.remove(key)

        def __iter__(self):
            return iter(self._keys)

        def __reversed__(self):
            return reversed(self._keys)

        def popitem(self):
            if not self:
                raise KeyError
            key = self._keys.pop()
            value = dict.pop(self, key)
            return key, value

        def __reduce__(self):
            items = [[k, self[k]] for k in self]
            inst_dict = vars(self).copy()
            inst_dict.pop('_keys', None)
            return (self.__class__, (items,), inst_dict)

        # Methods with indirect access via the above methods

        setdefault = MutableMapping.setdefault
        update = MutableMapping.update
        pop = MutableMapping.pop
        keys = MutableMapping.keys
        values = MutableMapping.values
        items = MutableMapping.items

        def __repr__(self):
            pairs = ', '.join(map('%r: %r'.__mod__, self.items()))
            return '%s({%s})' % (self.__class__.__name__, pairs)

        def copy(self):
            return self.__class__(self)

        @classmethod
        def fromkeys(cls, iterable, value=None):
            d = cls()
            for key in iterable:
                d[key] = value
            return d
    ## end of http://code.activestate.com/recipes/576669/ }}}


def offset_processor(offset):
    """ Skip ``offset`` from the given iterator. This can
    be used in combination with the ``headers_processor`` to
    apply the result of a header scan to the table.

    :param offset: Offset to be skipped
    :type offset: int
    """
    def apply_offset(row_set, row):
        if not hasattr(row_set, '_offset'):
            row_set._offset = 0
        if row_set._offset >= offset:
            return row
        row_set._offset += 1
    return apply_offset


def null_processor(nulls):
    """ Replaces every occurrence of items from `nulls` with None.

    :param nulls: List of items to be replaced
    :type nulls: list
    """
    def apply_replace(row_set, row):
        def replace(cell):
            if cell.value in nulls:
                cell.value = None
            return cell
        return [replace(cell) for cell in row]
    return apply_replace

########NEW FILE########
__FILENAME__ = zip
import zipfile

import messytables


class ZIPTableSet(messytables.TableSet):
    """ Reads TableSets from inside a ZIP file """

    def __init__(self, fileobj):
        """
        On error it will raise messytables.ReadError.
        """
        tables = []
        found = []
        z = zipfile.ZipFile(fileobj, 'r')
        try:
            for f in z.infolist():
                ext = None
                if "." in f.filename:
                    ext = f.filename[f.filename.rindex(".") + 1:]

                try:
                    filetables = messytables.any.any_tableset(
                        z.open(f), extension=ext)
                except ValueError as e:
                    found.append(f.filename + ": " + e.message)
                    continue

                tables.extend(filetables.tables)

            if len(tables) == 0:
                raise messytables.ReadError('''ZIP file has no recognized
                    tables (%s).''' % ', '.join(found))
        finally:
            z.close()
        self._tables = tables

    @property
    def tables(self):
        """ Return the tables contained in any loadable
        files within the ZIP file.
        """
        return self._tables

########NEW FILE########
__FILENAME__ = test_any
# -*- coding: utf-8 -*-
import unittest

from . import horror_fobj
from nose.tools import assert_equal
from nose.plugins.skip import SkipTest
from messytables import (any_tableset, XLSTableSet, ZIPTableSet,
                         CSVTableSet, ODSTableSet,
                         ReadError)

suite = [{'filename': 'simple.csv', 'tableset': CSVTableSet},
         {'filename': 'simple.xls', 'tableset': XLSTableSet},
         {'filename': 'simple.xlsx', 'tableset': XLSTableSet},
         {'filename': 'simple.zip', 'tableset': ZIPTableSet},
         {'filename': 'simple.ods', 'tableset': ODSTableSet},
         {'filename': 'bian-anal-mca-2005-dols-eng-1011-0312-tab3.xlsm', 'tableset': XLSTableSet},
         ]

# Special handling for PDFTables - skip if not installed
try:
    import pdftables
except ImportError:
    got_pdftables = False
    suite.append({"filename": "simple.pdf", "tableset": False})
else:
    from messytables import PDFTableSet
    got_pdftables = True
    suite.append({"filename": "simple.pdf", "tableset": PDFTableSet})

def test_simple():
    for d in suite:
        yield check_no_filename, d
        yield check_filename, d


def check_no_filename(d):
    if not d['tableset']:
        raise SkipTest("Optional library not installed. Skipping")
    fh = horror_fobj(d['filename'])
    table_set = any_tableset(fh)
    assert isinstance(table_set, d['tableset']), type(table_set)


def check_filename(d):
    if not d['tableset']:
        raise SkipTest("Optional library not installed. Skipping")
    fh = horror_fobj(d['filename'])
    table_set = any_tableset(fh, extension=d['filename'], auto_detect=False)
    assert isinstance(table_set, d['tableset']), type(table_set)


class TestAny(unittest.TestCase):
    def test_xlsm(self):
        fh = horror_fobj('bian-anal-mca-2005-dols-eng-1011-0312-tab3.xlsm')
        table_set = any_tableset(fh, extension='xls')
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(62, len(data))

    def test_unknown(self):
        fh = horror_fobj('simple.unknown')
        self.assertRaises(ReadError, lambda: any_tableset(fh, extension='unknown'))

########NEW FILE########
__FILENAME__ = test_guessing
# -*- coding: utf-8 -*-
import unittest
import StringIO

from . import horror_fobj
from nose.tools import assert_equal
from messytables import (CSVTableSet, type_guess, headers_guess,
                         offset_processor, DateType, StringType,
                         DecimalType, IntegerType,
                         DateUtilType)


class TypeGuessTest(unittest.TestCase):
    def test_type_guess(self):
        csv_file = StringIO.StringIO('''
            1,   2012/2/12, 2,   02 October 2011
            2,   2012/2/12, 2,   02 October 2011
            2.4, 2012/2/12, 1,   1 May 2011
            foo, bar,       1000,
            4.3, ,          42,  24 October 2012
             ,   2012/2/12, 21,  24 December 2013''')
        rows = CSVTableSet(csv_file).tables[0]
        guessed_types = type_guess(rows.sample)

        assert_equal(guessed_types, [
            DecimalType(), DateType('%Y/%m/%d'),
            IntegerType(), DateType('%d %B %Y')])

    def test_type_guess_strict(self):
        import locale
        locale.setlocale(locale.LC_ALL, 'en_GB.UTF-8')
        csv_file = StringIO.StringIO('''
            1,   2012/2/12, 2,      2,02 October 2011,"100.234354"
            2,   2012/2/12, 1.1,    0,1 May 2011,"100,000,000.12"
            foo, bar,       1500,   0,,"NaN"
            4,   2012/2/12, 42,"-2,000",24 October 2012,"42"
            ,,,,,''')
        rows = CSVTableSet(csv_file).tables[0]
        guessed_types = type_guess(rows.sample, strict=True)
        assert_equal(guessed_types, [
            StringType(), StringType(),
            DecimalType(), IntegerType(), DateType('%d %B %Y'),
            DecimalType()])

    def test_strict_guessing_handles_padding(self):
        csv_file = StringIO.StringIO('''
            1,   , 2
            2,   , 1.1
            foo, , 1500''')
        rows = CSVTableSet(csv_file).tables[0]
        guessed_types = type_guess(rows.sample, strict=True)
        assert_equal(len(guessed_types), 3)
        assert_equal(guessed_types,
                     [StringType(), StringType(), DecimalType()])

    def test_non_strict_guessing_handles_padding(self):
        csv_file = StringIO.StringIO('''
            1,   , 2.1
            2,   , 1.1
            foo, , 1500''')
        rows = CSVTableSet(csv_file).tables[0]
        guessed_types = type_guess(rows.sample, strict=False)
        assert_equal(len(guessed_types), 3)
        assert_equal(guessed_types,
                     [IntegerType(), StringType(), DecimalType()])

    def test_guessing_uses_first_in_case_of_tie(self):
        csv_file = StringIO.StringIO('''
            2
            1.1
            1500''')
        rows = CSVTableSet(csv_file).tables[0]
        guessed_types = type_guess(
            rows.sample, types=[DecimalType, IntegerType], strict=False)
        assert_equal(guessed_types, [DecimalType()])

        guessed_types = type_guess(
            rows.sample, types=[IntegerType, DecimalType], strict=False)
        assert_equal(guessed_types, [IntegerType()])

    def test_strict_type_guessing_with_large_file(self):
        fh = horror_fobj('211.csv')
        rows = CSVTableSet(fh).tables[0]
        offset, headers = headers_guess(rows.sample)
        rows.register_processor(offset_processor(offset + 1))
        types = [StringType, IntegerType, DecimalType, DateUtilType]
        guessed_types = type_guess(rows.sample, types, True)
        assert_equal(len(guessed_types), 96)
        assert_equal(guessed_types, [
            IntegerType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            IntegerType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), IntegerType(), StringType(), DecimalType(),
            DecimalType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            IntegerType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            IntegerType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), DateUtilType(),
            DateUtilType(), DateUtilType(), DateUtilType(), StringType(),
            StringType(), StringType()])

    def test_file_with_few_strings_among_integers(self):
        fh = horror_fobj('mixedGLB.csv')
        rows = CSVTableSet(fh).tables[0]
        offset, headers = headers_guess(rows.sample)
        rows.register_processor(offset_processor(offset + 1))
        types = [StringType, IntegerType, DecimalType, DateUtilType]
        guessed_types = type_guess(rows.sample, types, True)
        assert_equal(len(guessed_types), 19)
        print guessed_types
        assert_equal(guessed_types, [
            IntegerType(), IntegerType(),
            IntegerType(), IntegerType(), IntegerType(), IntegerType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), StringType(), StringType(),
            StringType(), StringType(), IntegerType(), StringType(),
            StringType()])

########NEW FILE########
__FILENAME__ = test_properties
# -*- coding: utf-8 -*-

import unittest
from . import horror_fobj
from messytables.any import any_tableset
from nose.tools import assert_true, assert_equal, assert_false, assert_raises
import lxml.html

try:
    # Python 2.6 doesn't provide assert_is_instance
    # TODO move out into separate module, used by test/test_read.py too
    from nose.tools import assert_is_instance
except ImportError:
    def assert_is_instance(obj, cls, msg=None):
        if not isinstance(obj, cls):
            raise AssertionError('Expected an instance of %r, got a %r' % (
                                 cls, obj.__class__))


class TestCellProperties(unittest.TestCase):
    def test_core_properties(self):
        csv = any_tableset(horror_fobj('simple.csv'), extension="csv")
        for table in csv.tables:
            for row in table:
                for cell in row:
                    cell.properties  # vague existence
                    assert_false('anything' in cell.properties)


class TestCoreProperties(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.html = any_tableset(horror_fobj('rowcolspan.html'),
                                extension="html")
        cls.first_row = list(list(cls.html.tables)[0])[0]
        cls.real_cell = cls.first_row[1]

    def test_properties_implements_in(self):
        assert_true('html' in self.real_cell.properties)
        assert_false('invalid' in self.real_cell.properties)

    def test_properties_implements_keys(self):
        assert_equal(['_lxml', 'html'], self.real_cell.properties.keys())

    def test_properties_implements_items(self):
        self.real_cell.properties.items()

    def test_properties_implements_get(self):
        assert_equal('default', self.real_cell.properties.get(
            'not_in_properties', 'default'))
        assert_equal(None, self.real_cell.properties.get('not_in_properties'))


class TestHtmlProperties(unittest.TestCase):
    # <td colspan='2'> would create one 'real' and one 'fake' cell
    @classmethod
    def setUpClass(cls):
        cls.html = any_tableset(horror_fobj('rowcolspan.html'),
                                extension="html")
        cls.first_row = list(list(cls.html.tables)[0])[0]
        cls.real_cell = cls.first_row[1]
        cls.fake_cell = cls.first_row[2]

    def test_real_cells_have_properties(self):
        assert_equal(
            set(['_lxml', 'html']),
            set(self.real_cell.properties.keys()))

    def test_real_cells_have_lxml_property(self):
        lxml_element = self.real_cell.properties['_lxml']
        assert_is_instance(lxml_element, lxml.html.HtmlElement)
        assert_equal('<td colspan="2">06</td>',
                     lxml.html.tostring(lxml_element))

    def test_fake_cells_have_no_lxml_property(self):
        assert_raises(KeyError, lambda: self.fake_cell.properties['_lxml'])

    def test_real_cells_have_html_property(self):
        html = self.real_cell.properties['html']
        assert_is_instance(html, basestring)
        assert_equal('<td colspan="2">06</td>', html)

    def test_fake_cells_have_no_html_property(self):
        assert_raises(KeyError, lambda: self.fake_cell.properties['html'])

########NEW FILE########
__FILENAME__ = test_read
# -*- coding: utf-8 -*-
import unittest

from . import horror_fobj
from nose.tools import assert_equal, assert_true
from nose.plugins.skip import SkipTest
try:
    # Python 2.6 doesn't provide assert_is_instance
    from nose.tools import assert_is_instance
except ImportError:
    def assert_is_instance(obj, cls, msg=None):
        assert_true(isinstance(obj, cls))

from messytables import (CSVTableSet, StringType, HTMLTableSet,
                         ZIPTableSet, XLSTableSet, XLSXTableSet, PDFTableSet,
                         ODSTableSet, headers_guess, headers_processor,
                         offset_processor, DateType, FloatType,
                         IntegerType, rowset_as_jts,
                         types_processor, type_guess, null_processor)
import datetime


class ReadCsvTest(unittest.TestCase):
    def test_read_simple_csv(self):
        fh = horror_fobj('simple.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(7, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'date')
        assert_equal(row[1].value, 'temperature')

        for row in list(row_set):
            assert_equal(3, len(row))
            assert_equal(row[0].type, StringType())

    def test_read_complex_csv(self):
        fh = horror_fobj('complex.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(4, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'date')
        assert_equal(row[1].value, 'another date')
        assert_equal(row[2].value, 'temperature')
        assert_equal(row[3].value, 'place')

        for row in list(row_set):
            assert_equal(4, len(row))
            assert_equal(row[0].type, StringType())

    def test_overriding_sniffed(self):
        # semicolon separated values
        fh = horror_fobj('simple.csv')
        table_set = CSVTableSet(fh, delimiter=";")
        row_set = table_set.tables[0]
        assert_equal(7, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(len(row), 1)

    def test_read_head_padding_csv(self):
        fh = horror_fobj('weird_head_padding.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        offset, headers = headers_guess(row_set.sample)
        assert 11 == len(headers), headers
        assert_equal(u'1985', headers[1].strip())
        row_set.register_processor(headers_processor(headers))
        row_set.register_processor(offset_processor(offset + 1))
        data = list(row_set.sample)
        for row in row_set:
            assert_equal(11, len(row))
        value = data[1][0].value.strip()
        assert value == u'Gefchirurgie', value

    def test_read_head_offset_csv(self):
        fh = horror_fobj('simple.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        offset, headers = headers_guess(row_set.sample)
        assert_equal(offset, 0)
        row_set.register_processor(offset_processor(offset + 1))
        data = list(row_set.sample)
        assert_equal(int(data[0][1].value), 1)
        data = list(row_set)
        assert_equal(int(data[0][1].value), 1)

    def test_read_type_guess_simple(self):
        fh = horror_fobj('simple.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        types = type_guess(row_set.sample)
        expected_types = [DateType("%Y-%m-%d"), IntegerType(), StringType()]
        assert_equal(types, expected_types)

        row_set.register_processor(types_processor(types))
        data = list(row_set)
        header_types = map(lambda c: c.type, data[0])
        assert_equal(header_types, [StringType()] * 3)
        row_types = map(lambda c: c.type, data[2])
        assert_equal(expected_types, row_types)

    def test_apply_null_values(self):
        fh = horror_fobj('null.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        types = type_guess(row_set.sample, strict=True)
        expected_types = [IntegerType(), StringType(), IntegerType(),
                          StringType()]
        assert_equal(types, expected_types)

        row_set.register_processor(types_processor(types))
        data = list(row_set)
        # treat null as non empty text and 0 as non empty integer
        assert [x.empty for x in data[0]] == [False, False, False, False]
        assert [x.empty for x in data[1]] == [False, False, False, False]
        assert [x.empty for x in data[2]] == [False, False, True, True]
        assert [x.empty for x in data[3]] == [False, False, False, False]
        assert [x.empty for x in data[4]] == [False, False, False, True]
        assert [x.empty for x in data[5]] == [False, False, False, True]

        # we expect None for Integers and "" for empty strings in CSV
        assert [x.value for x in data[2]] == [3, "null", None, ""], data[2]

    def test_null_process(self):
        fh = horror_fobj('null.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        row_set.register_processor(null_processor(['null']))
        data = list(row_set)

        nones = [[x.value is None for x in row] for row in data]
        assert_equal(nones[0], [False, True, False, False])
        assert_equal(nones[1], [False, False, False, True])
        assert_equal(nones[2], [False, True, False, False])

        types = type_guess(row_set.sample, strict=True)
        expected_types = [IntegerType(), IntegerType(), IntegerType(),
                          IntegerType()]
        assert_equal(types, expected_types)

        row_set.register_processor(types_processor(types))

        # after applying the types, '' should become None for int columns
        data = list(row_set)
        nones = [[x.value is None for x in row] for row in data]
        assert_equal(nones[0], [False, True, False, False])
        assert_equal(nones[1], [False, False, False, True])
        assert_equal(nones[2], [False, True, True, True])

    def test_read_encoded_csv(self):
        fh = horror_fobj('utf-16le_encoded.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(328, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[1].value, 'Organisation_name')

    def test_long_csv(self):
        fh = horror_fobj('long.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(4000, len(data))

    def test_small_csv(self):
        fh = horror_fobj('small.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(1, len(data))

    def test_skip_initials(self):
        def rows(skip_policy):
            fh = horror_fobj('skip_initials.csv')
            table_set = CSVTableSet(fh,
                                    skipinitialspace=skip_policy)
            row_set = table_set.tables[0]
            return row_set

        second = lambda r: r[1].value

        assert "goodbye" in map(second, rows(True))
        assert "    goodbye" in map(second, rows(False))

    def test_guess_headers(self):
        fh = horror_fobj('weird_head_padding.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        offset, headers = headers_guess(row_set.sample)
        row_set.register_processor(headers_processor(headers))
        row_set.register_processor(offset_processor(offset + 1))
        data = list(row_set)
        assert 'Frauenheilkunde' in data[9][0].value, data[9][0].value

        fh = horror_fobj('weird_head_padding.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        row_set.register_processor(headers_processor(['foo', 'bar']))
        data = list(row_set)
        assert 'foo' in data[12][0].column, data[12][0]
        assert 'Chirurgie' in data[12][0].value, data[12][0].value

    def test_read_encoded_characters_csv(self):
        fh = horror_fobj('characters.csv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        offset, headers = headers_guess(row_set.sample)
        row_set.register_processor(headers_processor(headers))
        row_set.register_processor(offset_processor(offset + 1))
        data = list(row_set)
        assert_equal(382, len(data))
        assert_equal(data[0][2].value, u'')
        assert_equal(data[-1][2].value, u'')


class ReadZipTest(unittest.TestCase):
    def test_read_simple_zip(self):
        fh = horror_fobj('simple.zip')
        table_set = ZIPTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(7, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'date')
        assert_equal(row[1].value, 'temperature')

        for row in list(row_set):
            assert_equal(3, len(row))
            assert_equal(row[0].type, StringType())


class ReadTsvTest(unittest.TestCase):
    def test_read_simple_tsv(self):
        fh = horror_fobj('example.tsv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(141, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'hour')
        assert_equal(row[1].value, 'expr1_0_imp')
        for row in list(row_set):
            assert_equal(17, len(row))
            assert_equal(row[0].type, StringType())


class ReadSsvTest(unittest.TestCase):
    def test_read_simple_ssv(self):
        # semicolon separated values
        fh = horror_fobj('simple.ssv')
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(7, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'date')
        assert_equal(row[1].value, 'temperature')

        for row in list(row_set):
            assert_equal(3, len(row))
            assert_equal(row[0].type, StringType())


class ReadODSTest(unittest.TestCase):
    def test_read_simple_ods(self):
        fh = horror_fobj('simple.ods')
        table_set = ODSTableSet(fh)
        assert_equal(1, len(table_set.tables))
        row_set = table_set.tables[0]
        row = list(row_set.sample)[0]
        assert_equal(row[0].value, 'Name')
        assert_equal(row[1].value, 'Age')
        assert_equal(row[2].value, 'When')
        total = 4
        for row in row_set.sample:
            total = total - 1
            assert 3 == len(row), row
        assert_equal(total, 0)

    def test_read_large_ods(self):
        fh = horror_fobj('large.ods')
        table_set = ODSTableSet(fh)
        assert_equal(6, len(table_set.tables))
        row_set = table_set.tables[0]
        row = row_set.raw().next()
        assert len(row) == 5, len(row)
        for row in row_set.sample:
            assert len(row) == 5, len(row)

    def test_annotated_ods(self):
        fh = horror_fobj('annotated.ods')
        table_set = ODSTableSet(fh)
        assert_equal(4, len(table_set.tables))
        row_set = table_set.tables[0]
        for row in row_set.sample:
            assert len(row) == 1, len(row)

        row_set = table_set.tables[1]
        l = len(list(row_set.sample))
        assert 87 == l, l


class XlsxBackwardsCompatibilityTest(unittest.TestCase):
    def test_that_xlsx_is_handled_by_xls_table_set(self):
        """
        Should emit a DeprecationWarning.
        """
        fh = horror_fobj('simple.xlsx')
        assert_is_instance(XLSXTableSet(fh), XLSTableSet)


class ReadXlsTest(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.large_xlsx_table_set = XLSTableSet(   # TODO
            horror_fobj('large.xlsx'))

    def test_read_simple_xls(self):
        fh = horror_fobj('simple.xls')
        table_set = XLSTableSet(fh)
        assert_equal(1, len(table_set.tables))
        row_set = table_set.tables[0]
        first_row = list(row_set.sample)[0]
        third_row = list(row_set.sample)[2]

        assert_is_instance(first_row[0].value, unicode)
        assert_is_instance(first_row[1].value, unicode)
        assert_is_instance(first_row[2].value, unicode)

        assert_is_instance(third_row[0].value, datetime.datetime)
        assert_is_instance(third_row[1].value, float)
        assert_is_instance(third_row[2].value, unicode)

        assert_equal(first_row[0].value, 'date')
        assert_equal(first_row[1].value, 'temperature')
        assert_equal(first_row[2].value, 'place')

        assert_equal(third_row[0].value, datetime.datetime(2011, 1, 2, 0, 0))
        assert_equal(third_row[1].value, -1)
        assert_equal(third_row[2].value, u'Galway')

        for row in list(row_set):
            assert 3 == len(row), row

    def test_read_head_offset_excel(self):
        fh = horror_fobj('simple.xls')
        table_set = XLSTableSet(fh)
        row_set = table_set.tables[0]
        offset, headers = headers_guess(row_set.sample)
        assert_equal(offset, 0)
        row_set.register_processor(offset_processor(offset + 1))
        data = list(row_set.sample)
        assert_equal(int(data[0][1].value), 1)
        data = list(row_set)
        assert_equal(int(data[0][1].value), 1)

    def test_read_simple_xlsx(self):
        fh = horror_fobj('simple.xlsx')
        table_set = XLSTableSet(fh)
        assert_equal(1, len(table_set.tables))
        row_set = table_set.tables[0]
        first_row = list(row_set.sample)[0]
        third_row = list(row_set.sample)[2]

        assert_is_instance(first_row[0].value, unicode)
        assert_is_instance(first_row[1].value, unicode)
        assert_is_instance(first_row[2].value, unicode)

        assert_is_instance(third_row[0].value, datetime.datetime)
        assert_is_instance(third_row[1].value, float)
        assert_is_instance(third_row[2].value, unicode)

        assert_equal(first_row[0].value, 'date')
        assert_equal(first_row[1].value, 'temperature')
        assert_equal(first_row[2].value, 'place')

        assert_equal(third_row[0].value, datetime.datetime(2011, 1, 2, 0, 0))
        assert_equal(third_row[1].value, -1.0)
        assert_equal(third_row[2].value, u'Galway')

        for row in list(row_set):
            assert 3 == len(row), row

    def test_large_file_report_sheet_has_11_cols_52_rows(self):
        table = self.large_xlsx_table_set['Report']
        num_rows = len(list(table))
        num_cols = len(list(table)[0])

        assert_equal(52, num_rows)
        assert_equal(11, num_cols)
        num_cells = sum(len(row) for row in table)
        assert_equal(num_rows * num_cols, num_cells)

    def test_large_file_data_sheet_has_11_cols_8547_rows(self):
        table = self.large_xlsx_table_set['data']
        num_rows = len(list(table))
        num_cols = len(list(table)[0])

        assert_equal(8547, num_rows)
        assert_equal(11, num_cols)
        num_cells = sum(len(row) for row in table)
        assert_equal(num_rows * num_cols, num_cells)

    def test_large_file_criteria_sheet_has_5_cols_12_rows(self):
        table = self.large_xlsx_table_set['criteria']
        num_rows = len(list(table))
        num_cols = len(list(table)[0])

        assert_equal(5, num_cols)
        assert_equal(12, num_rows)
        num_cells = sum(len(row) for row in table)
        assert_equal(num_rows * num_cols, num_cells)

    def test_read_type_know_simple(self):
        fh = horror_fobj('simple.xls')
        table_set = XLSTableSet(fh)
        row_set = table_set.tables[0]
        row = list(row_set.sample)[1]
        types = [c.type for c in row]
        assert_equal(types, [DateType(None), FloatType(), StringType()])

    def test_bad_first_sheet(self):
        # First sheet appears to have no cells
        fh = horror_fobj('problematic_first_sheet.xls')
        table_set = XLSTableSet(fh)
        tables = table_set.tables
        assert_equal(0, len(list(tables[0].sample)))
        assert_equal(1000, len(list(tables[1].sample)))


class ReadHtmlTest(unittest.TestCase):
    def test_read_real_html(self):
        fh = horror_fobj('html.html')
        table_set = HTMLTableSet(fh)
        row_set = table_set.tables[0]
        assert_equal(200, len(list(row_set)))
        row = list(row_set.sample)[0]
        assert_equal(row[0].value.strip(), 'HDI Rank')
        assert_equal(row[1].value.strip(), 'Country')
        assert_equal(row[4].value.strip(), '2010')

    def test_read_span_html(self):
        fh = horror_fobj('rowcolspan.html')
        table_set = HTMLTableSet(fh)
        row_set = table_set.tables[0]

        magic = {}
        for y, row in enumerate(row_set):
            for x, cell in enumerate(row):
                magic[(x, y)] = cell.value

        tests = {(0, 0): '05',
                 (0, 2): '25',
                 (0, 3): '',
                 (1, 3): '36',
                 (1, 6): '66',
                 (4, 7): '79',
                 (4, 8): '89'}

        for test in tests:
            assert_equal(magic[test], tests[test])

    def test_that_outer_table_contains_nothing(self):
        fh = horror_fobj('complex.html')
        tables = {}
        for table in HTMLTableSet(fh).tables:
            tables[table.name] = table

        # outer_table should contain no meaningful data
        outer_table = list(tables['Table 2 of 2'])
        assert_equal(len(outer_table), 1)
        assert_equal(len(outer_table[0]), 1)
        assert_equal(outer_table[0][0].value.
                     replace(" ", "").
                     replace("\n", ""),
                     "headfootbody")

    def test_that_inner_table_contains_data(self):
        fh = horror_fobj('complex.html')
        tables = {}
        for table in HTMLTableSet(fh).tables:
            tables[table.name] = table

        inner_table = tables['Table 1 of 2']
        cell_values = []
        for row in inner_table:
            for cell in row:
                cell_values.append(cell.value)
        assert_equal(['head', 'body', 'foot'], cell_values)

    def test_rowset_as_schema(self):
        from StringIO import StringIO as sio
        ts = CSVTableSet(sio('''name,dob\nmk,2012-01-02\n'''))
        rs = ts.tables[0]
        jts = rowset_as_jts(rs).as_dict()
        assert_equal(jts['fields'], [
            {'type': 'string', 'id': u'name', 'label': u'name'},
            {'type': 'date', 'id': u'dob', 'label': u'dob'}])

    def test_html_table_name(self):
        fh = horror_fobj('html.html')
        table_set = HTMLTableSet(fh)
        assert_equal('Table 1 of 3', table_set.tables[0].name)
        assert_equal('Table 2 of 3', table_set.tables[1].name)
        assert_equal('Table 3 of 3', table_set.tables[2].name)


class ReadPdfTest(unittest.TestCase):
    def setUp(self):
        with horror_fobj('simple.pdf') as fh:
            try:
                PDFTableSet(fh)
            except ImportError:
                # Optional library isn't installed. Skip the tests.
                raise SkipTest("pdftables is not installed, skipping PDF tests")

    def test_read_simple_pdf(self):
        with horror_fobj('simple.pdf') as fh:
            table_set = PDFTableSet(fh)
        assert_equal(1, len(list(table_set.tables)))

    def test_pdf_names(self):
        with horror_fobj('simple.pdf') as fh:
            table_set = PDFTableSet(fh)
        assert_equal('Table 1 of 1 on page 1 of 1',
                     table_set.tables[0].name)

########NEW FILE########
__FILENAME__ = test_rowset
# -*- coding: utf-8 -*-

import unittest
from . import horror_fobj
from messytables.any import any_tableset


class TestRowSet(unittest.TestCase):
    def test_repr_ascii_not_unicode(self):
        """
        __repr__ must return a str (not unicode), see object.__repr__(self) in
        http://docs.python.org/2/reference/datamodel.html
        """
        fh = horror_fobj('unicode_sheet_name.xls')
        table_set = any_tableset(fh, extension='xls')

        x = repr(table_set.tables)
        self.assertTrue(isinstance(x, str))

########NEW FILE########
__FILENAME__ = test_stream
# -*- coding: utf-8 -*-
import unittest
import urllib2
import requests
import StringIO

from . import horror_fobj
from nose.tools import assert_equal
import httpretty

from messytables import CSVTableSet, XLSTableSet


class StreamInputTest(unittest.TestCase):
    @httpretty.activate
    def test_http_csv(self):
        url = 'http://www.messytables.org/static/long.csv'
        httpretty.register_uri(
            httpretty.GET, url,
            body=horror_fobj('long.csv').read(),
            content_type="application/csv")
        fh = urllib2.urlopen(url)
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(4000, len(data))

    @httpretty.activate
    def test_http_csv_requests(self):
        url = 'http://www.messytables.org/static/long.csv'
        httpretty.register_uri(
            httpretty.GET, url,
            body=horror_fobj('long.csv').read(),
            content_type="application/csv")
        r = requests.get(url, stream=True)
        # no full support for non blocking version yet, use urllib2
        fh = StringIO.StringIO(r.raw.read())
        table_set = CSVTableSet(fh, encoding='utf-8')
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(4000, len(data))

    @httpretty.activate
    def test_http_csv_encoding(self):
        url = 'http://www.messytables.org/static/utf-16le_encoded.csv'
        httpretty.register_uri(
            httpretty.GET, url,
            body=horror_fobj('utf-16le_encoded.csv').read(),
            content_type="application/csv")
        fh = urllib2.urlopen(url)
        table_set = CSVTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(328, len(data))

    @httpretty.activate
    def test_http_xls(self):
        url = 'http://www.messytables.org/static/simple.xls'
        httpretty.register_uri(
            httpretty.GET, url,
            body=horror_fobj('simple.xls').read(),
            content_type="application/ms-excel")
        fh = urllib2.urlopen(url)
        table_set = XLSTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(7, len(data))

    @httpretty.activate
    def test_http_xlsx(self):
        url = 'http://www.messytables.org/static/simple.xlsx'
        httpretty.register_uri(
            httpretty.GET, url,
            body=horror_fobj('simple.xlsx').read(),
            content_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        fh = urllib2.urlopen(url)
        table_set = XLSTableSet(fh)
        row_set = table_set.tables[0]
        data = list(row_set)
        assert_equal(7, len(data))

########NEW FILE########
__FILENAME__ = test_tableset
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import unittest
from . import horror_fobj
from messytables.any import any_tableset
from messytables.core import RowSet


class TestTableSet(unittest.TestCase):
    def test_get_item(self):
        fh = horror_fobj('simple.xls')
        table_set = any_tableset(fh, extension='xls')

        self.assertTrue(isinstance(table_set['simple.csv'], RowSet))
        self.assertRaises(KeyError, lambda: table_set['non-existent'])

        # TODO: It would be good if we could manipulate a tableset to have
        # multiple row sets of the same name, then enable the following test.

        # self.assertRaises(Error, lambda: table_set['duplicated-name'])

########NEW FILE########
__FILENAME__ = test_unit
# -*- coding: utf-8 -*-
import unittest

from messytables import dateparser, Cell


class DateParserTest(unittest.TestCase):
    def test_date_regex(self):
        assert dateparser.is_date('2012 12 22')
        assert dateparser.is_date('2012/12/22')
        assert dateparser.is_date('2012-12-22')
        assert dateparser.is_date('22.12.2012')
        assert dateparser.is_date('12 12 22')
        assert dateparser.is_date('22 Dec 2012')
        assert dateparser.is_date('2012 12 22 13:17')
        assert dateparser.is_date('2012 12 22 T 13:17')


class CellReprTest(unittest.TestCase):
    def test_repr_ok(self):
        repr(Cell(value=u"\xa0"))

########NEW FILE########

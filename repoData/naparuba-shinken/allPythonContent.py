__FILENAME__ = shinken-arbiter
shinken-arbiter
########NEW FILE########
__FILENAME__ = shinken-broker
shinken-broker
########NEW FILE########
__FILENAME__ = shinken-poller
shinken-poller
########NEW FILE########
__FILENAME__ = shinken-reactionner
shinken-reactionner
########NEW FILE########
__FILENAME__ = shinken-receiver
shinken-receiver
########NEW FILE########
__FILENAME__ = shinken-scheduler
shinken-scheduler
########NEW FILE########
__FILENAME__ = cli
#!/usr/bin/env python

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


import os

from shinken.objects import Host
from shinken.log import logger, cprint

# Will be populated by the shinken CLI command
CONFIG = None



############# ********************        SERVE           ****************###########
def serve(port):
    port = int(port)
    logger.info("Serving documentation at port %s" % port)
    import SimpleHTTPServer
    import SocketServer
    doc_dir   = CONFIG['paths']['doc']
    html_dir  = os.path.join(doc_dir, 'build', 'html')
    os.chdir(html_dir)
    try:
        Handler = SimpleHTTPServer.SimpleHTTPRequestHandler
        httpd = SocketServer.TCPServer(("", port), Handler)
        httpd.serve_forever()
    except KeyboardInterrupt:
        pass
    except Exception, exp:
        logger.error(exp)

def do_desc(cls='host'):
    properties = Host.properties
    prop_names = properties.keys()
    prop_names.sort()
    for k in prop_names:
        v = properties[k]
        if v.has_default:
            print k, '(%s)' % v.default
        else:
            print k



exports = {
    do_desc : {
        'keywords': ['desc'],
        'args': [
            {'name' : '--cls', 'default':'host', 'description':'Object type to describe'},

            ],
        'description': 'List this object type properties'
        },
    }

########NEW FILE########
__FILENAME__ = cli
#!/usr/bin/env python

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


import os

from shinken.log import logger, cprint

# Will be populated by the shinken CLI command
CONFIG = None



############# ********************        SERVE           ****************###########
def serve(port):
    port = int(port)
    logger.info("Serving documentation at port %s" % port)
    import SimpleHTTPServer
    import SocketServer
    doc_dir   = CONFIG['paths']['doc']
    html_dir  = os.path.join(doc_dir, 'build', 'html')
    os.chdir(html_dir)
    try:
        Handler = SimpleHTTPServer.SimpleHTTPRequestHandler
        httpd = SocketServer.TCPServer(("", port), Handler)
        httpd.serve_forever()
    except KeyboardInterrupt:
        pass
    except Exception, exp:
        logger.error(exp)

def do_serve(port='8080'):
    if port is None:
        port = '8080'
    logger.debug("WILL CALL serve with %s" % port)
    serve(port)




################" *********************** COMPILE *************** ##################
def _compile():
    try:
        from sphinx import main
    except ImportError:
        logger.error('Cannot import the sphinx lib, please install it')
        return
    doc_dir     = CONFIG['paths']['doc']
    html_dir = os.path.join(doc_dir, 'build', 'html')
    doctrees_dir = os.path.join(doc_dir, 'build', 'doctrees')
    source_dir = os.path.join(doc_dir, 'source')

    try:
        s = 'sphinx-build -b html -d %s %s %s' % (doctrees_dir, source_dir, html_dir)
        args = s.split(' ')
        main(args)
    except Exception, exp:
        logger.error(exp)
    return

def do_compile():
    logger.debug("CALL compile")
    _compile()


exports = {
    do_serve : {
        'keywords': ['doc-serve'],
        'args': [
            {'name' : '--port', 'default':'8080', 'description':'Port to expose the http doc. Default to 8080'},

            ],
        'description': 'Publish the online doc on this server'
        },

    do_compile  : {'keywords': ['doc-compile'], 'args': [],
                  'description': 'Compile the doc before enabling it online'
                  },
    }

########NEW FILE########
__FILENAME__ = cli
#!/usr/bin/env python

# Copyright (C) 2009-2014:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


import pycurl
import os
import sys
import stat
import json
import tempfile
import tarfile
import urllib
import shutil
from StringIO import StringIO
from shinken.log import logger, cprint

# Will be populated by the shinken CLI command
CONFIG = None



############# ********************        PUBLISH           ****************###########

def read_package_json(fd):
    buf = fd.read()
    fd.close()
    buf = buf.decode('utf8', 'ignore')
    try:
        package_json = json.loads(buf)
    except ValueError, exp:
        logger.error("Bad package.json file : %s" % exp)
        sys.exit(2)
    if not package_json:
        logger.error("Bad package.json file")
        sys.exit(2)
    return package_json




def create_archive(to_pack):
    # First try to look if the directory we are trying to pack is valid
    to_pack = os.path.abspath(to_pack)
    if not os.path.exists(to_pack):
        logger.error("Error : the directory to pack is missing %s" % to_pack)
        sys.exit(2)
    logger.debug("Preparing to pack the directory %s" % to_pack)
    package_json_p = os.path.join(to_pack, 'package.json')
    if not os.path.exists(package_json_p):
        logger.error("Error : Missing file %s" % package_json_p)
        sys.exit(2)
    package_json = read_package_json(open(package_json_p))

    name = package_json.get('name', None)
    if not name:
        logger.error('Missing name entry in the package.json file. Cannot pack')
        sys.exit(2)


    # return True for files we want to exclude
    def tar_exclude_filter(f):
        # if the file start with .git, we bail out
        # Also ending with ~ (Thanks emacs...)
        if f.startswith('./.git') or f.startswith('.git'):
            return True
        if f.endswith('~'):
            return True
        return False

    # Now prepare a destination file
    tmp_dir  = tempfile.gettempdir()
    tmp_file = os.path.join(tmp_dir, name+'.tar.gz')
    tar = tarfile.open(tmp_file, "w:gz")
    os.chdir(to_pack)
    tar.add(".",arcname='.', exclude=tar_exclude_filter)
    tar.close()
    logger.debug("Saved file %s" % tmp_file)
    return tmp_file


def publish_archive(archive):
    # Now really publish it
    proxy = CONFIG['shinken.io']['proxy']
    api_key = CONFIG['shinken.io']['api_key']

    # Ok we will push the file with a 10s timeout
    c = pycurl.Curl()
    c.setopt(c.POST, 1)
    c.setopt(c.CONNECTTIMEOUT, 30)
    c.setopt(c.TIMEOUT, 300)
    if proxy:
        c.setopt(c.PROXY, proxy)
    c.setopt(c.URL, "http://shinken.io/push")
    c.setopt(c.HTTPPOST, [("api_key", api_key),
                          ("data",
                           (c.FORM_FILE, str(archive),
                            c.FORM_CONTENTTYPE, "application/x-gzip"))
                          ])
    response = StringIO()
    c.setopt(pycurl.WRITEFUNCTION, response.write)
    c.setopt(c.VERBOSE, 1)
    try:
        c.perform()
    except pycurl.error, exp:
        logger.error("There was a critical error : %s" % exp)
        return
    r = c.getinfo(pycurl.HTTP_CODE)
    c.close()
    if r != 200:
        logger.error("There was a critical error : %s" % response.getvalue())
        sys.exit(2)
    else:
        ret  = json.loads(response.getvalue().replace('\\/', '/'))
        status = ret.get('status')
        text   = ret.get('text')
        if status == 200:
            logger.log(text)
        else:
            logger.error(text)


def do_publish(to_pack='.'):
    logger.debug("WILL CALL PUBLISH.py with %s" % to_pack)
    archive = create_archive(to_pack)
    publish_archive(archive)




################" *********************** SEARCH *************** ##################
def search(look_at):
    # Now really publish it
    proxy = CONFIG['shinken.io']['proxy']
    api_key = CONFIG['shinken.io']['api_key']

    # Ok we will push the file with a 10s timeout
    c = pycurl.Curl()
    c.setopt(c.POST, 0)
    c.setopt(c.CONNECTTIMEOUT, 30)
    c.setopt(c.TIMEOUT, 300)
    if proxy:
        c.setopt(c.PROXY, proxy)

    args = {'keywords':','.join(look_at)}
    c.setopt(c.URL, str('shinken.io/searchcli?'+urllib.urlencode(args)))
    response = StringIO()
    c.setopt(pycurl.WRITEFUNCTION, response.write)
    #c.setopt(c.VERBOSE, 1)
    try:
        c.perform()
    except pycurl.error, exp:
        logger.error("There was a critical error : %s" % exp)
        return

    r = c.getinfo(pycurl.HTTP_CODE)
    c.close()
    if r != 200:
        logger.error("There was a critical error : %s" % response.getvalue())
        sys.exit(2)
    else:
        ret  = json.loads(response.getvalue().replace('\\/', '/'))
        status = ret.get('status')
        result   = ret.get('result')
        if status != 200:
            logger.log(result)
            return []
        return result



def print_search_matches(matches):
    if len(matches) == 0:
        logger.warning("No match founded in shinken.io")
        return
    # We will sort and uniq results (maybe we got a all search
    # so we will have both pack&modules, but some are both
    ps = {}
    names = [p['name'] for p in matches]
    names = list(set(names))
    names.sort()
    
    for p in matches:
        name = p['name']
        ps[name] = p
    
    for name in names:
        p = ps[name]
        user_id = p['user_id']
        keywords = p['keywords']
        description = p['description']
        cprint('%s ' %  name , 'green', end='')
        cprint('(%s) [%s] : %s' %  (user_id, ','.join(keywords), description))




def do_search(*look_at):
    # test for  generic search 
    if  look_at == ('all',):
        matches = []
        look_at = ('pack',)
        matches += search(look_at)
        look_at = ('module',)
        matches += search(look_at)
    else:
        logger.debug("CALL SEARCH WITH ARGS %s" % str(look_at))
        matches = search(look_at)
    if matches == [] : print ('you are unlucky, use "shinken search all" for a complete list ')
    print_search_matches(matches)








################" *********************** INVENTORY *************** ##################
def inventor(look_at):
    # Now really publish it
    inventory = CONFIG['paths']['inventory']
    logger.debug("dumping inventory %s" % inventory)
    # get all sub-direcotries
 
    for d in os.listdir(inventory):
        if os.path.exists(os.path.join(inventory, d, 'package.json')):
            if not look_at or d in look_at:
                print d
            # If asked, dump the content.package content
            if look_at or d in look_at:
                content_p = os.path.join(inventory, d, 'content.json')
                if not os.path.exists(content_p):
                    logger.error('Missing %s file' % content_p)
                    continue
                try:
                    j = json.loads(open(content_p, 'r').read())
                except Exception, exp:
                    logger.error('Bad %s file "%s"' % (content_p, exp))
                    continue
                for d in j:
                    s = ''
                    if d['type'] == '5': # tar direcotry
                        s += '(d)'
                    else:
                        s += '(f)'
                    s += d['name']
                    print s


def do_inventory(*look_at):
    inventor(look_at)








####################   ***************** INSTALL ************ ###################

def _copytree(src, dst, symlinks=False, ignore=None):
    for item in os.listdir(src):
        s = os.path.join(src, item)
        d = os.path.join(dst, item)
        if os.path.isdir(s):
            if not os.path.exists(d):
                os.mkdir(d)
            _copytree(s, d, symlinks, ignore)
        else:
            shutil.copy2(s, d)

# Do a chmod -R +x
def _chmodplusx(d):
    for item in os.listdir(d):
        p = os.path.join(d, item)
        if os.path.isdir(p):
            _chmodplusx(p)
        else:
            st = os.stat(p)
            os.chmod(p, st.st_mode | stat.S_IEXEC | stat.S_IXGRP | stat.S_IXOTH)





def grab_package(pname):
    cprint('Grabbing : ' , end='')
    cprint('%s' %  pname, 'green')

    # Now really publish it
    proxy = CONFIG['shinken.io']['proxy']
    api_key = CONFIG['shinken.io']['api_key']

    # Ok we will push the file with a 5m timeout
    c = pycurl.Curl()
    c.setopt(c.POST, 0)
    c.setopt(c.CONNECTTIMEOUT, 30)
    c.setopt(c.TIMEOUT, 300)
    if proxy:
        c.setopt(c.PROXY, proxy)

    c.setopt(c.URL, str('shinken.io/grab/%s' % pname))
    response = StringIO()
    c.setopt(pycurl.WRITEFUNCTION, response.write)
    #c.setopt(c.VERBOSE, 1)
    try:
        c.perform()
    except pycurl.error, exp:
        logger.error("There was a critical error : %s" % exp)
        return ''

    r = c.getinfo(pycurl.HTTP_CODE)
    c.close()
    if r != 200:
        logger.error("There was a critical error : %s" % response.getvalue())
        sys.exit(2)
    else:
        ret = response.getvalue()
        logger.debug("CURL result len : %d " % len(ret))
        return ret



def grab_local(d):
    # First try to look if the directory we are trying to pack is valid
    to_pack = os.path.abspath(d)
    if not os.path.exists(to_pack):
        err = "Error : the directory to install is missing %s" % to_pack
        logger.error(err)
        raise Exception(err)

    package_json_p = os.path.join(to_pack, 'package.json')
    if not os.path.exists(package_json_p):
        logger.error("Error : Missing file %s" % package_json_p)
        sys.exit(2)
    package_json = read_package_json(open(package_json_p))

    pname = package_json.get('name', None)
    if not pname:
        err = 'Missing name entry in the package.json file. Cannot install'
        logger.error(err)
        raise Exception(err)

    # return True for files we want to exclude
    def tar_exclude_filter(f):
        # if the file start with .git, we bail out
        # Also ending with ~ (Thanks emacs...)
        if f.startswith('./.git'):
            return True
        if f.endswith('~'):
            return True
        return False

    # Now prepare a destination file
    tmp_file  = tempfile.mktemp()
    tar = tarfile.open(tmp_file, "w:gz")
    os.chdir(to_pack)
    tar.add(".",arcname='.', exclude=tar_exclude_filter)
    tar.close()
    fd = open(tmp_file, 'rb')
    raw = fd.read()
    fd.close()

    return (pname, raw)



def install_package(pname, raw):
    logger.debug("Installing the package %s (size:%d)" % (pname, len(raw)))
    if len(raw) == 0:
        logger.error('The package %s cannot be found' % pname)
        return
    tmpdir = os.path.join(tempfile.gettempdir(), pname)
    logger.debug("Unpacking the package into %s" % tmpdir)

    if os.path.exists(tmpdir):
        logger.debug("Removing previous tmp dir %s" % tmpdir)
        shutil.rmtree(tmpdir)
    logger.debug("Creating temporary dir %s" % tmpdir)
    os.mkdir(tmpdir)

    package_content = []

    # open a file with the content
    f = StringIO(raw)
    tar_file = tarfile.open(fileobj=f, mode="r")
    logger.debug("Tar file contents:")
    for i in tar_file.getmembers():
        path = i.name
        if path == '.':
            continue
        if path.startswith('/') or '..' in path:
            logger.error("SECURITY: the path %s seems dangerous!" % path)
            return
        # Adding all files into the package_content list
        package_content.append( {'name':i.name, 'mode':i.mode, 'type':i.type, 'size':i.size} )
        logger.debug("\t%s" % path)
    # Extract all in the tmpdir
    tar_file.extractall(tmpdir)
    tar_file.close()

    # Now we look at the package.json that will give us our name and co
    package_json_p = os.path.join(tmpdir, 'package.json')
    if not os.path.exists(package_json_p):
        logger.error("Error : bad archive : Missing file %s" % package_json_p)
        return None
    package_json = read_package_json(open(package_json_p))
    logger.debug("Package.json content %s " % package_json)

    modules_dir = CONFIG['paths']['modules']
    share_dir   = CONFIG['paths']['share']
    packs_dir   = CONFIG['paths']['packs']
    etc_dir     = CONFIG['paths']['etc']
    doc_dir     = CONFIG['paths']['doc']
    inventory_dir     = CONFIG['paths']['inventory']
    libexec_dir     = CONFIG['paths'].get('libexec', os.path.join(CONFIG['paths']['lib'], 'libexec'))
    test_dir   = CONFIG['paths'].get('test', '/__DONOTEXISTS__')
    for d in (modules_dir, share_dir, packs_dir, doc_dir, inventory_dir):
        if not os.path.exists(d):
            logger.error("The installation directory %s is missing!" % d)
            return

    # Now install the package from $TMP$/share/* to $SHARE$/*
    p_share  = os.path.join(tmpdir, 'share')
    logger.debug("TMPDIR:%s aahre_dir:%s pname:%s" %(tmpdir, share_dir, pname))
    if os.path.exists(p_share):
        logger.info("Installing the share package data")
        # shutil will do the create dir
        _copytree(p_share, share_dir)
        logger.info("Copy done in the share directory %s" % share_dir)


    logger.debug("TMPDIR:%s modules_dir:%s pname:%s" %(tmpdir, modules_dir, pname))
    # Now install the package from $TMP$/module/* to $MODULES$/pname/*
    p_module = os.path.join(tmpdir, 'module')
    if os.path.exists(p_module):
        logger.info("Installing the module package data")
        mod_dest = os.path.join(modules_dir, pname)
        if os.path.exists(mod_dest):
            logger.info("Removing previous module install at %s" % mod_dest)

            shutil.rmtree(mod_dest)
        # shutil will do the create dir
        shutil.copytree(p_module, mod_dest)
        logger.info("Copy done in the module directory %s" % mod_dest)


    p_doc  = os.path.join(tmpdir, 'doc')
    logger.debug("TMPDIR:%s doc_dir:%s pname:%s" %(tmpdir, doc_dir, pname))
    # Now install the package from $TMP$/doc/* to $MODULES$/doc/source/89_packages/pname/*
    if os.path.exists(p_doc):
        logger.info("Installing the doc package data")
        doc_dest = os.path.join(doc_dir, 'source', '89_packages', pname)
        if os.path.exists(doc_dest):
            logger.info("Removing previous doc install at %s" % doc_dest)

            shutil.rmtree(doc_dest)
        # shutil will do the create dir
        shutil.copytree(p_doc, doc_dest)
        logger.info("Copy done in the doc directory %s" % doc_dest)


    # Now install the pack from $TMP$/pack/* to $PACKS$/pname/*
    p_pack = os.path.join(tmpdir, 'pack')
    if os.path.exists(p_pack):
        logger.info("Installing the pack package data")
        pack_dest = os.path.join(packs_dir, pname)
        if os.path.exists(pack_dest):
            logger.info("Removing previous pack install at %s" % pack_dest)
            shutil.rmtree(pack_dest)
        # shutil will do the create dir
        shutil.copytree(p_pack, pack_dest)
        logger.info("Copy done in the pack directory %s" % pack_dest)

    # Now install the etc from $TMP$/etc/* to $ETC$/etc/*
    p_etc = os.path.join(tmpdir, 'etc')
    if os.path.exists(p_etc):
        logger.info("Merging the etc package data into your etc directory")
        # We don't use shutils because it NEED etc_dir to be non existant...
        # Come one guys..... cp is not as terrible as this...
        _copytree(p_etc, etc_dir)
        logger.info("Copy done in the etc directory %s" % etc_dir)


    # Now install the tests from $TMP$/tests/* to $TESTS$/tests/*
    # if the last one is specified on the configuration file (optionnal)
    p_tests = os.path.join(tmpdir, 'test')
    if os.path.exists(p_tests) and os.path.exists(test_dir):
        logger.info("Merging the test package data into your test directory")
        # We don't use shutils because it NEED etc_dir to be non existant...
        # Come one guys..... cp is not as terrible as this...
        logger.debug("COPYING %s into %s" % (p_tests, test_dir))
        _copytree(p_tests, test_dir)
        logger.info("Copy done in the test directory %s" % test_dir)

    # Now install the libexec things from $TMP$/libexec/* to $LIBEXEC$/*
    # but also chmod a+x the plugins copied
    p_libexec = os.path.join(tmpdir, 'libexec')
    if os.path.exists(p_libexec) and os.path.exists(libexec_dir):
        logger.info("Merging the libexec package data into your libexec directory")
        logger.debug("COPYING %s into %s" % (p_libexec, libexec_dir))
        # Before be sure all files in there are +x
        _chmodplusx(p_libexec)
        _copytree(p_libexec, libexec_dir)
        logger.info("Copy done in the libexec directory %s" % libexec_dir)


    # then samve the package.json into the inventory dir
    p_inv = os.path.join(inventory_dir, pname)
    if not os.path.exists(p_inv):
        os.mkdir(p_inv)
    shutil.copy2(package_json_p, os.path.join(p_inv, 'package.json'))
    # and the package content
    cont = open(os.path.join(p_inv, 'content.json'), 'w')
    cont.write(json.dumps(package_content))
    cont.close()
    
    # We now clean (rm) the tmpdir we don't need any more
    try:
        shutil.rmtree(tmpdir, ignore_errors=True)
        # cannot remove? not a crime
    except OSError:
        pass

    # THE END, output all is OK :D
    cprint('OK ', 'green', end='')
    cprint('%s' % pname)





def do_install(pname, local, download_only):
    raw = ''
    if local:
        pname, raw = grab_local(pname)

    if not local:
        raw = grab_package(pname)

    if download_only:
        tmpf = os.path.join(tempfile.gettempdir(), pname+'.tar.gz')
        try:
            f = open(tmpf, 'wb')
            f.write(raw)
            f.close()
            cprint('Download OK: %s' %  tmpf, 'green')
        except Exception, exp:
            logger.error("Package save fail: %s" % exp)
        return

    install_package(pname, raw)




exports = {
    do_publish : {
        'keywords': ['publish'],
        'args': [
            {'name' : 'to_pack', 'default':'.', 'description':'Package directory. Default to .'},

            ],
        'description': 'Publish a package on shinken.io. Valid api key required'
        },

    do_search  : {'keywords': ['search'], 'args': [],
                  'description': 'Search a package on shinken.io by looking at its keywords'
                  },
    do_install : {
        'keywords': ['install'],
        'args': [
            {'name' : 'pname', 'description':'Package to install'},
            {'name' : '--local', 'description':'Use a local directory instead of the shinken.io version', 'type': 'bool'},
            {'name' : '--download-only', 'description':'Only download the package', 'type': 'bool'},
            ],
        'description' : 'Grab and install a package from shinken.io'
        },
    do_inventory  : {'keywords': ['inventory'], 'args': [],
       'description': 'List locally installed packages'
       },
    }

########NEW FILE########
__FILENAME__ = PythonClient
#!/usr/bin/env python

#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
#
import csv
import time
import sys
sys.path.append('gen-py')

try:
    from org.shinken_monitoring.tsca import StateService
    from org.shinken_monitoring.tsca.ttypes import *
except:
    print "Can't import tsca stub."
    print "Have you run thrift --gen py ../../../../shinken/modules/tsca/tsca.thrift ?"
    sys.exit(1)

from thrift import Thrift
from thrift.transport import TSocket
from thrift.transport import TTransport
from thrift.protocol import TBinaryProtocol

try:

    # Make socket
    transport = TSocket.TSocket('localhost', 9090)

    # Buffering is critical. Raw sockets are very slow
    transport = TTransport.TBufferedTransport(transport)

    # Wrap in a protocol
    protocol = TBinaryProtocol.TBinaryProtocol(transport)

    # Create a client to use the protocol encoder
    client = StateService.Client(protocol)

    # Connect!
    transport.open()
    # Thrift server wait a list of list whith the following args:
    #      '''
    #      Read the list result
    #       Value n1: Timestamp
    #       Value n2: Hostname
    #       Value n3: Service
    #       Value n4: Return Code
    #       Value n5: Output
    #      '''
    states_list = []
    data = dataArgs()
    cr = csv.reader(open(sys.argv[1], "rb"))
    for elt in cr:
        trace = State()
        trace.timestamp = long(round(time.time()))
        trace.hostname = elt[0]
        trace.serv = elt[1]
        trace.output = elt[2]
        trace.rc = ReturnCode.OK
        states_list.append(trace)
    data.states = states_list
    client.submit_list(data)
    # Close!
    transport.close()

except Thrift.TException, tx:
    print '%s' % tx.message

########NEW FILE########
__FILENAME__ = zmq_broker_client
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2013:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#    Thomas Cellerier, thomascellerier@gmail.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This is an example client for the zmq_broker module.
# This will listen for notifications using the given
# serialization method on the given ZeroMQ endpoint
# using the given ZeroMQ topic filter.
#
# Examples:
# python zmq_broker_client.py "json" "tcp://127.0.0.1:12345" "host"
# python zmq_broker_client.py "msgpack" "ipc:///tmp/shinken_pub" ""
# python zmq_broker_client.py "json" "tcp://172.23.2.189:9067" "log"
import zmq
import sys

# Usage
if len(sys.argv) > 1:
    if sys.argv[1] == "--help" or sys.argv[1] == "-h":
        print("Usage: python zmq_broker_client.py [json|msgpack] [<zmq endpoint>] [<zmq topic>]")
        sys.exit(-1)

# Serialization method
method = ""
if len(sys.argv) < 2 or sys.argv[1] == "json":
	import json
	method = "json"
elif sys.argv[1] == "msgpack":
	import msgpack
	method = "msgpack"
else:
	print("Invalid serialization method.")
	sys.exit(-1)

# ZeroMQ endpoint
sub_endpoint = "tcp://127.0.0.1:12345"
if len(sys.argv) > 2:
	sub_endpoint = sys.argv[2]

# ZeroMQ Suscription Topic
topic = ""
if len(sys.argv) > 3:
	topic = sys.argv[3]
	
# Subscribe
context = zmq.Context()
s_sub = context.socket(zmq.SUB)
s_sub.setsockopt(zmq.SUBSCRIBE, topic)
s_sub.connect(sub_endpoint)
print("Listening for shinken notifications.")

# Process incoming messages
while True:
	topic = s_sub.recv()
	print("Got msg on topic: " + topic)
	data = s_sub.recv()
	if method == "json":
		json_data = json.loads(data)
		pretty_msg = json.dumps(json_data, sort_keys=True, indent=4)
		print(pretty_msg)
	elif method == "msgpack":
		msg = msgpack.unpackb(data, use_list=False)
		print(msg)
s_sub.close()
context.term()


########NEW FILE########
__FILENAME__ = checkmodule
#!/usr/bin/env python
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    David GUENAULT, dguenault@monitoring-fr.org
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import getopt


def main(argv):
    try:
        opts, args = getopt.getopt(argv, "m:")
        ret = 0
        for o, a in opts:
            if o == "-m":
                try:
                    exec("import " + a)
                    print "OK"
                except:
                    print "KO"
                    ret = 2
    except:
        ret = 1
    sys.exit(ret)

if __name__ == "__main__":
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = splitlivelogs
#!/usr/bin/env python
# Copyright (C) 2009-2011:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This script will take the sqlite database of the livestatus module and
split up the contents in single datafiles (1 for each day of data found).
"""

import sys
import optparse
import os

sys.path.append("..")
sys.path.append("../shinken")
sys.path.append("../../shinken")
sys.path.append("../../../shinken")
#sys.path.append("../bin")
#sys.path.append(os.path.abspath("bin"))


#import shinken
from shinken.modules.livestatus_broker.livestatus_db import LiveStatusDb

parser = optparse.OptionParser(
    "%prog [options] -d database [-a archive]")
parser.add_option('-d', '--database', action='store',
                  dest="database",
                  help="The sqlite datafile of your livestatus module")
parser.add_option('-a', '--archive', action='store',
                  dest="archive_path",
                  help="(optional) path to the archive directory")

opts, args = parser.parse_args()

if not opts.database:
    parser.error("Requires at least the database file (option -d/--database")
if not opts.archive_path:
    opts.archive_path = os.path.join(os.path.dirname(opts.database), 'archives')
    pass

# Protect for windows multiprocessing that will RELAUNCH all
if __name__ == '__main__':
    if os.path.exists(opts.database):
        try:
            os.stat(opts.archive_path)
        except:
            os.mkdir(opts.archive_path)
        dbh = LiveStatusDb(opts.database, opts.archive_path, 3600)
        dbh.log_db_do_archive()
        dbh.close()
    else:
        print "database %s does not exist" % opts.database


# For perf tuning:

########NEW FILE########
__FILENAME__ = sql2mdb
#!/usr/bin/python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2012 :
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Olivier Hanesse, olivier.hanesse@gmail.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import time
import datetime
import os
import string
import re

try:
    import shinken
except ImportError:
    # If importing shinken fails, try to load from current directory
    # or parent directory to support running without installation.
    # Submodules will then be loaded from there, too.
    import imp
    imp.load_module('shinken', *imp.find_module('shinken', [os.path.realpath("."), os.path.realpath(".."), os.path.join(os.path.abspath(os.path.dirname(sys.argv[0])), "..")]))

from shinken.objects.config import Config
from shinken.objects.module import Module
from shinken.objects.item import Item
from shinken.modulesmanager import ModulesManager
from shinken.basemodule import BaseModule
from shinken.log import logger
from shinken.modules.logstore_sqlite import get_instance as get_instance_sqlite
from shinken.modules.logstore_mongodb import get_instance as get_instance_mongodb
from shinken.modules.logstore_sqlite import LiveStatusLogStoreError
from shinken.modules.livestatus_broker.log_line import Logline

class Dummy:
    def add(self, o):
        pass

def row_factory(cursor, row):
    """Handler for the sqlite fetch method."""
    return Logline(sqlite_cursor=cursor.description, sqlite_row=row)


class Converter(object):
    def __init__(self, file):

        logger.load_obj(Dummy())
        self.conf = Config()
        buf = self.conf.read_config([file])
        raw_objects = self.conf.read_config_buf(buf)
        self.conf.create_objects_for_type(raw_objects, 'arbiter')
        self.conf.create_objects_for_type(raw_objects, 'module')
        self.conf.early_arbiter_linking()
        self.conf.create_objects(raw_objects)
        for mod in self.conf.modules:
            if mod.module_type == 'logstore_sqlite':
                self.mod_sqlite = get_instance_sqlite(mod)
                self.mod_sqlite.init()
            if mod.module_type == 'logstore_mongodb':
                self.mod_mongodb = get_instance_mongodb(mod)


if __name__ == '__main__':
    if (len(sys.argv) < 2):
        print "usage: sql2mdb shinken-specifig.cfg"
        sys.exit(1)
    conv = Converter(sys.argv[1])
    print conv.mod_mongodb
    print conv.mod_sqlite
    print conv.mod_sqlite.archive_path
    conv.mod_sqlite.use_aggressie_sql = False

    try:
        conv.mod_sqlite.open()
    except Exception, e:
        print "problem opening the sqlite db", e
        sys.exit(1)
    try:
        conv.mod_mongodb.open()
    except Exception, e:
        conv.mod_sqlite.close()
        print "problem opening the mongodb", e
        sys.exit(1)

    for dateobj, handle, archive, fromtime, totime in conv.mod_sqlite.log_db_relevant_files(0, time.time()):
        try:
            if handle == "main":
                print "attach %s" % archive
                dbresult = conv.mod_sqlite.execute('SELECT * FROM logs', [], row_factory)
            else:
                conv.mod_sqlite.commit()
                print "attach %s" % archive
                conv.mod_sqlite.execute_attach("ATTACH DATABASE '%s' AS %s" % (archive, handle))
                dbresult = conv.mod_sqlite.execute('SELECT * FROM %s.logs' % (handle,), [], row_factory)
                conv.mod_sqlite.execute("DETACH DATABASE %s" % handle)
            # now we have the data of one day
            for res in dbresult:
                values = res.as_dict()
                try:
                    conv.mod_mongodb.db[conv.mod_mongodb.collection].insert(values)
                except Exception, e:
                    print "problem opening the mongodb", e
                    time.sleep(5)
                    conv.mod_mongodb.db[conv.mod_mongodb.collection].insert(values)
            print "wrote %d records" % len(dbresult)

        except LiveStatusLogStoreError, e:
            print "An error occurred:", e.args[0]
    conv.mod_sqlite.close()
    conv.mod_mongodb.close()


########NEW FILE########
__FILENAME__ = sendmailservice
#!/usr/bin/env python
#   Autor: David Hannequin <david.hannequin@gmail.com>
#   Date: 24 Oct 2011

import sys
import os
import getopt
import argparse
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

TEXT_template = """***** Shinken Notification *****

Notification: %(notify)s

Service: %(service)s
Host: %(hostname)s
Address: %(hostaddress)s
State: %(state)s
Date/Time: %(datetime)s
Additional Info: %(output)s
"""

HTML_template = '''<html>
<head></head><body>
<style type="text/css">
.recovery { color:ForestGreen }
.acknowledgement { color:ForestGreen }
.problem { color: red }
.ok { color:ForestGreen }
.critical { color:red }
.warning { color:orange }
.unknown { color:gray }
.bold { font-weight:bold }
</style>
<strong> ***** Shinken Notification ***** </strong><br><br>
Notification: <span class="%(notify)s bold">%(notify)s</span><br><br>
State: <span class="%(state)s bold">%(state)s</span><br><br>
Service: %(service)s <br>
Host: %(hostname)s <br>
Address: %(hostaddress)s <br>
Date/Time: %(datetime)s<br>
Additional Info : %(output)s
</body></html>
'''

parser = argparse.ArgumentParser()
parser.add_argument('-n', '--notification', default='unknown', dest='notify')
parser.add_argument('-s', '--servicedesc', default='unknown', dest='service')
parser.add_argument('-H', '--hostname', default='unknown')
parser.add_argument('-a', '--hostaddress', default='unknown')
parser.add_argument('-r', '--servicestate', default='unknown', dest='state')
parser.add_argument('-i', '--shortdatetime', default='unknown', dest='datetime')
parser.add_argument('-o', '--output', default='')
group = parser.add_argument_group('Mail options')
group.add_argument('-t', '--to')
group.add_argument('-S', '--sender')
group.add_argument('--server', default='localhost')
group.add_argument('--port', default=smtplib.SMTP_PORT, type=int)
args = parser.parse_args()

subject = ("** %(notify)s alert - %(hostname)s/%(service)s is %(state)s **"
           % vars(args))

## Create message container - the correct MIME type is multipart/alternative.
msg = MIMEMultipart('alternative')
msg['Subject'] = subject
msg['From'] = args.sender
msg['To'] = args.to

# Create the body of the message (a plain-text and an HTML version).
#
# According to RFC 2046, the last part of a multipart message, in this
# case the HTML message, is best and preferred.
#
# :fixme: need to encode the body if not ascii, see
# http://mg.pov.lt/blog/unicode-emails-in-python.html for a nice
# solution.
#
msg.attach(MIMEText(TEXT_template % vars(args), 'plain'))
# :fixme: need to html-escape all values and encode the body
msg.attach(MIMEText(HTML_template % vars(args), 'html'))

# Send the message via local SMTP server.
s = smtplib.SMTP(args.server, args.port)
# sendmail function takes 3 arguments: sender's address, recipient's address
# and message to send - here it is sent as one string.
s.sendmail(args.sender, args.to, msg.as_string())
s.quit()

########NEW FILE########
__FILENAME__ = nsca_client
# This is a very quick and dirty code for David so he can work on its
# sikuli agent and report as nsca the results.
#
# This need to be clean a lot, it's still a server and should be a
# client class :) I can do it after my "new baby holidays" are
# finished ;)
#
# J. Gabes

import time
import select
import socket
import struct
import random


def decrypt_xor(data, key):
    keylen = len(key)
    crypted = [chr(ord(data[i]) ^ ord(key[i % keylen]))
               for i in xrange(len(data))]
    return ''.join(crypted)


class NSCA_client():

    def __init__(self, host, port, encryption_method, password):
        self.host = host
        self.port = port
        self.encryption_method = encryption_method
        self.password = password
        self.rng = random.Random(password)

    def get_objects(self):
        """
        This is the main function that is called in the CONFIGURATION
        phase.
        """
        print "[Dummy] ask me for objects to return"
        r = {'hosts': []}
        h = {'name': 'dummy host from dummy arbiter module',
             'register': '0',
             }
        r['hosts'].append(h)
        print "[Dummy] Returning to Arbiter the hosts:", r
        return r

    def send_init_packet(self, socket):
        '''
        Build an init packet
         00-127: IV
         128-131: unix timestamp
        '''
        iv = ''.join([chr(self.rng.randrange(256)) for i in xrange(128)])
        init_packet = struct.pack("!128sI", iv, int(time.mktime(time.gmtime())))
        socket.send(init_packet)
        return iv

    def read_check_result(self, data, iv):
        '''
        Read the check result
         00-01: Version
         02-05: CRC32
         06-09: Timestamp
         10-11: Return code
         12-75: hostname
         76-203: service
         204-715: output of the plugin
         716-720: padding
        '''
        if len(data) != 720:
            return None

        if self.encryption_method == 1:
            data = decrypt_xor(data, self.password)
            data = decrypt_xor(data, iv)

        (version, pad1, crc32, timestamp, rc, hostname_dirty, service_dirty,
         output_dirty, pad2) = struct.unpack("!hhIIh64s128s512sh", data)
        hostname = hostname_dirty.partition("\0", 1)[0]
        service = service_dirty.partition("\0", 1)[0]
        output = output_dirty.partition("\0", 1)[0]
        return (timestamp, rc, hostname, service, output)

    def post_command(self, timestamp, rc, hostname, service, output):
        '''
        Send a check result command to the arbiter
        '''
        if len(service) == 0:
            extcmd = ("[%lu] PROCESS_HOST_CHECK_RESULT;%s;%d;%s\n"
                      % (timestamp, hostname, rc, output))
        else:
            extcmd = ("[%lu] PROCESS_SERVICE_CHECK_RESULT;%s;%s;%d;%s\n"
                      % (timestamp, hostname, service, rc, output))

        print "want to send", extcmd

        #e = ExternalCommand(extcmd)
        #self.from_q.put(e)


    def main(self):
        """
        This is the main loop of the process when in 'external' mode.
        """
        #self.set_exit_handler()
        self.interrupted = False
        backlog = 5
        size = 8192
        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        #server.setblocking(0)
        server.connect((self.host, self.port))
        #server.listen(backlog)
        input = [server]
        databuffer = {}
        IVs = {}

        init = server.recv(size)
        print "got init", init

        #init_packet = struct.pack("!128sI",iv,int(time.mktime(time.gmtime())))
        (iv, t) = struct.unpack("!128sI", init)
        print "IV", iv
        print "T", t

        version = 0
        pad1 = 0
        crc32 = 0
        timestamp = int(time.time())
        rc = 2
        hostname_dirty = "moncul"
        service_dirty = "fonctionnne"
        output_dirty = "blablalba"
        pad2 = 0
        '''
        Read the check result
         00-01: Version
         02-05: CRC32
         06-09: Timestamp
         10-11: Return code
         12-75: hostname
         76-203: service
         204-715: output of the plugin
         716-720: padding
        '''
        init_packet = struct.pack(
            "!hhIIh64s128s512sh",
            version, pad1, crc32, timestamp, rc, hostname_dirty,
            service_dirty, output_dirty, pad2)
        print "Create packent len", len(init_packet)
        #(version, pad1, crc32, timestamp, rc, hostname_dirty, service_dirty,
        # output_dirty, pad2) = struct.unpack("!hhIIh64s128s512sh",data)

        data = decrypt_xor(init_packet, iv)
        data = decrypt_xor(data, self.password)

        server.send(data)
        raise SystemExit(0)

        while not self.interrupted:
            print "Loop"
            inputready, outputready, exceptready = select.select(input, [], [], 1)

            for s in inputready:
                if s == server:
                    # handle the server socket
                    #client, address = server.accept()
                    iv = self.send_init_packet(client)
                    IVs[client] = iv
                    input.append(client)
                else:
                    # handle all other sockets
                    data = s.recv(size)
                    if s in databuffer:
                        databuffer[s] += data
                    else:
                        databuffer[s] = data
                    if len(databuffer[s]) == 720:
                        # end-of-transmission or an empty line was received
                        (timestamp, rc, hostname, service, output) = self.read_check_result(databuffer[s], IVs[s])
                        del databuffer[s]
                        del IVs[s]
                        self.post_command(timestamp, rc, hostname, service,
                                          output)
                        try:
                            s.shutdown(2)
                        except Exception, exp:
                            print exp
                        s.close()
                        input.remove(s)

if __name__ == "__main__":
    parser = optparse.OptionParser(
                      version="Python NSCA client version %s" % VERSION)
    parser.add_option("-H", "--hostname", default='localhost',
                      help="NSCA server IP (default: %default)")
    parser.add_option("-P", "--port", type="int", default='5667',
                      help="NSCA server port (default: %default)")
    parser.add_option("-e", "--encryption", default='1',
                      help=("Encryption mode used by NSCA server "
                            "(default: %default)"))
    parser.add_option("-p", "--password", default='helloworld',
                      help=("Password for encryption, should be the same as "
                            "NSCA server (default: %default)"))
    parser.add_option("-d", "--delimiter", default='\t',
                      help="Argument delimiter (defaults to the tab-character)")

    opts, args = parser.parse_args()

    if args:
        parser.error("does not take any positional arguments")

    nsca = NSCA_client(opts.hostname, opts.port, opts.encryption, opts.password)
    nsca.main()

########NEW FILE########
__FILENAME__ = check_shinken_load
#!/usr/bin/env python
#
#   Autors: David Hannequin <david.hannequin@gmail.com>,
#           Hartmut Goebel <h.goebel@crazy-compilers.com>
#   Date: 2012-07-12
#
# Requires: Python >= 2.7 or Python plus argparse
#

import os
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--warning', default='3,2,1')
    parser.add_argument('-c', '--critical', default='4,3,2')
    args = parser.parse_args()

    critical = map(float, args.critical.split(','))
    warning = map(float, args.warning.split(','))

    (cload1, cload5, cload15) = critical
    (wload1, wload5, wload15) = warning

    (load1, load5, load15) = os.getloadavg()

    if load1 >= cload1 or load5 >= cload5 or load15 >= cload15:
        print ('CRITICAL - Load average : %s,%s,%s|load1=%s;load5=%s;load15=%s'
               % (load1, load5, load15, load1, load5, load15))
        raise SystemExit(2)
    elif load1 >= wload1 or load5 >= wload5 or load15 >= wload15:
        print ('WARNING - Load average : %s,%s,%s|load1=%s;load5=%s;load15=%s'
               % (load1, load5, load15, load1, load5, load15))
        raise SystemExit(1)
    else:
        print ('OK - Load average : %s,%s,%s|load1=%s;load5=%s;load15=%s'
               % (load1, load5, load15, load1, load5, load15))
        raise SystemExit(0)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = check_shinken_mem
#!/usr/bin/env python
#
#   Autors: David Hannequin <david.hannequin@gmail.com>,
#           Hartmut Goebel <h.goebel@crazy-compilers.com>
#   Date: 2012-07-12
#
# Requires: Python >= 2.7 or Python plus argparse
# Platform: Linux
#

import argparse


def MemValues():
    """
    Read total mem, free mem and cached from /proc/meminfo

    This is linux-only.
    """
    for line in open('/proc/meminfo').readlines():
        if line.startswith('MemTotal:'):
            memTotal = line.split()[1]
        if line.startswith('MemFree:'):
            memFree = line.split()[1]
        if line.startswith('Cached:'):
            memCached = line.split()[1]
    # :fixme: fails if one of these lines is missing in /proc/meminfo
    return memTotal, memCached, memFree


def percentFreeMem():
    memTotal, memCached, memFree = MemValues()
    return (((int(memFree) + int(memCached)) * 100) / int(memTotal))


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--warning', default='80', type=int)
    parser.add_argument('-c', '--critical', default='90', type=int)
    args = parser.parse_args()

    critical = args.critical
    warning = args.warning

    pmemUsage = 100 - percentFreeMem()

    if pmemUsage >= critical:
        print ('CRITICAL - Memory usage: %2.1f%% |mem=%s' % (pmemUsage, pmemUsage))
        raise SystemExit(2)
    elif pmemUsage >= warning:
        print ('WARNING - Memory usage: %2.1f%% |mem=%s' % (pmemUsage, pmemUsage))
        raise SystemExit(1)
    else:
        print ('OK - Memory usage: %2.1f%% |mem=%s' % (pmemUsage, pmemUsage))
        raise SystemExit(0)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Shinken documentation build configuration file, created by
# sphinx-quickstart on Wed Nov 13 01:01:23 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../..'))
import shinken

# Fix for missing modules
class Mock(object):
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return Mock()

    def version_info(self):
        return [0,0]

    @classmethod
    def __getattr__(cls, name):
        if name in ('__file__', '__path__'):
            return '/dev/null'
        elif name[0] == name[0].upper():
            mockType = type(name, (), {})
            mockType.__module__ = __name__
            return mockType
        else:
            return Mock()

MOCK_MODULES = ['MySQLdb',
                '_mysql_exceptions',
                'cx_Oracle',
                'log',
                'pymongo',
                'pycurl',
                ]
for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = Mock()

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.pngmath', 'sphinx.ext.ifconfig', 'sphinx.ext.viewcode', 'sphinx.ext.graphviz', 'sphinx.ext.inheritance_diagram']

# Debian 6 do NOT have such extension
try:
   import sphinx.ext.mathjax
   extensions.append('sphinx.ext.mathjax')
except ImportError:
   pass


# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Shinken Manual'
copyright = u'2013, Shinken Team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.4'
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
language = 'en'

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#html_theme = 'default'
html_theme = 'sphinx_rtd_theme'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []
html_theme_path = ["../theme"]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Shinkendoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Shinken.tex', u'Shinken Documentation',
   u'Shinken Team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'shinken', u'Shinken Documentation',
     [u'Shinken Team'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Shinken', u'Shinken Documentation',
   u'Shinken Team', 'Shinken', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'Shinken'
epub_author = u'Shinken Team'
epub_publisher = u'Shinken Team'
epub_copyright = u'2013, Shinken Team'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = doc_shinken_scrapper
#!/usr/bin/python

from StringIO import StringIO
import os

import requests
from lxml import etree

#application_name = "shinken"
application_name = ""
url = "http://www.shinken-monitoring.org/wiki"
sitemap_url = url + "/start?do=index"
sitemap_ajax_url = url + "/lib/exe/ajax.php"
parser = etree.HTMLParser()

raw_res = requests.get(sitemap_url)

res = StringIO(raw_res.content)
tree = etree.parse(res, parser)

index = tree.find("//div[@id='index__tree']")


def parse_level(level, root=""):
    items = level.findall(".//li")
#    import pdb;pdb.set_trace()
    for item in items:
        title = item.find("./div//strong")
        if not title is None:
            new_root = root + ":" + title.text
            print "Browse namespace : %s" % new_root
            data = {'call': 'index',
                    'idx' : new_root,
                    }
            raw_ajax_res = requests.post(sitemap_ajax_url, data=data)
            ajax_res = StringIO(raw_ajax_res.content)
            ajax_parser = etree.HTMLParser()
            ajax_tree = etree.parse(ajax_res, ajax_parser)
#            print raw_ajax_res.content
            parse_level(ajax_tree, new_root)
#            http://www.shinken-monitoring.org/wiki/lib/exe/ajax.php
#            print title.getparent().getparent()
        else:
#            import pdb;pdb.set_trace()
            page_name = item.find("./div//a").text
            page_url = url + "/" + root + ":" + page_name + "?do=export_raw"
            page_raw_res = requests.get(page_url)
#            tmp_root = root.replace(":", "/")
            tmp_root = root
            if tmp_root.startswith(":"):
                tmp_root = tmp_root[1:]
            try:
                os.makedirs(os.path.join('pages', application_name, tmp_root).replace(":", "/"))
            except OSError, e:
                #print e
                pass
            file_name = os.path.join('pages', application_name, tmp_root, page_name + ".txt")
            file_name = file_name.replace(":", "/")
            replace_dict = find_media(page_raw_res.content)
            # Change links
            modified_page_raw = page_raw_res.content
            modified_page_raw = modified_page_raw.replace("official:official_", "official:")
            modified_page_raw = modified_page_raw.replace("official_", "official:")
            modified_page_raw = modified_page_raw.replace("/official/", ":%s:official:" % application_name)
            modified_page_raw = modified_page_raw.replace("[[official:", "[[%s:official:" % application_name)
            modified_page_raw = modified_page_raw.replace("[[:", "[[:%s:" % application_name)
#            modified_page_raw = modified_page_raw.replace(":green_dot.16x16.png", ":shinken:green_dot.16x16.png")
#            modified_page_raw = modified_page_raw.replace(":red_dot.16x16.png", ":shinken:red_dot.16x16.png")
#            modified_page_raw = modified_page_raw.replace(":orange_dot.16x16.png", ":shinken:orange_dot.16x16.png")
            # Change media links
#            modified_page_raw = modified_page_raw.replace("{{:official:images:", "{{%s:official:" % application_name)
            for k, v in replace_dict.items():
                #print k, v
#                if v.find("images/") != -1 or k.find("images/"):
#                    import pdb;pdb.set_trace()
                modified_page_raw = modified_page_raw.replace(k, v.replace("/", ":"))
            modified_page_raw = modified_page_raw.replace(":images/", ":images/:")

# DISABLE: add :shinken:
#            modified_page_raw = modified_page_raw.replace("{{ :", "{{ :shinken:")
#            modified_page_raw = modified_page_raw.replace(":shinken:shinken:", ":shinken:")


#            if replace_dict:
#                import pdb;pdb.set_trace()
#            modified_page_raw = modified_page_raw.replace("{{:official:", "{{/%s/official/" % application_name)
#            if modified_page_raw.find("{{/") != -1:
#                import pdb;pdb.set_trace()
            f = open(file_name, "w")
            print "    Writing file : %s" % file_name
            f.write(modified_page_raw)
            f.close()

def find_media(raw_data):
    medias = raw_data.split("{{")
    replace_dict = {}
    if len(medias) > 1:
        for m in medias[1:]:
            media = m.split("}}")[0]
            if media.startswith("http"):
                continue
#            if media.find(".png") == -1:
#                import pdb;pdb.set_trace()
            media = media.split("png")[0] + "png"
            media = media.replace(":", "/")
            media = media.strip()
            if not media.endswith("png"):
                continue
            media_url = url + "/_media" + media

# DISABLE: add :shinken:
            #replace_dict[media] = ":shinken:" + media.replace("/", ":")
            replace_dict[media] = media.replace("/", ":")


            print "        Get media : %s - %s" % (media, media_url)
            media_folder = 'media/' + application_name + "/" + os.path.dirname(media)
            try:
                os.makedirs(media_folder)
            except OSError, e:
                #print e
                pass 
            media_res = requests.get(media_url)
            media_file = os.path.join(media_folder, os.path.basename(media))
            print "        Writing media : %s" % media_file
#            print media_res.content
            f = open(media_file, "w")
            f.write(media_res.content)
            f.close()
    return replace_dict




           
parse_level(index)

lonely_pages = [
    ("http://www.shinken-monitoring.org/wiki/official/start?do=export_raw&do=export_raw", "official"),
    ("http://www.shinken-monitoring.org/wiki/packs/start?do=export_raw&do=export_raw", "packs"),
]

for p, tmp_root in lonely_pages:
    page_name = "start"
    page_raw_res = requests.get(p)
    try:
        os.makedirs(os.path.join('pages', application_name, tmp_root).replace(":", "/"))
    except OSError, e:
        #print e
        pass
    file_name = os.path.join('pages', application_name, tmp_root, page_name + ".txt")
    file_name = file_name.replace(":", "/")
   #print file_name
    replace_dict = find_media(page_raw_res.content)
    # Change links
    modified_page_raw = page_raw_res.content
    modified_page_raw = modified_page_raw.replace("official:official_", "official:")
    modified_page_raw = modified_page_raw.replace("official_", "official:")
    modified_page_raw = modified_page_raw.replace("/official/", ":%s:official:" % application_name)
    modified_page_raw = modified_page_raw.replace("[[official:", "[[%s:official:" % application_name)
    modified_page_raw = modified_page_raw.replace("[[:", "[[:%s:" % application_name)
    # Change media links
    for k, v in replace_dict.items():
     #   print k, v
        modified_page_raw = modified_page_raw.replace(k, v.replace("/", ":"))
    modified_page_raw = modified_page_raw.replace(":images/", ":images/:")

# DISABLE: add :shinken:
    modified_page_raw = modified_page_raw.replace("{{ :", "{{ :shinken:")
    modified_page_raw = modified_page_raw.replace(":shinken:shinken:", ":shinken:")

    f = open(file_name, "w")
    print "    Writing file : %s" % file_name
    f.write(modified_page_raw)
    f.close()



# for i in `grep -R "^| "|grep Prev|cut -d ":" -f 1|uniq`; do  sed -i 's/^| .*Prev.*//' $i; done
# for i in `grep -R "^| "|grep Next|cut -d ":" -f 1|uniq`; do  sed -i 's/^| .*Next.*//' $i; done
# for i in `grep -R "^| "|grep About|cut -d ":" -f 1|uniq`; do sed -i 's/^| .*About.*//' $i; done
# for i in `grep -R "^| "|grep Home|cut -d ":" -f 1|uniq` ; do sed -i 's/^| .*Home.*//' $i; done
# for i in `grep -R "===== Cha" . -l`; do sed -i 's/^===== C\(.*\)=====$/====== C\1======/' $i; done


########NEW FILE########
__FILENAME__ = doku2rst
#!/usr/bin/python
# -*- coding: utf-8 -*-

import re
import os
from urllib import urlretrieve

input_folder = "pages/"
output_folder = "../source"

chapters = {'about': '01',
            'gettingstarted': '02',
            'configuringshinken': '03',
            'runningshinken': '04',
            'thebasics': '05',
            'advancedtopics': '06',
            'configobjects': '07',
            'securityandperformancetuning': '08',
            'integrationwithothersoftware': '09',
            'shinkenaddons': '10',
            'development': '11',
            }

external_links = {}
internal_links = {}
output = []

# Functions


def title1(text):
    length = len(text)
    output = "\n\n%s\n%s\n%s\n\n" % ("=" * length, text, "=" * length)
    write(output)

def title2(text):
    length = len(text)
    output = "\n\n%s\n%s\n\n" % (text, "=" * length)
    write(output)

def title3(text):
    length = len(text)
    output = "\n\n%s\n%s\n\n" % (text, "-" * length)
    write(output)

def title4(text):
    length = len(text)
    output = "\n\n%s\n%s\n\n" % (text, "~" * length)
    write(output)

def title5(text):
    length = len(text)
    output = "\n\n%s\n%s\n\n" % (text, "*" * length)
    write(output)

def external_link(links):
    for link in links.items():
        output = "\n.. _%s: %s" % link
        write(output)

def normal_text(text):
    output = text
    write(output)

def get_image(image_url, image_path):
    path = "/".join((output_folder, image_path))
    try:
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        urlretrieve(image_url, path)
    except:
        import pdb;pdb.set_trace()

def get_lengths(lengths, row):
    if lengths is None:
        lengths = [len(cell) for cell in row]
    else:
        if len(lengths) > len(row):
            row = row + [''] * (len(lengths) - len(row))
        row_lengths = [len(cell) for cell in  row]
        lengths = map(lambda x: max([i for i in x]), zip(row_lengths, lengths))
    return lengths


# Main
for root, dirs, files in os.walk(input_folder):
    for filename in files:
        f = open(os.path.join(root, filename), 'r')
        # Set vars
        in_code = False
        in_note = False
        in_table = False
        in_tagcode = False
        nb_col = None
        external_links = {}
        internal_links = {}
        tables = []
        rows = []
        output = []

        # open file
        rst_filename = filename[:-3] + "rst"
        chapter = rst_filename.split("-", 1)
        if len(chapter) <= 1:
            chapter = "raws"
        elif not chapter[0] in chapters:
            chapter = "raws"
        else:
            chapter = chapter[0]
            chapter = chapters[chapter] + "_" + chapter
        if root.endswith('configobjects'):
            chapter = '07_configobjects'
        chapter_folder = os.path.join(output_folder, chapter)
        if not os.path.exists(chapter_folder):
            os.makedirs(chapter_folder)
        rst_file = os.path.join(chapter_folder, rst_filename)
        fw = open(rst_file, 'w')

        def write(text):
            fw.write(text)


        # Write the first line
        ref_target = ".. _%s:\n\n" % filename[:-4]
        write(ref_target)

        # parse line !
        for line in f:
            o_line = line
            # always strip line ???
#            line = line.strip('\n')
            # nagivations links
            #m = re.match("\|.*Prev.*Up.*Next.*\|", line.strip())
            m = re.match("\|.*Next.*\|", line.strip())
            if m:
                # we don't want it
                continue
            m = re.match("\|.*Chapter [0-9]*.*\|", line.strip())
            if m:
                # we don't want it
                continue
            m = re.match("\|.*Part .*\|", line.strip())
            if m:
                # we don't want it
                continue
            # Title 1
            m = re.match("===== ?Chapter [0-9]*\.(.*) ?=====", line)
            if m:
                # get datas
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title1,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue
            # Title 2
            m = re.match("====== ?(.*) ?======", line)
            if m:
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title1,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue
            # Title 2
            m = re.match("===== ?(.*) ?=====", line)
            if m:
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title2,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue
            # Title 3
            m = re.match("==== ?(.*) ?====", line)
            if m:
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title3,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue
            # Title 4
            m = re.match("=== ?(.*) ?===", line)
            if m:
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title4,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue
            # Title 5
            m = re.match("== ?(.*) ?==", line)
            if m:
                title = m.groups()[0]
                # prepare datas
                attrs = {'text': title}
                data = {
                        'fnt': title5,
                        'attrs': attrs,
                        }
                # store datas
                output.append(data)
                # Disable in_code
                in_code = False
                # next line
                continue

            # Normal line
            # Search external links
            m = re.search("\[\[https?://(.*?)\|(.*?)\]\]", line)
            if m:
                links = re.findall("\[\[(https?)://(.*?)\|(.*?)\]\]", line)
                for link in links:
                    line = re.sub("\[\[https?://(.*?)\|(.*?)\]\]", "`%s`_" % link[2], line, count=1, flags=0)
                    external_links[link[2]] = link[0] + "://" + link[1]

            m = re.search("\[\[https?://(.*?)\]\]", line)
            if m:
                links = re.findall("\[\[(https?)://(.*?)\]\]", line)
                for link in links:
                    line = re.sub("\[\[https?://(.*?)\]\]", "`%s`_" % link[1], line, count=1, flags=0)
                    external_links[link[1]] = link[0] + "://" + link[1]
            
            # Search internal links
            m = re.search("\[\[(.*?)\|(.*?)\]\]", line)
            if m:
                links = re.findall("\[\[(.*?)\|(.*?)\]\]", line)
                for link in links:
                    ref = link[0].split(":")[-1]
                    ref_text = link[1].strip()
                    line = re.sub("\[\[(.*?)\|(.*?)\]\]", ":ref:`%s <%s>`" % (ref_text, ref), line, count=1, flags=0)
                    if ref.startswith("configuringshinken/configobjects/"):
                        ref = ref.replace("configuringshinken/configobjects/", '')
                    internal_links[ref_text] = ref

            m = re.search("\[\[(.*?)\]\]", line)
            if m:
                links = re.findall("\[\[(.*?)\]\]", line)
                for link in links:
                    ref = link.split(":")[-1]
                    ref_text = ref
                    line = re.sub("\[\[(.*?)\]\]", ":ref:`%s` <%s>" % (ref_text, ref), line, count=1, flags=0)
                    if ref.startswith("configuringshinken/configobjects/"):
                        ref = ref.replace("configuringshinken/configobjects/", '')
                    internal_links[ref_text] = ref

            # Search image
            m = re.search("\{\{(.*?)\|(.*?)\}\}", line)
            if m:
                images = re.findall("\{\{(.*?)\|(.*?)\}\}", line)
                for image, text in images:
                    # TODO prepare image var
                    path = image.replace(":shinken:", "")
                    path = path.replace(":", "/")
                    img_filename = os.path.basename(path)
                    path = os.path.dirname(path)
                    # Download images
                    image_url = image.replace(":shinken:", "")
                    image_url = image_url.replace(":", "/")
                    image_url = "http://www.shinken-monitoring.org/wiki/_media/" + image_url
                    ##
#                    path = os.path.join("_static/images/", path)
                    path = "_static/images/" + path
                    image_path = os.path.join(path, img_filename)
#                    if not os.path.exists(path):
#                        os.makedirs(path)
                    get_image(image_url, image_path)
                    # TODO add \n after the image ???
                    image_rst_path = "/" + image_path
                    line = re.sub("\{\{(.*?)\}\}", "\n\n.. image:: %s\n   :scale: 90 %%\n\n" % image_rst_path, line, count=1, flags=0)

            m = re.search("\{\{(.*?)\}\}", line)
            if m:
                images = re.findall("\{\{(.*?)\}\}", line)
                for image in images:
                    # TODO prepare image var
                    path = image.replace(":shinken:", "")
                    path = path.replace(":", "/")
                    img_filename = os.path.basename(path)
                    path = os.path.dirname(path)
                    # Download images
                    image_url = image.replace(":shinken:", "")
                    image_url = image_url.replace(":", "/")
                    image_url = "http://www.shinken-monitoring.org/wiki/_media/" + image_url
                    ##
#                    path = os.path.join("_static/images/", path)
                    path = "_static/images/" + path
                    image_path = os.path.join(path, img_filename)
#                    if not os.path.exists(path):
#                        os.makedirs(path)
                    get_image(image_url, image_path)
                    # TODO add \n after the image ???
                    image_rst_path = "/" + image_path
                    line = re.sub("\{\{(.*?)\}\}", "\n\n.. image:: %s\n   :scale: 90 %%\n\n" % image_rst_path, line, count=1, flags=0)




            # Emphasis
            m = re.search("[^/](//[^/]*//)[^/]", line)
            if m:
                emphasis = re.findall("[^/](//[^/]*//)[^/]", line)
                for emph in emphasis:
                    new = "*%s*" % emph[2:-2]
                    line = line.replace(emph, new)



            # Code with tag
            m1 = re.search("<code>", line)
            m2 = re.search("</code>", line)
            if m2 and in_tagcode == True:
                # end code
                line = line.replace("</code>", "")
                in_tagcode = False
            elif m1 and in_tagcode == False:
                # start code
                line = line.replace("<code>", "\n::\n\n  ")
                in_tagcode = True



            # code with spaces
            m = re.search("^  *[-\*]", line)
            # end code
            if m and in_code == True and line.strip() != "::":
                # Code/list merged
                in_code = False
                line = re.sub("^ *", "", line)
                line = "\n" + line

            # end code
            if in_code == True and not re.search("^  *", line) and line.strip() != '':
                in_code = False
            m = re.search("^  *[^- \*]", line)

            # start code
            if m and in_code == False:
                in_code = True
                line = re.sub("^  ", "\n::\n\n  ", line)


            # if in code ....
            if in_code == True or in_tagcode == True:
                # In code
                if not line.startswith("  "):
                    line = "  " + line

            # if NOT in code...
            if in_code == False and in_tagcode == False:
                line = re.sub("\\\\", "\\\\", line)


            # Note
            m1 = re.search("<note>", line)
            m2 = re.search("<note warning>", line)
            m3 = re.search("<note tip>", line)
            m4 = re.search("<note important>", line)
            if m1:
                line = line.replace("<note>", ".. note::  ")
                in_note = True
            elif m2:
                line = line.replace("<note warning>", ".. warning::  ")
                in_note = True
            elif m3:
                line = line.replace("<note tip>", ".. tip::  ")
                in_note = True
            elif m4:
                line = line.replace("<note important>", ".. important::  ")
                in_note = True
            elif in_note == True:
                line = "   " + line

            m = re.search("</note>", line)
            if m:
                line = line.replace("</note>", "")
                in_note = False

            line = line.replace(u"".encode('utf-8'), '"')
            line = line.replace(u"".encode('utf-8'), '"')
            line = line.replace(u"".encode('utf-8'), '"')
            line = line.replace(u"".encode('utf-8'), '"')
            line = line.replace(u"".encode('utf-8'), '"')

#            if line.find("$HOSTACKAUTHORNAME$") != -1:
#                import pdb;pdb.set_trace()


            # table
            m1 = re.match(" *\^.*\^ *", line.strip())
            m2 = re.match(" *\|.*\| *", line.strip())
            if m1:
                # Table header
                in_table = True
                line = line.strip()[1:-1]
                cells = [c.strip() for c in line.split('^')]
                if nb_col is None:
                    nb_col = len(cells)
                rows.append(cells)
                # don't write this line
                continue 
            elif m2:
                in_table = True
                line = line.strip()[1:-1]
                cells = [c.strip() for c in line.split('|')]
                if nb_col is None:
                    nb_col = len(cells)
                rows.append(cells)
                # don't write this line
                continue
            elif m is None and in_table == True:
                in_table = False
                borders_len = reduce(get_lengths, rows, None)
                line = "\n\n" + " ".join(["=" * i for i in borders_len])
                for row in rows:
                    f_row = " ".join(['{:{fill}{align}%d}'] * nb_col)
                    f_row = f_row % tuple(borders_len)
                    if nb_col > len(row):
                        row = row + [''] * (nb_col - len(row))
                    f_row = f_row.format(*row, fill=" ", align="<")
                    line += "\n" + f_row
                line += "\n" + " ".join(["=" * i for i in borders_len]) + "\n\n"
                rows = []
                nb_col = None

            # prepare datas
            attrs = {'text': line}
            data = {
                    'fnt': normal_text,
                    'attrs': attrs,
                    }
            # store datas
            output.append(data)

        # write lines
        for data in output:
            data['fnt'](**data['attrs'])
        external_link(external_links)

        # close file
        fw.close()


# echo 
print "mv ../source/raws/about.rst ../source/01_about/"
print "mv ../source/raws/ch07.rst ../source/02_gettingstarted/"
print "mv ../source/raws/part-problemsandimpacts.rst ../source/06_advancedtopics/"


########NEW FILE########
__FILENAME__ = check_shinken
#!/usr/bin/env python
#
# Copyright (C) 2009-2011:
#    Denis GERMAIN, dt.germain@gmail.com
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.
"""
check_shinken.py:
    This check is getting daemons state from a arbiter connection.
"""

import os
import socket
from optparse import OptionParser

# Exit statuses recognized by Nagios and thus by Shinken
OK = 0
WARNING = 1
CRITICAL = 2
UNKNOWN = 3

# Name of the Pyro Object we are searching
PYRO_OBJECT = 'ForArbiter'
daemon_types = ['arbiter', 'broker', 'scheduler', 'poller', 'reactionner']


# Try to import all Shinken stuff
try:
    import shinken
except ImportError:
    # If importing shinken fails, try to load from current directory
    # or parent directory to support running without installation.
    # Submodules will then be loaded from there, too.
    import imp
    if not hasattr(os, "getuid") or os.getuid() != 0:
        imp.load_module('shinken', *imp.find_module('shinken', [".", ".."]))

try:
    import shinken.pyro_wrapper as pyro
    from shinken.pyro_wrapper import Pyro
except ImportError, exp:
    print ('CRITICAL : check_shinken requires the Python Pyro module.'
           'Please install it. (%s)' % exp)
    raise SystemExit(CRITICAL)


def check_deamons_numbers(result, target):
    total_number = len(result)
    alive_number = total_spare_number = alive_spare_number = 0
    dead_list = []
    for n, e in result.iteritems():
        if e['spare']:
            total_spare_number += 1
        if e['alive']:
            alive_number += 1
            if e['spare']:
                alive_spare_number += 1
        else:
            dead_list.append(n)
    dead_number = total_number - alive_number
    dead_list = ','.join(dead_list)

    # TODO: perfdata to graph deamons would be nice (in big HA architectures)
    # if alive_number <= critical, then we have a big problem
    if alive_number < options.critical:
        print ("CRITICAL - only %d/%d %s(s) UP. Down elements : %s"
               % (alive_number, total_number, target, dead_list))
        raise SystemExit(CRITICAL)
    # We are not in a case where there is no more daemons, but are
    # there daemons down?
    elif dead_number >= options.warning:
        print ("WARNING - %d/%d %s(s) DOWN :%s"
               % (dead_number, total_number, target, dead_list))
        raise SystemExit(WARNING)
        # Everything seems fine. But that's no surprise, is it?
    else:
        print ("OK - %d/%d %s(s) UP, with %d/%d spare(s) UP"
               % (alive_number, total_number, target,
                  alive_spare_number, total_spare_number))
        raise SystemExit(OK)

# Adding options. None are required, check_shinken will use shinken defaults
# TODO: Add more control in args problem and usage than the default
# OptionParser one
parser = OptionParser()
parser.add_option('-a', '--hostname', dest='hostname', default='127.0.0.1')
parser.add_option('-p', '--portnumber', dest='portnum', default=7770, type=int)
parser.add_option('-s', '--ssl', dest='ssl', default=False)
# TODO: Add a list of correct values for target and don't authorize
# anything else
parser.add_option('-t', '--target', dest='target')
parser.add_option('-d', '--daemonname', dest='daemon', default='')
# In HA architectures, a warning should be displayed if there's one
# daemon down
parser.add_option('-w', '--warning', dest='warning', default=1, type=int)
# If no deamon is left, display a critical (but shinken will be
# probably dead already)
parser.add_option('-c', '--critical', dest='critical', default=0, type=int)
parser.add_option('-T', '--timeout', dest='timeout', default=10, type=float)

# Retrieving options
options, args = parser.parse_args()
# TODO: for now, helpme doesn't work as desired
options.helpme = False

# Check for required option target
if not getattr(options, 'target'):
    print ('CRITICAL - target is not specified; '
           'You must specify which daemons you want to check!')
    parser.print_help()
    raise SystemExit(CRITICAL)
elif options.target not in daemon_types:
    print 'CRITICAL - target', options.target, 'is not a Shinken daemon!'
    parser.print_help()
    raise SystemExit(CRITICAL)

uri = pyro.create_uri(options.hostname, options.portnum, PYRO_OBJECT,
                      options.ssl)

# Set the default socket connection to the timeout, by default it's 10s
socket.setdefaulttimeout(float(options.timeout))

con = None
try:
    con = Pyro.core.getProxyForURI(uri)
    pyro.set_timeout(con, options.timeout)
except Exception, exp:
    print "CRITICAL : the Arbiter is not reachable : (%s)." % exp
    raise SystemExit(CRITICAL)



if options.daemon:
    # We just want a check for a single satellite daemon
    # Only OK or CRITICAL here
    daemon_name = options.daemon
    try:
        result = con.get_satellite_status(options.target, daemon_name)
    except Exception, exp:
        print "CRITICAL : the Arbiter is not reachable : (%s)." % exp
        raise SystemExit(CRITICAL)

    if result:
        if result['alive']:
            print 'OK - ', daemon_name, 'alive'
            raise SystemExit(OK)
        else:
            print 'CRITICAL -', daemon_name, ' down'
            raise SystemExit(CRITICAL)
    else:
        print 'UNKNOWN - %s status could not be retrieved' % daemon_name
        raise SystemExit(UNKNOWN)
else:
    # If no daemonname is specified, we want a general overview of the
    # "target" daemons
    result = {}

    try:
        daemon_list = con.get_satellite_list(options.target)
    except Exception, exp:
        print "CRITICAL : the Arbiter is not reachable : (%s)." % exp
        raise SystemExit(CRITICAL)

    for daemon_name in daemon_list:
        # Getting individual daemon and putting status info in the
        # result dictionary
        try:
            result[daemon_name] = con.get_satellite_status(options.target, daemon_name)
        except Exception, exp:
            print "CRITICAL : the Arbiter is not reachable : (%s)." % exp
            raise SystemExit(CRITICAL)

    # Now we have all data
    if result:
        check_deamons_numbers(result, options.target)
    else:
        print 'UNKNOWN - Arbiter could not retrieve status for', options.target
        raise SystemExit(UNKNOWN)

########NEW FILE########
__FILENAME__ = cluster_discovery_runner
#!/usr/bin/env python
# Copyright (C) 2009-2012:
#    Camille, VACQUIE
#    Romain, FORLOT, romain.forlot@sydel.fr
# 
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.
#
###############################################################
#
# cluster_discovery_runner.py script simply try to get informations
# from HACMP mib and failback on Safekit mib. SNMP for both product
# need to be activated. For Safekit, add a proxy into snmpd conf to
# include its mib into the master agent netsnmp.
#
# For SNMPv3 we created a default user using the command :
# net-snmp-config --create-snmpv3-user -a "mypassword" myuser
# Here the user name is myuser and his password is mypassword
#
###############################################################

### modules import
import netsnmp
import optparse
import re

##########
#  menu  #
##########

parser = optparse.OptionParser('%prog [options] -H HOSTADRESS -C SNMPCOMMUNITYREAD -O ARG1 -V SNMPVERSION -l SNMPSECNAME -L SNMPSECLEVEL -p SNMPAUTHPROTO -x SNMPAUTHPASS')

# user name and password are defined in /var/lib/net-snmp/snmpd.conf
parser.add_option("-H", "--hostname", dest="hostname", help="Hostname to scan")
parser.add_option("-C", "--community", dest="community", help="Community to scan (default:public)")
parser.add_option("-O", "--os", dest="os", help="OS from scanned host")
parser.add_option("-V", "--version", dest="version", type=int, help="Version number for SNMP (1, 2 or 3; default:1)")
parser.add_option("-l", "--login", dest="snmpv3_user", help="User name for snmpv3(default:admin)")
parser.add_option("-L", "--level", dest="snmpv3_level", help="Security level for snmpv3(default:authNoPriv)")
parser.add_option("-p", "--authproto", dest="snmpv3_auth", help="Authentication protocol for snmpv3(default:MD5)")
parser.add_option("-x", "--authpass", dest="snmpv3_auth_pass", help="Authentication password for snmpv3(default:monpassword)")


opts, args = parser.parse_args()

hostname = opts.hostname
os = opts.os

clSolution_by_os = { 'aix' : 'hacmp',
       'linux': 'safekit',
     }

if not opts.hostname:
    parser.error("Requires one host and its os to scan (option -H)")

if not opts.os:
    parser.error("Requires the os host(option -O)")

if opts.community:
    community = opts.community
else:
    community = 'public'

if opts.version:
    version = opts.version
else:
    version = 1

if opts.snmpv3_user:
    snmpv3_user = opts.snmpv3_user
else:
    snmpv3_user = 'myuser'

if opts.snmpv3_level:
    snmpv3_level = opts.snmpv3_level
else:
    snmpv3_level = 'authNoPriv'

if opts.snmpv3_auth:
    snmpv3_auth = opts.snmpv3_auth
else:
    snmpv3_auth = 'MD5'

if opts.snmpv3_auth_pass:
    snmpv3_auth_pass = opts.snmpv3_auth_pass
else:
    snmpv3_auth_pass = 'mypassword'

oid_safekit_moduleName = ".1.3.6.1.4.1.107.175.10.1.1.2"
oid_hacmp_clusterName = ".1.3.6.1.4.1.2.3.1.2.1.5.1.2"


##############
#  functions #
############## 

### Search for cluster solution, between safekit or hacmp, presents on the target
def get_cluster_discovery(oid):
    name= netsnmp.Varbind(oid)
    result = netsnmp.snmpwalk(name, Version=version, DestHost=hostname, Community=community, SecName=snmpv3_user, SecLevel=snmpv3_level, AuthProto=snmpv3_auth, AuthPass=snmpv3_auth_pass)
    nameList = list(result)
    return nameList

### format the modules list and display them on the standard output
def get_cluster_discovery_output(list):
    names = []
    if list :
        for elt in list:
            names.append(elt)
        print "%s::%s=1"%(hostname, clSolution)# To add tag
        print "%s::_%s_modules=%s"%(hostname, clSolution, ','.join(names))# Host macros by Safekit modules
    else : 
        print "%s::%s=0"%(hostname, clSolution)# No cluster detected

###############
#  execution  #
###############

scan = []
clSolution = clSolution_by_os[os]


scan = get_cluster_discovery(oid_hacmp_clusterName)
if not scan:
    scan = get_cluster_discovery(oid_safekit_moduleName)
    clSolution = 'safekit'

get_cluster_discovery_output(scan)

########NEW FILE########
__FILENAME__ = fs_discovery_runner
#!/usr/bin/env python
# Copyright (C) 2009-2012:
#    Camille, VACQUIE
#    Romain, FORLOT, romain.forlot@sydel.fr
# 
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.
#
###############################################################
#
# First of all, the fs_discovery_runner.py script get the list 
# of the files systems back from the nmap device list with SNMP 
# protocol. The OID used by SNMP to recover datas is particular
# to each OS type.
# And then it converts the listed files systems writing and
# display it on the standard output.
# For example : / will be translate into _root and /var will be
# translate into _var
#
# For SNMPv3 we created a default user using the command :
# net-snmp-config --create-snmpv3-user -a "mypassword" myuser
# Here the user name is myuser and his password is mypassword
#
###############################################################


### modules import
import netsnmp
import optparse
import re

##########
#  menu  #
##########

parser = optparse.OptionParser('%prog [options] -H HOSTADRESS -C SNMPCOMMUNITYREAD -O ARG1 -V SNMPVERSION -l SNMPSECNAME -L SNMPSECLEVEL -p SNMPAUTHPROTO -x SNMPAUTHPASS')

# user name and password are defined in /var/lib/net-snmp/snmpd.conf
# default parameters are defined in /usr/local/shinken/etc/resource.cfg
parser.add_option("-H", "--hostname", dest="hostname", help="Hostname to scan")
parser.add_option("-m", "--mode", dest="mode", help="Discovery mode : [ macros | tags ]. Macros will creates host macros and tags will add tags for each fs detected.")
parser.add_option("-C", "--community", dest="community", help="Community to scan (default:public)")
parser.add_option("-O", "--os", dest="os", help="OS from scanned host")
parser.add_option("-V", "--version", dest="version", type=int, help="Version number for SNMP (1, 2 or 3; default:1)")
parser.add_option("-l", "--login", dest="snmpv3_user", help="User name for snmpv3(default:admin)")
parser.add_option("-L", "--level", dest="snmpv3_level", help="Security level for snmpv3(default:authNoPriv)")
parser.add_option("-p", "--authproto", dest="snmpv3_auth", help="Authentication protocol for snmpv3(default:MD5)")
parser.add_option("-x", "--authpass", dest="snmpv3_auth_pass", help="Authentication password for snmpv3(default:monpassword)")


opts, args = parser.parse_args()

hostname = opts.hostname
os = opts.os

mode = { 'macros' : '_fs',
         'tags' : 'fs',
}

if not opts.hostname:
    parser.error("Requires one host and its os to scan (option -H)")

if not opts.mode:
    parser.error("Requires mode. Please choose between macros or tags")

if not opts.os:
    parser.error("Requires the os host(option -O)")

if opts.community:
    community = opts.community
else:
    community = 'public'

if opts.version:
    version = opts.version
else:
    version = 1

if opts.snmpv3_user:
    snmpv3_user = opts.snmpv3_user
else:
    snmpv3_user = 'myuser'

if opts.snmpv3_level:
    snmpv3_level = opts.snmpv3_level
else:
    snmpv3_level = 'authNoPriv'

if opts.snmpv3_auth:
    snmpv3_auth = opts.snmpv3_auth
else:
    snmpv3_auth = 'MD5'

if opts.snmpv3_auth_pass:
    snmpv3_auth_pass = opts.snmpv3_auth_pass
else:
    snmpv3_auth_pass = 'mypassword'

oid_aix_linux = ".1.3.6.1.2.1.25.3.8.1.2"# hrFSMountPoint
oid_hpux = ".1.3.6.1.4.1.11.2.3.1.2.2.1.10"# fileSystemName


##############
#  functions #
############## 

### Search for files systems presents on the target
def get_fs_discovery(oid):
    hrFSMountPoint = netsnmp.Varbind(oid)
    result = netsnmp.snmpwalk(hrFSMountPoint, Version=version, DestHost=hostname, Community=community, SecName=snmpv3_user, SecLevel=snmpv3_level, AuthProto=snmpv3_auth, AuthPass=snmpv3_auth_pass)
    #PrivProto=snmpv3_priv, PrivPass=snmpv3_priv_pass
    fsList = list(result)
    return fsList

### converts the listed files systems writing and display them on the standard output
def get_fs_discovery_output(liste):
    fsTbl = []
    for element in liste:
        elt = re.sub(r'\W', '_', element)# conversion from / to _
        if elt == '_':# if _ is the only detected character
            elt = re.sub(r'^_$', '_root', elt)# so we replace _ with _root
        fsTbl.append(elt)
    print "%s::%s=%s"%(hostname, mode[opts.mode], ','.join(fsTbl))# display like in the nmap model

###############
#  execution  #
###############

scan = []

if os == 'aix':
    scan = get_fs_discovery(oid_aix_linux)
elif os == 'linux':
    scan = get_fs_discovery(oid_aix_linux)
elif os == 'hp-ux':
    scan = get_fs_discovery(oid_hpux)

get_fs_discovery_output(scan)

########NEW FILE########
__FILENAME__ = nmap_discovery_runner
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# sudo nmap 192.168.0.1 --min-rate 1000 --max-retries 0 -sU -sT -T4 -O --traceroute -oX toto.xml

import optparse
import sys
import os
import tempfile
import subprocess

try:
    # xml.etree.ElementTree is new in Python 2.5
    from xml.etree.ElementTree import ElementTree
except ImportError:
    sys.exit("This script needs the Python ElementTree module. Please install it")

VERSION = '0.1.1'
# Fred : command launched depending on os detection
if os.name != 'nt': 
    DEFAULT_CMD = "sudo nmap %s -sU -sT --min-rate %d --max-retries %d -T4 -O -oX %s"
else:
    DEFAULT_CMD = "nmap %s -sU -sT --min-rate %d --max-retries %d -T4 -O -oX %s"
    
parser = optparse.OptionParser(
    "%prog [options] -t nmap scanning targets",
    version="%prog " + VERSION)

parser.add_option('-t', '--targets', dest="targets",
                  help="NMap scanning targets.")
parser.add_option('-v', '--verbose', dest="verbose", action='store_true',
                  help="Verbose output.")
parser.add_option('--min-rate', dest="min_rate",
                  help="Min rate option for nmap (number of parallel packets to launch. By default 1000)")
parser.add_option('--max-retries', dest="max_retries",
                  help="Max retries option for nmap (number of packet send retry). By default 0 - no retry -)")
parser.add_option('-s', '--simulate', dest="simulate",
                  help="Simulate a launch by reading an nmap XML output instead of launching a new one.")

targets = []
opts, args = parser.parse_args()

if not opts.simulate:
    simulate = None
else:
    simulate = opts.simulate


if not opts.simulate and not opts.targets:
    parser.error("Requires at least one nmap target for scanning (option -t/--targets)")
else:
    targets.append(opts.targets)

min_rate = 1000
if opts.min_rate:
    min_rate = int(opts.min_rate)

max_retries = 0
if opts.max_retries:
    max_retries = int(opts.max_retries)


if not opts.verbose:
    verbose = False
else:
    verbose = True

if args:
    targets.extend(args)

print "Got our target", targets


def debug(txt):
    if verbose:
        print txt


# Says if a host is up or not
def is_up(h):
    status = h.find('status')
    state = status.attrib['state']
    return state == 'up'


class DetectedHost:
    def __init__(self):
        self.ip = ''
        self.mac_vendor = ''
        self.host_name = ''

        self.os_possibilities = []
        self.os = ('', '')
        self.open_ports = []

        self.parent = ''

    # Keep the first name we've got
    def set_host_name(self, name):
        if self.host_name == '':
            self.host_name = name

    # Get a identifier for this host
    def get_name(self):
        if self.host_name != '':
            return self.host_name
        if self.ip != '':
            return self.ip
        return None

    # We look for the host VMWare
    def is_vmware_esx(self):
        # If it's not a virtual machine bail out
        if self.mac_vendor != 'VMware':
            return False
        # If we got all theses ports, we are quite ok for
        # a VMWare host
        needed_ports = [22, 80, 443, 902, 903, 5989]
        for p in needed_ports:
            if p not in self.open_ports:
                # find one missing port, not a VMWare host
                return False
        # Ok all ports are found, we are a ESX :)
        return True

    # Says if we are a virtual machine or not
    def is_vmware_vm(self):
        # special case: the esx host itself
        if self.is_vmware_esx():
            return False
        # Else, look at the mac vendor
        return self.mac_vendor == 'VMware'

    # Fill the different os possibilities
    def add_os_possibility(self, os, osgen, accuracy, os_type, vendor):
        self.os_possibilities.append((os, osgen, accuracy, os_type, vendor))

    # We search if our potential parent is present in the
    # other detected hosts. If so, set it as my parent
    def look_for_parent(self, all_hosts):
        self.parents = []
        parent = self.parent
        debug("Look for my parent %s -> %s" % (self.get_name(), parent))
        # Ok, we didn't find any parent
        # we bail out
        if parent == '':
            return
        for h in all_hosts:
            debug("Is it you? %s" % h.get_name())
            if h.get_name() == parent:
                debug("Houray, we find our parent %s -> %s" % (self.get_name(), h.get_name()))
                self.parents.append(h.get_name())

    # Look at ours oses and see which one is the better
    def compute_os(self):
        self.os_name = 'Unknown OS'
        self.os_version = 'Unknown Version'
        self.os_type = 'Unknown Type'
        self.os_vendor = 'Unknown Vendor'

        # Bailout if we got no os :(
        if len(self.os_possibilities) == 0:
            return

        max_accuracy = 0
        for (os, osgen, accuracy, os_type, vendor) in self.os_possibilities:
            if accuracy > max_accuracy:
                max_accuracy = accuracy

        # now get the entry with the max value, the first one
        for (os, osgen, accuracy, os_type, vendor) in self.os_possibilities:
            print "Can be", (os, osgen, accuracy, os_type, vendor)
            if accuracy == max_accuracy:
                self.os = (os, osgen, os_type, vendor)
                break

        print "Will dump", self.os

        # Ok, unknown os... not good
        if self.os == ('', '', '', ''):
            return

        self.os_name = self.os[0].lower()
        self.os_version = self.os[1].lower()
        self.os_type = self.os[2].lower()
        self.os_vendor = self.os[3].lower()

    # Return the string of the 'discovery' items
    def get_discovery_output(self):
        r = []
        r.append('%s::isup=1' % self.get_name())
        r.append(self.get_discovery_system())
        r.append(self.get_discovery_macvendor())
        op = self.get_discovery_ports()
        if op != '':
            r.append(op)
        par = self.get_discovery_parents()
        if par != '':
            r.append(par)
        fqdn = self.get_dicovery_fqdn()
        if fqdn != '':
            r.append(fqdn)
        ip = self.get_discovery_ip()
        if ip != '':
            r.append(ip)
        return r

    # for system output
    def get_discovery_system(self):
        r = '%s::os=%s' % (self.get_name(), self.os_name) + '\n'
        r += '%s::osversion=%s' % (self.get_name(), self.os_version) + '\n'
        r += '%s::ostype=%s' % (self.get_name(), self.os_type) + '\n'
        r += '%s::osvendor=%s' % (self.get_name(), self.os_vendor)
        return r

    def get_discovery_macvendor(self):
        return '%s::macvendor=%s' % (self.get_name(), self.mac_vendor)

    def get_discovery_ports(self):
        if self.open_ports == []:
            return ''
        return '%s::openports=%s' % (self.get_name(), ','.join([str(p) for p in self.open_ports]))

    def get_discovery_parents(self):
        if self.parents == []:
            return ''
        return '%s::parents=%s' % (self.get_name(), ','.join(self.parents))

    def get_dicovery_fqdn(self):
        if self.host_name == '':
            return ''
        return '%s::fqdn=%s' % (self.get_name(), self.host_name)

    def get_discovery_ip(self):
        if self.ip == '':
            return ''
        return '%s::ip=%s' % (self.get_name(), self.ip)


if not simulate:
    (_, tmppath) = tempfile.mkstemp()

    print "propose a tmppath", tmppath

    # Fred : command launched depending on os detection
    # cmd = "nmap %s -sU -sT --min-rate %d --max-retries %d -T4 -O -oX %s" % (' '.join(targets), min_rate, max_retries, tmppath)
    cmd = DEFAULT_CMD % (' '.join(targets), min_rate, max_retries, tmppath)
    print "Launching command,", cmd
    try:
        nmap_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            close_fds=False, shell=True)
    except OSError, exp:
        print "Debug: Error in launching command:", cmd, exp
        sys.exit(2)

    print "Try to communicate"
    (stdoutdata, stderrdata) = nmap_process.communicate()

    if nmap_process.returncode != 0:
        print "Error: the nmap return an error: '%s'" % stderrdata
        sys.exit(2)

    # Fred : no need to print nmap result catched ...
    # print "Got it", (stdoutdata, stderrdata)
    print "Got it !"

    xml_input = tmppath
else:  # simulate mode
    xml_input = simulate

tree = ElementTree()
try:
    tree.parse(xml_input)
except IOError, exp:
    print "Error opening file '%s': %s" % (xml_input, exp)
    sys.exit(2)

hosts = tree.findall('host')
debug("Number of hosts: %d" % len(hosts))

all_hosts = []

for h in hosts:
    # Bypass non up hosts
    if not is_up(h):
        continue

    dh = DetectedHost()

    # Now we get the ipaddr and the mac vendor
    # for future VMWare matching
    #print h.__dict__
    addrs = h.findall('address')
    for addr in addrs:
        #print "Address", addr.__dict__
        addrtype = addr.attrib['addrtype']
        if addrtype == 'ipv4':
            dh.ip = addr.attrib['addr']
        if addrtype == "mac":
            if 'vendor' in addr.attrib:
                dh.mac_vendor = addr.attrib['vendor'].lower()

    # Now we've got the hostnames
    host_names = h.findall('hostnames')
    for h_name in host_names:
        h_names = h_name.findall('hostname')
        for h_n in h_names:
            #print 'hname', h_n.__dict__
            #print 'Host name', h_n.attrib['name']
            dh.set_host_name(h_n.attrib['name'])

    # Now print the traceroute
    traces = h.findall('trace')
    for trace in traces:
        #print trace.__dict__
        hops = trace.findall('hop')
        #print "Number of hops", len(hops)
        distance = len(hops)
        if distance >= 2:
            for hop in hops:
                ttl = int(hop.attrib['ttl'])
                #We search for the direct father
                if ttl == distance-1:
                    #print ttl
                    #print "Super hop", hop.__dict__
                    # Get the host name if possible, if not
                    # take the IP
                    if 'host' in hop.attrib:
                        dh.parent = hop.attrib['host']
                    else:
                        dh.parent = hop.attrib['ipaddr']

    # Now the OS detection
    ios = h.find('os')
    # Fred : if no OS detected by nmap (localhost on Windows does not detect OS !)
    if ios:
        #print os.__dict__
        cls = ios.findall('osclass')
        for c in cls:
            #print "Class", c.__dict__
            family = c.attrib['osfamily']
            accuracy = c.attrib['accuracy']
            osgen = c.attrib.get('osgen', '')
            os_type = c.attrib.get('type', '')
            vendor = c.attrib.get('vendor', '')
            #print "Type:", family, osgen, accuracy
            dh.add_os_possibility(family, osgen, accuracy, os_type, vendor)
        # Ok we can compute our OS now :)
        dh.compute_os()
    else:
        debug(" No OS detected !")
        family = 'Unknown'
        accuracy = 'Unknown'
        osgen = 'Unknown'
        os_type = 'Unknown'
        vendor = 'Unknown'
        #print "Type:", family, osgen, accuracy
        dh.add_os_possibility(family, osgen, accuracy, os_type, vendor)
        dh.compute_os()

    # Now the ports :)
    allports = h.findall('ports')
    for ap in allports:
        ports = ap.findall('port')
        for p in ports:
            #print "Port", p.__dict__
            p_id = p.attrib['portid']
            s = p.find('state')
            #print s.__dict__
            state = s.attrib['state']
            if state == 'open':
                dh.open_ports.append(int(p_id))

    #print dh.__dict__
    all_hosts.append(dh)
    #print "\n\n"



for h in all_hosts:
    name = h.get_name()
    if not name:
        continue

    debug("Doing name %s" % name)
    #path = os.path.join(output_dir, name+'.discover')
    #print "Want path", path
    #f = open(path, 'wb')
    #cPickle.dump(h, f)
    #f.close()
    debug(str(h.__dict__))
    # And generate the configuration too
    h.look_for_parent(all_hosts)
    #c.fill_system_conf()
    #c.fill_ports_services()
    #c.fill_system_services()
    #c.write_host_configuration()
    #print "Host config", c.get_cfg_for_host()
    #c.write_services_configuration()
    #print "Service config"
    #print c.get_cfg_for_services()
    #print c.__dict__
    print '\n'.join(h.get_discovery_output())
    #print "\n\n\n"


# Try to remove the temppath
try:
    os.unlink(tmppath)
except Exception:
    pass

########NEW FILE########
__FILENAME__ = SAN_discover_runner
#!/usr/bin/env python
# Copyright (C) 2009-2012:
#    Romain, FORLOT, romain.forlot@sydel.fr
# 
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.
#
###############################################################
#
# This script aimed to discover SAN devices in your network.
# Only IBM DS devices are supported for now.
# This use SMcli to manage Array.
#
###############################################################

### modules import
import optparse
import re
import subprocess
import socket
import fcntl
import struct
 
SIOCGIFNETMASK = 0x891b
eth_dev_name='eth0'

##########
#  menu  #
##########

parser = optparse.OptionParser('%prog [options] -t target') 
cmd = { 'ibm_ds' : '/opt/IBM_DS/client/SMcli',
        'example' : '/path/to/cmd',
      }

# user name and password are defined in /var/lib/net-snmp/snmpd.conf
# default parameters are defined in /usr/local/shinken/etc/resource.cfg
parser.add_option('-t', '--target', dest='target', help='IP to manage. One at a time only')
parser.add_option('-v', '--vendor', dest='vendor', help='specify SAN vendor [ibm_ds|...]')
parser.add_option('-n', '--network', action='store_true', dest='network', help='Take controller IP which are on same network as you are')
parser.add_option('-d', '--debug', action='store_true', dest='debug', help='be more verbose')


opts, args = parser.parse_args()

target = opts.target
vendor = opts.vendor

if opts.debug:
    debug = True
else:
    debug = False

def debuging(txt):
    if debug:
        print txt

if opts.network:
    network = True

if not opts.target:
    parser.error('Require at least one ip (option -t)')
if not opts.vendor:
    parser.error('Require SAN vendor name. [ibm_ds|...]')

SANvendor = { 'ibm_ds' : { 'add_cmd' : [ cmd['ibm_ds'], '-A', target ],
                           'getprofile_cmd' : [ cmd['ibm_ds'], target, '-c', 'show storagesubsystem profile;' ],
                           'sanname_regex' :  re.compile('PROFILE FOR STORAGE SUBSYSTEM:\s(?P<sanname>\w+)\s+.*$', re.S|re.M),
                           'controllers_ip_regex' : re.compile('IP address:\s+((?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))', re.S|re.M),
                         },
              'example' : { 'add_cmd' : [ cmd['example'], 'arg1', 'arg2' ],
                            'getprofile_cmd' : [ cmd['example'], 'arg1', 'arg2' ],
                            'sanname_regex' :  re.compile('(?P<sanname>\w+)', re.S|re.M),
                            'controllers_ip_regex' : re.compile('IP address:\s+((?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))', re.S|re.M),
                          },
}

##############
#  functions #
############## 

### Code snippet to retrieve some system network informations
def get_network_mask(ifname):
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    netmask = fcntl.ioctl(s, SIOCGIFNETMASK, struct.pack('256s', ifname))[20:24]
    return socket.inet_ntoa(netmask)
 
def get_ip_address(ifname):
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(fcntl.ioctl(
        s.fileno(),
        0x8915,  # SIOCGIFADDR
        struct.pack('256s', ifname[:15])
    )[20:24])

def address_in_network(ip,net):
   ipaddr = struct.unpack('L',socket.inet_aton(ip))[0]
   netaddr,bits = net.split('/')
   netmask = struct.unpack('L',socket.inet_aton(netaddr))[0] & struct.unpack('L',socket.inet_aton(bits))[0]
   return ipaddr & netmask == netmask

### Search for cluster software presents on target
def set_ip():
    addip = v['add_cmd']
    adding = subprocess.Popen(' '.join(addip), stdout=subprocess.PIPE, shell=True)
    debuging(adding.communicate()[0])
    adding.wait()
    
def get_SAN_profile():
    sanprofile = v['getprofile_cmd']
    get_managed_dev = subprocess.Popen(sanprofile,stdout=subprocess.PIPE)
    stdoutdata = get_managed_dev.communicate()[0]
    debuging(stdoutdata)
    return stdoutdata

def get_name(san_profile):
    getsanname = v['sanname_regex'].search(san_profile)
    try:
        sanname = getsanname.group('sanname')
    except AttributeError:
        print('Can not retrieve San name')
    return sanname

def get_controllers_ip(san_profile, keep_on_same_network=False):
    ctrl = v['controllers_ip_regex'].findall(san_profile)
    debuging('Find ip : %s' % ctrl)
    if keep_on_same_network:
        my_ip = get_ip_address(eth_dev_name)
        my_netmask = get_network_mask(eth_dev_name)
        my_subnet_unpacked = struct.unpack('L', socket.inet_aton(my_ip))[0] & struct.unpack('L', socket.inet_aton(my_netmask))[0]
        my_subnet = socket.inet_ntoa(struct.pack('L', my_subnet_unpacked))
        n = [ my_subnet, my_netmask ]
        i = 0
        for ip in ctrl:
            if not address_in_network(ip, '/'.join(n)):
               ctrl.pop(i)
            i += 1
    return ctrl
    
### converts the listed files systems writing and display them on the standard output
def get_discovery_output(sanname, ctrlIP):
    i = 1
    for ip in ctrlIP:
        print '%s::_ctrl%d=%s'%(sanname, i, ip)
        i += 1

###############
#     main    #
###############

v = SANvendor[vendor]

# Add ip in client software managing SAN device
set_ip()

# Get SAN profile from client
profile = get_SAN_profile()

# Get SAN device name
sanname = get_name(profile)

# Get List of controllers IP
ctrl_ip = get_controllers_ip(profile, network)

get_discovery_output(sanname,ctrl_ip)

########NEW FILE########
__FILENAME__ = vmware_discovery_runner
#!/usr/bin/env python
#
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel <h.goebel@goebel-consult.de>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import shutil
import optparse
from subprocess import Popen, PIPE

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load simple json
    try:
        import simplejson as json
    except ImportError:
        raise SystemExit("Error: you need the json or simplejson module "
                         "for this script")

VERSION = '0.1'


def search_for_check_esx3():
    """Search for the check_esx3.pl file."""
    me = os.path.abspath(__file__)
    my_dir = os.path.dirname(me)
    possible_paths = [os.path.join(my_dir, 'check_esx3.pl'),
                      '/var/lib/nagios/check_esx3.pl',
                      '/var/lib/plugins/nagios/check_esx3.pl',
                      '/var/lib/shinken/check_esx3.pl',
                      '/usr/local/nagios/libexec/check_esx3.pl',
                      '/usr/local/shinken/libexec/check_esx3.pl',
                      'c:\\shinken\\libexec\\check_esx3.pl']

    for p in possible_paths:
        print "Look for", p
        if os.path.exists(p):
            print "Found a check_esx3.pl at", p
            return p
    return None


def _split_rules(rules):
    """Split and clean the rules from a string to a list"""
    return [r.strip() for r in rules.split('|')]


def _apply_rules(name, rules):
    """Apply rules on the objects names"""
    if 'nofqdn' in rules:
        name = name.split('.', 1)[0]
    if 'lower' in rules:
        name = name.lower()
    return name


def get_vmware_hosts(check_esx_path, vcenter, user, password):
    """
    Get a list of all hosts from a VCenter.
    """
    list_host_cmd = [check_esx_path, '-D', vcenter, '-u', user, '-p', password,
                     '-l', 'runtime', '-s', 'listhost']

    print "Got host list"
    print ' '.join(list_host_cmd)
    p = Popen(list_host_cmd, stdout=PIPE, stderr=PIPE)
    output = p.communicate()

    print "Exit status", p.returncode
    if p.returncode == 2:
        print "Error: check_esx3.pl returnes an error:", output
        raise SystemExit(2)

    parts = output[0].split(':')
    hsts_raw = parts[1].split('|')[0]
    hsts_raw_lst = hsts_raw.split(',')

    hosts = []
    for hst_raw in hsts_raw_lst:
        hst_raw = hst_raw.strip()
        # look as server4.mydomain(UP)
        elts = hst_raw.split('(')
        hst = elts[0]
        hosts.append(hst)

    return hosts


def get_vm_of_host(check_esx_path, vcenter, host, user, password):
    """Get a list of all virtual machines on a specific host."""
    print "Listing host", host
    list_vm_cmd = [check_esx_path, '-D', vcenter, '-H', host,
                   '-u', user, '-p', password,
                   '-l', 'runtime', '-s', 'list']
    print ' '.join(list_vm_cmd)
    p = Popen(list_vm_cmd, stdout=PIPE)
    output = p.communicate()

    print "Exit status", p.returncode
    if p.returncode == 2:
        print "Error: check_esx3.pl returnes an error:", output
        raise SystemExit(2)

    parts = output[0].split(':')
    # Maybe we got a 'CRITICAL - There are no VMs.' message,
    # if so, we bypass this host
    if len(parts) < 2:
        return None

    vms_raw = parts[1].split('|')[0]
    vms_raw_lst = vms_raw.split(',')

    lst = []
    for vm_raw in vms_raw_lst:
        vm_raw = vm_raw.strip()
        # look as MYVM(UP)
        elts = vm_raw.split('(')
        vm = elts[0]
        lst.append(vm)
    return lst


def print_all_links(res, rules):
    """Create all tuples of the links for the hosts"""
    r = []
    for host in res:
        host_name = _apply_rules(host, rules)
        print "%s::esxhostname=%s" % (host_name, host_name)
        print "%s::isesxhost=1" % host_name
        for vm in res[host]:
            # First we apply rules on the names
            vm_name = _apply_rules(vm, rules)
            #v = (('host', host_name),('host', vm_name))
            print "%s::vmname=%s" % (vm_name, vm_name)
            print "%s::isesxvm=1" % vm_name
            print "%s::esxhost=%s" % (vm_name, host_name)
            #r.append(v)
    return r


def write_output(r, path):
    try:
        f = open(path + '.tmp', 'w')
        buf = json.dumps(r)
        f.write(buf)
        f.close()
        shutil.move(path + '.tmp', path)
        print "File %s written" % path
    except IOError, exp:
        raise SystemExit("Error writing the file %s: %s" % (path, exp))


def main(check_esx_path, vcenter, user, password, rules):
    rules = _split_rules(rules)
    res = {}
    hosts = get_vmware_hosts(check_esx_path, vcenter, user, password)

    for host in hosts:
        lst = get_vm_of_host(check_esx_path, vcenter, host, user, password)
        if lst:
            res[host] = lst

    print_all_links(res, rules)

    #write_output(r, output)
    print "Finished!"


if __name__ == "__main__":
    parser = optparse.OptionParser(
        version="Shinken VMware links dumping script version %s" % VERSION)
    parser.add_option("-x", "--esx3-path", dest='check_esx_path',
                      help="Full path of the check_esx3.pl script (default: %default)")
    parser.add_option("-V", "--vcenter", '--Vcenter',
                      help="The IP/DNS address of your Vcenter host.")
    parser.add_option("-u", "--user",
                      help="User name to connect to this Vcenter")
    parser.add_option("-p", "--password",
                      help="The password of this user")
    parser.add_option('-r', '--rules', default='',
                      help="Rules of name transformation. Valid names are: "
                      "`lower`: to lower names, "
                      "`nofqdn`: keep only the first name (server.mydomain.com -> server)."
                      "You can use several rules like `lower|nofqdn`")

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    if opts.vcenter is None:
        parser.error("missing -V or --Vcenter option for the vcenter IP/DNS address")
    if opts.user is None:
        parser.error("missing -u or --user option for the vcenter username")
    if opts.password is None:
        error = True
        parser.error("missing -p or --password option for the vcenter password")
    if opts.check_esx_path is None:
        p = search_for_check_esx3()
        # Not given, try to find one
        if p is None:
            parser.error("Sorry, I cannot find check_esx3.pl, please specify "
                         "it with -x")
        #else set it :)
        opts.check_esx_path = p
    else:
        if not os.path.exists(opts.check_esx_path):
            parser.error("the path %s for the check_esx3.pl script is wrong, missing file" % opts.check_esx_path)

    main(**opts.__dict__)

########NEW FILE########
__FILENAME__ = windows_shares_discovery_runner
#!/usr/bin/env python
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import optparse
import subprocess

VERSION = '1.0'

def p_debug(s):
    if debug:
        print "DEBUG:", s

def get_elements(line):
    elts = line.split('|', 2)
    if len(elts) < 2:
        p_debug("Not a good line: %r" % line)
        return None
    return elts


parser = optparse.OptionParser(
    "%prog [options] -H HOSTADRESS -u DOMAIN\\USER -p PASSWORD",
    version="%prog " + VERSION)

parser.add_option('-H', "--hostname",
                  help="Hostname to scan")
parser.add_option('-u', '--user', default='guest',
                  help="Username to scan with. Default to '%default'")
parser.add_option('-p', '--password', default='',
                  help="Password of your user. Default to ''")
parser.add_option('-d', "--debug", action='store_true',
                  help="Debug mode")

opts, args = parser.parse_args()

if not opts.hostname:
    parser.error("Requires one host to scan (option -H)")

hostname = opts.hostname
user = opts.user
debug = opts.debug
password = opts.password

cred = '%s%%%s' % (user, password)

cmd = ["smbclient", '--user', cred, '--grepable', '-L', hostname]
p_debug("Launching command %s" % cmd)
try:
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE,
        close_fds=True)
except OSError, exp:
    print "Error in launching command:", cmd, exp
    raise SystemExit(2)

p_debug("Try to communicate with the subprocess")
(stdoutdata, stderrdata) = process.communicate()

if process.returncode != 0:
    print "Error: the share scanner return an error: '%s'" % (stderrdata + stdoutdata)
    raise SystemExit(2)

disks = []
printers = []

p_debug("Good return" + stdoutdata)


for line in stdoutdata.splitlines():
    elts = get_elements(line.strip())
    # Skip strange lines
    if not elts:
        continue
    typ, sharename, desc = elts
    if typ == 'Printer':
        printers.append(sharename)
    if typ == 'Disk' and not sharename.endswith('$'):
        disks.append(sharename)


if len(disks) > 0:
    print "%s::shares_detected=1" % hostname
    print "%s::_shares=%s" % (hostname, ','.join(disks))

if len(printers) > 0:
    print "%s::printers_detected=1" % hostname
    print "%s::_printers=%s" % (hostname, ','.join(printers))

########NEW FILE########
__FILENAME__ = dump_vmware_hosts
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel <h.goebel@goebel-consult.de>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import sys
import shlex
import shutil
import optparse
from subprocess import Popen, PIPE

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load
    # simple json (it can be hard json?! It's 2 functions guy!)
    try:
        import simplejson as json
    except ImportError:
        sys.exit("Error: you need the json or simplejson module for this script")

VERSION = '0.1'


# Split and clean the rules from a string to a list
def _split_rules(rules):
    return [r.strip() for r in rules.split('|')]


# Apply all rules on the objects names
def _apply_rules(name, rules):
    if 'nofqdn' in rules:
        name = name.split('.', 1)[0]
    if 'lower' in rules:
        name = name.lower()
    return name


# Get all vmware hosts from a VCenter and return the list
def get_vmware_hosts(check_esx_path, vcenter, user, password):
    list_host_cmd = [check_esx_path, '-D', vcenter, '-u', user, '-p', password,
                     '-l', 'runtime', '-s', 'listhost']

    output = Popen(list_host_cmd, stdout=PIPE).communicate()

    parts = output[0].split(':')
    if len(parts) == 1 or not '|' in parts[1]:
        print "ERROR : there was an error with the esx3.pl command. Plase fix it : '%s'" % " ".join(parts)
        sys.exit(2)
    hsts_raw = parts[1].split('|')[0]
    hsts_raw_lst = hsts_raw.split(',')

    hosts = []
    for hst_raw in hsts_raw_lst:
        hst_raw = hst_raw.strip()
        # look as server4.mydomain(UP)
        elts = hst_raw.split('(')
        hst = elts[0]
        hosts.append(hst)

    return hosts


# For a specific host, ask all VM on it to the VCenter
def get_vm_of_host(check_esx_path, vcenter, host, user, password):
    print "Listing host", host
    list_vm_cmd = [check_esx_path, '-D', vcenter, '-H', host,
                   '-u', user, '-p', password,
                   '-l', 'runtime', '-s', 'list']
    output = Popen(list_vm_cmd, stdout=PIPE).communicate()
    parts = output[0].split(':')
    # Maybe we got a 'CRITICAL - There are no VMs.' message,
    # if so, we bypass this host
    if len(parts) < 2:
        return None

    vms_raw = parts[1].split('|')[0]
    vms_raw_lst = vms_raw.split(',')

    lst = []
    for vm_raw in vms_raw_lst:
        vm_raw = vm_raw.strip()
        # look as MYVM(UP)
        elts = vm_raw.split('(')
        vm = elts[0]
        lst.append(vm)
    return lst


# Create all tuples of the links for the hosts
def create_all_links(res, rules):
    r = []
    for host in res:
        for vm in res[host]:
            # First we apply rules on the names
            host_name = _apply_rules(host, rules)
            vm_name = _apply_rules(vm, rules)
            v = (('host', host_name), ('host', vm_name))
            r.append(v)
    return r


def write_output(elements, path, rules):
    try:
        f = open(path + '.tmp', 'wb')
        for e in elements:
            e = e.strip()
            e = _apply_rules(e, rules)
            f.write('%s\n' % e)
        f.close()
        shutil.move(path + '.tmp', path)
        print "File %s wrote" % path
    except IOError, exp:
        sys.exit("Error writing the file %s: %s" % (path, exp))


def main(check_esx_path, vcenter, user, password, output, rules, vm_only, esx_only):
    rules = _split_rules(rules)
    res = {}
    hosts = get_vmware_hosts(check_esx_path, vcenter, user, password)
    if esx_only:
        write_output(hosts, output, rules)
        print "Created %d hosts" % len(hosts)
        sys.exit(0)

    vms = []
    for host in hosts:
        lst = get_vm_of_host(check_esx_path, vcenter, host, user, password)
        if lst:
            vms.extend(lst)
    write_output(vms, output, rules)

    print "Created %d hosts" % len(vms)
    print "Finished!"


# Here we go!
if __name__ == "__main__":
    # Manage the options
    parser = optparse.OptionParser(
        version="Shinken VMware links dumping script version %s" % VERSION)
    parser.add_option("-o", "--output",
                      help="Path of the generated mapping file.")
    parser.add_option("-x", "--esx3-path", dest='check_esx_path',
                      default='/usr/local/nagios/libexec/check_esx3.pl',
                      help="Full path of the check_esx3.pl script (default: %default)")
    parser.add_option("-V", "--vcenter", '--Vcenter',
                      help="tThe IP/DNS address of your Vcenter host.")
    parser.add_option("-u", "--user",
                      help="User name to connect to this Vcenter")
    parser.add_option("-p", "--password",
                      help="The password of this user")
    parser.add_option('-r', '--rules', default='',
                      help="Rules of name transformation. Valid names are: "
                      "`lower`: to lower names, "
                      "`nofqdn`: keep only the first name (server.mydomain.com -> server)."
                      "You can use several rules like `lower|nofqdn`")
    parser.add_option('--esx', default='', dest='esx_only', action='store_true',
                      help="Dump only the ESX hosts")
    parser.add_option('--vm', default='', dest='vm_only', action='store_true',
                      help="Dump only the VM hosts")
    

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    if not opts.vm_only and not opts.esx_only:
        parser.error("missing --esx or --vm option. Please choose one")

    if opts.vcenter is None:
        parser.error("missing -V or --Vcenter option for the vcenter IP/DNS address")
    if opts.user is None:
        parser.error("missing -u or --user option for the vcenter username")
    if opts.password is None:
        error = True
        parser.error("missing -p or --password option for the vcenter password")
    if not os.path.exists(opts.check_esx_path):
        parser.error("the path %s for the check_esx3.pl script is wrong, missing file" % opts.check_esx_path)
    if opts.output is None:
        parser.error("missing -o or --output option for the output mapping file")

    main(**opts.__dict__)

########NEW FILE########
__FILENAME__ = external_mapping
#!/usr/bin/env python
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This program transforms a flat dependency file into a json one so it
can be loaded in hot_dependencies_arbiter module

The input file format is:
  host1 ":" vm1
  host2 ":" vm2
  ...

Spaces around host- and vm-names will be stripped. Lines starting with
a `#` will be ignored.

You can now get a live update of your dependency tree in shinken for
your xen/virtualbox/qemu. All you have to do is finding a way to
modify this flat file when you do a live migration.

For example, you can use a script like this in your crontab::

  dsh -Mc -g mydom0group 'xm list' | \
      awk "/vm-/ { print \$1 }"' > /tmp/shinken_flat_mapping

"""


import os
import sys
import optparse

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load simple json
    try:
        import simplejson as json
    except ImportError:
        raise SystemExit("Error: you need the json or simplejson module "
                         "for this script")

VERSION = '0.2'


def main(input_file, output_file, type):
    # Check if input_file is newer than output_file
    if os.path.exists(output_file):
        if os.path.getmtime(output_file) >= os.path.getmtime(input_file):
            print "Nothing to do"
            return True
    r = []
    flatmappingfile = open(input_file)
    try:
        for line in flatmappingfile:
            if line.startswith('#'):
                # this is a comment line, skip it
                continue
            parts = line.split(':')
            if type == 'service' :
                v = (('service', parts[0].strip()), ('service', parts[1].strip()))
            else:
                v = (('host', parts[0].strip()), ('host', parts[1].strip()))
            r.append(v)
    finally:
        flatmappingfile.close()

    jsonmappingfile = open(output_file, 'w')
    try:
        json.dump(r, jsonmappingfile)
    finally:
        jsonmappingfile.close()


if __name__ == "__main__":
    parser = optparse.OptionParser(
        version="Shinken external flat mapping file to json mapping %s" % VERSION)
    parser.add_option("-o", "--output", dest='output_file',
                      default='/tmp/external_mapping_file.json',
                      help="Path of the generated json mapping file.")
    parser.add_option("-i", "--input", dest='input_file',
                      default='/tmp/shinken_flat_mapping',
                      help="Path of the flat mapping input file.")
    parser.add_option("-t", "--type", dest='type',
                      default='host', help='it is a service or host dependency. ( host | service. Default : host)')

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    main(**vars(opts))

########NEW FILE########
__FILENAME__ = link_libvirt_host_vm
#!/usr/bin/env python
# Copyright (C) 2012:
#    Thibault Cohen, thibault.cohen@savoirfairelinux.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This program use libvirt to put host parent-child relations in a json one so it
can be loaded in hot_dependencies_arbiter module
"""

import timeit
import os
import sys
import optparse
import signal

import libvirt

class TimeoutException(Exception): 
    pass 

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load simple json
    try:
        import simplejson as json
    except ImportError:
        raise SystemExit("Error: you need the json or simplejson module "
                         "for this script")

VERSION = '0.1'


def main(uris, output_file, ignore):

    def timeout_handler(signum, frame):
        raise TimeoutException()

    ignored_doms = []
    r = []

    if ignore:
        ignored_doms = ignore.split(",")
        
    for uri in uris.split(","):
        signal.signal(signal.SIGALRM, timeout_handler) 
        signal.alarm(10) # triger alarm in 10 seconds
        try:
            conn = libvirt.openReadOnly(uri)
        except libvirt.libvirtError, e:
            print "Libvirt connection error: `%s'" % e.message.replace("\r", "")
            print "Let's try next URI"
            continue
        except TimeoutException:
            print "Libvirt Request timeout"
            print "Let's try next URI"
            continue
        except Exception, e:
            print "Unknown Error: %s" % str(e)
            print "Let's try next URI..."
            continue
            
        hypervisor = conn.getHostname()
        # List all VM (stopped and started)
        for dom in [conn.lookupByName(name) for name in conn.listDefinedDomains()]\
                        + [conn.lookupByID(vmid) for vmid in conn.listDomainsID()]:
            domain_name = dom.name()
            if domain_name in ignored_doms:
                continue
            v = (('host', hypervisor.strip()), ('host', domain_name.strip()))
            r.append(v)

    r = set(r)
    r = list(r)
    jsonmappingfile = open(output_file, 'w')
    try:
        json.dump(r, jsonmappingfile)
    finally:
        jsonmappingfile.close()


if __name__ == "__main__":
    parser = optparse.OptionParser(
        version="Shinken libvirt mapping to json mapping %s" % VERSION)
    parser.add_option("-o", "--output", dest='output_file',
                      default='/tmp/libvirt_mapping_file.json',
                      help="Path of the generated json mapping file.\n"
                      "Default: /tmp/libvirt_mapping_file.json")
    parser.add_option("-u", "--uris", dest='uris',
                      help="Libvirt URIS separated by comma")
    parser.add_option("-i", "--ignore", dest='ignore',
                      default=None,
                      help="Ignore hosts (separated by comma)\n"
                           "Default: None")

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    if opts.uris is None:
        print "At least one URI is mandatory"
        sys.exit(2)

    main(**vars(opts))

########NEW FILE########
__FILENAME__ = link_vmware_host_vm
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel <h.goebel@goebel-consult.de>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import sys
import shlex
import shutil
import optparse
from subprocess import Popen, PIPE

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load
    # simple json (it can be hard json?! It's 2 functions guy!)
    try:
        import simplejson as json
    except ImportError:
        sys.exit("Error: you need the json or simplejson module for this script")

VERSION = '0.1'


# Split and clean the rules from a string to a list
def _split_rules(rules):
    return [r.strip() for r in rules.split('|')]


# Apply all rules on the objects names
def _apply_rules(name, rules):
    if 'nofqdn' in rules:
        name = name.split('.', 1)[0]
    if 'lower' in rules:
        name = name.lower()
    return name


# Get all vmware hosts from a VCenter and return the list
def get_vmware_hosts(check_esx_path, vcenter, user, password):
    list_host_cmd = [check_esx_path, '-D', vcenter, '-u', user, '-p', password,
                     '-l', 'runtime', '-s', 'listhost']

    output = Popen(list_host_cmd, stdout=PIPE).communicate()

    parts = output[0].split(':')
    hsts_raw = parts[1].split('|')[0]
    hsts_raw_lst = hsts_raw.split(',')

    hosts = []
    for hst_raw in hsts_raw_lst:
        hst_raw = hst_raw.strip()
        # look as server4.mydomain(UP)
        elts = hst_raw.split('(')
        hst = elts[0]
        hosts.append(hst)

    return hosts


# For a specific host, ask all VM on it to the VCenter
def get_vm_of_host(check_esx_path, vcenter, host, user, password):
    print "Listing host", host
    list_vm_cmd = [check_esx_path, '-D', vcenter, '-H', host,
                   '-u', user, '-p', password,
                   '-l', 'runtime', '-s', 'list']
    output = Popen(list_vm_cmd, stdout=PIPE).communicate()
    parts = output[0].split(':')
    # Maybe we got a 'CRITICAL - There are no VMs.' message,
    # if so, we bypass this host
    if len(parts) < 2:
        return None

    vms_raw = parts[1].split('|')[0]
    vms_raw_lst = vms_raw.split(',')

    lst = []
    for vm_raw in vms_raw_lst:
        vm_raw = vm_raw.strip()
        # look as MYVM(UP)
        elts = vm_raw.split('(')
        vm = elts[0]
        lst.append(vm)
    return lst


# Create all tuples of the links for the hosts
def create_all_links(res, rules):
    r = []
    for host in res:
        for vm in res[host]:
            # First we apply rules on the names
            host_name = _apply_rules(host, rules)
            vm_name = _apply_rules(vm, rules)
            v = (('host', host_name), ('host', vm_name))
            r.append(v)
    return r


def write_output(r, path):
    try:
        f = open(path + '.tmp', 'wb')
        buf = json.dumps(r)
        f.write(buf)
        f.close()
        shutil.move(path + '.tmp', path)
        print "File %s wrote" % path
    except IOError, exp:
        sys.exit("Error writing the file %s: %s" % (path, exp))


def main(check_esx_path, vcenter, user, password, output, rules):
    rules = _split_rules(rules)
    res = {}
    hosts = get_vmware_hosts(check_esx_path, vcenter, user, password)

    for host in hosts:
        lst = get_vm_of_host(check_esx_path, vcenter, host, user, password)
        if lst:
            res[host] = lst

    r = create_all_links(res, rules)
    print "Created %d links" % len(r)

    write_output(r, output)
    print "Finished!"


# Here we go!
if __name__ == "__main__":
    # Manage the options
    parser = optparse.OptionParser(
        version="Shinken VMware links dumping script version %s" % VERSION)
    parser.add_option("-o", "--output",
                      help="Path of the generated mapping file.")
    parser.add_option("-x", "--esx3-path", dest='check_esx_path',
                      default='/usr/local/nagios/libexec/check_esx3.pl',
                      help="Full path of the check_esx3.pl script (default: %default)")
    parser.add_option("-V", "--vcenter", '--Vcenter',
                      help="tThe IP/DNS address of your Vcenter host.")
    parser.add_option("-u", "--user",
                      help="User name to connect to this Vcenter")
    parser.add_option("-p", "--password",
                      help="The password of this user")
    parser.add_option('-r', '--rules', default='',
                      help="Rules of name transformation. Valid names are: "
                      "`lower`: to lower names, "
                      "`nofqdn`: keep only the first name (server.mydomain.com -> server)."
                      "You can use several rules like `lower|nofqdn`")

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    if opts.vcenter is None:
        parser.error("missing -V or --Vcenter option for the vcenter IP/DNS address")
    if opts.user is None:
        parser.error("missing -u or --user option for the vcenter username")
    if opts.password is None:
        error = True
        parser.error("missing -p or --password option for the vcenter password")
    if not os.path.exists(opts.check_esx_path):
        parser.error("the path %s for the check_esx3.pl script is wrong, missing file" % opts.check_esx_path)
    if opts.output is None:
        parser.error("missing -o or --output option for the output mapping file")

    main(**opts.__dict__)

########NEW FILE########
__FILENAME__ = link_xen_host_vm
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# /usr/local/shinken/libexec/link_xen_host_vm.py
# This file is proposed for Shinken to link vm and xenserver.
# Devers Renaud rdevers@chavers.org
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


import sys
import XenAPI
from string import split
import shutil
import optparse

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load
    # simple json (it can be hard json?! It's 2 functions guy!)
    try:
        import simplejson as json
    except ImportError:
        sys.exit("Error: you need the json or simplejson module for this script")

VERSION = '0.1'

# Split and clean the rules from a string to a list
def _split_rules(rules):
    return [r.strip() for r in rules.split('|')]


# Apply all rules on the objects names
def _apply_rules(name, rules):
    if 'nofqdn' in rules:
        name = name.split(' ', 1)[0]
        name = name.split('.', 1)[0]
    if 'lower' in rules:
        name = name.lower()
    return name


def create_all_links(res,rules):
    r = []
    for host in res:
        for vm in res[host]:
            # First we apply rules on the names
            host_name = _apply_rules(host,rules)
            vm_name = _apply_rules(vm,rules)
            v = (('host', host_name), ('host', vm_name))
            r.append(v)
    return r

def write_output(path,r):
    try:
        f = open(path + '.tmp', 'wb')
        buf = json.dumps(r)
        f.write(buf)
        f.close()
        shutil.move(path + '.tmp', path)
        print "File %s wrote" % path
    except IOError, exp:
        sys.exit("Error writing the file %s: %s" % (path, exp))

def con_poolmaster(xs, user, password):
  try:
    s = XenAPI.Session("http://%s" % xs)
    s.xenapi.login_with_password(user,password)
    return s
  except XenAPI.Failure, msg:
     if  msg.details[0] == "HOST_IS_SLAVE":
        host = msg.details[1]
        s = XenAPI.Session("http://%s" % host)
        s.xenapi.login_with_password(user, password)
        return s
     else:
        print "Error: pool con:",  xs, sys.exc_info()[0]
        pass
  except:
    print "Error: pool con:",  xs, sys.exc_info()[0]
    pass
  return None

def main(output, user, password, rules, xenserver):
  res = {}
  for xs in xenserver:
    try:
      s = con_poolmaster(xs, user, password)
      vms = s.xenapi.VM.get_all()
      for vm in vms:
        record = s.xenapi.VM.get_record(vm)
        if not(record["is_a_template"]) and not(record["is_control_domain"]):
          vhost = s.xenapi.VM.get_resident_on(vm)
          if vhost != "OpaqueRef:NULL":
            host = s.xenapi.host.get_hostname(vhost)
            vm_name = s.xenapi.VM.get_name_label(vm)
            if host in res.keys():
              res[host].append(vm_name)
            else:
              res[host] = [vm_name]
      s.xenapi.session.logout()
    except:
      pass
  r = create_all_links(res,rules)
  print "Created %d links" % len(r)

  write_output(output, r)
  print "Finished!"

if __name__ == "__main__":
    # Manage the options
    parser = optparse.OptionParser(
        version="Shinken XenServer/XCP links dumping script version %s" % VERSION)
    parser.add_option("-o", "--output",
                      default='/tmp/xen_mapping_file.json',
                      help="Path of the generated mapping file.")
    parser.add_option("-u", "--user",
                      help="User name to connect to this Vcenter")
    parser.add_option("-p", "--password",
                      help="The password of this user")
    parser.add_option('-r', '--rules', default='',
                      help="Rules of name transformation. Valid names are: "
                      "`lower`: to lower names, "
                      "`nofqdn`: keep only the first name (server.mydomain.com -> server)."
                      "You can use several rules like `lower|nofqdn`")
    parser.add_option('-x','--xenserver',action="append",
                      help="multiple ip/fqdn of your XenServer/XCP poll master (or member). "
                      "ex: -x poolmaster1 -x poolmaster2 -x poolmaster3 "
                      "If pool member was use, the poll master was found")

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    if opts.user is None:
        parser.error("missing -u or --user option for the pool master username")
    if opts.password is None:
        error = True
        parser.error("missing -p or --password option for the pool master password")
    if opts.output is None:
        parser.error("missing -o or --output option for the output mapping file")
    if opts.xenserver is None:
        parser.error("missing -x or --xenserver option for pool master list")

    main(**opts.__dict__)

########NEW FILE########
__FILENAME__ = notify_by_xmpp
#!/usr/bin/python -tt
# skvidal@fedoraproject.org, modified by David Laval
# gplv2+

## XMPP notification
#define command{
#    command_name    notify-host-by-xmpp
#    command_line    $PLUGINSDIR$/notify_by_xmpp.py -a $PLUGINSDIR$/notify_by_xmpp.ini "Host '$HOSTALIAS$' is $HOSTSTATE$ - Info : $HOSTOUTPUT$" $CONTACTEMAIL$
#}
#
#define command{
#    command_name    notify-service-by-xmpp
#    command_line    $PLUGINSDIR$/notify_by_xmpp.py -a $PLUGINSDIR$/notify_by_xmpp.ini "$NOTIFICATIONTYPE$ $HOSTNAME$ $SERVICED ESC$ $SERVICESTATE$ $SERVICEOUTPUT$ $LONGDATETIME$" $CONTACTEMAIL$
#}

# needs a config file to get username/pass/other info format is:

#[xmpp_account]
#server=jabber.org
#port=5222
#username=yourusername
#password=yourpasssword
#resource=monitoring

defaults = {'server':'jabber.org',
            'port':'5222',
            'resource':'monitoring'}

# until xmppony is inplace

import warnings
warnings.simplefilter("ignore")

import xmpp
from xmpp.protocol import Message


from optparse import OptionParser
import ConfigParser
import sys
import os


parser = OptionParser()
parser.add_option("-a", dest="authfile", default=None, help="file to retrieve username/password/server/port/resource information from")
opts, args = parser.parse_args()

conf = ConfigParser.ConfigParser(defaults=defaults)
if not opts.authfile or not os.path.exists(opts.authfile):
   print "no config/auth file specified, can't continue"
   sys.exit(1)

conf.read(opts.authfile)
if not conf.has_section('xmpp_account') or not conf.has_option('xmpp_account', 'username') or not conf.has_option('xmpp_account', 'password'):
    print "cannot find at least one of: config section 'xmpp_account' or username or password"
    sys.exit(1)
server = conf.get('xmpp_account', 'server')
username = conf.get('xmpp_account', 'username')
password = conf.get('xmpp_account', 'password')
resource = conf.get('xmpp_account', 'resource')
port = conf.get('xmpp_account', 'port')


if len(args) < 1:
    print "xmppsend message [to whom, multiple args]"
    sys.exit(1)

msg = args[0]

msg = msg.replace('\\n', '\n')

c = xmpp.Client(server=server, port=port, debug=[])
con  = c.connect()
if not con:
    print "Error: could not connect to server: %s:%s" % (c.Server, c.Port)
    sys.exit(1)

auth = c.auth(user=username, password=password, resource=resource)
if not auth:
    print "Error: Could not authenticate to server: %s:%s" % (c.Server, c.Port)
    sys.exit(1)

if len(args) < 2:
    r = c.getRoster()
    for user in r.keys():
        if user == username:
            continue
        c.send(Message(user, '%s' % msg))
else:
    for user in args[1:]:
        c.send(Message(user, '%s' % msg))


    

########NEW FILE########
__FILENAME__ = send_nsca
#!/usr/bin/env python
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Hanesse Olivier, olivier.hanesse@gmail.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import optparse

try:
    import pynsca
    from pynsca import NSCANotifier
except ImportError:
    raise SystemExit("Error: you need the pynsca module for this script")

VERSION = '0.1'


def main(hostname, port, encryption, password):
    notif = NSCANotifier(hostname, port, encryption, password)

    for line in sys.stdin.readlines():
        line = line.rstrip()
        if not line:
            continue
        notif = line.split(opts.delimiter)
        if len(notif) == 3:
            # only host, rc, output
            notif.insert(1, '')  # insert service
        # line consists of host, service, rc, output
        assert len(notif) == 4
        notif.svc_result(*notif)


if __name__ == "__main__":
    parser = optparse.OptionParser(
                      version="Python NSCA client version %s" % VERSION)
    parser.add_option("-H", "--hostname", default='localhost',
                      help="NSCA server IP (default: %default)")
    parser.add_option("-P", "--port", type="int", default='5667',
                      help="NSCA server port (default: %default)")
    parser.add_option("-e", "--encryption", default='1',
                      help=("Encryption mode used by NSCA server "
                            "(default: %default)"))
    parser.add_option("-p", "--password", default='helloworld',
                      help=("Password for encryption, should be the same as "
                            "NSCA server (default: %default)"))
    parser.add_option("-d", "--delimiter", default='\t',
                      help="Argument delimiter (defaults to the tab-character)")

    opts, args = parser.parse_args()

    if args:
        parser.error("does not take any positional arguments")

    main(opts.hostname, opts.port, opts.encryption, opts.password)

########NEW FILE########
__FILENAME__ = service_dependency_mapping
#!/usr/bin/env python
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This program get hosts informations from running arbiter daemon and 
get service dependencies definition from config pack flat files then
dump services dependencies according to the config files to a json
that can be loaded in hot_dependencies_arbiter module.

servicedependencies file in pack use template host_name that will be
matched in hosts 'use' directive to apply those servicedependency
definition to hosts.

"""


import os, sys, optparse, cPickle, shutil
import shinken.daemons.arbiterdaemon
from shinken.arbiterlink import ArbiterLink
from shinken.http_client import HTTPExceptions 
from shinken.log import logger
from shinken.objects.config import Config

# Try to load json (2.5 and higer) or simplejson if failed (python2.4)
try:
    import json
except ImportError:
    # For old Python version, load simple json
    try:
        import simplejson as json
    except ImportError:
        raise SystemExit("Error: you need the json or simplejson module "
                         "for this script")


sat_types = ['arbiter', 'scheduler', 'poller', 'reactionner',
             'receiver', 'broker']

VERSION = '0.2'

class ShinkenAdmin():

    def __init__(self):
        self.arb = None 
        self.conf = None
        self.addr = 'localhost'
        self.port = '7770'
        self.arb_name = 'arbiter-master'

    def do_connect(self, verbose=False):
        '''
        Connect to an arbiter daemon
        Syntax: connect [host]:[port]
        Ex: for Connecting to server, port 7770
        > connect server:7770
        Ex: connect to localhost, port 7770
        > connect
        '''
    
        if verbose:
            print "Connection to %s:%s" % (self.addr, self.port)
        ArbiterLink.use_ssl = False
        self.arb = ArbiterLink({'arbiter_name': self.arb_name, 'address': self.addr, 'port': self.port})
        self.arb.fill_default()
        self.arb.pythonize()
        self.arb.update_infos()
        if not self.arb.reachable:
            sys.exit("Connection to the arbiter got a problem")
        print "Connection OK"
    
    def getconf(self, config):
        '''
        Get the data in the arbiter for a table and some properties
        like hosts  host_name realm
        '''
        files = [config]
        conf = Config()
        conf.read_config_silent = 1

        # Get hosts objects
        properties = [ 'host_name','use','act_depend_of']
        hosts = self.arb.get_objects_properties('hosts', properties)

        # Get services dependencies
        svcdep_buf = conf.read_config(files)
        svc_dep = conf.read_config_buf(svcdep_buf)['servicedependency']

        return (hosts, svc_dep)

    def load_svc_mapping(self, hosts, svc_dep, verbose=False):
        '''
        Make tuples mapping service dependencies. Return a list of tuples 
        and need hosts and service dependencies parameter.
        '''
        r = []
        # Search for host matching "use" template
        for dep in svc_dep:
            # Get host_name and dependent_host_name field from servicedependency
            # config file in packs. Usually values are host's pack template.
            parent_host_name = self.split_and_merge(dep['host_name'])
            try:
                dependent_host_name = self.split_and_merge(dep['dependent_host_name'])
            except KeyError:
                dependent_host_name = parent_host_name
            if verbose:
                print ""
                print 'Service dependency host_name', parent_host_name
                print 'Service dependency dependent_host_name', dependent_host_name

            # Make list before process them by splitting comma separated values.
            dep['service_description'] = self.split_and_merge(dep['service_description'])
            dep['dependent_service_description'] = self.split_and_merge(dep['dependent_service_description'])
            # Construct dependencies tuples
            # Search in host all hosts that use template host_name
            parent_svc_tuples = []
            dependent_svc_tuples = []
            for parent_svc in dep['service_description']:
                parent_svc_tuples += [[ ('service', host[0] + "," + parent_svc) for host in hosts if host_name in host[1] ] for host_name in parent_host_name ]
            for dependent_svc in dep['dependent_service_description']:
                dependent_svc_tuples += [[ ('service', host[0] + "," + dependent_svc) for host in hosts if host_name in host[1] ] for host_name in dependent_host_name ]

            # No need to separate tuples by services here so we merge them
            dependent_tuples = self.split_and_merge(dependent_svc_tuples, split=False)

            if verbose:
                print 'Parent service dependencies tuples list', parent_svc_tuples
                print 'Dependent service dependencies tuples list', dependent_svc_tuples

            # Process !
            for parent_tuples in parent_svc_tuples:
                r.append(self.make_all_dep_tuples(hosts, parent_tuples, dependent_tuples))

        if verbose:
            print ""
            print "Result:", r
        return r

    def make_all_dep_tuples(self, hosts, parent_tuples=[()], dependent_tuples=[[()]] ):
        '''
        List imbrication : List_by_services : [ List_by_hosts : [ Service_dependency_tuples : ( ) ] ]
        '''
        res = []
        for ptuple in parent_tuples:
            parent = { 'host_name' : self.get_dependency_tuple_host_name(ptuple), 'svc_desc' : self.get_dependency_tuple_service_description(ptuple) }
            # Dive into dependent services
            for dtuple in dependent_tuples:
                dependent = { 'host_name' : self.get_dependency_tuple_host_name(dtuple), 'svc_desc' : self.get_dependency_tuple_service_description(dtuple) }
                dependent['host_object'] = next( host for host in hosts if host[0] == dependent['host_name'] )
                res = self.make_dep_tuple(parent, dependent, ptuple, dtuple, res)

        return res

    def make_dep_tuple(self, parent, dependent, ptuple, dtuple, res):
        '''
        Search host dependency and make tuple according to it.
        '''
        try:
            dependent_host_parent = self.get_host_dependency(dependent['host_object'])
            if parent['host_name'] == dependent_host_parent:
                res = (ptuple, dtuple)
        except IndexError:
            if parent['host_name'] == dependent['host_name']:
                res = (ptuple, dtuple)

        return res

    def get_host_dependency(self, dependent_host):
        '''
        Get parent host_name attribute of host.
        '''
        return dependent_host[2][0][0].host_name

    def get_dependency_tuple_host_name(self, tuple):
        '''
        Just get the host name part of a dependency tuple.
        A dependency tuples is : ( 'service', 'host_name, service_description' )
        '''
        return tuple[1].split(',')[0]

    def get_dependency_tuple_service_description(self, tuple):
        '''
        Just get the service description part of a dependency tuple.
        A dependency tuples is : ( 'service', 'host_name, service_description' )
        '''
        return tuple[1].split(',')[1]


    def split_and_merge(self, list, split=True):
        '''
        Split a list on comma separator and merge resulting lists
        into an uniq list then return it
        '''
        res = []
        for elt in list:
            if split:
                res += elt.split(',')
            else:
                res += elt
        return res

    def clean_empty_value(self, r):
        '''
        Empty value comes from unused config pack and then service dep
        is created but without nothing...
        '''
        r_cleaned = []
        for elt in r:
            if elt != []:
                r_cleaned.append(elt)

        return r_cleaned

    def main(self, output_file, config, verbose):
        self.do_connect(verbose)

        # Get needed conf
        hosts, svc_dep = self.getconf(config)
        if verbose:
            print "Hosts:", hosts
            print "Service Dep:", svc_dep

        # Make the map
        r = self.load_svc_mapping(hosts, svc_dep, verbose)

        # Clean mapping from empty value
        r = self.clean_empty_value(r)

        # Write ouput file
        try:
            f = open(output_file + '.tmp', 'wb')
            buf = json.dumps(r)
            f.write(buf)
            f.close()
            shutil.move(output_file + '.tmp', output_file)
            print "File %s wrote" % output_file
        except IOError, exp:
            sys.exit("Error writing the file %s: %s" % (output_file, exp))
        jsonmappingfile = open(output_file, 'w')
        try:
            json.dump(r, jsonmappingfile)
        finally:
            jsonmappingfile.close()


if __name__ == "__main__":
    parser = optparse.OptionParser(
        version="Shinken service hot dependency according to packs (or custom) definition to json mapping %s" % VERSION)
    parser.add_option("-o", "--output", dest='output_file',
                      default='/tmp/shinken_service_dependency:mapping.json',
                      help="Path of the generated json mapping file.")
    parser.add_option('-c', '--config', dest='config', help='Shinken main config file.')
    parser.add_option('-v', '--verbose', action='store_true', dest='verbose', help='More verbosity. Used to debug')

    opts, args = parser.parse_args()
    if args:
        parser.error("does not take any positional arguments")

    ShinkenAdmin().main(**vars(opts))

########NEW FILE########
__FILENAME__ = generate_manpages
#!/usr/bin/env python

# Author: Thibault Cohen <thibault.cohen@savoirfairelinux.com>
# Inspired from http://docutils.sourceforge.net/tools/rst2man.py

import locale
import os
try:
    locale.setlocale(locale.LC_ALL, '')
except:
    pass

from docutils.core import publish_file
from docutils.writers import manpage


output_folder = os.path.join(os.path.abspath(os.path.dirname(__file__)), "manpages")
source_folder = os.path.join(os.path.abspath(os.path.dirname(__file__)), "sources")

for current_folder, subfolders, files in os.walk(source_folder):
    for rst_file in files:
        if rst_file.endswith(".rst"):
            input_file = os.path.join(current_folder, rst_file)
            output_file = os.path.join(output_folder, os.path.splitext(rst_file)[0] + ".8")
            publish_file(source_path=input_file,
                         destination_path=output_file,
                         writer=manpage.Writer()
                         )

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This Class is an example of an Arbiter module
# Here for the configuration phase AND running one

import time

from shinken.basemodule import BaseModule
from shinken.external_command import ExternalCommand
from shinken.log import logger

properties = {
    'daemons': ['arbiter'],
    'type': 'dummy_arbiter',
    'external': True,
    }


# called by the plugin manager to get a broker
def get_instance(plugin):
    logger.info("[Dummy Arbiter] Get a Dummy arbiter module for plugin %s" % plugin.get_name())
    instance = Dummy_arbiter(plugin)
    return instance


# Just print some stuff
class Dummy_arbiter(BaseModule):
    def __init__(self, mod_conf):
        BaseModule.__init__(mod_conf)

    # Called by Arbiter to say 'let's prepare yourself guy'
    def init(self):
        logger.info("[Dummy Arbiter] Initialization of the dummy arbiter module")
        #self.return_queue = self.properties['from_queue']


    # Ok, main function that is called in the CONFIGURATION phase
    def get_objects(self):
        logger.info("[Dummy Arbiter] Ask me for objects to return")
        r = {'hosts': []}
        h = {'name': 'dummy host from dummy arbiter module',
             'register': '0',
             }

        r['hosts'].append(h)
        r['hosts'].append({
                            'host_name': "dummyhost1",
                            'use': 'linux-server',
                            'address': 'localhost'
                            })
        logger.info("[Dummy Arbiter] Returning to Arbiter the hosts: %s" % str(r))

        return r

    def hook_late_configuration(self, conf):
        logger.info("[Dummy Arbiter] Dummy in hook late config")

    def do_loop_turn(self):
        logger.info("[Dummy Arbiter] Raise a external command as example")
        e = ExternalCommand('Viva la revolution')
        self.from_q.put(e)
        time.sleep(1)

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This Class is an example of a broker  module

from shinken.basemodule import BaseModule
from shinken.log import logger

properties = {
    'daemons': ['broker'],
    'type': 'dummy_broker',
    'external': False,
    }


# called by the plugin manager to get a broker
def get_instance(mod_conf):
    logger.info("[Dummy Broker] Get a Dummy broker module for plugin %s" % mod_conf.get_name())
    instance = Dummy_broker(mod_conf)
    return instance


# Just print some stuff
class Dummy_broker(BaseModule):

    def __init__(self, mod_conf, foo):
        BaseModule.__init__(self, mod_conf)


    # Called by Broker to say 'let's prepare yourself guy'
    def init(self):
        logger.info("[Dummy Broker] Initialization of the dummy broker module")


      
    # An host check have just arrived, we UPDATE data info with this                                                                                                      
#    def manage_brok(self, b):
#        #Do things
#        pass
    

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This Class is an example of a broker  module

import time

from shinken.basemodule import BaseModule
from shinken.log import logger

properties = {
    'daemons': ['broker'],
    'type': 'dummy_broker_external',
    'external': True,
    }


# called by the plugin manager to get a broker
def get_instance(mod_conf):
    logger.info("[Dummy Broker] Get a Dummy broker module for plugin %s" % mod_conf.get_name())
    instance = Dummy_broker(mod_conf)
    return instance


# Just print some stuff
class Dummy_broker(BaseModule):

    def __init__(self, mod_conf):
        BaseModule.__init__(self, mod_conf)


    # Called by Broker to say 'let's prepare yourself guy'
    def init(self):
        logger.info("[Dummy Broker] Initialization of the dummy broker module")


    
    # When you are in "external" mode, that is the main loop of your process
    def main(self):
        self.set_proctitle(self.name)

        self.set_exit_handler()
        i = 0
        while not self.interrupted:
            i += 1
            time.sleep(0.1)
            if i % 10 == 0:
                logger.info('[Dummy Broker External] Ping')

        logger.info('[Dummy Broker External] Exiting')

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This Class is an example of an Scheduler module
# Here for the configuration phase AND running one

import sys
import signal
import time
from Queue import Empty

from shinken.basemodule import BaseModule
from shinken.log import logger

properties = {
    'daemons': ['poller'],
    'type': 'dummy_poller',
    'external': False,
    # To be a real worker module, you must set this
    'worker_capable': True,
}


# called by the plugin manager to get a broker
def get_instance(mod_conf):
    logger.info("[Dummy Poller] Get a Dummy poller module for plugin %s" % mod_conf.get_name())
    instance = Dummy_poller(mod_conf)
    return instance


# Just print some stuff
class Dummy_poller(BaseModule):

    def __init__(self, mod_conf):
        BaseModule.__init__(self, mod_conf)

    # Called by poller to say 'let's prepare yourself guy'
    def init(self):
        logger.info("[Dummy Poller] Initialization of the dummy poller module")
        self.i_am_dying = False

    # Get new checks if less than nb_checks_max
    # If no new checks got and no check in queue,
    # sleep for 1 sec
    # REF: doc/shinken-action-queues.png (3)
    def get_new_checks(self):
        try:
            while(True):
                logger.debug("[Dummy Poller] I %d wait for a message" % self.id)
                msg = self.s.get(block=False)
                if msg is not None:
                    self.checks.append(msg.get_data())
                logger.debug("[Dummy Poller] I, %d, got a message!" % self.id)
        except Empty, exp:
            if len(self.checks) == 0:
                time.sleep(1)

    # Launch checks that are in status
    # REF: doc/shinken-action-queues.png (4)
    def launch_new_checks(self):
        # queue
        for chk in self.checks:
            if chk.status == 'queue':
                logger.warning("[Dummy Poller] Dummy (bad) check for %s" % str(chk.command))
                chk.exit_status = 2
                chk.get_outputs('All is NOT SO well', 8012)
                chk.status = 'done'
                chk.execution_time = 0.1

    # Check the status of checks
    # if done, return message finished :)
    # REF: doc/shinken-action-queues.png (5)
    def manage_finished_checks(self):
        to_del = []
        for action in self.checks:
            to_del.append(action)
            try:
                self.returns_queue.put(action)
            except IOError, exp:
                logger.info("[Dummy Poller] %d exiting: %s" % (self.id, exp))
                sys.exit(2)
        for chk in to_del:
            self.checks.remove(chk)

    # id = id of the worker
    # s = Global Queue Master->Slave
    # m = Queue Slave->Master
    # return_queue = queue managed by manager
    # c = Control Queue for the worker
    def work(self, s, returns_queue, c):
        logger.info("[Dummy Poller] Module Dummy started!")
        ## restore default signal handler for the workers:
        signal.signal(signal.SIGTERM, signal.SIG_DFL)
        timeout = 1.0
        self.checks = []
        self.returns_queue = returns_queue
        self.s = s
        self.t_each_loop = time.time()
        while True:
            begin = time.time()
            msg = None
            cmsg = None

            # If we are dying (big problem!) we do not
            # take new jobs, we just finished the current one
            if not self.i_am_dying:
                # REF: doc/shinken-action-queues.png (3)
                self.get_new_checks()
                # REF: doc/shinken-action-queues.png (4)
                self.launch_new_checks()
            # REF: doc/shinken-action-queues.png (5)
            self.manage_finished_checks()

            # Now get order from master
            try:
                cmsg = c.get(block=False)
                if cmsg.get_type() == 'Die':
                    logger.info("[Dummy Poller] %d : Dad say we are dying..." % self.id)
                    break
            except:
                pass

            timeout -= time.time() - begin
            if timeout < 0:
                timeout = 1.0

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# This Class is an example of an Scheduler module
# Here for the configuration phase AND running one

from shinken.basemodule import BaseModule
from shinken.log import logger

properties = {
    'daemons': ['scheduler'],
    'type': 'dummy_scheduler',
    'external': False,
    'phases': ['retention'],
    }


# called by the plugin manager to get a broker
def get_instance(mod_conf):
    logger.info("[Dummy Scheduler] Get a Dummy scheduler module for plugin %s" % mod_conf.get_name())
    instance = Dummy_scheduler(mod_conf, foo="bar")
    return instance


# Just print some stuff
class Dummy_scheduler(BaseModule):

    def __init__(self, mod_conf, foo):
        BaseModule.__init__(self, mod_conf)
        self.myfoo = foo

    # Called by Scheduler to say 'let's prepare yourself guy'
    def init(self):
        logger.info("[Dummy Scheduler] Initialization of the dummy scheduler module")
        # self.return_queue = self.properties['from_queue']


    # Ok, main function that is called in the retention creation pass
    def update_retention_objects(self, sched, log_mgr):
        logger.info("[Dummy Scheduler] Asking me to update the retention objects")

    # Should return if it succeed in the retention load or not
    def load_retention_objects(self, sched, log_mrg):
        logger.info("[Dummy Scheduler] Asking me to load the retention objects")
        return False

# From now external is not used in the scheduler job
#    #When you are in "external" mode, that is the main loop of your process
#    def main(self):
#        while True:
#            print "Raise a external command as example"
#            e = ExternalCommand('Viva la revolution')
#            self.return_queue.put(e)
#            time.sleep(1)

########NEW FILE########
__FILENAME__ = acknowledge
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class Acknowledge:
    """
    Allows you to acknowledge the current problem for the specified service.
    By acknowledging the current problem, future notifications (for the same
    servicestate) are disabled.
    """
    id = 1

    # Just to list the properties we will send as pickle
    # so to others daemons, all but NOT REF
    properties = {
        'id': None,
        'sticky': None,
        'notify': None,
        'end_time': None,
        'author': None,
        'comment': None,
        }
    # If the "sticky" option is set to one (1), the acknowledgement
    # will remain until the service returns to an OK state. Otherwise
    # the acknowledgement will automatically be removed when the
    # service changes state. In this case Web interfaces set a value
    # of (2).
    #
    # If the "notify" option is set to one (1), a notification will be
    # sent out to contacts indicating that the current service problem
    # has been acknowledged.
    #
    # <WTF??>
    # If the "persistent" option is set to one (1), the comment
    # associated with the acknowledgement will survive across restarts
    # of the Shinken process. If not, the comment will be deleted the
    # next time Shinken restarts. "persistent" not only means "survive
    # restarts", but also
    #
    # => End of comment Missing!!
    # </WTF??>

    def __init__(self, ref, sticky, notify, persistent,
                 author, comment, end_time=0):
        self.id = self.__class__.id
        self.__class__.id += 1
        self.ref = ref  # pointer to srv or host we are applied
        self.sticky = sticky
        self.notify = notify
        self.end_time = end_time
        self.author = author
        self.comment = comment

    # Call by pickle for dataify the ackn
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)
        return res

    # Inversed function of getstate
    def __setstate__(self, state):
        cls = self.__class__
        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])
        # If load a old ack, set the end_time to 0 which refers to infinite
        if not hasattr(self, 'end_time'):
            self.end_time = 0

########NEW FILE########
__FILENAME__ = action
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import time
import shlex
import sys
import subprocess
import signal

# Try to read in non-blocking mode, from now this only from now on
# Unix systems
try:
    import fcntl
except ImportError:
    fcntl = None

from shinken.util import safe_print
from shinken.log import logger

__all__ = ('Action')

valid_exit_status = (0, 1, 2, 3)

only_copy_prop = ('id', 'status', 'command', 't_to_go', 'timeout',
                  'env', 'module_type', 'execution_time', 'u_time', 's_time')

shellchars = ('!', '$', '^', '&', '*', '(', ')', '~', '[', ']',
                   '|', '{', '}', ';', '<', '>', '?', '`')


# Try to read a fd in a non blocking mode
def no_block_read(output):
    fd = output.fileno()
    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
    try:
        return output.read()
    except:
        return ''




class __Action(object):
    """
    This abstract class is used just for having a common id for both
    actions and checks.
    """
    id = 0

    # Ok when we load a previous created element, we should
    # not start at 0 for new object, so we must raise the Action.id
    # if need
    @staticmethod
    def assume_at_least_id(_id):
        Action.id = max(Action.id, _id)


    def set_type_active(self):
        "Dummy function, only useful for checks"
        pass

    def set_type_passive(self):
        "Dummy function, only useful for checks"
        pass

    def get_local_environnement(self):
        """

        Mix the env and the environment variables into a new local
        env dict.

        Note: We cannot just update the global os.environ because this
        would effect all other checks.
        """
        # Do not use copy.copy() here, as the resulting copy still
        # changes the real environment (it is still a os._Environment
        # instance).
        local_env = os.environ.copy()
        for p in self.env:
            local_env[p] = self.env[p].encode('utf8')
        return local_env


    def execute(self):
        """
        Start this action command. The command will be executed in a
        subprocess.
        """

        self.status = 'launched'
        self.check_time = time.time()
        self.wait_time = 0.0001
        self.last_poll = self.check_time
        # Get a local env variables with our additional values
        self.local_env = self.get_local_environnement()

        # Initialize stdout and stderr. we will read them in small parts
        # if the fcntl is available
        self.stdoutdata = ''
        self.stderrdata = ''

        return self.execute__()  ## OS specific part


    def get_outputs(self, out, max_plugins_output_length):
        #print "Get only," , max_plugins_output_length, "bytes"
        # Squeeze all output after max_plugins_output_length
        out = out[:max_plugins_output_length]
        # manage escaped pipes
        out = out.replace('\|', '___PROTECT_PIPE___')
        # Then cuts by lines
        elts = out.split('\n')
        # For perf data
        elts_line1 = elts[0].split('|')
        # First line before | is output, and strip it
        self.output = elts_line1[0].strip().replace('___PROTECT_PIPE___', '|')
        # Init perfdata as void
        self.perf_data = ''
        # After | is perfdata, and strip it
        if len(elts_line1) > 1:
            self.perf_data = elts_line1[1].strip().replace('___PROTECT_PIPE___', '|')
        # Now manage others lines. Before the | it's long_output
        # And after it's all perf_data, \n join
        long_output = []
        in_perfdata = False
        for line in elts[1:]:
            # if already in perfdata, direct append
            if in_perfdata:
                self.perf_data += ' ' + line.strip().replace('___PROTECT_PIPE___', '|')
            else:  # not already in? search for the | part :)
                elts = line.split('|', 1)
                # The first part will always be long_output
                long_output.append(elts[0].strip().replace('___PROTECT_PIPE___', '|'))
                if len(elts) > 1:
                    in_perfdata = True
                    self.perf_data += ' ' + elts[1].strip().replace('___PROTECT_PIPE___', '|')
        # long_output is all non output and perfline, join with \n
        self.long_output = '\n'.join(long_output)


    def check_finished(self, max_plugins_output_length):
        # We must wait, but checks are variable in time
        # so we do not wait the same for an little check
        # than a long ping. So we do like TCP: slow start with *2
        # but do not wait more than 0.1s.
        self.last_poll = time.time()

        _, _, child_utime, child_stime, _ = os.times()
        if self.process.poll() is None:
            self.wait_time = min(self.wait_time * 2, 0.1)
            #time.sleep(wait_time)
            now = time.time()

            # If the fcntl is available (unix) we try to read in a
            # asynchronous mode, so we won't block the PIPE at 64K buffer
            # (deadlock...)
            if fcntl:
                self.stdoutdata += no_block_read(self.process.stdout)
                self.stderrdata += no_block_read(self.process.stderr)


            if (now - self.check_time) > self.timeout:
                self.kill__()
                #print "Kill for timeout", self.process.pid,
                #print self.command, now - self.check_time
                self.status = 'timeout'
                self.execution_time = now - self.check_time
                self.exit_status = 3
                # Do not keep a pointer to the process
                del self.process
                # Get the user and system time
                _, _, n_child_utime, n_child_stime, _ = os.times()
                self.u_time = n_child_utime - child_utime
                self.s_time = n_child_stime - child_stime
                return
            return

        # Get standards outputs from the communicate function if we do
        # not have the fcntl module (Windows, and maybe some special
        # unix like AIX)
        if not fcntl:
            (self.stdoutdata, self.stderrdata) = self.process.communicate()
        else:
            # The command was to quick and finished even before we can
            # polled it first. So finish the read.
            self.stdoutdata += no_block_read(self.process.stdout)
            self.stderrdata += no_block_read(self.process.stderr)

        self.exit_status = self.process.returncode

        # we should not keep the process now
        del self.process

        # if the exit status is abnormal, we add stderr to the output
        # TODO: Abnormal should be logged properly no?
        if self.exit_status not in valid_exit_status:
            self.stdoutdata = self.stdoutdata + self.stderrdata
        elif ('sh: -c: line 0: unexpected EOF while looking for matching'
              in self.stderrdata
              or ('sh: -c:' in self.stderrdata and ': Syntax' in self.stderrdata)
              or 'sh: Syntax error: Unterminated quoted string'
              in self.stderrdata):
            # Very, very ugly. But subprocess._handle_exitstatus does
            # not see a difference between a regular "exit 1" and a
            # bailing out shell. Strange, because strace clearly shows
            # a difference. (exit_group(1) vs. exit_group(257))
            self.stdoutdata = self.stdoutdata + self.stderrdata
            self.exit_status = 3
        # Now grep what we want in the output
        self.get_outputs(self.stdoutdata, max_plugins_output_length)

        # We can clean the useless properties now
        del self.stdoutdata
        del self.stderrdata

        self.status = 'done'
        self.execution_time = time.time() - self.check_time
        # Also get the system and user times
        _, _, n_child_utime, n_child_stime, _ = os.times()
        self.u_time = n_child_utime - child_utime
        self.s_time = n_child_stime - child_stime


    def copy_shell__(self, new_i):
        """
        Copy all attributes listed in 'only_copy_prop' from `self` to
        `new_i`.
        """
        for prop in only_copy_prop:
            setattr(new_i, prop, getattr(self, prop))
        return new_i


    def got_shell_characters(self):
        for c in self.command:
            if c in shellchars:
                return True
        return False


###
### OS specific "execute__" & "kill__" are defined by "Action" class
### definition:
###

if os.name != 'nt':

    class Action(__Action):

        # We allow direct launch only for 2.7 and higher version
        # because if a direct launch crash, under this the file handles
        # are not releases, it's not good.
        def execute__(self, force_shell=sys.version_info < (2, 7)):
            # If the command line got shell characters, we should go
            # in a shell mode. So look at theses parameters
            force_shell |= self.got_shell_characters()

            # 2.7 and higher Python version need a list of args for cmd
            # and if not force shell (if, it's useless, even dangerous)
            # 2.4->2.6 accept just the string command
            if sys.version_info < (2, 7) or force_shell:
                cmd = self.command.encode('utf8', 'ignore')
            else:
                try:
                    cmd = shlex.split(self.command.encode('utf8', 'ignore'))
                except Exception, exp:
                    self.output = 'Not a valid shell command: ' + exp.__str__()
                    self.exit_status = 3
                    self.status = 'done'
                    self.execution_time = time.time() - self.check_time
                    return


            #safe_print("Launching", cmd)
            #safe_print("With env", self.local_env)

            # Now: GO for launch!
            # logger.debug("Launching: %s" % (self.command.encode('utf8', 'ignore')))

            # The preexec_fn=os.setsid is set to give sons a same
            # process group. See
            # http://www.doughellmann.com/PyMOTW/subprocess/ for
            # detail about this.
            try:
                self.process = subprocess.Popen(
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                    close_fds=True, shell=force_shell, env=self.local_env,
                    preexec_fn=os.setsid)
            except OSError, exp:
                logger.error("Fail launching command: %s %s %s"
                             % (self.command, exp, force_shell))
                # Maybe it's just a shell we try to exec. So we must retry
                if (not force_shell and exp.errno == 8
                    and exp.strerror == 'Exec format error'):
                    return self.execute__(True)
                self.output = exp.__str__()
                self.exit_status = 2
                self.status = 'done'
                self.execution_time = time.time() - self.check_time

                # Maybe we run out of file descriptor. It's not good at all!
                if exp.errno == 24 and exp.strerror == 'Too many open files':
                    return 'toomanyopenfiles'

        def kill__(self):
            # We kill a process group because we launched them with
            # preexec_fn=os.setsid and so we can launch a whole kill
            # tree instead of just the first one
            os.killpg(self.process.pid, signal.SIGKILL)
            # Try to force close the descriptors, because python seems to have problems with them
            for fd in [self.process.stdout, self.process.stderr]:
                try:
                    fd.close()
                except:
                    pass


else:

    import ctypes
    TerminateProcess = ctypes.windll.kernel32.TerminateProcess


    class Action(__Action):

        def execute__(self):
            # 2.7 and higher Python version need a list of args for cmd
            # 2.4->2.6 accept just the string command
            if sys.version_info < (2, 7):
                cmd = self.command
            else:
                try:
                    cmd = shlex.split(self.command.encode('utf8', 'ignore'))
                except Exception, exp:
                    self.output = 'Not a valid shell command: ' + exp.__str__()
                    self.exit_status = 3
                    self.status = 'done'
                    self.execution_time = time.time() - self.check_time
                    return

            try:
                self.process = subprocess.Popen(
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                    env=self.local_env, shell=True)
            except WindowsError, exp:
                logger.info("We kill the process: %s %s" % (exp, self.command))
                self.status = 'timeout'
                self.execution_time = time.time() - self.check_time

        def kill__(self):
            TerminateProcess(int(self.process._handle), -1)

########NEW FILE########
__FILENAME__ = arbiterlink
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import socket

from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.http_client import HTTPExceptions
from shinken.log import logger


""" TODO: Add some comment about this class for the doc"""
class ArbiterLink(SatelliteLink):
    id = 0
    my_type = 'arbiter'
    properties = SatelliteLink.properties.copy()
    properties.update({
        'arbiter_name':    StringProp(),
        'host_name':       StringProp(default=socket.gethostname()),
        'port':            IntegerProp(default='7770'),
    })

    def get_name(self):
        return self.arbiter_name

    def get_config(self):
        return self.con.get('get_config')

    # Check is required when prop are set:
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        for prop, entry in cls.properties.items():
            if not hasattr(self, prop) and entry.required:
                # This should raise an error afterwards?
                # Log the issue
                logger.warning("%s arbiterlink is missing %s property" % (self.get_name(), prop))
                self.debug("%s arbiterlink is missing %s property" % (self.get_name(), prop))
                state = False  # Bad boy...
        return state


    # Look for ourself as an arbiter. If we search for a specific arbiter name, go forit
    # If not look be our fqdn name, or if not, our hostname
    def is_me(self, lookup_name):
        logger.info("And arbiter is launched with the hostname:%s from an arbiter point of view of addr:%s" % (self.host_name, socket.getfqdn()))
        if lookup_name:
            return lookup_name == self.get_name()
        else:
            return self.host_name == socket.getfqdn() or self.host_name == socket.gethostname()


    def give_satellite_cfg(self):
        return {'port': self.port, 'address': self.address, 'name': self.arbiter_name, 'use_ssl':self.use_ssl, 'hard_ssl_name_check':self.hard_ssl_name_check}


    def do_not_run(self):
        if self.con is None:
            self.create_connection()
        try:
            self.con.get('do_not_run')
            return True
        except HTTPExceptions, exp:
            self.con = None
            return False

    def get_satellite_list(self, daemon_type):
        if self.con is None:
            self.create_connection()
        try:
            r = self.con.get_satellite_list(daemon_type)
            return r
        except HTTPExceptions, exp:
            self.con = None
            return []


    def get_satellite_status(self, daemon_type, name):
        if self.con is None:
            self.create_connection()
        try:
            r = self.con.get_satellite_status(daemon_type, name)
            return r
        except HTTPExceptions, exp:
            self.con = None
            return {}


    def get_all_states(self):
        if self.con is None:
            self.create_connection()
        try:
            r = self.con.get('get_all_states')
            return r
        except HTTPExceptions, exp:
            self.con = None
            return None


    def get_objects_properties(self, table, properties=[]):
        if self.con is None:
            self.create_connection()
        try:
            print properties
            r = self.con.get('get_objects_properties', {'table' : table, 'properties' : properties})
            return r
        except HTTPExceptions, exp:
            self.con = None
            return None


class ArbiterLinks(SatelliteLinks):
    name_property = "name"
    inner_class = ArbiterLink


    # We must have a realm property, so we find our realm
    def linkify(self, modules):
        self.linkify_s_by_plug(modules)

########NEW FILE########
__FILENAME__ = autoslots
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""The AutoSlots Class is a MetaClass: it manages how other classes
 are created (Classes, not instances of theses classes).
 Here it's role is to create the __slots__ list of the class with
 all properties of Class.properties and Class.running_properties
 so we do not have to add manually all properties to the __slots__
 list when we add a new entry"""


class AutoSlots(type):

    # new is call when we create a new Class
    # that have metaclass = AutoSlots
    # CLS is AutoSlots
    # name is string of the Class (like Service)
    # bases are the Classes of which Class inherits (like SchedulingItem)
    # dct is the new Class dict (like all method of Service)
    # Some properties names are not allowed in __slots__ like 2d_coords of
    # Host, so we must tag them in properties with no_slots
    def __new__(cls, name, bases, dct):
        # Thanks to Bertrand Mathieu to the set idea
        slots = dct.get('__slots__', set())
        # Now get properties from properties and running_properties
        if 'properties' in dct:
            props = dct['properties']
            slots.update((p for p in props
                          if not props[p].no_slots))
        if 'running_properties' in dct:
            props = dct['running_properties']
            slots.update((p for p in props
                          if not props[p].no_slots))
        dct['__slots__'] = tuple(slots)
        return type.__new__(cls, name, bases, dct)

########NEW FILE########
__FILENAME__ = basemodule
#!/usr/bin/env python
#
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This python module contains the class BaseModule
that shinken modules will subclass
"""

import os
import signal
import time
from re import compile
from multiprocessing import Queue, Process

from shinken.log import logger

# TODO: use a class for defining the module "properties" instead of
# plain dict??  Like:
'''
class ModuleProperties(object):
    def __init__(self, type, phases, external=False)
        self.type = type
        self.phases = phases
        self.external = external
'''
# and  have the new modules instanciate this like follow:
'''
properties = ModuleProperties('the_module_type', the_module_phases, is_mod_ext)
'''

# The `properties dict defines what the module can do and
# if it's an external module or not.
properties = {
    # name of the module type ; to distinguish between them:
    'type': None,

    # is the module "external" (external means here a daemon module)?
    'external': True,

    # Possible configuration phases where the module is involved:
    'phases': ['configuration', 'late_configuration', 'running', 'retention'],
    }


class ModulePhases:
    """TODO: Add some comment about this class for the doc"""
    # TODO: why not use simply integers instead of string
    # to represent the different phases??
    CONFIGURATION = 1
    LATE_CONFIGURATION = 2
    RUNNING = 4
    RETENTION = 8


class BaseModule(object):
    """This is the base class for the shinken modules.
    Modules can be used by the different shinken daemons/services
    for different tasks.
    Example of task that a shinken module can do:
     - load additional configuration objects.
     - recurrently save hosts/services status/perfdata
       informations in different format.
     - ...
     """

    def __init__(self, mod_conf):
        """Instanciate a new module.
        There can be many instance of the same type.
        'mod_conf' is module configuration object
        for this new module instance.
        """
        self.myconf = mod_conf
        self.name = mod_conf.get_name()
        # We can have sub modules
        self.modules = getattr(mod_conf, 'modules', [])
        self.props = mod_conf.properties.copy()
        # TODO: choose between 'props' or 'properties'..
        self.interrupted = False
        self.properties = self.props
        self.is_external = self.props.get('external', False)
        # though a module defined with no phase is quite useless .
        self.phases = self.props.get('phases', [])
        self.phases.append(None)
        # the queue the module will receive data to manage
        self.to_q = None
        # the queue the module will put its result data
        self.from_q = None
        self.process = None
        self.illegal_char = compile(r'[^\w-]')
        self.init_try = 0
        # We want to know where we are load from? (broker, scheduler, etc)
        self.loaded_into = 'unknown'


    def init(self):
        """Handle this module "post" init ; just before it'll be started.
        Like just open necessaries file(s), database(s),
        or whatever the module will need.
        """
        pass


    def set_loaded_into(self, daemon_name):
        self.loaded_into = daemon_name


    def create_queues(self, manager=None):
        """The manager is None on android, but a true Manager() elsewhere
        Create the shared queues that will be used by shinken daemon
        process and this module process.
        But clear queues if they were already set before recreating new one.
        """
        self.clear_queues(manager)
        # If no Manager() object, go with classic Queue()
        if not manager:
            self.from_q = Queue()
            self.to_q = Queue()
        else:
            self.from_q = manager.Queue()
            self.to_q = manager.Queue()


    def clear_queues(self, manager):
        """Release the resources associated to the queues of this instance"""
        for q in (self.to_q, self.from_q):
            if q is None:
                continue
            # If we got no manager, we direct call the clean
            if not manager:
                q.close()
                q.join_thread()
            #else:
            #    q._callmethod('close')
            #    q._callmethod('join_thread')
        self.to_q = self.from_q = None


    # Start this module process if it's external. if not -> donothing
    def start(self, http_daemon=None):

        if not self.is_external:
            return
        self.stop_process()
        logger.info("Starting external process for instance %s" % (self.name))
        p = Process(target=self._main, args=())

        # Under windows we should not call start() on an object that got
        # its process as object, so we remove it and we set it in a earlier
        # start
        try:
            del self.properties['process']
        except:
            pass

        p.start()
        # We save the process data AFTER the fork()
        self.process = p
        self.properties['process'] = p  # TODO: temporary
        logger.info("%s is now started ; pid=%d" % (self.name, p.pid))


    def __kill(self):
        """Sometime terminate() is not enough, we must "help"
        external modules to die...
        """

        if os.name == 'nt':
            self.process.terminate()
        else:
            # Ok, let him 1 second before really KILL IT
            os.kill(self.process.pid, signal.SIGTERM)
            time.sleep(1)
            # You do not let me another choice guy...
            if self.process.is_alive():
                os.kill(self.process.pid, signal.SIGKILL)


    def stop_process(self):
        """Request the module process to stop and release it"""
        if self.process:
            logger.info("I'm stopping module '%s' process pid:%s " %
                       (self.get_name(), self.process.pid))
            self.process.terminate()
            self.process.join(timeout=1)
            if self.process.is_alive():
                logger.info("The process is still alive, I help it to die")
                self.__kill()
            self.process = None


    ## TODO: are these 2 methods really needed?
    def get_name(self):
        return self.name


    def has(self, prop):
        """The classic has: do we have a prop or not?"""
        return hasattr(self, prop)


    # For in scheduler modules, we will not send all broks to external
    # modules, only what they really want
    def want_brok(self, b):
        return True


    def manage_brok(self, brok):
        """Request the module to manage the given brok.
        There a lot of different possible broks to manage.
        """
        manage = getattr(self, 'manage_' + brok.type + '_brok', None)
        if manage:
            # Be sure the brok is prepared before call it
            brok.prepare()
            return manage(brok)


    def manage_signal(self, sig, frame):
        self.interrupted = True


    def set_signal_handler(self, sigs=None):
        if sigs is None:
            sigs = (signal.SIGINT, signal.SIGTERM)

        for sig in sigs:
            signal.signal(sig, self.manage_signal)

    set_exit_handler = set_signal_handler


    def do_stop(self):
        """Called just before the module will exit
        Put in this method all you need to cleanly
        release all open resources used by your module
        """
        pass

    def do_loop_turn(self):
        """For external modules only:
        implement in this method the body of you main loop
        """
        raise NotImplementedError()

    def set_proctitle(self, name):
        try:
            from setproctitle import setproctitle
            setproctitle("shinken-%s module: %s" % (self.loaded_into, name))
        except:
            pass

    def _main(self):
        """module "main" method. Only used by external modules."""
        self.set_proctitle(self.name)

        from http_daemon import daemon_inst
        if daemon_inst:
            daemon_inst.shutdown()
        
        self.set_signal_handler()
        logger.info("[%s[%d]]: Now running.." % (self.name, os.getpid()))
        # Will block here!
        self.main()
        self.do_stop()
        logger.info("[%s]: exiting now.." % (self.name))

    # TODO: apparently some modules would uses "work" as the main method??
    work = _main

########NEW FILE########
__FILENAME__ = bin
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This file is to be imported by every Shinken service component:
Arbiter, Scheduler, etc. It just checks for the main requirement of
Shinken.
"""

import sys

VERSION = "2.0.3"


# Make sure people are using Python 2.6 or higher
if sys.version_info < (2, 6):
    sys.exit("Shinken requires as a minimum Python 2.6.x, sorry")
elif sys.version_info >= (3,):
    sys.exit("Shinken is not yet compatible with Python 3.x, sorry")


########NEW FILE########
__FILENAME__ = borg
#!/usr/bin/env python
#
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class Borg(object):
    """ Here is the new-style Borg
    (not much more complex then the "old-style")
    """
    __shared_state = {}

    def __init__(self):
        #print "Init Borg", self.__dict__, self.__class__.__shared_state
        self.__dict__ = self.__class__.__shared_state

########NEW FILE########
__FILENAME__ = brok
#!/usr/bin/env python
#
# -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import cPickle


class Brok:
    """A Brok is a piece of information exported by Shinken to the Broker.
    Broker can do whatever he wants with it.
    """
    __slots__ = ('__dict__', 'id', 'type', 'data', 'prepared', 'instance_id')
    id = 0
    my_type = 'brok'

    def __init__(self, type, data):
        self.type = type
        self.id = self.__class__.id
        self.__class__.id += 1
        self.data = cPickle.dumps(data, cPickle.HIGHEST_PROTOCOL)
        self.prepared = False

    def __str__(self):
        return str(self.__dict__) + '\n'

    # We unserialize the data, and if some prop were
    # add after the serialize pass, we integer them in the data
    def prepare(self):
        # Maybe the brok is a old daemon one or was already prepared
        # if so, the data is already ok
        if hasattr(self, 'prepared') and not self.prepared:
            self.data = cPickle.loads(self.data)
            if hasattr(self, 'instance_id'):
                self.data['instance_id'] = self.instance_id
        self.prepared = True

########NEW FILE########
__FILENAME__ = brokerlink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp


class BrokerLink(SatelliteLink):
    """TODO: Add some comment about this class for the doc"""
    id = 0
    my_type = 'broker'
    properties = SatelliteLink.properties.copy()
    properties.update({
        'broker_name': StringProp(fill_brok=['full_status'], to_send=True),
        'port': IntegerProp(default='7772', fill_brok=['full_status']),
    })

    def get_name(self):
        return self.broker_name

    def register_to_my_realm(self):
        self.realm.brokers.append(self)


class BrokerLinks(SatelliteLinks):
    """TODO: Add some comment about this class for the doc"""
    name_property = "broker_name"
    inner_class = BrokerLink

########NEW FILE########
__FILENAME__ = check
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.action import Action
from shinken.property import UnusedProp, BoolProp, IntegerProp, FloatProp
from shinken.property import CharProp, StringProp, ListProp
from shinken.autoslots import AutoSlots


class Check(Action):
    """ ODO: Add some comment about this class for the doc"""
    # AutoSlots create the __slots__ with properties and
    # running_properties names

    ###FIXME : reenable AutoSlots if possible
    #__metaclass__ = AutoSlots

    my_type = 'check'

    properties = {
        'is_a':         StringProp(default='check'),
        'type':         StringProp(default=''),
        '_in_timeout':  BoolProp(default=False),
        'status':       StringProp(default=''),
        'exit_status':  IntegerProp(default=3),
        'state':        IntegerProp(default=0),
        'output':       StringProp(default=''),
        'long_output':  StringProp(default=''),
        'ref':          IntegerProp(default=-1),
        't_to_go':      IntegerProp(default=0),
        'depend_on':    StringProp(default=[]),
        'dep_check':    StringProp(default=[]),
        'check_time':   IntegerProp(default=0),
        'execution_time': FloatProp(default=0.0),
        'u_time':       FloatProp(default=0.0),
        's_time':       FloatProp(default=0.0),
        'perf_data':    StringProp(default=''),
        'check_type':   IntegerProp(default=0),
        'poller_tag':   StringProp(default='None'),
        'reactionner_tag':   StringProp(default='None'),
        'env':          StringProp(default={}),
        'internal':     BoolProp(default=False),
        'module_type':  StringProp(default='fork'),
        'worker':       StringProp(default='none'),
        'from_trigger': BoolProp(default=False),
    }

    def __init__(self, status, command, ref, t_to_go, dep_check=None, id=None,
                 timeout=10, poller_tag='None', reactionner_tag='None',
                 env={}, module_type='fork', from_trigger=False, dependency_check=False):

        self.is_a = 'check'
        self.type = ''
        if id is None:  # id != None is for copy call only
            self.id = Action.id
            Action.id += 1
        self._in_timeout = False
        self.timeout = timeout
        self.status = status
        self.exit_status = 3
        self.command = command
        self.output = ''
        self.long_output = ''
        self.ref = ref
        #self.ref_type = ref_type
        self.t_to_go = t_to_go
        self.depend_on = []
        if dep_check is None:
            self.depend_on_me = []
        else:
            self.depend_on_me = [dep_check]
        self.check_time = 0
        self.execution_time = 0
        self.u_time = 0  # user executon time
        self.s_time = 0  # system execution time
        self.perf_data = ''
        self.check_type = 0  # which kind of check result? 0=active 1=passive
        self.poller_tag = poller_tag
        self.reactionner_tag = reactionner_tag
        self.module_type = module_type
        self.env = env
        # we keep the reference of the poller that will take us
        self.worker = 'none'
        # If it's a business rule, manage it as a special check
        if ref and ref.got_business_rule or command.startswith('_internal'):
            self.internal = True
        else:
            self.internal = False
        self.from_trigger = from_trigger
        self.dependency_check = dependency_check


    def copy_shell(self):
        """return a copy of the check but just what is important for execution
        So we remove the ref and all
        """

        # We create a dummy check with nothing in it, just defaults values
        return self.copy_shell__(Check('', '', '', '', '', id=self.id))

    def get_return_from(self, c):
        self.exit_status = c.exit_status
        self.output = c.output
        self.long_output = c.long_output
        self.check_time = c.check_time
        self.execution_time = c.execution_time
        self.perf_data = c.perf_data
        self.u_time = c.u_time
        self.s_time = c.s_time


    def is_launchable(self, t):
        return t > self.t_to_go

    def __str__(self):
        return "Check %d status:%s command:%s ref:%s" % \
               (self.id, self.status, self.command, self.ref)

    def get_id(self):
        return self.id

    def set_type_active(self):
        self.check_type = 0

    def set_type_passive(self):
        self.check_type = 1

    def is_dependent(self):
        return self.dependency_check

########NEW FILE########
__FILENAME__ = livestatus
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import socket
import asyncore
import time
from log import logger


class LSSyncConnection:
    def __init__(self, addr='127.0.0.1', port=50000, path=None, timeout=10):
        self.addr = addr
        self.port = port
        self.path = path
        self.timeout = timeout

        # We must know if the socket is alive or not
        self.alive = False

        # Now we can inti the sockets
        if path:
            self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
            self.type = 'unix'
        else:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.type = 'tcp'

        # We can now set the socket timeout
        self.socket.settimeout(timeout)
        self.connect()

    def connect(self):
        if not self.alive:
            if self.type == 'unix':
                target = self.path
            else:
                target = (self.addr, self.port)

            try:
                self.socket.connect(target)
                self.alive = True
            except IOError, exp:
                self.alive = False
                logger.warning("Connection problem: %s" % str(exp))

    def read(self, size):
        res = ""
        while size > 0:
            data = self.socket.recv(size)
            l = len(data)

            if l == 0:
                logger.warning("0 size read")
                return res  #: TODO raise an error

            size = size - l
            res = res + data
        return res

    def launch_query(self, query):
        if not self.alive:
            self.connect()
        if not query.endswith("\n"):
            query += "\n"
        query += "OutputFormat: python\nKeepAlive: on\nResponseHeader: fixed16\n\n"

        try:
            self.socket.send(query)
            data = self.read(16)
            code = data[0:3]
            logger.debug("RAW DATA: %s" % data)

            length = int(data[4:15])
            logger.debug("Len: %d" % length)

            data = self.read(length)
            logger.debug("DATA: %s" % data)

            if code == "200":
                try:
                    return eval(data)
                except:
                    logger.warning("BAD VALUE RETURN (data=%s)" % data)
                    return None
            else:
                logger.warning("BAD RETURN CODE (code= %s, data=%s" % (code, data))
                return None
        except IOError, exp:
            self.alive = False
            logger.warning("SOCKET ERROR (%s)" % str(exp))
            return None

    def exec_command(self, command):
        if not self.alive:
            self.connect()
        if not command.endswith("\n"):
            command += "\n"

        try:
            self.socket.send("COMMAND " + command + "\n")
        except IOError, exp:
            self.alive = False
            logger.warning("COMMAND EXEC error: %s" % str(exp))


# Query class for define a query, and its states
class Query(object):

    id = 0

    def __init__(self, q):
        # The query string
        if not q.endswith("\n"):
            q += "\n"
        q += "OutputFormat: python\nKeepAlive: on\nResponseHeader: fixed16\n\n"

        self.q = q
        self.id = Query.id
        Query.id += 1
        # Got some states PENDING -> PICKUP -> DONE
        self.state = 'PENDING'
        self.result = None
        self.duration = 0
        # By default, an error :)
        self.return_code = '500'

    def get(self):
        #print "Someone ask my query", self.q
        self.state = 'PICKUP'
        self.duration = time.time()
        return self.q

    def put(self, r):
        self.result = r
        self.state = 'DONE'
        self.duration = time.time() - self.duration
        #print "Got a result", r


class LSAsynConnection(asyncore.dispatcher):
    def __init__(self, addr='127.0.0.1', port=50000, path=None, timeout=10):
        asyncore.dispatcher.__init__(self)
        self.addr = addr
        self.port = port
        self.path = path
        self.timeout = timeout

        # We must know if the socket is alive or not
        self.alive = False

        # Now we can inti the sockets
        if path:
            self.create_socket(socket.AF_UNIX, socket.SOCK_STREAM)
            self.type = 'unix'
        else:
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.type = 'tcp'

        # We can now set the socket timeout
        self.socket.settimeout(timeout)
        self.do_connect()

        # And our queries
        #q = Query('GET hosts\nColumns name\n')
        self.queries = []
        self.results = []

        self.current = None

    def stack_query(self, q):
        self.queries.append(q)

    # Get a query and put it in current
    def get_query(self):
        q = self.queries.pop()
        self.current = q
        return q

    def do_connect(self):
        if not self.alive:
            if self.type == 'unix':
                target = self.path
            else:
                target = (self.addr, self.port)
            try:
                self.connect(target)
                self.alive = True
            except IOError, exp:
                self.alive = False
                logger.warning("Connection problem: %s" % str(exp))
                self.handle_close()

    def do_read(self, size):
        res = ""
        while size > 0:
            data = self.socket.recv(size)
            l = len(data)
            if l == 0:
                logger.warning("0 size read")
                return res  #: TODO raise an error

            size = size - l
            res = res + data
        return res

    def exec_command(self, command):
        if not self.alive:
            self.do_connect()
        if not command.endswith("\n"):
            command += "\n"

        try:
            self.socket.send("COMMAND " + command + "\n")
        except IOError, exp:
            self.alive = False
            logger.warning("COMMAND EXEC error: %s" % str(exp))

    def handle_connect(self):
        pass
        #print "In handle_connect"

    def handle_close(self):
        logger.debug("Closing connection")
        self.current = None
        self.queries = []
        self.close()

    # Check if we are in timeout. If so, just bailout
    # and set the correct return code from timeout
    # case
    def look_for_timeout(self):
        logger.debug("Look for timeout")
        now = time.time()
        if now - self.start_time > self.timeout:
            if self.unknown_on_timeout:
                rc = 3
            else:
                rc = 2
            message = 'Error: connection timeout after %d seconds' % self.timeout
            self.set_exit(rc, message)

    # We got a read for the socket. We do it if we do not already
    # finished. Maybe it's just a SSL handshake continuation, if so
    # we continue it and wait for handshake finish
    def handle_read(self):
        #print "Handle read"

        q = self.current
        # get a read but no current query? Not normal!

        if not q:
            #print "WARNING: got LS read while no current query in progress. I return"
            return

        try:
            data = self.do_read(16)
            code = data[0:3]
            q.return_code = code

            length = int(data[4:15])
            data = self.do_read(length)

            if code == "200":
                try:
                    d = eval(data)
                    #print d
                    q.put(d)
                except:
                    q.put(None)
            else:
                q.put(None)
                return None
        except IOError, exp:
            self.alive = False
            logger.warning("SOCKET ERROR: %s" % str(exp))
            return q.put(None)

        # Now the current is done. We put in in our results queue
        self.results.append(q)
        self.current = None

    # Did we finished our job?
    def writable(self):
        b = (len(self.queries) != 0 and not self.current)
        #print "Is writable?", b
        return b

    def readable(self):
        b = self.current is not None
        #print "Readable", b
        return True

    # We can write to the socket. If we are in the ssl handshake phase
    # we just continue it and return. If we finished it, we can write our
    # query
    def handle_write(self):
        if not self.writable():
            logger.debug("Not writable, I bail out")
            return

        #print "handle write"
        try:
            q = self.get_query()
            sent = self.send(q.get())
        except socket.error, exp:
            logger.debug("Write fail: %s" % str(exp))
            return

        #print "Sent", sent, "data"


    # We are finished only if we got no pending queries and
    # no in progress query too
    def is_finished(self):
        #print "State:", self.current, len(self.queries)
        return self.current == None and len(self.queries) == 0

    # Will loop over the time until all returns are back
    def wait_returns(self):
        while self.alive and not self.is_finished():
            asyncore.poll(timeout=0.001)

    def get_returns(self):
        r = self.results
        self.results = self.results[:]
        return r

    def launch_raw_query(self, query):
        if not self.alive:
            logger.debug("Cannot launch query. Connection is closed")
            return None

        if not self.is_finished():
            logger.debug("Try to launch a new query in a normal mode but the connection already got async queries in progress")
            return None

        q = Query(query)
        self.stack_query(q)
        self.wait_returns()
        q = self.results.pop()
        return q.result


class LSConnectionPool(object):
    def __init__(self, con_addrs):
        self.connections = []
        for s in con_addrs:
            if s.startswith('tcp:'):
                s = s[4:]
                addr = s.split(':')[0]
                port = int(s.split(':')[1])
                con = LSAsynConnection(addr=addr, port=port)
            elif s.startswith('unix:'):
                s = s[5:]
                path = s
                con = LSAsynConnection(path=path)
            else:
                logger.info("Unknown connection type for %s" % s)

            self.connections.append(con)

    def launch_raw_query(self, query):
        for c in self.connections:
            q = Query(query)
            c.stack_query(q)
        still_working = [c for c in self.connections if c.alive and not c.is_finished()]
        while len(still_working) > 0:
            asyncore.poll(timeout=0.001)
            still_working = [c for c in self.connections if c.alive and not c.is_finished()]
        # Now get all results
        res = []
        for c in self.connections:
            if len(c.get_returns()) > 0:
                q = c.get_returns().pop()
                r = q.result
                logger.debug(str(r))
                res.extend(r)
        return res


if __name__ == "__main__":
    c = LSAsynConnection()
    import time
    t = time.time()

    q = Query('GET hosts\nColumns name\n')
    #c.stack_query(q)
    #q2 = Query('GET hosts\nColumns name\n')
    #c.stack_query(q)

    #print "Start to wait"
    #c.wait_returns()
    #print "End to wait"
    #print "Results", c.get_returns()
    #while time.time() - t < 1:
    #    asyncore.poll()


    #while time.time() - t < 1:
    #    asyncore.poll()
    #print c.launch_query('GET hosts\nColumns name')
    #print c.__dict__

    #print "Launch raw query"
    #r = c.launch_raw_query('GET hosts\nColumns name\n')
    #print "Result", r

    cp = LSConnectionPool(['tcp:localhost:50000', 'tcp:localhost:50000'])
    r = cp.launch_raw_query('GET hosts\nColumns name last_check\n')
    logger.debug("Result= %s" % str(r))
    import time
    logger.debug(int(time.time()))

########NEW FILE########
__FILENAME__ = LSB
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#    Nicolas Dupeux, nicolas.dupeux@arkea.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import time
import asyncore
import getopt
sys.path.append("..")
sys.path.append("../..")
from livestatus import LSAsynConnection, Query

""" Benchmark of the livestatus broker"""


class QueryGenerator(object):
    """Generate a livestatus query"""

    def get(self):
        pass


class SimpleQueryGenerator(QueryGenerator):
    def __init__(self, querys, name="sqg"):
        self.querys = querys
        self.name = name
        self.i = 0

    def get(self):
        query = self.querys[self.i]
        query_class = "%s-%s" % (self.name, self.i)
        self.i += 1
        if self.i >= len(self.querys):
            self.i = 0
        return (query_class, query)


class FileQueryGenerator(SimpleQueryGenerator):
    def __init__(self, filename):
        f = open(filename, "r")
        querys = []
        for query in f:
            query = query.replace("\\n", "\n")
            querys.append(query)
        SimpleQueryGenerator.__init__(self, querys, filename)


def usage():
    print " -n requests     Number of requests to perform [Default: 10]"
    print " -c concurrency  Number of multiple requests to make [Default: 1]"


def mean(numberList):
    if len(numberList) == 0:
        return float('nan')

    floatNums = [float(x) for x in numberList]
    return sum(floatNums) / len(numberList)


def median(numberList):
    sorted_values = sorted(numberList)

    if len(sorted_values) % 2 == 1:
        return sorted_values[(len(sorted_values) + 1) / 2 - 1]
    else:
        lower = sorted_values[len(sorted_values) / 2 - 1]
        upper = sorted_values[len(sorted_values) / 2]

    return (float(lower + upper)) / 2


def run(url, requests, concurrency, qg):
    if (concurrency > requests):
        concurrency = requests

    remaining = requests

    conns = []
    queries_durations = {}
    if url.startswith('tcp:'):
        url = url[4:]
        addr = url.split(':')[0]
        port = int(url.split(':')[1])
    else:
        return

    for x in xrange(0, concurrency):
        conns.append(LSAsynConnection(addr=addr, port=port))
        (query_class, query_str) = qg.get()
        q = Query(query_str)
        q.query_class = query_class
        conns[x].stack_query(q)

    print "Start queries"
    t = time.time()
    while remaining > 0:
        asyncore.poll(timeout=1)
        for c in conns:
            if c.is_finished():
                # Store query duration to compute stats
                q = c.results.pop()
                duration = q.duration
                if (not queries_durations.has_key(q.query_class)):
                    queries_durations[q.query_class] = []
                queries_durations[q.query_class].append(q.duration)
                sys.stdout.flush()
                remaining -= 1

                # Print a dot every 10 completed queries
                if (remaining % 10 == 0):
                    print '.',
                    sys.stdout.flush()

                # Run another query
                (query_class, query_str) = qg.get()
                q = Query(query_str)
                q.query_class = query_class
                c.stack_query(q)
    running_time = time.time() - t
    print "End queries"

    print "\n==============="
    print "Execution report"
    print "==============="
    print "Running time is %04f s" % running_time
    print "Query Class          nb  min      max       mean     median"
    for query_class, durations in queries_durations.items():
        print "%s %03d %03f %03f %03f %03f" % (query_class.ljust(20), len(durations), min(durations), max(durations), mean(durations), median(durations))


def main(argv):
    # Defaults values
    concurrency = 5
    requests = 20
    url = "tcp:localhost:50000"

    try:
        opts, args = getopt.getopt(argv, "hc:n:", "help")
    except getopt.GetoptError:
        usage()
        sys.exit(2)
    for opt, arg in opts:
        if opt in ("-h", "--help"):
            usage()
            sys.exit()
        elif opt == "-c":
            concurrency = int(arg)
        elif opt == "-n":
            requests = int(arg)

    if len(args) >= 1:
        url = args[0]

    print "Running %s queries on %s" % (requests, url)
    print "Concurrency level %s " % (concurrency)

    qg = FileQueryGenerator("thruk_tac.queries")

    run(url, requests, concurrency, qg)

if __name__ == "__main__":
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = commandcall
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.autoslots import AutoSlots
from shinken.property import StringProp, BoolProp, IntegerProp


class DummyCommandCall(object):
    """Ok, slots are fun: you cannot set the __autoslots__
     on the same class you use, fun isn't it? So we define*
     a dummy useless class to get such :)
    """
    pass


class CommandCall(DummyCommandCall):
    """This class is use when a service, contact or host define
    a command with args.
    """
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    #__slots__ = ('id', 'call', 'command', 'valid', 'args', 'poller_tag',
    #             'reactionner_tag', 'module_type', '__dict__')
    id = 0
    my_type = 'CommandCall'

    properties = {
        'call':            StringProp(),
        'command':         StringProp(),
        'poller_tag':      StringProp(default='None'),
        'reactionner_tag': StringProp(default='None'),
        'module_type':     StringProp(default='fork'),
        'valid':           BoolProp(default=False),
        'args':            StringProp(default=[]),
        'timeout':         IntegerProp(default='-1'),
        'late_relink_done':BoolProp(default=False),
        'enable_environment_macros': BoolProp(default=0),
    }

    def __init__(self, commands, call, poller_tag='None',
                 reactionner_tag='None', enable_environment_macros=0):
        self.id = self.__class__.id
        self.__class__.id += 1
        self.call = call
        self.timeout = -1
        # Now split by ! and get command and args
        self.get_command_and_args()
        self.command = commands.find_by_name(self.command.strip())
        self.late_relink_done = False  # To do not relink again and again the same commandcall
        if self.command is not None:
            self.valid = True
        else:
            self.valid = False
        if self.valid:
            # If the host/service do not give an override poller_tag, take
            # the one of the command
            self.poller_tag = poller_tag  # from host/service
            self.reactionner_tag = reactionner_tag
            self.module_type = self.command.module_type
            self.enable_environment_macros = self.command.enable_environment_macros
            self.timeout = int(self.command.timeout)
            if self.valid and poller_tag is 'None':
                # from command if not set
                self.poller_tag = self.command.poller_tag
            # Same for reactionner tag
            if self.valid and reactionner_tag is 'None':
                # from command if not set
                self.reactionner_tag = self.command.reactionner_tag

    def get_command_and_args(self):
        """We want to get the command and the args with ! splitting.
        but don't forget to protect against the \! to do not split them
        """

        # First protect
        p_call = self.call.replace('\!', '___PROTECT_EXCLAMATION___')
        tab = p_call.split('!')
        self.command = tab[0]
        # Reverse the protection
        self.args = [s.replace('___PROTECT_EXCLAMATION___', '!')
                     for s in tab[1:]]

    # If we didn't already lately relink us, do it
    def late_linkify_with_command(self, commands):
        if self.late_relink_done:
            return
        self.late_relink_done = True
        c = commands.find_by_name(self.command)
        self.command = c

    def is_valid(self):
        return self.valid

    def __str__(self):
        return str(self.__dict__)

    def get_name(self):
        return self.call

    def __getstate__(self):
        """Call by pickle to dataify the comment
        because we DO NOT WANT REF in this pickleisation!
        """
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}

        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)

        # The command is a bit special, we just put it's name
        # or a '' if need
        if self.command and not isinstance(self.command, basestring):
            res['command'] = self.command.get_name()
        # Maybe it's a repickle of a unpickle thing... (like with deepcopy). If so
        # only take the value
        elif self.command and isinstance(self.command, basestring):
            res['command'] = self.command
        else:
            res['command'] = ''

        return res

    def __setstate__(self, state):
        """Inverted function of getstate"""
        cls = self.__class__
        # We move during 1.0 to a dict state
        # but retention file from 0.8 was tuple
        if isinstance(state, tuple):
            self.__setstate_pre_1_0__(state)
            return

        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])

    def __setstate_pre_1_0__(self, state):
        """In 1.0 we move to a dict save. Before, it was
        a tuple save, like
        ({'id': 11}, {'poller_tag': 'None', 'reactionner_tag': 'None',
        'command_line': u'/usr/local/nagios/bin/rss-multiuser',
        'module_type': 'fork', 'command_name': u'notify-by-rss'})
        """
        for d in state:
            for k, v in d.items():
                setattr(self, k, v)

########NEW FILE########
__FILENAME__ = comment
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

""" TODO: Add some comment about this class for the doc"""
class Comment:
    id = 1

    properties = {
        'entry_time':   None,
        'persistent':   None,
        'author':       None,
        'comment':      None,
        'comment_type': None,
        'entry_type':   None,
        'source':       None,
        'expires':      None,
        'expire_time':  None,
        'can_be_deleted': None,

        # TODO: find a very good way to handle the downtime "ref".
        # ref must effectively not be in properties because it points
        # onto a real object.
        #'ref':  None
    }

    # Adds a comment to a particular service. If the "persistent" field
    # is set to zero (0), the comment will be deleted the next time
    # Shinken is restarted. Otherwise, the comment will persist
    # across program restarts until it is deleted manually.
    def __init__(self, ref, persistent, author, comment, comment_type, entry_type, source, expires, expire_time):
        self.id = self.__class__.id
        self.__class__.id += 1
        self.ref = ref  # pointer to srv or host we are apply
        self.entry_time = int(time.time())
        self.persistent = persistent
        self.author = author
        self.comment = comment
        # Now the hidden attributes
        # HOST_COMMENT=1,SERVICE_COMMENT=2
        self.comment_type = comment_type
        # USER_COMMENT=1,DOWNTIME_COMMENT=2,FLAPPING_COMMENT=3,ACKNOWLEDGEMENT_COMMENT=4
        self.entry_type = entry_type
        # COMMENTSOURCE_INTERNAL=0,COMMENTSOURCE_EXTERNAL=1
        self.source = source
        self.expires = expires
        self.expire_time = expire_time
        self.can_be_deleted = False

    def __str__(self):
        return "Comment id=%d %s" % (self.id, self.comment)

    # Call by pickle for dataify the ackn
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)
        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__

        # Maybe it's not a dict but a list like in the old 0.4 format
        # so we should call the 0.4 function for it
        if isinstance(state, list):
            self.__setstate_deprecated__(state)
            return

        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])

        # to prevent from duplicating id in comments:
        if self.id >= cls.id:
            cls.id = self.id + 1

    # This function is DEPRECATED and will be removed in a future version of
    # Shinken. It should not be useful any more after a first load/save pass.
    # Inverted function of getstate
    def __setstate_deprecated__(self, state):
        cls = self.__class__
        # Check if the len of this state is like the previous,
        # if not, we will do errors!
        # -1 because of the 'id' prop
        if len(cls.properties) != (len(state) - 1):
            self.debug("Passing comment")
            return

        self.id = state.pop()
        for prop in cls.properties:
            val = state.pop()
            setattr(self, prop, val)
        if self.id >= cls.id:
            cls.id = self.id + 1

########NEW FILE########
__FILENAME__ = complexexpression
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re
from shinken.util import strip_and_uniq


"""
Here is a node class for complex_expression(s) and a factory to create them
"""

class ComplexExpressionNode(object):
    def __init__(self):
        self.operand = None
        self.sons = []
        self.configuration_errors = []
        self.not_value = False
        # If leaf, the content will be the hostgroup or hosts
        # that are selected with this node
        self.leaf = False
        self.content = None
        
        
        
    def __str__(self):
        if not self.leaf:
            return "Op:'%s' Leaf:%s Sons:'[%s] IsNot:%s'" % (self.operand, self.leaf, ','.join([str(s) for s in self.sons]), self.not_value)
        else:
            return 'IS LEAF %s' % self.content


    def resolve_elements(self):
        # If it's a leaf, we just need to dump a set with the content of the node
        if self.leaf:
            #print "Is a leaf", self.content
            if not self.content:
                return set()

            return set(self.content)
        
        
        # first got the not ones in a list, and the other in the other list
        not_nodes = [s for s in self.sons if s.not_value]
        positiv_nodes = [s for s in self.sons if not s.not_value] # ok a not not is hard to read...

        #print "Not nodes", not_nodes
        #print "Positiv nodes", positiv_nodes

        # By default we are using a OR rule
        if not self.operand:
            self.operand = '|'

        res = set()

        #print "Will now merge all of this", self.operand

        # The operand will change the positiv loop only
        i = 0
        for n in positiv_nodes:
            node_members = n.resolve_elements()
            if self.operand == '|':
                #print "OR rule", node_members
                res = res.union(node_members)
            elif self.operand == '&':
                #print "AND RULE", node_members
                # The first elements of an AND rule should be used
                if i == 0:
                    res = node_members
                else:
                    res = res.intersection(node_members)
            i += 1

        # And we finally remove all NOT elements from the result
        for n in not_nodes:
            node_members = n.resolve_elements()
            res = res.difference(node_members)
        
        return res
    

    # Check for empty (= not found) leaf nodes
    def is_valid(self):

        valid = True
        if not self.sons:
            valid = False
        else:
            for s in self.sons:
                if isinstance(s, DependencyNode) and not s.is_valid():
                    self.configuration_errors.extend(s.configuration_errors)
                    valid = False
        return valid



""" TODO: Add some comment about this class for the doc"""
class ComplexExpressionFactory(object):
    def __init__(self, ctx='hostgroups', grps=None, all_elements=None):
        self.ctx = ctx
        self.grps = grps
        self.all_elements = all_elements
        

    # the () will be eval in a recursiv way, only one level of ()
    def eval_cor_pattern(self, pattern):
        pattern = pattern.strip()
        #print "eval_cor_pattern::", pattern
        complex_node = False

        # Look if it's a complex pattern (with rule) or
        # if it's a leaf ofit, like a host/service
        for m in '()+&|,':
            if m in pattern:
                complex_node = True

        node = ComplexExpressionNode()
        #print "Is so complex?", complex_node, pattern, node
        
        # if it's a single expression like !linux or production
        # we will get the objects from it and return a leaf node
        if not complex_node:
            # If it's a not value, tag the node and find
            # the name without this ! operator
            if pattern.startswith('!'):
                node.not_value = True
                pattern = pattern[1:]
            
            node.operand = self.ctx
            node.leaf = True
            obj, error = self.find_object(pattern)
            if obj is not None:
                node.content = obj
            else:
                node.configuration_errors.append(error)
            return node

        in_par = False
        tmp = ''
        stacked_par = 0
        for c in pattern:
            #print "MATCHING", c
            if c == ',' or c == '|':
                # Maybe we are in a par, if so, just stack it
                if in_par:
                    #print ", in a par, just staking it"
                    tmp += c
                else:
                    # Oh we got a real cut in an expression, if so, cut it
                    #print "REAL , for cutting"
                    tmp = tmp.strip()
                    node.operand = '|'
                    if tmp != '':
                        #print "Will analyse the current str", tmp
                        o = self.eval_cor_pattern(tmp)
                        node.sons.append(o)
                    tmp = ''

            elif c == '&' or c == '+':
                # Maybe we are in a par, if so, just stack it
                if in_par:
                    #print " & in a par, just staking it"
                    tmp += c
                else:
                    # Oh we got a real cut in an expression, if so, cut it
                    #print "REAL & for cutting"
                    tmp = tmp.strip()
                    node.operand = '&'
                    if tmp != '':
                        #print "Will analyse the current str", tmp
                        o = self.eval_cor_pattern(tmp)
                        node.sons.append(o)
                    tmp = ''
            
            elif c == '(':
                stacked_par += 1
                #print "INCREASING STACK TO", stacked_par
                
                in_par = True
                tmp = tmp.strip()
                # Maybe we just start a par, but we got some things in tmp
                # that should not be good in fact !
                if stacked_par == 1 and tmp != '':
                    #TODO : real error
                    print "ERROR : bad expression near", tmp
                    continue

                # If we are already in a par, add this (
                # but not if it's the first one so
                if stacked_par > 1:
                    tmp += c
                    #o = self.eval_cor_pattern(tmp)
                    #print "1( I've %s got new sons" % pattern , o
                    #node.sons.append(o)
                    
            elif c == ')':
                #print "Need closeing a sub expression?", tmp
                stacked_par -= 1

                if stacked_par < 0:
                    # TODO : real error
                    print "Error : bad expression near", tmp, "too much ')'"
                    continue
                
                if stacked_par == 0:
                    #print "THIS is closing a sub compress expression", tmp
                    tmp = tmp.strip()
                    o = self.eval_cor_pattern(tmp)
                    node.sons.append(o)
                    in_par = False
                    # OK now clean the tmp so we start clean
                    tmp = ''
                    continue

                # ok here we are still in a huge par, we just close one sub one
                tmp += c
            # Maybe it's a classic character, if so, continue
            else:
                tmp += c

        # Be sure to manage the trainling part when the line is done
        tmp = tmp.strip()
        if tmp != '':
            #print "Managing trainling part", tmp
            o = self.eval_cor_pattern(tmp)
            #print "4end I've %s got new sons" % pattern , o
            node.sons.append(o)

        #print "End, tmp", tmp
        #print "R %s:" % pattern, node
        return node


    # We've got an object, like super-grp, so we should link th group here
    def find_object(self, pattern):
        obj = None
        error = None
        pattern = pattern.strip()

        
        if pattern == '*':
            obj = [h.host_name for h in self.all_elements.items.values()
                   if getattr(h, 'host_name', '') != '' and not h.is_tpl()]
            return obj, error

        
        # Ok a more classic way

        #print "GRPS", self.grps
        
        if self.ctx == 'hostgroups':
            # Ok try to find this hostgroup
            hg = self.grps.find_by_name(pattern)
            # Maybe it's an known one?
            if not hg:
                error = "Error : cannot find the %s of the expression '%s'" % (self.ctx, pattern)
                return hg, error
            # Ok the group is found, get the elements!
            elts = hg.get_hosts().split(',')
            elts = strip_and_uniq(elts)

            # Maybe the hostgroup memebrs is '*', if so expand with all hosts
            if '*' in elts:
                elts.extend([h.host_name for h in self.all_elements.items.values()
                             if getattr(h, 'host_name', '') != '' and not h.is_tpl()])
                # And remove this strange hostname too :)
                elts.remove('*')
            return elts, error
                
        else: #templates
            obj = self.grps.find_hosts_that_use_template(pattern)
        
        return obj, error

    

########NEW FILE########
__FILENAME__ = contactdowntime
#!/usr/bin/env python

# -*- coding: utf-8 -*-


# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
from shinken.log import logger

""" TODO: Add some comment about this class for the doc"""
class ContactDowntime:
    id = 1

    # Just to list the properties we will send as pickle
    # so to others daemons, so all but NOT REF
    properties = {
        #'activate_me':  None,
        #'entry_time':   None,
        #'fixed':        None,
        'start_time':   None,
        #'duration':     None,
        #'trigger_id':   None,
        'end_time':     None,
        #'real_end_time': None,
        'author':       None,
        'comment':      None,
        'is_in_effect': None,
        #'has_been_triggered': None,
        'can_be_deleted': None,
        }

    # Schedule a contact downtime. It's far more easy than a host/service
    # one because we got a beginning, and an end. That's all for running.
    # got also an author and a comment for logging purpose.
    def __init__(self, ref, start_time, end_time, author, comment):
        self.id = self.__class__.id
        self.__class__.id += 1
        self.ref = ref  # pointer to srv or host we are apply
        self.start_time = start_time
        self.end_time = end_time
        self.author = author
        self.comment = comment
        self.is_in_effect = False
        self.can_be_deleted = False
        #self.add_automatic_comment()


    # Check if we came into the activation of this downtime
    def check_activation(self):
        now = time.time()
        was_is_in_effect = self.is_in_effect
        self.is_in_effect = (self.start_time <= now <= self.end_time)
        logger.info("CHECK ACTIVATION:%s" % (self.is_in_effect))

        # Raise a log entry when we get in the downtime
        if not was_is_in_effect and self.is_in_effect:
            self.enter()

        # Same for exit purpose
        if was_is_in_effect and not self.is_in_effect:
            self.exit()

    def in_scheduled_downtime(self):
        return self.is_in_effect

    # The referenced host/service object enters now a (or another) scheduled
    # downtime. Write a log message only if it was not already in a downtime
    def enter(self):
        self.ref.raise_enter_downtime_log_entry()

    # The end of the downtime was reached.
    def exit(self):
        self.ref.raise_exit_downtime_log_entry()
        self.can_be_deleted = True

    # A scheduled downtime was prematurely canceled
    def cancel(self):
        self.is_in_effect = False
        self.ref.raise_cancel_downtime_log_entry()
        self.can_be_deleted = True

    # Call by pickle to dataify the comment
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        #print "Asking a getstate for a downtime on", self.ref.get_dbg_name()
        cls = self.__class__
        # id is not in *_properties
        res = [self.id]
        for prop in cls.properties:
            res.append(getattr(self, prop))
        # We reverse because we want to recreate
        # By check at properties in the same order
        res.reverse()
        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__
        self.id = state.pop()
        for prop in cls.properties:
            val = state.pop()
            setattr(self, prop, val)
        if self.id >= cls.id:
            cls.id = self.id + 1

########NEW FILE########
__FILENAME__ = daemon
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import errno
import sys
import time
import signal
import select
import random
import ConfigParser
import json
import threading
import inspect
import traceback
import cStringIO
import logging
import inspect

# Try to see if we are in an android device or not
is_android = True
try:
    import android
except ImportError:
    is_android = False

if not is_android:
    from multiprocessing import Queue, Manager, active_children, cpu_count
    from multiprocessing.managers import SyncManager
else:
    from multiprocessing import active_children

import http_daemon
from shinken.http_daemon import HTTPDaemon, InvalidWorkDir
from shinken.log import logger
from shinken.modulesctx import modulesctx
from shinken.modulesmanager import ModulesManager
from shinken.property import StringProp, BoolProp, PathProp, ConfigPathProp, IntegerProp, LogLevelProp


try:
    import pwd, grp
    from pwd import getpwnam
    from grp import getgrnam


    def get_cur_user():
        return pwd.getpwuid(os.getuid()).pw_name


    def get_cur_group():
        return grp.getgrgid(os.getgid()).gr_name
except ImportError, exp:  # Like in nt system or Android
    # temporary workaround:
    def get_cur_user():
        return "shinken"


    def get_cur_group():
        return "shinken"


##########################   DAEMON PART    ###############################
# The standard I/O file descriptors are redirected to /dev/null by default.
REDIRECT_TO = getattr(os, "devnull", "/dev/null")

UMASK = 027
from shinken.bin import VERSION

""" TODO: Add some comment about this class for the doc"""
class InvalidPidFile(Exception): pass


""" Interface for Inter satellites communications """
class Interface(object):

    #  'app' is to be set to the owner of this interface.
    def __init__(self, app):
        self.app = app
        self.running_id = "%d.%d" % (time.time(), random.random())

    
    doc = 'Test the connexion to the daemon. Returns: pong'
    def ping(self):
        return "pong"
    ping.need_lock = False
    ping.doc = doc


    doc = 'Get the current running id of the daemon (scheduler)'
    def get_running_id(self):
        return self.running_id
    get_running_id.need_lock = False
    get_running_id.doc = doc


    doc = 'Send a new configuration to the daemon (internal)'
    def put_conf(self, conf):
        self.app.new_conf = conf
    put_conf.method = 'post'
    put_conf.doc = doc


    doc = 'Ask the daemon to wait a new conf'
    def wait_new_conf(self):
        self.app.cur_conf = None
    wait_new_conf.need_lock = False
    wait_new_conf.doc = doc


    doc = 'Does the daemon got an active configuration'
    def have_conf(self):
        return self.app.cur_conf is not None
    have_conf.need_lock = False
    have_conf.doc = doc

    
    doc = 'Set the current log level in [NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL, UNKNOWN]'
    def set_log_level(self, loglevel):
        return logger.set_level(loglevel)
    set_log_level.doc = doc


    doc = 'Get the current log level in [NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL, UNKNOWN]'
    def get_log_level(self):
        return {logging.NOTSET: 'NOTSET', logging.DEBUG:'DEBUG',
                logging.INFO: 'INFO', logging.WARNING: 'WARNING',
                logging.ERROR : 'ERROR', logging.CRITICAL : 'CRITICAL'}.get(logger._level, 'UNKNOWN')
    get_log_level.doc = doc

    doc = 'List the methods available on the daemon'
    def api(self):
        return self.app.http_daemon.registered_fun_names
    api.doc = doc


    doc = 'List the api methods and their parameters'
    def api_full(self):
        res = {}
        for (fname, f) in self.app.http_daemon.registered_fun.iteritems():
            fclean = fname.replace('_', '-')
            argspec = inspect.getargspec(f)
            args = [a for a in argspec.args if a != 'self']
            defaults = self.app.http_daemon.registered_fun_defaults.get(fname, {})
            e = {}
            # Get a string about the args and co
            _s_nondef_args = ', '.join([a for a in args if a not in defaults])
            _s_def_args = ', '.join( ['%s=%s' % (k,v) for (k,v) in defaults.iteritems()] )
            _s_args = ''
            if _s_nondef_args:
                _s_args +=_s_nondef_args
            if _s_def_args:
                _s_args += ', '+_s_def_args
            e['proto'] = '%s(%s)' % (fclean, _s_args)
            e['need_lock'] = getattr(f, 'need_lock', True)
            e['method'] = getattr(f, 'method', 'GET').upper()
            e['encode'] = getattr(f, 'encode', 'json')
            doc = getattr(f, 'doc', '')
            if doc:
                e['doc'] = doc
            res[fclean] = e
        return res
    api.doc = doc


# If we are under android, we can't give parameters
if is_android:
    DEFAULT_WORK_DIR = '/sdcard/sl4a/scripts/'
    DEFAULT_LIB_DIR  = DEFAULT_WORK_DIR
else:
    DEFAULT_WORK_DIR = '/var/run/shinken/'
    DEFAULT_LIB_DIR  = '/var/lib/shinken/'


class Daemon(object):

    properties = {
        # workdir is relative to $(dirname "$0"/..)
        # where "$0" is the path of the file being executed,
        # in python normally known as:
        #
        #  os.path.join( os.getcwd(), sys.argv[0] )
        #
        # as returned once the daemon is started.
        'workdir':       PathProp(default=DEFAULT_WORK_DIR),
        'modules_dir':    PathProp(default=os.path.join(DEFAULT_LIB_DIR, 'modules')),
        'host':          StringProp(default='0.0.0.0'),
        'user':          StringProp(default=get_cur_user()),
        'group':         StringProp(default=get_cur_group()),
        'use_ssl':       BoolProp(default='0'),
        'server_key':     StringProp(default='etc/certs/server.key'),
        'ca_cert':       StringProp(default='etc/certs/ca.pem'),
        'server_cert':   StringProp(default='etc/certs/server.cert'),
        'use_local_log': BoolProp(default='1'),
        'log_level':     LogLevelProp(default='WARNING'),
        'hard_ssl_name_check':    BoolProp(default='0'),
        'idontcareaboutsecurity': BoolProp(default='0'),
        'daemon_enabled':BoolProp(default='1'),
        'spare':         BoolProp(default='0'),
        'max_queue_size': IntegerProp(default='0'),
        'daemon_thread_pool_size': IntegerProp(default='8'),
        'http_backend':  StringProp(default='auto'),
    }

    def __init__(self, name, config_file, is_daemon, do_replace, debug, debug_file):

        self.check_shm()

        self.name = name
        self.config_file = config_file
        self.is_daemon = is_daemon
        self.do_replace = do_replace
        self.debug = debug
        self.debug_file = debug_file
        self.interrupted = False

        # Track time
        now = time.time()
        self.program_start = now
        self.t_each_loop = now  # used to track system time change
        self.sleep_time = 0.0  # used to track the time we wait

        self.http_daemon = None

        # Log init
        #self.log = logger
        #self.log.load_obj(self)
        logger.load_obj(self)

        self.new_conf = None  # used by controller to push conf
        self.cur_conf = None

        # Flag to know if we need to dump memory or not
        self.need_dump_memory = False

        # Flag to dump objects or not
        self.need_objects_dump = False

        # Keep a trace of the local_log file desc if needed
        self.local_log_fd = None

        # Put in queue some debug output we will raise
        # when we will be in daemon
        self.debug_output = []

        # We will initialize the Manager() when we load modules
        # and be really forked()
        self.manager = None

        os.umask(UMASK)
        self.set_exit_handler()


    # At least, lose the local log file if needed
    def do_stop(self):
        # Maybe the modules manager is not even created!
        if getattr(self, 'modules_manager', None):
            # We save what we can but NOT for the scheduler
            # because the current sched object is a dummy one
            # and the old one has already done it!
            if not hasattr(self, 'sched'):
                self.hook_point('save_retention')
            # And we quit
            print('Stopping all modules')
            self.modules_manager.stop_all()
            print('Stopping inter-process message')
        if self.http_daemon:
            # Release the lock so the daemon can shutdown without problem
            try:
                self.http_daemon.lock.release()
            except:
                pass
            self.http_daemon.shutdown()
        logger.quit()


    def request_stop(self):
        self.unlink()
        self.do_stop()
        # Brok facilities are no longer available simply print the message to STDOUT
        print ("Stopping daemon. Exiting", )
        sys.exit(0)


    # Maybe this daemon is configured to NOT run, if so, bailout
    def look_for_early_exit(self):
        if not self.daemon_enabled:
            logger.info('This daemon is disabled in configuration. Bailing out')
            self.request_stop()


    def do_loop_turn(self):
        raise NotImplementedError()


    # Main loop for nearly all daemon
    # the scheduler is not managed by it :'(
    def do_mainloop(self):
        while True:
            self.do_loop_turn()
            # If ask us to dump memory, do it
            if self.need_dump_memory:
                self.dump_memory()
                self.need_dump_memory = False
            if self.need_objects_dump:
                logger.debug('Dumping objects')
                self.need_objects_dump = False
            # Maybe we ask us to die, if so, do it :)
            if self.interrupted:
                break
        self.request_stop()


    def do_load_modules(self):
        self.modules_manager.load_and_init()
        logger.info("I correctly loaded the modules: [%s]" % (','.join([inst.get_name() for inst in self.modules_manager.instances])))


    # Dummy method for adding broker to this daemon
    def add(self, elt):
        pass


    def dump_memory(self):
        logger.info("I dump my memory, it can take a minute")
        try:
            from guppy import hpy
            hp = hpy()
            logger.info(hp.heap())
        except ImportError:
            logger.warning('I do not have the module guppy for memory dump, please install it')


    def load_config_file(self):
        self.parse_config_file()
        if self.config_file is not None:
            # Some paths can be relatives. We must have a full path by taking
            # the config file by reference
            self.relative_paths_to_full(os.path.dirname(self.config_file))


    def load_modules_manager(self):
        self.modules_manager = ModulesManager(self.name, self.find_modules_path(), [])
        # Set the modules watchdogs
        # TOFIX: Beware, the arbiter do not have the max_queue_size property
        self.modules_manager.set_max_queue_size(getattr(self, 'max_queue_size', 0))
        # And make the module manager load the sub-process Queue() manager
        self.modules_manager.load_manager(self.manager)



    def change_to_workdir(self):
        self.workdir = os.path.abspath(self.workdir)
        try:
            os.chdir(self.workdir)
        except Exception, e:
            raise InvalidWorkDir(e)
        self.debug_output.append("Successfully changed to workdir: %s" % (self.workdir))


    def unlink(self):
        logger.debug("Unlinking %s" % self.pidfile)
        try:
            os.unlink(self.pidfile)
        except Exception, e:
            logger.error("Got an error unlinking our pidfile: %s" % (e))


    # Look if we need a local log or not
    def register_local_log(self):
        # The arbiter doesn't have such attribute
        if hasattr(self, 'use_local_log') and self.use_local_log:
            try:
                #self.local_log_fd = self.log.register_local_log(self.local_log)
                self.local_log_fd = logger.register_local_log(self.local_log)
            except IOError, exp:
                logger.error("Opening the log file '%s' failed with '%s'" % (self.local_log, exp))
                sys.exit(2)
            logger.info("Using the local log file '%s'" % self.local_log)


    # Only on linux: Check for /dev/shm write access
    def check_shm(self):
        import stat
        shm_path = '/dev/shm'
        if os.name == 'posix' and os.path.exists(shm_path):
            # We get the access rights, and we check them
            mode = stat.S_IMODE(os.lstat(shm_path)[stat.ST_MODE])
            if not mode & stat.S_IWUSR or not mode & stat.S_IRUSR:
                logger.critical("The directory %s is not writable or readable. Please make it read writable: %s" % (shm_path, shm_path))
                sys.exit(2)


    def __open_pidfile(self, write=False):
        ## if problem on opening or creating file it'll be raised to the caller:
        try:
            p = os.path.abspath(self.pidfile)
            self.debug_output.append("Opening pid file: %s" % p)
            # Windows do not manage the rw+ mode, so we must open in read mode first, then reopen it write mode...
            if not write and os.path.exists(p):
                self.fpid = open(p, 'r+')
            else:  # If it doesn't exist too, we create it as void
                self.fpid = open(p, 'w+')
        except Exception as err:
            raise InvalidPidFile(err)


    # Check (in pidfile) if there isn't already a daemon running. If yes and do_replace: kill it.
    # Keep in self.fpid the File object to the pidfile. Will be used by writepid.
    def check_parallel_run(self):
        # TODO: other daemon run on nt
        if os.name == 'nt':
            logger.warning("The parallel daemon check is not available on nt")
            self.__open_pidfile(write=True)
            return

        # First open the pid file in open mode
        self.__open_pidfile()
        try:
            pid = int(self.fpid.readline().strip(' \r\n'))
        except Exception as err:
            logger.info("Stale pidfile exists at %s (%s). Reusing it." % (err, self.pidfile))
            return

        try:
            os.kill(pid, 0)
        except Exception as err: # consider any exception as a stale pidfile.
            # this includes :
            #  * PermissionError when a process with same pid exists but is executed by another user.
            #  * ProcessLookupError: [Errno 3] No such process.
            logger.info("Stale pidfile exists (%s), Reusing it." % err)
            return

        if not self.do_replace:
            raise SystemExit("valid pidfile exists (pid=%s) and not forced to replace. Exiting." % pid)

        self.debug_output.append("Replacing previous instance %d" % pid)
        try:
            os.kill(pid, signal.SIGQUIT)
        except os.error as err:
            if err.errno != errno.ESRCH:
                raise

        self.fpid.close()
        ## TODO: give some time to wait that previous instance finishes?
        time.sleep(1)
        # we must also reopen the pid file in write mode
        # because the previous instance should have deleted it!!
        self.__open_pidfile(write=True)


    def write_pid(self, pid=None):
        if pid is None:
            pid = os.getpid()
        self.fpid.seek(0)
        self.fpid.truncate()
        self.fpid.write("%d" % (pid))
        self.fpid.close()
        del self.fpid  ## no longer needed


    # Close all the process file descriptors. Skip the descriptors
    # present in the skip_close_fds list
    def close_fds(self, skip_close_fds):
        # First we manage the file descriptor, because debug file can be
        # relative to pwd
        import resource
        maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]
        if (maxfd == resource.RLIM_INFINITY):
            maxfd = 1024

        # Iterate through and close all file descriptors.
        for fd in range(0, maxfd):
            if fd in skip_close_fds:
                continue
            try:
                os.close(fd)
            except OSError:  # ERROR, fd wasn't open to begin with (ignored)
                pass


    # Go in "daemon" mode: close unused fds, redirect stdout/err,
    # chdir, umask, fork-setsid-fork-writepid
    def daemonize(self, skip_close_fds=None):
        if skip_close_fds is None:
            skip_close_fds = tuple()

        self.debug_output.append("Redirecting stdout and stderr as necessary..")
        if self.debug:
            fdtemp = os.open(self.debug_file, os.O_CREAT | os.O_WRONLY | os.O_TRUNC)
        else:
            fdtemp = os.open(REDIRECT_TO, os.O_RDWR)

        ## We close all fd but what we need:
        self.close_fds(skip_close_fds + (self.fpid.fileno(), fdtemp))

        os.dup2(fdtemp, 1)  # standard output (1)
        os.dup2(fdtemp, 2)  # standard error (2)

        # Now the fork/setsid/fork..
        try:
            pid = os.fork()
        except OSError, e:
            raise Exception, "%s [%d]" % (e.strerror, e.errno)
        if pid != 0:
            # In the father: we check if our child exit correctly
            # it has to write the pid of our future little child..
            def do_exit(sig, frame):
                logger.error("Timeout waiting child while it should have quickly returned ; something weird happened")
                os.kill(pid, 9)
                sys.exit(1)
            # wait the child process to check its return status:
            signal.signal(signal.SIGALRM, do_exit)
            signal.alarm(3)  # forking & writing a pid in a file should be rather quick..
            # if it's not then something wrong can already be on the way so let's wait max 3 secs here.
            pid, status = os.waitpid(pid, 0)
            if status != 0:
                logger.error("Something weird happened with/during second fork: status=", status)
            os._exit(status != 0)

        # halfway to daemonize..
        os.setsid()
        try:
            pid = os.fork()
        except OSError, e:
            raise Exception, "%s [%d]" % (e.strerror, e.errno)
        if pid != 0:
            # we are the last step and the real daemon is actually correctly created at least.
            # we have still the last responsibility to write the pid of the daemon itself.
            self.write_pid(pid)
            os._exit(0)

        self.fpid.close()
        del self.fpid
        self.pid = os.getpid()
        self.debug_output.append("We are now fully daemonized :) pid=%d" % self.pid)
        # We can now output some previously silenced debug output
        logger.warning("Printing stored debug messages prior to our daemonization")
        for s in self.debug_output:
            logger.debug(s)
        del self.debug_output


    def do_daemon_init_and_start(self, use_pyro=True):
        self.change_to_user_group()
        self.change_to_workdir()
        self.check_parallel_run()
        if use_pyro:
            self.setup_pyro_daemon()

        # Setting log level
        logger.set_level(self.log_level)
        # Force the debug level if the daemon is said to start with such level
        if self.debug:
            logger.set_level('DEBUG')

        # Then start to log all in the local file if asked so
        self.register_local_log()
        if self.is_daemon:
            socket_fds = [sock.fileno() for sock in self.http_daemon.get_sockets()]
            # Do not close the local_log file too if it's open
            if self.local_log_fd:
                socket_fds.append(self.local_log_fd)

            socket_fds = tuple(socket_fds)
            self.daemonize(skip_close_fds=socket_fds)
        else:
            self.write_pid()

        # Now we can start our Manager
        # interprocess things. It's important!
        if is_android:
            self.manager = None
        else:
            # The Manager is a sub-process, so we must be sure it won't have
            # a socket of your http server alive
            self.manager = SyncManager(('127.0.0.1',0))
            def close_http_daemon(daemon):
                try:
                    # Be sure to release the lock so there won't be lock in shutdown phase
                    daemon.lock.release()
                except Exception, exp:
                    pass
                daemon.shutdown()
            # Some multiprocessing lib got problems with start() that cannot take args
            # so we must look at it before
            startargs = inspect.getargspec(self.manager.start)
            # startargs[0] will be ['self'] if old multiprocessing lib
            # and ['self', 'initializer', 'initargs'] in newer ones
            # note: windows do not like pickle http_daemon...
            if os.name != 'nt' and len(startargs[0]) > 1:
                self.manager.start(close_http_daemon, initargs=(self.http_daemon,))
            else:
                self.manager.start()
            # Keep this daemon in the http_daemn module
        # Will be add to the modules manager later

        # Now start the http_daemon thread
        self.http_thread = None
        if use_pyro:
            # Directly acquire it, so the http_thread will wait for us
            self.http_daemon.lock.acquire()
            self.http_thread = threading.Thread(None, self.http_daemon_thread, 'http_thread')
            # Don't lock the main thread just because of the http thread
            self.http_thread.daemon = True
            self.http_thread.start()


    # TODO: we do not use pyro anymore, change the function name....
    def setup_pyro_daemon(self):
        if hasattr(self, 'use_ssl'):  # "common" daemon
            ssl_conf = self
        else:
            ssl_conf = self.conf     # arbiter daemon..

        use_ssl = ssl_conf.use_ssl
        ca_cert = ssl_cert = ssl_key = ''
        http_backend = self.http_backend

        # The SSL part
        if use_ssl:
            ssl_cert = os.path.abspath(str(ssl_conf.server_cert))
            if not os.path.exists(ssl_cert):
                logger.error('Error : the SSL certificate %s is missing (server_cert). Please fix it in your configuration' % ssl_cert)
                sys.exit(2)
            ca_cert = os.path.abspath(str(ssl_conf.ca_cert))
            logger.info("Using ssl ca cert file: %s" % ca_cert)
            ssl_key = os.path.abspath(str(ssl_conf.server_key))
            if not os.path.exists(ssl_key):
                logger.error('Error : the SSL key %s is missing (server_key). Please fix it in your configuration' % ssl_key)
                sys.exit(2)
            logger.info("Using ssl server cert/key files: %s/%s" % (ssl_cert, ssl_key))

            if ssl_conf.hard_ssl_name_check:
                logger.info("Enabling hard SSL server name verification")

        # Let's create the HTTPDaemon, it will be exec after
        self.http_daemon = HTTPDaemon(self.host, self.port, http_backend, use_ssl, ca_cert, ssl_key, ssl_cert, ssl_conf.hard_ssl_name_check, self.daemon_thread_pool_size)
        http_daemon.daemon_inst = self.http_daemon

    # Global loop part
    def get_socks_activity(self, socks, timeout):
        # some os are not managing void socks list, so catch this
        # and just so a simple sleep instead
        if socks == []:
            time.sleep(timeout)
            return []
        try:
            ins, _, _ = select.select(socks, [], [], timeout)
        except select.error, e:
            errnum, _ = e
            if errnum == errno.EINTR:
                return []
            raise
        return ins


    # Find the absolute path of the shinken module directory and returns it.
    # If the directory do not exist, we must exit!
    def find_modules_path(self):
        if not hasattr(self, 'modules_dir') or not self.modules_dir:
            logger.error("Your configuration is missing the path to the modules (modules_dir). I set it by default to /var/lib/shinken/modules. Please configure it")
            self.modules_dir = '/var/lib/shinken/modules'
        self.modules_dir = os.path.abspath(self.modules_dir)
        logger.info("Modules directory: %s" % (self.modules_dir))
        if not os.path.exists(self.modules_dir):
            logger.error("The modules directory '%s' is missing! Bailing out. Please fix your configuration" % self.modules_dir)
            raise Exception("The modules directory '%s' is missing! Bailing out. Please fix your configuration" % self.modules_dir)

        # Ok remember to populate the modulesctx object
        modulesctx.set_modulesdir(self.modules_dir)

        return self.modules_dir


    # modules can have process, and they can die
    def check_and_del_zombie_modules(self):
        # Active children make a join with every one, useful :)
        act = active_children()
        self.modules_manager.check_alive_instances()
        # and try to restart previous dead :)
        self.modules_manager.try_to_restart_deads()


    # Just give the uid of a user by looking at it's name
    def find_uid_from_name(self):
        try:
            return getpwnam(self.user)[2]
        except KeyError, exp:
            logger.error("The user %s is unknown" % self.user)
            return None

    # Just give the gid of a group by looking at its name
    def find_gid_from_name(self):
        try:
            return getgrnam(self.group)[2]
        except KeyError, exp:
            logger.error("The group %s is unknown" % self.group)
            return None

    # Change user of the running program. Just insult the admin
    # if he wants root run (it can override). If change failed we sys.exit(2)
    def change_to_user_group(self, insane=None):
        if insane is None:
            insane = not self.idontcareaboutsecurity

        if is_android:
            logger.warning("You can't change user on this system")
            return

        # TODO: change user on nt
        if os.name == 'nt':
            logger.warning("You can't change user on this system")
            return

        if (self.user == 'root' or self.group == 'root') and not insane:
            logger.error("You want the application run under the root account?")
            logger.error("I am not agree with it. If you really want it, put:")
            logger.error("idontcareaboutsecurity=yes")
            logger.error("in the config file")
            logger.error("Exiting")
            sys.exit(2)

        uid = self.find_uid_from_name()
        gid = self.find_gid_from_name()

        if uid is None or gid is None:
            logger.error("uid or gid is none. Exiting")
            sys.exit(2)

        # Maybe the os module got the initgroups function. If so, try to call it.
        # Do this when we are still root
        if hasattr(os, 'initgroups'):
            logger.info('Trying to initialize additional groups for the daemon')
            try:
                os.initgroups(self.user, gid)
            except OSError, e:
                logger.warning('Cannot call the additional groups setting with initgroups (%s)' % e.strerror)
        try:
            # First group, then user :)
            os.setregid(gid, gid)
            os.setreuid(uid, uid)
        except OSError, e:
            logger.error("cannot change user/group to %s/%s (%s [%d]). Exiting" % (self.user, self.group, e.strerror, e.errno))
            sys.exit(2)


    # Parse self.config_file and get all properties in it.
    # If some properties need a pythonization, we do it.
    # Also put default value in the properties if some are missing in the config_file
    def parse_config_file(self):
        properties = self.__class__.properties
        if self.config_file is not None:
            config = ConfigParser.ConfigParser()
            config.read(self.config_file)
            if config._sections == {}:
                logger.error("Bad or missing config file: %s " % self.config_file)
                sys.exit(2)
            try:
                for (key, value) in config.items('daemon'):
                    if key in properties:
                        value = properties[key].pythonize(value)
                    setattr(self, key, value)
            except ConfigParser.InterpolationMissingOptionError, e:
                e = str(e)
                wrong_variable = e.split('\n')[3].split(':')[1].strip()
                logger.error("Incorrect or missing variable '%s' in config file : %s" % (wrong_variable, self.config_file))
                sys.exit(2)
        else:
            logger.warning("No config file specified, use defaults parameters")
        # Now fill all defaults where missing parameters
        for prop, entry in properties.items():
            if not hasattr(self, prop):
                value = entry.pythonize(entry.default)
                setattr(self, prop, value)


    # Some paths can be relatives. We must have a full path by taking
    # the config file by reference
    def relative_paths_to_full(self, reference_path):
        #print "Create relative paths with", reference_path
        properties = self.__class__.properties
        for prop, entry in properties.items():
            if isinstance(entry, ConfigPathProp):
                path = getattr(self, prop)
                if not os.path.isabs(path):
                    new_path = os.path.join(reference_path, path)
                    #print "DBG: changing", entry, "from", path, "to", new_path
                    path = new_path
                setattr(self, prop, path)
                #print "Setting %s for %s" % (path, prop)


    def manage_signal(self, sig, frame):
        logger.debug("I'm process %d and I received signal %s" % (os.getpid(), str(sig)))
        if sig == signal.SIGUSR1:  # if USR1, ask a memory dump
            self.need_dump_memory = True
        elif sig == signal.SIGUSR2: # if USR2, ask objects dump
            self.need_objects_dump = True
        else:  # Ok, really ask us to die :)
            self.interrupted = True


    def set_exit_handler(self):
        func = self.manage_signal
        if os.name == "nt":
            try:
                import win32api
                win32api.SetConsoleCtrlHandler(func, True)
            except ImportError:
                version = ".".join(map(str, sys.version_info[:2]))
                raise Exception("pywin32 not installed for Python " + version)
        else:
            for sig in (signal.SIGTERM, signal.SIGINT, signal.SIGUSR1, signal.SIGUSR2):
                signal.signal(sig, func)


    def get_header(self):
        return ["Shinken %s" % VERSION,
                "Copyright (c) 2009-2014:",
                "Gabes Jean (naparuba@gmail.com)",
                "Gerhard Lausser, Gerhard.Lausser@consol.de",
                "Gregory Starck, g.starck@gmail.com",
                "Hartmut Goebel, h.goebel@goebel-consult.de",
                "License: AGPL"]


    def print_header(self):
        for line in self.get_header():
            logger.info(line)


    # Main fonction of the http daemon thread will loop forever unless we stop the root daemon
    def http_daemon_thread(self):
        logger.info("Starting HTTP daemon")

        # The main thing is to have a pool of X concurrent requests for the http_daemon,
        # so "no_lock" calls can always be directly answer without having a "locked" version to
        # finish
        print "GO FOR IT"
        try:
            self.http_daemon.run()
        except Exception, exp:
            logger.error('The HTTP daemon failed with the error %s, exiting' % str(exp))
            output = cStringIO.StringIO()
            traceback.print_exc(file=output)
            logger.error("Back trace of this error: %s" % (output.getvalue()))
            output.close()
            self.do_stop()
            # Hard mode exit from a thread
            os._exit(2)


    # Wait up to timeout to handle the pyro daemon requests.
    # If suppl_socks is given it also looks for activity on that list of fd.
    # Returns a 3-tuple:
    # If timeout: first arg is 0, second is [], third is possible system time change value
    # If not timeout (== some fd got activity):
    #  - first arg is elapsed time since wait,
    #  - second arg is sublist of suppl_socks that got activity.
    #  - third arg is possible system time change value, or 0 if no change.
    def handleRequests(self, timeout, suppl_socks=None):
        if suppl_socks is None:
            suppl_socks = []
        before = time.time()
        socks = []
        if suppl_socks:
            socks.extend(suppl_socks)
        # Release the lock so the http_thread can manage request during we are waiting
        if self.http_daemon:
            self.http_daemon.lock.release()
        # Ok give me the socks taht moved during the timeout max
        ins = self.get_socks_activity(socks, timeout)
        # Ok now get back the global lock!
        if self.http_daemon:
            self.http_daemon.lock.acquire()
        tcdiff = self.check_for_system_time_change()
        before += tcdiff
        # Increase our sleep time for the time go in select
        self.sleep_time += time.time() - before
        if len(ins) == 0:  # trivial case: no fd activity:
            return 0, [], tcdiff
        # HERE WAS THE HTTP, but now it's managed in an other thread
        #for sock in socks:
        #    if sock in ins and sock not in suppl_socks:
        #        ins.remove(sock)
        # Track in elapsed the WHOLE time, even with handling requests
        elapsed = time.time() - before
        if elapsed == 0:  # we have done a few instructions in 0 second exactly!? quantum computer?
            elapsed = 0.01  # but we absolutely need to return!= 0 to indicate that we got activity
        return elapsed, ins, tcdiff


    # Check for a possible system time change and act correspondingly.
    # If such a change is detected then we return the number of seconds that changed. 0 if no time change was detected.
    # Time change can be positive or negative:
    # positive when we have been sent in the future and negative if we have been sent in the past.
    def check_for_system_time_change(self):
        now = time.time()
        difference = now - self.t_each_loop

        # If we have more than 15 min time change, we need to compensate it
        if abs(difference) > 900:
            self.compensate_system_time_change(difference)
        else:
            difference = 0

        self.t_each_loop = now

        return difference


    # Default action for system time change. Actually a log is done
    def compensate_system_time_change(self, difference):
        logger.warning('A system time change of %s has been detected.  Compensating...' % difference)


    # Use to wait conf from arbiter.
    # It send us conf in our http_daemon. It put the have_conf prop
    # if he send us something
    # (it can just do a ping)
    def wait_for_initial_conf(self, timeout=1.0):
        logger.info("Waiting for initial configuration")
        cur_timeout = timeout
        # Arbiter do not already set our have_conf param
        while not self.new_conf and not self.interrupted:
            elapsed, _, _ = self.handleRequests(cur_timeout)
            if elapsed:
                cur_timeout -= elapsed
                if cur_timeout > 0:
                    continue
                cur_timeout = timeout
            sys.stdout.write(".")
            sys.stdout.flush()


    # We call the function of modules that got the this
    # hook function
    def hook_point(self, hook_name):
        for inst in self.modules_manager.instances:
            full_hook_name = 'hook_' + hook_name
            if hasattr(inst, full_hook_name):
                f = getattr(inst, full_hook_name)
                try:
                    f(self)
                except Exception, exp:
                    logger.warning('The instance %s raised an exception %s. I disabled it, and set it to restart later' % (inst.get_name(), str(exp)))
                    self.modules_manager.set_to_restart(inst)


    # Dummy function for daemons. Get all retention data
    # So a module can save them
    def get_retention_data(self):
        return []


    # Save, to get back all data
    def restore_retention_data(self, data):
        pass

########NEW FILE########
__FILENAME__ = arbiterdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import os
import time
import traceback
from Queue import Empty
import socket
import traceback
import cStringIO
import cPickle
import copy
import json

from shinken.objects.config import Config
from shinken.external_command import ExternalCommandManager
from shinken.dispatcher import Dispatcher
from shinken.daemon import Daemon, Interface
from shinken.log import logger
from shinken.brok import Brok
from shinken.external_command import ExternalCommand
from shinken.util import jsonify_r

# Interface for the other Arbiter
# It connects, and together we decide who's the Master and who's the Slave, etc.
# Here is a also a function to get a new conf from the master
class IForArbiter(Interface):
    
    doc = 'Does the daemon got a configuration (internal)'
    def have_conf(self, magic_hash):
        # Beware, we got an str in entry, not an int
        magic_hash = int(magic_hash)
        # I've got a conf and a good one
        if self.app.cur_conf and self.app.cur_conf.magic_hash == magic_hash:
            return True
        else:  # I've no conf or a bad one
            return False
    have_conf.doc = doc

    
    doc = 'Put a new configuration to the daemon'
    # The master Arbiter is sending us a new conf in a pickle way. Ok, we take it
    def put_conf(self, conf):
        conf = cPickle.loads(conf)
        super(IForArbiter, self).put_conf(conf)
        self.app.must_run = False
    put_conf.method = 'POST'
    put_conf.doc = doc


    doc = 'Get the managed configuration (internal)'
    def get_config(self):
        return self.app.conf
    get_config.doc = doc


    doc = 'Ask the daemon to do not run'
    # The master arbiter asks me not to run!
    def do_not_run(self):
        # If I'm the master, ignore the command
        if self.app.is_master:
            logger.debug("Received message to not run. I am the Master, ignore and continue to run.")
        # Else, I'm just a spare, so I listen to my master
        else:
            logger.debug("Received message to not run. I am the spare, stopping.")
            self.app.last_master_speack = time.time()
            self.app.must_run = False
    do_not_run.need_lock = False
    do_not_run.doc = doc


    doc = 'Get the satellite names sort by type'
    # Here a function called by check_shinken to get daemons list
    def get_satellite_list(self, daemon_type=''):
        res = {}
        for t in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver',
                  'broker']:
            if daemon_type and daemon_type != t:
                continue
            satellite_list = []
            res[t] = satellite_list
            daemon_name_attr = t + "_name"
            daemons = self.app.get_daemons(t)
            for dae in daemons:
                if hasattr(dae, daemon_name_attr):
                    satellite_list.append(getattr(dae, daemon_name_attr))
        return res
    get_satellite_list.doc = doc


    doc = 'Dummy call for the arbiter'
    # Dummy call. We are the master, we manage what we want
    def what_i_managed(self):
        return {}
    what_i_managed.need_lock = False
    what_i_managed.doc = doc

    
    doc = 'Return all the data of the satellites'
    # We will try to export all data from our satellites, but only the json-able fields
    def get_all_states(self):
        res = {}
        for t in ['arbiter', 'scheduler', 'poller', 'reactionner', 'receiver',
                  'broker']:
            lst = []
            res[t] = lst
            for d in getattr(self.app.conf, t+'s'):
                cls = d.__class__
                e = {}
                ds = [cls.properties, cls.running_properties]
                for _d in ds:
                    for prop in _d:
                        if hasattr(d, prop):
                            v = getattr(d, prop)
                            # give a try to a json able object
                            try:
                                json.dumps(v)
                                e[prop] = v
                            except Exception, exp:
                                print exp
                    lst.append(e)
                        
        return lst
    get_all_states.doc = doc
    
    
    # Try to give some properties of our objects
    doc = 'Dump all objects of the type in [hosts, services, contacts, commands, hostgroups, servicegroups]'
    def get_objects_properties(self, table):
        logger.debug('ASK:: table= %s' % str(table))
        objs = getattr(self.app.conf, table, None)
        logger.debug("OBJS:: %s" % str(objs))
        if objs is None or len(objs) == 0:
            return []
        res = []
        for obj in objs:
            l = jsonify_r(obj)
            res.append(l)
        return res
    get_objects_properties.doc = doc


# Main Arbiter Class
class Arbiter(Daemon):

    def __init__(self, config_files, is_daemon, do_replace, verify_only, debug, debug_file, profile=None, analyse=None, migrate=None, arb_name=''):

        super(Arbiter, self).__init__('arbiter', config_files[0], is_daemon, do_replace, debug, debug_file)

        self.config_files = config_files
        self.verify_only = verify_only
        self.analyse = analyse
        self.migrate = migrate
        self.arb_name = arb_name

        self.broks = {}
        self.is_master = False
        self.me = None

        self.nb_broks_send = 0

        # Now tab for external_commands
        self.external_commands = []

        self.fifo = None

        # Used to work out if we must still be alive or not
        self.must_run = True

        self.interface = IForArbiter(self)
        self.conf = Config()



    # Use for adding things like broks
    def add(self, b):
        if isinstance(b, Brok):
            self.broks[b.id] = b
        elif isinstance(b, ExternalCommand):
            self.external_commands.append(b)
        else:
            logger.warning('Cannot manage object type %s (%s)' % (type(b), b))

    # We must push our broks to the broker
    # because it's stupid to make a crossing connection
    # so we find the broker responsible for our broks,
    # and we send it to him
    # TODO: better find the broker, here it can be dead?
    # or not the good one?
    def push_broks_to_broker(self):
        for brk in self.conf.brokers:
            # Send only if alive of course
            if brk.manage_arbiters and brk.alive:
                is_send = brk.push_broks(self.broks)
                if is_send:
                    # They are gone, we keep none!
                    self.broks.clear()

    # We must take external_commands from all satellites
    # like brokers, pollers, reactionners or receivers
    def get_external_commands_from_satellites(self):
        sat_lists = [self.conf.brokers, self.conf.receivers,
                     self.conf.pollers, self.conf.reactionners]
        for lst in sat_lists:
            for sat in lst:
                # Get only if alive of course
                if sat.alive:
                    new_cmds = sat.get_external_commands()
                    for new_cmd in new_cmds:
                        self.external_commands.append(new_cmd)

    # Our links to satellites can raise broks. We must send them
    def get_broks_from_satellitelinks(self):
        tabs = [self.conf.brokers, self.conf.schedulers,
                    self.conf.pollers, self.conf.reactionners,
                self.conf.receivers]
        for tab in tabs:
            for s in tab:
                new_broks = s.get_all_broks()
                for b in new_broks:
                    self.add(b)

    # Our links to satellites can raise broks. We must send them
    def get_initial_broks_from_satellitelinks(self):
        tabs = [self.conf.brokers, self.conf.schedulers,
                self.conf.pollers, self.conf.reactionners,
                self.conf.receivers]
        for tab in tabs:
            for s in tab:
                b = s.get_initial_status_brok()
                self.add(b)


    # Load the external commander
    def load_external_command(self, e):
        self.external_command = e
        self.fifo = e.open()


    def get_daemon_links(self, daemon_type):
        # the attribute name to get these differs for schedulers and arbiters
        return daemon_type + 's'


    def load_config_file(self):
        logger.info("Loading configuration")
        # REF: doc/shinken-conf-dispatching.png (1)
        buf = self.conf.read_config(self.config_files)
        raw_objects = self.conf.read_config_buf(buf)

        logger.debug("Opening local log file")

        # First we need to get arbiters and modules
        # so we can ask them for objects
        self.conf.create_objects_for_type(raw_objects, 'arbiter')
        self.conf.create_objects_for_type(raw_objects, 'module')

        self.conf.early_arbiter_linking()

        # Search which Arbiterlink I am
        for arb in self.conf.arbiters:
            if arb.is_me(self.arb_name):
                arb.need_conf = False
                self.me = arb
                self.is_master = not self.me.spare
                if self.is_master:
                    logger.info("I am the master Arbiter: %s" % arb.get_name())
                else:
                    logger.info("I am a spare Arbiter: %s" % arb.get_name())

                # Set myself as alive ;)
                self.me.alive = True
            else:  # not me
                arb.need_conf = True

        if not self.me:
            sys.exit("Error: I cannot find my own Arbiter object, I bail out. \
                     To solve it, please change the host_name parameter in \
                     the object Arbiter in the file shinken-specific.cfg. \
                     With the value %s \
                     Thanks." % socket.gethostname())

        logger.info("My own modules: " + ','.join([m.get_name() for m in self.me.modules]))

        self.modules_dir = getattr(self.conf, 'modules_dir', '')

        # Ok it's time to load the module manager now!
        self.load_modules_manager()
        # we request the instances without them being *started*
        # (for those that are concerned ("external" modules):
        # we will *start* these instances after we have been daemonized (if requested)
        self.modules_manager.set_modules(self.me.modules)
        self.do_load_modules()

        # Call modules that manage this read configuration pass
        self.hook_point('read_configuration')

        # Now we ask for configuration modules if they
        # got items for us
        for inst in self.modules_manager.instances:
            #TODO : clean
            if 'configuration' in inst.phases:
                try:
                    r = inst.get_objects()
                except Exception, exp:
                    logger.error("Instance %s raised an exception %s. Log and continue to run" % (inst.get_name(), str(exp)))
                    output = cStringIO.StringIO()
                    traceback.print_exc(file=output)
                    logger.error("Back trace of this remove: %s" % (output.getvalue()))
                    output.close()
                    continue

                types_creations = self.conf.types_creations
                for k in types_creations:
                    (cls, clss, prop) = types_creations[k]
                    if prop in r:
                        for x in r[prop]:
                            # test if raw_objects[k] are already set - if not, add empty array
                            if not k in raw_objects:
                                raw_objects[k] = []
                            # now append the object
                            raw_objects[k].append(x)
                        logger.debug("Added %i objects to %s from module %s" % (len(r[prop]), k, inst.get_name()))

        ### Resume standard operations ###
        self.conf.create_objects(raw_objects)

        # Maybe conf is already invalid
        if not self.conf.conf_is_correct:
            sys.exit("***> One or more problems was encountered while processing the config files...")

        # Change Nagios2 names to Nagios3 ones
        self.conf.old_properties_names_to_new()

        # Manage all post-conf modules
        self.hook_point('early_configuration')

        # Ok here maybe we should stop because we are in a pure migration run
        if self.migrate:
            print "Migration MODE. Early exiting from configuration relinking phase"
            return

        # Load all file triggers
        self.conf.load_triggers()

        # Create Template links
        self.conf.linkify_templates()

        # All inheritances
        self.conf.apply_inheritance()

        # Explode between types
        self.conf.explode()

        # Create Name reversed list for searching list
        self.conf.create_reversed_list()

        # Cleaning Twins objects
        self.conf.remove_twins()

        # Implicit inheritance for services
        self.conf.apply_implicit_inheritance()

        # Fill default values
        self.conf.fill_default()

        # Remove templates from config
        self.conf.remove_templates()

        # We compute simple item hash
        self.conf.compute_hash()

        # We removed templates, and so we must recompute the
        # search lists
        self.conf.create_reversed_list()

        # Overrides sepecific service instaces properties
        self.conf.override_properties()

        # Pythonize values
        self.conf.pythonize()

        # Removes service exceptions based on host configuration
        count = self.conf.remove_exclusions()

        if count > 0:
            # We removed excluded services, and so we must recompute the
            # search lists
            self.conf.create_reversed_list()

        # Linkify objects to each other
        self.conf.linkify()

        # applying dependencies
        self.conf.apply_dependencies()

        # Hacking some global parameters inherited from Nagios to create
        # on the fly some Broker modules like for status.dat parameters
        # or nagios.log one if there are none already available
        self.conf.hack_old_nagios_parameters()

        # Raise warning about currently unmanaged parameters
        if self.verify_only:
            self.conf.warn_about_unmanaged_parameters()

        # Explode global conf parameters into Classes
        self.conf.explode_global_conf()

        # set our own timezone and propagate it to other satellites
        self.conf.propagate_timezone_option()

        # Look for business rules, and create the dep tree
        self.conf.create_business_rules()
        # And link them
        self.conf.create_business_rules_dependencies()


        # Warn about useless parameters in Shinken
        if self.verify_only:
            self.conf.notice_about_useless_parameters()

        # Manage all post-conf modules
        self.hook_point('late_configuration')

        # Correct conf?
        self.conf.is_correct()

        # Maybe some elements where not wrong, so we must clean if possible
        self.conf.clean()

        # If the conf is not correct, we must get out now
        # if not self.conf.conf_is_correct:
        #    sys.exit("Configuration is incorrect, sorry, I bail out")

        # REF: doc/shinken-conf-dispatching.png (2)
        logger.info("Cutting the hosts and services into parts")
        self.confs = self.conf.cut_into_parts()

        # The conf can be incorrect here if the cut into parts see errors like
        # a realm with hosts and not schedulers for it
        if not self.conf.conf_is_correct:
            self.conf.show_errors()
            err = "Configuration is incorrect, sorry, I bail out"
            logger.error(err)
            sys.exit(err)

        logger.info('Things look okay - No serious problems were detected during the pre-flight check')

        # Clean objects of temporary/unnecessary attributes for live work:
        self.conf.clean()

        # Exit if we are just here for config checking
        if self.verify_only:
            sys.exit(0)

        if self.analyse:
            self.launch_analyse()
            sys.exit(0)

        # Some properties need to be "flatten" (put in strings)
        # before being send, like realms for hosts for example
        # BEWARE: after the cutting part, because we stringify some properties
        self.conf.prepare_for_sending()

        # Ok, here we must check if we go on or not.
        # TODO: check OK or not
        self.log_level = self.conf.log_level
        self.use_local_log = self.conf.use_local_log
        self.local_log = self.conf.local_log
        self.pidfile = os.path.abspath(self.conf.lock_file)
        self.idontcareaboutsecurity = self.conf.idontcareaboutsecurity
        self.user = self.conf.shinken_user
        self.group = self.conf.shinken_group
        self.daemon_enabled = self.conf.daemon_enabled
        self.daemon_thread_pool_size = self.conf.daemon_thread_pool_size
        self.http_backend = getattr(self.conf, 'http_backend', 'auto')

        # If the user sets a workdir, lets use it. If not, use the
        # pidfile directory
        if self.conf.workdir == '':
            self.workdir = os.path.abspath(os.path.dirname(self.pidfile))
        else:
            self.workdir = self.conf.workdir
        #print "DBG curpath=", os.getcwd()
        #print "DBG pidfile=", self.pidfile
        #print "DBG workdir=", self.workdir

        ##  We need to set self.host & self.port to be used by do_daemon_init_and_start
        self.host = self.me.address
        self.port = self.me.port

        logger.info("Configuration Loaded")


    def launch_analyse(self):
        try:
            import json
        except ImportError:
            logger.error("Error: json is need for statistics file saving. Please update your python version to 2.6")
            sys.exit(2)

        logger.info("We are doing an statistic analysis on the dump file" % self.analyse)
        stats = {}
        types = ['hosts', 'services', 'contacts', 'timeperiods', 'commands', 'arbiters',
                 'schedulers', 'pollers', 'reactionners', 'brokers', 'receivers', 'modules',
                 'realms']
        for t in types:
            lst = getattr(self.conf, t)
            nb = len([i for i in lst])
            stats['nb_' + t] = nb
            logger.info("Got %s for %s" % (nb, t))

        max_srv_by_host = max([len(h.services) for h in self.conf.hosts])
        logger.info("Max srv by host" % max_srv_by_host)
        stats['max_srv_by_host'] = max_srv_by_host

        f = open(self.analyse, 'w')
        s = json.dumps(stats)
        logger.info("Saving stats data to a file" % s)
        f.write(s)
        f.close()


    def go_migrate(self):
        print "***********"*5
        print "WARNING : this feature is NOT supported in this version!"
        print "***********"*5

        migration_module_name = self.migrate.strip()
        mig_mod = self.conf.modules.find_by_name(migration_module_name)
        if not mig_mod:
            print "Cannot find the migration module %s. Please configure it" % migration_module_name
            sys.exit(2)

        print self.modules_manager.instances
        # Ok now all we need is the import module
        self.modules_manager.set_modules([mig_mod])
        self.do_load_modules()
        print self.modules_manager.instances
        if len(self.modules_manager.instances) == 0:
            print "Error during the initialization of the import module. Bailing out"
            sys.exit(2)
        print "Configuration migrating in progress..."
        mod  = self.modules_manager.instances[0]
        f = getattr(mod, 'import_objects', None)
        if not f or not callable(f):
            print "Import module is missing the import_objects function. Bailing out"
            sys.exit(2)

        objs = {}
        types = ['hosts', 'services', 'commands', 'timeperiods', 'contacts']
        for t in types:
            print "New type", t
            objs[t] = []
            for i in getattr(self.conf, t):
                d = i.get_raw_import_values()
                if d:
                    objs[t].append(d)
            f(objs)
        # Ok we can exit now
        sys.exit(0)



    # Main loop function
    def main(self):
        try:
            # Log will be broks
            for line in self.get_header():
                logger.info(line)

            self.load_config_file()

            # Maybe we are in a migration phase. If so, we will bailout here
            if self.migrate:
                self.go_migrate()

            # Look if we are enabled or not. If ok, start the daemon mode
            self.look_for_early_exit()
            self.do_daemon_init_and_start()

            self.uri_arb = self.http_daemon.register(self.interface)#, "ForArbiter")

            # ok we are now fully daemonized (if requested)
            # now we can start our "external" modules (if any):
            self.modules_manager.start_external_instances()

            # Ok now we can load the retention data
            self.hook_point('load_retention')

            ## And go for the main loop
            self.do_mainloop()
        except SystemExit, exp:
            # With a 2.4 interpreter the sys.exit() in load_config_file
            # ends up here and must be handled.
            sys.exit(exp.code)
        except Exception, exp:
            logger.critical("I got an unrecoverable error. I have to exit")
            logger.critical("You can log a bug ticket at https://github.com/naparuba/shinken/issues/new to get help")
            logger.critical("Exception trace follows: %s" % (traceback.format_exc()))
            raise


    def setup_new_conf(self):
        """ Setup a new conf received from a Master arbiter. """
        conf = self.new_conf
        self.new_conf = None
        self.cur_conf = conf
        self.conf = conf
        for arb in self.conf.arbiters:
            if (arb.address, arb.port) == (self.host, self.port):
                self.me = arb
                arb.is_me = lambda x: True  # we now definitively know who we are, just keep it.
            else:
                arb.is_me = lambda x: False  # and we know who we are not, just keep it.


    def do_loop_turn(self):
        # If I am a spare, I wait for the master arbiter to send me
        # true conf.
        if self.me.spare:
            logger.debug("I wait for master")
            self.wait_for_master_death()

        if self.must_run:
            # Main loop
            self.run()


    # Get 'objects' from external modules
    # It can be used to get external commands for example
    def get_objects_from_from_queues(self):
        for f in self.modules_manager.get_external_from_queues():
            #print "Groking from module instance %s" % f
            while True:
                try:
                    o = f.get(block=False)
                    self.add(o)
                except Empty:
                    break
                # Maybe the queue had problems
                # log it and quit it
                except (IOError, EOFError), exp:
                    logger.error("An external module queue got a problem '%s'" % str(exp))
                    break


    # We wait (block) for arbiter to send us something
    def wait_for_master_death(self):
        logger.info("Waiting for master death")
        timeout = 1.0
        self.last_master_speack = time.time()

        # Look for the master timeout
        master_timeout = 300
        for arb in self.conf.arbiters:
            if not arb.spare:
                master_timeout = arb.check_interval * arb.max_check_attempts
        logger.info("I'll wait master for %d seconds" % master_timeout)


        while not self.interrupted:
            elapsed, _, tcdiff = self.handleRequests(timeout)
            # if there was a system Time Change (tcdiff) then we have to adapt last_master_speak:
            if self.new_conf:
                self.setup_new_conf()
            if tcdiff:
                self.last_master_speack += tcdiff
            if elapsed:
                self.last_master_speack = time.time()
                timeout -= elapsed
                if timeout > 0:
                    continue

            timeout = 1.0
            sys.stdout.write(".")
            sys.stdout.flush()

            # Now check if master is dead or not
            now = time.time()
            if now - self.last_master_speack > master_timeout:
                logger.info("Arbiter Master is dead. The arbiter %s take the lead" % self.me.get_name())
                self.must_run = True
                break


    # Take all external commands, make packs and send them to
    # the schedulers
    def push_external_commands_to_schedulers(self):
        # Now get all external commands and put them into the
        # good schedulers
        for ext_cmd in self.external_commands:
            self.external_command.resolve_command(ext_cmd)

        # Now for all alive schedulers, send the commands
        for sched in self.conf.schedulers:
            cmds = sched.external_commands
            if len(cmds) > 0 and sched.alive:
                logger.debug("Sending %d commands to scheduler %s" % (len(cmds), sched.get_name()))
                sched.run_external_commands(cmds)
            # clean them
            sched.external_commands = []


    # We will log if there are time period activations
    # change as NOTICE in logs.
    def check_and_log_tp_activation_change(self):
        for tp in self.conf.timeperiods:
            tp.check_and_log_activation_change()


    # Main function
    def run(self):
        # Before running, I must be sure who am I
        # The arbiters change, so we must re-discover the new self.me
        for arb in self.conf.arbiters:
            if arb.is_me(self.arb_name):
                self.me = arb

        if self.conf.human_timestamp_log:
            logger.set_human_format()
        logger.info("Begin to dispatch configurations to satellites")
        self.dispatcher = Dispatcher(self.conf, self.me)
        self.dispatcher.check_alive()
        self.dispatcher.check_dispatch()
        # REF: doc/shinken-conf-dispatching.png (3)
        self.dispatcher.dispatch()

        # Now we can get all initial broks for our satellites
        self.get_initial_broks_from_satellitelinks()

        suppl_socks = None

        # Now create the external commander. It's just here to dispatch
        # the commands to schedulers
        e = ExternalCommandManager(self.conf, 'dispatcher')
        e.load_arbiter(self)
        self.external_command = e

        logger.debug("Run baby, run...")
        timeout = 1.0

        while self.must_run and not self.interrupted:
            elapsed, ins, _ = self.handleRequests(timeout, suppl_socks)

            # If FIFO, read external command
            if ins:
                now = time.time()
                ext_cmds = self.external_command.get()
                if ext_cmds:
                    for ext_cmd in ext_cmds:
                        self.external_commands.append(ext_cmd)
                else:
                    self.fifo = self.external_command.open()
                    if self.fifo is not None:
                        suppl_socks = [self.fifo]
                    else:
                        suppl_socks = None
                elapsed += time.time() - now

            if elapsed or ins:
                timeout -= elapsed
                if timeout > 0:  # only continue if we are not over timeout
                    continue

            # Timeout
            timeout = 1.0  # reset the timeout value

            # Try to see if one of my module is dead, and
            # try to restart previously dead modules :)
            self.check_and_del_zombie_modules()

            # Call modules that manage a starting tick pass
            self.hook_point('tick')

            # Look for logging timeperiods activation change (active/inactive)
            self.check_and_log_tp_activation_change()

            # Now the dispatcher job
            self.dispatcher.check_alive()
            self.dispatcher.check_dispatch()
            # REF: doc/shinken-conf-dispatching.png (3)
            self.dispatcher.dispatch()
            self.dispatcher.check_bad_dispatch()

            # Now get things from our module instances
            self.get_objects_from_from_queues()

            # Maybe our satellites links raise new broks. Must reap them
            self.get_broks_from_satellitelinks()

            # One broker is responsible for our broks,
            # we must give him our broks
            self.push_broks_to_broker()
            self.get_external_commands_from_satellites()
            #self.get_external_commands_from_receivers()
            # send_conf_to_schedulers()

            if self.nb_broks_send != 0:
                logger.debug("Nb Broks send: %d" % self.nb_broks_send)
            self.nb_broks_send = 0

            self.push_external_commands_to_schedulers()

            # It's sent, do not keep them
            # TODO: check if really sent. Queue by scheduler?
            self.external_commands = []

            # If asked to dump my memory, I will do it
            if self.need_dump_memory:
                self.dump_memory()
                self.need_dump_memory = False


    def get_daemons(self, daemon_type):
        """ Returns the daemons list defined in our conf for the given type """
        # shouldn't the 'daemon_types' (whatever it is above) be always present?
        return getattr(self.conf, daemon_type + 's', None)


    # Helper functions for retention modules
    # So we give our broks and external commands
    def get_retention_data(self):
        r = {}
        r['broks'] = self.broks
        r['external_commands'] = self.external_commands
        return r


    # Get back our data from a retention module
    def restore_retention_data(self, data):
        broks = data['broks']
        external_commands = data['external_commands']
        self.broks.update(broks)
        self.external_commands.extend(external_commands)

########NEW FILE########
__FILENAME__ = brokerdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import sys
import time
import traceback
import cPickle
import base64
import zlib
import threading
from multiprocessing import active_children
from Queue import Empty

from shinken.satellite import BaseSatellite
from shinken.property import PathProp, IntegerProp
from shinken.util import sort_by_ids
from shinken.log import logger
from shinken.external_command import ExternalCommand
from shinken.http_client import HTTPClient, HTTPExceptions
from shinken.daemon import Daemon, Interface

class IStats(Interface):
    """ 
    Interface for various stats about broker activity
    """
    
    doc = 'Get raw stats from the daemon'
    def get_raw_stats(self):
        app = self.app
        res = {}

        insts = [inst for inst in app.modules_manager.instances if inst.is_external]
        for inst in insts:
            try:
                res.append( {'module_name':inst.get_name(), 'queue_size':inst.to_q.qsize()})
            except Exception, exp:
                res.append( {'module_name':inst.get_name(), 'queue_size':0})
        
        return res
    get_raw_stats.doc = doc


# Our main APP class
class Broker(BaseSatellite):

    properties = BaseSatellite.properties.copy()
    properties.update({
        'pidfile':   PathProp(default='brokerd.pid'),
        'port':      IntegerProp(default='7772'),
        'local_log': PathProp(default='brokerd.log'),
    })

    def __init__(self, config_file, is_daemon, do_replace, debug, debug_file, profile=''):

        super(Broker, self).__init__('broker', config_file, is_daemon, do_replace, debug, debug_file)

        # Our arbiters
        self.arbiters = {}

        # Our pollers and reactionners
        self.pollers = {}
        self.reactionners = {}

        # Modules are load one time
        self.have_modules = False

        # Can have a queue of external_commands given by modules
        # will be processed by arbiter
        self.external_commands = []

        # All broks to manage
        self.broks = []  # broks to manage
        # broks raised this turn and that needs to be put in self.broks
        self.broks_internal_raised = []
        # broks raised by the arbiters, we need a lock so the push can be in parallel
        # to our current activities and won't lock the arbiter
        self.arbiter_broks = []
        self.arbiter_broks_lock = threading.RLock()

        self.timeout = 1.0

        self.istats = IStats(self)
        

    # Schedulers have some queues. We can simplify the call by adding
    # elements into the proper queue just by looking at their type
    # Brok -> self.broks
    # TODO: better tag ID?
    # External commands -> self.external_commands
    def add(self, elt):
        cls_type = elt.__class__.my_type
        if cls_type == 'brok':
            # For brok, we TAG brok with our instance_id
            elt.instance_id = 0
            self.broks_internal_raised.append(elt)
            return
        elif cls_type == 'externalcommand':
            logger.debug("Enqueuing an external command '%s'" % str(ExternalCommand.__dict__))
            self.external_commands.append(elt)
        # Maybe we got a Message from the modules, it's way to ask something
        # like from now a full data from a scheduler for example.
        elif cls_type == 'message':
            # We got a message, great!
            logger.debug(str(elt.__dict__))
            if elt.get_type() == 'NeedData':
                data = elt.get_data()
                # Full instance id means: I got no data for this scheduler
                # so give me all dumbass!
                if 'full_instance_id' in data:
                    c_id = data['full_instance_id']
                    source = elt.source
                    logger.info('The module %s is asking me to get all initial data from the scheduler %d' % (source, c_id))
                    # so we just reset the connection and the running_id, it will just get all new things
                    try:
                        self.schedulers[c_id]['con'] = None
                        self.schedulers[c_id]['running_id'] = 0
                    except KeyError:  # maybe this instance was not known, forget it
                        logger.warning("the module %s ask me a full_instance_id for an unknown ID (%d)!" % (source, c_id))
            # Maybe a module tells me that it's dead, I must log it's last words...
            if elt.get_type() == 'ICrash':
                data = elt.get_data()
                logger.error('the module %s just crash! Please look at the traceback:' % data['name'])
                logger.error(data['trace'])

                # The module death will be looked for elsewhere and restarted.


    # Get the good tabs for links by the kind. If unknown, return None
    def get_links_from_type(self, type):
        t = {'scheduler': self.schedulers, 'arbiter': self.arbiters, \
             'poller': self.pollers, 'reactionner': self.reactionners}
        if type in t:
            return t[type]
        return None



    # Check if we do not connect to often to this
    def is_connection_try_too_close(self, elt):
        now = time.time()
        last_connection = elt['last_connection']
        if now - last_connection < 5:
            return  True
        return False


    # initialize or re-initialize connection with scheduler or
    # arbiter if type == arbiter
    def pynag_con_init(self, id, type='scheduler'):
        # Get the good links tab for looping..
        links = self.get_links_from_type(type)
        if links is None:
            logger.debug('Type unknown for connection! %s' % type)
            return

        if type == 'scheduler':
            # If sched is not active, I do not try to init
            # it is just useless
            is_active = links[id]['active']
            if not is_active:
                return

        # If we try to connect too much, we slow down our tests
        if self.is_connection_try_too_close(links[id]):
            return

        # Ok, we can now update it
        links[id]['last_connection'] = time.time()

        # DBG: print "Init connection with", links[id]['uri']
        running_id = links[id]['running_id']
        # DBG: print "Running id before connection", running_id
        uri = links[id]['uri']
        try:
            con = links[id]['con'] = HTTPClient(uri=uri, strong_ssl=links[id]['hard_ssl_name_check'])
        except HTTPExceptions, exp:
            # But the multiprocessing module is not compatible with it!
            # so we must disable it immediately after
            logger.info("Connection problem to the %s %s: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            return


        try:
            # initial ping must be quick
            con.get('ping')
            new_run_id = con.get('get_running_id')
            new_run_id = float(new_run_id)
            # data transfer can be longer

            # The schedulers have been restarted: it has a new run_id.
            # So we clear all verifs, they are obsolete now.
            if new_run_id != running_id:
                logger.debug("[%s] New running id for the %s %s: %s (was %s)" % (self.name, type, links[id]['name'], new_run_id, running_id))
                links[id]['broks'].clear()
                # we must ask for a new full broks if
                # it's a scheduler
                if type == 'scheduler':
                    logger.debug("[%s] I ask for a broks generation to the scheduler %s" % (self.name, links[id]['name']))
                    con.get('fill_initial_broks', {'bname':self.name}, wait='long')
            # Ok all is done, we can save this new running id
            links[id]['running_id'] = new_run_id
        except HTTPExceptions, exp:
            logger.info("Connection problem to the %s %s: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            return
        except KeyError, exp:
            logger.info("the %s '%s' is not initialized: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            traceback.print_stack()
            return

        logger.info("Connection OK to the %s %s" % (type, links[id]['name']))

    # Get a brok. Our role is to put it in the modules
    # DO NOT CHANGE data of b!!!
    # REF: doc/broker-modules.png (4-5)
    def manage_brok(self, b):
        # Call all modules if they catch the call
        for mod in self.modules_manager.get_internal_instances():
            try:
                mod.manage_brok(b)
            except Exception, exp:
                logger.debug(str(exp.__dict__))
                logger.warning("The mod %s raise an exception: %s, I'm tagging it to restart later" % (mod.get_name(), str(exp)))
                logger.warning("Exception type: %s" % type(exp))
                logger.warning("Back trace of this kill: %s" % (traceback.format_exc()))
                self.modules_manager.set_to_restart(mod)


    # Add broks (a tab) to different queues for
    # internal and external modules
    def add_broks_to_queue(self, broks):
        # Ok now put in queue broks to be managed by
        # internal modules
        self.broks.extend(broks)


    # Each turn we get all broks from
    # self.broks_internal_raised and we put them in
    # self.broks
    def interger_internal_broks(self):
        self.add_broks_to_queue(self.broks_internal_raised)
        self.broks_internal_raised = []


    # We will get in the broks list the broks from the arbiters,
    # but as the arbiter_broks list can be push by arbiter without Global lock,
    # we must protect this with he list lock
    def interger_arbiter_broks(self):
        with self.arbiter_broks_lock:
            self.add_broks_to_queue(self.arbiter_broks)
            self.arbiter_broks = []


    # Get 'objects' from external modules
    # right now on nobody uses it, but it can be useful
    # for modules like livestatus to raise external
    # commands for example
    def get_objects_from_from_queues(self):
        for f in self.modules_manager.get_external_from_queues():
            full_queue = True
            while full_queue:
                try:
                    o = f.get(block=False)
                    self.add(o)
                except Empty:
                    full_queue = False


    # We get new broks from schedulers
    # REF: doc/broker-modules.png (2)
    def get_new_broks(self, type='scheduler'):
        # Get the good links tab for looping..
        links = self.get_links_from_type(type)
        if links is None:
            logger.debug('Type unknown for connection! %s' % type)
            return

        # We check for new check in each schedulers and put
        # the result in new_checks
        for sched_id in links:
            try:
                con = links[sched_id]['con']
                if con is not None:  # None = not initialized
                    t0 = time.time()
                    # Before ask a call that can be long, do a simple ping to be sure it is alive
                    con.get('ping')
                    tmp_broks = con.get('get_broks', {'bname':self.name}, wait='long')
                    try:
                        _t = base64.b64decode(tmp_broks)
                        _t = zlib.decompress(_t)
                        tmp_broks = cPickle.loads(_t)
                    except (TypeError, zlib.error, cPickle.PickleError), exp:
                        logger.error('Cannot load broks data from %s : %s' % (links[sched_id]['name'], exp))
                        links[sched_id]['con'] = None
                        continue
                    logger.debug("%s Broks get in %s" % (len(tmp_broks), time.time() - t0))
                    for b in tmp_broks.values():
                        b.instance_id = links[sched_id]['instance_id']

                    # Ok, we can add theses broks to our queues
                    self.add_broks_to_queue(tmp_broks.values())

                else:  # no con? make the connection
                    self.pynag_con_init(sched_id, type=type)
            # Ok, con is not known, so we create it
            except KeyError, exp:
                logger.debug("Key error for get_broks : %s" % str(exp))
                self.pynag_con_init(sched_id, type=type)
            except HTTPExceptions, exp:
                logger.warning("Connection problem to the %s %s: %s" % (type, links[sched_id]['name'], str(exp)))
                links[sched_id]['con'] = None
            # scheduler must not #be initialized
            except AttributeError, exp:
                logger.warning("The %s %s should not be initialized: %s" % (type, links[sched_id]['name'], str(exp)))
            # scheduler must not have checks
            #  What the F**k? We do not know what happened,
            # so.. bye bye :)
            except Exception, x:
                logger.error(str(x))
                logger.error(traceback.format_exc())
                sys.exit(1)


    # Helper function for module, will give our broks
    def get_retention_data(self):
        return self.broks


    # Get back our broks from a retention module
    def restore_retention_data(self, data):
        self.broks.extend(data)


    def do_stop(self):
        act = active_children()
        for a in act:
            a.terminate()
            a.join(1)
        super(Broker, self).do_stop()


    def setup_new_conf(self):
        conf = self.new_conf
        self.new_conf = None
        self.cur_conf = conf
        # Got our name from the globals
        g_conf = conf['global']
        if 'broker_name' in g_conf:
            name = g_conf['broker_name']
        else:
            name = 'Unnamed broker'
        self.name = name
        logger.load_obj(self, name)

        logger.debug("[%s] Sending us configuration %s" % (self.name, conf))
        # If we've got something in the schedulers, we do not
        # want it anymore
        # self.schedulers.clear()
        for sched_id in conf['schedulers']:
            # Must look if we already have it to do not overdie our broks
            already_got = False

            # We can already got this conf id, but with another address
            if sched_id in self.schedulers:
                new_addr = conf['schedulers'][sched_id]['address']
                old_addr = self.schedulers[sched_id]['address']
                new_port = conf['schedulers'][sched_id]['port']
                old_port = self.schedulers[sched_id]['port']
                # Should got all the same to be ok :)
                if new_addr == old_addr and new_port == old_port:
                    already_got = True

            if already_got:
                broks = self.schedulers[sched_id]['broks']
                running_id = self.schedulers[sched_id]['running_id']
            else:
                broks = {}
                running_id = 0
            s = conf['schedulers'][sched_id]
            self.schedulers[sched_id] = s

            # replacing scheduler address and port by those defined in satellitemap
            if s['name'] in g_conf['satellitemap']:
                s = dict(s)  # make a copy
                s.update(g_conf['satellitemap'][s['name']])
            proto = 'http'
            if s['use_ssl']:
                 proto = 'https'
            uri = '%s://%s:%s/' % (proto, s['address'], s['port'])
            self.schedulers[sched_id]['uri'] = uri

            self.schedulers[sched_id]['broks'] = broks
            self.schedulers[sched_id]['instance_id'] = s['instance_id']
            self.schedulers[sched_id]['running_id'] = running_id
            self.schedulers[sched_id]['active'] = s['active']
            self.schedulers[sched_id]['last_connection'] = 0

        logger.info("We have our schedulers: %s " % self.schedulers)

        # Now get arbiter
        for arb_id in conf['arbiters']:
            # Must look if we already have it
            already_got = arb_id in self.arbiters
            if already_got:
                broks = self.arbiters[arb_id]['broks']
            else:
                broks = {}
            a = conf['arbiters'][arb_id]
            self.arbiters[arb_id] = a

            # replacing arbiter address and port by those defined in satellitemap
            if a['name'] in g_conf['satellitemap']:
                a = dict(a)  # make a copy
                a.update(g_conf['satellitemap'][a['name']])

            proto = 'http'
            if a['use_ssl']:
                 proto = 'https'
            uri = '%s://%s:%s/' % (proto, a['address'], a['port'])
            self.arbiters[arb_id]['uri'] = uri

            self.arbiters[arb_id]['broks'] = broks
            self.arbiters[arb_id]['instance_id'] = 0  # No use so all to 0
            self.arbiters[arb_id]['running_id'] = 0
            self.arbiters[arb_id]['last_connection'] = 0

            # We do not connect to the arbiter. Connection hangs

        logger.info("We have our arbiters: %s " % self.arbiters)

        # Now for pollers
        for pol_id in conf['pollers']:
            # Must look if we already have it
            already_got = pol_id in self.pollers
            if already_got:
                broks = self.pollers[pol_id]['broks']
                running_id = self.schedulers[sched_id]['running_id']
            else:
                broks = {}
                running_id = 0
            p = conf['pollers'][pol_id]
            self.pollers[pol_id] = p

            # replacing poller address and port by those defined in satellitemap
            if p['name'] in g_conf['satellitemap']:
                p = dict(p)  # make a copy
                p.update(g_conf['satellitemap'][p['name']])

            proto = 'http'
            if p['use_ssl']:
                 proto = 'https'

            uri = '%s://%s:%s/' % (proto, p['address'], p['port'])
            self.pollers[pol_id]['uri'] = uri

            self.pollers[pol_id]['broks'] = broks
            self.pollers[pol_id]['instance_id'] = 0  # No use so all to 0
            self.pollers[pol_id]['running_id'] = running_id
            self.pollers[pol_id]['last_connection'] = 0

#                    #And we connect to it
#                    self.app.pynag_con_init(pol_id, 'poller')

        logger.info("We have our pollers: %s" % self.pollers)

        # Now reactionners
        for rea_id in conf['reactionners']:
            # Must look if we already have it
            already_got = rea_id in self.reactionners
            if already_got:
                broks = self.reactionners[rea_id]['broks']
                running_id = self.schedulers[sched_id]['running_id']
            else:
                broks = {}
                running_id = 0

            r = conf['reactionners'][rea_id]
            self.reactionners[rea_id] = r

            # replacing reactionner address and port by those defined in satellitemap
            if r['name'] in g_conf['satellitemap']:
                r = dict(r)  # make a copy
                r.update(g_conf['satellitemap'][r['name']])

            proto = 'http'
            if r['use_ssl']:
                 proto = 'https'
            uri = '%s://%s:%s/' % (proto, r['address'], r['port'])
            self.reactionners[rea_id]['uri'] = uri

            self.reactionners[rea_id]['broks'] = broks
            self.reactionners[rea_id]['instance_id'] = 0  # No use so all to 0
            self.reactionners[rea_id]['running_id'] = running_id
            self.reactionners[rea_id]['last_connection'] = 0

#                    #And we connect to it
#                    self.app.pynag_con_init(rea_id, 'reactionner')

        logger.info("We have our reactionners: %s" % self.reactionners)

        if not self.have_modules:
            self.modules = mods = conf['global']['modules']
            self.have_modules = True
            logger.info("We received modules %s " % mods)

            # Ok now start, or restart them!
            # Set modules, init them and start external ones
            self.modules_manager.set_modules(self.modules)
            self.do_load_modules()
            self.modules_manager.start_external_instances()



        # Set our giving timezone from arbiter
        use_timezone = conf['global']['use_timezone']
        if use_timezone != 'NOTSET':
            logger.info("Setting our timezone to %s" % use_timezone)
            os.environ['TZ'] = use_timezone
            time.tzset()

        # Connection init with Schedulers
        for sched_id in self.schedulers:
            self.pynag_con_init(sched_id, type='scheduler')

        for pol_id in self.pollers:
            self.pynag_con_init(pol_id, type='poller')

        for rea_id in self.reactionners:
            self.pynag_con_init(rea_id, type='reactionner')

    # An arbiter ask us to wait for a new conf, so we must clean
    # all our mess we did, and close modules too
    def clean_previous_run(self):
        # Clean all lists
        self.schedulers.clear()
        self.pollers.clear()
        self.reactionners.clear()
        self.broks = self.broks[:]
        self.broks_internal_raised = self.broks_internal_raised[:]
        with self.arbiter_broks_lock:
            self.arbiter_broks = self.arbiter_broks[:]
        self.external_commands = self.external_commands[:]

        # And now modules
        self.have_modules = False
        self.modules_manager.clear_instances()


    def do_loop_turn(self):
        logger.debug("Begin Loop: managing old broks (%d)" % len(self.broks))

        # Dump modules Queues size
        insts = [inst for inst in self.modules_manager.instances if inst.is_external]
        for inst in insts:
            try:
                logger.debug("External Queue len (%s): %s" % (inst.get_name(), inst.to_q.qsize()))
            except Exception, exp:
                logger.debug("External Queue len (%s): Exception! %s" % (inst.get_name(), exp))

        # Begin to clean modules
        self.check_and_del_zombie_modules()

        # Maybe the arbiter ask us to wait for a new conf
        # If true, we must restart all...
        if self.cur_conf is None:
            # Clean previous run from useless objects
            # and close modules
            self.clean_previous_run()

            self.wait_for_initial_conf()
            # we may have been interrupted or so; then
            # just return from this loop turn
            if not self.new_conf:
                return
            self.setup_new_conf()

        # Now we check if arbiter speak to us in the pyro_daemon.
        # If so, we listen for it
        # When it pushes conf to us, we reinit connections
        self.watch_for_new_conf(0.0)
        if self.new_conf:
            self.setup_new_conf()

        # Maybe the last loop we raised some broks internally
        # we should integrate them in broks
        self.interger_internal_broks()
        # Also reap broks sent from the arbiters
        self.interger_arbiter_broks()
        
        # And from schedulers
        self.get_new_broks(type='scheduler')
        # And for other satellites
        self.get_new_broks(type='poller')
        self.get_new_broks(type='reactionner')

        # Sort the brok list by id
        self.broks.sort(sort_by_ids)

        # and for external queues
        # REF: doc/broker-modules.png (3)
        # We put to external queues broks that was not already send
        t0 = time.time()
        # We are sending broks as a big list, more efficient than one by one
        queues = self.modules_manager.get_external_to_queues()
        to_send = [b for b in self.broks if getattr(b, 'need_send_to_ext', True)]

        for q in queues:
            q.put(to_send)

        # No more need to send them
        for b in to_send:
            b.need_send_to_ext = False
        logger.debug("Time to send %s broks (%d secs)" % (len(to_send), time.time() - t0))

        # We must had new broks at the end of the list, so we reverse the list
        self.broks.reverse()

        start = time.time()
        while len(self.broks) != 0:
            now = time.time()
            # Do not 'manage' more than 1s, we must get new broks
            # every 1s
            if now - start > 1:
                break

            b = self.broks.pop()
            # Ok, we can get the brok, and doing something with it
            # REF: doc/broker-modules.png (4-5)
            # We un serialize the brok before consume it
            b.prepare()
            self.manage_brok(b)

            nb_broks = len(self.broks)

            # Ok we manage brok, but we still want to listen to arbiter
            self.watch_for_new_conf(0.0)

            # if we got new broks here from arbiter, we should break the loop
            # because such broks will not be managed by the
            # external modules before this loop (we pop them!)
            if len(self.broks) != nb_broks:
                break

        # Maybe external modules raised 'objects'
        # we should get them
        self.get_objects_from_from_queues()

        # Maybe we do not have something to do, so we wait a little
        # TODO: redone the diff management....
        if len(self.broks) == 0:
            while self.timeout > 0:
                begin = time.time()
                self.watch_for_new_conf(1.0)
                end = time.time()
                self.timeout = self.timeout - (end - begin)
            self.timeout = 1.0

            # print "get new broks watch new conf 1: end", len(self.broks)

        # Say to modules it's a new tick :)
        self.hook_point('tick')

    #  Main function, will loop forever
    def main(self):
        try:
            self.load_config_file()

            for line in self.get_header():
                logger.info(line)

            logger.info("[Broker] Using working directory: %s" % os.path.abspath(self.workdir))

            # Look if we are enabled or not. If ok, start the daemon mode
            self.look_for_early_exit()
            self.do_daemon_init_and_start()
            self.load_modules_manager()
            
            self.uri2 = self.http_daemon.register(self.interface)#, "ForArbiter")
            logger.debug("The Arbiter uri it at %s" % self.uri2)

            self.uri3 = self.http_daemon.register(self.istats)

            #  We wait for initial conf
            self.wait_for_initial_conf()
            if not self.new_conf:
                return

            self.setup_new_conf()

            # We already init modules during the new conf thing
            # Set modules, init them and start external ones
            #self.modules_manager.set_modules(self.modules)
            #self.do_load_modules()
            #self.modules_manager.start_external_instances()



            # Do the modules part, we have our modules in self.modules
            # REF: doc/broker-modules.png (1)
            self.hook_point('load_retention')

            # Now the main loop
            self.do_mainloop()

        except Exception, exp:
            logger.critical("I got an unrecoverable error. I have to exit")
            logger.critical("You can log a bug ticket at https://github.com/naparuba/shinken/issues/new to get help")
            logger.critical("Back trace of it: %s" % (traceback.format_exc()))
            raise

########NEW FILE########
__FILENAME__ = pollerdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.satellite import Satellite
from shinken.property import PathProp, IntegerProp


# Our main APP class
class Poller(Satellite):
    do_checks = True    # I do checks
    do_actions = False  # but no actions

    properties = Satellite.properties.copy()
    properties.update({
        'pidfile':   PathProp(default='pollerd.pid'),
        'port':      IntegerProp(default='7771'),
        'local_log': PathProp(default='pollerd.log'),
    })

    def __init__(self, config_file, is_daemon, do_replace, debug, debug_file, profile):
        super(Poller, self).__init__('poller', config_file, is_daemon, do_replace, debug, debug_file)

########NEW FILE########
__FILENAME__ = reactionnerdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.



# This class is an application that launches actions for the schedulers
# Actions can be:
#    Notifications
#    Event handlers
#
# When running the Reactionner will :
#   Respond to Pyro pings from Arbiter
#   Listen for new configurations from Arbiter
#
# The configuration consists of a list of Schedulers for which
# the Reactionner will launch actions for.

from shinken.satellite import Satellite
from shinken.property import PathProp, IntegerProp


class Reactionner(Satellite):
    do_checks = False  # I do not do checks
    do_actions = True

    properties = Satellite.properties.copy()
    properties.update({
        'pidfile':   PathProp(default='reactionnerd.pid'),
        'port':      IntegerProp(default='7769'),
        'local_log': PathProp(default='reactionnerd.log'),
    })

    def __init__(self, config_file, is_daemon, do_replace, debug, debug_file, profile=''):
        super(Reactionner, self).__init__('reactionner', config_file, is_daemon, do_replace, debug, debug_file)

########NEW FILE########
__FILENAME__ = receiverdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import time
import traceback
import sys

from multiprocessing import active_children
from Queue import Empty


from shinken.satellite import Satellite
from shinken.property import PathProp, IntegerProp
from shinken.log import logger
from shinken.external_command import ExternalCommand, ExternalCommandManager
from shinken.http_client import HTTPClient, HTTPExceptions
from shinken.daemon import Daemon, Interface

class IStats(Interface):
    """ 
    Interface for various stats about broker activity
    """

    doc = '''Get raw stats from the daemon:
  * command_buffer_size: external command buffer size
'''
    def get_raw_stats(self):
        app = self.app
        res = {'command_buffer_size': len(app.external_commands)}
        return res
    get_raw_stats.doc = doc


# Our main APP class
class Receiver(Satellite):

    properties = Satellite.properties.copy()
    properties.update({
        'pidfile':   PathProp(default='receiverd.pid'),
        'port':      IntegerProp(default='7773'),
        'local_log': PathProp(default='receiverd.log'),
    })

    def __init__(self, config_file, is_daemon, do_replace, debug, debug_file):

        super(Receiver, self).__init__('receiver', config_file, is_daemon, do_replace, debug, debug_file)

        # Our arbiters
        self.arbiters = {}

        # Our pollers and reactionners
        self.pollers = {}
        self.reactionners = {}

        # Modules are load one time
        self.have_modules = False

        # Can have a queue of external_commands give by modules
        # will be taken by arbiter to process
        self.external_commands = []
        # and the unprocessed one, a buffer
        self.unprocessed_external_commands = []

        # All broks to manage
        self.broks = []  # broks to manage
        # broks raised this turn and that need to be put in self.broks
        self.broks_internal_raised = []

        self.host_assoc = {}
        self.direct_routing = False

        self.istats = IStats(self)
        

    # Schedulers have some queues. We can simplify call by adding
    # elements into the proper queue just by looking at their type
    # Brok -> self.broks
    # TODO: better tag ID?
    # External commands -> self.external_commands
    def add(self, elt):
        cls_type = elt.__class__.my_type
        if cls_type == 'brok':
            # For brok, we TAG brok with our instance_id
            elt.instance_id = 0
            self.broks_internal_raised.append(elt)
            return
        elif cls_type == 'externalcommand':
            logger.debug("Enqueuing an external command: %s" % str(ExternalCommand.__dict__))
            self.unprocessed_external_commands.append(elt)


    def push_host_names(self, sched_id, hnames):
        for h in hnames:
            self.host_assoc[h] = sched_id


    def get_sched_from_hname(self, hname):
        i = self.host_assoc.get(hname, None)
        e = self.schedulers.get(i, None)
        return e


    # Get a brok. Our role is to put it in the modules
    # THEY MUST DO NOT CHANGE data of b!!!
    # REF: doc/receiver-modules.png (4-5)
    def manage_brok(self, b):
        to_del = []
        # Call all modules if they catch the call
        for mod in self.modules_manager.get_internal_instances():
            try:
                mod.manage_brok(b)
            except Exception, exp:
                logger.warning("The mod %s raise an exception: %s, I kill it" % (mod.get_name(), str(exp)))
                logger.warning("Exception type: %s" % type(exp))
                logger.warning("Back trace of this kill: %s" % (traceback.format_exc()))
                to_del.append(mod)
        # Now remove mod that raise an exception
        self.modules_manager.clear_instances(to_del)


    # Get 'objects' from external modules
    # from now nobody use it, but it can be useful
    # for a module like livestatus to raise external
    # commands for example
    def get_objects_from_from_queues(self):
        for f in self.modules_manager.get_external_from_queues():
            full_queue = True
            while full_queue:
                try:
                    o = f.get(block=False)
                    self.add(o)
                except Empty:
                    full_queue = False


    def do_stop(self):
        act = active_children()
        for a in act:
            a.terminate()
            a.join(1)
        super(Receiver, self).do_stop()


    def setup_new_conf(self):
        conf = self.new_conf
        self.new_conf = None
        self.cur_conf = conf
        # Got our name from the globals
        if 'receiver_name' in conf['global']:
            name = conf['global']['receiver_name']
        else:
            name = 'Unnamed receiver'
        self.name = name
        logger.load_obj(self, name)
        self.direct_routing = conf['global']['direct_routing']

        g_conf = conf['global']

        # If we've got something in the schedulers, we do not want it anymore
        for sched_id in conf['schedulers']:

            already_got = False

            # We can already got this conf id, but with another address
            if sched_id in self.schedulers:
                new_addr = conf['schedulers'][sched_id]['address']
                old_addr = self.schedulers[sched_id]['address']
                new_port = conf['schedulers'][sched_id]['port']
                old_port = self.schedulers[sched_id]['port']
                # Should got all the same to be ok :)
                if new_addr == old_addr and new_port == old_port:
                    already_got = True

            if already_got:
                logger.info("[%s] We already got the conf %d (%s)" % (self.name, sched_id, conf['schedulers'][sched_id]['name']))
                wait_homerun = self.schedulers[sched_id]['wait_homerun']
                actions = self.schedulers[sched_id]['actions']
                external_commands = self.schedulers[sched_id]['external_commands']
                con = self.schedulers[sched_id]['con']

            s = conf['schedulers'][sched_id]
            self.schedulers[sched_id] = s

            if s['name'] in g_conf['satellitemap']:
                s.update(g_conf['satellitemap'][s['name']])
            uri = 'http://%s:%s/' % (s['address'], s['port'])

            self.schedulers[sched_id]['uri'] = uri
            if already_got:
                self.schedulers[sched_id]['wait_homerun'] = wait_homerun
                self.schedulers[sched_id]['actions'] = actions
                self.schedulers[sched_id]['external_commands'] = external_commands
                self.schedulers[sched_id]['con'] = con
            else:
                self.schedulers[sched_id]['wait_homerun'] = {}
                self.schedulers[sched_id]['actions'] = {}
                self.schedulers[sched_id]['external_commands'] = []
                self.schedulers[sched_id]['con'] = None
            self.schedulers[sched_id]['running_id'] = 0
            self.schedulers[sched_id]['active'] = s['active']

            # Do not connect if we are a passive satellite
            if self.direct_routing and not already_got:
                # And then we connect to it :)
                self.pynag_con_init(sched_id)



        logger.debug("[%s] Sending us configuration %s" % (self.name, conf))

        if not self.have_modules:
            self.modules = mods = conf['global']['modules']
            self.have_modules = True
            logger.info("We received modules %s " % mods)

        # Set our giving timezone from arbiter
        use_timezone = conf['global']['use_timezone']
        if use_timezone != 'NOTSET':
            logger.info("Setting our timezone to %s" % use_timezone)
            os.environ['TZ'] = use_timezone
            time.tzset()


        # Now create the external commander. It's just here to dispatch
        # the commands to schedulers
        e = ExternalCommandManager(None, 'receiver')
        e.load_receiver(self)
        self.external_command = e



    # Take all external commands, make packs and send them to
    # the schedulers
    def push_external_commands_to_schedulers(self):
        # If we are not in a direct routing mode, just bailout after
        # faking resolving the commands
        if not self.direct_routing:
            self.external_commands.extend(self.unprocessed_external_commands)
            self.unprocessed_external_commands = []
            return
        
        # Now get all external commands and put them into the
        # good schedulers
        for ext_cmd in self.unprocessed_external_commands:
            self.external_command.resolve_command(ext_cmd)
            self.external_commands.append(ext_cmd)
        
        # And clean the previous one
        self.unprocessed_external_commands = []
        
        # Now for all alive schedulers, send the commands
        for sched_id in self.schedulers:
            sched = self.schedulers[sched_id]
            extcmds = sched['external_commands']
            cmds = [extcmd.cmd_line for extcmd in extcmds]
            con = sched.get('con', None)
            sent = False
            if not con:
                logger.warning("The scheduler is not connected" % sched)
                self.pynag_con_init(sched_id)
                con = sched.get('con', None)
            
            # If there are commands and the scheduler is alive
            if len(cmds) > 0 and con:
                logger.debug("Sending %d commands to scheduler %s" % (len(cmds), sched))
                try:
                    #con.run_external_commands(cmds)
                    con.post('run_external_commands', {'cmds':cmds})
                    sent = True
                # Not connected or sched is gone
                except (HTTPExceptions, KeyError), exp:
                    logger.debug('manage_returns exception:: %s,%s ' % (type(exp), str(exp)))
                    self.pynag_con_init(sched_id)
                    return
                except AttributeError, exp:  # the scheduler must  not be initialized
                    logger.debug('manage_returns exception:: %s,%s ' % (type(exp), str(exp)))
                except Exception, exp:
                    logger.error("A satellite raised an unknown exception: %s (%s)" % (exp, type(exp)))
                    raise

            # If we sent or not the commands, just clean the scheduler list.
            self.schedulers[sched_id]['external_commands'] = []
            
            # If we sent them, remove the commands of this scheduler of the arbiter list
            if sent:
                # and remove them from the list for the arbiter (if not, we will send it twice
                for extcmd in extcmds:
                    self.external_commands.remove(extcmd)
            

    def do_loop_turn(self):
        sys.stdout.write(".")
        sys.stdout.flush()

        # Begin to clean modules
        self.check_and_del_zombie_modules()

        # Now we check if arbiter speak to us in the pyro_daemon.
        # If so, we listen for it
        # When it push us conf, we reinit connections
        self.watch_for_new_conf(0.0)
        if self.new_conf:
            self.setup_new_conf()

#        # Maybe the last loop we raised some broks internally
#        # we should integrate them in broks
#        self.integer_internal_broks()

#        # And from schedulers
#        self.get_new_broks(type='scheduler')
#        # And for other satellites
#        self.get_new_broks(type='poller')
#        self.get_new_broks(type='reactionner')

#        # Sort the brok list by id
#        self.broks.sort(sort_by_ids)

#        # and for external queues
#        # REF: doc/receiver-modules.png (3)
#        for b in self.broks:
#            # if b.type != 'log':
#            #     print "Receiver: put brok id: %d" % b.id
#            for q in self.modules_manager.get_external_to_queues():
#                q.put(b)

#        # We must had new broks at the end of the list, so we reverse the list
#        self.broks.reverse()

        start = time.time()
        ## while len(self.broks) != 0:
        ##     now = time.time()
        ##     # Do not 'manage' more than 1s, we must get new broks
        ##     # every 1s
        ##     if now - start > 1:
        ##         break
        ##
        ##     b = self.broks.pop()
        ##     # Ok, we can get the brok, and doing something with it
        ##     # REF: doc/receiver-modules.png (4-5)
        ##     self.manage_brok(b)
        ##
        ##     nb_broks = len(self.broks)
        ##
        ##     # Ok we manage brok, but we still want to listen to arbiter
        ##     self.watch_for_new_conf(0.0)
        ##
        ##     # if we got new broks here from arbiter, we should break the loop
        ##     # because such broks will not be managed by the
        ##     # external modules before this loop (we pop them!)
        ##     if len(self.broks) != nb_broks:
        ##         break

        # Maybe external modules raised 'objects'
        # we should get them
        self.get_objects_from_from_queues()

        self.push_external_commands_to_schedulers()

        # Maybe we do not have something to do, so we wait a little
        if len(self.broks) == 0:
            # print "watch new conf 1: begin", len(self.broks)
            self.watch_for_new_conf(1.0)
            # print "get enw broks watch new conf 1: end", len(self.broks)


    #  Main function, will loop forever
    def main(self):
        try:
            self.load_config_file()
            # Look if we are enabled or not. If ok, start the daemon mode
            self.look_for_early_exit()

            for line in self.get_header():
                logger.info(line)

            logger.info("[Receiver] Using working directory: %s" % os.path.abspath(self.workdir))

            self.do_daemon_init_and_start()

            self.load_modules_manager()

            self.uri2 = self.http_daemon.register(self.interface)#, "ForArbiter")
            logger.debug("The Arbiter uri it at %s" % self.uri2)

            self.uri3 = self.http_daemon.register(self.istats)

            #  We wait for initial conf
            self.wait_for_initial_conf()
            if not self.new_conf:
                return

            self.setup_new_conf()

            self.modules_manager.set_modules(self.modules)
            self.do_load_modules()
            # and start external modules too
            self.modules_manager.start_external_instances()

            # Do the modules part, we have our modules in self.modules
            # REF: doc/receiver-modules.png (1)


            # Now the main loop
            self.do_mainloop()

        except Exception, exp:
            logger.critical("I got an unrecoverable error. I have to exit")
            logger.critical("You can log a bug ticket at https://github.com/naparuba/shinken/issues/new to get help")
            logger.critical("Back trace of it: %s" % (traceback.format_exc()))
            raise

########NEW FILE########
__FILENAME__ = schedulerdaemon
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import signal
import time
import traceback
import cPickle
import zlib
import base64

from shinken.scheduler import Scheduler
from shinken.macroresolver import MacroResolver
from shinken.external_command import ExternalCommandManager
from shinken.daemon import Daemon
from shinken.property import PathProp, IntegerProp
from shinken.log import logger
from shinken.satellite import BaseSatellite, IForArbiter as IArb, Interface
import shinken.objects.config
from shinken.util import nighty_five_percent

# Interface for Workers

class IChecks(Interface):
    """ Interface for Workers:
They connect here and see if they are still OK with our running_id, if not, they must drop their checks """

    # poller or reactionner is asking us our running_id
    #def get_running_id(self):
    #    return self.running_id

    # poller or reactionner ask us actions
    def get_checks(self, do_checks=False, do_actions=False, poller_tags=['None'], \
                       reactionner_tags=['None'], worker_name='none', \
                       module_types=['fork']):
        #print "We ask us checks"
        do_checks = (do_checks == 'True')
        do_actions = (do_actions == 'True')
        res = self.app.get_to_run_checks(do_checks, do_actions, poller_tags, reactionner_tags, worker_name, module_types)
        #print "Sending %d checks" % len(res)
        self.app.nb_checks_send += len(res)

        return base64.b64encode(zlib.compress(cPickle.dumps(res), 2))
        #return zlib.compress(cPickle.dumps(res), 2)
    get_checks.encode = 'raw'


    # poller or reactionner are putting us results
    def put_results(self, results):
        nb_received = len(results)
        self.app.nb_check_received += nb_received
        if nb_received != 0:
            logger.debug("Received %d results" % nb_received)
        for result in results:
            result.set_type_active()
        self.app.waiting_results.extend(results)

        #for c in results:
        #self.sched.put_results(c)
        return True
    put_results.method = 'post'


class IBroks(Interface):
    """ Interface for Brokers:
They connect here and get all broks (data for brokers). Data must be ORDERED! (initial status BEFORE update...) """

    # A broker ask us broks
    def get_broks(self, bname):
        # Maybe it was not registered as it should, if so,
        # do it for it
        if not bname in self.app.brokers:
            self.fill_initial_broks(bname)

        # Now get the broks for this specific broker
        res = self.app.get_broks(bname)
        # got only one global counter for broks
        self.app.nb_broks_send += len(res)
        # we do not more have a full broks in queue
        self.app.brokers[bname]['has_full_broks'] = False
        return base64.b64encode(zlib.compress(cPickle.dumps(res), 2))
        #return zlib.compress(cPickle.dumps(res), 2)
    get_broks.encode = 'raw'


    # A broker is a new one, if we do not have
    # a full broks, we clean our broks, and
    # fill it with all new values
    def fill_initial_broks(self, bname):
        if bname not in self.app.brokers:
            logger.info("A new broker just connected : %s" % bname)
            self.app.brokers[bname] = {'broks' : {}, 'has_full_broks' : False}
        e = self.app.brokers[bname]
        if not e['has_full_broks']:
            e['broks'].clear()
            self.app.fill_initial_broks(bname, with_logs=True)


class IStats(Interface):
    """ 
    Interface for various stats about scheduler activity
    """

    doc = '''Get raw stats from the daemon:
  * nb_scheduled: number of scheduled checks (to launch in the future)
  * nb_inpoller: number of check take by the pollers
  * nb_zombies: number of zombie checks (should be close to zero)
  * nb_notifications: number of notifications+event handlers
  * latency: avg,min,max latency for the services (should be <10s)
'''
    def get_raw_stats(self):
        sched = self.app.sched
        res = {}
        res['nb_scheduled'] = len([c for c in sched.checks.values() if c.status == 'scheduled'])
        res['nb_inpoller']  = len([c for c in sched.checks.values() if c.status == 'inpoller'])
        res['nb_zombies']   = len([c for c in sched.checks.values() if c.status == 'zombie'])
        res['nb_notifications'] = len(sched.actions)
        
        # Get a overview of the latencies with just
        # a 95 percentile view, but lso min/max values
        latencies = [s.latency for s in sched.services]
        lat_avg, lat_min, lat_max = nighty_five_percent(latencies)
        res['latency'] = (0.0,0.0,0.0)
        if lat_avg:
            res['latency'] = (lat_avg, lat_min, lat_max)
        return res
    get_raw_stats.doc = doc



class IForArbiter(IArb):
    """ Interface for Arbiter. We ask him a for a conf and after that listen for instructions
        from the arbiter. The arbiter is the interface to the administrator, so we must listen
        carefully and give him the information he wants. Which could be for another scheduler """

    # arbiter is sending us a external command.
    # it can send us global command, or specific ones
    def run_external_commands(self, cmds):
        self.app.sched.run_external_commands(cmds)
    run_external_commands.method = 'POST'


    def put_conf(self, conf):
        self.app.sched.die()
        super(IForArbiter, self).put_conf(conf)
    put_conf.method = 'POST'


    # Call by arbiter if it thinks we are running but we must not (like
    # if I was a spare that take a conf but the master returns, I must die
    # and wait for a new conf)
    # Us: No please...
    # Arbiter: I don't care, hasta la vista baby!
    # Us: ... <- Nothing! We are dead! you didn't follow or what??
    def wait_new_conf(self):
        logger.debug("Arbiter wants me to wait for a new configuration")
        self.app.sched.die()
        super(IForArbiter, self).wait_new_conf()



'''
class Injector(Interface):
    # A broker ask us broks
    def inject(self, bincode):
        
        # first we need to get a real code object
        import marshal
        print "Calling Inject mode"
        code = marshal.loads(bincode)
        result = None
        exec code
        try:
            return result
        except NameError, exp:
            return None
'''



# The main app class
class Shinken(BaseSatellite):

    properties = BaseSatellite.properties.copy()
    properties.update({
        'pidfile':   PathProp(default='schedulerd.pid'),
        'port':      IntegerProp(default='7768'),
        'local_log': PathProp(default='schedulerd.log'),
    })

    # Create the shinken class:
    # Create a Pyro server (port = arvg 1)
    # then create the interface for arbiter
    # Then, it wait for a first configuration
    def __init__(self, config_file, is_daemon, do_replace, debug, debug_file, profile=''):

        BaseSatellite.__init__(self, 'scheduler', config_file, is_daemon, do_replace, debug, debug_file)

        self.interface = IForArbiter(self)
        self.istats = IStats(self)
        self.sched = Scheduler(self)

        self.ichecks = None
        self.ibroks = None
        self.must_run = True

        # Now the interface
        self.uri = None
        self.uri2 = None

        # And possible links for satellites
        # from now only pollers
        self.pollers = {}
        self.reactionners = {}
        self.brokers = {}


    def do_stop(self):
        if self.http_daemon:
            if self.ibroks:
                self.http_daemon.unregister(self.ibroks)
            if self.ichecks:
                self.http_daemon.unregister(self.ichecks)
        super(Shinken, self).do_stop()


    def compensate_system_time_change(self, difference):
        """ Compensate a system time change of difference for all hosts/services/checks/notifs """
        logger.warning("A system time change of %d has been detected. Compensating..." % difference)
        # We only need to change some value
        self.program_start = max(0, self.program_start + difference)

        # Then we compensate all host/services
        for h in self.sched.hosts:
            h.compensate_system_time_change(difference)
        for s in self.sched.services:
            s.compensate_system_time_change(difference)

        # Now all checks and actions
        for c in self.sched.checks.values():
            # Already launch checks should not be touch
            if c.status == 'scheduled' and c.t_to_go is not None:
                t_to_go = c.t_to_go
                ref = c.ref
                new_t = max(0, t_to_go + difference)
                if ref.check_period is not None:
                    # But it's no so simple, we must match the timeperiod
                    new_t = ref.check_period.get_next_valid_time_from_t(new_t)
                # But maybe no there is no more new value! Not good :(
                # Say as error, with error output
                if new_t is None:
                    c.state = 'waitconsume'
                    c.exit_status = 2
                    c.output = '(Error: there is no available check time after time change!)'
                    c.check_time = time.time()
                    c.execution_time = 0
                else:
                    c.t_to_go = new_t
                    ref.next_chk = new_t

        # Now all checks and actions
        for c in self.sched.actions.values():
            # Already launch checks should not be touch
            if c.status == 'scheduled':
                t_to_go = c.t_to_go

                #  Event handler do not have ref
                ref = getattr(c, 'ref', None)
                new_t = max(0, t_to_go + difference)

                # Notification should be check with notification_period
                if c.is_a == 'notification':
                    if ref.notification_period:
                        # But it's no so simple, we must match the timeperiod
                        new_t = ref.notification_period.get_next_valid_time_from_t(new_t)
                    # And got a creation_time variable too
                    c.creation_time = c.creation_time + difference

                # But maybe no there is no more new value! Not good :(
                # Say as error, with error output
                if new_t is None:
                    c.state = 'waitconsume'
                    c.exit_status = 2
                    c.output = '(Error: there is no available check time after time change!)'
                    c.check_time = time.time()
                    c.execution_time = 0
                else:
                    c.t_to_go = new_t

    def manage_signal(self, sig, frame):
        logger.warning("Received a SIGNAL %s" % sig)
        # If we got USR1, just dump memory
        if sig == signal.SIGUSR1:
            self.sched.need_dump_memory = True
        elif sig == signal.SIGUSR2: #usr2, dump objects
            self.sched.need_objects_dump = True
        else:  # if not, die :)
            self.sched.die()
            self.must_run = False
            Daemon.manage_signal(self, sig, frame)


    def do_loop_turn(self):
        # Ok, now the conf
        self.wait_for_initial_conf()
        if not self.new_conf:
            return
        logger.info("New configuration received")
        self.setup_new_conf()
        logger.info("New configuration loaded")
        self.sched.run()


    def setup_new_conf(self):
        pk = self.new_conf
        conf_raw = pk['conf']
        override_conf = pk['override_conf']
        modules = pk['modules']
        satellites = pk['satellites']
        instance_name = pk['instance_name']
        push_flavor = pk['push_flavor']
        skip_initial_broks = pk['skip_initial_broks']

        t0 = time.time()
        conf = cPickle.loads(conf_raw)
        logger.debug("Conf received at %d. Unserialized in %d secs" % (t0, time.time() - t0))

        self.new_conf = None

        # Tag the conf with our data
        self.conf = conf
        self.conf.push_flavor = push_flavor
        self.conf.instance_name = instance_name
        self.conf.skip_initial_broks = skip_initial_broks

        self.cur_conf = conf
        self.override_conf = override_conf
        self.modules = modules
        self.satellites = satellites
        #self.pollers = self.app.pollers

        if self.conf.human_timestamp_log:
            logger.set_human_format()

        # Now We create our pollers
        for pol_id in satellites['pollers']:
            # Must look if we already have it
            already_got = pol_id in self.pollers
            p = satellites['pollers'][pol_id]
            self.pollers[pol_id] = p

            if p['name'] in override_conf['satellitemap']:
                p = dict(p)  # make a copy
                p.update(override_conf['satellitemap'][p['name']])

            uri = 'http://%s:%s/' % (p['address'], p['port'])
            self.pollers[pol_id]['uri'] = uri
            self.pollers[pol_id]['last_connection'] = 0

        # First mix conf and override_conf to have our definitive conf
        for prop in self.override_conf:
            #print "Overriding the property %s with value %s" % (prop, self.override_conf[prop])
            val = self.override_conf[prop]
            setattr(self.conf, prop, val)

        if self.conf.use_timezone != '':
            logger.debug("Setting our timezone to %s" % str(self.conf.use_timezone))
            os.environ['TZ'] = self.conf.use_timezone
            time.tzset()

        if len(self.modules) != 0:
            logger.debug("I've got %s modules" % str(self.modules))

        # TODO: if scheduler had previous modules instanciated it must clean them!
        self.modules_manager.set_modules(self.modules)
        self.do_load_modules()

        # give it an interface
        # But first remove previous interface if exists
        if self.ichecks is not None:
            logger.debug("Deconnecting previous Check Interface")
            self.http_daemon.unregister(self.ichecks)
        # Now create and connect it
        self.ichecks = IChecks(self.sched)
        self.http_daemon.register(self.ichecks)
        logger.debug("The Scheduler Interface uri is: %s" % self.uri)
        
        # Same for Broks
        if self.ibroks is not None:
            logger.debug("Deconnecting previous Broks Interface")
            self.http_daemon.unregister(self.ibroks)
        # Create and connect it
        self.ibroks = IBroks(self.sched)
        self.http_daemon.register(self.ibroks)

        logger.info("Loading configuration.")
        self.conf.explode_global_conf()

        # we give sched it's conf
        self.sched.reset()
        self.sched.load_conf(self.conf)
        self.sched.load_satellites(self.pollers, self.reactionners)

        # We must update our Config dict macro with good value
        # from the config parameters
        self.sched.conf.fill_resource_macros_names_macros()
        #print "DBG: got macros", self.sched.conf.macros

        # Creating the Macroresolver Class & unique instance
        m = MacroResolver()
        m.init(self.conf)
        
        #self.conf.dump()
        #self.conf.quick_debug()
        
        # Now create the external commander
        # it's a applyer: it role is not to dispatch commands,
        # but to apply them
        e = ExternalCommandManager(self.conf, 'applyer')
        
        # Scheduler need to know about external command to
        # activate it if necessary
        self.sched.load_external_command(e)

        # External command need the sched because he can raise checks
        e.load_scheduler(self.sched)

        # We clear our schedulers managed (it's us :) )
        # and set ourself in it
        self.schedulers = {self.conf.instance_id: self.sched}


    # Give the arbiter the data about what I manage
    # for me it's just my instance_id and my push flavor
    def what_i_managed(self):
        if hasattr(self, 'conf'):
            return {self.conf.instance_id: self.conf.push_flavor}
        else:
            return {}

    # our main function, launch after the init
    def main(self):
        try:
            self.load_config_file()
            self.look_for_early_exit()
            self.do_daemon_init_and_start()
            self.load_modules_manager()
            self.http_daemon.register(self.interface)
            self.http_daemon.register(self.istats)

            #self.inject = Injector(self.sched)
            #self.http_daemon.register(self.inject)

            self.http_daemon.unregister(self.interface)
            self.uri = self.http_daemon.uri
            logger.info("[scheduler] General interface is at: %s" % self.uri)
            self.do_mainloop()
        except Exception, exp:
            logger.critical("I got an unrecoverable error. I have to exit")
            logger.critical("You can log a bug ticket at https://github.com/naparuba/shinken/issues/new to get help")
            logger.critical("Back trace of it: %s" % (traceback.format_exc()))
            raise

########NEW FILE########
__FILENAME__ = daterange
#!/usr/bin/env python

# -*- coding: utf-8 -*-


# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time, calendar

from shinken.util import get_sec_from_morning, get_day, get_start_of_day, get_end_of_day
from shinken.log import logger

# Get the day number (like 27 in July Tuesday 27 2010 for call:
# 2010, July, Tuesday, -1 (last Tuesday of July 2010)
def find_day_by_weekday_offset(year, month, weekday, offset):
    # get the id of the weekday (1 for Tuesday)
    weekday_id = Daterange.get_weekday_id(weekday)
    if weekday_id is None:
        return None

    # same for month
    month_id = Daterange.get_month_id(month)
    if month_id is None:
        return None

    # thanks calendar :)
    cal = calendar.monthcalendar(year, month_id)

    # If we ask for a -1 day, just reverse cal
    if offset < 0:
        offset = abs(offset)
        cal.reverse()

    # ok go for it
    nb_found = 0
    try:
        for i in xrange(0, offset + 1):
            # in cal 0 mean "there are no day here :)"
            if cal[i][weekday_id] != 0:
                nb_found += 1
            if nb_found == offset:
                return cal[i][weekday_id]
        return None
    except:
        return None


def find_day_by_offset(year, month, offset):
    month_id = Daterange.get_month_id(month)
    if month_id is None:
        return None
    (tmp, days_in_month) = calendar.monthrange(year, month_id)
    if offset >= 0:
        return min(offset, days_in_month)
    else:
        return max(1, days_in_month + offset + 1)


class Timerange:

    # entry is like 00:00-24:00
    def __init__(self, entry):
        entries = entry.split('-')
        start = entries[0]
        end = entries[1]
        sentries = start.split(':')
        self.hstart = int(sentries[0])
        self.mstart = int(sentries[1])
        eentries = end.split(':')
        self.hend = int(eentries[0])
        self.mend = int(eentries[1])

    def __str__(self):
        return str(self.__dict__)

    def get_sec_from_morning(self):
        return self.hstart*3600 + self.mstart*60

    def get_first_sec_out_from_morning(self):
        # If start at 0:0, the min out is the end
        if self.hstart == 0 and self.mstart == 0:
            return self.hend*3600 + self.mend*60
        return 0

    def is_time_valid(self, t):
        sec_from_morning = get_sec_from_morning(t)
        return self.hstart*3600 + self.mstart* 60  <= sec_from_morning <= self.hend*3600 + self.mend* 60


""" TODO: Add some comment about this class for the doc"""
class Daterange:
    weekdays = {'monday': 0, 'tuesday': 1, 'wednesday': 2, 'thursday': 3, \
                    'friday': 4, 'saturday': 5, 'sunday': 6}
    months = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, \
                  'june': 6, 'july': 7, 'august': 8, 'september': 9, \
                  'october': 10, 'november': 11, 'december': 12}

    def __init__(self, syear, smon, smday, swday, swday_offset,
                 eyear, emon, emday, ewday, ewday_offset, skip_interval, other):
        self.syear = int(syear)
        self.smon = smon
        self.smday = int(smday)
        self.swday = swday
        self.swday_offset = int(swday_offset)
        self.eyear = int(eyear)
        self.emon = emon
        self.emday = int(emday)
        self.ewday = ewday
        self.ewday_offset = int(ewday_offset)
        self.skip_interval = int(skip_interval)
        self.other = other
        self.timeranges = []

        for timeinterval in other.split(','):
            self.timeranges.append(Timerange(timeinterval.strip()))

    def __str__(self):
        return '' # str(self.__dict__)

    # By default, daterange are correct
    def is_correct(self):
        return True

    def get_month_id(cls, month):
        try:
            return Daterange.months[month]
        except:
            return None
    get_month_id = classmethod(get_month_id)

    # @memoized
    def get_month_by_id(cls, id):
        id = id % 12
        for key in Daterange.months:
            if id == Daterange.months[key]:
                return key
        return None
    get_month_by_id = classmethod(get_month_by_id)

    def get_weekday_id(cls, weekday):
        try:
            return Daterange.weekdays[weekday]
        except:
            return None
    get_weekday_id = classmethod(get_weekday_id)

    def get_weekday_by_id(cls, id):
        id = id % 7
        for key in Daterange.weekdays:
            if id == Daterange.weekdays[key]:
                return key
        return None
    get_weekday_by_id = classmethod(get_weekday_by_id)

    def get_start_and_end_time(self, ref=None):
        logger.warning("Calling function get_start_and_end_time which is not implemented")

    def is_time_valid(self, t):
        #print "****Look for time valid for", time.asctime(time.localtime(t))
        if self.is_time_day_valid(t):
            #print "is time day valid"
            for tr in self.timeranges:
                #print tr, "is valid?", tr.is_time_valid(t)
                if tr.is_time_valid(t):
                    #print "return True"
                    return True
        return False

    def get_min_sec_from_morning(self):
        mins = []
        for tr in self.timeranges:
            mins.append(tr.get_sec_from_morning())
        return min(mins)

    def get_min_sec_out_from_morning(self):
        mins = []
        for tr in self.timeranges:
            mins.append(tr.get_first_sec_out_from_morning())
        return min(mins)

    def get_min_from_t(self, t):
        if self.is_time_valid(t):
            return t
        t_day_epoch = get_day(t)
        tr_mins = self.get_min_sec_from_morning()
        return t_day_epoch + tr_mins

    def is_time_day_valid(self, t):
        (start_time, end_time) = self.get_start_and_end_time(t)
        if start_time <= t <= end_time:
            return True
        else:
            return False

    def is_time_day_invalid(self, t):
        (start_time, end_time) = self.get_start_and_end_time(t)
        if start_time <= t <= end_time:
            return False
        else:
            return True

    def get_next_future_timerange_valid(self, t):
        #print "Look for get_next_future_timerange_valid for t", t, time.asctime(time.localtime(t))
        sec_from_morning = get_sec_from_morning(t)
        starts = []
        for tr in self.timeranges:
            tr_start = tr.hstart * 3600 + tr.mstart * 60
            if tr_start >= sec_from_morning:
                starts.append(tr_start)
        if starts != []:
            return min(starts)
        else:
            return None

    def get_next_future_timerange_invalid(self, t):
        #print 'Call for get_next_future_timerange_invalid from ', time.asctime(time.localtime(t))
        sec_from_morning = get_sec_from_morning(t)
        #print 'sec from morning', sec_from_morning
        ends = []
        for tr in self.timeranges:
            tr_start = tr.hstart * 3600 + tr.mstart * 60
            if tr_start >= sec_from_morning:
                ends.append(tr_start)
            tr_end = tr.hend * 3600 + tr.mend * 60
            if tr_end >= sec_from_morning:
                ends.append(tr_end)
        #print "Ends:", ends
        # Remove the last second of the day for 00->24h"
        if 86400 in ends:
            ends.remove(86400)
        if ends != []:
            return min(ends)
        else:
            return None

    def get_next_valid_day(self, t):
        if self.get_next_future_timerange_valid(t) is None:
            # this day is finish, we check for next period
            (start_time, end_time) = self.get_start_and_end_time(get_day(t)+86400)
        else:
            (start_time, end_time) = self.get_start_and_end_time(t)

        if t <= start_time:
            return get_day(start_time)

        if self.is_time_day_valid(t):
            return get_day(t)
        return None

    def get_next_valid_time_from_t(self, t):
        #print "\tDR Get next valid from:", time.asctime(time.localtime(t))
        #print "DR Get next valid from:", t
        if self.is_time_valid(t):
            return t

        #print "DR Get next valid from:", time.asctime(time.localtime(t))
        # First we search fot the day of t
        t_day = self.get_next_valid_day(t)

        #print "DR: T next valid day", time.asctime(time.localtime(t_day))

        # We search for the min of all tr.start > sec_from_morning
        # if it's the next day, use a start of the day search for timerange
        if t < t_day:
            sec_from_morning = self.get_next_future_timerange_valid(t_day)
        else:  # t is in this day, so look from t (can be in the evening or so)
            sec_from_morning = self.get_next_future_timerange_valid(t)
        #print "DR: sec from morning", sec_from_morning

        if sec_from_morning is not None:
            if t_day is not None and sec_from_morning is not None:
                return t_day + sec_from_morning

        # Then we search for the next day of t
        # The sec will be the min of the day
        t = get_day(t) + 86400
        t_day2 = self.get_next_valid_day(t)
        sec_from_morning = self.get_next_future_timerange_valid(t_day2)
        if t_day2 is not None and sec_from_morning is not None:
            return t_day2 + sec_from_morning
        else:
            # I'm not find any valid time
            return None

    def get_next_invalid_day(self, t):
        #print "Look in", self.__dict__
        #print 'DR: get_next_invalid_day for', time.asctime(time.localtime(t))
        if self.is_time_day_invalid(t):
            #print "EARLY RETURN"
            return t

        next_future_timerange_invalid = self.get_next_future_timerange_invalid(t)
        #print "next_future_timerange_invalid:", next_future_timerange_invalid

        # If today there is no more unavailable timerange, search the next day
        if next_future_timerange_invalid is None:
            #print 'DR: get_next_future_timerange_invalid is None'
            #this day is finish, we check for next period
            (start_time, end_time) = self.get_start_and_end_time(get_day(t))
        else:
            #print 'DR: get_next_future_timerange_invalid is', time.asctime(time.localtime(next_future_timerange_invalid))
            (start_time, end_time) = self.get_start_and_end_time(t)

        #(start_time, end_time) = self.get_start_and_end_time(t)

        #print "START", time.asctime(time.localtime(start_time)), "END", time.asctime(time.localtime(end_time))
        # The next invalid day can be t day if there a possible
        # invalid time range (timerange is not 00->24
        if next_future_timerange_invalid is not None:
            if start_time <= t <= end_time:
                #print "Early Return next invalid day:", time.asctime(time.localtime(get_day(t)))
                return get_day(t)
            if start_time >= t:
                #print "start_time >= t:", time.asctime(time.localtime(get_day(start_time)))
                return get_day(start_time)
        else:
            # Else, there is no possibility than in our start_time<->end_time we got
            # any invalid time (full period out). So it's end_time+1 sec (tomorrow of end_time)
            # print "Full period out, got end_time", time.asctime(time.localtime(get_day(end_time +1)))
            return get_day(end_time + 1)

        return None

    def get_next_invalid_time_from_t(self, t):
        if not self.is_time_valid(t):
            return t

        # First we search fot the day of t
        t_day = self.get_next_invalid_day(t)
        #print "FUCK NEXT DAY", time.asctime(time.localtime(t_day))

        # We search for the min of all tr.start > sec_from_morning
        # if it's the next day, use a start of the day search for timerange
        if t < t_day:
            sec_from_morning = self.get_next_future_timerange_invalid(t_day)
        else:  # t is in this day, so look from t (can be in the evening or so)
            sec_from_morning = self.get_next_future_timerange_invalid(t)
        #print "DR: sec from morning", sec_from_morning

        # tr can't be valid, or it will be return at the beginning
        #sec_from_morning = self.get_next_future_timerange_invalid(t)

        # Ok we've got a next invalid day and a invalid possibility in
        # timerange, so the next invalid is this day+sec_from_morning
        #print "T_day", t_day, "Sec from morning", sec_from_morning
        if t_day is not None and sec_from_morning is not None:
            return t_day + sec_from_morning + 1

        # We've got a day but no sec_from_morning: the timerange is full (0->24h)
        # so the next invalid is this day at the day_start
        if t_day is not None and sec_from_morning is None:
            return t_day

        # Then we search for the next day of t
        # The sec will be the min of the day
        t = get_day(t) + 86400
        t_day2 = self.get_next_invalid_day(t)
        sec_from_morning = self.get_next_future_timerange_invalid(t_day2)
        if t_day2 is not None and sec_from_morning is not None:
            return t_day2 + sec_from_morning + 1

        if t_day2 is not None and sec_from_morning is None:
            return t_day2
        else:
            # I'm not find any valid time
            return None




""" TODO: Add some comment about this class for the doc"""
class CalendarDaterange(Daterange):
    def get_start_and_end_time(self, ref=None):
        start_time = get_start_of_day(self.syear, int(self.smon), self.smday)
        end_time = get_end_of_day(self.eyear, int(self.emon), self.emday)
        return (start_time, end_time)



""" TODO: Add some comment about this class for the doc"""
class StandardDaterange(Daterange):
    def __init__(self, day, other):
        self.other = other
        self.timeranges = []

        for timeinterval in other.split(','):
            self.timeranges.append(Timerange(timeinterval.strip()))
        self.day = day

    # It's correct only if the weekday (Sunday, etc) is a valid one
    def is_correct(self):
        b = self.day in Daterange.weekdays
        if not b:
            logger.error("Error: %s is not a valid day" % self.day)
        return b

    def get_start_and_end_time(self, ref=None):
        now = time.localtime(ref)
        self.syear = now.tm_year
        self.month = now.tm_mon
        #month_start_id = now.tm_mon
        #month_start = Daterange.get_month_by_id(month_start_id)
        self.wday = now.tm_wday
        day_id = Daterange.get_weekday_id(self.day)
        today_morning = get_start_of_day(now.tm_year, now.tm_mon, now.tm_mday)
        tonight = get_end_of_day(now.tm_year, now.tm_mon, now.tm_mday)
        day_diff = (day_id - now.tm_wday) % 7
        return (today_morning + day_diff*86400, tonight + day_diff*86400)


""" TODO: Add some comment about this class for the doc"""
class MonthWeekDayDaterange(Daterange):

    # It's correct only if the weekday (Sunday, etc) is a valid one
    def is_correct(self):
        b = True
        b &= self.swday in Daterange.weekdays
        if not b:
            logger.error("Error: %s is not a valid day" % self.swday)

        b &= self.ewday in Daterange.weekdays
        if not b:
            logger.error("Error: %s is not a valid day" % self.ewday)

        return b

    def get_start_and_end_time(self, ref=None):
        now = time.localtime(ref)

        if self.syear == 0:
            self.syear = now.tm_year
        month_id = Daterange.get_month_id(self.smon)
        day_start = find_day_by_weekday_offset(self.syear, self.smon, self.swday, self.swday_offset)
        start_time = get_start_of_day(self.syear, month_id, day_start)

        if self.eyear == 0:
            self.eyear = now.tm_year
        month_end_id = Daterange.get_month_id(self.emon)
        day_end = find_day_by_weekday_offset(self.eyear, self.emon, self.ewday, self.ewday_offset)
        end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        now_epoch = time.mktime(now)
        if start_time > end_time:  # the period is between years
            if now_epoch > end_time:  # check for next year
                day_end = find_day_by_weekday_offset(self.eyear + 1, self.emon, self.ewday, self.ewday_offset)
                end_time = get_end_of_day(self.eyear + 1, month_end_id, day_end)
            else:
                # it s just that the start was the last year
                day_start = find_day_by_weekday_offset(self.syear - 1, self.smon, self.swday, self.swday_offset)
                start_time = get_start_of_day(self.syear - 1, month_id, day_start)
        else:
            if now_epoch > end_time:
                # just have to check for next year if necessary
                day_start = find_day_by_weekday_offset(self.syear + 1, self.smon, self.swday, self.swday_offset)
                start_time = get_start_of_day(self.syear + 1, month_id, day_start)
                day_end = find_day_by_weekday_offset(self.eyear + 1, self.emon, self.ewday, self.ewday_offset)
                end_time = get_end_of_day(self.eyear + 1, month_end_id, day_end)

        return (start_time, end_time)


""" TODO: Add some comment about this class for the doc"""
class MonthDateDaterange(Daterange):
    def get_start_and_end_time(self, ref=None):
        now = time.localtime(ref)
        if self.syear == 0:
            self.syear = now.tm_year
        month_start_id = Daterange.get_month_id(self.smon)
        day_start = find_day_by_offset(self.syear, self.smon, self.smday)
        start_time = get_start_of_day(self.syear, month_start_id, day_start)

        if self.eyear == 0:
            self.eyear = now.tm_year
        month_end_id = Daterange.get_month_id(self.emon)
        day_end = find_day_by_offset(self.eyear, self.emon, self.emday)
        end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        now_epoch = time.mktime(now)
        if start_time > end_time:  # the period is between years
            if now_epoch > end_time:
                # check for next year
                day_end = find_day_by_offset(self.eyear + 1, self.emon, self.emday)
                end_time = get_end_of_day(self.eyear + 1, month_end_id, day_end)
            else:
                # it s just that start was the last year
                day_start = find_day_by_offset(self.syear-1, self.smon, self.emday)
                start_time = get_start_of_day(self.syear-1, month_start_id, day_start)
        else:
            if now_epoch > end_time:
                # just have to check for next year if necessary
                day_start = find_day_by_offset(self.syear+1, self.smon, self.emday)
                start_time = get_start_of_day(self.syear+1, month_start_id, day_start)
                day_end = find_day_by_offset(self.eyear+1, self.emon, self.emday)
                end_time = get_end_of_day(self.eyear+1, month_end_id, day_end)

        return (start_time, end_time)


""" TODO: Add some comment about this class for the doc"""
class WeekDayDaterange(Daterange):
    def get_start_and_end_time(self, ref=None):
        now = time.localtime(ref)

        # If no year, it's our year
        if self.syear == 0:
            self.syear = now.tm_year
        month_start_id = now.tm_mon
        month_start = Daterange.get_month_by_id(month_start_id)
        day_start = find_day_by_weekday_offset(self.syear, month_start, self.swday, self.swday_offset)
        start_time = get_start_of_day(self.syear, month_start_id, day_start)

        # Same for end year
        if self.eyear == 0:
            self.eyear = now.tm_year
        month_end_id = now.tm_mon
        month_end = Daterange.get_month_by_id(month_end_id)
        day_end = find_day_by_weekday_offset(self.eyear, month_end, self.ewday, self.ewday_offset)
        end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        # Maybe end_time is before start. So look for the
        # next month
        if start_time > end_time:
            month_end_id = month_end_id + 1
            if month_end_id > 12:
                month_end_id = 1
                self.eyear += 1
            month_end = Daterange.get_month_by_id(month_end_id)
            day_end = find_day_by_weekday_offset(self.eyear, month_end, self.ewday, self.ewday_offset)
            end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        now_epoch = time.mktime(now)
        # But maybe we look not enought far. We should add a month
        if end_time < now_epoch:
            month_end_id = month_end_id + 1
            month_start_id = month_start_id + 1
            if month_end_id > 12:
                month_end_id = 1
                self.eyear += 1
            if month_start_id > 12:
                month_start_id = 1
                self.syear += 1
            # First start
            month_start = Daterange.get_month_by_id(month_start_id)
            day_start = find_day_by_weekday_offset(self.syear, month_start, self.swday, self.swday_offset)
            start_time = get_start_of_day(self.syear, month_start_id, day_start)
            # Then end
            month_end = Daterange.get_month_by_id(month_end_id)
            day_end = find_day_by_weekday_offset(self.eyear, month_end, self.ewday, self.ewday_offset)
            end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        return (start_time, end_time)


""" TODO: Add some comment about this class for the doc"""
class MonthDayDaterange(Daterange):
    def get_start_and_end_time(self, ref=None):
        now = time.localtime(ref)
        if self.syear == 0:
            self.syear = now.tm_year
        month_start_id = now.tm_mon
        month_start = Daterange.get_month_by_id(month_start_id)
        day_start = find_day_by_offset(self.syear, month_start, self.smday)
        start_time = get_start_of_day(self.syear, month_start_id, day_start)

        if self.eyear == 0:
            self.eyear = now.tm_year
        month_end_id = now.tm_mon
        month_end = Daterange.get_month_by_id(month_end_id)
        day_end = find_day_by_offset(self.eyear, month_end, self.emday)
        end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        now_epoch = time.mktime(now)

        if start_time > end_time:
            month_end_id = month_end_id + 1
            if month_end_id > 12:
                month_end_id = 1
                self.eyear += 1
            day_end = find_day_by_offset(self.eyear, month_end, self.emday)
            end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        if end_time < now_epoch:
            month_end_id = month_end_id + 1
            month_start_id = month_start_id + 1
            if month_end_id > 12:
                month_end_id = 1
                self.eyear += 1
            if month_start_id > 12:
                month_start_id = 1
                self.syear += 1

            # For the start
            month_start = Daterange.get_month_by_id(month_start_id)
            day_start = find_day_by_offset(self.syear, month_start, self.smday)
            start_time = get_start_of_day(self.syear, month_start_id, day_start)

            # For the end
            month_end = Daterange.get_month_by_id(month_end_id)
            day_end = find_day_by_offset(self.eyear, month_end, self.emday)
            end_time = get_end_of_day(self.eyear, month_end_id, day_end)

        return (start_time, end_time)

########NEW FILE########
__FILENAME__ = db
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class DB(object):
    """DB is a generic class for SQL Database"""

    def __init__(self, table_prefix=''):
        self.table_prefix = table_prefix

    def stringify(self, val):
        """Get a unicode from a value"""
        # If raw string, go in unicode
        if isinstance(val, str):
            val = val.decode('utf8', 'ignore').replace("'", "''")
        elif isinstance(val, unicode):
            val = val.replace("'", "''")
        else:  # other type, we can str
            val = unicode(str(val))
            val = val.replace("'", "''")
        return val

    def create_insert_query(self, table, data):
        """Create a INSERT query in table with all data of data (a dict)"""
        query = u"INSERT INTO %s " % (self.table_prefix + table)
        props_str = u' ('
        values_str = u' ('
        i = 0  # f or the ',' problem... look like C here...
        for prop in data:
            i += 1
            val = data[prop]
            # Boolean must be catch, because we want 0 or 1, not True or False
            if isinstance(val, bool):
                if val:
                    val = 1
                else:
                    val = 0

            # Get a string of the value
            val = self.stringify(val)

            if i == 1:
                props_str = props_str + u"%s " % prop
                values_str = values_str + u"'%s' " % val
            else:
                props_str = props_str + u", %s " % prop
                values_str = values_str + u", '%s' " % val

        # Ok we've got data, let's finish the query
        props_str = props_str + u' )'
        values_str = values_str + u' )'
        query = query + props_str + u' VALUES' + values_str
        return query

    def create_update_query(self, table, data, where_data):
        """Create a update query of table with data, and use where data for
        the WHERE clause
        """
        query = u"UPDATE %s set " % (self.table_prefix + table)

        # First data manage
        query_follow = ''
        i = 0  # for the , problem...
        for prop in data:
            # Do not need to update a property that is in where
            # it is even dangerous, will raise a warning
            if prop not in where_data:
                i += 1
                val = data[prop]
                # Boolean must be catch, because we want 0 or 1, not True or False
                if isinstance(val, bool):
                    if val:
                        val = 1
                    else:
                        val = 0

                # Get a string of the value
                val = self.stringify(val)

                if i == 1:
                    query_follow += u"%s='%s' " % (prop, val)
                else:
                    query_follow += u", %s='%s' " % (prop, val)

        # Ok for data, now WHERE, same things
        where_clause = u" WHERE "
        i = 0  # For the 'and' problem
        for prop in where_data:
            i += 1
            val = where_data[prop]
            # Boolean must be catch, because we want 0 or 1, not True or False
            if isinstance(val, bool):
                if val:
                    val = 1
                else:
                    val = 0

            # Get a string of the value
            val = self.stringify(val)

            if i == 1:
                where_clause += u"%s='%s' " % (prop, val)
            else:
                where_clause += u"and %s='%s' " % (prop, val)

        query = query + query_follow + where_clause
        return query

    def fetchone(self):
        """Just get an entry"""
        return self.db_cursor.fetchone()
        
    def fetchall(self):
        """Get all entry"""
        return self.db_cursor.fetchall()

########NEW FILE########
__FILENAME__ = db_mysql
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import MySQLdb
from MySQLdb import IntegrityError
from MySQLdb import ProgrammingError
import _mysql_exceptions


from shinken.db import DB
from shinken.log import logger

class DBMysql(DB):
    """DBMysql is a MySQL access database class"""

    def __init__(self, host, user, password, database, character_set,
                 table_prefix='', port=3306):
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.character_set = character_set
        self.table_prefix = table_prefix
        self.port = port

    def connect_database(self):
        """Create the database connection
        TODO: finish (begin :) ) error catch and conf parameters...
        Import to catch exception
        """

        # self.db = MySQLdb.connect (host = "localhost", user = "root",
        #                            passwd = "root", db = "merlin")
        self.db = MySQLdb.connect(host=self.host, user=self.user,
                                  passwd=self.password, db=self.database,
                                  port=self.port)
        self.db.set_character_set(self.character_set)
        self.db_cursor = self.db.cursor()
        self.db_cursor.execute('SET NAMES %s;' % self.character_set)
        self.db_cursor.execute('SET CHARACTER SET %s;' % self.character_set)
        self.db_cursor.execute('SET character_set_connection=%s;' %
                               self.character_set)
        # Thanks:
        # http://www.dasprids.de/blog/2007/12/17/python-mysqldb-and-utf-8
        # for utf8 code :)

    def execute_query(self, query, do_debug=False):
        """Just run the query
        TODO: finish catch
        """
        if do_debug:
            logger.debug("[MysqlDB]I run query %s" % query)
        try:
            self.db_cursor.execute(query)
            self.db.commit()
            return True
        except IntegrityError, exp:
            logger.warning("[MysqlDB] A query raised an integrity error:" \
                  " %s, %s" % (query, exp))
            return False
        except ProgrammingError, exp:
            logger.warning("[MysqlDB] A query raised a programming error:" \
                  " %s, %s" % (query, exp))
            return False

########NEW FILE########
__FILENAME__ = db_oracle
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

# Failed to import will be catch by __init__.py
from cx_Oracle import connect as connect_function
from cx_Oracle import IntegrityError as IntegrityError_exp
from cx_Oracle import ProgrammingError as ProgrammingError_exp
from cx_Oracle import DatabaseError as DatabaseError_exp
from cx_Oracle import InternalError as InternalError_exp
from cx_Oracle import DataError as DataError_exp
from cx_Oracle import OperationalError as OperationalError_exp

from shinken.db import DB
from shinken.log import logger

connect_function = None
IntegrityError_exp = None
ProgrammingError_exp = None
DatabaseError_exp = None
InternalError_exp = None
DataError_exp = None
OperationalError_exp = None


class DBOracle(DB):
    """Manage connection and query execution against Oracle databases."""

    def __init__(self, user, password, database, table_prefix=''):
        self.user = user
        self.password = password
        self.database = database
        self.table_prefix = table_prefix

    def connect_database(self):
        """Create the database connection
        TODO: finish (begin :) ) error catch and conf parameters...
        """

        connstr = '%s/%s@%s' % (self.user, self.password, self.database)

        self.db = connect_function(connstr)
        self.db_cursor = self.db.cursor()
        self.db_cursor.arraysize = 50

    def execute_query(self, query):
        """ Execute a query against an Oracle database.
        """
        logger.debug("[DBOracle] Execute Oracle query %s\n" % (query))
        try:
            self.db_cursor.execute(query)
            self.db.commit()
        except IntegrityError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise an integrity error:" \
                  " %s, %s" % (query, exp))
        except ProgrammingError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise a programming error:" \
                  " %s, %s" % (query, exp))
        except DatabaseError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise a database error:" \
                  " %s, %s" % (query, exp))
        except InternalError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise an internal error:" \
                  " %s, %s" % (query, exp))
        except DataError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise a data error:" \
                  " %s, %s" % (query, exp))
        except OperationalError_exp, exp:
            logger.warning("[DBOracle] Warning: a query raise an operational error:" \
                  " %s, %s" % (query, exp))
        except Exception, exp:
            logger.warning("[DBOracle] Warning: a query raise an unknown error:" \
                  " %s, %s" % (query, exp))
            logger.warning(exp.__dict__)

########NEW FILE########
__FILENAME__ = db_sqlite
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from db import DB
from shinken.log import logger
import sqlite3


class DBSqlite(DB):
    """DBSqlite is a sqlite access database class"""

    def __init__(self, db_path, table_prefix=''):
        self.table_prefix = table_prefix
        self.db_path = db_path

    def connect_database(self):
        """Create the database connection"""
        self.db = sqlite3.connect(self.db_path)
        self.db_cursor = self.db.cursor()

    def execute_query(self, query):
        """Just run the query"""
        logger.debug("[SqliteDB] Info: I run query '%s'" % query)
        self.db_cursor.execute(query)
        self.db.commit()

########NEW FILE########
__FILENAME__ = dependencynode
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re
from shinken.log import logger
from shinken.util import filter_any, filter_none
from shinken.util import filter_host_by_name, filter_host_by_regex, filter_host_by_group, filter_host_by_template
from shinken.util import filter_service_by_name
from shinken.util import filter_service_by_regex_name
from shinken.util import filter_service_by_regex_host_name
from shinken.util import filter_service_by_host_name
from shinken.util import filter_service_by_bp_rule_label
from shinken.util import filter_service_by_hostgroup_name
from shinken.util import filter_service_by_host_template_name
from shinken.util import filter_service_by_servicegroup_name
from shinken.util import filter_host_by_bp_rule_label
from shinken.util import filter_service_by_host_bp_rule_label


"""
Here is a node class for dependency_node(s) and a factory to create them
"""
class DependencyNode(object):
    def __init__(self):
        self.operand = None
        self.sons = []
        # Of: values are a triple OK,WARN,CRIT
        self.of_values = ('0', '0', '0')
        self.is_of_mul = False
        self.configuration_errors = []
        self.not_value = False

    def __str__(self):
        return "Op:'%s' Val:'%s' Sons:'[%s]' IsNot:'%s'" % (self.operand, self.of_values, ','.join([str(s) for s in self.sons]), self.not_value)


    def get_reverse_state(self, state):
        # Warning is still warning
        if state == 1:
            return 1
        if state == 0:
            return 2
        if state == 2:
            return 0
        # should not go here...
        return state


    # We will get the state of this node, by looking at the state of
    # our sons, and apply our operand
    def get_state(self):
        #print "Ask state of me", self

        # If we are a host or a service, wee just got the host/service
        # hard state
        if self.operand in ['host', 'service']:
            return self.get_simple_node_state()
        else:
            return self.get_complex_node_state()


    # Returns a simple node direct state (such as an host or a service). No
    # calculation is needed
    def get_simple_node_state(self):
        state = self.sons[0].last_hard_state_id
        #print "Get the hard state (%s) for the object %s" % (state, self.sons[0].get_name())
        # Make DOWN look as CRITICAL (2 instead of 1)
        if self.operand == 'host' and state == 1:
            state = 2
        # Maybe we are a NOT node, so manage this
        if self.not_value:
            # We inverse our states
            if self.operand == 'host' and state == 1:
                return 0
            if self.operand == 'host' and state == 0:
                return 1
            # Critical -> OK
            if self.operand == 'service' and state == 2:
                return 0
            # OK -> CRITICAL (warning is untouched)
            if self.operand == 'service' and state == 0:
                return 2
        return state


    # Calculates a complex node state based on its sons state, and its operator
    def get_complex_node_state(self):
        if self.operand == '|':
            return self.get_complex_or_node_state()

        elif self.operand == '&':
            return self.get_complex_and_node_state()

        #  It's an Xof rule
        else:
            return self.get_complex_xof_node_state()


    # Calculates a complex node state with an | operand
    def get_complex_or_node_state(self):
        # First we get the state of all our sons
        states = [s.get_state() for s in self.sons]
        # Next we calculate the best state
        best_state = min(states)
        # Then we handle eventual not value
        if self.not_value:
            return self.get_reverse_state(best_state)
        return best_state


    # Calculates a complex node state with an & operand
    def get_complex_and_node_state(self):
        # First we get the state of all our sons
        states = [s.get_state() for s in self.sons]
        # Next we calculate the worst state
        if 2 in states:
            worst_state = 2
        else:
            worst_state = max(states)
        # Then we handle eventual not value
        if self.not_value:
            return self.get_reverse_state(worst_state)
        return worst_state


    # Calculates a complex node state with an Xof operand
    def get_complex_xof_node_state(self):
        # First we get the state of all our sons
        states = [s.get_state() for s in self.sons]

        # We search for OK, WARN or CRIT applications
        # And we will choice between them
        nb_search_ok = self.of_values[0]
        nb_search_warn = self.of_values[1]
        nb_search_crit = self.of_values[2]

        # We look for each application
        nb_sons = len(states)
        nb_ok = len([s for s in states if s == 0])
        nb_warn = len([s for s in states if s == 1])
        nb_crit = len([s for s in states if s == 2])

        #print "NB:", nb_ok, nb_warn, nb_crit

        # Ok and Crit apply with their own values
        # Warn can apply with warn or crit values
        # so a W C can raise a Warning, but not enough for
        # a critical
        def get_state_for(nb_tot, nb_real, nb_search):
            if nb_search.endswith('%'):
                nb_search = int(nb_search[:-1])
                if nb_search < 0:
                    # nb_search is negative, so +
                    nb_search = max(100 + nb_search, 0)
                apply_for = float(nb_real) / nb_tot * 100 >= nb_search
            else:
                nb_search = int(nb_search)
                if nb_search < 0:
                    # nb_search is negative, so +
                    nb_search = max(nb_tot + nb_search, 0)
                apply_for = nb_real >= nb_search
            return apply_for

        ok_apply = get_state_for(nb_sons, nb_ok, nb_search_ok)
        warn_apply = get_state_for(nb_sons, nb_warn + nb_crit, nb_search_warn)
        crit_apply = get_state_for(nb_sons, nb_crit, nb_search_crit)

        #print "What apply?", ok_apply, warn_apply, crit_apply

        # return the worst state that apply
        if crit_apply:
            if self.not_value:
                return self.get_reverse_state(2)
            return 2

        if warn_apply:
            if self.not_value:
                return self.get_reverse_state(1)
            return 1

        if ok_apply:
            if self.not_value:
                return self.get_reverse_state(0)
            return 0

        # Maybe even OK is not possible, if so, it depends if the admin
        # ask a simple form Xof: or a multiple one A,B,Cof:
        # the simple should give OK, the mult should give the worst state
        if self.is_of_mul:
            #print "Is mul, send 0"
            if self.not_value:
                return self.get_reverse_state(0)
            return 0
        else:
            #print "not mul, return worst", worse_state
            if 2 in states:
                worst_state = 2
            else:
                worst_state = max(states)
            if self.not_value:
                return self.get_reverse_state(worst_state)
            return worst_state


    # return a list of all host/service in our node and below
    def list_all_elements(self):
        r = []

        # We are a host/service
        if self.operand in ['host', 'service']:
            return [self.sons[0]]

        for s in self.sons:
            r.extend(s.list_all_elements())

        # and uniq the result
        return list(set(r))


    # If we are a of: rule, we can get some 0 in of_values,
    # if so, change them with NB sons instead
    def switch_zeros_of_values(self):
        nb_sons = len(self.sons)
        # Need a list for assignment
        self.of_values = list(self.of_values)
        for i in [0, 1, 2]:
            if self.of_values[i] == '0':
                self.of_values[i] = str(nb_sons)
        self.of_values = tuple(self.of_values)


    # Check for empty (= not found) leaf nodes
    def is_valid(self):

        valid = True
        if not self.sons:
            valid = False
        else:
            for s in self.sons:
                if isinstance(s, DependencyNode) and not s.is_valid():
                    self.configuration_errors.extend(s.configuration_errors)
                    valid = False
        return valid



""" TODO: Add some comment about this class for the doc"""
class DependencyNodeFactory(object):

    host_flags = "grlt"
    service_flags = "grl"

    def __init__(self, bound_item):
        self.bound_item = bound_item


    # the () will be eval in a recursiv way, only one level of ()
    def eval_cor_pattern(self, pattern, hosts, services, running=False):
        pattern = pattern.strip()
        #print "***** EVAL ", pattern
        complex_node = False

        # Look if it's a complex pattern (with rule) or
        # if it's a leaf ofit, like a host/service
        for m in '()+&|':
            if m in pattern:
                complex_node = True

        # If it's a simple node, evaluate it directly
        if complex_node is False:
            return self.eval_simple_cor_pattern(pattern, hosts, services, running)
        else:
            return self.eval_complex_cor_pattern(pattern, hosts, services, running)


    # Checks if an expression is an Xof pattern, and parses its components if
    # so. In such a case, once parsed, returns the cleaned patten.
    def eval_xof_pattern(self, node, pattern):
        p = "^(-?\d+%?),*(-?\d*%?),*(-?\d*%?) *of: *(.+)"
        r = re.compile(p)
        m = r.search(pattern)
        if m is not None:
            #print "Match the of: thing N=", m.groups()
            node.operand = 'of:'
            g = m.groups()
            # We can have a Aof: rule, or a multiple A,B,Cof: rule.
            mul_of = (g[1] != u'' and g[2] != u'')
            # If multi got (A,B,C)
            if mul_of:
                node.is_of_mul = True
                node.of_values = (g[0], g[1], g[2])
            else:  # if not, use A,0,0, we will change 0 after to put MAX
                node.of_values = (g[0], '0', '0')
            pattern = m.groups()[3]
        return pattern


    # Evaluate a complex correlation expression, such as an &, |, nested
    # expressions in par, and so on.
    def eval_complex_cor_pattern(self, pattern, hosts, services, running=False):
        node = DependencyNode()
        pattern = self.eval_xof_pattern(node, pattern)

        in_par = False
        tmp = ''
        son_is_not = False  # We keep is the next son will be not or not
        stacked_par = 0
        for c in pattern:
            if c == '(':
                stacked_par += 1
                #print "INCREASING STACK TO", stacked_par

                in_par = True
                tmp = tmp.strip()
                # Maybe we just start a par, but we got some things in tmp
                # that should not be good in fact !
                if stacked_par == 1 and tmp != '':
                    #TODO : real error
                    print "ERROR : bad expression near", tmp
                    continue

                # If we are already in a par, add this (
                # but not if it's the first one so
                if stacked_par > 1:
                    tmp += c

            elif c == ')':
                #print "Need closeing a sub expression?", tmp
                stacked_par -= 1

                if stacked_par < 0:
                    # TODO : real error
                    print "Error : bad expression near", tmp, "too much ')'"
                    continue

                if stacked_par == 0:
                    #print "THIS is closing a sub compress expression", tmp
                    tmp = tmp.strip()
                    o = self.eval_cor_pattern(tmp, hosts, services, running)
                    # Maybe our son was notted
                    if son_is_not:
                        o.not_value = True
                        son_is_not = False
                    node.sons.append(o)
                    in_par = False
                    # OK now clean the tmp so we start clean
                    tmp = ''
                    continue

                # ok here we are still in a huge par, we just close one sub one
                tmp += c

            # Expressions in par will be parsed in a sub node after. So just
            # stack pattern
            elif in_par:
                tmp += c

            # Until here, we're not in par

            # Manage the NOT for an expression. Only allow ! at the beginning
            # of an host or an host,service expression.
            elif c == '!':
                tmp = tmp.strip()
                if tmp and tmp[0] != '!':
                    print "Error : bad expression near", tmp, "wrong position for '!'"
                    continue
                # Flags next node not state
                son_is_not = True
                # DO NOT keep the c in tmp, we consumed it

            #print "MATCHING", c, pattern
            elif c == '&' or c == '|':
                # Oh we got a real cut in an expression, if so, cut it
                #print "REAL & for cutting"
                tmp = tmp.strip()
                # Look at the rule viability
                if node.operand is not None and node.operand != 'of:' and c != node.operand:
                    # Should be logged as a warning / info? :)
                    return None

                if node.operand != 'of:':
                    node.operand = c
                if tmp != '':
                    #print "Will analyse the current str", tmp
                    o = self.eval_cor_pattern(tmp, hosts, services, running)
                    # Maybe our son was notted
                    if son_is_not:
                        o.not_value = True
                        son_is_not = False
                    node.sons.append(o)
                tmp = ''

            # Maybe it's a classic character or we're in par, if so, continue
            else:
                tmp += c

        # Be sure to manage the trainling part when the line is done
        tmp = tmp.strip()
        if tmp != '':
            #print "Managing trainling part", tmp
            o = self.eval_cor_pattern(tmp, hosts, services, running)
            # Maybe our son was notted
            if son_is_not:
                o.not_value = True
                son_is_not = False
            #print "4end I've %s got new sons" % pattern , o
            node.sons.append(o)

        # We got our nodes, so we can update 0 values of of_values
        # with the number of sons
        node.switch_zeros_of_values()

        return node


    # Evaluate a simple correlation expression, such as an host, an host + a
    # service, or expand an host or service expression.
    def eval_simple_cor_pattern(self, pattern, hosts, services, running=False):
        node = DependencyNode()
        pattern = self.eval_xof_pattern(node, pattern)

        #print "Try to find?", pattern
        # If it's a not value, tag the node and find
        # the name without this ! operator
        if pattern.startswith('!'):
            node.not_value = True
            pattern = pattern[1:]
        # Is the pattern an expression to be expanded?
        if re.search(r"^([%s]+|\*):" % self.host_flags, pattern) or \
                re.search(r",\s*([%s]+:.*|\*)$" % self.service_flags, pattern):
            # o is just extracted its attributes, then trashed.
            o = self.expand_expression(pattern, hosts, services, running)
            if node.operand != 'of:':
                node.operand = '&'
            node.sons.extend(o.sons)
            node.configuration_errors.extend(o.configuration_errors)
            node.switch_zeros_of_values()
        else:
            node.operand = 'object'
            obj, error = self.find_object(pattern, hosts, services)
            if obj is not None:
                # Set host or service
                node.operand = obj.__class__.my_type
                node.sons.append(obj)
            else:
                if running is False:
                    node.configuration_errors.append(error)
                else:
                    # As business rules are re-evaluated at run time on
                    # each scheduling loop, if the rule becomes invalid
                    # because of a badly written macro modulation, it
                    # should be notified upper for the error to be
                    # displayed in the check output.
                    raise Exception(error)
        return node


    # We've got an object, like h1,db1 that mean the
    # db1 service of the host db1, or just h1, that mean
    # the host h1.
    def find_object(self, pattern, hosts, services):
        #print "Finding object", pattern
        obj = None
        error = None
        is_service = False
        # h_name, service_desc are , separated
        elts = pattern.split(',')
        host_name = elts[0].strip()
        # If host_name is empty, use the host_name the business rule is bound to
        if not host_name:
            host_name = self.bound_item.host_name
        # Look if we have a service
        if len(elts) > 1:
            is_service = True
            service_description = elts[1].strip()
        if is_service:
            obj = services.find_srv_by_name_and_hostname(host_name, service_description)
            if not obj:
                error = "Business rule uses unknown service %s/%s" % (host_name, service_description)
        else:
            obj = hosts.find_by_name(host_name)
            if not obj:
                error = "Business rule uses unknown host %s" % (host_name,)
        return obj, error


    # Tries to expand a host or service expression into a dependency node tree
    # using (host|service)group membership, regex, or labels as item selector.
    def expand_expression(self, pattern, hosts, services, running=False):
        error = None
        node = DependencyNode()
        node.operand = '&'
        elts = [e.strip() for e in pattern.split(',')]
        # If host_name is empty, use the host_name the business rule is bound to
        if not elts[0]:
            elts[0] = self.bound_item.host_name
        filters = []
        # Looks for hosts/services using appropriate filters
        try:
            if len(elts) > 1:
                # We got a service expression
                host_expr, service_expr = elts
                filters.extend(self.get_srv_host_filters(host_expr))
                filters.extend(self.get_srv_service_filters(service_expr))
                items = services.find_by_filter(filters)
            else:
                # We got an host expression
                host_expr = elts[0]
                filters.extend(self.get_host_filters(host_expr))
                items = hosts.find_by_filter(filters)
        except re.error, e:
            error = "Business rule uses invalid regex %s: %s" % (pattern, e)
        else:
            if not items:
                error = "Business rule got an empty result for pattern %s" % pattern

        # Checks if we got result
        if error:
            if running is False:
                node.configuration_errors.append(error)
            else:
                # As business rules are re-evaluated at run time on
                # each scheduling loop, if the rule becomes invalid
                # because of a badly written macro modulation, it
                # should be notified upper for the error to be
                # displayed in the check output.
                raise Exception(error)
            return node

        # Creates dependency node subtree
        for item in items:
            # Creates a host/service node
            son = DependencyNode()
            son.operand = item.__class__.my_type
            son.sons.append(item)
            # Appends it to wrapping node
            node.sons.append(son)

        node.switch_zeros_of_values()
        return node


    # Generates filter list on a hosts host_name
    def get_host_filters(self, expr):
        if expr == "*":
            return [filter_any]
        match = re.search(r"^([%s]+):(.*)" % self.host_flags, expr)
        
        if match is None:
            return [filter_host_by_name(expr)]
        flags, expr = match.groups()
        if "g" in flags:
            return [filter_host_by_group(expr)]
        elif "r" in flags:
            return [filter_host_by_regex(expr)]
        elif "l" in flags:
            return [filter_host_by_bp_rule_label(expr)]
        elif "t" in flags:
            return [filter_host_by_template(expr)]
        else:
            return [filter_none]


    # Generates filter list on services host_name
    def get_srv_host_filters(self, expr):
        if expr == "*":
            return [filter_any]
        match = re.search(r"^([%s]+):(.*)" % self.host_flags, expr)
        if match is None:
            return [filter_service_by_host_name(expr)]
        flags, expr = match.groups()

        if "g" in flags:
            return [filter_service_by_hostgroup_name(expr)]
        elif "r" in flags:
            return [filter_service_by_regex_host_name(expr)]
        elif "l" in flags:
            return [filter_service_by_host_bp_rule_label(expr)]
        elif "t" in flags:
            return [filter_service_by_host_template_name(expr)]
        else:
            return [filter_none]


    # Generates filter list on services service_description
    def get_srv_service_filters(self, expr):
        if expr == "*":
            return [filter_any]
        match = re.search(r"^([%s]+):(.*)" % self.service_flags, expr)
        if match is None:
            return [filter_service_by_name(expr)]
        flags, expr = match.groups()

        if "g" in flags:
            return [filter_service_by_servicegroup_name(expr)]
        elif "r" in flags:
            return [filter_service_by_regex_name(expr)]
        elif "l" in flags:
            return [filter_service_by_bp_rule_label(expr)]
        else:
            return [filter_none]

########NEW FILE########
__FILENAME__ = discoverymanager
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.



import sys
import cPickle
import os
import re
import time
import copy
import random
import string
# Always initialize random...
random.seed(time.time())
try:
    import uuid
except ImportError:
    uuid = None

try:
    from pymongo.connection import Connection
except ImportError:
    Connection = None

from shinken.log import logger
from shinken.objects import *
from shinken.objects.config import Config
from shinken.macroresolver import MacroResolver
from shinken.modulesmanager import ModulesManager



def get_uuid(self):
    if uuid:
        return uuid.uuid1().hex
    # Ok for old python like 2.4, we will lie here :)
    return int(random.random()*sys.maxint)


# Look if the name is a IPV4 address or not
def is_ipv4_addr(name):
    p = r"^([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])$"
    return (re.match(p, name) is not None)



def by_order(r1, r2):
    if r1.discoveryrule_order == r2.discoveryrule_order:
        return 0
    if r1.discoveryrule_order > r2.discoveryrule_order:
        return 1
    if r1.discoveryrule_order < r2.discoveryrule_order:
        return -1

class DiscoveredHost(object):
    my_type = 'host' # we fake our type for the macro resolving

    macros = {
        'HOSTNAME':          'name',
        }

    def __init__(self, name, rules, runners, merge=False, first_level_only=False):
        self.name = name
        self.data = {}
        self.rules = rules
        self.runners = runners
        self.merge = merge

        self.matched_rules = []
        self.launched_runners = []

        self.in_progress_runners = []
        self.properties = {}
        self.customs = {}
        self.first_level_only = first_level_only

    # In final phase, we keep only _ properties and
    # rule based one
    def update_properties(self, final_phase=False):
        d = {}
        if final_phase:
            for (k,v) in self.data.iteritems():
                if k.startswith('_'):
                    d[k] = v
        else:
            d = copy.copy(self.data)

        d['host_name'] = self.name
        # Set address directive if an ip exists
        if self.data.has_key('ip'):
            d['address'] = self.data['ip']

        self.matched_rules.sort(by_order)

        for r in self.matched_rules:
            for k,v in r.writing_properties.iteritems():
                # If it's a + (add) property, append
                if k.startswith('+'):
                    kprop = k[1:]
                    # If the d do not already have this prop,
                    # create list
                    if not kprop in d:
                        print 'New prop',kprop
                        d[kprop]=[]
                
                elif not k.startswith('-'):
                    kprop = k
                    if not kprop in d:
                        print 'New prop',kprop
                    else:
                        print 'Prop',kprop,'reset with new value'
                    d[kprop]=[]

                for prop in string.split(v,','):
                    prop=prop.strip()
                    #checks that prop does not already exist and adds
                    if not prop in d[kprop]:
                        if len(d[kprop]) > 0:
                            print 'Already got', ','.join(d[kprop]), 'add', prop
                        else:
                            print 'Add',prop
                        d[kprop].append(prop)

            # Now look for - (rem) property
            for k,v in r.writing_properties.iteritems():
                if k.startswith('-'):
                    kprop = k[1:]
                    if kprop in d:
                        for prop in string.split(v,','):
                            prop = prop.strip()
                            if prop in d[kprop]:
                                print 'Already got', ','.join(d[kprop]), 'rem', prop
                                d[kprop].remove(prop)
 
        # Change join prop list in string with a ',' separator
        for (k,v) in d.iteritems():
            if type(d[k]).__name__=='list':
                d[k]=','.join(d[k])

        self.properties = d
        print 'Update our properties', self.name, d
        
        # For macro-resolving, we should have our macros too
        self.customs = {}
        for (k,v) in self.properties.iteritems():
            self.customs['_'+k.upper()] = v

            
    # Manager ask us our properties for the configuration, so
    # we keep only rules properties and _ ones
    def get_final_properties(self):
        self.update_properties(final_phase=True)
        return self.properties
            

    def get_to_run(self):
        self.in_progress_runners = []
        
        if self.first_level_only:
            return

        for r in self.runners:
            # If we already launched it, we don't want it :)
            if r in self.launched_runners:
                print 'Sorry', r.get_name(), 'was already launched'
                continue
            # First level discovery are for large scan, so not for here
            if r.is_first_level():
                print 'Sorry', r.get_name(), 'is first level'
                continue
            # And of course it must match our data
            print 'Is ', r.get_name(), 'matching??', r.is_matching_disco_datas(self.properties)
            if r.is_matching_disco_datas(self.properties):
                self.in_progress_runners.append(r)
            


    def need_to_run(self):
        return len(self.in_progress_runners) != 0
    


    # Now we try to match all our hosts with the rules
    def match_rules(self):
        print 'And our data?', self.data
        for r in self.rules:
            # If the rule was already successfully for this host, skip it
            if r in self.matched_rules:
                print 'We already apply the rule', r.get_name(), 'for the host', self.name
                continue
            print 'Looking for match with a new rule', r.get_name(), 'for the host', self.name
            if r.is_matching_disco_datas(self.data):
                self.matched_rules.append(r)
                print "Generating a new rule", self.name, r.writing_properties
        self.update_properties()



    def read_disco_buf(self, buf):
        print 'Read buf in', self.name
        for l in buf.split('\n'):
            #print ""
            # If it's not a disco line, bypass it
            if not re.search('::', l):
                continue
            #print "line", l
            elts = l.split('::', 1)
            if len(elts) <= 1:
                #print "Bad discovery data"
                continue
            name = elts[0].strip()

            # We can choose to keep only the basename
            # of the nameid, so strip the fqdn
            # But not if it's a plain ipv4 addr
            #TODO : gt this! if self.conf.strip_idname_fqdn:
            if not is_ipv4_addr(name):
                name = name.split('.', 1)[0]
            
            data = '::'.join(elts[1:])

            # Maybe it's not me?
            if name != self.name:
                if not self.merge:
                    print 'Bad data for me? I bail out data!'
                    data = ''
                else:
                    print 'Bad data for me? Let\'s switch !'
                    self.name = name

            # Now get key,values
            if not '=' in data:
                continue

            elts = data.split('=', 1)
            if len(elts) <= 1:
                continue

            key = elts[0].strip()
            value = elts[1].strip()
            print "INNER -->", name, key, value
            self.data[key] = value


    def launch_runners(self):
        for r in self.in_progress_runners:
            print "I", self.name, " is launching", r.get_name(), "with a %d seconds timeout" % 3600
            r.launch(timeout=3600, ctx=[self])
            self.launched_runners.append(r)


    def wait_for_runners_ends(self):
        all_ok = False
        while not all_ok:
            print 'Loop wait runner for', self.name
            all_ok = True
            for r in self.in_progress_runners:
                if not r.is_finished():
                    #print "Check finished of", r.get_name()
                    r.check_finished()
                b = r.is_finished()
                if not b:
                    #print r.get_name(), "is not finished"
                    all_ok = False
            time.sleep(0.1)


    def get_runners_outputs(self):
        for r in self.in_progress_runners:
            if r.is_finished():
                print'Get output', self.name, r.discoveryrun_name, r.current_launch
                if r.current_launch.exit_status != 0:
                    print "Error on run"
        raw_disco_data = '\n'.join(r.get_output() for r in self.in_progress_runners if r.is_finished())
        if len(raw_disco_data) != 0:
            print "Got Raw disco data", raw_disco_data
        else:
            print "Got no data!"
            for r in self.in_progress_runners:
                print "DBG", r.current_launch
        # Now get the data for me :)
        self.read_disco_buf(raw_disco_data)


class DiscoveryManager:
    def __init__(self, path, macros, overwrite, runners, output_dir=None, dbmod='', db_direct_insert=False, only_new_hosts=False, backend=None, modules_path='', merge=False, conf=None, first_level_only=False):
        # i am arbiter-like
        self.log = logger
        self.overwrite = overwrite
        self.runners = runners
        self.output_dir = output_dir
        self.dbmod = dbmod
        self.db_direct_insert = db_direct_insert
        self.only_new_hosts = only_new_hosts
        self.log.load_obj(self)
        self.merge= merge
        self.config_files = [path]
        # For specific backend, to override the classic file/db behavior
        self.backend = backend
        self.modules_path = modules_path
        self.first_level_only = first_level_only

        if not conf:
            self.conf = Config()

            
            buf = self.conf.read_config(self.config_files)
        
            # Add macros on the end of the buf so they will
            # overwrite the resource.cfg ones
            for (m, v) in macros:
                buf += '\n$%s$=%s\n' % (m, v)

            raw_objects = self.conf.read_config_buf(buf)
            self.conf.create_objects_for_type(raw_objects, 'arbiter')
            self.conf.create_objects_for_type(raw_objects, 'module')
            self.conf.early_arbiter_linking()
            self.conf.create_objects(raw_objects)
            self.conf.linkify_templates()
            self.conf.apply_inheritance()
            self.conf.explode()
            self.conf.create_reversed_list()
            self.conf.remove_twins()
            self.conf.apply_implicit_inheritance()
            self.conf.fill_default()
            self.conf.remove_templates()
            self.conf.pythonize()
            self.conf.linkify()
            self.conf.apply_dependencies()
            self.conf.is_correct()
        else:
            self.conf = conf

        self.discoveryrules = self.conf.discoveryrules
        self.discoveryruns = self.conf.discoveryruns
        
        m = MacroResolver()
        m.init(self.conf)
        
        # Hash = name, and in it (key, value)
        self.disco_data = {}
        # Hash = name, and in it rules that apply
        self.disco_matches = {}

        self.init_database()
        self.init_backend()


    def add(self, obj):
        pass


    # We try to init the database connection
    def init_database(self):
        self.dbconnection = None
        self.db = None

        if self.dbmod == '':
            return

        for mod in self.conf.modules:
            if getattr(mod, 'module_name', '') == self.dbmod:
                if Connection is None:
                    print "ERROR : cannot use Mongodb database : please install the pymongo library"
                    break
                # Now try to connect
                try:
                    uri = mod.uri
                    database = mod.database
                    self.dbconnection = Connection(uri)
                    self.db = getattr(self.dbconnection, database)
                    print "Connection to Mongodb:%s:%s is OK" % (uri, database)
                except Exception, exp:
                    logger.error('Database init : %s' % exp)


    # We try to init the backend if we got one
    def init_backend(self):
        if not self.backend or not isinstance(self.backend, basestring):
            return

        print "Doing backend init"
        for mod in self.conf.modules:
            if getattr(mod, 'module_name', '') == self.backend:
                print "We found our backend", mod.get_name()
                self.backend = mod
        if not self.backend:
            print "ERROR : cannot find the module %s" % self.backend
            sys.exit(2)
        self.modules_manager = ModulesManager('discovery', self.modules_path, [])
        self.modules_manager.set_modules([mod])
        self.modules_manager.load_and_init()
        self.backend = self.modules_manager.instances[0]
        print "We got our backend!", self.backend



    def loop_discovery(self):
        still_loop = True
        i = 0
        while still_loop:
            i += 1
            print '\n'
            print 'LOOP'*10, i
            still_loop = False
            for (name, dh) in self.disco_data.iteritems():
                dh.update_properties()
                to_run = dh.get_to_run()
                print 'Still to run for', name, to_run
                if dh.need_to_run():
                    still_loop = True
                    dh.launch_runners()
                    dh.wait_for_runners_ends()
                    dh.get_runners_outputs()
                    dh.match_rules()



    def read_disco_buf(self):
        buf = self.raw_disco_data
        for l in buf.split('\n'):
            #print ""
            # If it's not a disco line, bypass it
            if not re.search('::', l):
                continue
            #print "line", l
            elts = l.split('::', 1)
            if len(elts) <= 1:
                #print "Bad discovery data"
                continue
            name = elts[0].strip()

            # We can choose to keep only the basename
            # of the nameid, so strip the fqdn
            # But not if it's a plain ipv4 addr
            if self.conf.strip_idname_fqdn:
                if not is_ipv4_addr(name):
                    name = name.split('.', 1)[0]
            
            data = '::'.join(elts[1:])
            
            # Register the name
            if not name in self.disco_data:
                self.disco_data[name] = DiscoveredHost(name, self.discoveryrules, self.discoveryruns, merge=self.merge, first_level_only=self.first_level_only)

            # Now get key,values
            if not '=' in data:
                continue

            elts = data.split('=',1)
            if len(elts) <= 1:
                continue

            dh = self.disco_data[name]
            key = elts[0].strip()
            value = elts[1].strip()
            print "-->", name, key, value
            dh.data[key] = value


    # Now we try to match all our hosts with the rules
    def match_rules(self):
        for (name, dh) in self.disco_data.iteritems():
            for r in self.discoveryrules:
                # If the rule was already successfully for this host, skip it
                if r in dh.matched_rules:
                    print 'We already apply the rule', r.get_name(), 'for the host', name
                    continue
                if r.is_matching_disco_datas(dh.data):
                    dh.matched_rules.append(r)
                    if name not in self.disco_matches:
                        self.disco_matches[name] = []
                    self.disco_matches[name].append(r)
                    print "Generating", name, r.writing_properties
            dh.update_properties()


    def is_allowing_runners(self, name):
        name = name.strip()

        # If we got no value, it's * by default
        if '*' in self.runners:
            return True

        #print self.runners
        #If we match the name, ok
        for r in self.runners:
            r_name = r.strip()
            #print "Look", r_name, name
            if r_name == name:
                return True

        # Not good, so not run this!
        return False


    def allowed_runners(self):
        return [r for r in self.discoveryruns if self.is_allowing_runners(r.get_name())]


    def launch_runners(self):
        allowed_runners = self.allowed_runners()

        if len(allowed_runners) == 0:
            print "ERROR : there is no matching runners selected!"
            return

        for r in allowed_runners:
            print "I'm launching", r.get_name(), "with a %d seconds timeout" % self.conf.runners_timeout
            r.launch(timeout=self.conf.runners_timeout)


    def wait_for_runners_ends(self):
        all_ok = False
        while not all_ok:
            '''
            all_ok = True
            for r in self.allowed_runners():
                if not r.is_finished():
                    #print "Check finished of", r.get_name()
                    r.check_finished()
                b = r.is_finished()
                if not b:
                    #print r.get_name(), "is not finished"
                    all_ok = False
            '''
            all_ok = self.is_all_ok()
            time.sleep(0.1)


    def is_all_ok(self):
        all_ok = True
        for r in self.allowed_runners():
            if not r.is_finished():
                #print "Check finished of", r.get_name()
                r.check_finished()
            b = r.is_finished()
            if not b:
                #print r.get_name(), "is not finished"
                all_ok = False
        return all_ok
        

    def get_runners_outputs(self):
        for r in self.allowed_runners():
            if r.is_finished():
                print r.discoveryrun_name, r.current_launch
                if r.current_launch.exit_status != 0:
                    print "Error on run"
        self.raw_disco_data = '\n'.join(r.get_output() for r in self.allowed_runners() if r.is_finished())
        if len(self.raw_disco_data) != 0:
            print "Got Raw disco data", self.raw_disco_data
        else:
            print "Got no data!"
            for r in self.allowed_runners():
                print "DBG", r.current_launch


    # Write all configuration we've got
    def write_config(self):
        # Store host to del in a separate array to remove them after look over items
        items_to_del = []
        still_duplicate_items = True
        managed_element =True
        while still_duplicate_items:
            # If we didn't work in the last loop, bail out
            if not managed_element:
                still_duplicate_items = False
            print "LOOP"
            managed_element = False
            for name in self.disco_data:
                if name in items_to_del:
                    continue
                managed_element = True
                print('Search same host to merge.')
                dha = self.disco_data[name]
                # Searching same host and update host macros
                for oname in self.disco_data:
                    dhb = self.disco_data[oname]
                    # When same host but different properties are detected
                    if dha.name == dhb.name and dha.properties != dhb.properties:
                        for (k,v) in dhb.properties.iteritems():
                            # Merge host macros if their properties are different
                            if k.startswith('_') and dha.properties.has_key(k) and dha.properties[k] != dhb.properties[k]:
                                dha.data[k] = dha.properties[k] + ',' + v
                                print('Merged host macro:', k, dha.properties[k])
                                items_to_del.append(oname)

                        print('Merged '+ oname + ' in ' + name)
                        dha.update_properties()
                    else:
                        still_duplicate_items = False
                        
        # Removing merged element
        for item in items_to_del:
            print('Deleting '+item)
            del self.disco_data[item]

        # New loop to reflect changes in self.disco_data since it isn't possible
        # to modify a dict object when reading it.
        for name in self.disco_data:
            print "Writing", name, "configuration"
            self.write_host_config(name)
            self.write_service_config(name)


    # We search for all rules of type host, and we merge them
    def write_host_config(self, host):
        dh = self.disco_data[host]

        d = dh.get_final_properties()
        final_host = dh.name

        print "Will generate an host", d
        # Maybe we do not got a directory output, but
        # a bdd one.
        if self.output_dir:
            self.write_host_config_to_file(final_host, d)

        # Maybe we want a database insert
        if self.db:
            self.write_host_config_to_db(final_host, d)

        
        if self.backend:
            self.backend.write_host_config_to_db(final_host, d)
            
        
    # Will wrote all properties/values of d for the host
    # in the file
    def write_host_config_to_file(self, host, d):
        p = os.path.join(self.output_dir, host)
        print "Want to create host path", p
        try:
            os.mkdir(p)
        except OSError, exp:
            # If directory already exist, it's not a problem
            if not exp.errno != '17':
                print "Cannot create the directory '%s' : '%s'" % (p, exp)
                return
        cfg_p = os.path.join(p, host+'.cfg')
        if os.path.exists(cfg_p) and not self.overwrite:
            print "The file '%s' already exists" % cfg_p
            return

        buf = self.get_cfg_bufer(d, 'host')

        # Ok, we create it so (or overwrite)
        try:
            fd = open(cfg_p, 'w')
            fd.write(buf)
            fd.close()
        except OSError, exp:
            print "Cannot create the file '%s' : '%s'" % (cfg_p, exp)
            return


    # Generate all service for a host
    def write_service_config(self, host):
        srv_rules = {}
        dh = self.disco_data[host]
        for r in dh.matched_rules:
            if r.creation_type == 'service':
                if 'service_description' in r.writing_properties:
                    desc = r.writing_properties['service_description']
                    if not desc in srv_rules:
                        srv_rules[desc] = []
                    srv_rules[desc].append(r)

        #print "Generate services for", host
        #print srv_rules
        for (desc, rules) in srv_rules.items():
            d = {'service_description' : desc, 'host_name' : host}
            for r in rules:
                d.update(r.writing_properties)
            print "Generating", desc, d

            # Maybe we do not got a directory output, but
            # a bdd one.
            if self.output_dir:
                self.write_service_config_to_file(host, desc, d)


    # Will wrote all properties/values of d for the host
    # in the file
    def write_service_config_to_file(self, host, desc, d):
        p = os.path.join(self.output_dir, host)

        # The host conf should already exist
        cfg_host_p = os.path.join(p, host+'.cfg')
        if not os.path.exists(cfg_host_p):
            print "No host configuration available, I bail out"
            return

        cfg_p = os.path.join(p, desc+'.cfg')
        if os.path.exists(cfg_p) and not self.overwrite:
            print "The file '%s' already exists" % cfg_p
            return

        buf = self.get_cfg_bufer(d, 'service')

        # Ok, we create it so (or overwrite)
        try:
            fd = open(cfg_p, 'w')
            fd.write(buf)
            fd.close()
        except OSError, exp:
            print "Cannot create the file '%s' : '%s'" % (cfg_p, exp)
            return

            
    # Create a define t { } with data in d
    def get_cfg_bufer(self, d, t):
        tab = ['define %s {' % t]
        for (key, value) in d.items():
            tab.append('  %s   %s' % (key, value))
        tab.append('}\n')
        return '\n'.join(tab)
        

    # Will wrote all properties/values of d for the host
    # in the database
    def write_host_config_to_db(self, host, d):
        table = None
        # Maybe we directly insert/enable the hosts,
        # or in the SkonfUI we want to go with an intermediate
        # table to select/enable only some
        if self.db_direct_insert:
            table = self.db.hosts
        else:
            table = self.db.discovered_hosts
        cur = table.find({'host_name': host})
        exists = cur.count() > 0
        if exists and not self.overwrite:
            print "The host '%s' already exists in the database table %s" % (host, table)
            return

        #It can be the same check if db_direct_insert but whatever
        if self.only_new_hosts:
            for t in [self.db.hosts, self.db.discovered_hosts]:
                r = table.find({'_id': host})
                if r.count() > 0:
                    print "This is not a new host on", self.db.hosts
                    return
                
        print "Saving in database", d
        d['_id'] = host
        d['_discovery_state'] = 'discovered'
        table.save(d)
        print "saved"
        del d['_id']

########NEW FILE########
__FILENAME__ = dispatcher
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
 This is the class of the dispatcher. Its role is to dispatch
 configurations to other elements like schedulers, reactionner,
 pollers, receivers and brokers. It is responsible for high availability part. If an
 element dies and the element type has a spare, it sends the config of the
 dead one to the spare
"""

import time
import random
import itertools

from shinken.util import alive_then_spare_then_deads
from shinken.log import logger

# Always initialize random :)
random.seed()


# Dispatcher Class
class Dispatcher:

    # Load all elements, set them as not assigned
    # and add them to elements, so loop will be easier :)
    def __init__(self, conf, arbiter):
        self.arbiter = arbiter
        # Pointer to the whole conf
        self.conf = conf
        self.realms = conf.realms
        # Direct pointer to important elements for us

        for sat_type in ('arbiters', 'schedulers', 'reactionners', 'brokers', 'receivers', 'pollers'):
            setattr(self, sat_type, getattr(self.conf, sat_type))

            # for each satellite, we look if current arbiter have a specific satellitemap value setted for this satellite
            # if so, we give this map to the satellite (used to build satellite URI later)
            if arbiter is None:
                continue

            key = sat_type[:-1] + '_name'  # i.e: schedulers -> scheduler_name
            for satellite in getattr(self, sat_type):
                sat_name = getattr(satellite, key)
                satellite.set_arbiter_satellitemap(arbiter.satellitemap.get(sat_name, {}))

        self.dispatch_queue = {'schedulers': [], 'reactionners': [],
                                'brokers': [], 'pollers': [], 'receivers': []}
        self.elements = []  # all elements, sched and satellites
        self.satellites = []  # only satellites not schedulers

        for cfg in self.conf.confs.values():
            cfg.is_assigned = False
            cfg.assigned_to = None
            # We try to remember each "push", so we
            # can know with configuration ids+flavor
            # if a satellite already got it or not :)
            cfg.push_flavor = 0

        # Add satellites in the good lists
        self.elements.extend(self.schedulers)

        # Others are in 2 lists
        self.elements.extend(self.reactionners)
        self.satellites.extend(self.reactionners)
        self.elements.extend(self.pollers)
        self.satellites.extend(self.pollers)
        self.elements.extend(self.brokers)
        self.satellites.extend(self.brokers)
        self.elements.extend(self.receivers)
        self.satellites.extend(self.receivers)

        # Some flag about dispatch need or not
        self.dispatch_ok = False
        self.first_dispatch_done = False

        # Prepare the satellites confs
        for satellite in self.satellites:
            satellite.prepare_for_conf()

        # Some properties must be given to satellites from global
        # configuration, like the max_plugins_output_length to pollers
        parameters = {'max_plugins_output_length': self.conf.max_plugins_output_length}
        for poller in self.pollers:
            poller.add_global_conf_parameters(parameters)

        # Reset need_conf for all schedulers.
        for s in self.schedulers:
            s.need_conf = True
        # Same for receivers
        for rec in self.receivers:
            rec.need_conf = True


    # checks alive elements
    def check_alive(self):
        for elt in self.elements:
            #print "Updating elements", elt.get_name(), elt.__dict__
            elt.update_infos()

            # Not alive needs new need_conf
            # and spare too if they do not have already a conf
            # REF: doc/shinken-scheduler-lost.png (1)
            if not elt.alive or hasattr(elt, 'conf') and elt.conf is None:
                elt.need_conf = True

        for arb in self.arbiters:
            # If not me, but not the master too
            if arb != self.arbiter and arb.spare:
                arb.update_infos()
                #print "Arb", arb.get_name(), "alive?", arb.alive, arb.__dict__


    # Check if all active items are still alive
    # the result goes into self.dispatch_ok
    # TODO: finish need conf
    def check_dispatch(self):
        # Check if the other arbiter has a conf, but only if I am a master
        for arb in self.arbiters:
            # If not me and I'm a master
            if arb != self.arbiter and self.arbiter and not self.arbiter.spare:
                if not arb.have_conf(self.conf.magic_hash):
                    if not hasattr(self.conf, 'whole_conf_pack'):
                        logger.error('CRITICAL: the arbiter try to send a configureion but it is not a MASTER one?? Look at your configuration.')
                        continue
                    arb.put_conf(self.conf.whole_conf_pack)
                    # Remind it that WE are the master here!
                    arb.do_not_run()
                else:
                    # Ok, it already has the conf. I remember that
                    # it does not have to run, I'm still alive!
                    arb.do_not_run()

        # We check for confs to be dispatched on alive scheds. If not dispatched, need dispatch :)
        # and if dispatch on a failed node, remove the association, and need a new dispatch
        for r in self.realms:
            for cfg_id in r.confs:
                push_flavor = r.confs[cfg_id].push_flavor
                sched = r.confs[cfg_id].assigned_to
                if sched is None:
                    if self.first_dispatch_done:
                        logger.info("Scheduler configuration %d is unmanaged!!" % cfg_id)
                    self.dispatch_ok = False
                else:
                    if not sched.alive:
                        self.dispatch_ok = False  # so we ask a new dispatching
                        logger.warning("Scheduler %s had the configuration %d but is dead, I am not happy." % (sched.get_name(), cfg_id))
                        sched.conf.assigned_to = None
                        sched.conf.is_assigned = False
                        sched.conf.push_flavor = 0
                        sched.push_flavor = 0
                        sched.conf = None
                    # Maybe the scheduler restarts, so is alive but without the conf we think it was managing
                    # so ask it what it is really managing, and if not, put the conf unassigned
                    if not sched.do_i_manage(cfg_id, push_flavor):
                        self.dispatch_ok = False  # so we ask a new dispatching
                        logger.warning("Scheduler %s did not managed its configuration %d,I am not happy." % (sched.get_name(), cfg_id))
                        if sched.conf:
                            sched.conf.assigned_to = None
                            sched.conf.is_assigned = False
                            sched.conf.push_flavor = 0
                        sched.push_flavor = 0
                        sched.need_conf = True
                        sched.conf = None
                    # Else: ok the conf is managed by a living scheduler

        # Maybe satellites are alive, but do not have a cfg yet.
        # I think so. It is not good. I ask a global redispatch for
        # the cfg_id I think is not correctly dispatched.
        for r in self.realms:
            for cfg_id in r.confs:
                push_flavor = r.confs[cfg_id].push_flavor
                try:
                    for kind in ('reactionner', 'poller', 'broker', 'receiver'):
                        # We must have the good number of satellite or we are not happy
                        # So we are sure to raise a dispatch every loop a satellite is missing
                        if len(r.to_satellites_managed_by[kind][cfg_id]) < r.get_nb_of_must_have_satellites(kind):
                            logger.warning("Missing satellite %s for configuration %d:" % (kind, cfg_id))

                            # TODO: less violent! Must only resent to who need?
                            # must be caught by satellite who sees that it already has the conf (hash)
                            # and do nothing
                            self.dispatch_ok = False  # so we will redispatch all
                            r.to_satellites_need_dispatch[kind][cfg_id] = True
                            r.to_satellites_managed_by[kind][cfg_id] = []
                        for satellite in r.to_satellites_managed_by[kind][cfg_id]:
                            # Maybe the sat was marked as not alive, but still in
                            # to_satellites_managed_by. That means that a new dispatch
                            # is needed
                            # Or maybe it is alive but I thought that this reactionner managed the conf
                            # and it doesn't. I ask a full redispatch of these cfg for both cases
                            
                            if push_flavor == 0 and satellite.alive:
                                logger.warning('[%s] The %s %s manage a unmanaged configuration' % (r.get_name(), kind, satellite.get_name()))
                                continue
                            if not satellite.alive or (satellite.reachable and not satellite.do_i_manage(cfg_id, push_flavor)):
                                logger.warning('[%s] The %s %s seems to be down, I must re-dispatch its role to someone else.' % (r.get_name(), kind, satellite.get_name()))
                                self.dispatch_ok = False  # so we will redispatch all
                                r.to_satellites_need_dispatch[kind][cfg_id] = True
                                r.to_satellites_managed_by[kind][cfg_id] = []
                # At the first pass, there is no cfg_id in to_satellites_managed_by
                except KeyError:
                    pass

        # Look for receivers. If they got conf, it's ok, if not, need a simple
        # conf
        for r in self.realms:
            for rec in r.receivers:
                # If the receiver does not have a conf, must got one :)
                if rec.reachable and not rec.got_conf():
                    self.dispatch_ok = False  # so we will redispatch all
                    rec.need_conf = True


    # Imagine a world where... oh no, wait...
    # Imagine a master got the conf and the network is down
    # a spare takes it (good :) ). Like the Empire, the master
    # strikes back! It was still alive! (like Elvis). It still got conf
    # and is running! not good!
    # Bad dispatch: a link that has a conf but I do not allow this
    # so I ask it to wait a new conf and stop kidding.
    def check_bad_dispatch(self):
        for elt in self.elements:
            if hasattr(elt, 'conf'):
                # If element has a conf, I do not care, it's a good dispatch
                # If dead: I do not ask it something, it won't respond..
                if elt.conf is None and elt.reachable:
                    # print "Ask", elt.get_name() , 'if it got conf'
                    if elt.have_conf():
                        logger.warning("The element %s have a conf and should not have one! I ask it to idle now" % elt.get_name())
                        elt.active = False
                        elt.wait_new_conf()
                        # I do not care about order not send or not. If not,
                        # The next loop will resent it
                    # else:
                    #    print "No conf"

        # I ask satellites which sched_id they manage. If I do not agree, I ask
        # them to remove it
        for satellite in self.satellites:
            kind = satellite.get_my_type()
            if satellite.reachable:
                cfg_ids = satellite.managed_confs  # what_i_managed()
                # I do not care about satellites that do nothing, they already
                # do what I want :)
                if len(cfg_ids) != 0:
                    id_to_delete = []
                    for cfg_id in cfg_ids:
                        # DBG print kind, ":", satellite.get_name(), "manage cfg id:", cfg_id
                        # Ok, we search for realms that have the conf
                        for r in self.realms:
                            if cfg_id in r.confs:
                                # Ok we've got the realm, we check its to_satellites_managed_by
                                # to see if reactionner is in. If not, we remove he sched_id for it
                                if not satellite in r.to_satellites_managed_by[kind][cfg_id]:
                                    id_to_delete.append(cfg_id)
                    # Maybe we removed all cfg_id of this reactionner
                    # We can put it idle, no active and wait_new_conf
                    if len(id_to_delete) == len(cfg_ids):
                        satellite.active = False
                        logger.info("I ask %s to wait a new conf" % satellite.get_name())
                        satellite.wait_new_conf()
                    else:
                        # It is not fully idle, just less cfg
                        for id in id_to_delete:
                            logger.info("I ask to remove configuration N%d from %s" % (id, satellite.get_name()))
                            satellite.remove_from_conf(id)


    # Make an ORDERED list of schedulers so we can
    # send them conf in this order for a specific realm
    def get_scheduler_ordered_list(self, r):
        # get scheds, alive and no spare first
        scheds = []
        for s in r.schedulers:
            scheds.append(s)

        # now the spare scheds of higher realms
        # they are after the sched of realm, so
        # they will be used after the spare of
        # the realm
        for higher_r in r.higher_realms:
            for s in higher_r.schedulers:
                if s.spare:
                    scheds.append(s)

        # Now we sort the scheds so we take master, then spare
        # the dead, but we do not care about them
        scheds.sort(alive_then_spare_then_deads)
        scheds.reverse()  # pop is last, I need first

        print_sched = [s.get_name() for s in scheds]
        print_sched.reverse()

        return scheds


    # Manage the dispatch
    # REF: doc/shinken-conf-dispatching.png (3)
    def dispatch(self):
        # Ok, we pass at least one time in dispatch, so now errors are True errors
        self.first_dispatch_done = True

        # If no needed to dispatch, do not dispatch :)
        if not self.dispatch_ok:
            for r in self.realms:
                conf_to_dispatch = [cfg for cfg in r.confs.values() if not cfg.is_assigned]
                nb_conf = len(conf_to_dispatch)
                if nb_conf > 0:
                    logger.info("Dispatching Realm %s" % r.get_name())
                    logger.info('[%s] Dispatching %d/%d configurations' % (r.get_name(), nb_conf, len(r.confs)))

                # Now we get in scheds all scheduler of this realm and upper so
                # we will send them conf (in this order)
                scheds = self.get_scheduler_ordered_list(r)

                if nb_conf > 0:
                    print_string = '[%s] Schedulers order: %s' % (r.get_name(), ','.join([s.get_name() for s in scheds]))
                    logger.info(print_string)

                # Try to send only for alive members
                scheds = [s for s in scheds if s.alive]

                # Now we do the real job
                # every_one_need_conf = False
                for conf in conf_to_dispatch:
                    logger.info('[%s] Dispatching configuration %s' % (r.get_name(), conf.id))

                    # If there is no alive schedulers, not good...
                    if len(scheds) == 0:
                        logger.info('[%s] but there a no alive schedulers in this realm!' % r.get_name())

                    # we need to loop until the conf is assigned
                    # or when there are no more schedulers available
                    while True:
                        try:
                            sched = scheds.pop()
                        except IndexError:  # No more schedulers.. not good, no loop
                            # need_loop = False
                            # The conf does not need to be dispatch
                            cfg_id = conf.id
                            for kind in ('reactionner', 'poller', 'broker', 'receiver'):
                                r.to_satellites[kind][cfg_id] = None
                                r.to_satellites_need_dispatch[kind][cfg_id] = False
                                r.to_satellites_managed_by[kind][cfg_id] = []
                            break

                        logger.info('[%s] Trying to send conf %d to scheduler %s' % (r.get_name(), conf.id, sched.get_name()))
                        if not sched.need_conf:
                            logger.info('[%s] The scheduler %s do not need conf, sorry' % (r.get_name(), sched.get_name()))
                            continue

                        # We tag conf with the instance_name = scheduler_name
                        instance_name = sched.scheduler_name
                        # We give this configuration a new 'flavor'
                        conf.push_flavor = random.randint(1, 1000000)
                        # REF: doc/shinken-conf-dispatching.png (3)
                        # REF: doc/shinken-scheduler-lost.png (2)
                        override_conf = sched.get_override_configuration()
                        satellites_for_sched = r.get_satellites_links_for_scheduler()
                        s_conf = r.serialized_confs[conf.id]
                        # Prepare the conf before sending it
                        conf_package = {'conf': s_conf, 'override_conf': override_conf,
                                        'modules': sched.modules, 'satellites': satellites_for_sched,
                                        'instance_name': sched.scheduler_name, 'push_flavor': conf.push_flavor,
                                        'skip_initial_broks': sched.skip_initial_broks,
                                        }

                        t1 = time.time()
                        is_sent = sched.put_conf(conf_package)
                        logger.debug("Conf is sent in %d" % (time.time() - t1))
                        if not is_sent:
                            logger.warning('[%s] configuration dispatching error for scheduler %s' % (r.get_name(), sched.get_name()))
                            continue
                        
                        logger.info('[%s] Dispatch OK of conf in scheduler %s' % (r.get_name(), sched.get_name()))
                        
                        sched.conf = conf
                        sched.push_flavor = conf.push_flavor
                        sched.need_conf = False
                        conf.is_assigned = True
                        conf.assigned_to = sched
                        
                        # We update all data for this scheduler
                        sched.managed_confs = {conf.id: conf.push_flavor}
                        
                        # Now we generate the conf for satellites:
                        cfg_id = conf.id
                        for kind in ('reactionner', 'poller', 'broker', 'receiver'):
                            r.to_satellites[kind][cfg_id] = sched.give_satellite_cfg()
                            r.to_satellites_need_dispatch[kind][cfg_id] = True
                            r.to_satellites_managed_by[kind][cfg_id] = []

                        # Ok, the conf is dispatched, no more loop for this
                        # configuration
                        break

            # We pop conf to dispatch, so it must be no more conf...
            conf_to_dispatch = [cfg for cfg in self.conf.confs.values() if not cfg.is_assigned]
            nb_missed = len(conf_to_dispatch)
            if nb_missed > 0:
                logger.warning("All schedulers configurations are not dispatched, %d are missing" % nb_missed)
            else:
                logger.info("OK, all schedulers configurations are dispatched :)")
                self.dispatch_ok = True

            # Sched without conf in a dispatch ok are set to no need_conf
            # so they do not raise dispatch where no use
            if self.dispatch_ok:
                for sched in self.schedulers.items.values():
                    if sched.conf is None:
                        # print "Tagging sched", sched.get_name(), "so it do not ask anymore for conf"
                        sched.need_conf = False

            arbiters_cfg = {}
            for arb in self.arbiters:
                arbiters_cfg[arb.id] = arb.give_satellite_cfg()

            # We put the satellites conf with the "new" way so they see only what we want
            for r in self.realms:
                for cfg in r.confs.values():
                    cfg_id = cfg.id
                    # flavor if the push number of this configuration send to a scheduler
                    flavor = cfg.push_flavor
                    for kind in ('reactionner', 'poller', 'broker', 'receiver'):
                        if r.to_satellites_need_dispatch[kind][cfg_id]:
                            cfg_for_satellite_part = r.to_satellites[kind][cfg_id]

                            # make copies of potential_react list for sort
                            satellites = []
                            for satellite in r.get_potential_satellites_by_type(kind):
                                satellites.append(satellite)
                            satellites.sort(alive_then_spare_then_deads)

                            # Only keep alive Satellites and reachable ones
                            satellites = [s for s in satellites if s.alive and s.reachable]

                            # If we got a broker, we make the list to pop a new
                            # item first for each scheduler, so it will smooth the load
                            # But the spare must stay at the end ;)
                            # WARNING : skip this if we are in a complet broker link realm
                            if kind == "broker" and not r.broker_complete_links:
                                nospare = [s for s in satellites if not s.spare]
                                # Should look over the list, not over
                                if len(nospare) != 0:
                                    idx = cfg_id % len(nospare)
                                    #print "No spare", nospare
                                    spares = [s for s in satellites if s.spare]
                                    #print "Spare", spares
                                    #print "Got 1", nospare[idx:]
                                    #print "Got 2", nospare[:-idx+1]
                                    new_satellites = nospare[idx:]
                                    for _b in nospare[:-idx+1]:
                                        if _b not in new_satellites:
                                            new_satellites.append(_b)
                                    #new_satellites.extend(nospare[:-idx+1])
                                    #print "New satellites", cfg_id, new_satellites
                                    #for s in new_satellites:
                                    #    print "New satellites", cfg_id, s.get_name()
                                    satellites = new_satellites
                                    satellites.extend(spares)

                            # Dump the order where we will send conf
                            satellite_string = "[%s] Dispatching %s satellite with order: " % (r.get_name(), kind)
                            for satellite in satellites:
                                satellite_string += '%s (spare:%s), ' % (satellite.get_name(), str(satellite.spare))
                            logger.info(satellite_string)
                            
                            # Now we dispatch cfg to every one ask for it
                            nb_cfg_sent = 0
                            for satellite in satellites:
                                # Send only if we need, and if we can
                                if nb_cfg_sent < r.get_nb_of_must_have_satellites(kind) and satellite.alive:
                                    satellite.cfg['schedulers'][cfg_id] = cfg_for_satellite_part
                                    if satellite.manage_arbiters:
                                        satellite.cfg['arbiters'] = arbiters_cfg
                                    
                                    # Brokers should have poller/reactionners links too
                                    if kind == "broker":
                                        r.fill_broker_with_poller_reactionner_links(satellite)

                                    is_sent = False
                                    # Maybe this satellite already got this configuration, so skip it
                                    if satellite.do_i_manage(cfg_id, flavor):
                                        logger.info('[%s] Skipping configuration %d send to the %s %s: it already got it' % (r.get_name(), cfg_id, kind, satellite.get_name()))
                                        is_sent = True
                                    else:  # ok, it really need it :)
                                        logger.info('[%s] Trying to send configuration to %s %s' % (r.get_name(), kind, satellite.get_name()))
                                        is_sent = satellite.put_conf(satellite.cfg)

                                    if is_sent:
                                        satellite.active = True
                                        logger.info('[%s] Dispatch OK of configuration %s to %s %s' % (r.get_name(), cfg_id, kind, satellite.get_name()))
                                        # We change the satellite configuration, update our data
                                        satellite.known_conf_managed_push(cfg_id, flavor)
                                        
                                        nb_cfg_sent += 1
                                        r.to_satellites_managed_by[kind][cfg_id].append(satellite)

                                        # If we got a broker, the conf_id must be sent to only ONE
                                        # broker in a classic realm.
                                        if kind == "broker" and not r.broker_complete_links:
                                            break
                                        
                                        #If receiver, we must send the hostnames of this configuration
                                        if kind == 'receiver':
                                            hnames = [h.get_name() for h in cfg.hosts]
                                            logger.debug("[%s] Sending %s hostnames to the receiver %s" % (r.get_name(), len(hnames), satellite.get_name()))
                                            satellite.push_host_names(cfg_id, hnames)
                            # else:
                            #    #I've got enough satellite, the next ones are considered spares
                            if nb_cfg_sent == r.get_nb_of_must_have_satellites(kind):
                                logger.info("[%s] OK, no more %s sent need" % (r.get_name(), kind))
                                r.to_satellites_need_dispatch[kind][cfg_id] = False

            # And now we dispatch receivers. It's easier, they need ONE conf
            # in all their life :)
            for r in self.realms:
                for rec in r.receivers:
                    if rec.need_conf:
                        logger.info('[%s] Trying to send configuration to receiver %s' % (r.get_name(), rec.get_name()))
                        is_sent = False
                        if rec.reachable:
                            is_sent = rec.put_conf(rec.cfg)
                        else:
                            logger.info('[%s] Skyping configuration sent to offline receiver %s' % (r.get_name(), rec.get_name()))
                        if is_sent:
                            rec.active = True
                            rec.need_conf = False
                            logger.info('[%s] Dispatch OK of configuration to receiver %s' % (r.get_name(), rec.get_name()))
                        else:
                            logger.error('[%s] Dispatching failed for receiver %s' % (r.get_name(), rec.get_name()))

########NEW FILE########
__FILENAME__ = downtime
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import datetime, time
from shinken.comment import Comment
from shinken.property import BoolProp, IntegerProp, StringProp
from shinken.brok import Brok
from shinken.log import logger

""" Schedules downtime for a specified service. If the "fixed" argument is set
 to one (1), downtime will start and end at the times specified by the
 "start" and "end" arguments.
 Otherwise, downtime will begin between the "start" and "end" times and last
 for "duration" seconds. The "start" and "end" arguments are specified
 in time_t format (seconds since the UNIX epoch). The specified service
 downtime can be triggered by another downtime entry if the "trigger_id"
 is set to the ID of another scheduled downtime entry.
 Set the "trigger_id" argument to zero (0) if the downtime for the
 specified service should not be triggered by another downtime entry.

"""
class Downtime:
    id = 1

    # Just to list the properties we will send as pickle
    # so to others daemons, so all but NOT REF
    properties = {
        'activate_me':  StringProp(default=[]),
        'entry_time':   IntegerProp(default=0,  fill_brok=['full_status']),
        'fixed':        BoolProp(default=True,  fill_brok=['full_status']),
        'start_time':   IntegerProp(default=0,  fill_brok=['full_status']),
        'duration':     IntegerProp(default=0,  fill_brok=['full_status']),
        'trigger_id':   IntegerProp(default=0),
        'end_time':     IntegerProp(default=0,  fill_brok=['full_status']),
        'real_end_time': IntegerProp(default=0),
        'author':       StringProp(default='',  fill_brok=['full_status']),
        'comment':      StringProp(default=''),
        'is_in_effect': BoolProp(default=False),
        'has_been_triggered': BoolProp(default=False),
        'can_be_deleted': BoolProp(default=False),

        # TODO: find a very good way to handle the downtime "ref".
        # ref must effectively not be in properties because it points
        # onto a real object.
        #'ref': None
    }

    def __init__(self, ref, start_time, end_time, fixed, trigger_id, duration, author, comment):
        now = datetime.datetime.now()
        self.id = int(time.mktime(now.timetuple())*1e6 + now.microsecond)
        self.__class__.id = self.id + 1
        self.ref = ref  # pointer to srv or host we are apply
        self.activate_me = []  # The other downtimes i need to activate
        self.entry_time = int(time.time())
        self.fixed = fixed
        self.start_time = start_time
        self.duration = duration
        self.trigger_id = trigger_id
        if self.trigger_id != 0:  # triggered plus fixed makes no sense
            self.fixed = False
        self.end_time = end_time
        if fixed:
            self.duration = end_time - start_time
        # This is important for flexible downtimes. Here start_time and
        # end_time mean: in this time interval it is possible to trigger
        # the beginning of the downtime which lasts for duration.
        # Later, when a non-ok event happens, real_end_time will be
        # recalculated from now+duration
        # end_time will be displayed in the web interface, but real_end_time
        # is used internally
        self.real_end_time = end_time
        self.author = author
        self.comment = comment
        self.is_in_effect = False    # fixed: start_time has been reached, flexible: non-ok checkresult
        self.has_been_triggered = False  # another downtime has triggered me
        self.can_be_deleted = False
        self.add_automatic_comment()

    def __str__(self):
        if self.is_in_effect == True:
            active = "active"
        else:
            active = "inactive"
        if self.fixed == True:
            type = "fixed"
        else:
            type = "flexible"
        return "%s %s Downtime id=%d %s - %s" % (active, type, self.id, time.ctime(self.start_time), time.ctime(self.end_time))

    def trigger_me(self, other_downtime):
        self.activate_me.append(other_downtime)

    def in_scheduled_downtime(self):
        return self.is_in_effect

    # The referenced host/service object enters now a (or another) scheduled
    # downtime. Write a log message only if it was not already in a downtime
    def enter(self):
        res = []
        self.is_in_effect = True
        if self.fixed == False:
            now = time.time()
            self.real_end_time = now + self.duration
        if self.ref.scheduled_downtime_depth == 0:
            self.ref.raise_enter_downtime_log_entry()
            self.ref.create_notifications('DOWNTIMESTART')
        self.ref.scheduled_downtime_depth += 1
        self.ref.in_scheduled_downtime = True
        for dt in self.activate_me:
            res.extend(dt.enter())
        return res

    # The end of the downtime was reached.
    def exit(self):
        res = []
        if self.is_in_effect == True:
            # This was a fixed or a flexible+triggered downtime
            self.is_in_effect = False
            self.ref.scheduled_downtime_depth -= 1
            if self.ref.scheduled_downtime_depth == 0:
                self.ref.raise_exit_downtime_log_entry()
                self.ref.create_notifications('DOWNTIMEEND')
                self.ref.in_scheduled_downtime = False
        else:
            # This was probably a flexible downtime which was not triggered
            # In this case it silently disappears
            pass
        self.del_automatic_comment()
        self.can_be_deleted = True
        # when a downtime ends and the service was critical
        # a notification is sent with the next critical check
        # So we should set a flag here which signals consume_result
        # to send a notification
        self.ref.in_scheduled_downtime_during_last_check = True
        return res

    # A scheduled downtime was prematurely canceled
    def cancel(self):
        res = []
        self.is_in_effect = False
        self.ref.scheduled_downtime_depth -= 1
        if self.ref.scheduled_downtime_depth == 0:
            self.ref.raise_cancel_downtime_log_entry()
            self.ref.in_scheduled_downtime = False
        self.del_automatic_comment()
        self.can_be_deleted = True
        self.ref.in_scheduled_downtime_during_last_check = True
        # Nagios does not notify on canceled downtimes
        #res.extend(self.ref.create_notifications('DOWNTIMECANCELLED'))
        # Also cancel other downtimes triggered by me
        for dt in self.activate_me:
            res.extend(dt.cancel())
        return res

    # Scheduling a downtime creates a comment automatically
    def add_automatic_comment(self):
        if self.fixed == True:
            text = "This %s has been scheduled for fixed downtime from %s to %s. Notifications for the %s will not be sent out during that time period." % (self.ref.my_type, time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.start_time)), time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.end_time)), self.ref.my_type)
        else:
            hours, remainder = divmod(self.duration, 3600)
            minutes, seconds = divmod(remainder, 60)
            text = "This %s has been scheduled for flexible downtime starting between %s and %s and lasting for a period of %d hours and %d minutes.  Notifications for the %s will not be sent out during that time period." % (self.ref.my_type, time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.start_time)), time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.end_time)), hours, minutes, self.ref.my_type)
        if self.ref.my_type == 'host':
            comment_type = 1
        else:
            comment_type = 2
        c = Comment(self.ref, False, "(Nagios Process)", text, comment_type, 2, 0, False, 0)
        self.comment_id = c.id
        self.extra_comment = c
        self.ref.add_comment(c)

    def del_automatic_comment(self):
        # Extra comment can be None if we load it from a old version of Shinken
        # TODO: remove it in a future version when every one got upgrade
        if self.extra_comment is not None:
            self.extra_comment.can_be_deleted = True
        #self.ref.del_comment(self.comment_id)


    # Fill data with info of item by looking at brok_type
    # in props of properties or running_properties
    def fill_data_brok_from(self, data, brok_type):
        cls = self.__class__
        # Now config properties
        for prop, entry in cls.properties.items():
            if hasattr(prop, 'fill_brok'):
                if brok_type in entry['fill_brok']:
                    data[prop] = getattr(self, prop)

    # Get a brok with initial status
    def get_initial_status_brok(self):
        data = {'id': self.id}

        self.fill_data_brok_from(data, 'full_status')
        b = Brok('downtime_raise', data)
        return b

    # Call by pickle for dataify the downtime
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)
        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__

        # Maybe it's not a dict but a list like in the old 0.4 format
        # so we should call the 0.4 function for it
        if isinstance(state, list):
            self.__setstate_deprecated__(state)
            return

        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])

        if self.id >= cls.id:
            cls.id = self.id + 1

    # This function is DEPRECATED and will be removed in a future version of
    # Shinken. It should not be useful any more after a first load/save pass.

    # Inversed function of getstate
    def __setstate_deprecated__(self, state):
        cls = self.__class__
        # Check if the len of this state is like the previous,
        # if not, we will do errors!
        # -1 because of the 'id' prop
        if len(cls.properties) != (len(state) - 1):
            logger.info("Passing downtime")
            return

        self.id = state.pop()
        for prop in cls.properties:
            val = state.pop()
            setattr(self, prop, val)
        if self.id >= cls.id:
            cls.id = self.id + 1

########NEW FILE########
__FILENAME__ = easter
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.log import logger

def episode_iv():
    hst = 'towel.blinkenlights.nl'

    from telnetlib import Telnet

    t = Telnet(hst)
    while True:
        buf = t.read_until('mesfesses', 0.1)
        logger.info(buf)


def perdu():
    import urllib
    f = urllib.urlopen("http://www.perdu.com")
    logger.info(f.read())


def myip():
    import urllib
    f = urllib.urlopen("http://whatismyip.org/")
    logger.info(f.read())


def naheulbeuk():
    import os
    import urllib2
    from cStringIO import StringIO

    from PIL import Image
    import aalib

    if os.getenv('TERM') == 'linux':
        screen = aalib.LinuxScreen
    else:
        screen = aalib.AnsiScreen
    screen = screen(width=128, height=128)
    fp = StringIO(urllib2.urlopen('http://www.penofchaos.com/warham/bd/images/NBK-win7portrait-Nain02.JPG').read())
    image = Image.open(fp).convert('L').resize(screen.virtual_size)
    screen.put_image((0, 0), image)
    logger.info(screen.render())
                    


def dark():
    r"""
                       .-.
                      |_:_|
                     /(_Y_)\
                    ( \/M\/ )
 '.               _.'-/'-'\-'._
   ':           _/.--'[[[[]'--.\_
     ':        /_'  : |::"| :  '.\
       ':     //   ./ |oUU| \.'  :\
         ':  _:'..' \_|___|_/ :   :|
           ':.  .'  |_[___]_|  :.':\
            [::\ |  :  | |  :   ; : \
             '-'   \/'.| |.' \  .;.' |
             |\_    \  '-'   :       |
             |  \    \ .:    :   |   |
             |   \    | '.   :    \  |
             /       \   :. .;       |
            /     |   |  :__/     :  \\
           |  |   |    \:   | \   |   ||
          /    \  : :  |:   /  |__|   /|
      snd |     : : :_/_|  /'._\  '--|_\
          /___.-/_|-'   \  \
                         '-'

"""
    logger.info(dark.__doc__)


def get_coffee():
    r"""

                        (
                          )     (
                           ___...(-------)-....___
                       .-""       )    (          ""-.
                 .-'``'|-._             )         _.-|
                /  .--.|   `""---...........---""`   |
               /  /    |                             |
               |  |    |                             |
                \  \   |                             |
                 `\ `\ |                             |
                   `\ `|                             |
                   _/ /\                             /
                  (__/  \                           /
               _..---""` \                         /`""---.._
            .-'           \                       /          '-.
           :               `-.__             __.-'              :
           :                  ) ""---...---"" (                 :
            '._               `"--...___...--"`              _.'
          jgs \""--..__                              __..--""/
               '._     "'"----.....______.....----"'"     _.'
                  `""--..,,_____            _____,,..--""`
                                `"'"----"'"`


"""
    logger.info(get_coffee.__doc__)


########NEW FILE########
__FILENAME__ = eventhandler
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

from action import Action
from shinken.property import IntegerProp, StringProp, FloatProp
from shinken.autoslots import AutoSlots

""" TODO: Add some comment about this class for the doc"""
class EventHandler(Action):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    my_type = 'eventhandler'

    properties = {
        'is_a':           StringProp(default='eventhandler'),
        'type':           StringProp(default=''),
        '_in_timeout':    StringProp(default=False),
        'status':         StringProp(default=''),
        'exit_status':    StringProp(default=3),
        'output':         StringProp(default=''),
        'long_output':    StringProp(default=''),
        't_to_go':        StringProp(default=0),
        'check_time':     StringProp(default=0),
        'execution_time': FloatProp(default=0),
        'u_time':         FloatProp(default=0.0),
        's_time':         FloatProp(default=0.0),
        'env':            StringProp(default={}),
        'perf_data':      StringProp(default=''),
        'sched_id':       IntegerProp(default=0),
        'timeout':        IntegerProp(default=10),
        'check_time':     IntegerProp(default=0),
        'command':        StringProp(default=''),
        'module_type':    StringProp(default='fork'),
        'worker':         StringProp(default='none'),
        'reactionner_tag':     StringProp(default='None'),
    }

    # id = 0  #Is common to Actions
    def __init__(self, command, id=None, ref=None, timeout=10, env={}, \
                     module_type='fork', reactionner_tag='None'):
        self.is_a = 'eventhandler'
        self.type = ''
        self.status = 'scheduled'
        if id is None:  # id != None is for copy call only
            self.id = Action.id
            Action.id += 1
        self.ref = ref
        self._in_timeout = False
        self.timeout = timeout
        self.exit_status = 3
        self.command = command
        self.output = ''
        self.long_output = ''
        self.t_to_go = time.time()
        self.check_time = 0
        self.execution_time = 0
        self.u_time = 0
        self.s_time = 0
        self.perf_data = ''
        self.env = {}
        self.module_type = module_type
        self.worker = 'none'
        self.reactionner_tag = reactionner_tag


    # return a copy of the check but just what is important for execution
    # So we remove the ref and all
    def copy_shell(self):
        # We create a dummy check with nothing in it, just defaults values
        return self.copy_shell__(EventHandler('', id=self.id))

    def get_return_from(self, e):
        self.exit_status = e.exit_status
        self.output = e.output
        self.long_output = getattr(e, 'long_output', '')
        self.check_time = e.check_time
        self.execution_time = getattr(e, 'execution_time', 0.0)
        self.perf_data = getattr(e, 'perf_data', '')

    # <TMI!!>
    def get_outputs(self, out, max_plugins_output_length):
        elts = out.split('\n')
        # For perf data
        elts_line1 = elts[0].split('|')
        # First line before | is output
        self.output = elts_line1[0]
        # After | is perfdata
        if len(elts_line1) > 1:
            self.perf_data = elts_line1[1]
        # The others lines are long_output
        if len(elts) > 1:
            self.long_output = '\n'.join(elts[1:])
    # </TMI!!>

    def is_launchable(self, t):
        return t >= self.t_to_go

    def __str__(self):
        return "Check %d status:%s command:%s" % (self.id, self.status, self.command)

    def get_id(self):
        return self.id

    # Call by pickle to dataify the comment
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)

        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__
        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])
        if not hasattr(self, 'worker'):
            self.worker = 'none'
        if not getattr(self, 'module_type', None):
            self.module_type = 'fork'
        # s_time and u_time are added between 1.2 and 1.4
        if not hasattr(self, 'u_time'):
            self.u_time = 0
            self.s_time = 0

########NEW FILE########
__FILENAME__ = external_command
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import time

from shinken.util import to_int, to_bool, split_semicolon
from shinken.downtime import Downtime
from shinken.contactdowntime import ContactDowntime
from shinken.comment import Comment
from shinken.commandcall import CommandCall
from shinken.log import logger, console_logger
from shinken.pollerlink import PollerLink
from shinken.eventhandler import EventHandler

MODATTR_NONE = 0
MODATTR_NOTIFICATIONS_ENABLED = 1
MODATTR_ACTIVE_CHECKS_ENABLED = 2
MODATTR_PASSIVE_CHECKS_ENABLED = 4
MODATTR_EVENT_HANDLER_ENABLED = 8
MODATTR_FLAP_DETECTION_ENABLED = 16
MODATTR_FAILURE_PREDICTION_ENABLED = 32
MODATTR_PERFORMANCE_DATA_ENABLED = 64
MODATTR_OBSESSIVE_HANDLER_ENABLED = 128
MODATTR_EVENT_HANDLER_COMMAND = 256
MODATTR_CHECK_COMMAND = 512
MODATTR_NORMAL_CHECK_INTERVAL = 1024
MODATTR_RETRY_CHECK_INTERVAL = 2048
MODATTR_MAX_CHECK_ATTEMPTS = 4096
MODATTR_FRESHNESS_CHECKS_ENABLED = 8192
MODATTR_CHECK_TIMEPERIOD = 16384
MODATTR_CUSTOM_VARIABLE = 32768
MODATTR_NOTIFICATION_TIMEPERIOD = 65536



""" TODO: Add some comment about this class for the doc"""
class ExternalCommand:
    my_type = 'externalcommand'

    def __init__(self, cmd_line):
        self.cmd_line = cmd_line


""" TODO: Add some comment about this class for the doc"""
class ExternalCommandManager:

    commands = {
        'CHANGE_CONTACT_MODSATTR': {'global': True, 'args': ['contact', None]},
        'CHANGE_CONTACT_MODHATTR': {'global': True, 'args': ['contact', None]},
        'CHANGE_CONTACT_MODATTR': {'global': True, 'args': ['contact', None]},
        'CHANGE_CONTACT_HOST_NOTIFICATION_TIMEPERIOD': {'global': True, 'args': ['contact', 'time_period']},
        'ADD_SVC_COMMENT': {'global': False, 'args': ['service', 'to_bool', 'author', None]},
        'ADD_HOST_COMMENT': {'global': False, 'args': ['host', 'to_bool', 'author', None]},
        'ACKNOWLEDGE_SVC_PROBLEM': {'global': False, 'args': ['service', 'to_int', 'to_bool', 'to_bool', 'author', None]},
        'ACKNOWLEDGE_HOST_PROBLEM': {'global': False, 'args': ['host', 'to_int', 'to_bool', 'to_bool', 'author', None]},
        'ACKNOWLEDGE_SVC_PROBLEM_EXPIRE': {'global': False, 'args': ['service', 'to_int', 'to_bool', 'to_bool', 'to_int', 'author', None]},
        'ACKNOWLEDGE_HOST_PROBLEM_EXPIRE': {'global': False, 'args': ['host', 'to_int', 'to_bool', 'to_bool', 'to_int', 'author', None]},
        'CHANGE_CONTACT_SVC_NOTIFICATION_TIMEPERIOD': {'global': True, 'args': ['contact', 'time_period']},
        'CHANGE_CUSTOM_CONTACT_VAR': {'global': True, 'args': ['contact', None, None]},
        'CHANGE_CUSTOM_HOST_VAR': {'global': False, 'args': ['host', None, None]},
        'CHANGE_CUSTOM_SVC_VAR': {'global': False, 'args': ['service', None, None]},
        'CHANGE_GLOBAL_HOST_EVENT_HANDLER': {'global': True, 'args': ['command']},
        'CHANGE_GLOBAL_SVC_EVENT_HANDLER': {'global': True, 'args': ['command']},
        'CHANGE_HOST_CHECK_COMMAND': {'global': False, 'args': ['host', 'command']},
        'CHANGE_HOST_CHECK_TIMEPERIOD': {'global': False, 'args': ['host', 'time_period']},
        'CHANGE_HOST_EVENT_HANDLER': {'global': False, 'args': ['host', 'command']},
        'CHANGE_HOST_MODATTR': {'global': False, 'args': ['host', 'to_int']},
        'CHANGE_MAX_HOST_CHECK_ATTEMPTS': {'global': False, 'args': ['host', 'to_int']},
        'CHANGE_MAX_SVC_CHECK_ATTEMPTS': {'global': False, 'args': ['service', 'to_int']},
        'CHANGE_NORMAL_HOST_CHECK_INTERVAL': {'global': False, 'args': ['host', 'to_int']},
        'CHANGE_NORMAL_SVC_CHECK_INTERVAL': {'global': False, 'args': ['service', 'to_int']},
        'CHANGE_RETRY_HOST_CHECK_INTERVAL': {'global': False, 'args': ['service', 'to_int']},
        'CHANGE_RETRY_SVC_CHECK_INTERVAL': {'global': False, 'args': ['service', 'to_int']},
        'CHANGE_SVC_CHECK_COMMAND': {'global': False, 'args': ['service', 'command']},
        'CHANGE_SVC_CHECK_TIMEPERIOD': {'global': False, 'args': ['service', 'time_period']},
        'CHANGE_SVC_EVENT_HANDLER': {'global': False, 'args': ['service', 'command']},
        'CHANGE_SVC_MODATTR': {'global': False, 'args': ['service', 'to_int']},
        'CHANGE_SVC_NOTIFICATION_TIMEPERIOD': {'global': False, 'args': ['service', 'time_period']},
        'DELAY_HOST_NOTIFICATION': {'global': False, 'args': ['host', 'to_int']},
        'DELAY_SVC_NOTIFICATION': {'global': False, 'args': ['service', 'to_int']},
        'DEL_ALL_HOST_COMMENTS': {'global': False, 'args': ['host']},
        'DEL_ALL_HOST_DOWNTIMES': {'global': False, 'args': ['host']},
        'DEL_ALL_SVC_COMMENTS': {'global': False, 'args': ['service']},
        'DEL_ALL_SVC_DOWNTIMES': {'global': False, 'args': ['service']},
        'DEL_CONTACT_DOWNTIME': {'global': True, 'args': ['to_int']},
        'DEL_HOST_COMMENT': {'global': True, 'args': ['to_int']},
        'DEL_HOST_DOWNTIME': {'global': True, 'args': ['to_int']},
        'DEL_SVC_COMMENT': {'global': True, 'args': ['to_int']},
        'DEL_SVC_DOWNTIME': {'global': True, 'args': ['to_int']},
        'DISABLE_ALL_NOTIFICATIONS_BEYOND_HOST': {'global': False, 'args': ['host']},
        'DISABLE_CONTACTGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['contact_group']},
        'DISABLE_CONTACTGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['contact_group']},
        'DISABLE_CONTACT_HOST_NOTIFICATIONS': {'global': True, 'args': ['contact']},
        'DISABLE_CONTACT_SVC_NOTIFICATIONS': {'global': True, 'args': ['contact']},
        'DISABLE_EVENT_HANDLERS': {'global': True, 'args': []},
        'DISABLE_FAILURE_PREDICTION': {'global': True, 'args': []},
        'DISABLE_FLAP_DETECTION': {'global': True, 'args': []},
        'DISABLE_HOSTGROUP_HOST_CHECKS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOSTGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOSTGROUP_PASSIVE_HOST_CHECKS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOSTGROUP_PASSIVE_SVC_CHECKS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOSTGROUP_SVC_CHECKS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOSTGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['host_group']},
        'DISABLE_HOST_AND_CHILD_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'DISABLE_HOST_CHECK': {'global': False, 'args': ['host']},
        'DISABLE_HOST_EVENT_HANDLER': {'global': False, 'args': ['host']},
        'DISABLE_HOST_FLAP_DETECTION': {'global': False, 'args': ['host']},
        'DISABLE_HOST_FRESHNESS_CHECKS': {'global': True, 'args': []},
        'DISABLE_HOST_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'DISABLE_HOST_SVC_CHECKS': {'global': False, 'args': ['host']},
        'DISABLE_HOST_SVC_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'DISABLE_NOTIFICATIONS': {'global': True, 'args': []},
        'DISABLE_PASSIVE_HOST_CHECKS': {'global': False, 'args': ['host']},
        'DISABLE_PASSIVE_SVC_CHECKS': {'global': False, 'args': ['service']},
        'DISABLE_PERFORMANCE_DATA': {'global': True, 'args': []},
        'DISABLE_SERVICEGROUP_HOST_CHECKS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICEGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICEGROUP_SVC_CHECKS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICEGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['service_group']},
        'DISABLE_SERVICE_FLAP_DETECTION': {'global': False, 'args': ['service']},
        'DISABLE_SERVICE_FRESHNESS_CHECKS': {'global': True, 'args': []},
        'DISABLE_SVC_CHECK': {'global': False, 'args': ['service']},
        'DISABLE_SVC_EVENT_HANDLER': {'global': False, 'args': ['service']},
        'DISABLE_SVC_FLAP_DETECTION': {'global': False, 'args': ['service']},
        'DISABLE_SVC_NOTIFICATIONS': {'global': False, 'args': ['service']},
        'ENABLE_ALL_NOTIFICATIONS_BEYOND_HOST': {'global': False, 'args': ['host']},
        'ENABLE_CONTACTGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['contact_group']},
        'ENABLE_CONTACTGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['contact_group']},
        'ENABLE_CONTACT_HOST_NOTIFICATIONS': {'global': True, 'args': ['contact']},
        'ENABLE_CONTACT_SVC_NOTIFICATIONS': {'global': True, 'args': ['contact']},
        'ENABLE_EVENT_HANDLERS': {'global': True, 'args': []},
        'ENABLE_FAILURE_PREDICTION': {'global': True, 'args': []},
        'ENABLE_FLAP_DETECTION': {'global': True, 'args': []},
        'ENABLE_HOSTGROUP_HOST_CHECKS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOSTGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOSTGROUP_PASSIVE_HOST_CHECKS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOSTGROUP_PASSIVE_SVC_CHECKS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOSTGROUP_SVC_CHECKS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOSTGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['host_group']},
        'ENABLE_HOST_AND_CHILD_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'ENABLE_HOST_CHECK': {'global': False, 'args': ['host']},
        'ENABLE_HOST_EVENT_HANDLER': {'global': False, 'args': ['host']},
        'ENABLE_HOST_FLAP_DETECTION': {'global': False, 'args': ['host']},
        'ENABLE_HOST_FRESHNESS_CHECKS': {'global': True, 'args': []},
        'ENABLE_HOST_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'ENABLE_HOST_SVC_CHECKS': {'global': False, 'args': ['host']},
        'ENABLE_HOST_SVC_NOTIFICATIONS': {'global': False, 'args': ['host']},
        'ENABLE_NOTIFICATIONS': {'global': True, 'args': []},
        'ENABLE_PASSIVE_HOST_CHECKS': {'global': False, 'args': ['host']},
        'ENABLE_PASSIVE_SVC_CHECKS': {'global': False, 'args': ['service']},
        'ENABLE_PERFORMANCE_DATA': {'global': True, 'args': []},
        'ENABLE_SERVICEGROUP_HOST_CHECKS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICEGROUP_HOST_NOTIFICATIONS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICEGROUP_SVC_CHECKS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICEGROUP_SVC_NOTIFICATIONS': {'global': True, 'args': ['service_group']},
        'ENABLE_SERVICE_FRESHNESS_CHECKS': {'global': True, 'args': []},
        'ENABLE_SVC_CHECK': {'global': False, 'args': ['service']},
        'ENABLE_SVC_EVENT_HANDLER': {'global': False, 'args': ['service']},
        'ENABLE_SVC_FLAP_DETECTION': {'global': False, 'args': ['service']},
        'ENABLE_SVC_NOTIFICATIONS': {'global': False, 'args': ['service']},
        'PROCESS_FILE': {'global': True, 'args': [None, 'to_bool']},
        'PROCESS_HOST_CHECK_RESULT': {'global': False, 'args': ['host', 'to_int', None]},
        'PROCESS_HOST_OUTPUT': {'global': False, 'args': ['host', None]},
        'PROCESS_SERVICE_CHECK_RESULT': {'global': False, 'args': ['service', 'to_int', None]},
        'PROCESS_SERVICE_OUTPUT': {'global': False, 'args': ['service', None]},
        'READ_STATE_INFORMATION': {'global': True, 'args': []},
        'REMOVE_HOST_ACKNOWLEDGEMENT': {'global': False, 'args': ['host']},
        'REMOVE_SVC_ACKNOWLEDGEMENT': {'global': False, 'args': ['service']},
        'RESTART_PROGRAM': {'global': True, 'internal': True, 'args': []},
        'SAVE_STATE_INFORMATION': {'global': True, 'args': []},
        'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME': {'global': False, 'args': ['host', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_AND_PROPAGATE_TRIGGERED_HOST_DOWNTIME': {'global': False, 'args': ['host', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_CONTACT_DOWNTIME': {'global': True, 'args': ['contact', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_FORCED_HOST_CHECK': {'global': False, 'args': ['host', 'to_int']},
        'SCHEDULE_FORCED_HOST_SVC_CHECKS': {'global': False, 'args': ['host', 'to_int']},
        'SCHEDULE_FORCED_SVC_CHECK': {'global': False, 'args': ['service', 'to_int']},
        'SCHEDULE_HOSTGROUP_HOST_DOWNTIME': {'global': True, 'args': ['host_group', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_HOSTGROUP_SVC_DOWNTIME': {'global': True, 'args': ['host_group', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_HOST_CHECK': {'global': False, 'args': ['host', 'to_int']},
        'SCHEDULE_HOST_DOWNTIME': {'global': False, 'args': ['host', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_HOST_SVC_CHECKS': {'global': False, 'args': ['host', 'to_int']},
        'SCHEDULE_HOST_SVC_DOWNTIME': {'global': False, 'args': ['host', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_SERVICEGROUP_HOST_DOWNTIME': {'global': True, 'args': ['service_group', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_SERVICEGROUP_SVC_DOWNTIME': {'global': True, 'args': ['service_group', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SCHEDULE_SVC_CHECK': {'global': False, 'args': ['service', 'to_int']},
        'SCHEDULE_SVC_DOWNTIME': {'global': False, 'args': ['service', 'to_int', 'to_int', 'to_bool', 'to_int', 'to_int', 'author', None]},
        'SEND_CUSTOM_HOST_NOTIFICATION': {'global': False, 'args': ['host', 'to_int', 'author', None]},
        'SEND_CUSTOM_SVC_NOTIFICATION': {'global': False, 'args': ['service', 'to_int', 'author', None]},
        'SET_HOST_NOTIFICATION_NUMBER': {'global': False, 'args': ['host', 'to_int']},
        'SET_SVC_NOTIFICATION_NUMBER': {'global': False, 'args': ['service', 'to_int']},
        'SHUTDOWN_PROGRAM': {'global': True, 'args': []},
        'START_ACCEPTING_PASSIVE_HOST_CHECKS': {'global': True, 'args': []},
        'START_ACCEPTING_PASSIVE_SVC_CHECKS': {'global': True, 'args': []},
        'START_EXECUTING_HOST_CHECKS': {'global': True, 'args': []},
        'START_EXECUTING_SVC_CHECKS': {'global': True, 'args': []},
        'START_OBSESSING_OVER_HOST': {'global': False, 'args': ['host']},
        'START_OBSESSING_OVER_HOST_CHECKS': {'global': True, 'args': []},
        'START_OBSESSING_OVER_SVC': {'global': False, 'args': ['service']},
        'START_OBSESSING_OVER_SVC_CHECKS': {'global': True, 'args': []},
        'STOP_ACCEPTING_PASSIVE_HOST_CHECKS': {'global': True, 'args': []},
        'STOP_ACCEPTING_PASSIVE_SVC_CHECKS': {'global': True, 'args': []},
        'STOP_EXECUTING_HOST_CHECKS': {'global': True, 'args': []},
        'STOP_EXECUTING_SVC_CHECKS': {'global': True, 'args': []},
        'STOP_OBSESSING_OVER_HOST': {'global': False, 'args': ['host']},
        'STOP_OBSESSING_OVER_HOST_CHECKS': {'global': True, 'args': []},
        'STOP_OBSESSING_OVER_SVC': {'global': False, 'args': ['service']},
        'STOP_OBSESSING_OVER_SVC_CHECKS': {'global': True, 'args': []},
        'LAUNCH_SVC_EVENT_HANDLER': {'global': False, 'args': ['service']},
        'LAUNCH_HOST_EVENT_HANDLER': {'global': False, 'args': ['host']},
        # Now internal calls
        'ADD_SIMPLE_HOST_DEPENDENCY': {'global': False, 'args': ['host', 'host']},
        'DEL_HOST_DEPENDENCY': {'global': False, 'args': ['host', 'host']},
        'ADD_SIMPLE_POLLER': {'global': True, 'internal': True, 'args': [None, None, None, None]},
    }

    def __init__(self, conf, mode):
        self.mode = mode
        if conf:
            self.conf = conf
            self.hosts = conf.hosts
            self.services = conf.services
            self.contacts = conf.contacts
            self.hostgroups = conf.hostgroups
            self.commands = conf.commands
            self.servicegroups = conf.servicegroups
            self.contactgroups = conf.contactgroups
            self.timeperiods = conf.timeperiods
            self.pipe_path = conf.command_file
        
        self.fifo = None
        self.cmd_fragments = ''
        if self.mode == 'dispatcher':
            self.confs = conf.confs
        # Will change for each command read, so if a command need it,
        # it can get it
        self.current_timestamp = 0

    def load_scheduler(self, scheduler):
        self.sched = scheduler

    def load_arbiter(self, arbiter):
        self.arbiter = arbiter

    def load_receiver(self, receiver):
        self.receiver = receiver


    def open(self):
        # At the first open del and create the fifo
        if self.fifo is None:
            if os.path.exists(self.pipe_path):
                os.unlink(self.pipe_path)

            if not os.path.exists(self.pipe_path):
                os.umask(0)
                try:
                    os.mkfifo(self.pipe_path, 0660)
                    open(self.pipe_path, 'w+', os.O_NONBLOCK)
                except OSError, exp:
                    self.error("Pipe creation failed (%s): %s" % (self.pipe_path, str(exp)))
                    return None
        self.fifo = os.open(self.pipe_path, os.O_NONBLOCK)
        return self.fifo


    def get(self):
        buf = os.read(self.fifo, 8096)
        r = []
        fullbuf = len(buf) == 8096 and True or False
        # If the buffer ended with a fragment last time, prepend it here
        buf = self.cmd_fragments + buf
        buflen = len(buf)
        self.cmd_fragments = ''
        if fullbuf and buf[-1] != '\n':
            # The buffer was full but ends with a command fragment
            r.extend([ExternalCommand(s) for s in (buf.split('\n'))[:-1] if s])
            self.cmd_fragments = (buf.split('\n'))[-1]
        elif buflen:
            # The buffer is either half-filled or full with a '\n' at the end.
            r.extend([ExternalCommand(s) for s in buf.split('\n') if s])
        else:
            # The buffer is empty. We "reset" the fifo here. It will be
            # re-opened in the main loop.
            os.close(self.fifo)
        return r


    def resolve_command(self, excmd):
        # Maybe the command is invalid. Bailout
        try:
            command = excmd.cmd_line
        except AttributeError, exp:
            logger.debug("resolve_command:: error with command %s: %s" % (excmd, exp))
            return

        # Strip and get utf8 only strings
        command = command.strip()

        # Only log if we are in the Arbiter
        if self.mode == 'dispatcher' and self.conf.log_external_commands:
            logger.info('EXTERNAL COMMAND: ' + command.rstrip())
        r = self.get_command_and_args(command, excmd)

        # If we are a receiver, bail out here
        if self.mode == 'receiver':
            return
        
        if r is not None:
            is_global = r['global']
            if not is_global:
                c_name = r['c_name']
                args = r['args']
                logger.debug("Got commands %s %s" % (c_name, str(args)))
                f = getattr(self, c_name)
                apply(f, args)
            else:
                command = r['cmd']
                self.dispatch_global_command(command)


    # Ok the command is not for every one, so we search
    # by the hostname which scheduler have the host. Then send
    # the command
    def search_host_and_dispatch(self, host_name, command, extcmd):
        logger.debug("Calling search_host_and_dispatch for %s" % host_name)

        # If we are a receiver, just look in the receiver 
        if self.mode == 'receiver':
            logger.info("Receiver looking a scheduler for the external command %s %s" % (host_name, command))
            sched = self.receiver.get_sched_from_hname(host_name)
            logger.debug("Receiver found a scheduler: %s" % (sched))
            if sched:
                logger.info("Receiver pushing external command to scheduler %s" % (sched))
                sched['external_commands'].append(extcmd)
            return
        
        host_found = False
        for cfg in self.confs.values():
            if cfg.hosts.find_by_name(host_name) is not None:
                logger.debug("Host %s found in a configuration" % host_name)
                if cfg.is_assigned:
                    host_found = True
                    sched = cfg.assigned_to
                    logger.debug("Sending command to the scheduler %s" % sched.get_name())
                    #sched.run_external_command(command)
                    sched.external_commands.append(command)
                    break
                else:
                    logger.warning("Problem: a configuration is found, but is not assigned!")

        if not host_found:
            logger.warning("Passive check result was received for host '%s', but the host could not be found!" % host_name)
            #print "Sorry but the host", host_name, "was not found"


    # The command is global, so sent it to every schedulers
    def dispatch_global_command(self, command):
        for sched in self.conf.schedulers:
            logger.debug("Sending a command '%s' to scheduler %s" % (command, sched))
            if sched.alive:
                #sched.run_external_command(command)
                sched.external_commands.append(command)


    # We need to get the first part, the command name, and the reference ext command object
    def get_command_and_args(self, command, extcmd=None):
        #safe_print("Trying to resolve", command)
        command = command.rstrip()
        elts = split_semicolon(command)  # danger!!! passive checkresults with perfdata
        part1 = elts[0]

        elts2 = part1.split(' ')
        #print "Elts2:", elts2
        if len(elts2) != 2:
            logger.debug("Malformed command '%s'" % command)
            return None
        ts = elts2[0]
        # Now we will get the timestamps as [123456]
        if not ts.startswith('[') or not ts.endswith(']'):
            logger.debug("Malformed command '%s'" % command)
            return None
        # Ok we remove the [ ]
        ts = ts[1:-1]
        try:  # is an int or not?
            self.current_timestamp = to_int(ts)
        except ValueError:
            logger.debug("Malformed command '%s'" % command)
            return None

        # Now get the command
        c_name = elts2[1]

        #safe_print("Get command name", c_name)
        if c_name not in ExternalCommandManager.commands:
            logger.debug("Command '%s' is not recognized, sorry" % c_name)
            return None

        # Split again based on the number of args we expect. We cannot split
        # on every ; because this character may appear in the perfdata of
        # passive check results.
        entry = ExternalCommandManager.commands[c_name]

        # Look if the command is purely internal or not
        internal = False
        if 'internal' in entry and entry['internal']:
            internal = True

        numargs = len(entry['args'])
        if numargs and 'service' in entry['args']:
            numargs += 1
        elts = split_semicolon(command, numargs)

        logger.debug("mode= %s, global= %s" % (self.mode, str(entry['global'])))
        if self.mode == 'dispatcher' and entry['global']:
            if not internal:
                logger.debug("Command '%s' is a global one, we resent it to all schedulers" % c_name)
                return {'global': True, 'cmd': command}

        #print "Is global?", c_name, entry['global']
        #print "Mode:", self.mode
        #print "This command have arguments:", entry['args'], len(entry['args'])

        args = []
        i = 1
        in_service = False
        tmp_host = ''
        try:
            for elt in elts[1:]:
                logger.debug("Searching for a new arg: %s (%d)" % (elt, i))
                val = elt.strip()
                if val.endswith('\n'):
                    val = val[:-1]

                logger.debug("For command arg: %s" % val)

                if not in_service:
                    type_searched = entry['args'][i-1]
                    #safe_print("Search for a arg", type_searched)

                    if type_searched == 'host':
                        if self.mode == 'dispatcher' or self.mode == 'receiver':
                            self.search_host_and_dispatch(val, command, extcmd)
                            return None
                        h = self.hosts.find_by_name(val)
                        if h is not None:
                            args.append(h)

                    elif type_searched == 'contact':
                        c = self.contacts.find_by_name(val)
                        if c is not None:
                            args.append(c)

                    elif type_searched == 'time_period':
                        t = self.timeperiods.find_by_name(val)
                        if t is not None:
                            args.append(t)

                    elif type_searched == 'to_bool':
                        args.append(to_bool(val))

                    elif type_searched == 'to_int':
                        args.append(to_int(val))

                    elif type_searched in ('author', None):
                        args.append(val)

                    elif type_searched == 'command':
                        c = self.commands.find_by_name(val)
                        if c is not None:
                            # the find will be redone by
                            # the commandCall creation, but != None
                            # is useful so a bad command will be caught
                            args.append(val)

                    elif type_searched == 'host_group':
                        hg = self.hostgroups.find_by_name(val)
                        if hg is not None:
                            args.append(hg)

                    elif type_searched == 'service_group':
                        sg = self.servicegroups.find_by_name(val)
                        if sg is not None:
                            args.append(sg)

                    elif type_searched == 'contact_group':
                        cg = self.contact_groups.find_by_name(val)
                        if cg is not None:
                            args.append(cg)

                    # special case: service are TWO args host;service, so one more loop
                    # to get the two parts
                    elif type_searched == 'service':
                        in_service = True
                        tmp_host = elt.strip()
                        #safe_print("TMP HOST", tmp_host)
                        if tmp_host[-1] == '\n':
                            tmp_host = tmp_host[:-1]
                        if self.mode == 'dispatcher':
                            self.search_host_and_dispatch(tmp_host, command, extcmd)
                            return None

                    i += 1
                else:
                    in_service = False
                    srv_name = elt
                    if srv_name[-1] == '\n':
                        srv_name = srv_name[:-1]
                    # If we are in a receiver, bailout now.
                    if self.mode == 'receiver':
                        self.search_host_and_dispatch(tmp_host, command, extcmd)
                        return None

                    #safe_print("Got service full", tmp_host, srv_name)
                    s = self.services.find_srv_by_name_and_hostname(tmp_host, srv_name)
                    if s is not None:
                        args.append(s)
                    else:  # error, must be logged
                        logger.warning("A command was received for service '%s' on host '%s', but the service could not be found!" % (srv_name, tmp_host))

        except IndexError:
            logger.debug("Sorry, the arguments are not corrects")
            return None
        #safe_print('Finally got ARGS:', args)
        if len(args) == len(entry['args']):
            #safe_print("OK, we can call the command", c_name, "with", args)
            return {'global': False, 'c_name': c_name, 'args': args}
            #f = getattr(self, c_name)
            #apply(f, args)
        else:
            logger.debug("Sorry, the arguments are not corrects (%s)" % str(args))
            return None

    # CHANGE_CONTACT_MODSATTR;<contact_name>;<value>
    def CHANGE_CONTACT_MODSATTR(self, contact, value):  # TODO
        contact.modified_service_attributes = long(value)

    # CHANGE_CONTACT_MODHATTR;<contact_name>;<value>
    def CHANGE_CONTACT_MODHATTR(self, contact, value):  # TODO
        contact.modified_host_attributes = long(value)

    # CHANGE_CONTACT_MODATTR;<contact_name>;<value>
    def CHANGE_CONTACT_MODATTR(self, contact, value):
        contact.modified_attributes = long(value)

    # CHANGE_CONTACT_HOST_NOTIFICATION_TIMEPERIOD;<contact_name>;<notification_timeperiod>
    def CHANGE_CONTACT_HOST_NOTIFICATION_TIMEPERIOD(self, contact, notification_timeperiod):
        contact.modified_host_attributes |= MODATTR_NOTIFICATION_TIMEPERIOD
        contact.host_notification_period = notification_timeperiod
        self.sched.get_and_register_status_brok(contact)

    # ADD_SVC_COMMENT;<host_name>;<service_description>;<persistent>;<author>;<comment>
    def ADD_SVC_COMMENT(self, service, persistent, author, comment):
        c = Comment(service, persistent, author, comment, 2, 1, 1, False, 0)
        service.add_comment(c)
        self.sched.add(c)

    # ADD_HOST_COMMENT;<host_name>;<persistent>;<author>;<comment>
    def ADD_HOST_COMMENT(self, host, persistent, author, comment):
        c = Comment(host, persistent, author, comment, 1, 1, 1, False, 0)
        host.add_comment(c)
        self.sched.add(c)

    # ACKNOWLEDGE_SVC_PROBLEM;<host_name>;<service_description>;<sticky>;<notify>;<persistent>;<author>;<comment>
    def ACKNOWLEDGE_SVC_PROBLEM(self, service, sticky, notify, persistent, author, comment):
        service.acknowledge_problem(sticky, notify, persistent, author, comment)

    # ACKNOWLEDGE_HOST_PROBLEM;<host_name>;<sticky>;<notify>;<persistent>;<author>;<comment>
    # TODO: add a better ACK management
    def ACKNOWLEDGE_HOST_PROBLEM(self, host, sticky, notify, persistent, author, comment):
        host.acknowledge_problem(sticky, notify, persistent, author, comment)

    # ACKNOWLEDGE_SVC_PROBLEM_EXPIRE;<host_name>;<service_description>;<sticky>;<notify>;<persistent>;<end_time>;<author>;<comment>
    def ACKNOWLEDGE_SVC_PROBLEM_EXPIRE(self, service, sticky, notify, persistent, end_time, author, comment):
        service.acknowledge_problem(sticky, notify, persistent, author, comment, end_time=end_time)

    # ACKNOWLEDGE_HOST_PROBLEM_EXPIRE;<host_name>;<sticky>;<notify>;<persistent>;<end_time>;<author>;<comment>
    # TODO: add a better ACK management
    def ACKNOWLEDGE_HOST_PROBLEM_EXPIRE(self, host, sticky, notify, persistent, end_time, author, comment):
        host.acknowledge_problem(sticky, notify, persistent, author, comment, end_time=end_time)

    # CHANGE_CONTACT_SVC_NOTIFICATION_TIMEPERIOD;<contact_name>;<notification_timeperiod>
    def CHANGE_CONTACT_SVC_NOTIFICATION_TIMEPERIOD(self, contact, notification_timeperiod):
        contact.modified_service_attributes |= MODATTR_NOTIFICATION_TIMEPERIOD
        contact.service_notification_period = notification_timeperiod
        self.sched.get_and_register_status_brok(contact)

    # CHANGE_CUSTOM_CONTACT_VAR;<contact_name>;<varname>;<varvalue>
    def CHANGE_CUSTOM_CONTACT_VAR(self, contact, varname, varvalue):
        contact.modified_attributes |= MODATTR_CUSTOM_VARIABLE
        contact.customs[varname.upper()] = varvalue

    # CHANGE_CUSTOM_HOST_VAR;<host_name>;<varname>;<varvalue>
    def CHANGE_CUSTOM_HOST_VAR(self, host, varname, varvalue):
        host.modified_attributes |= MODATTR_CUSTOM_VARIABLE
        host.customs[varname.upper()] = varvalue

    # CHANGE_CUSTOM_SVC_VAR;<host_name>;<service_description>;<varname>;<varvalue>
    def CHANGE_CUSTOM_SVC_VAR(self, service, varname, varvalue):
        service.modified_attributes |= MODATTR_CUSTOM_VARIABLE
        service.customs[varname.upper()] = varvalue

    # CHANGE_GLOBAL_HOST_EVENT_HANDLER;<event_handler_command>
    def CHANGE_GLOBAL_HOST_EVENT_HANDLER(self, event_handler_command):
        # TODO: MODATTR_EVENT_HANDLER_COMMAND
        pass

    # CHANGE_GLOBAL_SVC_EVENT_HANDLER;<event_handler_command> # TODO
    def CHANGE_GLOBAL_SVC_EVENT_HANDLER(self, event_handler_command):
        # TODO: MODATTR_EVENT_HANDLER_COMMAND
        pass

    # CHANGE_HOST_CHECK_COMMAND;<host_name>;<check_command>
    def CHANGE_HOST_CHECK_COMMAND(self, host, check_command):
        host.modified_attributes |= MODATTR_CHECK_COMMAND
        host.check_command = CommandCall(self.commands, check_command, poller_tag=host.poller_tag)
        self.sched.get_and_register_status_brok(host)

    # CHANGE_HOST_CHECK_TIMEPERIOD;<host_name>;<timeperiod>
    def CHANGE_HOST_CHECK_TIMEPERIOD(self, host, timeperiod):  # TODO is timeperiod a string or a Timeperiod object?
        host.modified_attributes |= MODATTR_CHECK_TIMEPERIOD
        host.check_period = timeperiod
        self.sched.get_and_register_status_brok(host)

    # CHANGE_HOST_EVENT_HANDLER;<host_name>;<event_handler_command>
    def CHANGE_HOST_EVENT_HANDLER(self, host, event_handler_command):
        host.modified_attributes |= MODATTR_EVENT_HANDLER_COMMAND
        host.event_handler = CommandCall(self.commands, event_handler_command)
        self.sched.get_and_register_status_brok(host)

    # CHANGE_HOST_MODATTR;<host_name>;<value>
    def CHANGE_HOST_MODATTR(self, host, value):
        host.modified_attributes = long(value)

    # CHANGE_MAX_HOST_CHECK_ATTEMPTS;<host_name>;<check_attempts>
    def CHANGE_MAX_HOST_CHECK_ATTEMPTS(self, host, check_attempts):
        host.modified_attributes |= MODATTR_MAX_CHECK_ATTEMPTS
        host.max_check_attempts = check_attempts
        if host.state_type == 'HARD' and host.state == 'UP' and host.attempt > 1:
            host.attempt = host.max_check_attempts
        self.sched.get_and_register_status_brok(host)

    # CHANGE_MAX_SVC_CHECK_ATTEMPTS;<host_name>;<service_description>;<check_attempts>
    def CHANGE_MAX_SVC_CHECK_ATTEMPTS(self, service, check_attempts):
        service.modified_attributes |= MODATTR_MAX_CHECK_ATTEMPTS
        service.max_check_attempts = check_attempts
        if service.state_type == 'HARD' and service.state == 'OK' and service.attempt > 1:
            service.attempt = service.max_check_attempts
        self.sched.get_and_register_status_brok(service)

    # CHANGE_NORMAL_HOST_CHECK_INTERVAL;<host_name>;<check_interval>
    def CHANGE_NORMAL_HOST_CHECK_INTERVAL(self, host, check_interval):
        host.modified_attributes |= MODATTR_NORMAL_CHECK_INTERVAL
        old_interval = host.check_interval
        host.check_interval = check_interval
        # If there were no regular checks (interval=0), then schedule
        # a check immediately.
        if old_interval == 0 and host.checks_enabled:
            host.schedule(force=False, force_time=int(time.time()))
        self.sched.get_and_register_status_brok(host)

    # CHANGE_NORMAL_SVC_CHECK_INTERVAL;<host_name>;<service_description>;<check_interval>
    def CHANGE_NORMAL_SVC_CHECK_INTERVAL(self, service, check_interval):
        service.modified_attributes |= MODATTR_NORMAL_CHECK_INTERVAL
        old_interval = service.check_interval
        service.check_interval = check_interval
        # If there were no regular checks (interval=0), then schedule
        # a check immediately.
        if old_interval == 0 and service.checks_enabled:
            service.schedule(force=False, force_time=int(time.time()))
        self.sched.get_and_register_status_brok(service)

    # CHANGE_RETRY_HOST_CHECK_INTERVAL;<host_name>;<check_interval>
    def CHANGE_RETRY_HOST_CHECK_INTERVAL(self, host, check_interval):
        host.modified_attributes |= MODATTR_RETRY_CHECK_INTERVAL
        host.retry_interval = check_interval
        self.sched.get_and_register_status_brok(host)

    # CHANGE_RETRY_SVC_CHECK_INTERVAL;<host_name>;<service_description>;<check_interval>
    def CHANGE_RETRY_SVC_CHECK_INTERVAL(self, service, check_interval):
        service.modified_attributes |= MODATTR_RETRY_CHECK_INTERVAL
        service.retry_interval = check_interval
        self.sched.get_and_register_status_brok(service)

    # CHANGE_SVC_CHECK_COMMAND;<host_name>;<service_description>;<check_command>
    def CHANGE_SVC_CHECK_COMMAND(self, service, check_command):
        service.modified_attributes |= MODATTR_CHECK_COMMAND
        service.check_command = CommandCall(self.commands, check_command, poller_tag=service.poller_tag)
        self.sched.get_and_register_status_brok(service)

    # CHANGE_SVC_CHECK_TIMEPERIOD;<host_name>;<service_description>;<check_timeperiod>
    def CHANGE_SVC_CHECK_TIMEPERIOD(self, service, check_timeperiod):
        service.modified_attributes |= MODATTR_CHECK_TIMEPERIOD
        service.check_period = check_timeperiod
        self.sched.get_and_register_status_brok(service)

    # CHANGE_SVC_EVENT_HANDLER;<host_name>;<service_description>;<event_handler_command>
    def CHANGE_SVC_EVENT_HANDLER(self, service, event_handler_command):
        service.modified_attributes |= MODATTR_EVENT_HANDLER_COMMAND
        service.event_handler = CommandCall(self.commands, event_handler_command)
        self.sched.get_and_register_status_brok(service)

    # CHANGE_SVC_MODATTR;<host_name>;<service_description>;<value>
    def CHANGE_SVC_MODATTR(self, service, value):
        service.modified_attributes = long(value)

    # CHANGE_SVC_NOTIFICATION_TIMEPERIOD;<host_name>;<service_description>;<notification_timeperiod>
    def CHANGE_SVC_NOTIFICATION_TIMEPERIOD(self, service, notification_timeperiod):
        service.modified_attributes |= MODATTR_NOTIFICATION_TIMEPERIOD
        service.notification_period = notification_timeperiod
        self.sched.get_and_register_status_brok(service)

    # DELAY_HOST_NOTIFICATION;<host_name>;<notification_time>
    def DELAY_HOST_NOTIFICATION(self, host, notification_time):
        host.first_notification_delay = notification_time
        self.sched.get_and_register_status_brok(host)

    # DELAY_SVC_NOTIFICATION;<host_name>;<service_description>;<notification_time>
    def DELAY_SVC_NOTIFICATION(self, service, notification_time):
        service.first_notification_delay = notification_time
        self.sched.get_and_register_status_brok(service)

    # DEL_ALL_HOST_COMMENTS;<host_name>
    def DEL_ALL_HOST_COMMENTS(self, host):
        for c in host.comments:
            self.DEL_HOST_COMMENT(c.id)

    # DEL_ALL_HOST_COMMENTS;<host_name>
    def DEL_ALL_HOST_DOWNTIMES(self, host):
        for dt in host.downtimes:
            self.DEL_HOST_DOWNTIME(dt.id)

    # DEL_ALL_SVC_COMMENTS;<host_name>;<service_description>
    def DEL_ALL_SVC_COMMENTS(self, service):
        for c in service.comments:
            self.DEL_SVC_COMMENT(c.id)

    # DEL_ALL_SVC_COMMENTS;<host_name>;<service_description>
    def DEL_ALL_SVC_DOWNTIMES(self, service):
        for dt in service.downtimes:
            self.DEL_SVC_DOWNTIME(dt.id)

    # DEL_CONTACT_DOWNTIME;<downtime_id>
    def DEL_CONTACT_DOWNTIME(self, downtime_id):
        if downtime_id in self.sched.contact_downtimes:
            self.sched.contact_downtimes[downtime_id].cancel()

    # DEL_HOST_COMMENT;<comment_id>
    def DEL_HOST_COMMENT(self, comment_id):
        if comment_id in self.sched.comments:
            self.sched.comments[comment_id].can_be_deleted = True

    # DEL_HOST_DOWNTIME;<downtime_id>
    def DEL_HOST_DOWNTIME(self, downtime_id):
        if downtime_id in self.sched.downtimes:
            self.sched.downtimes[downtime_id].cancel()

    # DEL_SVC_COMMENT;<comment_id>
    def DEL_SVC_COMMENT(self, comment_id):
        if comment_id in self.sched.comments:
            self.sched.comments[comment_id].can_be_deleted = True

    # DEL_SVC_DOWNTIME;<downtime_id>
    def DEL_SVC_DOWNTIME(self, downtime_id):
        if downtime_id in self.sched.downtimes:
            self.sched.downtimes[downtime_id].cancel()

    # DISABLE_ALL_NOTIFICATIONS_BEYOND_HOST;<host_name>
    def DISABLE_ALL_NOTIFICATIONS_BEYOND_HOST(self, host):
        pass

    # DISABLE_CONTACTGROUP_HOST_NOTIFICATIONS;<contactgroup_name>
    def DISABLE_CONTACTGROUP_HOST_NOTIFICATIONS(self, contactgroup):
        for contact in contactgroup:
            self.DISABLE_CONTACT_HOST_NOTIFICATIONS(contact)

    # DISABLE_CONTACTGROUP_SVC_NOTIFICATIONS;<contactgroup_name>
    def DISABLE_CONTACTGROUP_SVC_NOTIFICATIONS(self, contactgroup):
        for contact in contactgroup:
            self.DISABLE_CONTACT_SVC_NOTIFICATIONS(contact)

    # DISABLE_CONTACT_HOST_NOTIFICATIONS;<contact_name>
    def DISABLE_CONTACT_HOST_NOTIFICATIONS(self, contact):
        if contact.host_notifications_enabled:
            contact.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            contact.host_notifications_enabled = False
            self.sched.get_and_register_status_brok(contact)

    # DISABLE_CONTACT_SVC_NOTIFICATIONS;<contact_name>
    def DISABLE_CONTACT_SVC_NOTIFICATIONS(self, contact):
        if contact.service_notifications_enabled:
            contact.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            contact.service_notifications_enabled = False
            self.sched.get_and_register_status_brok(contact)

    # DISABLE_EVENT_HANDLERS
    def DISABLE_EVENT_HANDLERS(self):
        if self.conf.enable_event_handlers:
            self.conf.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            self.conf.enable_event_handlers = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_FAILURE_PREDICTION
    def DISABLE_FAILURE_PREDICTION(self):
        if self.conf.enable_failure_prediction:
            self.conf.modified_attributes |= MODATTR_FAILURE_PREDICTION_ENABLED
            self.conf.enable_failure_prediction = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_FLAP_DETECTION
    def DISABLE_FLAP_DETECTION(self):
        if self.conf.enable_flap_detection:
            self.conf.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            self.conf.enable_flap_detection = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()
            # Is need, disable flap state for hosts and services
            for service in self.conf.services:
                if service.is_flapping:
                    service.is_flapping = False
                    service.flapping_changes = []
                    self.sched.get_and_register_status_brok(service)
            for host in self.conf.hosts:
                if host.is_flapping:
                    host.is_flapping = False
                    host.flapping_changes = []
                    self.sched.get_and_register_status_brok(host)

    # DISABLE_HOSTGROUP_HOST_CHECKS;<hostgroup_name>
    def DISABLE_HOSTGROUP_HOST_CHECKS(self, hostgroup):
        for host in hostgroup:
            self.DISABLE_HOST_CHECK(host)

    # DISABLE_HOSTGROUP_HOST_NOTIFICATIONS;<hostgroup_name>
    def DISABLE_HOSTGROUP_HOST_NOTIFICATIONS(self, hostgroup):
        for host in hostgroup:
            self.DISABLE_HOST_NOTIFICATIONS(host)

    # DISABLE_HOSTGROUP_PASSIVE_HOST_CHECKS;<hostgroup_name>
    def DISABLE_HOSTGROUP_PASSIVE_HOST_CHECKS(self, hostgroup):
        for host in hostgroup:
            self.DISABLE_PASSIVE_HOST_CHECKS(host)

    # DISABLE_HOSTGROUP_PASSIVE_SVC_CHECKS;<hostgroup_name>
    def DISABLE_HOSTGROUP_PASSIVE_SVC_CHECKS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.DISABLE_PASSIVE_SVC_CHECKS(service)

    # DISABLE_HOSTGROUP_SVC_CHECKS;<hostgroup_name>
    def DISABLE_HOSTGROUP_SVC_CHECKS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.DISABLE_SVC_CHECK(service)

    # DISABLE_HOSTGROUP_SVC_NOTIFICATIONS;<hostgroup_name>
    def DISABLE_HOSTGROUP_SVC_NOTIFICATIONS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.DISABLE_SVC_NOTIFICATIONS(service)

    # DISABLE_HOST_AND_CHILD_NOTIFICATIONS;<host_name>
    def DISABLE_HOST_AND_CHILD_NOTIFICATIONS(self, host):
        pass

    # DISABLE_HOST_CHECK;<host_name>
    def DISABLE_HOST_CHECK(self, host):
        if host.active_checks_enabled:
            host.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            host.disable_active_checks()
            self.sched.get_and_register_status_brok(host)

    # DISABLE_HOST_EVENT_HANDLER;<host_name>
    def DISABLE_HOST_EVENT_HANDLER(self, host):
        if host.event_handler_enabled:
            host.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            host.event_handler_enabled = False
            self.sched.get_and_register_status_brok(host)

    # DISABLE_HOST_FLAP_DETECTION;<host_name>
    def DISABLE_HOST_FLAP_DETECTION(self, host):
        if host.flap_detection_enabled:
            host.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            host.flap_detection_enabled = False
            # Maybe the host was flapping, if so, stop flapping
            if host.is_flapping:
                host.is_flapping = False
                host.flapping_changes = []
            self.sched.get_and_register_status_brok(host)

    # DISABLE_HOST_FRESHNESS_CHECKS
    def DISABLE_HOST_FRESHNESS_CHECKS(self):
        if self.conf.check_host_freshness:
            self.conf.modified_attributes |= MODATTR_FRESHNESS_CHECKS_ENABLED
            self.conf.check_host_freshness = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_HOST_NOTIFICATIONS;<host_name>
    def DISABLE_HOST_NOTIFICATIONS(self, host):
        if host.notifications_enabled:
            host.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            host.notifications_enabled = False
            self.sched.get_and_register_status_brok(host)

    # DISABLE_HOST_SVC_CHECKS;<host_name>
    def DISABLE_HOST_SVC_CHECKS(self, host):
        for s in host.services:
            self.DISABLE_SVC_CHECK(s)

    # DISABLE_HOST_SVC_NOTIFICATIONS;<host_name>
    def DISABLE_HOST_SVC_NOTIFICATIONS(self, host):
        for s in host.services:
            self.DISABLE_SVC_NOTIFICATIONS(s)
            self.sched.get_and_register_status_brok(s)

    # DISABLE_NOTIFICATIONS
    def DISABLE_NOTIFICATIONS(self):
        if self.conf.enable_notifications:
            self.conf.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            self.conf.enable_notifications = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_PASSIVE_HOST_CHECKS;<host_name>
    def DISABLE_PASSIVE_HOST_CHECKS(self, host):
        if host.passive_checks_enabled:
            host.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            host.passive_checks_enabled = False
            self.sched.get_and_register_status_brok(host)

    # DISABLE_PASSIVE_SVC_CHECKS;<host_name>;<service_description>
    def DISABLE_PASSIVE_SVC_CHECKS(self, service):
        if service.passive_checks_enabled:
            service.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            service.passive_checks_enabled = False
            self.sched.get_and_register_status_brok(service)

    # DISABLE_PERFORMANCE_DATA
    def DISABLE_PERFORMANCE_DATA(self):
        if self.conf.process_performance_data:
            self.conf.modified_attributes |= MODATTR_PERFORMANCE_DATA_ENABLED
            self.conf.process_performance_data = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_SERVICEGROUP_HOST_CHECKS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_HOST_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_HOST_CHECK(service.host)

    # DISABLE_SERVICEGROUP_HOST_NOTIFICATIONS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_HOST_NOTIFICATIONS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_HOST_NOTIFICATIONS(service.host)

    # DISABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_PASSIVE_HOST_CHECKS(service.host)

    # DISABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_PASSIVE_SVC_CHECKS(service)

    # DISABLE_SERVICEGROUP_SVC_CHECKS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_SVC_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_SVC_CHECK(service)

    # DISABLE_SERVICEGROUP_SVC_NOTIFICATIONS;<servicegroup_name>
    def DISABLE_SERVICEGROUP_SVC_NOTIFICATIONS(self, servicegroup):
        for service in servicegroup:
            self.DISABLE_SVC_NOTIFICATIONS(service)

    # DISABLE_SERVICE_FLAP_DETECTION;<host_name>;<service_description>
    def DISABLE_SERVICE_FLAP_DETECTION(self, service):
        if service.flap_detection_enabled:
            service.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            service.flap_detection_enabled = False
            # Maybe the service was flapping, if so, stop flapping
            if service.is_flapping:
                service.is_flapping = False
                service.flapping_changes = []
            self.sched.get_and_register_status_brok(service)

    # DISABLE_SERVICE_FRESHNESS_CHECKS
    def DISABLE_SERVICE_FRESHNESS_CHECKS(self):
        if self.conf.check_service_freshness:
            self.conf.modified_attributes |= MODATTR_FRESHNESS_CHECKS_ENABLED
            self.conf.check_service_freshness = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # DISABLE_SVC_CHECK;<host_name>;<service_description>
    def DISABLE_SVC_CHECK(self, service):
        if service.active_checks_enabled:
            service.disable_active_checks()
            service.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.sched.get_and_register_status_brok(service)

    # DISABLE_SVC_EVENT_HANDLER;<host_name>;<service_description>
    def DISABLE_SVC_EVENT_HANDLER(self, service):
        if service.event_handler_enabled:
            service.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            service.event_handler_enabled = False
            self.sched.get_and_register_status_brok(service)

    # DISABLE_SVC_FLAP_DETECTION;<host_name>;<service_description>
    def DISABLE_SVC_FLAP_DETECTION(self, service):
        self.DISABLE_SERVICE_FLAP_DETECTION(service)

    # DISABLE_SVC_NOTIFICATIONS;<host_name>;<service_description>
    def DISABLE_SVC_NOTIFICATIONS(self, service):
        if service.notifications_enabled:
            service.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            service.notifications_enabled = False
            self.sched.get_and_register_status_brok(service)

    # ENABLE_ALL_NOTIFICATIONS_BEYOND_HOST;<host_name>
    def ENABLE_ALL_NOTIFICATIONS_BEYOND_HOST(self, host):
        pass

    # ENABLE_CONTACTGROUP_HOST_NOTIFICATIONS;<contactgroup_name>
    def ENABLE_CONTACTGROUP_HOST_NOTIFICATIONS(self, contactgroup):
        for contact in contactgroup:
            self.ENABLE_CONTACT_HOST_NOTIFICATIONS(contact)

    # ENABLE_CONTACTGROUP_SVC_NOTIFICATIONS;<contactgroup_name>
    def ENABLE_CONTACTGROUP_SVC_NOTIFICATIONS(self, contactgroup):
        for contact in contactgroup:
            self.ENABLE_CONTACT_SVC_NOTIFICATIONS(contact)

    # ENABLE_CONTACT_HOST_NOTIFICATIONS;<contact_name>
    def ENABLE_CONTACT_HOST_NOTIFICATIONS(self, contact):
        if not contact.host_notifications_enabled:
            contact.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            contact.host_notifications_enabled = True
            self.sched.get_and_register_status_brok(contact)

    # ENABLE_CONTACT_SVC_NOTIFICATIONS;<contact_name>
    def ENABLE_CONTACT_SVC_NOTIFICATIONS(self, contact):
        if not contact.service_notifications_enabled:
            contact.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            contact.service_notifications_enabled = True
            self.sched.get_and_register_status_brok(contact)

    # ENABLE_EVENT_HANDLERS
    def ENABLE_EVENT_HANDLERS(self):
        if not self.conf.enable_event_handlers:
            self.conf.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            self.conf.enable_event_handlers = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_FAILURE_PREDICTION
    def ENABLE_FAILURE_PREDICTION(self):
        if not self.conf.enable_failure_prediction:
            self.conf.modified_attributes |= MODATTR_FAILURE_PREDICTION_ENABLED
            self.conf.enable_failure_prediction = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_FLAP_DETECTION
    def ENABLE_FLAP_DETECTION(self):
        if not self.conf.enable_flap_detection:
            self.conf.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            self.conf.enable_flap_detection = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_HOSTGROUP_HOST_CHECKS;<hostgroup_name>
    def ENABLE_HOSTGROUP_HOST_CHECKS(self, hostgroup):
        for host in hostgroup:
            self.ENABLE_HOST_CHECK(host)

    # ENABLE_HOSTGROUP_HOST_NOTIFICATIONS;<hostgroup_name>
    def ENABLE_HOSTGROUP_HOST_NOTIFICATIONS(self, hostgroup):
        for host in hostgroup:
            self.ENABLE_HOST_NOTIFICATIONS(host)

    # ENABLE_HOSTGROUP_PASSIVE_HOST_CHECKS;<hostgroup_name>
    def ENABLE_HOSTGROUP_PASSIVE_HOST_CHECKS(self, hostgroup):
        for host in hostgroup:
            self.ENABLE_PASSIVE_HOST_CHECKS(host)

    # ENABLE_HOSTGROUP_PASSIVE_SVC_CHECKS;<hostgroup_name>
    def ENABLE_HOSTGROUP_PASSIVE_SVC_CHECKS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.ENABLE_PASSIVE_SVC_CHECKS(service)

    # ENABLE_HOSTGROUP_SVC_CHECKS;<hostgroup_name>
    def ENABLE_HOSTGROUP_SVC_CHECKS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.ENABLE_SVC_CHECK(service)

    # ENABLE_HOSTGROUP_SVC_NOTIFICATIONS;<hostgroup_name>
    def ENABLE_HOSTGROUP_SVC_NOTIFICATIONS(self, hostgroup):
        for host in hostgroup:
            for service in host.services:
                self.ENABLE_SVC_NOTIFICATIONS(service)

    # ENABLE_HOST_AND_CHILD_NOTIFICATIONS;<host_name>
    def ENABLE_HOST_AND_CHILD_NOTIFICATIONS(self, host):
        pass

    # ENABLE_HOST_CHECK;<host_name>
    def ENABLE_HOST_CHECK(self, host):
        if not host.active_checks_enabled:
            host.active_checks_enabled = True
            host.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.sched.get_and_register_status_brok(host)

    # ENABLE_HOST_EVENT_HANDLER;<host_name>
    def ENABLE_HOST_EVENT_HANDLER(self, host):
        if not host.event_handler_enabled:
            host.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            host.event_handler_enabled = True
            self.sched.get_and_register_status_brok(host)

    # ENABLE_HOST_FLAP_DETECTION;<host_name>
    def ENABLE_HOST_FLAP_DETECTION(self, host):
        if not host.flap_detection_enabled:
            host.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            host.flap_detection_enabled = True
            self.sched.get_and_register_status_brok(host)

    # ENABLE_HOST_FRESHNESS_CHECKS
    def ENABLE_HOST_FRESHNESS_CHECKS(self):
        if not self.conf.check_host_freshness:
            self.conf.modified_attributes |= MODATTR_FRESHNESS_CHECKS_ENABLED
            self.conf.check_host_freshness = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_HOST_NOTIFICATIONS;<host_name>
    def ENABLE_HOST_NOTIFICATIONS(self, host):
        if not host.notifications_enabled:
            host.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            host.notifications_enabled = True
            self.sched.get_and_register_status_brok(host)

    # ENABLE_HOST_SVC_CHECKS;<host_name>
    def ENABLE_HOST_SVC_CHECKS(self, host):
        for s in host.services:
            self.ENABLE_SVC_CHECK(s)

    # ENABLE_HOST_SVC_NOTIFICATIONS;<host_name>
    def ENABLE_HOST_SVC_NOTIFICATIONS(self, host):
        for s in host.services:
            self.ENABLE_SVC_NOTIFICATIONS(s)
            self.sched.get_and_register_status_brok(s)

    # ENABLE_NOTIFICATIONS
    def ENABLE_NOTIFICATIONS(self):
        if not self.conf.enable_notifications:
            self.conf.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            self.conf.enable_notifications = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_PASSIVE_HOST_CHECKS;<host_name>
    def ENABLE_PASSIVE_HOST_CHECKS(self, host):
        if not host.passive_checks_enabled:
            host.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            host.passive_checks_enabled = True
            self.sched.get_and_register_status_brok(host)

    # ENABLE_PASSIVE_SVC_CHECKS;<host_name>;<service_description>
    def ENABLE_PASSIVE_SVC_CHECKS(self, service):
        if not service.passive_checks_enabled:
            service.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            service.passive_checks_enabled = True
            self.sched.get_and_register_status_brok(service)

    # ENABLE_PERFORMANCE_DATA
    def ENABLE_PERFORMANCE_DATA(self):
        if not self.conf.process_performance_data:
            self.conf.modified_attributes |= MODATTR_PERFORMANCE_DATA_ENABLED
            self.conf.process_performance_data = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_SERVICEGROUP_HOST_CHECKS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_HOST_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_HOST_CHECK(service.host)

    # ENABLE_SERVICEGROUP_HOST_NOTIFICATIONS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_HOST_NOTIFICATIONS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_HOST_NOTIFICATIONS(service.host)

    # ENABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_PASSIVE_HOST_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_PASSIVE_HOST_CHECKS(service.host)

    # ENABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_PASSIVE_SVC_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_PASSIVE_SVC_CHECKS(service)

    # ENABLE_SERVICEGROUP_SVC_CHECKS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_SVC_CHECKS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_SVC_CHECK(service)

    # ENABLE_SERVICEGROUP_SVC_NOTIFICATIONS;<servicegroup_name>
    def ENABLE_SERVICEGROUP_SVC_NOTIFICATIONS(self, servicegroup):
        for service in servicegroup:
            self.ENABLE_SVC_NOTIFICATIONS(service)

    # ENABLE_SERVICE_FRESHNESS_CHECKS
    def ENABLE_SERVICE_FRESHNESS_CHECKS(self):
        if not self.conf.check_service_freshness:
            self.conf.modified_attributes |= MODATTR_FRESHNESS_CHECKS_ENABLED
            self.conf.check_service_freshness = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # ENABLE_SVC_CHECK;<host_name>;<service_description>
    def ENABLE_SVC_CHECK(self, service):
        if not service.active_checks_enabled:
            service.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            service.active_checks_enabled = True
            self.sched.get_and_register_status_brok(service)

    # ENABLE_SVC_EVENT_HANDLER;<host_name>;<service_description>
    def ENABLE_SVC_EVENT_HANDLER(self, service):
        if not service.event_handler_enabled:
            service.modified_attributes |= MODATTR_EVENT_HANDLER_ENABLED
            service.event_handler_enabled = True
            self.sched.get_and_register_status_brok(service)

    # ENABLE_SVC_FLAP_DETECTION;<host_name>;<service_description>
    def ENABLE_SVC_FLAP_DETECTION(self, service):
        if not service.flap_detection_enabled:
            service.modified_attributes |= MODATTR_FLAP_DETECTION_ENABLED
            service.flap_detection_enabled = True
            self.sched.get_and_register_status_brok(service)

    # ENABLE_SVC_NOTIFICATIONS;<host_name>;<service_description>
    def ENABLE_SVC_NOTIFICATIONS(self, service):
        if not service.notifications_enabled:
            service.modified_attributes |= MODATTR_NOTIFICATIONS_ENABLED
            service.notifications_enabled = True
            self.sched.get_and_register_status_brok(service)

    # PROCESS_FILE;<file_name>;<delete>
    def PROCESS_FILE(self, file_name, delete):
        pass

    # TODO: say that check is PASSIVE
    # PROCESS_HOST_CHECK_RESULT;<host_name>;<status_code>;<plugin_output>
    def PROCESS_HOST_CHECK_RESULT(self, host, status_code, plugin_output):
        #raise a PASSIVE check only if needed
        if self.conf.log_passive_checks:
            console_logger.info('PASSIVE HOST CHECK: %s;%d;%s'
                                % (host.get_name().decode('utf8', 'ignore'), status_code, plugin_output.decode('utf8', 'ignore')))
        now = time.time()
        cls = host.__class__
        # If globally disable OR locally, do not launch
        if cls.accept_passive_checks and host.passive_checks_enabled:
            # Maybe the check is just too old, if so, bail out!
            if self.current_timestamp < host.last_chk:
                return

            i = host.launch_check(now, force=True)
            c = None
            for chk in host.checks_in_progress:
                if chk.id == i:
                    c = chk
            # Should not be possible to not find the check, but if so, don't crash
            if not c:
                console_logger.error('Passive host check failed. Cannot find the check id %s' % i)
                return
            # Now we 'transform the check into a result'
            # So exit_status, output and status is eaten by the host
            c.exit_status = status_code
            c.get_outputs(plugin_output, host.max_plugins_output_length)
            c.status = 'waitconsume'
            c.check_time = self.current_timestamp  # we are using the external command timestamps
            # Set the corresponding host's check_type to passive=1
            c.set_type_passive()
            self.sched.nb_check_received += 1
            # Ok now this result will be read by scheduler the next loop

    # PROCESS_HOST_OUTPUT;<host_name>;<plugin_output>
    def PROCESS_HOST_OUTPUT(self, host, plugin_output):
        self.PROCESS_HOST_CHECK_RESULT(host, host.state_id, plugin_output)

    # PROCESS_SERVICE_CHECK_RESULT;<host_name>;<service_description>;<return_code>;<plugin_output>
    def PROCESS_SERVICE_CHECK_RESULT(self, service, return_code, plugin_output):
        # raise a PASSIVE check only if needed
        if self.conf.log_passive_checks:
            console_logger.info('PASSIVE SERVICE CHECK: %s;%s;%d;%s'
                                % (service.host.get_name().decode('utf8', 'ignore'), service.get_name().decode('utf8', 'ignore'),
                                   return_code, plugin_output.decode('utf8', 'ignore')))
        now = time.time()
        cls = service.__class__
        # If globally disable OR locally, do not launch
        if cls.accept_passive_checks and service.passive_checks_enabled:
            # Maybe the check is just too old, if so, bail out!
            if self.current_timestamp < service.last_chk:
                return

            c = None
            i = service.launch_check(now, force=True)
            for chk in service.checks_in_progress:
                if chk.id == i:
                    c = chk
            # Should not be possible to not find the check, but if so, don't crash
            if not c:
                console_logger.error('Passive service check failed. Cannot find the check id %s' % i)
                return                
            # Now we 'transform the check into a result'
            # So exit_status, output and status is eaten by the service
            c.exit_status = return_code
            c.get_outputs(plugin_output, service.max_plugins_output_length)
            c.status = 'waitconsume'
            c.check_time = self.current_timestamp  # we are using the external command timestamps
            # Set the corresponding service's check_type to passive=1
            c.set_type_passive()
            self.sched.nb_check_received += 1
            # Ok now this result will be reap by scheduler the next loop


    # PROCESS_SERVICE_CHECK_RESULT;<host_name>;<service_description>;<plugin_output>
    def PROCESS_SERVICE_OUTPUT(self, service, plugin_output):
        self.PROCESS_SERVICE_CHECK_RESULT(service, service.state_id, plugin_output)

    # READ_STATE_INFORMATION
    def READ_STATE_INFORMATION(self):
        pass

    # REMOVE_HOST_ACKNOWLEDGEMENT;<host_name>
    def REMOVE_HOST_ACKNOWLEDGEMENT(self, host):
        host.unacknowledge_problem()

    # REMOVE_SVC_ACKNOWLEDGEMENT;<host_name>;<service_description>
    def REMOVE_SVC_ACKNOWLEDGEMENT(self, service):
        service.unacknowledge_problem()

    # RESTART_PROGRAM
    def RESTART_PROGRAM(self):
        restart_cmd = self.commands.find_by_name('restart_shinken')
        if not restart_cmd:
            console_logger.error("Cannot restart Shinken : missing command named 'restart_shinken'. Please add one")
            return
        restart_cmd_line = restart_cmd.command_line
        
        # Ok get an event handler command that will run in 15min max
        e = EventHandler(restart_cmd_line, timeout=900)
        # Ok now run it
        e.execute()
        # And wait for the command to finish
        while not e.status in ('done', 'timeout'):
            e.check_finished(64000)
        if e.status == 'timeout' or e.exit_status != 0:
            console_logger.error("Cannot restart Shinken : the 'restart_shinken' command failed with the error code '%d' and the text '%s'." % (e.exit_status, e.output))
            return
        # Ok here the command succeed, we can now wait our death
        console_logger.info("%s\%s" % (e.output, e.long_output))
        console_logger.info("RESTART command launched. Waiting for the new daemon to kill us")
        
        
        

    # SAVE_STATE_INFORMATION
    def SAVE_STATE_INFORMATION(self):
        pass

    # SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME(self, host, start_time, end_time, fixed, trigger_id, duration, author, comment):
        pass

    # SCHEDULE_AND_PROPAGATE_TRIGGERED_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_AND_PROPAGATE_TRIGGERED_HOST_DOWNTIME(self, host, start_time, end_time, fixed, trigger_id, duration, author, comment):
        pass

    # SCHEDULE_CONTACT_DOWNTIME;<contact_name>;<start_time>;<end_time>;<author>;<comment>
    def SCHEDULE_CONTACT_DOWNTIME(self, contact, start_time, end_time, author, comment):
        dt = ContactDowntime(contact, start_time, end_time, author, comment)
        contact.add_downtime(dt)
        self.sched.add(dt)
        self.sched.get_and_register_status_brok(contact)

    # SCHEDULE_FORCED_HOST_CHECK;<host_name>;<check_time>
    def SCHEDULE_FORCED_HOST_CHECK(self, host, check_time):
        host.schedule(force=True, force_time=check_time)
        self.sched.get_and_register_status_brok(host)

    # SCHEDULE_FORCED_HOST_SVC_CHECKS;<host_name>;<check_time>
    def SCHEDULE_FORCED_HOST_SVC_CHECKS(self, host, check_time):
        for s in host.services:
            self.SCHEDULE_FORCED_SVC_CHECK(s, check_time)
            self.sched.get_and_register_status_brok(s)

    # SCHEDULE_FORCED_SVC_CHECK;<host_name>;<service_description>;<check_time>
    def SCHEDULE_FORCED_SVC_CHECK(self, service, check_time):
        service.schedule(force=True, force_time=check_time)
        self.sched.get_and_register_status_brok(service)

    # SCHEDULE_HOSTGROUP_HOST_DOWNTIME;<hostgroup_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_HOSTGROUP_HOST_DOWNTIME(self, hostgroup, start_time, end_time, fixed, trigger_id, duration, author, comment):
        pass

    # SCHEDULE_HOSTGROUP_SVC_DOWNTIME;<hostgroup_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_HOSTGROUP_SVC_DOWNTIME(self, hostgroup, start_time, end_time, fixed, trigger_id, duration, author, comment):
        pass

    # SCHEDULE_HOST_CHECK;<host_name>;<check_time>
    def SCHEDULE_HOST_CHECK(self, host, check_time):
        host.schedule(force=False, force_time=check_time)
        self.sched.get_and_register_status_brok(host)

    # SCHEDULE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_HOST_DOWNTIME(self, host, start_time, end_time, fixed, trigger_id, duration, author, comment):
        dt = Downtime(host, start_time, end_time, fixed, trigger_id, duration, author, comment)
        host.add_downtime(dt)
        self.sched.add(dt)
        self.sched.get_and_register_status_brok(host)
        if trigger_id != 0 and trigger_id in self.sched.downtimes:
            self.sched.downtimes[trigger_id].trigger_me(dt)

    # SCHEDULE_HOST_SVC_CHECKS;<host_name>;<check_time>
    def SCHEDULE_HOST_SVC_CHECKS(self, host, check_time):
        for s in host.services:
            self.SCHEDULE_SVC_CHECK(s, check_time)
            self.sched.get_and_register_status_brok(s)

    # SCHEDULE_HOST_SVC_DOWNTIME;<host_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_HOST_SVC_DOWNTIME(self, host, start_time, end_time, fixed, trigger_id, duration, author, comment):
        for s in host.services:
            self.SCHEDULE_SVC_DOWNTIME(s, start_time, end_time, fixed, trigger_id, duration, author, comment)

    # SCHEDULE_SERVICEGROUP_HOST_DOWNTIME;<servicegroup_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_SERVICEGROUP_HOST_DOWNTIME(self, servicegroup, start_time, end_time, fixed, trigger_id, duration, author, comment):
        for h in [s.host for s in servicegroup.get_services()]:
            self.SCHEDULE_HOST_DOWNTIME(h, start_time, end_time, fixed, trigger_id, duration, author, comment)

    # SCHEDULE_SERVICEGROUP_SVC_DOWNTIME;<servicegroup_name>;<start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_SERVICEGROUP_SVC_DOWNTIME(self, servicegroup, start_time, end_time, fixed, trigger_id, duration, author, comment):
        for s in servicegroup.get_services():
            self.SCHEDULE_SVC_DOWNTIME(s, start_time, end_time, fixed, trigger_id, duration, author, comment)

    # SCHEDULE_SVC_CHECK;<host_name>;<service_description>;<check_time>
    def SCHEDULE_SVC_CHECK(self, service, check_time):
        service.schedule(force=False, force_time=check_time)
        self.sched.get_and_register_status_brok(service)

    # SCHEDULE_SVC_DOWNTIME;<host_name>;<service_description><start_time>;<end_time>;<fixed>;<trigger_id>;<duration>;<author>;<comment>
    def SCHEDULE_SVC_DOWNTIME(self, service, start_time, end_time, fixed, trigger_id, duration, author, comment):
        dt = Downtime(service, start_time, end_time, fixed, trigger_id, duration, author, comment)
        service.add_downtime(dt)
        self.sched.add(dt)
        self.sched.get_and_register_status_brok(service)
        if trigger_id != 0 and trigger_id in self.sched.downtimes:
            self.sched.downtimes[trigger_id].trigger_me(dt)

    # SEND_CUSTOM_HOST_NOTIFICATION;<host_name>;<options>;<author>;<comment>
    def SEND_CUSTOM_HOST_NOTIFICATION(self, host, options, author, comment):
        pass

    # SEND_CUSTOM_SVC_NOTIFICATION;<host_name>;<service_description>;<options>;<author>;<comment>
    def SEND_CUSTOM_SVC_NOTIFICATION(self, service, options, author, comment):
        pass

    # SET_HOST_NOTIFICATION_NUMBER;<host_name>;<notification_number>
    def SET_HOST_NOTIFICATION_NUMBER(self, host, notification_number):
        pass

    # SET_SVC_NOTIFICATION_NUMBER;<host_name>;<service_description>;<notification_number>
    def SET_SVC_NOTIFICATION_NUMBER(self, service, notification_number):
        pass

    # SHUTDOWN_PROGRAM
    def SHUTDOWN_PROGRAM(self):
        pass

    # START_ACCEPTING_PASSIVE_HOST_CHECKS
    def START_ACCEPTING_PASSIVE_HOST_CHECKS(self):
        if not self.conf.accept_passive_host_checks:
            self.conf.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            self.conf.accept_passive_host_checks = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_ACCEPTING_PASSIVE_SVC_CHECKS
    def START_ACCEPTING_PASSIVE_SVC_CHECKS(self):
        if not self.conf.accept_passive_service_checks:
            self.conf.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            self.conf.accept_passive_service_checks = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_EXECUTING_HOST_CHECKS
    def START_EXECUTING_HOST_CHECKS(self):
        if not self.conf.execute_host_checks:
            self.conf.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.conf.execute_host_checks = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_EXECUTING_SVC_CHECKS
    def START_EXECUTING_SVC_CHECKS(self):
        if not self.conf.execute_service_checks:
            self.conf.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.conf.execute_service_checks = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_HOST;<host_name>
    def START_OBSESSING_OVER_HOST(self, host):
        if not host.obsess_over_host:
            host.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            host.obsess_over_host = True
            self.sched.get_and_register_status_brok(host)

    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_services = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_ACCEPTING_PASSIVE_HOST_CHECKS
    def STOP_ACCEPTING_PASSIVE_HOST_CHECKS(self):
        if self.conf.accept_passive_host_checks:
            self.conf.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            self.conf.accept_passive_host_checks = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_ACCEPTING_PASSIVE_SVC_CHECKS
    def STOP_ACCEPTING_PASSIVE_SVC_CHECKS(self):
        if self.conf.accept_passive_service_checks:
            self.conf.modified_attributes |= MODATTR_PASSIVE_CHECKS_ENABLED
            self.conf.accept_passive_service_checks = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_EXECUTING_HOST_CHECKS
    def STOP_EXECUTING_HOST_CHECKS(self):
        if self.conf.execute_host_checks:
            self.conf.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.conf.execute_host_checks = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_EXECUTING_SVC_CHECKS
    def STOP_EXECUTING_SVC_CHECKS(self):
        if self.conf.execute_service_checks:
            self.conf.modified_attributes |= MODATTR_ACTIVE_CHECKS_ENABLED
            self.conf.execute_service_checks = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_OBSESSING_OVER_HOST;<host_name>
    def STOP_OBSESSING_OVER_HOST(self, host):
        if host.obsess_over_host:
            host.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            host.obsess_over_host = False
            self.sched.get_and_register_status_brok(host)

    # STOP_OBSESSING_OVER_HOST_CHECKS
    def STOP_OBSESSING_OVER_HOST_CHECKS(self):
        if self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # STOP_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def STOP_OBSESSING_OVER_SVC(self, service):
        if service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = False
            self.sched.get_and_register_status_brok(service)

    # STOP_OBSESSING_OVER_SVC_CHECKS
    def STOP_OBSESSING_OVER_SVC_CHECKS(self):
        if self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_services = False
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    ### Now the shinken specific ones
    # LAUNCH_SVC_EVENT_HANDLER;<host_name>;<service_description>
    def LAUNCH_SVC_EVENT_HANDLER(self, service):
        service.get_event_handlers(externalcmd=True)

    # LAUNCH_SVC_EVENT_HANDLER;<host_name>;<service_description>
    def LAUNCH_HOST_EVENT_HANDLER(self, host):
        host.get_event_handlers(externalcmd=True)

    # ADD_SIMPLE_HOST_DEPENDENCY;<host_name>;<host_name>
    def ADD_SIMPLE_HOST_DEPENDENCY(self, son, father):
        if not son.is_linked_with_host(father):
            logger.debug("Doing simple link between %s and %s" % (son.get_name(), father.get_name()))
            # Flag them so the modules will know that a topology change
            # happened
            son.topology_change = True
            father.topology_change = True
            # Now do the work
            # Add a dep link between the son and the father
            son.add_host_act_dependency(father, ['w', 'u', 'd'], None, True)
            self.sched.get_and_register_status_brok(son)
            self.sched.get_and_register_status_brok(father)

    # ADD_SIMPLE_HOST_DEPENDENCY;<host_name>;<host_name>
    def DEL_HOST_DEPENDENCY(self, son, father):
        if son.is_linked_with_host(father):
            logger.debug("Removing simple link between %s and %s" % (son.get_name(), father.get_name()))
            # Flag them so the modules will know that a topology change
            # happened
            son.topology_change = True
            father.topology_change = True
            # Now do the work
            son.del_host_act_dependency(father)
            self.sched.get_and_register_status_brok(son)
            self.sched.get_and_register_status_brok(father)

    # ADD_SIMPLE_POLLER;realm_name;poller_name;address;port
    def ADD_SIMPLE_POLLER(self, realm_name, poller_name, address, port):
        logger.debug("I need to add the poller (%s, %s, %s, %s)" % (realm_name, poller_name, address, port))

        # First we look for the realm
        r = self.conf.realms.find_by_name(realm_name)
        if r is None:
            logger.debug("Sorry, the realm %s is unknown" % realm_name)
            return

        logger.debug("We found the realm: %s" % str(r))
        # TODO: backport this in the config class?
        # We create the PollerLink object
        t = {'poller_name': poller_name, 'address': address, 'port': port}
        p = PollerLink(t)
        p.fill_default()
        p.pythonize()
        p.prepare_for_conf()
        parameters = {'max_plugins_output_length': self.conf.max_plugins_output_length}
        p.add_global_conf_parameters(parameters)
        self.arbiter.conf.pollers[p.id] = p
        self.arbiter.dispatcher.elements.append(p)
        self.arbiter.dispatcher.satellites.append(p)
        r.pollers.append(p)
        r.count_pollers()
        r.fill_potential_pollers()
        logger.debug("Poller %s added" % poller_name)
        logger.debug("Potential %s" % str(r.get_potential_satellites_by_type('poller')))


if __name__ == '__main__':

    FIFO_PATH = '/tmp/my_fifo'

    if os.path.exists(FIFO_PATH):
        os.unlink(FIFO_PATH)

    if not os.path.exists(FIFO_PATH):
        os.umask(0)
        os.mkfifo(FIFO_PATH, 0660)
        my_fifo = open(FIFO_PATH, 'w+')
        logger.debug("my_fifo: %s" % (my_fifo))

    logger.debug(open(FIFO_PATH, 'r').readline())

########NEW FILE########
__FILENAME__ = graph
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class Graph:
    """Graph is a class to make graph things like DFS checks or accessibility
    Why use an atomic bomb when a little hammer is enough?

    """

    def __init__(self):
        self.nodes = {}

    # Do not call twice...
    def add_node(self, node):
        self.nodes[node] = []

    # Just loop over nodes
    def add_nodes(self, nodes):
        for node in nodes:
            self.add_node(node)

    # Add an edge to the graph from->to
    def add_edge(self, from_node, to_node):
        # Maybe to_node is unknown
        if to_node not in self.nodes:
            self.add_node(to_node)

        try:
            self.nodes[from_node].append(to_node)
        # If from_node does not exist, add it with its son
        except KeyError, exp:
            self.nodes[from_node] = [to_node]

    # Return all nodes that are in a loop. So if return [], no loop
    def loop_check(self):
        in_loop = []
        # Add the tag for dfs check
        for node in self.nodes:
            node.dfs_loop_status = 'DFS_UNCHECKED'

        # Now do the job
        for node in self.nodes:
            # Run the dfs only if the node has not been already done */
            if node.dfs_loop_status == 'DFS_UNCHECKED':
                self.dfs_loop_search(node)
            # If LOOP_INSIDE, must be returned
            if node.dfs_loop_status == 'DFS_LOOP_INSIDE':
                in_loop.append(node)

        # Remove the tag
        for node in self.nodes:
            del node.dfs_loop_status

        return in_loop

    # DFS_UNCHECKED default value
    # DFS_TEMPORARY_CHECKED check just one time
    # DFS_OK no problem for node and its children
    # DFS_NEAR_LOOP has trouble sons
    # DFS_LOOP_INSIDE is a part of a loop!
    def dfs_loop_search(self, root):
        # Make the root temporary checked
        root.dfs_loop_status = 'DFS_TEMPORARY_CHECKED'

        # We are scanning the sons
        for child in self.nodes[root]:
            child_status = child.dfs_loop_status
            # If a child is not checked, check it
            if child_status == 'DFS_UNCHECKED':
                self.dfs_loop_search(child)
                child_status = child.dfs_loop_status

            # If a child has already been temporary checked, it's a problem,
            # loop inside, and its a acked status
            if child_status == 'DFS_TEMPORARY_CHECKED':
                child.dfs_loop_status = 'DFS_LOOP_INSIDE'
                root.dfs_loop_status = 'DFS_LOOP_INSIDE'

            # If a child has already been temporary checked, it's a problem, loop inside
            if child_status in ('DFS_NEAR_LOOP', 'DFS_LOOP_INSIDE'):
                # if a node is known to be part of a loop, do not let it be less
                if root.dfs_loop_status != 'DFS_LOOP_INSIDE':
                    root.dfs_loop_status = 'DFS_NEAR_LOOP'
                # We've already seen this child, it's a problem
                child.dfs_loop_status = 'DFS_LOOP_INSIDE'

        # If root have been modified, do not set it OK
        # A node is OK if and only if all of its children are OK
        # if it does not have a child, goes ok
        if root.dfs_loop_status == 'DFS_TEMPORARY_CHECKED':
            root.dfs_loop_status = 'DFS_OK'

    # Get accessibility packs of the graph: in one pack,
    # element are related in a way. Between packs, there is no relation
    # at all.
    # TODO: Make it work for directional graph too
    # Because for now, edge must be father->son AND son->father
    def get_accessibility_packs(self):
        packs = []
        # Add the tag for dfs check
        for node in self.nodes:
            node.dfs_loop_status = 'DFS_UNCHECKED'

        for node in self.nodes:
            # Run the dfs only if the node is not already done */
            if node.dfs_loop_status == 'DFS_UNCHECKED':
                packs.append(self.dfs_get_all_childs(node))

        # Remove the tag
        for node in self.nodes:
            del node.dfs_loop_status

        return packs

    # Return all my children, and all my grandchildren
    def dfs_get_all_childs(self, root):
        root.dfs_loop_status = 'DFS_CHECKED'

        ret = set()
        # Me
        ret.add(root)
        # And my sons
        ret.update(self.nodes[root])

        for child in self.nodes[root]:
            # I just don't care about already checked childs
            if child.dfs_loop_status == 'DFS_UNCHECKED':
                ret.update(self.dfs_get_all_childs(child))

        return list(ret)

########NEW FILE########
__FILENAME__ = http_client
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import socket
import cPickle
import zlib
import json

# Pycurl part
import pycurl
pycurl.global_init(pycurl.GLOBAL_ALL)
import urllib
from StringIO import StringIO

from shinken.bin import VERSION
from shinken.log import logger
PYCURL_VERSION = pycurl.version_info()[1]

class HTTPException(Exception):
    pass


HTTPExceptions = (HTTPException,)


class HTTPClient(object):
    def __init__(self, address='', port=0, use_ssl=False, timeout=3, data_timeout=120, uri='', strong_ssl=False):
        self.address = address
        self.port    = port
        self.timeout = timeout
        self.data_timeout = data_timeout
        
        if not uri:
            if use_ssl:
                self.uri = "https://%s:%s/" % (self.address, self.port)
            else:
                self.uri = "http://%s:%s/" % (self.address, self.port)
        else:
            self.uri = uri
        
        self.con     = pycurl.Curl()
        
        # Remove the Expect: 100-Continue default behavior of pycurl, because swsgiref do not
        # manage it
        self.con.setopt(pycurl.HTTPHEADER, ['Expect:', 'Keep-Alive: 300', 'Connection: Keep-Alive'])
        self.con.setopt(pycurl.USERAGENT,  'shinken:%s pycurl:%s' % (VERSION, PYCURL_VERSION) )
        self.con.setopt(pycurl.FOLLOWLOCATION, 1)
        self.con.setopt(pycurl.FAILONERROR, True)
        self.con.setopt(pycurl.CONNECTTIMEOUT, self.timeout)
        self.con.setopt(pycurl.HTTP_VERSION, pycurl.CURL_HTTP_VERSION_1_1)
        

        # Also set the SSL options to do not look at the certificates too much
        # unless the admin asked for it
        if strong_ssl:
            self.con.setopt(pycurl.SSL_VERIFYPEER, 1)
            self.con.setopt(pycurl.SSL_VERIFYHOST, 2)
        else:
            self.con.setopt(pycurl.SSL_VERIFYPEER, 0)
            self.con.setopt(pycurl.SSL_VERIFYHOST, 0)
            

    
    # Try to get an URI path
    def get(self, path, args={}, wait='short'):
        c = self.con
        c.setopt(c.POST, 0)
        c.setopt(pycurl.HTTPGET, 1)


        # For the TIMEOUT, it will depends if we are waiting for a long query or not
        # long:data_timeout, like for huge broks receptions
        # short:timeout, like for just "ok" connection
        if wait == 'short':
            c.setopt(c.TIMEOUT, self.timeout)
        else:
            c.setopt(c.TIMEOUT, self.data_timeout)

        #if proxy:
        #    c.setopt(c.PROXY, proxy)
        #print "GO TO", self.uri+path+'?'+urllib.urlencode(args)
        c.setopt(c.URL, str(self.uri+path+'?'+urllib.urlencode(args)))
        # Ok now manage the response
        response = StringIO()
        c.setopt(pycurl.WRITEFUNCTION, response.write)
        #c.setopt(c.VERBOSE, 1)
        try:
            c.perform()
        except pycurl.error, error:
            errno, errstr = error
            raise HTTPException ('Connexion error to %s : %s' % (self.uri, errstr))
        r = c.getinfo(pycurl.HTTP_CODE)
        # Do NOT close the connexion, we want a keep alive

        if r != 200:
            err = response.getvalue()
            logger.error("There was a critical error : %s" % err)
            raise Exception ('Connexion error to %s : %s' % (self.uri, r))
        else:
            # Manage special return of pycurl
            ret  = json.loads(response.getvalue().replace('\\/', '/'))
            # print "GOT RAW RESULT", ret, type(ret)
            return ret
        

    # Try to get an URI path
    def post(self, path, args, wait='short'):
        # Take args, pickle them and then compress the result
        for (k,v) in args.iteritems():
            args[k] = zlib.compress(cPickle.dumps(v), 2)
        # Ok go for it!
        
        c = self.con
        c.setopt(pycurl.HTTPGET, 0)
        c.setopt(c.POST, 1)

        # For the TIMEOUT, it will depends if we are waiting for a long query or not
        # long:data_timeout, like for huge broks receptions
        # short:timeout, like for just "ok" connection
        if wait == 'short':
            c.setopt(c.TIMEOUT, self.timeout)
        else:
            c.setopt(c.TIMEOUT, self.data_timeout)
        #if proxy:
        #    c.setopt(c.PROXY, proxy)
        # Pycurl want a list of tuple as args
        postargs = [(k,v) for (k,v) in args.iteritems()]
        c.setopt(c.HTTPPOST, postargs)
        c.setopt(c.URL, str(self.uri+path))
        # Ok now manage the response
        response = StringIO()
        c.setopt(pycurl.WRITEFUNCTION, response.write)
        #c.setopt(c.VERBOSE, 1)
        try:
            c.perform()
        except pycurl.error, error:
            errno, errstr = error
            raise HTTPException ('Connexion error to %s : %s' % (self.uri, errstr))

        r = c.getinfo(pycurl.HTTP_CODE)
        # Do NOT close the connexion
        #c.close()
        if r != 200:
            err = response.getvalue()
            logger.error("There was a critical error : %s" % err)
            raise Exception ('Connexion error to %s : %s' % (self.uri, r))
        else:
            # Manage special return of pycurl
            #ret  = json.loads(response.getvalue().replace('\\/', '/'))
            ret = response.getvalue()
            return ret

        
        
        # Should return us pong string
        return ret

    

########NEW FILE########
__FILENAME__ = http_daemon
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import select
import errno
import time
import socket
import select
import copy
import cPickle
import inspect
import json
import zlib
import threading
try:
    import ssl
except ImportError:
    ssl = None

try:
    from cherrypy import wsgiserver as cheery_wsgiserver
except ImportError:
    cheery_wsgiserver = None

from wsgiref import simple_server

from log import logger

# Let's load bottlecore! :)
from shinken.webui import bottlecore as bottle
bottle.debug(True)



class InvalidWorkDir(Exception):
    pass


class PortNotFree(Exception):
    pass





# CherryPy is allowing us to have a HTTP 1.1 server, and so have a KeepAlive
class CherryPyServer(bottle.ServerAdapter):
    def run(self, handler):  # pragma: no cover
        daemon_thread_pool_size = self.options['daemon_thread_pool_size']
        server = cheery_wsgiserver.CherryPyWSGIServer((self.host, self.port), handler, numthreads=daemon_thread_pool_size, shutdown_timeout=1)
        logger.info('Initializing a CherryPy backend with %d threads' % daemon_thread_pool_size)
        use_ssl = self.options['use_ssl']
        ca_cert = self.options['ca_cert']
        ssl_cert = self.options['ssl_cert']
        ssl_key = self.options['ssl_key']


        if use_ssl:
            server.ssl_certificate = ssl_cert
            server.ssl_private_key = ssl_key
        return server



class CherryPyBackend(object):
    def __init__(self, host, port, use_ssl, ca_cert, ssl_key, ssl_cert, hard_ssl_name_check, daemon_thread_pool_size):
        self.port = port
        self.use_ssl = use_ssl
        try:
            self.srv = bottle.run(host=host, port=port, server=CherryPyServer, quiet=False, use_ssl=use_ssl, ca_cert=ca_cert, ssl_key=ssl_key, ssl_cert=ssl_cert, daemon_thread_pool_size=daemon_thread_pool_size)
        except socket.error, exp:
            msg = "Error: Sorry, the port %d is not free: %s" % (self.port, str(exp))
            raise PortNotFree(msg)
        except Exception, e:
            # must be a problem with pyro workdir:
            raise InvalidWorkDir(e)


    # When call, it do not have a socket
    def get_sockets(self):
        return []


    # We stop our processing, but also try to hard close our socket as cherrypy is not doing it...
    def stop(self):
        #TODO: find why, but in ssl mode the stop() is locking, so bailout before
        if self.use_ssl:
            return
        try:
            self.srv.stop()
        except Exception, exp:
            logger.warning('Cannot stop the CherryPy backend : %s' % exp)


    # Will run and LOCK
    def run(self):
        try:
            self.srv.start()
        except socket.error, exp:
            msg = "Error: Sorry, the port %d is not free: %s" % (self.port, str(exp))
            raise PortNotFree(msg)
        finally:
            self.srv.stop()


# WSGIRef is the default HTTP server, it CAN manage HTTPS, but at a Huge cost for the client, because it's only HTTP1.0
# so no Keep-Alive, and in HTTPS it's just a nightmare
class WSGIREFAdapter (bottle.ServerAdapter):
    def run (self, handler):
        daemon_thread_pool_size = self.options['daemon_thread_pool_size']
        from wsgiref.simple_server import WSGIRequestHandler
        LoggerHandler = WSGIRequestHandler
        if self.quiet:
            class QuietHandler(WSGIRequestHandler):
                def log_request(*args, **kw): pass
            LoggerHandler = QuietHandler

        srv = simple_server.make_server(self.host, self.port, handler, handler_class=LoggerHandler)
        logger.info('Initializing a wsgiref backend with %d threads' % daemon_thread_pool_size)
        use_ssl = self.options['use_ssl']
        ca_cert = self.options['ca_cert']
        ssl_cert = self.options['ssl_cert']
        ssl_key = self.options['ssl_key']

        if use_ssl:
            if not ssl:
                logger.error("Missing python-openssl librairy, please install it to open a https backend")
                raise Exception("Missing python-openssl librairy, please install it to open a https backend")
            srv.socket = ssl.wrap_socket(srv.socket,
                                         keyfile=ssl_key, certfile=ssl_cert, server_side=True)
        return srv


class WSGIREFBackend(object):
    def __init__(self, host, port, use_ssl, ca_cert, ssl_key, ssl_cert, hard_ssl_name_check, daemon_thread_pool_size):
        self.daemon_thread_pool_size = daemon_thread_pool_size
        try:
            self.srv = bottle.run(host=host, port=port, server=WSGIREFAdapter, quiet=True, use_ssl=use_ssl, ca_cert=ca_cert, ssl_key=ssl_key, ssl_cert=ssl_cert, daemon_thread_pool_size=daemon_thread_pool_size)
        except socket.error, exp:
            msg = "Error: Sorry, the port %d is not free: %s" % (port, str(exp))
            raise PortNotFree(msg)
        except Exception, e:
            # must be a problem with pyro workdir:
            raise e


    def get_sockets(self):
        if self.srv.socket:
            return [self.srv.socket]
        else:
            return []


    def get_socks_activity(self, socks, timeout):
        try:
            ins, _, _ = select.select(socks, [], [], timeout)
        except select.error, e:
            errnum, _ = e
            if errnum == errno.EINTR:
                return []
            raise
        return ins


    # We are asking us to stop, so we close our sockets
    def stop(self):
        for s in self.get_sockets():
            try:
                s.close()
            except:
                pass
            self.srv.socket = None


    # Manually manage the number of threads
    def run(self):
        # Ok create the thread
        nb_threads = self.daemon_thread_pool_size
        # Keep a list of our running threads
        threads = []
        logger.info('Using a %d http pool size' % nb_threads)
        while True:
            # We must not run too much threads, so we will loop until
            # we got at least one free slot available
            free_slots = 0
            while free_slots <= 0:
                to_del = [t for t in threads if not t.is_alive()]
                _ = [t.join() for t in to_del]
                for t in to_del:
                    threads.remove(t)
                free_slots = nb_threads - len(threads)
                if free_slots <= 0:
                    time.sleep(0.01)

            socks = self.get_sockets()
            # Blocking for 0.1 s max here
            ins = self.get_socks_activity(socks, 0.1)
            if len(ins) == 0:  # trivial case: no fd activity:
                continue
            # If we got activity, Go for a new thread!
            for sock in socks:
                if sock in ins:
                    # GO!
                    t = threading.Thread(None, target=self.handle_one_request_thread, name='http-request', args=(sock,))
                    # We don't want to hang the master thread just because this one is still alive
                    t.daemon = True
                    t.start()
                    threads.append(t)


    def handle_one_request_thread(self, sock):
        self.srv.handle_request()



class HTTPDaemon(object):
        def __init__(self, host, port, http_backend, use_ssl, ca_cert, ssl_key, ssl_cert, hard_ssl_name_check, daemon_thread_pool_size):
            self.port = port
            self.host = host
            # Port = 0 means "I don't want HTTP server"
            if self.port == 0:
                return

            self.use_ssl = use_ssl
            
            self.registered_fun = {}
            self.registered_fun_names = []
            self.registered_fun_defaults = {}

            protocol = 'http'
            if use_ssl:
                protocol = 'https'
            self.uri = '%s://%s:%s' % (protocol, self.host, self.port)
            logger.info("Opening HTTP socket at %s" % self.uri)

            # Hack the BaseHTTPServer so only IP will be looked by wsgiref, and not names
            __import__('BaseHTTPServer').BaseHTTPRequestHandler.address_string = lambda x:x.client_address[0]

            if http_backend == 'cherrypy' or http_backend == 'auto' and cheery_wsgiserver:
                self.srv = CherryPyBackend(host, port, use_ssl, ca_cert, ssl_key, ssl_cert, hard_ssl_name_check, daemon_thread_pool_size)
            else:
                self.srv = WSGIREFBackend(host, port, use_ssl, ca_cert, ssl_key, ssl_cert, hard_ssl_name_check, daemon_thread_pool_size)

            self.lock = threading.RLock()

            #@bottle.error(code=500)
            #def error500(err):
            #    print err.__dict__
            #    return 'FUCKING ERROR 500', str(err)



        # Get the server socket but not if disabled or closed
        def get_sockets(self):
            if self.port == 0 or self.srv is None:
                return []
            return self.srv.get_sockets()


        def run(self):
            self.srv.run()


        def register(self, obj):
            methods = inspect.getmembers(obj, predicate=inspect.ismethod)
            merge = [fname for (fname, f) in methods if fname in self.registered_fun_names ]
            if merge != []:
                methods_in = [m.__name__ for m in obj.__class__.__dict__.values() if inspect.isfunction(m)]
                methods = [m for m in methods if m[0] in methods_in]
                print "picking only bound methods of class and not parents"
            print "List to register :%s" % methods
            for (fname, f) in methods:
                if fname.startswith('_'):
                    continue
                # Get the args of the function to catch them in the queries
                argspec = inspect.getargspec(f)
                args = argspec.args
                varargs = argspec.varargs
                keywords = argspec.keywords
                defaults = argspec.defaults
                # If we got some defauts, save arg=value so we can lookup
                # for them after
                if defaults:
                    default_args = zip(argspec.args[-len(argspec.defaults):],argspec.defaults)
                    _d = {}
                    for (argname, defavalue) in default_args:
                        _d[argname] = defavalue
                    self.registered_fun_defaults[fname] = _d
                # remove useless self in args, because we alredy got a bonded method f
                if 'self' in args:
                    args.remove('self')
                print "Registering", fname, args, obj
                self.registered_fun_names.append(fname)
                self.registered_fun[fname] = (f)
                # WARNING : we MUST do a 2 levels function here, or the f_wrapper
                # will be uniq and so will link to the last function again
                # and again
                def register_callback(fname, args, f, obj, lock):
                    def f_wrapper():
                        t0 = time.time()
                        args_time = aqu_lock_time = calling_time = json_time = 0
                        need_lock = getattr(f, 'need_lock', True)

                        # Warning : put the bottle.response set inside the wrapper
                        # because outside it will break bottle
                        d = {}
                        method = getattr(f, 'method', 'get').lower()
                        for aname in args:
                            v = None
                            if method == 'post':
                                v = bottle.request.forms.get(aname, None)
                                # Post args are zlibed and cPickled
                                if v is not None:
                                    v = zlib.decompress(v)
                                    v = cPickle.loads(v)
                            elif method == 'get':
                                v = bottle.request.GET.get(aname, None)
                            if v is None:
                                # Maybe we got a default value?
                                default_args = self.registered_fun_defaults.get(fname, {})
                                if not aname in default_args:
                                    raise Exception('Missing argument %s' % aname)
                                v = default_args[aname]
                            d[aname] = v
                        args_time = time.time() - t0

                        if need_lock:
                            logger.debug("HTTP: calling lock for %s" % fname)
                            lock.acquire()
                        aqu_lock_time = time.time() - t0
                        
                        try:
                            ret = f(**d)
                        # Always call the lock release if need
                        finally:
                            # Ok now we can release the lock
                            if need_lock:
                                lock.release()
                        calling_time = time.time() - t0
                        encode = getattr(f, 'encode', 'json').lower()
                        j = json.dumps(ret)
                        json_time = time.time() - t0
                        logger.debug("Debug perf: %s [args:%s] [aqu_lock:%s] [calling:%s] [json:%s]" % (
                                fname, args_time, aqu_lock_time, calling_time, json_time) )
                        
                        return j
                    # Ok now really put the route in place
                    bottle.route('/'+fname, callback=f_wrapper, method=getattr(f, 'method', 'get').upper())
                    # and the name with - instead of _ if need
                    fname_dash = fname.replace('_', '-')
                    if fname_dash != fname:
                        bottle.route('/'+fname_dash, callback=f_wrapper, method=getattr(f, 'method', 'get').upper())
                register_callback(fname, args, f, obj, self.lock)

            # Add a simple / page
            def slash():
                return "OK"
            bottle.route('/', callback=slash)


        def unregister(self, obj):
            return


        def handleRequests(self, s):
            self.srv.handle_request()


        def create_uri(address, port, obj_name, use_ssl=False):
            return "PYRO:%s@%s:%d" % (obj_name, address, port)


        def set_timeout(con, timeout):
            con._pyroTimeout = timeout


        # Close all sockets and delete the server object to be sure
        # no one is still alive
        def shutdown(self):
            self.srv.stop()
            self.srv = None


        def get_socks_activity(self, timeout):
            try:
                ins, _, _ = select.select(self.get_sockets(), [], [], timeout)
            except select.error, e:
                errnum, _ = e
                if errnum == errno.EINTR:
                    return []
                raise
            return ins


daemon_inst = None

########NEW FILE########
__FILENAME__ = load
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import math

# From advanced load average's code (another of my projects :) )
#def calc_load_load(load, exp,n):
#        load = n + exp*(load - n)
#        return (load, exp)

class Load:
    """This class is for having a easy Load calculation
    without having to send value at regular interval
    (but it's more efficient if you do this :) ) and without
    having a list or other stuff. It's just an object, an update and a get
    You can define m: the average for m minutes. The val is
    the initial value. It's better if it's 0 but you can choose.

    """

    def __init__(self, m=1, initial_value=0):
        self.exp = 0  # first exp
        self.m = m  # Number of minute of the avg
        self.last_update = 0  # last update of the value
        self.val = initial_value  # first value

    # 
    def update_load(self, new_val, forced_interval=None):
        # The first call do not change the value, just tag
        # the beginning of last_update
        # IF  we force : bail out all time thing
        if not forced_interval and self.last_update == 0:
            self.last_update = time.time()
            return
        now = time.time()
        try:
            if forced_interval:
                diff = forced_interval
            else:
                diff = now - self.last_update
            self.exp = 1 / math.exp(diff / (self.m * 60.0))
            self.val = new_val + self.exp * (self.val - new_val)
            self.last_update = now
        except OverflowError:  # if the time change without notice, we overflow :(
            pass
        except ZeroDivisionError:  # do not care
            pass

    def get_load(self):
        return self.val


if __name__ == '__main__':
    l = Load()
    t = time.time()
    for i in xrange(1, 300):
        l.update_load(1)
        print '[', int(time.time() - t), ']', l.get_load(), l.exp
        time.sleep(5)

########NEW FILE########
__FILENAME__ = log
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import logging
import sys
from logging.handlers import TimedRotatingFileHandler

from brok import Brok

def is_tty():
    # Look if we are in a tty or not
    if hasattr(sys.stdout, 'isatty'):
        return sys.stdout.isatty()
    return False

if is_tty():
    # Try to load the terminal color. Won't work under python 2.4
    try:
        from shinken.misc.termcolor import cprint
    except (SyntaxError, ImportError), exp:
        # Outch can't import a cprint, do a simple print
        def cprint(s, color='', end=''):
            print s
# Ok it's a daemon mode, if so, just print
else:
    def cprint(s, color='', end=''):
        print s


obj = None
name = None
local_log = None
human_timestamp_log = False


class Log:
    """Shinken logger class, wrapping access to Python logging standard library."""
    "Store the numeric value from python logging class"
    NOTSET   = logging.NOTSET
    DEBUG    = logging.DEBUG
    INFO     = logging.INFO
    WARNING  = logging.WARNING
    ERROR    = logging.ERROR
    CRITICAL = logging.CRITICAL

    def __init__(self):
        self._level = logging.NOTSET
        self.display_time = True
        self.display_level = True
        self.log_colors = {Log.WARNING:'yellow', Log.CRITICAL:'magenta', Log.ERROR:'red'}


    def load_obj(self, object, name_=None):
        """ We load the object where we will put log broks
        with the 'add' method
        """
        global obj
        global name
        obj = object
        name = name_


    @staticmethod
    def get_level_id(lvlName):
        """Convert a level name (string) to its integer value
           and vice-versa. Input a level and it will return a name.
           Raise KeyError when name or level not found
        """
        return logging._levelNames[lvlName]


    # We can have level as an int (logging.INFO) or a string INFO
    # if string, try to get the int value
    def get_level(self):
        return logging.getLogger().getEffectiveLevel()


    # We can have level as an int (logging.INFO) or a string INFO
    # if string, try to get the int value
    def set_level(self, level):
        if not isinstance(level, int):
            level = getattr(logging, level, None)
            if not level or not isinstance(level, int):
                raise TypeError('log level must be an integer')

        self._level = level
        logging.getLogger().setLevel(level)


    def set_display_time(self, b):
        self.display_time = b

    def set_display_level(self, b):
        self.display_level = b

    def debug(self, msg, *args, **kwargs):
        self._log(logging.DEBUG, msg, *args, **kwargs)

    def info(self, msg, *args, **kwargs):
        self._log(logging.INFO, msg, *args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        self._log(logging.WARNING, msg, *args, **kwargs)

    def error(self, msg, *args, **kwargs):
        self._log(logging.ERROR, msg, *args, **kwargs)

    def critical(self, msg, *args, **kwargs):
        self._log(logging.CRITICAL, msg, *args, **kwargs)

    def log(self, message, format=None, print_it=True):
        """Old log method, kept for NAGIOS compatibility
        What strings should not use the new format ??"""
        self._log(logging.INFO, message, format, print_it, display_level=False)

    def _log(self, level, message, format=None, print_it=True, display_level=True):
        """We enter a log message, we format it, and we add the log brok"""
        global obj
        global name
        global local_log
        global human_timestamp_log

        # ignore messages when message level is lower than Log level
        if level < self._level:
            return

        # display the level only if we ask locally and globaly
        display_level = display_level & self.display_level

        # We format the log in UTF-8
        if isinstance(message, str):
            message = message.decode('UTF-8', 'replace')

        if format is None:
            lvlname = logging.getLevelName(level)

            if display_level:
                if self.display_time:
                    fmt = u'[%(date)s] %(level)-9s %(name)s%(msg)s\n'
                else:
                    fmt = u'%(level)-9s %(name)s%(msg)s\n'
            else:
                if self.display_time:
                    fmt = u'[%(date)s] %(name)s%(msg)s\n'
                else:
                    fmt = u'%(name)s%(msg)s\n'

            args = {
                'date': (human_timestamp_log and time.asctime()
                         or int(time.time())),
                'level': lvlname.capitalize()+' :',
                'name': name and ('[%s] ' % name) or '',
                'msg': message
            }
            s = fmt % args
        else:
            s = format % message

        if print_it and len(s) > 1:            
            # Take a color so we can print if it's a TTY
            if is_tty():
                color = self.log_colors.get(level, None)
            else:
                color = None
            
            # Print to standard output.
            # If the daemon is launched with a non UTF8 shell
            # we can have problems in printing, work around it.
            try:
                cprint(s[:-1], color)
            except UnicodeEncodeError:
                print s.encode('ascii', 'ignore')


        # We create the brok and load the log message
        # DEBUG level logs are logged by the daemon locally
        # and must not be forwarded to other satellites, or risk overloading them.
        if level != logging.DEBUG and obj and hasattr(obj, 'add'):
            b = Brok('log', {'log': s})
            obj.add(b)

        # If local logging is enabled, log to the defined handler, file.
        if local_log is not None:
            logging.log(level, s.strip())


    def register_local_log(self, path, level=None):
        """The shinken logging wrapper can write to a local file if needed
        and return the file descriptor so we can avoid to
        close it.
        """
        global local_log

        if level is not None:
            self._level = level

        # Open the log and set to rotate once a day
        basic_log_handler = TimedRotatingFileHandler(path,
                                                     'midnight',
                                                     backupCount=5)
        basic_log_handler.setLevel(self._level)
        basic_log_formatter = logging.Formatter('%(asctime)s %(message)s')
        basic_log_handler.setFormatter(basic_log_formatter)
        logger = logging.getLogger()
        logger.addHandler(basic_log_handler)
        logger.setLevel(self._level)
        local_log = basic_log_handler

        # Return the file descriptor of this file
        return basic_log_handler.stream.fileno()

    def quit(self):
        """Close the local log file at program exit"""
        global local_log
        if local_log:
            self.debug("Closing %s local_log" % str(local_log))
            local_log.close()

    def set_human_format(self, on=True):
        """
        Set the output as human format.

        If the optional parameter `on` is False, the timestamps format
        will be reset to the default format.
        """
        global human_timestamp_log
        human_timestamp_log = bool(on)

logger = Log()

class __ConsoleLogger:
    """
    This wrapper class for logging and printing messages to stdout, too.

    :fixme: Implement this using an additional stream-handler, as soon
    as the logging system is based on the standard Python logging
    module.
    """
    def debug(self, msg, *args, **kwargs):
        self._log(Log.DEBUG, msg, *args, **kwargs)

    def info(self, msg, *args, **kwargs):
        kwargs.setdefault('display_level', False)
        self._log(Log.INFO, msg, *args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        self._log(Log.WARNING, msg, *args, **kwargs)

    def error(self, msg, *args, **kwargs):
        self._log(Log.ERROR, msg, *args, **kwargs)

    def critical(self, msg, *args, **kwargs):
        self._log(Log.CRITICAL, msg, *args, **kwargs)

    def alert(self, msg, *args, **kwargs):
        kwargs.setdefault('display_level', False)
        self._log(Log.CRITICAL, msg, *args, **kwargs)

    def _log(self, *args, **kwargs):
        # if `print_it` is not passed as an argument, set it to `true`
        kwargs.setdefault('print_it', True)
        logger._log(*args, **kwargs)


console_logger = __ConsoleLogger()

########NEW FILE########
__FILENAME__ = macroresolver
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


# This class resolve Macro in commands by looking at the macros list
# in Class of elements. It give a property that call be callable or not.
# It not callable, it's a simple property and replace the macro with the value
# If callable, it's a method that is called to get the value. for example, to
# get the number of service in a host, you call a method to get the
# len(host.services)

import re
import time

from shinken.borg import Borg


class MacroResolver(Borg):
    """Please Add a Docstring to describe the class here"""

    my_type = 'macroresolver'
    # Global macros
    macros = {
        'TOTALHOSTSUP':         '_get_total_hosts_up',
        'TOTALHOSTSDOWN':       '_get_total_hosts_down',
        'TOTALHOSTSUNREACHABLE': '_get_total_hosts_unreachable',
        'TOTALHOSTSDOWNUNHANDLED': '_get_total_hosts_unhandled',
        'TOTALHOSTSUNREACHABLEUNHANDLED': '_get_total_hosts_unreachable_unhandled',
        'TOTALHOSTPROBLEMS':    '_get_total_host_problems',
        'TOTALHOSTPROBLEMSUNHANDLED': '_get_total_host_problems_unhandled',
        'TOTALSERVICESOK':      '_get_total_service_ok',
        'TOTALSERVICESWARNING': '_get_total_services_warning',
        'TOTALSERVICESCRITICAL': '_get_total_services_critical',
        'TOTALSERVICESUNKNOWN': '_get_total_services_unknown',
        'TOTALSERVICESWARNINGUNHANDLED': '_get_total_services_warning_unhandled',
        'TOTALSERVICESCRITICALUNHANDLED': '_get_total_services_critical_unhandled',
        'TOTALSERVICESUNKNOWNUNHANDLED': '_get_total_services_unknown_unhandled',
        'TOTALSERVICEPROBLEMS': '_get_total_service_problems',
        'TOTALSERVICEPROBLEMSUNHANDLED': '_get_total_service_problems_unhandled',
        'LONGDATETIME':         '_get_long_date_time',
        'SHORTDATETIME':        '_get_short_date_time',
        'DATE':                 '_get_date',
        'TIME':                 '_get_time',
        'TIMET':                '_get_timet',
        'PROCESSSTARTTIME':     '_get_process_start_time',
        'EVENTSTARTTIME':       '_get_events_start_time',
    }

    # This must be called ONCE. It just put links for elements
    # by scheduler
    def init(self, conf):
        # For searching class and elements for ondemand
        # we need link to types
        self.conf = conf
        self.lists_on_demand = []
        self.hosts = conf.hosts
        # For special void host_name handling...
        self.host_class = self.hosts.inner_class
        self.lists_on_demand.append(self.hosts)
        self.services = conf.services
        self.contacts = conf.contacts
        self.lists_on_demand.append(self.contacts)
        self.hostgroups = conf.hostgroups
        self.lists_on_demand.append(self.hostgroups)
        self.commands = conf.commands
        self.servicegroups = conf.servicegroups
        self.lists_on_demand.append(self.servicegroups)
        self.contactgroups = conf.contactgroups
        self.lists_on_demand.append(self.contactgroups)
        self.illegal_macro_output_chars = conf.illegal_macro_output_chars
        self.output_macros = ['HOSTOUTPUT', 'HOSTPERFDATA', 'HOSTACKAUTHOR', 'HOSTACKCOMMENT', 'SERVICEOUTPUT', 'SERVICEPERFDATA', 'SERVICEACKAUTHOR', 'SERVICEACKCOMMENT']

        # Try cache :)
        #self.cache = {}


    # Return all macros of a string, so cut the $
    # And create a dict with it:
    # val: value, not set here
    # type: type of macro, like class one, or ARGN one
    def _get_macros(self, s):
        #if s in self.cache:
        #    return self.cache[s]

        p = re.compile(r'(\$)')
        elts = p.split(s)
        macros = {}
        in_macro = False
        for elt in elts:
            if elt == '$':
                in_macro = not in_macro
            elif in_macro:
                macros[elt] = {'val': '', 'type': 'unknown'}

        #self.cache[s] = macros
        if '' in macros:
            del macros['']
        return macros

    # Get a value from a property of a element
    # Prop can be a function or a property
    # So we call it or not
    def _get_value_from_element(self, elt, prop):
        try:
            value = getattr(elt, prop)
            if callable(value):
                return unicode(value())
            else:
                return unicode(value)
        except AttributeError, exp:
            # Return no value
            return ''
        except UnicodeError, exp:
            if isinstance(value, str):
                return unicode(value, 'utf8', errors='ignore')
            else:
                return ''


    # For some macros, we need to delete unwanted characters
    def _delete_unwanted_caracters(self, s):
        for c in self.illegal_macro_output_chars:
            s = s.replace(c, '')
        return s

    # return a dict with all environment variable came from
    # the macros of the datas object
    def get_env_macros(self, data):
        env = {}

        for o in data:
            cls = o.__class__
            macros = cls.macros
            for macro in macros:
                if macro.startswith("USER"):
                    break

                #print "Macro in %s: %s" % (o.__class__, macro)
                prop = macros[macro]
                value = self._get_value_from_element(o, prop)
                env['NAGIOS_%s' % macro] = value
            if hasattr(o, 'customs'):
                # make NAGIOS__HOSTMACADDR from _MACADDR
                for cmacro in o.customs:
                    env['NAGIOS__' + o.__class__.__name__.upper() + cmacro[1:].upper()] = o.customs[cmacro]

        return env

    # This function will look at elements in data (and args if it filled)
    # to replace the macros in c_line with real value.
    def resolve_simple_macros_in_string(self, c_line, data, args=None):
        # Now we prepare the classes for looking at the class.macros
        data.append(self)  # For getting global MACROS
        if hasattr(self, 'conf'):
            data.append(self.conf)  # For USERN macros
        clss = [d.__class__ for d in data]

        # we should do some loops for nested macros
        # like $USER1$ hiding like a ninja in a $ARG2$ Macro. And if
        # $USER1$ is pointing to $USER34$ etc etc, we should loop
        # until we reach the bottom. So the last loop is when we do
        # not still have macros :)
        still_got_macros = True
        nb_loop = 0
        while still_got_macros:
            nb_loop += 1
            # Ok, we want the macros in the command line
            macros = self._get_macros(c_line)

            # We can get out if we do not have macros this loop
            still_got_macros = (len(macros) != 0)
            #print "Still go macros:", still_got_macros

            # Put in the macros the type of macro for all macros
            self._get_type_of_macro(macros, clss)
            # Now we get values from elements
            for macro in macros:
                # If type ARGN, look at ARGN cutting
                if macros[macro]['type'] == 'ARGN' and args is not None:
                    macros[macro]['val'] = self._resolve_argn(macro, args)
                    macros[macro]['type'] = 'resolved'
                # If class, get value from properties
                if macros[macro]['type'] == 'class':
                    cls = macros[macro]['class']
                    for elt in data:
                        if elt is not None and elt.__class__ == cls:
                            prop = cls.macros[macro]
                            macros[macro]['val'] = self._get_value_from_element(elt, prop)
                            # Now check if we do not have a 'output' macro. If so, we must
                            # delete all special characters that can be dangerous
                            if macro in self.output_macros:
                                macros[macro]['val'] = self._delete_unwanted_caracters(macros[macro]['val'])
                if macros[macro]['type'] == 'CUSTOM':
                    cls_type = macros[macro]['class']
                    # Beware : only cut the first _HOST value, so the macro name can have it on it...
                    macro_name = re.split('_' + cls_type, macro, 1)[1].upper()
                    # Ok, we've got the macro like MAC_ADDRESS for _HOSTMAC_ADDRESS
                    # Now we get the element in data that have the type HOST
                    # and we check if it got the custom value
                    for elt in data:
                        if elt is not None and elt.__class__.my_type.upper() == cls_type:
                            if '_' + macro_name in elt.customs:
                                macros[macro]['val'] = elt.customs['_' + macro_name]
                            # Then look on the macromodulations, in reserver order, so
                            # the last to set, will be the firt to have. (yes, don't want to play
                            # with break and such things sorry...)
                            mms = getattr(elt, 'macromodulations', [])
                            for mm in mms[::-1]:
                                # Look if the modulation got the value, but also if it's currently active
                                if '_' + macro_name in mm.customs and mm.is_active():
                                    macros[macro]['val'] = mm.customs['_' + macro_name]
                if macros[macro]['type'] == 'ONDEMAND':
                    macros[macro]['val'] = self._resolve_ondemand(macro, data)

            # We resolved all we can, now replace the macro in the command call
            for macro in macros:
                c_line = c_line.replace('$'+macro+'$', macros[macro]['val'])

            # A $$ means we want a $, it's not a macro!
            # We replace $$ by a big dirty thing to be sure to not misinterpret it
            c_line = c_line.replace("$$", "DOUBLEDOLLAR")

            if nb_loop > 32:  # too much loop, we exit
                still_got_macros = False

        # We now replace the big dirty token we made by only a simple $
        c_line = c_line.replace("DOUBLEDOLLAR", "$")

        #print "Retuning c_line", c_line.strip()
        return c_line.strip()

    # Resolve a command with macro by looking at data classes.macros
    # And get macro from item properties.
    def resolve_command(self, com, data):
        c_line = com.command.command_line
        return self.resolve_simple_macros_in_string(c_line, data, args=com.args)

    # For all Macros in macros, set the type by looking at the
    # MACRO name (ARGN? -> argn_type,
    # HOSTBLABLA -> class one and set Host in class)
    # _HOSTTOTO -> HOST CUSTOM MACRO TOTO
    # $SERVICESTATEID:srv-1:Load$ -> MACRO SERVICESTATEID of
    # the service Load of host srv-1
    def _get_type_of_macro(self, macros, clss):
        for macro in macros:
            # ARGN Macros
            if re.match('ARG\d', macro):
                macros[macro]['type'] = 'ARGN'
                continue
            # USERN macros
            # are managed in the Config class, so no
            # need to look that here
            elif re.match('_HOST\w', macro):
                macros[macro]['type'] = 'CUSTOM'
                macros[macro]['class'] = 'HOST'
                continue
            elif re.match('_SERVICE\w', macro):
                macros[macro]['type'] = 'CUSTOM'
                macros[macro]['class'] = 'SERVICE'
                # value of macro: re.split('_HOST', '_HOSTMAC_ADDRESS')[1]
                continue
            elif re.match('_CONTACT\w', macro):
                macros[macro]['type'] = 'CUSTOM'
                macros[macro]['class'] = 'CONTACT'
                continue
            # On demand macro
            elif len(macro.split(':')) > 1:
                macros[macro]['type'] = 'ONDEMAND'
                continue
            # OK, classical macro...
            for cls in clss:
                if macro in cls.macros:
                    macros[macro]['type'] = 'class'
                    macros[macro]['class'] = cls
                    continue

    # Resolve MACROS for the ARGN
    def _resolve_argn(self, macro, args):
        # first, get the number of args
        id = None
        r = re.search('ARG(?P<id>\d+)', macro)
        if r is not None:
            id = int(r.group('id')) - 1
            try:
                return args[id]
            except IndexError:
                return ''

    # Resolve on-demand macro, quite hard in fact
    def _resolve_ondemand(self, macro, data):
        #print "\nResolving macro", macro
        elts = macro.split(':')
        nb_parts = len(elts)
        macro_name = elts[0]
        # Len 3 == service, 2 = all others types...
        if nb_parts == 3:
            val = ''
            #print "Got a Service on demand asking...", elts
            (host_name, service_description) = (elts[1], elts[2])
            # host_name can be void, so it's the host in data
            # that is important. We use our self.host_class to
            # find the host in the data :)
            if host_name == '':
                for elt in data:
                    if elt is not None and elt.__class__ == self.host_class:
                        host_name = elt.host_name
            # Ok now we get service
            s = self.services.find_srv_by_name_and_hostname(host_name, service_description)
            if s is not None:
                cls = s.__class__
                prop = cls.macros[macro_name]
                val = self._get_value_from_element(s, prop)
                #print "Got val:", val
                return val
        # Ok, service was easy, now hard part
        else:
            val = ''
            elt_name = elts[1]
            # Special case: elt_name can be void
            # so it's the host where it apply
            if elt_name == '':
                for elt in data:
                    if elt is not None and elt.__class__ == self.host_class:
                        elt_name = elt.host_name
            for list in self.lists_on_demand:
                cls = list.inner_class
                # We search our type by looking at the macro
                if macro_name in cls.macros:
                    prop = cls.macros[macro_name]
                    i = list.find_by_name(elt_name)
                    if i is not None:
                        val = self._get_value_from_element(i, prop)
                        # Ok we got our value :)
                        break
            return val
        return ''


    # Get Fri 15 May 11:42:39 CEST 2009
    def _get_long_date_time(self):
        return time.strftime("%a %d %b %H:%M:%S %Z %Y").decode('UTF-8', 'ignore')


    # Get 10-13-2000 00:30:28
    def _get_short_date_time(self):
        return time.strftime("%d-%m-%Y %H:%M:%S")


    # Get 10-13-2000
    def _get_date(self):
        return time.strftime("%d-%m-%Y")


    # Get 00:30:28
    def _get_time(self):
        return time.strftime("%H:%M:%S")


    # Get epoch time
    def _get_timet(self):
        return str(int(time.time()))

    def _get_total_hosts_up(self):
        return len([h for h in self.hosts if h.state == 'UP'])

    def _get_total_hosts_down(self):
        return len([h for h in self.hosts if h.state == 'DOWN'])

    def _get_total_hosts_unreachable(self):
        return len([h for h in self.hosts if h.state == 'UNREACHABLE'])

    # TODO
    def _get_total_hosts_unreachable_unhandled(self):
        return 0

    def _get_total_hosts_problems(self):
        return len([h for h in self.hosts if h.is_problem])

    def _get_total_hosts_problems_unhandled(self):
        return 0

    def _get_total_service_ok(self):
        return len([s for s in self.services if s.state == 'OK'])

    def _get_total_services_warning(self):
        return len([s for s in self.services if s.state == 'WARNING'])

    def _get_total_services_critical(self):
        return len([s for s in self.services if s.state == 'CRITICAL'])

    def _get_total_services_unknown(self):
        return len([s for s in self.services if s.state == 'UNKNOWN'])

    # TODO
    def _get_total_services_warning_unhandled(self):
        return 0

    def _get_total_services_critical_unhandled(self):
        return 0

    def _get_total_services_unknown_unhandled(self):
        return 0

    def _get_total_service_problems(self):
        return len([s for s in self.services if s.is_problem])

    def _get_total_service_problems_unhandled(self):
        return 0

    def _get_process_start_time(self):
        return 0

    def _get_events_start_time(self):
        return 0

########NEW FILE########
__FILENAME__ = memoized
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class memoized(object):
    """Decorator that caches a function's return value each time it is called.
    If called later with the same arguments, the cached value is returned, and
    not re-evaluated.

    """

    def __init__(self, func):
        self.func = func
        self.cache = {}

    def __call__(self, *args):
        try:
            return self.cache[args]
        except KeyError:
            self.cache[args] = value = self.func(*args)
            return value
        except TypeError:
            # uncatchable -- for instance, passing a list as an argument.
            # Better to not catch it than to blow up entirely.
            return self.func(*args)

    # Return the function's docstring.
    def __repr__(self):
        return self.func.__doc__

########NEW FILE########
__FILENAME__ = message
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class Message:
    """This is a simple message class for communications between actionners and
    workers

    """

    my_type = 'message'
    _type = None
    _data = None
    _from = None

    def __init__(self, id, type, data=None, source=None):
        self._type = type
        self._data = data
        self._from = id
        self.source = source

    def get_type(self):
        return self._type

    def get_data(self):
        return self._data

    def get_from(self):
        return self._from

    def str(self):
        return "Message from %d (%s), Type: %s Data: %s" % (self._from, self.source, self._type, self._data)

########NEW FILE########
__FILENAME__ = datamanager
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.util import safe_print
from shinken.misc.sorter import hst_srv_sort, last_state_change_earlier
from shinken.misc.filter  import only_related_to

class DataManager(object):
    def __init__(self):
        self.rg = None

    def load(self, rg):
        self.rg = rg

    # UI will launch us names in str, we got unicode
    # in our rg, so we must manage it here
    def get_host(self, hname):
        hname = hname.decode('utf8', 'ignore')
        return self.rg.hosts.find_by_name(hname)

    def get_service(self, hname, sdesc):
        hname = hname.decode('utf8', 'ignore')
        sdesc = sdesc.decode('utf8', 'ignore')
        return self.rg.services.find_srv_by_name_and_hostname(hname, sdesc)

    def get_all_hosts_and_services(self):
        all = []
        all.extend(self.rg.hosts)
        all.extend(self.rg.services)
        return all

    def get_contact(self, name):
        name = name.decode('utf8', 'ignore')
        return self.rg.contacts.find_by_name(name)

    def get_contacts(self):
        return self.rg.contacts

    def get_hostgroups(self):
        return self.rg.hostgroups

    def get_hostgroup(self, name):
        return self.rg.hostgroups.find_by_name(name)

    def get_servicegroups(self):
        return self.rg.servicegroups

    def get_servicegroup(self, name):
        return self.rg.servicegroups.find_by_name(name)

    # Get the hostgroups sorted by names, and zero size in the end
    # if selected one, put it in the first place
    def get_hostgroups_sorted(self, selected=''):
        r = []
        selected = selected.strip()

        hg_names = [hg.get_name() for hg in self.rg.hostgroups if len(hg.members) > 0 and hg.get_name() != selected]
        hg_names.sort()
        hgs = [self.rg.hostgroups.find_by_name(n) for n in hg_names]
        hgvoid_names = [hg.get_name() for hg in self.rg.hostgroups if len(hg.members) == 0 and hg.get_name() != selected]
        hgvoid_names.sort()
        hgvoids = [self.rg.hostgroups.find_by_name(n) for n in hgvoid_names]

        if selected:
            hg = self.rg.hostgroups.find_by_name(selected)
            if hg:
                r.append(hg)

        r.extend(hgs)
        r.extend(hgvoids)

        return r

    def get_hosts(self):
        return self.rg.hosts

    def get_services(self):
        return self.rg.services

    def get_schedulers(self):
        return self.rg.schedulers

    def get_pollers(self):
        return self.rg.pollers

    def get_brokers(self):
        return self.rg.brokers

    def get_receivers(self):
        return self.rg.receivers

    def get_reactionners(self):
        return self.rg.reactionners

    def get_program_start(self):
        for c in self.rg.configs.values():
            return c.program_start
        return None

    def get_realms(self):
        return self.rg.realms

    def get_realm(self, r):
        if r in self.rg.realms:
            return r
        return None

    # Get the hosts tags sorted by names, and zero size in the end
    def get_host_tags_sorted(self):
        r = []
        names = self.rg.tags.keys()
        names.sort()
        for n in names:
            r.append((n, self.rg.tags[n]))
        return r

    # Get the hosts tagged with a specific tag
    def get_hosts_tagged_with(self, tag):
        r = []
        for h in self.get_hosts():
            if tag in h.get_host_tags():
                r.append(h)
        return r

    # Get the services tags sorted by names, and zero size in the end
    def get_service_tags_sorted(self):
        r = []
        names = self.rg.services_tags.keys()
        names.sort()
        for n in names:
            r.append((n, self.rg.services_tags[n]))
        return r

    def get_important_impacts(self):
        res = []
        for s in self.rg.services:
            if s.is_impact and s.state not in ['OK', 'PENDING']:
                if s.business_impact > 2:
                    res.append(s)
        for h in self.rg.hosts:
            if h.is_impact and h.state not in ['UP', 'PENDING']:
                if h.business_impact > 2:
                    res.append(h)
        return res

    # Returns all problems
    def get_all_problems(self, to_sort=True, get_acknowledged=False):
        res = []
        if not get_acknowledged:
            res.extend([s for s in self.rg.services if s.state not in ['OK', 'PENDING'] and not s.is_impact and not s.problem_has_been_acknowledged and not s.host.problem_has_been_acknowledged])
            res.extend([h for h in self.rg.hosts if h.state not in ['UP', 'PENDING'] and not h.is_impact and not h.problem_has_been_acknowledged])
        else:
            res.extend([s for s in self.rg.services if s.state not in ['OK', 'PENDING'] and not s.is_impact])
            res.extend([h for h in self.rg.hosts if h.state not in ['UP', 'PENDING'] and not h.is_impact])

        if to_sort:
            res.sort(hst_srv_sort)
        return res

    # returns problems, but the most recent before
    def get_problems_time_sorted(self):
        pbs = self.get_all_problems(to_sort=False)
        pbs.sort(last_state_change_earlier)
        return pbs

    # Return all non managed impacts
    def get_all_impacts(self):
        res = []
        for s in self.rg.services:
            if s.is_impact and s.state not in ['OK', 'PENDING']:
                # If s is acked, pass
                if s.problem_has_been_acknowledged:
                    continue
                # We search for impacts that were NOT currently managed
                if len([p for p in s.source_problems if not p.problem_has_been_acknowledged]) > 0:
                    res.append(s)
        for h in self.rg.hosts:
            if h.is_impact and h.state not in ['UP', 'PENDING']:
                # If h is acked, pass
                if h.problem_has_been_acknowledged:
                    continue
                # We search for impacts that were NOT currently managed
                if len([p for p in h.source_problems if not p.problem_has_been_acknowledged]) > 0:
                    res.append(h)
        return res

    # Return the number of problems
    def get_nb_problems(self):
        return len(self.get_all_problems(to_sort=False))

    # Get the number of all problems, even the ack ones
    def get_nb_all_problems(self,user):
        res = []
        res.extend([s for s in self.rg.services if s.state not in ['OK', 'PENDING'] and not s.is_impact])
        res.extend([h for h in self.rg.hosts if h.state not in ['UP', 'PENDING'] and not h.is_impact])
        return len(only_related_to(res,user))

    # Return the number of impacts
    def get_nb_impacts(self):
        return len(self.get_all_impacts())

    def get_nb_elements(self):
        return len(self.rg.services) + len(self.rg.hosts)

    def get_important_elements(self):
        res = []
        # We want REALLY important things, so business_impact > 2, but not just IT elements that are
        # root problems, so we look only for config defined my_own_business_impact value too
        res.extend([s for s in self.rg.services if (s.business_impact > 2 and not 0 <= s.my_own_business_impact <= 2)])
        res.extend([h for h in self.rg.hosts if (h.business_impact > 2 and not 0 <= h.my_own_business_impact <= 2)])
        print "DUMP IMPORTANT"
        for i in res:
            safe_print(i.get_full_name(), i.business_impact, i.my_own_business_impact)
        return res

    # For all business impacting elements, and give the worse state
    # if warning or critical
    def get_overall_state(self):
        h_states = [h.state_id for h in self.rg.hosts if h.business_impact > 2 and h.is_impact and h.state_id in [1, 2]]
        s_states = [s.state_id for s in self.rg.services if s.business_impact > 2 and s.is_impact and s.state_id in [1, 2]]
        print "get_overall_state:: hosts and services business problems", h_states, s_states
        if len(h_states) == 0:
            h_state = 0
        else:
            h_state = max(h_states)
        if len(s_states) == 0:
            s_state = 0
        else:
            s_state = max(s_states)
        # Ok, now return the max of hosts and services states
        return max(h_state, s_state)

    # Same but for pure IT problems
    def get_overall_it_state(self):
        h_states = [h.state_id for h in self.rg.hosts if h.is_problem and h.state_id in [1, 2]]
        s_states = [s.state_id for s in self.rg.services if s.is_problem and s.state_id in [1, 2]]
        if len(h_states) == 0:
            h_state = 0
        else:
            h_state = max(h_states)
        if len(s_states) == 0:
            s_state = 0
        else:
            s_state = max(s_states)
        # Ok, now return the max of hosts and services states
        return max(h_state, s_state)

    # Get percent of all Services
    def get_per_service_state(self):
        all_services = self.rg.services
        problem_services = []
        problem_services.extend([s for s in self.rg.services if s.state not in ['OK', 'PENDING'] and not s.is_impact])
        if len(all_services) == 0:
            res = 0
        else:
            res = (100-(len(problem_services) *100)/len(all_services))
        return res
              
    # Get percent of all Hosts
    def get_per_hosts_state(self):
        all_hosts = self.rg.hosts
        problem_hosts = []
        problem_hosts.extend([s for s in self.rg.hosts if s.state not in ['UP', 'PENDING'] and not s.is_impact])
        if len(all_hosts) == 0:
            res = 0
        else:
            res = (100-(len(problem_hosts) *100)/len(all_hosts))
        return res
              

    # For all business impacting elements, and give the worse state
    # if warning or critical
    def get_len_overall_state(self):
        h_states = [h.state_id for h in self.rg.hosts if h.business_impact > 2 and h.is_impact and h.state_id in [1, 2]]
        s_states = [s.state_id for s in self.rg.services if  s.business_impact > 2 and s.is_impact and s.state_id in [1, 2]]
        print "get_len_overall_state:: hosts and services business problems", h_states, s_states
        # Just return the number of impacting elements
        return len(h_states) + len(s_states)

    # Return a tree of {'elt': Host, 'fathers': [{}, {}]}
    def get_business_parents(self, obj, levels=3):
        res = {'node': obj, 'fathers': []}
        ## if levels == 0:
        ##     return res

        for i in obj.parent_dependencies:
            # We want to get the levels deep for all elements, but
            # go as far as we should for bad elements
            if levels != 0 or i.state_id != 0:
                par_elts = self.get_business_parents(i, levels=levels - 1)
                res['fathers'].append(par_elts)

        print "get_business_parents::Give elements", res
        return res

    # Ok, we do not have true root problems, but we can try to guess isn't it?
    # We can just guess for services with the same services of this host in fact
    def guess_root_problems(self, obj):
        if obj.__class__.my_type != 'service':
            return []
        r = [s for s in obj.host.services if s.state_id != 0 and s != obj]
        return r

datamgr = DataManager()

########NEW FILE########
__FILENAME__ = filter
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
Helper functions for some filtering, like for user based
"""


# Get only user relevant items for the user
def only_related_to(lst, user):
    # if the user is an admin, show all
    if user.is_admin:
        return lst

    # Ok the user is a simple user, we should filter
    r = set()
    for i in lst:
        # Maybe the user is a direct contact
        if user in i.contacts:
            r.add(i)
            continue
        # TODO: add a notified_contact pass

        # Maybe it's a contact of a linked elements (source problems or impacts)
        is_find = False
        for s in i.source_problems:
            if user in s.contacts:
                r.add(i)
                is_find = True
        # Ok skip this object now
        if is_find:
            continue
        # Now impacts related maybe?
        for imp in i.impacts:
            if user in imp.contacts:
                r.add(i)

    return list(r)

########NEW FILE########
__FILENAME__ = md5crypt
#########################################################
# md5crypt.py
#
# 0423.2000 by michal wallace http://www.sabren.com/
# based on perl's Crypt::PasswdMD5 by Luis Munoz (lem@cantv.net)
# based on /usr/src/libcrypt/crypt.c from FreeBSD 2.2.5-RELEASE
#
# MANY THANKS TO
#
#  Carey Evans - http://home.clear.net.nz/pages/c.evans/
#  Dennis Marti - http://users.starpower.net/marti1/
#
#  For the patches that got this thing working!
#
#########################################################
"""md5crypt.py - Provides interoperable MD5-based crypt() function

SYNOPSIS

import md5crypt.py

cryptedpassword = md5crypt.md5crypt(password, salt);

DESCRIPTION

unix_md5_crypt() provides a crypt()-compatible interface to the
rather new MD5-based crypt() function found in modern operating systems.
It's based on the implementation found on FreeBSD 2.2.[56]-RELEASE and
contains the following license in it:

 "THE BEER-WARE LICENSE" (Revision 42):
 <phk@login.dknet.dk> wrote this file.  As long as you retain this notice you
 can do whatever you want with this stuff. If we meet some day, and you think
 this stuff is worth it, you can buy me a beer in return.   Poul-Henning Kamp

apache_md5_crypt() provides a function compatible with Apache's
.htpasswd files. This was contributed by Bryan Hart <bryan@eai.com>.

"""

MAGIC = '$1$'  # Magic string
ITOA64 = "./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"

from hashlib import md5


def to64 (v, n):
    ret = ''
    while (n - 1 >= 0):
        n = n - 1
        ret = ret + ITOA64[v & 0x3f]
        v = v >> 6
    return ret


def apache_md5_crypt (pw, salt):
    # change the Magic string to match the one used by Apache
    return unix_md5_crypt(pw, salt, '$apr1$')


def unix_md5_crypt(pw, salt, magic=None):

    if magic == None:
        magic = MAGIC

    # Take care of the magic string if present
    if salt[:len(magic)] == magic:
        salt = salt[len(magic):]

    # salt can have up to 8 characters:
    import string
    salt = string.split(salt, '$', 1)[0]
    salt = salt[:8]

    ctx = pw + magic + salt

    final = md5(pw + salt + pw).digest()

    for pl in range(len(pw), 0, -16):
        if pl > 16:
            ctx = ctx + final[:16]
        else:
            ctx = ctx + final[:pl]
    # Now the 'weird' xform (??)

    i = len(pw)
    while i:
        if i & 1:
            ctx = ctx + chr(0)  # if ($i & 1) { $ctx->add(pack("C", 0)); }
        else:
            ctx = ctx + pw[0]
        i = i >> 1

    final = md5(ctx).digest()

    # The following is supposed to make
    # things run slower.

    # my question: WTF???

    for i in range(1000):
        ctx1 = ''
        if i & 1:
            ctx1 = ctx1 + pw
        else:
            ctx1 = ctx1 + final[:16]

        if i % 3:
            ctx1 = ctx1 + salt

        if i % 7:
            ctx1 = ctx1 + pw

        if i & 1:
            ctx1 = ctx1 + final[:16]
        else:
            ctx1 = ctx1 + pw

        final = md5(ctx1).digest()
    # Final xform

    passwd = ''

    passwd = passwd + to64((int(ord(final[0])) << 16)
                           |(int(ord(final[6])) << 8)
                           |(int(ord(final[12]))), 4)

    passwd = passwd + to64((int(ord(final[1])) << 16)
                           |(int(ord(final[7])) << 8)
                           |(int(ord(final[13]))), 4)

    passwd = passwd + to64((int(ord(final[2])) << 16)
                           |(int(ord(final[8])) << 8)
                           |(int(ord(final[14]))), 4)

    passwd = passwd + to64((int(ord(final[3])) << 16)
                           |(int(ord(final[9])) << 8)
                           |(int(ord(final[15]))), 4)

    passwd = passwd + to64((int(ord(final[4])) << 16)
                           |(int(ord(final[10])) << 8)
                           |(int(ord(final[5]))), 4)

    passwd = passwd + to64((int(ord(final[11]))), 2)

    return magic + salt + '$' + passwd

## assign a wrapper function:
md5crypt = unix_md5_crypt

if __name__ == "__main__":
    print unix_md5_crypt("cat", "hat")

########NEW FILE########
__FILENAME__ = perfdata
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re
from shinken.util import to_best_int_float

perfdata_split_pattern = re.compile('([^=]+=\S+)')
# TODO: Improve this regex to not match strings like this:
# 'metric=45+e-456.56unit;50;80;0;45+-e45e-'
metric_pattern = re.compile('^([^=]+)=([\d\.\-\+eE]+)([\w\/%]*);?([\d\.\-\+eE:~@]+)?;?([\d\.\-\+eE:~@]+)?;?([\d\.\-\+eE]+)?;?([\d\.\-\+eE]+)?;?\s*')


# If we can return an int or a float, or None
# if we can't
def guess_int_or_float(val):
    try:
        return to_best_int_float(val)
    except Exception, exp:
        return None


# Class for one metric of a perf_data
class Metric:
    def __init__(self, s):
        self.name = self.value = self.uom = self.warning = self.critical = self.min = self.max = None
        s = s.strip()
        #print "Analysis string", s
        r = metric_pattern.match(s)
        if r:
            # Get the name but remove all ' in it
            self.name = r.group(1).replace("'", "")
            self.value = guess_int_or_float(r.group(2))
            self.uom = r.group(3)
            self.warning = guess_int_or_float(r.group(4))
            self.critical = guess_int_or_float(r.group(5))
            self.min = guess_int_or_float(r.group(6))
            self.max = guess_int_or_float(r.group(7))
            #print 'Name', self.name
            #print "Value", self.value
            #print "Res", r
            #print r.groups()
            if self.uom == '%':
                self.min = 0
                self.max = 100

    def __str__(self):
        s = "%s=%s%s" % (self.name, self.value, self.uom)
        if self.warning:
            s = s + ";%s" % (self.warning)
        if self.critical:
            s = s + ";%s" % (self.critical)
        return s


class PerfDatas:
    def __init__(self, s):
        elts = perfdata_split_pattern.findall(s)
        elts = [e for e in elts if e != '']
        self.metrics = {}
        for e in elts:
            m = Metric(e)
            if m.name is not None:
                self.metrics[m.name] = m

    def __iter__(self):
        return self.metrics.itervalues()

    def __len__(self):
        return len(self.metrics)

    def __getitem__(self, key):
        return self.metrics[key]

    def __contains__(self, key):
        return key in self.metrics


########NEW FILE########
__FILENAME__ = regenerator
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

# Import all objects we will need
from shinken.objects.host import Host, Hosts
from shinken.objects.hostgroup import Hostgroup, Hostgroups
from shinken.objects.service import Service, Services
from shinken.objects.servicegroup import Servicegroup, Servicegroups
from shinken.objects.contact import Contact, Contacts
from shinken.objects.contactgroup import Contactgroup, Contactgroups
from shinken.objects.notificationway import NotificationWay, NotificationWays
from shinken.objects.timeperiod import Timeperiod, Timeperiods
from shinken.objects.command import Command, Commands
from shinken.objects.config import Config
from shinken.schedulerlink import SchedulerLink, SchedulerLinks
from shinken.reactionnerlink import ReactionnerLink, ReactionnerLinks
from shinken.pollerlink import PollerLink, PollerLinks
from shinken.brokerlink import BrokerLink, BrokerLinks
from shinken.receiverlink import ReceiverLink, ReceiverLinks
from shinken.util import safe_print
from shinken.message import Message


# Class for a Regenerator. It will get broks, and "regenerate" real objects
# from them :)
class Regenerator(object):
    def __init__(self):

        # Our Real datas
        self.configs = {}
        self.hosts = Hosts([])
        self.services = Services([])
        self.notificationways = NotificationWays([])
        self.contacts = Contacts([])
        self.hostgroups = Hostgroups([])
        self.servicegroups = Servicegroups([])
        self.contactgroups = Contactgroups([])
        self.timeperiods = Timeperiods([])
        self.commands = Commands([])
        self.schedulers = SchedulerLinks([])
        self.pollers = PollerLinks([])
        self.reactionners = ReactionnerLinks([])
        self.brokers = BrokerLinks([])
        self.receivers = ReceiverLinks([])
        # From now we only look for realms names
        self.realms = set()
        self.tags = {}
        self.services_tags = {}

        # And in progress one
        self.inp_hosts = {}
        self.inp_services = {}
        self.inp_hostgroups = {}
        self.inp_servicegroups = {}
        self.inp_contactgroups = {}

        # Do not ask for full data resent too much
        self.last_need_data_send = time.time()

        # Flag to say if our data came from the scheduler or not
        # (so if we skip *initial* broks)
        self.in_scheduler_mode = False

        # The Queue where to launch message, will be fill from the broker
        self.from_q = None


    # Load an external queue for sending messages
    def load_external_queue(self, from_q):
        self.from_q = from_q


    # If we are called from a scheduler it self, we load the data from it
    def load_from_scheduler(self, sched):
        # Ok, we are in a scheduler, so we will skip some useless
        # steps
        self.in_scheduler_mode = True

        # Go with the data creation/load
        c = sched.conf
        # Simulate a drop conf
        b = sched.get_program_status_brok()
        b.prepare()
        self.manage_program_status_brok(b)

        # Now we will lie and directly map our objects :)
        print "Regenerator::load_from_scheduler"
        self.hosts = c.hosts
        self.services = c.services
        self.notificationways = c.notificationways
        self.contacts = c.contacts
        self.hostgroups = c.hostgroups
        self.servicegroups = c.servicegroups
        self.contactgroups = c.contactgroups
        self.timeperiods = c.timeperiods
        self.commands = c.commands
        # We also load the realm
        for h in self.hosts:
            self.realms.add(h.realm)
            break



    # If we are in a scheduler mode, some broks are dangerous, so
    # we will skip them
    def want_brok(self, brok):
        if self.in_scheduler_mode:
            return not brok.type in ['program_status', 'initial_host_status',
                             'initial_hostgroup_status', 'initial_service_status',
                             'initial_servicegroup_status', 'initial_contact_status',
                             'initial_contactgroup_status', 'initial_timeperiod_status',
                             'initial_command_status']
        # Ok you are wondering why we don't add initial_broks_done? It's because the LiveSTatus modules
        # need this part to do internal things. But don't worry, the vanilla regenerator
        # will just skip it in all_done_linking :D

        # Not in don't want? so want! :)
        return True


    def manage_brok(self, brok):
        """ Look for a manager function for a brok, and call it """
        manage = getattr(self, 'manage_' + brok.type + '_brok', None)
        # If we can and want it, got for it :)
        if manage and self.want_brok(brok):
            return manage(brok)


    def update_element(self, e, data):
        for prop in data:
            setattr(e, prop, data[prop])


    def create_reversed_list(self):
        self.hosts.create_reversed_list()
        self.hostgroups.create_reversed_list()
        self.contacts.create_reversed_list()
        self.contactgroups.create_reversed_list()
        self.notificationways.create_reversed_list()
        self.services.create_reversed_list()
        self.servicegroups.create_reversed_list()
        self.timeperiods.create_reversed_list()
        #self.modules.create_reversed_list()
        #self.resultmodulations.create_reversed_list()
        #self.criticitymodulations.create_reversed_list()
        #self.escalations.create_reversed_list()
        #self.discoveryrules.create_reversed_list()
        #self.discoveryruns.create_reversed_list()
        self.commands.create_reversed_list()


    # Now we get all data about an instance, link all this stuff :)
    def all_done_linking(self, inst_id):

        # In a scheduler we are already "linked" so we can skip this
        if self.in_scheduler_mode:
            safe_print("Regenerator: We skip the all_done_linking phase because we are in a scheduler")
            return

        start = time.time()
        safe_print("In ALL Done linking phase for instance", inst_id)
        # check if the instance is really defined, so got ALL the
        # init phase
        if not inst_id in self.configs.keys():
            safe_print("Warning: the instance %d is not fully given, bailout" % inst_id)
            return

        # Try to load the in progress list and make them available for
        # finding
        try:
            inp_hosts = self.inp_hosts[inst_id]
            inp_hosts.create_reversed_list()
            inp_hostgroups = self.inp_hostgroups[inst_id]
            inp_contactgroups = self.inp_contactgroups[inst_id]
            inp_services = self.inp_services[inst_id]
            inp_services.create_reversed_list()
            inp_servicegroups = self.inp_servicegroups[inst_id]
        except Exception, exp:
            print "Warning all done: ", exp
            return

        # Link HOSTGROUPS with hosts
        for hg in inp_hostgroups:
            new_members = []
            for (i, hname) in hg.members:
                h = inp_hosts.find_by_name(hname)
                if h:
                    new_members.append(h)
            hg.members = new_members

        # Merge HOSTGROUPS with real ones
        for inphg in inp_hostgroups:
            hgname = inphg.hostgroup_name
            hg = self.hostgroups.find_by_name(hgname)
            # If hte hostgroup already exist, just add the new
            # hosts into it
            if hg:
                hg.members.extend(inphg.members)
            else:  # else take the new one
                self.hostgroups[inphg.id] = inphg
        # We can declare hostgroups done
        self.hostgroups.create_reversed_list()

        # Now link HOSTS with hostgroups, and commands
        for h in inp_hosts:
            #print "Linking %s groups %s" % (h.get_name(), h.hostgroups)
            new_hostgroups = []
            for hgname in h.hostgroups.split(','):
                hg = self.hostgroups.find_by_name(hgname)
                if hg:
                    new_hostgroups.append(hg)
            h.hostgroups = new_hostgroups

            # Now link Command() objects
            self.linkify_a_command(h, 'check_command')
            self.linkify_a_command(h, 'event_handler')

            # Now link timeperiods
            self.linkify_a_timeperiod_by_name(h, 'notification_period')
            self.linkify_a_timeperiod_by_name(h, 'check_period')
            self.linkify_a_timeperiod_by_name(h, 'maintenance_period')

            # And link contacts too
            self.linkify_contacts(h, 'contacts')

            # Linkify tags
            for t in h.tags:
                if not t in self.tags:
                    self.tags[t] = 0
                self.tags[t] += 1

            # We can really declare this host OK now
            self.hosts[h.id] = h

        self.hosts.create_reversed_list()

        # Link SERVICEGROUPS with services
        for sg in inp_servicegroups:
            new_members = []
            for (i, sname) in sg.members:
                if i not in inp_services:
                    continue
                s = inp_services[i]
                new_members.append(s)
            sg.members = new_members

        # Merge SERVICEGROUPS with real ones
        for inpsg in inp_servicegroups:
            sgname = inpsg.servicegroup_name
            sg = self.servicegroups.find_by_name(sgname)
            # If the servicegroup already exist, just add the new
            # services into it
            if sg:
                sg.members.extend(inpsg.members)
            else:  # else take the new one
                self.servicegroups[inpsg.id] = inpsg
        # We can declare servicegroups done
        self.servicegroups.create_reversed_list()

        # Now link SERVICES with hosts, servicesgroups, and commands
        for s in inp_services:
            new_servicegroups = []
            for sgname in s.servicegroups.split(','):
                sg = self.servicegroups.find_by_name(sgname)
                if sg:
                    new_servicegroups.append(sg)
            s.servicegroups = new_servicegroups

            # Now link with host
            hname = s.host_name
            s.host = self.hosts.find_by_name(hname)
            if s.host:
                s.host.services.append(s)

            # Now link Command() objects
            self.linkify_a_command(s, 'check_command')
            self.linkify_a_command(s, 'event_handler')

            # Now link timeperiods
            self.linkify_a_timeperiod_by_name(s, 'notification_period')
            self.linkify_a_timeperiod_by_name(s, 'check_period')
            self.linkify_a_timeperiod_by_name(s, 'maintenance_period')

            # And link contacts too
            self.linkify_contacts(s, 'contacts')

            # Linkify services tags
            for t in s.tags:
                if not t in self.services_tags:
                    self.services_tags[t] = 0
                self.services_tags[t] += 1

            # We can really declare this host OK now
            self.services[s.id] = s
        self.services.optimize_service_search(self.hosts)


        # Add realm of theses hosts. Only the first is useful
        for h in inp_hosts:
            self.realms.add(h.realm)
            break

        # Now we can link all impacts/source problem list
        # but only for the new ones here of course
        for h in inp_hosts:
            self.linkify_dict_srv_and_hosts(h, 'impacts')
            self.linkify_dict_srv_and_hosts(h, 'source_problems')
            self.linkify_host_and_hosts(h, 'parents')
            self.linkify_host_and_hosts(h, 'childs')
            self.linkify_dict_srv_and_hosts(h, 'parent_dependencies')
            self.linkify_dict_srv_and_hosts(h, 'child_dependencies')


        # Now services too
        for s in inp_services:
            self.linkify_dict_srv_and_hosts(s, 'impacts')
            self.linkify_dict_srv_and_hosts(s, 'source_problems')
            self.linkify_dict_srv_and_hosts(s, 'parent_dependencies')
            self.linkify_dict_srv_and_hosts(s, 'child_dependencies')

        # Linking TIMEPERIOD exclude with real ones now
        for tp in self.timeperiods:
            new_exclude = []
            for ex in tp.exclude:
                exname = ex.timeperiod_name
                t = self.timeperiods.find_by_name(exname)
                if t:
                    new_exclude.append(t)
            tp.exclude = new_exclude

        # Link CONTACTGROUPS with contacts
        for cg in inp_contactgroups:
            new_members = []
            for (i, cname) in cg.members:
                c = self.contacts.find_by_name(cname)
                if c:
                    new_members.append(c)
            cg.members = new_members

        # Merge contactgroups with real ones
        for inpcg in inp_contactgroups:
            cgname = inpcg.contactgroup_name
            cg = self.contactgroups.find_by_name(cgname)
            # If the contactgroup already exist, just add the new
            # contacts into it
            if cg:
                cg.members.extend(inpcg.members)
            else:  # else take the new one
                self.contactgroups[inpcg.id] = inpcg
        # We can declare contactgroups done
        self.contactgroups.create_reversed_list()

        safe_print("ALL LINKING TIME"*10, time.time() - start)

        # clean old objects
        del self.inp_hosts[inst_id]
        del self.inp_hostgroups[inst_id]
        del self.inp_contactgroups[inst_id]
        del self.inp_services[inst_id]
        del self.inp_servicegroups[inst_id]


    # We look for o.prop (CommandCall) and we link the inner
    # Command() object with our real ones
    def linkify_a_command(self, o, prop):
        cc = getattr(o, prop, None)
        # if the command call is void, bypass it
        if not cc:
            setattr(o, prop, None)
            return
        cmdname = cc.command
        c = self.commands.find_by_name(cmdname)
        cc.command = c

    # We look at o.prop and for each command we relink it
    def linkify_commands(self, o, prop):
        v = getattr(o, prop, None)
        if not v:
            # If do not have a command list, put a void list instead
            setattr(o, prop, [])
            return

        for cc in v:
            cmdname = cc.command
            c = self.commands.find_by_name(cmdname)
            cc.command = c

    # We look at the timeperiod() object of o.prop
    # and we replace it with our true one
    def linkify_a_timeperiod(self, o, prop):
        t = getattr(o, prop, None)
        if not t:
            setattr(o, prop, None)
            return
        tpname = t.timeperiod_name
        tp = self.timeperiods.find_by_name(tpname)
        setattr(o, prop, tp)

    # same than before, but the value is a string here
    def linkify_a_timeperiod_by_name(self, o, prop):
        tpname = getattr(o, prop, None)
        if not tpname:
            setattr(o, prop, None)
            return
        tp = self.timeperiods.find_by_name(tpname)
        setattr(o, prop, tp)

    # We look at o.prop and for each contacts in it,
    # we replace it with true object in self.contacts
    def linkify_contacts(self, o, prop):
        v = getattr(o, prop)

        if not v:
            return

        new_v = []
        for cname in v:
            c = self.contacts.find_by_name(cname)
            if c:
                new_v.append(c)
        setattr(o, prop, new_v)

    # We got a service/host dict, we want to get back to a
    # flat list
    def linkify_dict_srv_and_hosts(self, o, prop):
        v = getattr(o, prop)

        if not v:
            setattr(o, prop, [])

        new_v = []
        #print "Linkify Dict SRV/Host", v, o.get_name(), prop
        for name in v['services']:
            elts = name.split('/')
            hname = elts[0]
            sdesc = elts[1]
            s = self.services.find_srv_by_name_and_hostname(hname, sdesc)
            if s:
                new_v.append(s)
        for hname in v['hosts']:
            h = self.hosts.find_by_name(hname)
            if h:
                new_v.append(h)
        setattr(o, prop, new_v)

    def linkify_host_and_hosts(self, o, prop):
        v = getattr(o, prop)

        if not v:
            setattr(o, prop, [])

        new_v = []
        for hname in v:
            h = self.hosts.find_by_name(hname)
            if h:
                new_v.append(h)
        setattr(o, prop, new_v)

############### Brok management part

    def before_after_hook(self, brok, obj):
        """
        This can be used by derived classes to compare the data in the brok
        with the object which will be updated by these data. For example,
        it is possible to find out in this method whether the state of a
        host or service has changed.
        """
        pass

####### INITIAL PART

    def manage_program_status_brok(self, b):
        data = b.data
        c_id = data['instance_id']
        safe_print("Regenerator: Creating config:", c_id)

        # We get a real Conf object ,adn put our data
        c = Config()
        self.update_element(c, data)

        # Clean all in_progress things.
        # And in progress one
        self.inp_hosts[c_id] = Hosts([])
        self.inp_services[c_id] = Services([])
        self.inp_hostgroups[c_id] = Hostgroups([])
        self.inp_servicegroups[c_id] = Servicegroups([])
        self.inp_contactgroups[c_id] = Contactgroups([])

        # And we save it
        self.configs[c_id] = c

        ## Clean the old "hard" objects

        # We should clean all previously added hosts and services
        safe_print("Clean hosts/service of", c_id)
        to_del_h = [h for h in self.hosts if h.instance_id == c_id]
        to_del_srv = [s for s in self.services if s.instance_id == c_id]

        safe_print("Cleaning host:%d srv:%d" % (len(to_del_h), len(to_del_srv)))
        # Clean hosts from hosts and hostgroups
        for h in to_del_h:
            safe_print("Deleting", h.get_name())
            del self.hosts[h.id]

        # Now clean all hostgroups too
        for hg in self.hostgroups:
            safe_print("Cleaning hostgroup %s:%d" % (hg.get_name(), len(hg.members)))
            # Exclude from members the hosts with this inst_id
            hg.members = [h for h in hg.members if h.instance_id != c_id]
            safe_print("Len after", len(hg.members))

        for s in to_del_srv:
            safe_print("Deleting", s.get_full_name())
            del self.services[s.id]

        # Now clean service groups
        for sg in self.servicegroups:
            sg.members = [s for s in sg.members if s.instance_id != c_id]

        # We now regenerate reversed list so the client will find only real objects
        self.create_reversed_list()


    # Get a new host. Add in in in progress tab
    def manage_initial_host_status_brok(self, b):
        data = b.data
        hname = data['host_name']
        inst_id = data['instance_id']

        # Try to get the inp progress Hosts
        try:
            inp_hosts = self.inp_hosts[inst_id]
        except Exception, exp:  # not good. we will cry in theprogram update
            print "Not good!", exp
            return
        #safe_print("Creating an host: %s in instance %d" % (hname, inst_id))

        h = Host({})
        self.update_element(h, data)

        # We need to rebuild Downtime and Comment relationship
        for dtc in h.downtimes + h.comments:
            dtc.ref = h

        # Ok, put in in the in progress hosts
        inp_hosts[h.id] = h


    # From now we only create an hostgroup in the in prepare
    # part. We will link at the end.
    def manage_initial_hostgroup_status_brok(self, b):
        data = b.data
        hgname = data['hostgroup_name']
        inst_id = data['instance_id']

        # Try to get the inp progress Hostgroups
        try:
            inp_hostgroups = self.inp_hostgroups[inst_id]
        except Exception, exp:  # not good. we will cry in theprogram update
            print "Not good!", exp
            return

        safe_print("Creating an hostgroup: %s in instance %d" % (hgname, inst_id))

        # With void members
        hg = Hostgroup([])

        # populate data
        self.update_element(hg, data)

        # We will link hosts into hostgroups later
        # so now only save it
        inp_hostgroups[hg.id] = hg


    def manage_initial_service_status_brok(self, b):
        data = b.data
        hname = data['host_name']
        sdesc = data['service_description']
        inst_id = data['instance_id']

        # Try to get the inp progress Hosts
        try:
            inp_services = self.inp_services[inst_id]
        except Exception, exp:  # not good. we will cry in theprogram update
            print "Not good!", exp
            return
        #safe_print("Creating a service: %s/%s in instance %d" % (hname, sdesc, inst_id))

        s = Service({})
        self.update_element(s, data)

        # We need to rebuild Downtime and Comment relationship
        for dtc in s.downtimes + s.comments:
            dtc.ref = s

        # Ok, put in in the in progress hosts
        inp_services[s.id] = s


    # We create a servicegroup in our in progress part
    # we will link it after
    def manage_initial_servicegroup_status_brok(self, b):
        data = b.data
        sgname = data['servicegroup_name']
        inst_id = data['instance_id']

        # Try to get the inp progress Hostgroups
        try:
            inp_servicegroups = self.inp_servicegroups[inst_id]
        except Exception, exp:  # not good. we will cry in theprogram update
            print "Not good!", exp
            return

        safe_print("Creating a servicegroup: %s in instance %d" % (sgname, inst_id))

        # With void members
        sg = Servicegroup([])

        # populate data
        self.update_element(sg, data)

        # We will link hosts into hostgroups later
        # so now only save it
        inp_servicegroups[sg.id] = sg


    # For Contacts, it's a global value, so 2 cases:
    # We got it -> we update it
    # We don't -> we create it
    # In both cases we need to relink it
    def manage_initial_contact_status_brok(self, b):
        data = b.data
        cname = data['contact_name']
        safe_print("Contact with data", data)
        c = self.contacts.find_by_name(cname)
        if c:
            self.update_element(c, data)
        else:
            safe_print("Creating Contact:", cname)
            c = Contact({})
            self.update_element(c, data)
            self.contacts[c.id] = c

        # Delete some useless contact values
        del c.host_notification_commands
        del c.service_notification_commands
        del c.host_notification_period
        del c.service_notification_period

        # Now manage notification ways too
        # Same than for contacts. We create or
        # update
        nws = c.notificationways
        safe_print("Got notif ways", nws)
        new_notifways = []
        for cnw in nws:
            nwname = cnw.notificationway_name
            nw = self.notificationways.find_by_name(nwname)
            if not nw:
                safe_print("Creating notif way", nwname)
                nw = NotificationWay([])
                self.notificationways[nw.id] = nw
            # Now update it
            for prop in NotificationWay.properties:
                if hasattr(cnw, prop):
                    setattr(nw, prop, getattr(cnw, prop))
            new_notifways.append(nw)

            # Linking the notification way
            # With commands
            self.linkify_commands(nw, 'host_notification_commands')
            self.linkify_commands(nw, 'service_notification_commands')

            # Now link timeperiods
            self.linkify_a_timeperiod(nw, 'host_notification_period')
            self.linkify_a_timeperiod(nw, 'service_notification_period')

        c.notificationways = new_notifways

        # Ok, declare this contact now :)
        # And notif ways too
        self.contacts.create_reversed_list()
        self.notificationways.create_reversed_list()


    # From now we only create an hostgroup with unlink data in the
    # in prepare list. We will link all of them at the end.
    def manage_initial_contactgroup_status_brok(self, b):
        data = b.data
        cgname = data['contactgroup_name']
        inst_id = data['instance_id']

        # Try to get the inp progress Contactgroups
        try:
            inp_contactgroups = self.inp_contactgroups[inst_id]
        except Exception, exp:  # not good. we will cry in theprogram update
            print "Not good!", exp
            return

        safe_print("Creating an contactgroup: %s in instance %d" % (cgname, inst_id))

        # With void members
        cg = Contactgroup([])

        # populate data
        self.update_element(cg, data)

        # We will link contacts into contactgroups later
        # so now only save it
        inp_contactgroups[cg.id] = cg


    # For Timeperiods we got 2 cases: do we already got the command or not.
    # if got: just update it
    # if not: create it and declare it in our main commands
    def manage_initial_timeperiod_status_brok(self, b):
        data = b.data
        #print "Creating timeperiod", data
        tpname = data['timeperiod_name']

        tp = self.timeperiods.find_by_name(tpname)
        if tp:
            # print "Already existing timeperiod", tpname
            self.update_element(tp, data)
        else:
            #print "Creating Timeperiod:", tpname
            tp = Timeperiod({})
            self.update_element(tp, data)
            self.timeperiods[tp.id] = tp
            # We add a timeperiod, we update the reversed list
            self.timeperiods.create_reversed_list()


    # For command we got 2 cases: do we already got the command or not.
    # if got: just update it
    # if not: create it and declare it in our main commands
    def manage_initial_command_status_brok(self, b):
        data = b.data
        cname = data['command_name']

        c = self.commands.find_by_name(cname)
        if c:
            #print "Already existing command", cname, "updating it"
            self.update_element(c, data)
        else:
            #print "Creating a new command", cname
            c = Command({})
            self.update_element(c, data)
            self.commands[c.id] = c
            # Ok, we can regenerate the reversed list so
            self.commands.create_reversed_list()


    def manage_initial_scheduler_status_brok(self, b):
        data = b.data
        scheduler_name = data['scheduler_name']
        print "Creating Scheduler:", scheduler_name, data
        sched = SchedulerLink({})
        print "Created a new scheduler", sched
        self.update_element(sched, data)
        print "Updated scheduler"
        #print "CMD:", c
        self.schedulers[scheduler_name] = sched
        print "scheduler added"


    def manage_initial_poller_status_brok(self, b):
        data = b.data
        poller_name = data['poller_name']
        print "Creating Poller:", poller_name, data
        poller = PollerLink({})
        print "Created a new poller", poller
        self.update_element(poller, data)
        print "Updated poller"
        #print "CMD:", c
        self.pollers[poller_name] = poller
        print "poller added"


    def manage_initial_reactionner_status_brok(self, b):
        data = b.data
        reactionner_name = data['reactionner_name']
        print "Creating Reactionner:", reactionner_name, data
        reac = ReactionnerLink({})
        print "Created a new reactionner", reac
        self.update_element(reac, data)
        print "Updated reactionner"
        #print "CMD:", c
        self.reactionners[reactionner_name] = reac
        print "reactionner added"


    def manage_initial_broker_status_brok(self, b):
        data = b.data
        broker_name = data['broker_name']
        print "Creating Broker:", broker_name, data
        broker = BrokerLink({})
        print "Created a new broker", broker
        self.update_element(broker, data)
        print "Updated broker"
        #print "CMD:", c
        self.brokers[broker_name] = broker
        print "broker added"


    def manage_initial_receiver_status_brok(self, b):
        data = b.data
        receiver_name = data['receiver_name']
        print "Creating Receiver:", receiver_name, data
        receiver = ReceiverLink({})
        print "Created a new receiver", receiver
        self.update_element(receiver, data)
        print "Updated receiver"
        #print "CMD:", c
        self.receivers[receiver_name] = receiver
        print "receiver added"



    # This brok is here when the WHOLE initial phase is done.
    # So we got all data, we can link all together :)
    def manage_initial_broks_done_brok(self, b):
        inst_id = b.data['instance_id']
        print "Finish the configuration of instance", inst_id
        self.all_done_linking(inst_id)


################# Status Update part

# A scheduler send us a "I'm alive" brok. If we never
# heard about this one, we got some problem and we
# ask him some initial data :)
    def manage_update_program_status_brok(self, b):
        data = b.data
        c_id = data['instance_id']

        # If we got an update about an unknown instance, cry and ask for a full
        # version!
        if not c_id in self.configs.keys():
            # Do not ask data too quickly, very dangerous
            # one a minute
            if time.time() - self.last_need_data_send > 60 and self.from_q is not None:
                print "I ask the broker for instance id data:", c_id
                msg = Message(id=0, type='NeedData', data={'full_instance_id': c_id})
                self.from_q.put(msg)
                self.last_need_data_send = time.time()
            return

        # Ok, good conf, we can update it
        c = self.configs[c_id]
        self.update_element(c, data)


    # In fact, an update of a host is like a check return
    def manage_update_host_status_brok(self, b):
        # There are some properties that should not change and are already linked
        # so just remove them
        clean_prop = ['check_command', 'hostgroups',
                      'contacts', 'notification_period', 'contact_groups',
                      'check_period', 'event_handler',
                      'maintenance_period', 'realm', 'customs', 'escalations']

        # some are only use when a topology change happened
        toplogy_change = b.data['topology_change']
        if not toplogy_change:
            other_to_clean = ['childs', 'parents', 'child_dependencies', 'parent_dependencies']
            clean_prop.extend(other_to_clean)

        data = b.data
        for prop in clean_prop:
            del data[prop]

        hname = data['host_name']
        h = self.hosts.find_by_name(hname)

        if h:
            self.before_after_hook(b, h)
            self.update_element(h, data)

            # We can have some change in our impacts and source problems.
            self.linkify_dict_srv_and_hosts(h, 'impacts')
            self.linkify_dict_srv_and_hosts(h, 'source_problems')

            # If the topology change, update it
            if toplogy_change:
                print "Topology change for", h.get_name(), h.parent_dependencies
                self.linkify_host_and_hosts(h, 'parents')
                self.linkify_host_and_hosts(h, 'childs')
                self.linkify_dict_srv_and_hosts(h, 'parent_dependencies')
                self.linkify_dict_srv_and_hosts(h, 'child_dependencies')

            # Relink downtimes and comments
            for dtc in h.downtimes + h.comments:
                dtc.ref = h


    # In fact, an update of a service is like a check return
    def manage_update_service_status_brok(self, b):
        # There are some properties that should not change and are already linked
        # so just remove them
        clean_prop = ['check_command', 'servicegroups',
                      'contacts', 'notification_period', 'contact_groups',
                      'check_period', 'event_handler',
                      'maintenance_period', 'customs', 'escalations']

        # some are only use when a topology change happened
        toplogy_change = b.data['topology_change']
        if not toplogy_change:
            other_to_clean = ['child_dependencies', 'parent_dependencies']
            clean_prop.extend(other_to_clean)

        data = b.data
        for prop in clean_prop:
            del data[prop]

        hname = data['host_name']
        sdesc = data['service_description']
        s = self.services.find_srv_by_name_and_hostname(hname, sdesc)
        if s:
            self.before_after_hook(b, s)
            self.update_element(s, data)

            # We can have some change in our impacts and source problems.
            self.linkify_dict_srv_and_hosts(s, 'impacts')
            self.linkify_dict_srv_and_hosts(s, 'source_problems')

            # If the topology change, update it
            if toplogy_change:
                self.linkify_dict_srv_and_hosts(s, 'parent_dependencies')
                self.linkify_dict_srv_and_hosts(s, 'child_dependencies')

            # Relink downtimes and comments with the service
            for dtc in s.downtimes + s.comments:
                dtc.ref = s


    def manage_update_broker_status_brok(self, b):
        data = b.data
        broker_name = data['broker_name']
        try:
            s = self.brokers[broker_name]
            self.update_element(s, data)
        except Exception:
            pass


    def manage_update_receiver_status_brok(self, b):
        data = b.data
        receiver_name = data['receiver_name']
        try:
            s = self.receivers[receiver_name]
            self.update_element(s, data)
        except Exception:
            pass


    def manage_update_reactionner_status_brok(self, b):
        data = b.data
        reactionner_name = data['reactionner_name']
        try:
            s = self.reactionners[reactionner_name]
            self.update_element(s, data)
        except Exception:
            pass


    def manage_update_poller_status_brok(self, b):
        data = b.data
        poller_name = data['poller_name']
        try:
            s = self.pollers[poller_name]
            self.update_element(s, data)
        except Exception:
            pass


    def manage_update_scheduler_status_brok(self, b):
        data = b.data
        scheduler_name = data['scheduler_name']
        try:
            s = self.schedulers[scheduler_name]
            self.update_element(s, data)
            #print "S:", s
        except Exception:
            pass


################# Check result and schedule part
    def manage_host_check_result_brok(self, b):
        data = b.data
        hname = data['host_name']

        h = self.hosts.find_by_name(hname)
        if h:
            self.before_after_hook(b, h)
            self.update_element(h, data)


    # this brok should arrive within a second after the host_check_result_brok
    def manage_host_next_schedule_brok(self, b):
        self.manage_host_check_result_brok(b)


    # A service check have just arrived, we UPDATE data info with this
    def manage_service_check_result_brok(self, b):
        data = b.data
        hname = data['host_name']
        sdesc = data['service_description']
        s = self.services.find_srv_by_name_and_hostname(hname, sdesc)
        if s:
            self.before_after_hook(b, s)
            self.update_element(s, data)


    # A service check update have just arrived, we UPDATE data info with this
    def manage_service_next_schedule_brok(self, b):
        self.manage_service_check_result_brok(b)

########NEW FILE########
__FILENAME__ = sorter
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
Helper functions for some sorting
"""


# Sort hosts and services by impact, states and co
def hst_srv_sort(s1, s2):
    if s1.business_impact > s2.business_impact:
        return -1
    if s2.business_impact > s1.business_impact:
        return 1

    # Ok, we compute a importance value so
    # For host, the order is UP, UNREACH, DOWN
    # For service: OK, UNKNOWN, WARNING, CRIT
    # And DOWN is before CRITICAL (potential more impact)
    tab = {'host': {0: 0, 1: 4, 2: 1},
           'service': {0: 0, 1: 2, 2: 3, 3: 1}
           }
    state1 = tab[s1.__class__.my_type].get(s1.state_id, 0)
    state2 = tab[s2.__class__.my_type].get(s2.state_id, 0)
    # ok, here, same business_impact
    # Compare warn and crit state
    if state1 > state2:
        return -1
    if state2 > state1:
        return 1

    # Ok, so by name...
    if s1.get_full_name() > s2.get_full_name():
        return 1
    else:
        return -1


# Sort hosts and services by impact, states and co
def worse_first(s1, s2):
    # Ok, we compute a importance value so
    # For host, the order is UP, UNREACH, DOWN
    # For service: OK, UNKNOWN, WARNING, CRIT
    # And DOWN is before CRITICAL (potential more impact)
    tab = {'host': {0: 0, 1: 4, 2: 1},
           'service': {0: 0, 1: 2, 2: 3, 3: 1}
           }
    state1 = tab[s1.__class__.my_type].get(s1.state_id, 0)
    state2 = tab[s2.__class__.my_type].get(s2.state_id, 0)

    # ok, here, same business_impact
    # Compare warn and crit state
    if state1 > state2:
        return -1
    if state2 > state1:
        return 1

    # Same? ok by business impact
    if s1.business_impact > s2.business_impact:
        return -1
    if s2.business_impact > s1.business_impact:
        return 1

    # Ok, so by name...
    # Ok, so by name...
    if s1.get_full_name() > s2.get_full_name():
        return -1
    else:
        return 1


# Sort hosts and services by last_state_change time
def last_state_change_earlier(s1, s2):
    # ok, here, same business_impact
    # Compare warn and crit state
    if s1.last_state_change > s2.last_state_change:
        return -1
    if s1.last_state_change < s2.last_state_change:
        return 1

    return 0

########NEW FILE########
__FILENAME__ = termcolor
# coding: utf-8
# Copyright (c) 2008-2011 Volvox Development Team
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
# Author: Konstantin Lepa <konstantin.lepa@gmail.com>

"""ANSII Color formatting for output in terminal."""

from __future__ import print_function
import os


__ALL__ = [ 'colored', 'cprint' ]

VERSION = (1, 1, 0)

ATTRIBUTES = dict(
        list(zip([
            'bold',
            'dark',
            '',
            'underline',
            'blink',
            '',
            'reverse',
            'concealed'
            ],
            list(range(1, 9))
            ))
        )
del ATTRIBUTES['']


HIGHLIGHTS = dict(
        list(zip([
            'on_grey',
            'on_red',
            'on_green',
            'on_yellow',
            'on_blue',
            'on_magenta',
            'on_cyan',
            'on_white'
            ],
            list(range(40, 48))
            ))
        )


COLORS = dict(
        list(zip([
            'grey',
            'red',
            'green',
            'yellow',
            'blue',
            'magenta',
            'cyan',
            'white',
            ],
            list(range(30, 38))
            ))
        )


RESET = '\033[0m'


def colored(text, color=None, on_color=None, attrs=None):
    """Colorize text.

    Available text colors:
        red, green, yellow, blue, magenta, cyan, white.

    Available text highlights:
        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.

    Available attributes:
        bold, dark, underline, blink, reverse, concealed.

    Example:
        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])
        colored('Hello, World!', 'green')
    """
    if os.getenv('ANSI_COLORS_DISABLED') is None:
        fmt_str = '\033[%dm%s'
        if color is not None:
            text = fmt_str % (COLORS[color], text)

        if on_color is not None:
            text = fmt_str % (HIGHLIGHTS[on_color], text)

        if attrs is not None:
            for attr in attrs:
                text = fmt_str % (ATTRIBUTES[attr], text)
        # Shinken mod
        if color is not None:
            text += RESET
    return text


def cprint(text, color=None, on_color=None, attrs=None, **kwargs):
    """Print colorize text.

    It accepts arguments of print function.
    """

    print((colored(text, color, on_color, attrs)), **kwargs)


if __name__ == '__main__':
    print('Current terminal type: %s' % os.getenv('TERM'))
    print('Test basic colors:')
    cprint('Grey color', 'grey')
    cprint('Red color', 'red')
    cprint('Green color', 'green')
    cprint('Yellow color', 'yellow')
    cprint('Blue color', 'blue')
    cprint('Magenta color', 'magenta')
    cprint('Cyan color', 'cyan')
    cprint('White color', 'white')
    print(('-' * 78))

    print('Test highlights:')
    cprint('On grey color', on_color='on_grey')
    cprint('On red color', on_color='on_red')
    cprint('On green color', on_color='on_green')
    cprint('On yellow color', on_color='on_yellow')
    cprint('On blue color', on_color='on_blue')
    cprint('On magenta color', on_color='on_magenta')
    cprint('On cyan color', on_color='on_cyan')
    cprint('On white color', color='grey', on_color='on_white')
    print('-' * 78)

    print('Test attributes:')
    cprint('Bold grey color', 'grey', attrs=['bold'])
    cprint('Dark red color', 'red', attrs=['dark'])
    cprint('Underline green color', 'green', attrs=['underline'])
    cprint('Blink yellow color', 'yellow', attrs=['blink'])
    cprint('Reversed blue color', 'blue', attrs=['reverse'])
    cprint('Concealed Magenta color', 'magenta', attrs=['concealed'])
    cprint('Bold underline reverse cyan color', 'cyan',
            attrs=['bold', 'underline', 'reverse'])
    cprint('Dark blink concealed white color', 'white',
            attrs=['dark', 'blink', 'concealed'])
    print(('-' * 78))

    print('Test mixing:')
    cprint('Underline red on grey color', 'red', 'on_grey',
            ['underline'])
    cprint('Reversed green on red color', 'green', 'on_red', ['reverse'])


########NEW FILE########
__FILENAME__ = modulesctx
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


import imp
import os
import sys

from shinken.log import logger

class ModulesContext(object):
    def __init__(self):
        pass

    def set_modulesdir(self, modulesdir):
        self.modules_dir = modulesdir

    def get_modulesdir(self):
        return self.modules_dir


    # Useful for a module to load another one, and get a handler to it
    def get_module(self, name):
        mod_dir  = os.path.abspath(os.path.join(self.modules_dir, name))
        if not mod_dir in sys.path:
            sys.path.append(mod_dir)
        mod_path = os.path.join(self.modules_dir, name, 'module.py')
        if not os.path.exists(mod_path):
            mod_path = os.path.join(self.modules_dir, name, 'module.pyc')
        try:
            if mod_path.endswith('.py'):
                r = imp.load_source(name, mod_path)
            else:
                r = imp.load_compiled(name, mod_path)
        except:
            logger.warning('The module %s cannot be founded or load' % mod_path)
            raise
        return r


modulesctx = ModulesContext()


########NEW FILE########
__FILENAME__ = modulesmanager
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import time
import sys
import traceback
import cStringIO
import imp

from shinken.basemodule import BaseModule
from shinken.log import logger

# We need to manage pre-2.0 module types with _ into the new 2.0 - mode
def uniform_module_type(s):
    return s.replace('_', '-')

class ModulesManager(object):
    """This class is use to manage modules and call callback"""

    def __init__(self, modules_type, modules_path, modules):
        self.modules_path = modules_path
        self.modules_type = modules_type
        self.modules = modules
        self.allowed_types = [uniform_module_type(plug.module_type) for plug in modules]
        self.imported_modules = []
        self.modules_assoc = []
        self.instances = []
        self.to_restart = []
        self.max_queue_size = 0
        self.manager = None


    def load_manager(self, manager):
        self.manager = manager


    # Set the modules requested for this manager
    def set_modules(self, modules):
        self.modules = modules
        self.allowed_types = [uniform_module_type(mod.module_type) for mod in modules]


    def set_max_queue_size(self, max_queue_size):
        self.max_queue_size = max_queue_size


    # Import, instanciate & "init" the modules we have been requested
    def load_and_init(self):
        self.load()
        self.get_instances()


    # Try to import the requested modules ; put the imported modules in self.imported_modules.
    # The previous imported modules, if any, are cleaned before.
    def load(self):
        now = int(time.time())
        # We get all modules file with .py
        modules_files = []#fname[:-3] for fname in os.listdir(self.modules_path)
                         #if fname.endswith(".py")]

        # And directories
        modules_files.extend([fname for fname in os.listdir(self.modules_path)
                               if os.path.isdir(os.path.join(self.modules_path, fname))])

        # Now we try to load them
        # So first we add their dir into the sys.path
        if not self.modules_path in sys.path:
            sys.path.append(self.modules_path)

        # We try to import them, but we keep only the one of
        # our type
        del self.imported_modules[:]
        for fname in modules_files:
            try:
                # Then we load the module.py inside this directory
                mod_file = os.path.abspath(os.path.join(self.modules_path, fname,'module.py'))
                mod_dir  =  os.path.dirname(mod_file)
                # We add this dir to sys.path so the module can load local files too
                sys.path.append(mod_dir)
                if not os.path.exists(mod_file):
                    mod_file = os.path.abspath(os.path.join(self.modules_path, fname,'module.pyc'))
                m = None
                if mod_file.endswith('.py'):
                    # important, equivalent to import fname from module.py
                    m = imp.load_source(fname, mod_file)
                else:
                    m = imp.load_compiled(fname, mod_file)
                m_dir = os.path.abspath(os.path.dirname(m.__file__))
                
                # Look if it's a valid module
                if not hasattr(m, 'properties'):
                    logger.warning('Bad module file for %s : missing properties dict' % mod_file)
                    continue
                
                # We want to keep only the modules of our type
                if self.modules_type in m.properties['daemons']:
                    self.imported_modules.append(m)
            except Exception, exp:
                # Oups, somethign went wrong here...
                logger.warning("Importing module %s: %s" % (fname, exp))

        # Now we want to find in theses modules the ones we are looking for
        del self.modules_assoc[:]
        for mod_conf in self.modules:
            module_type = uniform_module_type(mod_conf.module_type)
            is_find = False
            for module in self.imported_modules:
                if uniform_module_type(module.properties['type']) == module_type:
                    self.modules_assoc.append((mod_conf, module))
                    is_find = True
                    break
            if not is_find:
                # No module is suitable, we Raise a Warning
                logger.warning("The module type %s for %s was not found in modules!" % (module_type, mod_conf.get_name()))


    # Try to "init" the given module instance.
    # If late_start, don't look for last_init_try
    # Returns: True on successful init. False if instance init method raised any Exception.
    def try_instance_init(self, inst, late_start=False):
        try:
            logger.info("Trying to init module: %s" % inst.get_name())
            inst.init_try += 1
            # Maybe it's a retry
            if not late_start and inst.init_try > 1:
                # Do not try until 5 sec, or it's too loopy
                if inst.last_init_try > time.time() - 5:
                    return False
            inst.last_init_try = time.time()

            # If it's an external, create/update Queues()
            if inst.is_external:
                inst.create_queues(self.manager)

            inst.init()
        except Exception, e:
            logger.error("The instance %s raised an exception %s, I remove it!" % (inst.get_name(), str(e)))
            output = cStringIO.StringIO()
            traceback.print_exc(file=output)
            logger.error("Back trace of this remove: %s" % (output.getvalue()))
            output.close()
            return False
        return True


    # Request to "remove" the given instances list or all if not provided
    def clear_instances(self, insts=None):
        if insts is None:
            insts = self.instances[:]  # have to make a copy of the list
        for i in insts:
            self.remove_instance(i)


    # Put an instance to the restart queue
    def set_to_restart(self, inst):
        self.to_restart.append(inst)


    # actually only arbiter call this method with start_external=False..
    # Create, init and then returns the list of module instances that the caller needs.
    # If an instance can't be created or init'ed then only log is done.
    # That instance is skipped. The previous modules instance(s), if any, are all cleaned.
    def get_instances(self):
        self.clear_instances()
        for (mod_conf, module) in self.modules_assoc:
            try:
                mod_conf.properties = module.properties.copy()
                inst = module.get_instance(mod_conf)
                # Give the module the data to which module it is load from
                inst.set_loaded_into(self.modules_type)
                if inst is None:  # None = Bad thing happened :)
                    logger.info("get_instance for module %s returned None!" % (mod_conf.get_name()))
                    continue
                assert(isinstance(inst, BaseModule))
                self.instances.append(inst)
            except Exception, exp:
                s = str(exp)
                if isinstance(s, str):
                    s = s.decode('UTF-8', 'replace')
                logger.error("The module %s raised an exception %s, I remove it!" % (mod_conf.get_name(), s))
                output = cStringIO.StringIO()
                traceback.print_exc(file=output)
                logger.error("Back trace of this remove: %s" % (output.getvalue()))
                output.close()

        for inst in self.instances:
            # External are not init now, but only when they are started
            if not inst.is_external and not self.try_instance_init(inst):
                # If the init failed, we put in in the restart queue
                logger.warning("The module '%s' failed to init, I will try to restart it later" % inst.get_name())
                self.to_restart.append(inst)

        return self.instances


    # Launch external instances that are load correctly
    def start_external_instances(self, late_start=False):
        for inst in [inst for inst in self.instances if inst.is_external]:
            # But maybe the init failed a bit, so bypass this ones from now
            if not self.try_instance_init(inst, late_start=late_start):
                logger.warning("The module '%s' failed to init, I will try to restart it later" % inst.get_name())
                self.to_restart.append(inst)
                continue

            # ok, init succeed
            logger.info("Starting external module %s" % inst.get_name())
            inst.start()


    # Request to cleanly remove the given instance.
    # If instance is external also shutdown it cleanly
    def remove_instance(self, inst):
        # External instances need to be close before (process + queues)
        if inst.is_external:
            logger.debug("Ask stop process for %s" % inst.get_name())
            inst.stop_process()
            logger.debug("Stop process done")

        inst.clear_queues(self.manager)

        # Then do not listen anymore about it
        self.instances.remove(inst)


    def check_alive_instances(self):
        # Only for external
        for inst in self.instances:
            if not inst in self.to_restart:
                if inst.is_external and not inst.process.is_alive():
                    logger.error("The external module %s goes down unexpectedly!" % inst.get_name())
                    logger.info("Setting the module %s to restart" % inst.get_name())
                    # We clean its queues, they are no more useful
                    inst.clear_queues(self.manager)
                    self.to_restart.append(inst)
                    # Ok, no need to look at queue size now
                    continue

                # Now look for man queue size. If above value, the module should got a huge problem
                # and so bailout. It's not a perfect solution, more a watchdog
                # If max_queue_size is 0, don't check this
                if self.max_queue_size == 0:
                    continue
                # Ok, go launch the dog!
                queue_size = 0
                try:
                    queue_size = inst.to_q.qsize()
                except Exception, exp:
                    pass
                if queue_size > self.max_queue_size:
                    logger.error("The external module %s got a too high brok queue size (%s > %s)!" % (inst.get_name(), queue_size, self.max_queue_size))
                    logger.info("Setting the module %s to restart" % inst.get_name())
                    # We clean its queues, they are no more useful
                    inst.clear_queues(self.manager)
                    self.to_restart.append(inst)


    def try_to_restart_deads(self):
        to_restart = self.to_restart[:]
        del self.to_restart[:]
        for inst in to_restart:
            logger.debug("I should try to reinit %s" % inst.get_name())

            if self.try_instance_init(inst):
                logger.debug("Good, I try to restart %s" % inst.get_name())
                # If it's an external, it will start it
                inst.start()
                # Ok it's good now :)
            else:
                self.to_restart.append(inst)


    # Do not give to others inst that got problems
    def get_internal_instances(self, phase=None):
        return [inst for inst in self.instances if not inst.is_external and phase in inst.phases and inst not in self.to_restart]


    def get_external_instances(self, phase=None):
        return [inst for inst in self.instances if inst.is_external and phase in inst.phases and inst not in self.to_restart]


    def get_external_to_queues(self):
        return [inst.to_q for inst in self.instances if inst.is_external and inst not in self.to_restart]


    def get_external_from_queues(self):
        return [inst.from_q for inst in self.instances if inst.is_external and inst not in self.to_restart]


    def stop_all(self):
        # Ask internal to quit if they can
        for inst in self.get_internal_instances():
            if hasattr(inst, 'quit') and callable(inst.quit):
                inst.quit()

        self.clear_instances([inst for inst in self.instances if inst.is_external])

########NEW FILE########
__FILENAME__ = notification
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

from shinken.action import Action
from shinken.brok import Brok
from shinken.property import BoolProp, IntegerProp, StringProp, FloatProp
from shinken.autoslots import AutoSlots


class Notification(Action):
    """Please Add a Docstring to describe the class here"""

    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    my_type = 'notification'

    properties = {
        'is_a':                StringProp(default='notification'),
        'type':                StringProp(default=''),
        'notification_type':   IntegerProp(default=0, fill_brok=['full_status']),
        'start_time':          StringProp(default=0, fill_brok=['full_status']),
        'end_time':            StringProp(default=0, fill_brok=['full_status']),
        'contact_name':        StringProp(default='',fill_brok=['full_status']),
        'host_name':           StringProp(default='',fill_brok=['full_status']),
        'service_description': StringProp(default='',fill_brok=['full_status']),
        'reason_type':         StringProp(default=0, fill_brok=['full_status']),
        'state':               StringProp(default=0, fill_brok=['full_status']),
        'output':              StringProp(default='',fill_brok=['full_status']),
        'ack_author':          StringProp(default='',fill_brok=['full_status']),
        'ack_data':            StringProp(default='',fill_brok=['full_status']),
        'escalated':           BoolProp(default=False, fill_brok=['full_status']),
        'contacts_notified':   StringProp(default=0, fill_brok=['full_status']),
        'env':                 StringProp(default={}),
        'exit_status':         IntegerProp(default=3),
        'command_call':        StringProp(default=None),
        'execution_time':      FloatProp(default=0),
        'u_time':              FloatProp(default=0.0),
        's_time':              FloatProp(default=0.0),
        'contact':             StringProp(default=None),
        '_in_timeout':         BoolProp(default=False),
        'notif_nb':            IntegerProp(default=0),
        'status':              StringProp(default='scheduled'),
        't_to_go':             IntegerProp(default=0),
        'command':             StringProp(default=''),
        'sched_id':            IntegerProp(default=0),
        'timeout':             IntegerProp(default=10),
        'check_time':          IntegerProp(default=0),
        'module_type':         StringProp(default='fork', fill_brok=['full_status']),
        'worker':              StringProp(default='none'),
        'reactionner_tag':     StringProp(default='None'),
        'creation_time':       IntegerProp(default=0),
        'enable_environment_macros': BoolProp(default=0),
        # Keep a list of currently active escalations
        'already_start_escalations':  StringProp(default=set()),

    }

    macros = {
        'NOTIFICATIONTYPE':         'type',
        'NOTIFICATIONRECIPIENTS':   'recipients',
        'NOTIFICATIONISESCALATED':  'escalated',
        'NOTIFICATIONAUTHOR':       'author',
        'NOTIFICATIONAUTHORNAME':   'author_name',
        'NOTIFICATIONAUTHORALIAS':  'author_alias',
        'NOTIFICATIONCOMMENT':      'comment',
        'HOSTNOTIFICATIONNUMBER':   'notif_nb',
        'HOSTNOTIFICATIONID':       'id',
        'SERVICENOTIFICATIONNUMBER': 'notif_nb',
        'SERVICENOTIFICATIONID':    'id'
    }

    def __init__(self, type='PROBLEM' , status='scheduled', command='UNSET', command_call=None, ref=None, contact=None, t_to_go=0, \
                     contact_name='', host_name='', service_description='',
                     reason_type=1, state=0, ack_author='', ack_data='', \
                     escalated=False, contacts_notified=0, \
                     start_time=0, end_time=0, notification_type=0, id=None, \
                     notif_nb=1, timeout=10, env={}, module_type='fork', \
                     reactionner_tag='None', enable_environment_macros=0):

        self.is_a = 'notification'
        self.type = type
        if id is None:  # id != None is for copy call only
            self.id = Action.id
            Action.id += 1
        self._in_timeout = False
        self.timeout = timeout
        self.status = status
        self.exit_status = 3
        self.command = command
        self.command_call = command_call
        self.output = None
        self.execution_time = 0
        self.u_time = 0  # user executon time
        self.s_time = 0  # system execution time

        self.ref = ref

        # Set host_name and description from the ref
        try:
            self.host_name = self.ref.host_name
        except:
            self.host_name = host_name
        try:
            self.service_description = self.ref.service_description
        except:
            self.service_description = service_description

        self.env = env
        self.module_type = module_type
        #self.ref_type = ref_type
        self.t_to_go = t_to_go
        self.notif_nb = notif_nb
        self.contact = contact

        # For brok part
        self.contact_name = contact_name
        self.reason_type = reason_type
        self.state = state
        self.ack_author = ack_author
        self.ack_data = ack_data
        self.escalated = escalated
        self.contacts_notified = contacts_notified
        self.start_time = start_time
        self.end_time = end_time
        self.notification_type = notification_type

        self.creation_time = time.time()
        self.worker = 'none'
        self.reactionner_tag = reactionner_tag
        self.already_start_escalations = set()
        self.enable_environment_macros = enable_environment_macros

    # return a copy of the check but just what is important for execution
    # So we remove the ref and all
    def copy_shell(self):
        # We create a dummy check with nothing in it, just defaults values
        return self.copy_shell__(Notification('', '', '', '', '', '', '', id=self.id))

    def is_launchable(self, t):
        return t >= self.t_to_go

    def is_administrative(self):
        if self.type in ('PROBLEM', 'RECOVERY'):
            return False
        else:
            return True

    def __str__(self):
        return "Notification %d status:%s command:%s ref:%s t_to_go:%s" % (self.id, self.status, self.command, getattr(self, 'ref', 'unknown'), time.asctime(time.localtime(self.t_to_go)))

    def get_id(self):
        return self.id

    def get_return_from(self, n):
        self.exit_status = n.exit_status
        self.execution_time = n.execution_time
        #self.output = c.output
        #self.check_time = c.check_time
        #self.execution_time = c.execution_time


    # Fill data with info of item by looking at brok_type
    # in props of properties or running_properties
    def fill_data_brok_from(self, data, brok_type):
        cls = self.__class__
        # Now config properties
        for prop, entry in cls.properties.items():
            if brok_type in entry.fill_brok:
                data[prop] = getattr(self, prop)

    # Get a brok with initial status
    def get_initial_status_brok(self):
        data = {'id': self.id}

        self.fill_data_brok_from(data, 'full_status')
        b = Brok('notification_raise', data)
        return b

    # Call by pickle for dataify the comment
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)

        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__
        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])
        # Hook for load of 0.4 notification: there were no
        # creation time, must put one
        if not hasattr(self, 'creation_time'):
            self.creation_time = time.time()
        if not hasattr(self, 'reactionner_tag'):
            self.reactionner_tag = 'None'
        if not hasattr(self, 'worker'):
            self.worker = 'none'
        if not getattr(self, 'module_type', None):
            self.module_type = 'fork'
        if not hasattr(self, 'already_start_escalations'):
            self.already_start_escalations = set()
        if not hasattr(self, 'execution_time'):
            self.execution_time = 0
        # s_time and u_time are added between 1.2 and 1.4
        if not hasattr(self, 'u_time'):
            self.u_time = 0
            self.s_time = 0

########NEW FILE########
__FILENAME__ = businessimpactmodulation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


# The resultmodulation class is used for in scheduler modulation of results
# like the return code or the output.

import time

from item import Item, Items

from shinken.property import StringProp, IntegerProp


class Businessimpactmodulation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'businessimpactmodulation'

    properties = Item.properties.copy()
    properties.update({
            'business_impact_modulation_name': StringProp(),
            'business_impact':                IntegerProp(),
            'modulation_period':        StringProp(default=None),
    })

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.business_impact_modulation_name


class Businessimpactmodulations(Items):
    name_property = "business_impact_modulation_name"
    inner_class = Businessimpactmodulation

    def linkify(self, timeperiods):
        self.linkify_cm_by_tp(timeperiods)

    # We just search for each timeperiod the tp
    # and replace the name by the tp
    def linkify_cm_by_tp(self, timeperiods):
        for rm in self:
            mtp_name = rm.modulation_period.strip()

            # The new member list, in id
            mtp = timeperiods.find_by_name(mtp_name)

            if mtp_name != '' and mtp is None:
                err = "Error: the business impact modulation '%s' got an unknown modulation_period '%s'" % (rm.get_name(), mtp_name)
                rm.configuration_errors.append(err)

            rm.modulation_period = mtp

########NEW FILE########
__FILENAME__ = checkmodulation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

from item import Item, Items
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.util import to_name_if_possible
from shinken.log import logger


class CheckModulation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'checkmodulation'

    properties = Item.properties.copy()
    properties.update({
        'checkmodulation_name':          StringProp(fill_brok=['full_status']),
        'check_command':          StringProp(fill_brok=['full_status']),
        'check_period' :          StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
    })

    running_properties = Item.running_properties.copy()

    _special_properties = ('check_period',)

    macros = {}

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.checkmodulation_name


    # Will look at if our check_period is ok, and give our check_command if we got it
    def get_check_command(self, t_to_go):
        if not self.check_period or self.check_period.is_time_valid(t_to_go):
            return self.check_command
        return None


    # Should have all properties, or a void check_period
    def is_correct(self):
        state = True
        cls = self.__class__

        # Raised all previously saw errors like unknown commands or timeperiods
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[item::%s] %s" % (self.get_name(), err))

        for prop, entry in cls.properties.items():
            if prop not in cls._special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.warning("[checkmodulation::%s] %s property not set" % (self.get_name(), prop))
                    state = False  # Bad boy...

        # Ok now we manage special cases...
        # Service part
        if not hasattr(self, 'check_command'):
            logger.warning("[checkmodulation::%s] do not have any check_command defined" % self.get_name())
            state = False
        else:
            if self.check_command is None:
                logger.warning("[checkmodulation::%s] a check_command is missing" % self.get_name())
                state = False
            if not self.check_command.is_valid():
                logger.warning("[checkmodulation::%s] a check_command is invalid" % self.get_name())
                state = False

        # Ok just put None as check_period, means 24x7
        if not hasattr(self, 'check_period'):
            self.check_period = None

        return state


    # In the scheduler we need to relink the commandCall with
    # the real commands
    def late_linkify_cw_by_commands(self, commands):
        if self.check_command:
            self.check_command.late_linkify_with_command(commands)


class CheckModulations(Items):
    name_property = "checkmodulation_name"
    inner_class = CheckModulation


    def linkify(self, timeperiods, commands):
        self.linkify_with_timeperiods(timeperiods, 'check_period')
        self.linkify_one_command_with_commands(commands, 'check_command')


    def new_inner_member(self, name=None, params={}):
        if name is None:
            name = CheckModulation.id
        params['checkmodulation_name'] = name
        #print "Asking a new inner checkmodulation from name %s with params %s" % (name, params)
        cw = CheckModulation(params)
        self.items[cw.id] = cw

########NEW FILE########
__FILENAME__ = command
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items
from shinken.brok import Brok
from shinken.property import StringProp, IntegerProp, BoolProp
from shinken.autoslots import AutoSlots


# Ok, slots are fun: you cannot set the __autoslots__
# on the same class you use, fun isn't it? So we define*
# a dummy useless class to get such :)
class DummyCommand(object):
    pass


class Command(Item):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    id = 0
    my_type = "command"

    properties = Item.properties.copy()
    properties.update({
        'command_name': StringProp(fill_brok=['full_status']),
        'command_line': StringProp(fill_brok=['full_status']),
        'poller_tag':   StringProp(default='None'),
        'reactionner_tag':   StringProp(default='None'),
        'module_type':  StringProp(default=None),
        'timeout':      IntegerProp(default='-1'),
        'enable_environment_macros': BoolProp(default=0),
    })

    def __init__(self, params={}):
        setattr(self, 'id', self.__class__.id)
        #self.id = self.__class__.id
        self.__class__.id += 1

        self.init_running_properties()

        self.customs = {}

        for key in params:
            # delistify attributes if there is only one value
            params[key] = self.compact_unique_attr_value(params[key])
            # Manage customs values
            if key.startswith('_'):
                self.customs[key.upper()] = params[key]
            else:
                setattr(self, key, params[key])

        if not hasattr(self, 'timeout'):
            self.timeout = '-1'

        if not hasattr(self, 'poller_tag'):
            self.poller_tag = 'None'
        if not hasattr(self, 'enable_environment_macros'):
            self.enable_environment_macros = 0
        if not hasattr(self, 'reactionner_tag'):
            self.reactionner_tag = 'None'
        if not hasattr(self, 'module_type'):
            # If the command start with a _, set the module_type
            # as the name of the command, without the _
            if getattr(self, 'command_line', '').startswith('_'):
                module_type = getattr(self, 'command_line', '').split(' ')[0]
                # and we remove the first _
                self.module_type = module_type[1:]
            # If no command starting with _, be fork :)
            else:
                self.module_type = 'fork'

    def get_name(self):
        return self.command_name

    def pythonize(self):
        self.command_name = self.command_name.strip()
        self.timeout = int(self.timeout)

    def __str__(self):
        return str(self.__dict__)

    # Get a brok with initial status
    def get_initial_status_brok(self):
        cls = self.__class__
        my_type = cls.my_type
        data = {'id': self.id}

        self.fill_data_brok_from(data, 'full_status')
        b = Brok('initial_' + my_type + '_status', data)
        return b

    def fill_data_brok_from(self, data, brok_type):
        cls = self.__class__
        # Now config properties
        for prop, entry in cls.properties.items():
            # Is this property intended for broking?
            #if 'fill_brok' in entry[prop]:
            if brok_type in entry.fill_brok:
                if hasattr(self, prop):
                    data[prop] = getattr(self, prop)
                #elif 'default' in entry[prop]:
                #    data[prop] = entry.default



    # Call by pickle to dataify the comment
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)

        return res

    # Inversed function of getstate
    def __setstate__(self, state):
        cls = self.__class__
        # We move during 1.0 to a dict state
        # but retention file from 0.8 was tuple
        if isinstance(state, tuple):
            self.__setstate_pre_1_0__(state)
            return
        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])

    # In 1.0 we move to a dict save. Before, it was
    # a tuple save, like
    # ({'id': 11}, {'poller_tag': 'None', 'reactionner_tag': 'None',
    # 'command_line': u'/usr/local/nagios/bin/rss-multiuser',
    # 'module_type': 'fork', 'command_name': u'notify-by-rss'})
    def __setstate_pre_1_0__(self, state):
        for d in state:
            for k, v in d.items():
                setattr(self, k, v)


class Commands(Items):

    inner_class = Command
    name_property = "command_name"

########NEW FILE########
__FILENAME__ = config
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" Config is the class to read, load and manipulate the user
 configuration. It read a main cfg (nagios.cfg) and get all informations
 from it. It create objects, make link between them, clean them, and cut
 them into independent parts. The main user of this is Arbiter, but schedulers
 use it too (but far less)"""

import re
import sys
import string
import copy
import os
import socket
import itertools
import time
import random
import cPickle
from StringIO import StringIO
from multiprocessing import Process, Manager

from item import Item
from timeperiod import Timeperiod, Timeperiods
from service import Service, Services
from command import Command, Commands
from resultmodulation import Resultmodulation, Resultmodulations
from businessimpactmodulation import Businessimpactmodulation, Businessimpactmodulations
from escalation import Escalation, Escalations
from serviceescalation import Serviceescalation, Serviceescalations
from hostescalation import Hostescalation, Hostescalations
from host import Host, Hosts
from hostgroup import Hostgroup, Hostgroups
from realm import Realm, Realms
from contact import Contact, Contacts
from contactgroup import Contactgroup, Contactgroups
from notificationway import NotificationWay, NotificationWays
from checkmodulation import CheckModulation, CheckModulations
from macromodulation import MacroModulation, MacroModulations
from servicegroup import Servicegroup, Servicegroups
from servicedependency import Servicedependency, Servicedependencies
from hostdependency import Hostdependency, Hostdependencies
from module import Module, Modules
from discoveryrule import Discoveryrule, Discoveryrules
from discoveryrun import Discoveryrun, Discoveryruns
from hostextinfo import HostExtInfo, HostsExtInfo
from serviceextinfo import ServiceExtInfo, ServicesExtInfo
from trigger import Trigger, Triggers
from pack import Pack, Packs

from shinken.util import split_semicolon
from shinken.arbiterlink import ArbiterLink, ArbiterLinks
from shinken.schedulerlink import SchedulerLink, SchedulerLinks
from shinken.reactionnerlink import ReactionnerLink, ReactionnerLinks
from shinken.brokerlink import BrokerLink, BrokerLinks
from shinken.receiverlink import ReceiverLink, ReceiverLinks
from shinken.pollerlink import PollerLink, PollerLinks
from shinken.graph import Graph
from shinken.log import logger, console_logger
from shinken.property import UnusedProp, BoolProp, IntegerProp, CharProp, StringProp, LogLevelProp, ListProp
from shinken.daemon import get_cur_user, get_cur_group

no_longer_used_txt = 'This parameter is not longer take from the main file, but must be defined in the status_dat broker module instead. But Shinken will create you one if there are no present and use this parameter in it, so no worry.'
not_interresting_txt = 'We do not think such an option is interesting to manage.'


class Config(Item):
    cache_path = "objects.cache"
    my_type = "config"

    # Properties:
    # *required: if True, there is not default, and the config must put them
    # *default: if not set, take this value
    # *pythonize: function call to
    # *class_inherit: (Service, 'blabla'): must set this property to the
    #  Service class with name blabla
    #  if (Service, None): must set this property to the Service class with
    #  same name
    # *unused: just to warn the user that the option he use is no more used
    #  in Shinken
    # *usage_text: if present, will print it to explain why it's no more useful
    properties = {
        'prefix':                   StringProp(default='/usr/local/shinken/'),
        'workdir':                  StringProp(default='/var/run/shinken/'),
        'config_base_dir':          StringProp(default=''), # will be set when we will load a file
        'modules_dir':               StringProp(default='/var/lib/shinken/modules'),
        'use_local_log':            BoolProp(default='1'),
        'log_level':                LogLevelProp(default='WARNING'),
        'local_log':                StringProp(default='/var/log/shinken/arbiterd.log'),
        'log_file':                 UnusedProp(text=no_longer_used_txt),
        'object_cache_file':        UnusedProp(text=no_longer_used_txt),
        'precached_object_file':    UnusedProp(text='Shinken does not use precached_object_files. Skipping.'),
        'resource_file':            StringProp(default='/tmp/resources.txt'),
        'temp_file':                UnusedProp(text='Temporary files are not used in the shinken architecture. Skipping'),
        'status_file':              UnusedProp(text=no_longer_used_txt),
        'status_update_interval':   UnusedProp(text=no_longer_used_txt),
        'shinken_user':             StringProp(default=get_cur_user()),
        'shinken_group':            StringProp(default=get_cur_group()),
        'enable_notifications':     BoolProp(default='1', class_inherit=[(Host, None), (Service, None), (Contact, None)]),
        'execute_service_checks':   BoolProp(default='1', class_inherit=[(Service, 'execute_checks')]),
        'accept_passive_service_checks': BoolProp(default='1', class_inherit=[(Service, 'accept_passive_checks')]),
        'execute_host_checks':      BoolProp(default='1', class_inherit=[(Host, 'execute_checks')]),
        'accept_passive_host_checks': BoolProp(default='1', class_inherit=[(Host, 'accept_passive_checks')]),
        'enable_event_handlers':    BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'log_rotation_method':      CharProp(default='d'),
        'log_archive_path':         StringProp(default='/usr/local/shinken/var/archives'),
        'check_external_commands':  BoolProp(default='1'),
        'command_check_interval':   UnusedProp(text='another value than look always the file is useless, so we fix it.'),
        'command_file':             StringProp(default=''),
        'external_command_buffer_slots': UnusedProp(text='We do not limit the external command slot.'),
        'check_for_updates':        UnusedProp(text='network administrators will never allow such communication between server and the external world. Use your distribution packet manager to know if updates are available or go to the http://www.shinken-monitoring.org website instead.'),
        'bare_update_checks':       UnusedProp(text=None),
        'lock_file':                StringProp(default='/var/run/shinken/arbiterd.pid'),
        'retain_state_information': UnusedProp(text='sorry, retain state information will not be implemented because it is useless.'),
        'state_retention_file':     StringProp(default=''),
        'retention_update_interval': IntegerProp(default='60'),
        'use_retained_program_state': UnusedProp(text=not_interresting_txt),
        'use_retained_scheduling_info': UnusedProp(text=not_interresting_txt),
        'retained_host_attribute_mask': UnusedProp(text=not_interresting_txt),
        'retained_service_attribute_mask': UnusedProp(text=not_interresting_txt),
        'retained_process_host_attribute_mask': UnusedProp(text=not_interresting_txt),
        'retained_process_service_attribute_mask': UnusedProp(text=not_interresting_txt),
        'retained_contact_host_attribute_mask': UnusedProp(text=not_interresting_txt),
        'retained_contact_service_attribute_mask': UnusedProp(text=not_interresting_txt),
        'use_syslog':               BoolProp(default='0'),
        'log_notifications':        BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'log_service_retries':      BoolProp(default='1', class_inherit=[(Service, 'log_retries')]),
        'log_host_retries':         BoolProp(default='1', class_inherit=[(Host, 'log_retries')]),
        'log_event_handlers':       BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'log_initial_states':       BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'log_external_commands':    BoolProp(default='1'),
        'log_passive_checks':       BoolProp(default='1'),
        'global_host_event_handler': StringProp(default='', class_inherit=[(Host, 'global_event_handler')]),
        'global_service_event_handler': StringProp(default='', class_inherit=[(Service, 'global_event_handler')]),
        'sleep_time':               UnusedProp(text='this deprecated option is useless in the shinken way of doing.'),
        'service_inter_check_delay_method': UnusedProp(text='This option is useless in the Shinken scheduling. The only way is the smart way.'),
        'max_service_check_spread': IntegerProp(default='30', class_inherit=[(Service, 'max_check_spread')]),
        'service_interleave_factor': UnusedProp(text='This option is useless in the Shinken scheduling because it use a random distribution for initial checks.'),
        'max_concurrent_checks':    UnusedProp(text='Limiting the max concurrent checks is not helpful to got a good running monitoring server.'),
        'check_result_reaper_frequency': UnusedProp(text='Shinken do not use reaper process.'),
        'max_check_result_reaper_time': UnusedProp(text='Shinken do not use reaper process.'),
        'check_result_path':        UnusedProp(text='Shinken use in memory returns, not check results on flat file.'),
        'max_check_result_file_age': UnusedProp(text='Shinken do not use flat file check resultfiles.'),
        'host_inter_check_delay_method': UnusedProp(text='This option is unused in the Shinken scheduling because distribution of the initial check is a random one.'),
        'max_host_check_spread':    IntegerProp(default='30', class_inherit=[(Host, 'max_check_spread')]),
        'interval_length':          IntegerProp(default='60', class_inherit=[(Host, None), (Service, None)]),
        'auto_reschedule_checks':   BoolProp(managed=False, default='1'),
        'auto_rescheduling_interval': IntegerProp(managed=False, default='1'),
        'auto_rescheduling_window': IntegerProp(managed=False, default='180'),
        'use_aggressive_host_checking': BoolProp(default='0', class_inherit=[(Host, None)]),
        'translate_passive_host_checks': BoolProp(managed=False, default='1'),
        'passive_host_checks_are_soft': BoolProp(managed=False, default='1'),
        'enable_predictive_host_dependency_checks': BoolProp(managed=False, default='1', class_inherit=[(Host, 'enable_predictive_dependency_checks')]),
        'enable_predictive_service_dependency_checks': StringProp(managed=False, default='1'),
        'cached_host_check_horizon': IntegerProp(default='0', class_inherit=[(Host, 'cached_check_horizon')]),
        'cached_service_check_horizon': IntegerProp(default='0', class_inherit=[(Service, 'cached_check_horizon')]),
        'use_large_installation_tweaks': UnusedProp(text='this option is deprecated because in shinken it is just an alias for enable_environment_macros=0'),
        'free_child_process_memory': UnusedProp(text='this option is automatic in Python processes'),
        'child_processes_fork_twice': UnusedProp(text='fork twice is not use.'),
        'enable_environment_macros': BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'enable_flap_detection':    BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'low_service_flap_threshold': IntegerProp(default='20', class_inherit=[(Service, 'global_low_flap_threshold')]),
        'high_service_flap_threshold': IntegerProp(default='30', class_inherit=[(Service, 'global_high_flap_threshold')]),
        'low_host_flap_threshold':  IntegerProp(default='20', class_inherit=[(Host, 'global_low_flap_threshold')]),
        'high_host_flap_threshold': IntegerProp(default='30', class_inherit=[(Host, 'global_high_flap_threshold')]),
        'soft_state_dependencies':  BoolProp(managed=False, default='0'),
        'service_check_timeout':    IntegerProp(default='60', class_inherit=[(Service, 'check_timeout')]),
        'host_check_timeout':       IntegerProp(default='30', class_inherit=[(Host, 'check_timeout')]),
        'timeout_exit_status':      IntegerProp(default='2'),
        'event_handler_timeout':    IntegerProp(default='30', class_inherit=[(Host, None), (Service, None)]),
        'notification_timeout':     IntegerProp(default='30', class_inherit=[(Host, None), (Service, None)]),
        'ocsp_timeout':             IntegerProp(default='15', class_inherit=[(Service, None)]),
        'ochp_timeout':             IntegerProp(default='15', class_inherit=[(Host, None)]),
        'perfdata_timeout':         IntegerProp(default='5', class_inherit=[(Host, None), (Service, None)]),
        'obsess_over_services':     BoolProp(default='0', class_inherit=[(Service, 'obsess_over')]),
        'ocsp_command':             StringProp(default='', class_inherit=[(Service, None)]),
        'obsess_over_hosts':        BoolProp(default='0', class_inherit=[(Host, 'obsess_over')]),
        'ochp_command':             StringProp(default='', class_inherit=[(Host, None)]),
        'process_performance_data': BoolProp(default='1', class_inherit=[(Host, None), (Service, None)]),
        'host_perfdata_command':    StringProp(default='', class_inherit=[(Host, 'perfdata_command')]),
        'service_perfdata_command': StringProp(default='', class_inherit=[(Service, 'perfdata_command')]),
        'host_perfdata_file':       StringProp(default='', class_inherit=[(Host, 'perfdata_file')]),
        'service_perfdata_file':    StringProp(default='', class_inherit=[(Service, 'perfdata_file')]),
        'host_perfdata_file_template': StringProp(default='/tmp/host.perf', class_inherit=[(Host, 'perfdata_file_template')]),
        'service_perfdata_file_template': StringProp(default='/tmp/host.perf', class_inherit=[(Service, 'perfdata_file_template')]),
        'host_perfdata_file_mode':  CharProp(default='a', class_inherit=[(Host, 'perfdata_file_mode')]),
        'service_perfdata_file_mode': CharProp(default='a', class_inherit=[(Service, 'perfdata_file_mode')]),
        'host_perfdata_file_processing_interval': IntegerProp(managed=False, default='15'),
        'service_perfdata_file_processing_interval': IntegerProp(managed=False, default='15'),
        'host_perfdata_file_processing_command': StringProp(managed=False, default='', class_inherit=[(Host, 'perfdata_file_processing_command')]),
        'service_perfdata_file_processing_command': StringProp(managed=False, default=None),
        'check_for_orphaned_services': BoolProp(default='1', class_inherit=[(Service, 'check_for_orphaned')]),
        'check_for_orphaned_hosts': BoolProp(default='1', class_inherit=[(Host, 'check_for_orphaned')]),
        'check_service_freshness': BoolProp(default='1', class_inherit=[(Service, 'global_check_freshness')]),
        'service_freshness_check_interval': IntegerProp(default='60'),
        'check_host_freshness': BoolProp(default='1', class_inherit=[(Host, 'global_check_freshness')]),
        'host_freshness_check_interval': IntegerProp(default='60'),
        'additional_freshness_latency': IntegerProp(default='15', class_inherit=[(Host, None), (Service, None)]),
        'enable_embedded_perl': BoolProp(managed=False, default='1', help='It will surely never be managed, but it should not be useful with poller performances.'),
        'use_embedded_perl_implicitly': BoolProp(managed=False, default='0'),
        'date_format':          StringProp(managed=False, default=None),
        'use_timezone':         StringProp(default='', class_inherit=[(Host, None), (Service, None), (Contact, None)]),
        'illegal_object_name_chars': StringProp(default="""`~!$%^&*"|'<>?,()=""", class_inherit=[(Host, None), (Service, None), (Contact, None), (HostExtInfo, None)]),
        'illegal_macro_output_chars': StringProp(default='', class_inherit=[(Host, None), (Service, None), (Contact, None)]),
        'use_regexp_matching':  BoolProp(managed=False, default='0', help=' if you go some host or service definition like prod*, it will surely failed from now, sorry.'),
        'use_true_regexp_matching': BoolProp(managed=False, default=None),
        'admin_email':          UnusedProp(text='sorry, not yet implemented.'),
        'admin_pager':          UnusedProp(text='sorry, not yet implemented.'),
        'event_broker_options': UnusedProp(text='event broker are replaced by modules with a real configuration template.'),
        'broker_module':        StringProp(default=''),
        'debug_file':           UnusedProp(text=None),
        'debug_level':          UnusedProp(text=None),
        'debug_verbosity':      UnusedProp(text=None),
        'max_debug_file_size':  UnusedProp(text=None),
        'modified_attributes':  IntegerProp(default=0L),
        #'$USERn$: {'required':False, 'default':''} # Add at run in __init__

        # SHINKEN SPECIFIC
        'idontcareaboutsecurity': BoolProp(default='0'),
        'daemon_enabled'        : BoolProp(default='1'), # Put to 0 to disable the arbiter to run
        'daemon_thread_pool_size' : IntegerProp(default='8'),
        'flap_history': IntegerProp(default='20', class_inherit=[(Host, None), (Service, None)]),
        'max_plugins_output_length': IntegerProp(default='8192', class_inherit=[(Host, None), (Service, None)]),
        'no_event_handlers_during_downtimes': BoolProp(default='0', class_inherit=[(Host, None), (Service, None)]),

        # Interval between cleaning queues pass
        'cleaning_queues_interval': IntegerProp(default='900'),

        # Enable or not the notice about old Nagios parameters
        'disable_old_nagios_parameters_whining': BoolProp(default='0'),

        # Now for problem/impact states changes
        'enable_problem_impacts_states_change': BoolProp(default='0', class_inherit=[(Host, None), (Service, None)]),

        # More a running value in fact
        'resource_macros_names': ListProp(default=[]),

        # SSL PART
        # global boolean for know if we use ssl or not
        'use_ssl':               BoolProp(default='0', class_inherit=[(SchedulerLink, None), (ReactionnerLink, None),
                                                                (BrokerLink, None), (PollerLink, None), \
                                                                (ReceiverLink, None),  (ArbiterLink, None)]),
        'ca_cert':               StringProp(default='etc/certs/ca.pem'),
        'server_cert':           StringProp(default='etc/certs/server.cert'),
        'server_key':           StringProp(default='etc/certs/server.key'),
        'hard_ssl_name_check':   BoolProp(default='0'),

        # Log format
        'human_timestamp_log':   BoolProp(default='0'),

        ## Discovery part
        'strip_idname_fqdn':    BoolProp(default='1'),
        'runners_timeout':    IntegerProp(default='3600'),

        # pack_distribution_file is for keeping a distribution history
        # of the host distribution in the several "packs" so a same
        # scheduler will have more change of getting the same host
        'pack_distribution_file': StringProp(default='pack_distribution.dat'),

        ## WEBUI part
        'webui_lock_file':    StringProp(default='webui.pid'),
        'webui_port':    IntegerProp(default='8080'),
        'webui_host':    StringProp(default='0.0.0.0'),

        # Large env tweacks
        'use_multiprocesses_serializer':  BoolProp(default='0'),

    }

    macros = {
        'PREFIX':               'prefix',
        'MAINCONFIGFILE':       '',
        'STATUSDATAFILE':       '',
        'COMMENTDATAFILE':      '',
        'DOWNTIMEDATAFILE':     '',
        'RETENTIONDATAFILE':    '',
        'OBJECTCACHEFILE':      '',
        'TEMPFILE':             '',
        'TEMPPATH':             '',
        'LOGFILE':              '',
        'RESOURCEFILE':         '',
        'COMMANDFILE':          'command_file',
        'HOSTPERFDATAFILE':     '',
        'SERVICEPERFDATAFILE':  '',
        'ADMINEMAIL':           '',
        'ADMINPAGER':           ''
    #'USERn': '$USERn$' # Add at run time
    }

    # We create dict of objects
    # Type: 'name in objects': {Class of object, Class of objects,
    # 'property for self for the objects(config)'
    types_creations = {
        'timeperiod':       (Timeperiod, Timeperiods, 'timeperiods'),
        'service':          (Service, Services, 'services'),
        'servicegroup':     (Servicegroup, Servicegroups, 'servicegroups'),
        'command':          (Command, Commands, 'commands'),
        'host':             (Host, Hosts, 'hosts'),
        'hostgroup':        (Hostgroup, Hostgroups, 'hostgroups'),
        'contact':          (Contact, Contacts, 'contacts'),
        'contactgroup':     (Contactgroup, Contactgroups, 'contactgroups'),
        'notificationway':  (NotificationWay, NotificationWays, 'notificationways'),
        'checkmodulation':  (CheckModulation, CheckModulations, 'checkmodulations'),
        'macromodulation':  (MacroModulation, MacroModulations, 'macromodulations'),
        'servicedependency': (Servicedependency, Servicedependencies, 'servicedependencies'),
        'hostdependency':   (Hostdependency, Hostdependencies, 'hostdependencies'),
        'arbiter':          (ArbiterLink, ArbiterLinks, 'arbiters'),
        'scheduler':        (SchedulerLink, SchedulerLinks, 'schedulers'),
        'reactionner':      (ReactionnerLink, ReactionnerLinks, 'reactionners'),
        'broker':           (BrokerLink, BrokerLinks, 'brokers'),
        'receiver':         (ReceiverLink, ReceiverLinks, 'receivers'),
        'poller':           (PollerLink, PollerLinks, 'pollers'),
        'realm':            (Realm, Realms, 'realms'),
        'module':           (Module, Modules, 'modules'),
        'resultmodulation': (Resultmodulation, Resultmodulations, 'resultmodulations'),
        'businessimpactmodulation': (Businessimpactmodulation, Businessimpactmodulations, 'businessimpactmodulations'),
        'escalation':       (Escalation, Escalations, 'escalations'),
        'serviceescalation': (Serviceescalation, Serviceescalations, 'serviceescalations'),
        'hostescalation':   (Hostescalation, Hostescalations, 'hostescalations'),
        'discoveryrule':    (Discoveryrule, Discoveryrules, 'discoveryrules'),
        'discoveryrun':     (Discoveryrun, Discoveryruns, 'discoveryruns'),
        'hostextinfo':      (HostExtInfo, HostsExtInfo, 'hostsextinfo'),
        'serviceextinfo':   (ServiceExtInfo, ServicesExtInfo, 'servicesextinfo'),
    }

    # This tab is used to transform old parameters name into new ones
    # so from Nagios2 format, to Nagios3 ones
    old_properties = {
        'nagios_user':  'shinken_user',
        'nagios_group': 'shinken_group',
        'modulesdir': 'modules_dir',
    }

    read_config_silent = 0

    early_created_types = ['arbiter', 'module']

    configuration_types = ['void', 'timeperiod', 'command', 'contactgroup', 'hostgroup',
                           'contact', 'notificationway', 'checkmodulation', 'macromodulation', 'host', 'service', 'servicegroup',
                           'servicedependency', 'hostdependency', 'arbiter', 'scheduler',
                           'reactionner', 'broker', 'receiver', 'poller', 'realm', 'module',
                           'resultmodulation', 'escalation', 'serviceescalation', 'hostescalation',
                           'discoveryrun', 'discoveryrule', 'businessimpactmodulation',
                           'hostextinfo', 'serviceextinfo']


    def __init__(self):
        self.params = {}
        self.resource_macros_names = []
        # By default the conf is correct
        self.conf_is_correct = True
        # We tag the conf with a magic_hash, a random value to
        # idify this conf
        random.seed(time.time())
        self.magic_hash = random.randint(1, 100000)
        self.configuration_errors = []
        self.triggers_dirs = []
        self.triggers = Triggers({})
        self.packs_dirs = []
        self.packs = Packs({})

    def get_name(self):
        return 'global configuration file'

    # We've got macro in the resource file and we want
    # to update our MACRO dict with it
    def fill_resource_macros_names_macros(self):
        """ fill the macro dict will all value
        from self.resource_macros_names"""
        properties = self.__class__.properties
        macros = self.__class__.macros
        for macro_name in self.resource_macros_names:
            properties['$' + macro_name + '$'] = StringProp(default='')
            macros[macro_name] = '$' + macro_name + '$'

    def load_params(self, params):
        for elt in params:
            elts = elt.split('=', 1)
            if len(elts) == 1:  # error, there is no = !
                self.conf_is_correct = False
                logger.error("[config] the parameter %s is malformed! (no = sign)" % elts[0])
            else:
                self.params[elts[0]] = elts[1]
                setattr(self, elts[0], elts[1])
                # Maybe it's a variable as $USER$ or $ANOTHERVATRIABLE$
                # so look at the first character. If it's a $, it's a variable
                # and if it's end like it too
                if elts[0][0] == '$' and elts[0][-1] == '$':
                    macro_name = elts[0][1:-1]
                    self.resource_macros_names.append(macro_name)

    def _cut_line(self, line):
        #punct = '"#$%&\'()*+/<=>?@[\\]^`{|}~'
        tmp = re.split("[" + string.whitespace + "]+", line, 1)
        r = [elt for elt in tmp if elt != '']
        return r

    def read_config(self, files):
        # just a first pass to get the cfg_file and all files in a buf
        res = StringIO()

        for file in files:
            # We add a \n (or \r\n) to be sure config files are separated
            # if the previous does not finish with a line return
            res.write(os.linesep)
            res.write('# IMPORTEDFROM=%s' % (file) + os.linesep)
            if self.read_config_silent == 0:
                logger.info("[config] opening '%s' configuration file" % file)
            try:
                # Open in Universal way for Windows, Mac, Linux
                fd = open(file, 'rU')
                buf = fd.readlines()
                fd.close()
                self.config_base_dir = os.path.dirname(file)
            except IOError, exp:
                logger.error("[config] cannot open config file '%s' for reading: %s" % (file, exp))
                # The configuration is invalid because we have a bad file!
                self.conf_is_correct = False
                continue

            for line in buf:
                line = line.decode('utf8', 'replace')
                res.write(line)
                if line.endswith('\n'):
                    line = line[:-1]
                line = line.strip()
                if re.search("^cfg_file", line) or re.search("^resource_file", line):
                    elts = line.split('=', 1)
                    if os.path.isabs(elts[1]):
                        cfg_file_name = elts[1]
                    else:
                        cfg_file_name = os.path.join(self.config_base_dir, elts[1])
                    cfg_file_name = cfg_file_name.strip()
                    try:
                        fd = open(cfg_file_name, 'rU')
                        if self.read_config_silent == 0:
                            logger.info("Processing object config file '%s'" % cfg_file_name)
                        res.write(os.linesep + '# IMPORTEDFROM=%s' % (cfg_file_name) + os.linesep)
                        res.write(fd.read().decode('utf8', 'replace'))
                        # Be sure to add a line return so we won't mix files
                        res.write(os.linesep)
                        fd.close()
                    except IOError, exp:
                        logger.error("Cannot open config file '%s' for reading: %s" % (cfg_file_name, exp))
                        # The configuration is invalid because we have a bad file!
                        self.conf_is_correct = False
                elif re.search("^cfg_dir", line):
                    elts = line.split('=', 1)
                    if os.path.isabs(elts[1]):
                        cfg_dir_name = elts[1]
                    else:
                        cfg_dir_name = os.path.join(self.config_base_dir, elts[1])
                    # Ok, look if it's really a directory
                    if not os.path.isdir(cfg_dir_name):
                        logger.error("Cannot open config dir '%s' for reading" % cfg_dir_name)
                        self.conf_is_correct = False

                    # Look for .pack file into it :)
                    self.packs_dirs.append(cfg_dir_name)

                    # Now walk for it.
                    # BEWARE : we can follow symlinks only for python 2.6 and higher
                    args = {}
                    if sys.version_info >= (2, 6):
                        args['followlinks'] = True
                    for root, dirs, files in os.walk(cfg_dir_name, **args):
                        for file in files:
                            if re.search("\.cfg$", file):
                                if self.read_config_silent == 0:
                                    logger.info("Processing object config file '%s'" % os.path.join(root, file))
                                try:
                                    res.write(os.linesep + '# IMPORTEDFROM=%s' % (os.path.join(root, file)) + os.linesep)
                                    fd = open(os.path.join(root, file), 'rU')
                                    res.write(fd.read().decode('utf8', 'replace'))
                                    # Be sure to separate files data
                                    res.write(os.linesep)
                                    fd.close()
                                except IOError, exp:
                                    logger.error("Cannot open config file '%s' for reading: %s" % (os.path.join(root, file), exp))
                                    # The configuration is invalid
                                    # because we have a bad file!
                                    self.conf_is_correct = False
                elif re.search("^triggers_dir", line):
                    elts = line.split('=', 1)
                    if os.path.isabs(elts[1]):
                        trig_dir_name = elts[1]
                    else:
                        trig_dir_name = os.path.join(self.config_base_dir, elts[1])
                    # Ok, look if it's really a directory
                    if not os.path.isdir(trig_dir_name):
                        logger.error("Cannot open triggers dir '%s' for reading" % trig_dir_name)
                        self.conf_is_correct = False
                        continue
                    # Ok it's a valid one, I keep it
                    self.triggers_dirs.append(trig_dir_name)

        config = res.getvalue()
        res.close()
        return config

#        self.read_config_buf(res)

    def read_config_buf(self, buf):
        params = []
        objectscfg = {}
        types = self.__class__.configuration_types
        for t in types:
            objectscfg[t] = []

        tmp = []
        tmp_type = 'void'
        in_define = False
        almost_in_define=False
        continuation_line = False
        tmp_line = ''
        lines = buf.split('\n')
        line_nb = 0 # Keep the line number for the file path
        for line in lines:
            if line.startswith("# IMPORTEDFROM="):
                filefrom = line.split('=')[1]
                line_nb = 0 # reset the line number too
                continue

            line_nb += 1
            # Remove comments
            line = split_semicolon(line)[0].strip()

            # A backslash means, there is more to come
            if re.search("\\\s*$", line) is not None:
                continuation_line = True
                line = re.sub("\\\s*$", "", line)
                line = re.sub("^\s+", " ", line)
                tmp_line += line
                continue
            elif continuation_line:
                # Now the continuation line is complete
                line = re.sub("^\s+", "", line)
                line = tmp_line + line
                tmp_line = ''
                continuation_line = False
            # } alone in a line means stop the object reading
            if re.search("^\s*}\s*$", line) is not None:
                in_define = False

            # { alone in a line can mean start object reading
            if re.search("^\s*{\s*$", line) is not None and almost_in_define:
                almost_in_define=False
                in_define = True
                continue

            if re.search("^\s*#|^\s*$|^\s*}", line) is not None:
                pass
            # A define must be catch and the type save
            # The old entry must be save before
            elif re.search("^define", line) is not None:
                if re.search(".*{.*$", line) is not None:
                    in_define = True
                else:
                    almost_in_define=True

                if tmp_type not in objectscfg:
                    objectscfg[tmp_type] = []
                objectscfg[tmp_type].append(tmp)
                tmp = []
                tmp.append("imported_from " + filefrom+':%d'%line_nb)
                # Get new type
                elts = re.split('\s', line)
                # Maybe there was space before and after the type
                # so we must get all and strip it
                tmp_type = ' '.join(elts[1:]).strip()
                tmp_type = tmp_type.split('{')[0].strip()
            else:
                if in_define:
                    tmp.append(line)
                else:
                    params.append(line)

        # Maybe the type of the last element is unknown, declare it
        if not tmp_type in objectscfg:
            objectscfg[tmp_type] = []


        objectscfg[tmp_type].append(tmp)
        objects = {}

        #print "Params", params
        self.load_params(params)
        # And then update our MACRO dict
        self.fill_resource_macros_names_macros()

        for type in objectscfg:
            objects[type] = []
            for items in objectscfg[type]:
                tmp = {}
                for line in items:
                    elts = self._cut_line(line)
                    if elts == []:
                        continue
                    prop = elts[0]
                    if not prop in tmp:
                        tmp[prop] = []
                    value = ' '.join(elts[1:])
                    tmp[prop].append(value)
                if tmp != {}:
                    objects[type].append(tmp)

        return objects

    # We need to have some ghost objects like
    # the check_command bp_rule for business
    # correlator rules
    def add_ghost_objects(self, raw_objects):
        bp_rule = {'command_name': 'bp_rule', 'command_line': 'bp_rule'}
        raw_objects['command'].append(bp_rule)
        host_up = {'command_name': '_internal_host_up', 'command_line': '_internal_host_up'}
        raw_objects['command'].append(host_up)
        echo_obj = {'command_name': '_echo', 'command_line': '_echo'}
        raw_objects['command'].append(echo_obj)


    # We've got raw objects in string, now create real Instances
    def create_objects(self, raw_objects):
        """ Create real 'object' from dicts of prop/value """
        types_creations = self.__class__.types_creations

        # some types are already created in this time
        early_created_types = self.__class__.early_created_types

        # Before really create the objects, we add
        # ghost ones like the bp_rule for correlation
        self.add_ghost_objects(raw_objects)

        for t in types_creations:
            if t not in early_created_types:
                self.create_objects_for_type(raw_objects, t)


    def create_objects_for_type(self, raw_objects, type):
        types_creations = self.__class__.types_creations
        t = type
        # Ex: the above code do for timeperiods:
        # timeperiods = []
        # for timeperiodcfg in objects['timeperiod']:
        #    t = Timeperiod(timeperiodcfg)
        #    t.clean()
        #    timeperiods.append(t)
        # self.timeperiods = Timeperiods(timeperiods)

        (cls, clss, prop) = types_creations[t]
        # List where we put objects
        lst = []
        for obj_cfg in raw_objects[t]:
            # We create the object
            o = cls(obj_cfg)
            lst.append(o)
        # we create the objects Class and we set it in prop
        setattr(self, prop, clss(lst))


    # Here arbiter and modules objects should be prepare and link
    # before all others types
    def early_arbiter_linking(self):
        """ Prepare the arbiter for early operations """

        # Should look at hacking command_file module first
        self.hack_old_nagios_parameters_for_arbiter()

        self.modules.create_reversed_list()

        if len(self.arbiters) == 0:
            logger.warning("There is no arbiter, I add one in localhost:7770")
            a = ArbiterLink({'arbiter_name': 'Default-Arbiter',
                             'host_name': socket.gethostname(),
                             'address': 'localhost', 'port': '7770',
                             'spare': '0'})
            self.arbiters = ArbiterLinks([a])

        # First fill default
        self.arbiters.fill_default()
        self.modules.fill_default()

        #print "****************** Pythonize ******************"
        self.arbiters.pythonize()

        #print "****************** Linkify ******************"
        self.arbiters.linkify(self.modules)
        self.modules.linkify()

    # We will load all triggers .trig files from all triggers_dir
    def load_triggers(self):
        for p in self.triggers_dirs:
            self.triggers.load_file(p)

    # We will load all packs .pack files from all packs_dirs
    def load_packs(self):
        for p in self.packs_dirs:
            self.packs.load_file(p)

    # We use linkify to make the config more efficient: elements will be
    # linked, like pointers. For example, a host will have it's service,
    # and contacts directly in it's properties
    # REMEMBER: linkify AFTER explode...
    def linkify(self):
        """ Make 'links' between elements, like a host got a services list
        with all it's services in it """

        # First linkify myself like for some global commands
        self.linkify_one_command_with_commands(self.commands, 'ocsp_command')
        self.linkify_one_command_with_commands(self.commands, 'ochp_command')
        self.linkify_one_command_with_commands(self.commands, 'host_perfdata_command')
        self.linkify_one_command_with_commands(self.commands, 'service_perfdata_command')

        #print "Hosts"
        # link hosts with timeperiods and commands
        self.hosts.linkify(self.timeperiods, self.commands, \
                           self.contacts, self.realms, \
                           self.resultmodulations, self.businessimpactmodulations, \
                           self.escalations, self.hostgroups, self.triggers, self.checkmodulations,
                           self.macromodulations
                           )

        self.hostsextinfo.merge(self.hosts)

        # Do the simplify AFTER explode groups
        #print "Hostgroups"
        # link hostgroups with hosts
        self.hostgroups.linkify(self.hosts, self.realms)

        #print "Services"
        # link services with other objects
        self.services.linkify(self.hosts, self.commands, \
                              self.timeperiods, self.contacts,\
                              self.resultmodulations, self.businessimpactmodulations, \
                              self.escalations, self.servicegroups, self.triggers, self.checkmodulations,
                              self.macromodulations
                              )

        self.servicesextinfo.merge(self.services)

        #print "Service groups"
        # link servicegroups members with services
        self.servicegroups.linkify(self.services)

        # link notificationways with timeperiods and commands
        self.notificationways.linkify(self.timeperiods, self.commands)

        # link notificationways with timeperiods and commands
        self.checkmodulations.linkify(self.timeperiods, self.commands)

        # Link with timeperiods
        self.macromodulations.linkify(self.timeperiods)

        #print "Contactgroups"
        # link contacgroups with contacts
        self.contactgroups.linkify(self.contacts)

        #print "Contacts"
        # link contacts with timeperiods and commands
        self.contacts.linkify(self.timeperiods, self.commands,
                              self.notificationways)

        #print "Timeperiods"
        # link timeperiods with timeperiods (exclude part)
        self.timeperiods.linkify()

        #print "Servicedependency"
        self.servicedependencies.linkify(self.hosts, self.services,
                                         self.timeperiods)

        #print "Hostdependency"
        self.hostdependencies.linkify(self.hosts, self.timeperiods)

        #print "Resultmodulations"
        self.resultmodulations.linkify(self.timeperiods)

        self.businessimpactmodulations.linkify(self.timeperiods)

        #print "Escalations"
        self.escalations.linkify(self.timeperiods, self.contacts, \
                                     self.services, self.hosts)

        # Link discovery commands
        self.discoveryruns.linkify(self.commands)

        #print "Realms"
        self.realms.linkify()

        #print "Schedulers and satellites"
        # Link all links with realms
        #self.arbiters.linkify(self.modules)
        self.schedulers.linkify(self.realms, self.modules)
        self.brokers.linkify(self.realms, self.modules)
        self.receivers.linkify(self.realms, self.modules)
        self.reactionners.linkify(self.realms, self.modules)
        self.pollers.linkify(self.realms, self.modules)

        # Ok, now update all realms with backlinks of
        # satellites
        self.realms.prepare_for_satellites_conf()

    # Removes service exceptions based on host configuration
    def remove_exclusions(self):
        return self.services.remove_exclusions(self.hosts)

    # Some elements are maybe set as wrong after a is_correct, so clean them
    # if possible
    def clean(self):
        self.services.clean()

    # In the scheduler we need to relink the commandCall with
    # the real commands
    def late_linkify(self):
        props = ['ocsp_command', 'ochp_command', 'service_perfdata_command', 'host_perfdata_command']
        for prop in props:
            cc = getattr(self, prop, None)
            if cc:
                cc.late_linkify_with_command(self.commands)

        # But also other objects like hosts and services
        self.hosts.late_linkify_h_by_commands(self.commands)
        self.services.late_linkify_s_by_commands(self.commands)
        self.contacts.late_linkify_c_by_commands(self.commands)


    # Some properties are dangerous to be send like that
    # like realms linked in hosts. Realms are too big to send (too linked)
    # We are also pre-serializing the confs so the sending phase will
    # be quicker.
    def prepare_for_sending(self):
        # Preparing hosts and hostgroups for sending. Some properties
        # should be "flatten" before sent, like .realm object that should
        # be changed into names
        self.hosts.prepare_for_sending()
        self.hostgroups.prepare_for_sending()
        t1 = time.time()
        logger.info('[Arbiter] Serializing the configurations...')


        # There are two ways of configuration serializing
        # One if to use the serial way, the other is with use_multiprocesses_serializer
        # to call to sub-wrokers to do the job.
        # TODO : enable on windows? I'm not sure it will work, must give a test
        if os.name == 'nt' or not self.use_multiprocesses_serializer:
            logger.info('Using the default serialization pass')
            for r in self.realms:
                for (i, conf) in r.confs.iteritems():
                    # Remember to protect the local conf hostgroups too!
                    conf.hostgroups.prepare_for_sending()
                    logger.debug('[%s] Serializing the configuration %d' % (r.get_name(), i))
                    t0 = time.time()
                    r.serialized_confs[i] = cPickle.dumps(conf, cPickle.HIGHEST_PROTOCOL)
                    logger.debug("[config] time to serialize the conf %s:%s is %s" % (r.get_name(), i, time.time() - t0))
                    logger.debug("PICKLE LEN : %d" % len(r.serialized_confs[i]))
            # Now pickle the whole conf, for easy and quick spare send
            t0 = time.time()
            whole_conf_pack = cPickle.dumps(self, cPickle.HIGHEST_PROTOCOL)
            logger.debug("[config] time to serialize the global conf : %s" % (time.time() - t0))
            self.whole_conf_pack = whole_conf_pack
            print "TOTAL serializing in", time.time() - t1

        else:
            logger.info('Using the multiprocessing serialization pass')
            t1 = time.time()

            # We ask a manager to manage the communication with our children
            m = Manager()
            # The list will got all the strings from the children
            q = m.list()
            for r in self.realms:
                processes = []
                for (i, conf) in r.confs.iteritems():
                    # This function will be called by the children, and will give
                    # us the pickle result
                    def Serialize_config(q, rname, i, conf):
                        # Remember to protect the local conf hostgroups too!
                        conf.hostgroups.prepare_for_sending()
                        logger.debug('[%s] Serializing the configuration %d' % (rname, i))
                        t0 = time.time()
                        res = cPickle.dumps(conf, cPickle.HIGHEST_PROTOCOL)
                        logger.debug("[config] time to serialize the conf %s:%s is %s" % (rname, i, time.time() - t0))
                        q.append((i, res))

                    # Prepare a sub-process that will manage the pickle computation
                    p = Process(target=Serialize_config, name="serializer-%s-%d" %(r.get_name(), i) , args=(q, r.get_name(), i, conf))
                    p.start()
                    processes.append( (i, p) )

                # Here all sub-processes are launched for this realm, now wait for them to finish
                while len(processes) != 0:
                    to_del = []
                    for (i, p) in processes:
                        if p.exitcode is not None:
                            to_del.append((i, p))
                            # remember to join() so the children can die
                            p.join()
                    for (i, p) in to_del:
                        logger.debug("The sub process %s is done with the return code %d" % (p.name, p.exitcode))
                        processes.remove( (i, p ) )
                    # Don't be too quick to poll!
                    time.sleep(0.1)

                # Check if we got the good number of configuration, maybe one of the cildren got problems?
                if len(q) != len(r.confs):
                    logger.error("Something goes wrong in the configuration serializations, please restart Shinken Arbiter")
                    sys.exit(2)
                # Now get the serialized configuration and saved them into self
                for (i, cfg) in q:
                    r.serialized_confs[i] = cfg
            print "TOTAL TIME", time.time() - t1

            # Now pickle the whole configuration into one big pickle object, for the arbiter spares
            whole_queue = m.list()
            t0 = time.time()
            # The function that just compute the whole conf pickle string, but n a children
            def create_whole_conf_pack(whole_queue, self):
                logger.debug("[config] sub processing the whole configuration pack creation")
                whole_queue.append(cPickle.dumps(self, cPickle.HIGHEST_PROTOCOL))
                logger.debug("[config] sub processing the whole configuration pack creation finished")
            # Go for it
            p = Process(target=create_whole_conf_pack, args=(whole_queue, self), name='serializer-whole-configuration')
            p.start()
            # Wait for it to die
            while p.exitcode is None:
                time.sleep(0.1)
            p.join()
            # Maybe we don't have our result?
            if len(whole_queue) != 1:
                logger.error("Something goes wrong in the whole configuration pack creation, please restart Shinken Arbiter")
                sys.exit(2)

            #Get it and save it
            self.whole_conf_pack = whole_queue.pop()
            logger.debug("[config] time to serialize the global conf : %s" % (time.time() - t0))

            print "TOTAL serializing iin", time.time() - t2
            # Shutdown the manager, the sub-process should be gone now
            m.shutdown()


    def dump(self):
        print "Slots", Service.__slots__
        print 'Hosts:'
        for h in self.hosts:
            print '\t', h.get_name(), h.contacts
        print 'Services:'
        for s in self.services:
            print '\t', s.get_name(), s.contacts

    # It's used to change Nagios2 names to Nagios3 ones
    # For hosts and services
    def old_properties_names_to_new(self):
        super(Config, self).old_properties_names_to_new()
        self.hosts.old_properties_names_to_new()
        self.services.old_properties_names_to_new()
        self.notificationways.old_properties_names_to_new()
        self.contacts.old_properties_names_to_new()

    # It's used to warn about useless parameter and print why it's not use.
    def notice_about_useless_parameters(self):
        if not self.disable_old_nagios_parameters_whining:
            properties = self.__class__.properties
            for prop, entry in properties.items():
                if isinstance(entry, UnusedProp):
                    logger.warning("The parameter %s is useless and can be removed from the configuration (Reason: %s)" % (prop, entry.text))

    # It's used to raise warning if the user got parameter
    # that we do not manage from now
    def warn_about_unmanaged_parameters(self):
        properties = self.__class__.properties
        unmanaged = []
        for prop, entry in properties.items():
            if not entry.managed and hasattr(self, prop):
                if entry.help:
                    s = "%s: %s" % (prop, entry.help)
                else:
                    s = prop
                unmanaged.append(s)
        if len(unmanaged) != 0:
            mailing_list_uri = "https://lists.sourceforge.net/lists/listinfo/shinken-devel"
            logger.warning("The following parameter(s) are not currently managed.")

            for s in unmanaged:
                logger.info(s)

            logger.warning("Unmanaged configuration statement, do you really need it? Ask for it on the developer mailinglist %s or submit a pull request on the Shinken github " % mailing_list_uri)

    # Overrides specific instances properties
    def override_properties(self):
        self.services.override_properties(self.hosts)

    # Use to fill groups values on hosts and create new services
    # (for host group ones)
    def explode(self):
        # first elements, after groups
        #print "Contacts"
        self.contacts.explode(self.contactgroups, self.notificationways)
        #print "Contactgroups"
        self.contactgroups.explode()

        #print "Hosts"
        self.hosts.explode(self.hostgroups, self.contactgroups, self.triggers)
        #print "Hostgroups"
        self.hostgroups.explode()

        #print "Services"
        #print "Initially got nb of services: %d" % len(self.services.items)
        self.services.explode(self.hosts, self.hostgroups, self.contactgroups,
                              self.servicegroups, self.servicedependencies,
                              self.triggers)
        #print "finally got nb of services: %d" % len(self.services.items)
        #print "Servicegroups"
        self.servicegroups.explode()

        #print "Timeperiods"
        self.timeperiods.explode()

        self.hostdependencies.explode(self.hostgroups)

        #print "Servicedependency"
        self.servicedependencies.explode(self.hostgroups)

        # Serviceescalations hostescalations will create new escalations
        self.serviceescalations.explode(self.escalations)
        self.hostescalations.explode(self.escalations)
        self.escalations.explode(self.hosts, self.hostgroups,
                                 self.contactgroups)

        # Now the architecture part
        #print "Realms"
        self.realms.explode()

    # Remove elements will the same name, so twins :)
    # In fact only services should be acceptable with twins
    def remove_twins(self):
        #self.hosts.remove_twins()
        self.services.remove_twins()
        #self.contacts.remove_twins()
        #self.timeperiods.remove_twins()


    # Dependencies are important for scheduling
    # This function create dependencies linked between elements.
    def apply_dependencies(self):
        self.hosts.apply_dependencies()
        self.services.apply_dependencies()

    # Use to apply inheritance (template and implicit ones)
    # So elements will have their configured properties
    def apply_inheritance(self):
        # inheritance properties by template
        #print "Hosts"
        self.hosts.apply_inheritance()
        #print "Contacts"
        self.contacts.apply_inheritance()
        #print "Services"
        self.services.apply_inheritance(self.hosts)
        #print "Servicedependencies"
        self.servicedependencies.apply_inheritance(self.hosts)
        #print "Hostdependencies"
        self.hostdependencies.apply_inheritance()
        # Also timeperiods
        self.timeperiods.apply_inheritance()
        # Also "Hostextinfo"
        self.hostsextinfo.apply_inheritance()
        # Also "Serviceextinfo"
        self.servicesextinfo.apply_inheritance()

        # Now escalations too
        self.serviceescalations.apply_inheritance()
        self.hostescalations.apply_inheritance()
        self.escalations.apply_inheritance()

    # Use to apply implicit inheritance
    def apply_implicit_inheritance(self):
        #print "Services"
        self.services.apply_implicit_inheritance(self.hosts)

    # will fill properties for elements so they will have all theirs properties
    def fill_default(self):
        # Fill default for config (self)
        super(Config, self).fill_default()
        self.hosts.fill_default()
        self.hostgroups.fill_default()
        self.contacts.fill_default()
        self.contactgroups.fill_default()
        self.notificationways.fill_default()
        self.checkmodulations.fill_default()
        self.macromodulations.fill_default()
        self.services.fill_default()
        self.servicegroups.fill_default()
        self.resultmodulations.fill_default()
        self.businessimpactmodulations.fill_default()
        self.hostsextinfo.fill_default()
        self.servicesextinfo.fill_default()

        # Now escalations
        self.escalations.fill_default()

        # Also fill default of host/servicedep objects
        self.servicedependencies.fill_default()
        self.hostdependencies.fill_default()

        # Discovery part
        self.discoveryrules.fill_default()
        self.discoveryruns.fill_default()

        # first we create missing sat, so no other sat will
        # be created after this point
        self.fill_default_satellites()
        # now we have all elements, we can create a default
        # realm if need and it will be tagged to sat that do
        # not have an realm
        self.fill_default_realm()
        self.realms.fill_default() # also put default inside the realms themselves
        self.reactionners.fill_default()
        self.pollers.fill_default()
        self.brokers.fill_default()
        self.receivers.fill_default()
        self.schedulers.fill_default()

        # The arbiters are already done.
        # self.arbiters.fill_default()

        # Now fill some fields we can predict (like address for hosts)
        self.fill_predictive_missing_parameters()

    # Here is a special functions to fill some special
    # properties that are not filled and should be like
    # address for host (if not set, put host_name)
    def fill_predictive_missing_parameters(self):
        self.hosts.fill_predictive_missing_parameters()

    # Will check if a realm is defined, if not
    # Create a new one (default) and tag everyone that do not have
    # a realm prop to be put in this realm
    def fill_default_realm(self):
        if len(self.realms) == 0:
            # Create a default realm with default value =1
            # so all hosts without realm will be link with it
            default = Realm({'realm_name': 'Default', 'default': '1'})
            self.realms = Realms([default])
            logger.warning("No realms defined, I add one at %s" % default.get_name())
            lists = [self.pollers, self.brokers, self.reactionners, self.receivers, self.schedulers]
            for l in lists:
                for elt in l:
                    if not hasattr(elt, 'realm'):
                        elt.realm = 'Default'
                        logger.info("Tagging %s with realm %s" % (elt.get_name(), default.get_name()))

    # If a satellite is missing, we add them in the localhost
    # with defaults values
    def fill_default_satellites(self):
        if len(self.schedulers) == 0:
            logger.warning("No scheduler defined, I add one at localhost:7768")
            s = SchedulerLink({'scheduler_name': 'Default-Scheduler',
                               'address': 'localhost', 'port': '7768'})
            self.schedulers = SchedulerLinks([s])
        if len(self.pollers) == 0:
            logger.warning("No poller defined, I add one at localhost:7771")
            p = PollerLink({'poller_name': 'Default-Poller',
                            'address': 'localhost', 'port': '7771'})
            self.pollers = PollerLinks([p])
        if len(self.reactionners) == 0:
            logger.warning("No reactionner defined, I add one at localhost:7769")
            r = ReactionnerLink({'reactionner_name': 'Default-Reactionner',
                                 'address': 'localhost', 'port': '7769'})
            self.reactionners = ReactionnerLinks([r])
        if len(self.brokers) == 0:
            logger.warning("No broker defined, I add one at localhost:7772")
            b = BrokerLink({'broker_name': 'Default-Broker',
                            'address': 'localhost', 'port': '7772',
                            'manage_arbiters': '1'})
            self.brokers = BrokerLinks([b])

    # Return if one broker got a module of type: mod_type
    def got_broker_module_type_defined(self, mod_type):
        for b in self.brokers:
            for m in b.modules:
                if hasattr(m, 'module_type') and m.module_type == mod_type:
                    return True
        return False

    # return if one scheduler got a module of type: mod_type
    def got_scheduler_module_type_defined(self, mod_type):
        for b in self.schedulers:
            for m in b.modules:
                if hasattr(m, 'module_type') and m.module_type == mod_type:
                    return True
        return False

    # return if one arbiter got a module of type: mod_type
    # but this time it's tricky: the python pass is not done!
    # so look with strings!
    def got_arbiter_module_type_defined(self, mod_type):
        for a in self.arbiters:
            # Do like the linkify will do after....
            for m in getattr(a, 'modules', '').split(','):
                # So look at what the arbiter try to call as module
                m = m.strip()
                # Ok, now look in modules...
                for mod in self.modules:
                    # try to see if this module is the good type
                    if getattr(mod, 'module_type', '').strip() == mod_type.strip():
                        # if so, the good name?
                        if getattr(mod, 'module_name', '').strip() == m:
                            return True
        return False

    # Will ask for each host/service if the
    # check_command is a bp rule. If so, it will create
    # a tree structures with the rules
    def create_business_rules(self):
        self.hosts.create_business_rules(self.hosts, self.services)
        self.services.create_business_rules(self.hosts, self.services)

    # Will fill dep list for business rules
    def create_business_rules_dependencies(self):
        self.hosts.create_business_rules_dependencies()
        self.services.create_business_rules_dependencies()

    # It's used to hack some old Nagios parameters like
    # log_file or status_file: if they are present in
    # the global configuration and there is no such modules
    # in a Broker, we create it on the fly for all Brokers
    def hack_old_nagios_parameters(self):
        """ Create some 'modules' from all nagios parameters if they are set and
        the modules are not created """
        # We list all modules we will add to brokers
        mod_to_add = []
        mod_to_add_to_schedulers = []


        # For status_dat
        if hasattr(self, 'status_file') and self.status_file != '' and hasattr(self, 'object_cache_file'):
            # Ok, the user put such a value, we must look
            # if he forget to put a module for Brokers
            got_status_dat_module = self.got_broker_module_type_defined('status_dat')

            # We need to create the module on the fly?
            if not got_status_dat_module:
                data = {'object_cache_file': self.object_cache_file,
                        'status_file': self.status_file,
                        'module_name': 'Status-Dat-Autogenerated',
                         'module_type': 'status_dat'}
                mod = Module(data)
                mod.status_update_interval = getattr(self, 'status_update_interval', 15)
                mod_to_add.append(mod)

        # Now the log_file
        if hasattr(self, 'log_file') and self.log_file != '':
            # Ok, the user put such a value, we must look
            # if he forget to put a module for Brokers
            got_simple_log_module = self.got_broker_module_type_defined('simple_log')

            # We need to create the module on the fly?
            if not got_simple_log_module:
                data = {'module_type': 'simple_log', 'path': self.log_file,
                        'archive_path': self.log_archive_path,
                        'module_name': 'Simple-log-Autogenerated'}
                mod = Module(data)
                mod_to_add.append(mod)

        # Now the syslog facility
        if self.use_syslog:
            # Ok, the user want a syslog logging, why not after all
            got_syslog_module = self.got_broker_module_type_defined('syslog')

            # We need to create the module on the fly?
            if not got_syslog_module:
                data = {'module_type': 'syslog',
                        'module_name': 'Syslog-Autogenerated'}
                mod = Module(data)
                mod_to_add.append(mod)

        # Now the service_perfdata module
        if self.service_perfdata_file != '':
            # Ok, we've got a path for a service perfdata file
            got_service_perfdata_module = self.got_broker_module_type_defined('service_perfdata')

            # We need to create the module on the fly?
            if not got_service_perfdata_module:
                data = {'module_type': 'service_perfdata',
                        'module_name': 'Service-Perfdata-Autogenerated',
                        'path': self.service_perfdata_file,
                        'mode': self.service_perfdata_file_mode,
                        'template': self.service_perfdata_file_template}
                mod = Module(data)
                mod_to_add.append(mod)

        # Now the old retention file module
        if self.state_retention_file != '' and self.retention_update_interval != 0:
            # Ok, we've got a old retention file
            got_retention_file_module = self.got_scheduler_module_type_defined('nagios_retention_file')

            # We need to create the module on the fly?
            if not got_retention_file_module:
                data = {'module_type': 'nagios_retention_file',
                        'module_name': 'Nagios-Retention-File-Autogenerated',
                        'path': self.state_retention_file}
                mod = Module(data)
                mod_to_add_to_schedulers.append(mod)

        # Now the host_perfdata module
        if self.host_perfdata_file != '':
            # Ok, we've got a path for a host perfdata file
            got_host_perfdata_module = self.got_broker_module_type_defined('host_perfdata')

            # We need to create the module on the fly?
            if not got_host_perfdata_module:
                data = {'module_type': 'host_perfdata',
                        'module_name': 'Host-Perfdata-Autogenerated',
                        'path': self.host_perfdata_file, 'mode': self.host_perfdata_file_mode,
                        'template': self.host_perfdata_file_template}
                mod = Module(data)
                mod_to_add.append(mod)


        # We add them to the brokers if we need it
        if mod_to_add != []:
            logger.warning("I autogenerated some Broker modules, please look at your configuration")
            for m in mod_to_add:
                logger.warning("The module %s is autogenerated" % m.module_name)
                for b in self.brokers:
                    b.modules.append(m)

        # Then for schedulers
        if mod_to_add_to_schedulers != []:
            logger.warning("I autogenerated some Scheduler modules, please look at your configuration")
            for m in mod_to_add_to_schedulers:
                logger.warning("The module %s is autogenerated" % m.module_name)
                for b in self.schedulers:
                    b.modules.append(m)

    # It's used to hack some old Nagios parameters like
    # but for the arbiter, so very early in the run
    def hack_old_nagios_parameters_for_arbiter(self):
        """ Create some 'modules' from all nagios parameters if they are set and
        the modules are not created """
        # We list all modules we will add to arbiters
        mod_to_add = []

        # For command_file
        if getattr(self, 'command_file', '') != '':
            # Ok, the user put such a value, we must look
            # if he forget to put a module for arbiters
            got_named_pipe_module = self.got_arbiter_module_type_defined('named_pipe')

            # We need to create the module on the fly?
            if not got_named_pipe_module:
                data = {'command_file': self.command_file,
                        'module_name': 'NamedPipe-Autogenerated',
                        'module_type': 'named_pipe'}
                mod = Module(data)
                mod_to_add.append((mod, data))

        # We add them to the brokers if we need it
        if mod_to_add != []:
            logger.warning("I autogenerated some Arbiter modules, please look at your configuration")
            for (mod, data) in mod_to_add:
                logger.warning("Module %s was autogenerated" % data['module_name'])
                for a in self.arbiters:
                    a.modules = ','.join([getattr(a, 'modules', ''), data['module_name']])
                self.modules.items[mod.id] = mod

    # Set our timezone value and give it too to unset satellites
    def propagate_timezone_option(self):
        if self.use_timezone != '':
            # first apply myself
            os.environ['TZ'] = self.use_timezone
            time.tzset()

            tab = [self.schedulers, self.pollers, self.brokers, self.receivers, self.reactionners]
            for t in tab:
                for s in t:
                    if s.use_timezone == 'NOTSET':
                        setattr(s, 'use_timezone', self.use_timezone)

    # Link templates with elements
    def linkify_templates(self):
        """ Like for normal object, we link templates with each others """
        self.hosts.linkify_templates()
        self.contacts.linkify_templates()
        self.services.linkify_templates()
        self.servicedependencies.linkify_templates()
        self.hostdependencies.linkify_templates()
        self.timeperiods.linkify_templates()
        self.hostsextinfo.linkify_templates()
        self.servicesextinfo.linkify_templates()
        self.escalations.linkify_templates()
        # But also old srv and host escalations
        self.serviceescalations.linkify_templates()
        self.hostescalations.linkify_templates()

    # Reversed list is a dist with name for quick search by name
    def create_reversed_list(self):
        """ Create quick search lists for objects """
        self.hosts.create_reversed_list()
        self.hostgroups.create_reversed_list()
        self.contacts.create_reversed_list()
        self.contactgroups.create_reversed_list()
        self.notificationways.create_reversed_list()
        self.checkmodulations.create_reversed_list()
        self.macromodulations.create_reversed_list()
        self.services.create_reversed_list()
        self.servicegroups.create_reversed_list()
        self.timeperiods.create_reversed_list()
        #self.modules.create_reversed_list()
        self.resultmodulations.create_reversed_list()
        self.businessimpactmodulations.create_reversed_list()
        self.escalations.create_reversed_list()
        self.discoveryrules.create_reversed_list()
        self.discoveryruns.create_reversed_list()
        self.commands.create_reversed_list()
        self.triggers.create_reversed_list()

        # For services it's a special case
        # we search for hosts, then for services
        # it's quicker than search in all services
        self.services.optimize_service_search(self.hosts)

    # Some parameters are just not managed like O*HP commands
    # and regexp capabilities
    # True: OK
    # False: error in conf
    def check_error_on_hard_unmanaged_parameters(self):
        r = True
        if self.use_regexp_matching:
            logger.error("use_regexp_matching parameter is not managed.")
            r &= False
        #if self.ochp_command != '':
        #    logger.error("ochp_command parameter is not managed.")
        #    r &= False
        #if self.ocsp_command != '':
        #    logger.error("ocsp_command parameter is not managed.")
        #    r &= False
        return r

    # check if elements are correct or not (fill with defaults, etc)
    # Warning: this function call be called from a Arbiter AND
    # from and scheduler. The first one got everything, the second
    # does not have the satellites.
    def is_correct(self):
        """ Check if all elements got a good configuration """
        logger.info('Running pre-flight check on configuration data...')
        r = self.conf_is_correct

        # Globally unmanaged parameters
        if self.read_config_silent == 0:
            logger.info('Checking global parameters...')
        if not self.check_error_on_hard_unmanaged_parameters():
            r = False
            logger.error("Check global parameters failed")

        for x in ('hosts', 'hostgroups', 'contacts', 'contactgroups', 'notificationways',
                  'escalations', 'services', 'servicegroups', 'timeperiods', 'commands',
                  'hostsextinfo', 'servicesextinfo', 'checkmodulations', 'macromodulations'):
            if self.read_config_silent == 0:
                logger.info('Checking %s...' % (x))

            cur = getattr(self, x)
            if not cur.is_correct():
                r = False
                logger.error("\t%s conf incorrect!!" % (x))
            if self.read_config_silent == 0:
                logger.info('\tChecked %d %s' % (len(cur), x))

        # Hosts got a special check for loops
        if not self.hosts.no_loop_in_parents():
            r = False
            logger.error("Hosts: detected loop in parents ; conf incorrect")

        for x in ('servicedependencies', 'hostdependencies', 'arbiters', 'schedulers',
                   'reactionners', 'pollers', 'brokers', 'receivers', 'resultmodulations',
                   'discoveryrules', 'discoveryruns', 'businessimpactmodulations'):
            try:
                cur = getattr(self, x)
            except:
                continue
            if self.read_config_silent == 0:
                logger.info('Checking %s...' % (x))
            if not cur.is_correct():
                r = False
                logger.error("\t%s conf incorrect!!" % (x))
            if self.read_config_silent == 0:
                logger.info('\tChecked %d %s' % (len(cur), x))

        # Look that all scheduler got a broker that will take brok.
        # If there are no, raise an Error
        for s in self.schedulers:
            rea = s.realm
            if rea:
                if len(rea.potential_brokers) == 0:
                    logger.error("The scheduler %s got no broker in its realm or upper" % s.get_name())
                    self.add_error("Error: the scheduler %s got no broker in its realm or upper" % s.get_name())
                    r = False

        # Check that for each poller_tag of a host, a poller exists with this tag
        # TODO: need to check that poller are in the good realm too
        hosts_tag = set()
        services_tag = set()
        pollers_tag = set()
        for h in self.hosts:
            hosts_tag.add(h.poller_tag)
        for s in self.services:
            services_tag.add(s.poller_tag)
        for p in self.pollers:
            for t in p.poller_tags:
                pollers_tag.add(t)
        if not hosts_tag.issubset(pollers_tag):
            for tag in hosts_tag.difference(pollers_tag):
                logger.error("Hosts exist with poller_tag %s but no poller got this tag" % tag)
                self.add_error("Error: hosts exist with poller_tag %s but no poller got this tag" % tag)
                r = False
        if not services_tag.issubset(pollers_tag):
            for tag in services_tag.difference(pollers_tag):
                logger.error("Services exist with poller_tag %s but no poller got this tag" % tag)
                self.add_error("Error: services exist with poller_tag %s but no poller got this tag" % tag)
                r = False


        # Check that all hosts involved in business_rules are from the same realm
        for l in [self.services, self.hosts]:
            for e in l:
                if e.got_business_rule:
                    e_ro = e.get_realm()
                    # Something was wrong in the conf, will be raised elsewhere
                    if not e_ro:
                        continue
                    e_r = e_ro.realm_name
                    for elt in e.business_rule.list_all_elements():
                        r_o = elt.get_realm()
                        # Something was wrong in the conf, will be raised elsewhere
                        if not r_o:
                            continue
                        elt_r = elt.get_realm().realm_name
                        if not elt_r == e_r:
                            logger.error("Business_rule '%s' got hosts from another realm: %s" % (e.get_full_name(), elt_r))
                            self.add_error("Error: Business_rule '%s' got hosts from another realm: %s" % (e.get_full_name(), elt_r))
                            r = False

        self.conf_is_correct = r


    # We've got strings (like 1) but we want python elements, like True
    def pythonize(self):
        # call item pythonize for parameters
        super(Config, self).pythonize()
        self.hosts.pythonize()
        self.hostgroups.pythonize()
        self.hostdependencies.pythonize()
        self.contactgroups.pythonize()
        self.contacts.pythonize()
        self.notificationways.pythonize()
        self.checkmodulations.pythonize()
        self.macromodulations.pythonize()
        self.servicegroups.pythonize()
        self.services.pythonize()
        self.servicedependencies.pythonize()
        self.resultmodulations.pythonize()
        self.businessimpactmodulations.pythonize()
        self.escalations.pythonize()
        self.discoveryrules.pythonize()
        self.discoveryruns.pythonize()
        # The arbiters are already done
        # self.arbiters.pythonize()
        self.schedulers.pythonize()
        self.realms.pythonize()
        self.reactionners.pythonize()
        self.pollers.pythonize()
        self.brokers.pythonize()
        self.receivers.pythonize()

    # Explode parameters like cached_service_check_horizon in the
    # Service class in a cached_check_horizon manner, o*hp commands
    # , etc
    def explode_global_conf(self):
        clss = [Service, Host, Contact, SchedulerLink,
                PollerLink, ReactionnerLink, BrokerLink,
                ReceiverLink, ArbiterLink, HostExtInfo]
        for cls in clss:
            cls.load_global_conf(self)

    # Clean useless elements like templates because they are not needed anymore
    def remove_templates(self):
        self.hosts.remove_templates()
        self.contacts.remove_templates()
        self.services.remove_templates()
        self.servicedependencies.remove_templates()
        self.hostdependencies.remove_templates()
        self.timeperiods.remove_templates()
        self.discoveryrules.remove_templates()
        self.discoveryruns.remove_templates()

    # We will compute simple element md5hash, so we can know
    # if they changed or not between the restart
    def compute_hash(self):
        self.hosts.compute_hash()
        self.contacts.pythonize()
        self.notificationways.pythonize()
        self.checkmodulations.pythonize()
        self.services.pythonize()
        self.resultmodulations.pythonize()
        self.businessimpactmodulations.pythonize()
        self.escalations.pythonize()
        self.discoveryrules.pythonize()
        self.discoveryruns.pythonize()

    # Add an error in the configuration error list so we can print them
    # all in one place
    def add_error(self, txt):
        err = txt
        self.configuration_errors.append(err)

        self.conf_is_correct = False

    # Now it's time to show all configuration errors
    def show_errors(self):
        for err in self.configuration_errors:
            logger.error(err)

    # Create packs of hosts and services so in a pack,
    # all dependencies are resolved
    # It create a graph. All hosts are connected to their
    # parents, and hosts without parent are connected to host 'root'.
    # services are link to the host. Dependencies are managed
    # REF: doc/pack-creation.png
    def create_packs(self, nb_packs):
        # We create a graph with host in nodes
        g = Graph()
        g.add_nodes(self.hosts)

        # links will be used for relations between hosts
        links = set()

        # Now the relations
        for h in self.hosts:
            # Add parent relations
            for p in h.parents:
                if p is not None:
                    links.add((p, h))
            # Add the others dependencies
            for (dep, tmp, tmp2, tmp3, tmp4) in h.act_depend_of:
                links.add((dep, h))
            for (dep, tmp, tmp2, tmp3, tmp4) in h.chk_depend_of:
                links.add((dep, h))

        # For services: they are link with their own host but we need
        # To have the hosts of service dep in the same pack too
        for s in self.services:
            for (dep, tmp, tmp2, tmp3, tmp4) in s.act_depend_of:
                # I don't care about dep host: they are just the host
                # of the service...
                if hasattr(dep, 'host'):
                    links.add((dep.host, s.host))
            # The other type of dep
            for (dep, tmp, tmp2, tmp3, tmp4) in s.chk_depend_of:
                links.add((dep.host, s.host))

        # For host/service that are business based, we need to
        # link them too
        for s in [s for s in self.services if s.got_business_rule]:
            for e in s.business_rule.list_all_elements():
                if hasattr(e, 'host'):  # if it's a service
                    if e.host != s.host:  # do not an host with itself
                        links.add((e.host, s.host))
                else:  # it's already a host
                    if e != s.host:
                        links.add((e, s.host))

        # Same for hosts of course
        for h in [h for h in self.hosts if h.got_business_rule]:
            for e in h.business_rule.list_all_elements():
                if hasattr(e, 'host'):  # if it's a service
                    if e.host != h:
                        links.add((e.host, h))
                else:  # e is a host
                    if e != h:
                        links.add((e, h))


        # Now we create links in the graph. With links (set)
        # We are sure to call the less add_edge
        for (dep, h) in links:
            g.add_edge(dep, h)
            g.add_edge(h, dep)

        # Access_list from a node il all nodes that are connected
        # with it: it's a list of ours mini_packs
        tmp_packs = g.get_accessibility_packs()

        # Now We find the default realm (must be unique or
        # BAD THINGS MAY HAPPEN )
        default_realm = None
        for r in self.realms:
            if hasattr(r, 'default') and r.default:
                default_realm = r

        # Now we look if all elements of all packs have the
        # same realm. If not, not good!
        for pack in tmp_packs:
            tmp_realms = set()
            for elt in pack:
                if elt.realm is not None:
                    tmp_realms.add(elt.realm)
            if len(tmp_realms) > 1:
                self.add_error("Error: the realm configuration of yours hosts is not good because there a more than one realm in one pack (host relations):")
                for h in pack:
                    if h.realm is None:
                        err = '   the host %s do not have a realm' % h.get_name()
                        self.add_error(err)
                    else:
                        err = '   the host %s is in the realm %s' % (h.get_name(), h.realm.get_name())
                        self.add_error(err)
            if len(tmp_realms) == 1:  # Ok, good
                r = tmp_realms.pop()  # There is just one element
                r.packs.append(pack)
            elif len(tmp_realms) == 0:  # Hum.. no realm value? So default Realm
                if default_realm is not None:
                    default_realm.packs.append(pack)
                else:
                    err = "Error: some hosts do not have a realm and you do not defined a default realm!"
                    self.add_error(err)
                    for h in pack:
                        err = '    Impacted host: %s ' % h.get_name()
                        self.add_error(err)

        # The load balancing is for a loop, so all
        # hosts of a realm (in a pack) will be dispatch
        # in the schedulers of this realm
        # REF: doc/pack-agregation.png

        # Count the numbers of elements in all the realms, to compare it the total number of hosts
        nb_elements_all_realms = 0
        for r in self.realms:
            #print "Load balancing realm", r.get_name()
            packs = {}
            # create roundrobin iterator for id of cfg
            # So dispatching is loadbalanced in a realm
            # but add a entry in the roundrobin tourniquet for
            # every weight point schedulers (so Weight round robin)
            weight_list = []
            no_spare_schedulers = [s for s in r.schedulers if not s.spare]
            nb_schedulers = len(no_spare_schedulers)

            # Maybe there is no scheduler in the realm, it's can be a
            # big problem if there are elements in packs
            nb_elements = 0
            for pack in r.packs:
                nb_elements += len(pack)
                nb_elements_all_realms += len(pack)
            logger.info("Number of hosts in the realm %s: %d "
                                "(distributed in %d linked packs)"
                                % (r.get_name(), nb_elements, len(r.packs)))

            if nb_schedulers == 0 and nb_elements != 0:
                err = "The realm %s has hosts but no scheduler!" % r.get_name()
                self.add_error(err)
                r.packs = []  # Dumb pack
                continue

            packindex = 0
            packindices = {}
            for s in no_spare_schedulers:
                packindices[s.id] = packindex
                packindex += 1
                for i in xrange(0, s.weight):
                    weight_list.append(s.id)

            rr = itertools.cycle(weight_list)

            # We must have nb_schedulers packs
            for i in xrange(0, nb_schedulers):
                packs[i] = []

            # Try to load the history association dict so we will try to
            # send the hosts in the same "pack"
            assoc = {}
            if os.path.exists(self.pack_distribution_file):
                logger.debug('Trying to open the distribution file %s'
                                    % self.pack_distribution_file)
                try:
                    f = open(self.pack_distribution_file, 'rb')
                    assoc = cPickle.load(f)
                    f.close()
                except Exception, exp:
                    logger.warning('Warning: cannot open the distribution file %s: %s' % (self.pack_distribution_file, str(exp)))


            # Now we explode the numerous packs into nb_packs reals packs:
            # we 'load balance' them in a roundrobin way
            for pack in r.packs:
                valid_value = False
                old_pack = -1
                for elt in pack:
                    #print 'Look for host', elt.get_name(), 'in assoc'
                    old_i = assoc.get(elt.get_name(), -1)
                    #print 'Founded in ASSOC: ', elt.get_name(),old_i
                    # Maybe it's a new, if so, don't count it
                    if old_i == -1:
                        continue
                    # Maybe it is the first we look at, if so, take it's value
                    if old_pack == -1 and old_i != -1:
                        #print 'First value set', elt.get_name(), old_i
                        old_pack = old_i
                        valid_value = True
                        continue
                    if old_i == old_pack:
                        #print 'I found a match between elements', old_i
                        valid_value = True
                    if old_i != old_pack:
                        #print 'Outch found a change sorry', old_i, old_pack
                        valid_value = False
                #print 'Is valid?', elt.get_name(), valid_value, old_pack
                i = None
                # If it's a valid sub pack and the pack id really exist, use it!
                if valid_value and old_pack in packindices:
                    #print 'Use a old id for pack', old_pack, [h.get_name() for h in pack]
                    i = old_pack
                else:  # take a new one
                    #print 'take a new id for pack', [h.get_name() for h in pack]
                    i = rr.next()

                for elt in pack:
                    #print 'We got the element', elt.get_full_name(), ' in pack', i, packindices
                    packs[packindices[i]].append(elt)
                    assoc[elt.get_name()] = i

            try:
                logger.info('Saving the distribution file %s' % self.pack_distribution_file)
                f = open(self.pack_distribution_file, 'wb')
                cPickle.dump(assoc, f)
                f.close()
            except Exception, exp:
                logger.warning('Could not save the distribution file %s: %s' % (self.pack_distribution_file, str(exp)))

            # Now in packs we have the number of packs [h1, h2, etc]
            # equal to the number of schedulers.
            r.packs = packs
        logger.info("Total number of hosts : %d"
                            % nb_elements_all_realms)
        if len(self.hosts) != nb_elements_all_realms:
            logger.warning("There are %d hosts defined, and %d hosts "
                                "dispatched in the realms. Some hosts have "
                                "been ignored"
                                % (len(self.hosts), nb_elements_all_realms))
            self.add_error("There are %d hosts defined, and %d hosts "
                           "dispatched in the realms. Some hosts have "
                           "been ignored"
                           % (len(self.hosts), nb_elements_all_realms))

    # Use the self.conf and make nb_parts new confs.
    # nbparts is equal to the number of schedulerlink
    # New confs are independent with checks. The only communication
    # That can be need is macro in commands
    def cut_into_parts(self):
        #print "Scheduler configured:", self.schedulers
        # I do not care about alive or not. User must have set a spare if need it
        nb_parts = len([s for s in self.schedulers if not s.spare])

        if nb_parts == 0:
            nb_parts = 1

        # We create dummy configurations for schedulers:
        # they are clone of the master
        # conf but without hosts and services (because they are dispatched between
        # theses configurations)
        self.confs = {}
        for i in xrange(0, nb_parts):
            #print "Create Conf:", i, '/', nb_parts -1
            cur_conf = self.confs[i] = Config()

            # Now we copy all properties of conf into the new ones
            for prop, entry in Config.properties.items():
                if entry.managed and not isinstance(entry, UnusedProp):
                    val = getattr(self, prop)
                    setattr(cur_conf, prop, val)
                    #print "Copy", prop, val

            # we need a deepcopy because each conf
            # will have new hostgroups
            cur_conf.id = i
            cur_conf.commands = self.commands
            cur_conf.timeperiods = self.timeperiods
            # Create hostgroups with just the name and same id, but no members
            new_hostgroups = []
            for hg in self.hostgroups:
                new_hostgroups.append(hg.copy_shell())
            cur_conf.hostgroups = Hostgroups(new_hostgroups)
            cur_conf.notificationways = self.notificationways
            cur_conf.checkmodulations = self.checkmodulations
            cur_conf.macromodulations = self.macromodulations
            cur_conf.contactgroups = self.contactgroups
            cur_conf.contacts = self.contacts
            cur_conf.triggers = self.triggers
            # Create hostgroups with just the name and same id, but no members
            new_servicegroups = []
            for sg in self.servicegroups:
                new_servicegroups.append(sg.copy_shell())
            cur_conf.servicegroups = Servicegroups(new_servicegroups)
            cur_conf.hosts = []  # will be fill after
            cur_conf.services = []  # will be fill after
            # The elements of the others conf will be tag here
            cur_conf.other_elements = {}
            # if a scheduler have accepted the conf
            cur_conf.is_assigned = False

        logger.info("Creating packs for realms")

        # Just create packs. There can be numerous ones
        # In pack we've got hosts and service
        # packs are in the realms
        # REF: doc/pack-creation.png
        self.create_packs(nb_parts)

        # We've got all big packs and get elements into configurations
        # REF: doc/pack-agregation.png
        offset = 0
        for r in self.realms:
            for i in r.packs:
                pack = r.packs[i]
                for h in pack:
                    h.pack_id = i
                    self.confs[i+offset].hosts.append(h)
                    for s in h.services:
                        self.confs[i+offset].services.append(s)
                # Now the conf can be link in the realm
                r.confs[i+offset] = self.confs[i+offset]
            offset += len(r.packs)
            del r.packs

        # We've nearly have hosts and services. Now we want REALS hosts (Class)
        # And we want groups too
        # print "Finishing packs"
        for i in self.confs:
            #print "Finishing pack Nb:", i
            cfg = self.confs[i]

            # Create ours classes
            cfg.hosts = Hosts(cfg.hosts)
            cfg.hosts.create_reversed_list()
            cfg.services = Services(cfg.services)
            cfg.services.create_reversed_list()
            # Fill host groups
            for ori_hg in self.hostgroups:
                hg = cfg.hostgroups.find_by_name(ori_hg.get_name())
                mbrs = ori_hg.members
                mbrs_id = []
                for h in mbrs:
                    if h is not None:
                        mbrs_id.append(h.id)
                for h in cfg.hosts:
                    if h.id in mbrs_id:
                        hg.members.append(h)
            # Fill servicegroup
            for ori_sg in self.servicegroups:
                sg = cfg.servicegroups.find_by_name(ori_sg.get_name())
                mbrs = ori_sg.members
                mbrs_id = []
                for s in mbrs:
                    if s is not None:
                        mbrs_id.append(s.id)
                for s in cfg.services:
                    if s.id in mbrs_id:
                        sg.members.append(s)

        # Now we fill other_elements by host (service are with their host
        # so they are not tagged)
        for i in self.confs:
            for h in self.confs[i].hosts:
                for j in [j for j in self.confs if j != i]:  # So other than i
                    self.confs[i].other_elements[h.get_name()] = i

        # We tag conf with instance_id
        for i in self.confs:
            self.confs[i].instance_id = i
            random.seed(time.time())


# ...
def lazy():
    # let's compute the "USER" properties and macros..
    for n in xrange(1, 256):
        n = str(n)
        Config.properties['$USER' + str(n) + '$'] = StringProp(default='')
        Config.macros['USER' + str(n)] = '$USER' + n + '$'

lazy()
del lazy

########NEW FILE########
__FILENAME__ = contact
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items

from shinken.util import strip_and_uniq
from shinken.property import BoolProp, IntegerProp, StringProp
from shinken.log import logger, console_logger

_special_properties = ('service_notification_commands', 'host_notification_commands',
                        'service_notification_period', 'host_notification_period',
                        'service_notification_options', 'host_notification_options',
                        'host_notification_commands', 'contact_name')

_simple_way_parameters = ('service_notification_period', 'host_notification_period',
                           'service_notification_options', 'host_notification_options',
                           'service_notification_commands', 'host_notification_commands',
                           'min_business_impact')


class Contact(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'contact'

    properties = Item.properties.copy()
    properties.update({
        'contact_name':     StringProp(fill_brok=['full_status']),
        'alias':            StringProp(default='none', fill_brok=['full_status']),
        'contactgroups':    StringProp(default='', fill_brok=['full_status']),
        'host_notifications_enabled': BoolProp(default='1', fill_brok=['full_status']),
        'service_notifications_enabled': BoolProp(default='1', fill_brok=['full_status']),
        'host_notification_period': StringProp(fill_brok=['full_status']),
        'service_notification_period': StringProp(fill_brok=['full_status']),
        'host_notification_options': StringProp(fill_brok=['full_status']),
        'service_notification_options': StringProp(fill_brok=['full_status']),
        'host_notification_commands': StringProp(fill_brok=['full_status']),
        'service_notification_commands': StringProp(fill_brok=['full_status']),
        'min_business_impact':    IntegerProp(default='0', fill_brok=['full_status']),
        'email':            StringProp(default='none', fill_brok=['full_status']),
        'pager':            StringProp(default='none', fill_brok=['full_status']),
        'address1':         StringProp(default='none', fill_brok=['full_status']),
        'address2':         StringProp(default='none', fill_brok=['full_status']),
        'address3':        StringProp(default='none', fill_brok=['full_status']),
        'address4':         StringProp(default='none', fill_brok=['full_status']),
        'address5':         StringProp(default='none', fill_brok=['full_status']),
        'address6':         StringProp(default='none', fill_brok=['full_status']),
        'can_submit_commands': BoolProp(default='0', fill_brok=['full_status']),
        'is_admin':         BoolProp(default='0', fill_brok=['full_status']),
        'retain_status_information': BoolProp(default='1', fill_brok=['full_status']),
        'notificationways': StringProp(default='', fill_brok=['full_status']),
        'password':        StringProp(default='NOPASSWORDSET', fill_brok=['full_status']),
    })

    running_properties = Item.running_properties.copy()
    running_properties.update({
        'modified_attributes': IntegerProp(default=0L, fill_brok=['full_status'], retention=True),
        'downtimes': StringProp(default=[], fill_brok=['full_status'], retention=True),
    })

    # This tab is used to transform old parameters name into new ones
    # so from Nagios2 format, to Nagios3 ones.
    # Or Shinken deprecated names like criticity
    old_properties = {
        'min_criticity': 'min_business_impact',
    }

    macros = {
        'CONTACTNAME': 'contact_name',
        'CONTACTALIAS': 'alias',
        'CONTACTEMAIL': 'email',
        'CONTACTPAGER': 'pager',
        'CONTACTADDRESS1': 'address1',
        'CONTACTADDRESS2': 'address2',
        'CONTACTADDRESS3': 'address3',
        'CONTACTADDRESS4': 'address4',
        'CONTACTADDRESS5': 'address5',
        'CONTACTADDRESS6': 'address6',
        'CONTACTGROUPNAME': 'get_groupname',
        'CONTACTGROUPNAMES': 'get_groupnames'
    }

    # For debugging purpose only (nice name)
    def get_name(self):
        try:
            return self.contact_name
        except AttributeError:
            return 'UnnamedContact'

    # Search for notification_options with state and if t is
    # in service_notification_period
    def want_service_notification(self, t, state, type, business_impact, cmd=None):
        if not self.service_notifications_enabled:
            return False

        # If we are in downtime, we do nto want notification
        for dt in self.downtimes:
            if dt.is_in_effect:
                return False

        # Now the rest is for sub notificationways. If one is OK, we are ok
        # We will filter in another phase
        for nw in self.notificationways:
            nw_b = nw.want_service_notification(t, state, type, business_impact, cmd)
            if nw_b:
                return True

        # Oh... no one is ok for it? so no, sorry
        return False

    # Search for notification_options with state and if t is in
    # host_notification_period
    def want_host_notification(self, t, state, type, business_impact, cmd=None):
        if not self.host_notifications_enabled:
            return False

        # If we are in downtime, we do nto want notification
        for dt in self.downtimes:
            if dt.is_in_effect:
                return False

        # Now it's all for sub notificationways. If one is OK, we are OK
        # We will filter in another phase
        for nw in self.notificationways:
            nw_b = nw.want_host_notification(t, state, type, business_impact, cmd)
            if nw_b:
                return True

        # Oh, nobody..so NO :)
        return False

    # Call to get our commands to launch a Notification
    def get_notification_commands(self, type):
        r = []
        # service_notification_commands for service
        notif_commands_prop = type + '_notification_commands'
        for nw in self.notificationways:
            r.extend(getattr(nw, notif_commands_prop))
        return r

    # Check is required prop are set:
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        # All of the above are checks in the notificationways part
        for prop, entry in cls.properties.items():
            if prop not in _special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.error("[contact::%s] %s property not set" % (self.get_name(), prop))
                    state = False  # Bad boy...

        # There is a case where there is no nw: when there is not special_prop defined
        # at all!!
        if self.notificationways == []:
            for p in _special_properties:
                if not hasattr(self, p):
                    logger.error("[contact::%s] %s property is missing" % (self.get_name(), p))
                    state = False

        if hasattr(self, 'contact_name'):
            for c in cls.illegal_object_name_chars:
                if c in self.contact_name:
                    logger.error("[contact::%s] %s character not allowed in contact_name" % (self.get_name(), c))
                    state = False
        else:
            if hasattr(self, 'alias'):  # take the alias if we miss the contact_name
                self.contact_name = self.alias

        return state

    # Raise a log entry when a downtime begins
    # CONTACT DOWNTIME ALERT: test_contact;STARTED; Contact has entered a period of scheduled downtime
    def raise_enter_downtime_log_entry(self):
        console_logger.info("CONTACT DOWNTIME ALERT: %s;STARTED; Contact has "
                            "entered a period of scheduled downtime"
                            % self.get_name())

    # Raise a log entry when a downtime has finished
    # CONTACT DOWNTIME ALERT: test_contact;STOPPED; Contact has exited from a period of scheduled downtime
    def raise_exit_downtime_log_entry(self):
        console_logger.info("CONTACT DOWNTIME ALERT: %s;STOPPED; Contact has "
                            "exited from a period of scheduled downtime"
                            % self.get_name())

    # Raise a log entry when a downtime prematurely ends
    # CONTACT DOWNTIME ALERT: test_contact;CANCELLED; Contact has entered a period of scheduled downtime
    def raise_cancel_downtime_log_entry(self):
        console_logger.info("CONTACT DOWNTIME ALERT: %s;CANCELLED; Scheduled "
                            "downtime for contact has been cancelled."
                            % self.get_name())


class Contacts(Items):
    name_property = "contact_name"
    inner_class = Contact

    def linkify(self, timeperiods, commands, notificationways):
        #self.linkify_with_timeperiods(timeperiods, 'service_notification_period')
        #self.linkify_with_timeperiods(timeperiods, 'host_notification_period')
        #self.linkify_command_list_with_commands(commands, 'service_notification_commands')
        #self.linkify_command_list_with_commands(commands, 'host_notification_commands')
        self.linkify_with_notificationways(notificationways)

    # We've got a notificationways property with , separated contacts names
    # and we want have a list of NotificationWay
    def linkify_with_notificationways(self, notificationways):
        for i in self:
            if not hasattr(i, 'notificationways'):
                continue
            new_notificationways = []
            for nw_name in strip_and_uniq(i.notificationways.split(',')):
                nw = notificationways.find_by_name(nw_name)
                if nw is not None:
                    new_notificationways.append(nw)
                else:
                    err = "The 'notificationways' of the %s '%s' named '%s' is unknown!" % (i.__class__.my_type, i.get_name(), nw_name)
                    i.configuration_errors.append(err)
            # Get the list, but first make elements uniq
            i.notificationways = list(set(new_notificationways))


    def late_linkify_c_by_commands(self, commands):
        for i in self:
            for nw in i.notificationways:
                nw.late_linkify_nw_by_commands(commands)


    # We look for contacts property in contacts and
    def explode(self, contactgroups, notificationways):
        # Contactgroups property need to be fullfill for got the informations
        self.apply_partial_inheritance('contactgroups')
        # _special properties maybe came from a template, so
        # import them before grok ourselves
        for prop in _special_properties:
            if prop == 'contact_name':
                continue
            self.apply_partial_inheritance(prop)

        # Register ourself into the contactsgroups we are in
        for c in self:
            if c.is_tpl() or not (hasattr(c, 'contact_name') and hasattr(c, 'contactgroups')):
                continue
            for cg in c.contactgroups.split(','):
                contactgroups.add_member(c.contact_name, cg.strip())

        # Now create a notification way with the simple parameter of the
        # contacts
        for c in self:
            if not c.is_tpl():
                need_notificationway = False
                params = {}
                for p in _simple_way_parameters:
                    if hasattr(c, p):
                        need_notificationway = True
                        params[p] = getattr(c, p)
                    else:  # put a default text value
                        # Remove the value and put a default value
                        setattr(c, p, '')


                if need_notificationway:
                    #print "Create notif way with", params
                    cname = getattr(c, 'contact_name', getattr(c, 'alias', ''))
                    nw_name = cname + '_inner_notificationway'
                    notificationways.new_inner_member(nw_name, params)

                    if not hasattr(c, 'notificationways'):
                        c.notificationways = nw_name
                    else:
                        c.notificationways = c.notificationways + ',' + nw_name

########NEW FILE########
__FILENAME__ = contactgroup
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.



# Contactgroups are groups for contacts
# They are just used for the config read and explode by elements

from itemgroup import Itemgroup, Itemgroups

from shinken.property import IntegerProp, StringProp
from shinken.log import logger


class Contactgroup(Itemgroup):
    id = 1
    my_type = 'contactgroup'

    properties = Itemgroup.properties.copy()
    properties.update({
        'id':                   IntegerProp(default=0, fill_brok=['full_status']),
        'contactgroup_name':    StringProp(fill_brok=['full_status']),
        'alias':                StringProp(fill_brok=['full_status']),
    })

    macros = {
        'CONTACTGROUPALIAS': 'alias',
        'CONTACTGROUPMEMBERS': 'get_members'
    }

    def get_contacts(self):
        return getattr(self, 'members', '')

    def get_name(self):
        return getattr(self, 'contactgroup_name', 'UNNAMED-CONTACTGROUP')

    def get_contactgroup_members(self):
        if self.has('contactgroup_members'):
            return self.contactgroup_members.split(',')
        else:
            return []

    # We fillfull properties with template ones if need
    # Because hostgroup we call may not have it's members
    # we call get_hosts_by_explosion on it
    def get_contacts_by_explosion(self, contactgroups):
        # First we tag the hg so it will not be explode
        # if a son of it already call it
        self.already_explode = True

        # Now the recursive part
        # rec_tag is set to False every CG we explode
        # so if True here, it must be a loop in HG
        # calls... not GOOD!
        if self.rec_tag:
            logger.error("[contactgroup::%s] got a loop in contactgroup definition" % self.get_name())
            if self.has('members'):
                return self.members
            else:
                return ''
        # Ok, not a loop, we tag it and continue
        self.rec_tag = True

        cg_mbrs = self.get_contactgroup_members()
        for cg_mbr in cg_mbrs:
            cg = contactgroups.find_by_name(cg_mbr.strip())
            if cg is not None:
                value = cg.get_contacts_by_explosion(contactgroups)
                if value is not None:
                    self.add_string_member(value)
        if self.has('members'):
            return self.members
        else:
            return ''


class Contactgroups(Itemgroups):
    name_property = "contactgroup_name"  # is used for finding contactgroup
    inner_class = Contactgroup

    def get_members_by_name(self, cgname):
        cg = self.find_by_name(cgname)
        if cg is None:
            return []
        return cg.get_contacts()

    def add_contactgroup(self, cg):
        self.items[cg.id] = cg

    def linkify(self, contacts):
        self.linkify_cg_by_cont(contacts)

    # We just search for each host the id of the host
    # and replace the name by the id
    def linkify_cg_by_cont(self, contacts):
        for cg in self:
            mbrs = cg.get_contacts()

            # The new member list, in id
            new_mbrs = []
            for mbr in mbrs:
                m = contacts.find_by_name(mbr)
                # Maybe the contact is missing, if so, must be put in unknown_members
                if m is not None:
                    new_mbrs.append(m)
                else:
                    cg.unknown_members.append(mbr)

            # Make members uniq
            new_mbrs = list(set(new_mbrs))

            # We find the id, we replace the names
            cg.replace_members(new_mbrs)

    # Add a contact string to a contact member
    # if the contact group do not exist, create it
    def add_member(self, cname, cgname):
        cg = self.find_by_name(cgname)
        # if the id do not exist, create the cg
        if cg is None:
            cg = Contactgroup({'contactgroup_name': cgname, 'alias': cgname, 'members': cname})
            self.add_contactgroup(cg)
        else:
            cg.add_string_member(cname)

    # Use to fill members with contactgroup_members
    def explode(self):
        # We do not want a same hg to be explode again and again
        # so we tag it
        for tmp_cg in self.items.values():
            tmp_cg.already_explode = False

        for cg in self.items.values():
            if cg.has('contactgroup_members') and not cg.already_explode:
                # get_contacts_by_explosion is a recursive
                # function, so we must tag hg so we do not loop
                for tmp_cg in self.items.values():
                    tmp_cg.rec_tag = False
                cg.get_contacts_by_explosion(self)

        # We clean the tags
        for tmp_cg in self.items.values():
            if hasattr(tmp_cg, 'rec_tag'):
                del tmp_cg.rec_tag
            del tmp_cg.already_explode

########NEW FILE########
__FILENAME__ = discoveryrule
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re
from copy import copy

from item import Item, Items
from shinken.objects.matchingitem import MatchingItem
from service import Service
from host import Host
from shinken.property import StringProp, ListProp, IntegerProp


class Discoveryrule(MatchingItem):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'discoveryrule'

    properties = Item.properties.copy()
    properties.update({
        'discoveryrule_name':    StringProp(),
        'creation_type':         StringProp(default='service'),
        'discoveryrule_order':   IntegerProp(default='0'),
        ## 'check_command':         StringProp (),
        ## 'service_description':   StringProp (),
        ## 'use':                   StringProp(),
    })

    running_properties = {
        'configuration_errors': ListProp(default=[]),
        }

    macros = {}

    # The init of a discovery will set the property of
    # Discoveryrule.properties as in setattr, but all others
    # will be in a list because we need to have all names
    # and not lost all in __dict__
    def __init__(self, params={}):
        cls = self.__class__

        # We have our own id of My Class type :)
        # use set attr for going into the slots
        # instead of __dict__ :)
        setattr(self, 'id', cls.id)
        cls.id += 1

        self.matches = {}  # for matching rules
        self.not_matches = {}  # for rules that should NOT match
        self.writing_properties = {}

        for key in params:
            # delistify attributes if there is only one value
            params[key] = self.compact_unique_attr_value(params[key])

        # Get the properties of the Class we want
        if not 'creation_type' in params:
            params['creation_type'] = 'service'

        map = {'service': Service, 'host': Host}
        t = params['creation_type']
        if not t in map:
            return
        tcls = map[t]

        # In my own property:
        #  -> in __dict__
        # In the properties of the 'creation_type' Class:
        #  -> in self.writing_properties
        # if not, in matches or not match (if key starts
        # with a !, it's a not rule)
        # -> in self.matches or self.not_matches
        # in writing properties if start with + (means 'add this')
        # in writing properties if start with - (means 'del this')
        for key in params:
            # Some key are quite special
            if key in cls.properties:
                setattr(self, key, params[key])
            elif key in ['use'] or key.startswith('+') or key.startswith('-') or key in tcls.properties or key.startswith('_'):
                self.writing_properties[key] = params[key]
            else:
                if key.startswith('!'):
                    key = key.split('!')[1]
                    self.not_matches[key] = params['!' + key]
                else:
                    self.matches[key] = params[key]

        # Then running prop :)
        cls = self.__class__
        # adding running properties like latency, dependency list, etc
        for prop, entry in cls.running_properties.items():
            # Copy is slow, so we check type
            # Type with __iter__ are list or dict, or tuple.
            # Item need it's own list, so qe copy
            val = entry.default
            if hasattr(val, '__iter__'):
                setattr(self, prop, copy(val))
            else:
                setattr(self, prop, val)

            # each instance to have his own running prop!


    # Output name
    def get_name(self):
        try:
            return self.discoveryrule_name
        except AttributeError:
            return "UnnamedDiscoveryRule"


class Discoveryrules(Items):
    name_property = "discoveryrule_name"
    inner_class = Discoveryrule

########NEW FILE########
__FILENAME__ = discoveryrun
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re
from copy import copy

from item import Item, Items

from shinken.objects.matchingitem import MatchingItem
from shinken.property import StringProp
from shinken.eventhandler import EventHandler
from shinken.macroresolver import MacroResolver


class Discoveryrun(MatchingItem):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'discoveryrun'

    properties = Item.properties.copy()
    properties.update({
        'discoveryrun_name': StringProp(),
        'discoveryrun_command': StringProp(),
    })

    running_properties = Item.running_properties.copy()
    running_properties.update({
        'current_launch': StringProp(default=None),
    })

    # The init of a discovery will set the property of
    # Discoveryrun.properties as in setattr, but all others
    # will be in a list because we need to have all names
    # and not lost all in __dict__
    def __init__(self, params={}):
        cls = self.__class__

        # We have our own id of My Class type :)
        # use set attr for going into the slots
        # instead of __dict__ :)
        setattr(self, 'id', cls.id)
        cls.id += 1

        self.matches = {}  # for matching rules
        self.not_matches = {}  # for rules that should NOT match

        # In my own property:
        #  -> in __dict__
        # if not, in matches or not match (if key starts
        # with a !, it's a not rule)
        # -> in self.matches or self.not_matches
        # in writing properties if start with + (means 'add this')
        for key in params:
            # delistify attributes if there is only one value
            params[key] = self.compact_unique_attr_value(params[key])
            if key in cls.properties:
                setattr(self, key, params[key])
            else:
                if key.startswith('!'):
                    key = key.split('!')[1]
                    self.not_matches[key] = params['!'+key]
                else:
                    self.matches[key] = params[key]

        # Then running prop :)
        cls = self.__class__
        # adding running properties like latency, dependency list, etc
        for prop, entry in cls.running_properties.items():
            # Copy is slow, so we check type
            # Type with __iter__ are list or dict, or tuple.
            # Item need it's own list, so qe copy
            val = entry.default
            if hasattr(val, '__iter__'):
                setattr(self, prop, copy(val))
            else:
                setattr(self, prop, val)

            # each instance to have his own running prop!


    # Output name
    def get_name(self):
        try:
            return self.discoveryrun_name
        except AttributeError:
            return "UnnamedDiscoveryRun"

    # A Run that is first level means that it do not have
    # any matching filter
    def is_first_level(self):
        return len(self.not_matches) + len(self.matches) == 0

    # Get an eventhandler object and launch it
    def launch(self, ctx=[], timeout=300):
        m = MacroResolver()
        cmd = m.resolve_command(self.discoveryrun_command, ctx)
        self.current_launch = EventHandler(cmd, timeout=timeout)
        self.current_launch.execute()

    def check_finished(self):
        max_output = 10 ** 9
        #print "Max output", max_output
        self.current_launch.check_finished(max_output)

    # Look if the current launch is done or not
    def is_finished(self):
        if self.current_launch == None:
            return True
        if self.current_launch.status in ('done', 'timeout'):
            return True
        return False

    # we use an EventHandler object, so we have output with a single line
    # and longoutput with the rest. We just need to return all
    def get_output(self):
        return '\n'.join([self.current_launch.output, self.current_launch.long_output])


class Discoveryruns(Items):
    name_property = "discoveryrun_name"
    inner_class = Discoveryrun

    def linkify(self, commands):
        for r in self:
            r.linkify_one_command_with_commands(commands, 'discoveryrun_command')

########NEW FILE########
__FILENAME__ = escalation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items

from shinken.util import strip_and_uniq
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.log import logger

_special_properties = ('contacts', 'contact_groups', 'first_notification_time', 'last_notification_time')
_special_properties_time_based = ('contacts', 'contact_groups', 'first_notification', 'last_notification')


class Escalation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'escalation'

    properties = Item.properties.copy()
    properties.update({
        'escalation_name':      StringProp(),
        'first_notification':   IntegerProp(),
        'last_notification':    IntegerProp(),
        'first_notification_time': IntegerProp(),
        'last_notification_time': IntegerProp(),
        # by default don't use the notification_interval defined in
        # the escalation, but the one defined by the object
        'notification_interval': IntegerProp(default='-1'),
        'escalation_period':    StringProp(default=''),
        'escalation_options':   ListProp(default='d,u,r,w,c'),
        'contacts':             StringProp(),
        'contact_groups':       StringProp(),
    })

    running_properties = Item.running_properties.copy()
    running_properties.update({
        'time_based': BoolProp(default=False),
    })

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.escalation_name

    # Return True if:
    # *time in in escalation_period or we do not have escalation_period
    # *status is in escalation_options
    # *the notification number is in our interval [[first_notification .. last_notification]]
    # if we are a classic escalation.
    # *If we are time based, we check if the time that we were in notification
    # is in our time interval
    def is_eligible(self, t, status, notif_number, in_notif_time, interval):
        small_states = {
            'WARNING': 'w',    'UNKNOWN': 'u',     'CRITICAL': 'c',
            'RECOVERY': 'r',   'FLAPPING': 'f',    'DOWNTIME': 's',
            'DOWN': 'd',       'UNREACHABLE': 'u', 'OK': 'o', 'UP': 'o'
        }

        # If we are not time based, we check notification numbers:
        if not self.time_based:
            # Begin with the easy cases
            if notif_number < self.first_notification:
                return False

            #self.last_notification = 0 mean no end
            if self.last_notification != 0 and notif_number > self.last_notification:
                return False
        # Else we are time based, we must check for the good value
        else:
            # Begin with the easy cases
            if in_notif_time < self.first_notification_time * interval:
                return False

            # self.last_notification = 0 mean no end
            if self.last_notification_time != 0 and in_notif_time > self.last_notification_time * interval:
                return False

        # If our status is not good, we bail out too
        if status in small_states and small_states[status] not in self.escalation_options:
            return False

        # Maybe the time is not in our escalation_period
        if self.escalation_period is not None and not self.escalation_period.is_time_valid(t):
            return False

        # Ok, I do not see why not escalade. So it's True :)
        return True

    # t = the reference time
    def get_next_notif_time(self, t_wished, status, creation_time, interval):
        small_states = {'WARNING': 'w', 'UNKNOWN': 'u', 'CRITICAL': 'c',
             'RECOVERY': 'r', 'FLAPPING': 'f', 'DOWNTIME': 's',
             'DOWN': 'd', 'UNREACHABLE': 'u', 'OK': 'o', 'UP': 'o'}

        # If we are not time based, we bail out!
        if not self.time_based:
            return None

        # Check if we are valid
        if status in small_states and small_states[status] not in self.escalation_options:
            return None

        # Look for the min of our future validity
        start = self.first_notification_time * interval + creation_time

        # If we are after the classic next time, we are not asking for a smaller interval
        if start > t_wished:
            return None

        # Maybe the time we found is not a valid one....
        if self.escalation_period is not None and not self.escalation_period.is_time_valid(start):
            return None

        # Ok so I ask for my start as a possibility for the next notification time
        return start

    # Check is required prop are set:
    # template are always correct
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        # If we got the _time parameters, we are time based. Unless, we are not :)
        if hasattr(self, 'first_notification_time') or hasattr(self, 'last_notification_time'):
            self.time_based = True
            special_properties = _special_properties_time_based
        else:  # classic ones
            special_properties = _special_properties

        for prop, entry in cls.properties.items():
            if prop not in special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.info('%s: I do not have %s' % (self.get_name(), prop))
                    state = False  # Bad boy...

        # Raised all previously saw errors like unknown contacts and co
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.info(err)

        # Ok now we manage special cases...
        if not hasattr(self, 'contacts') and not hasattr(self, 'contact_groups'):
            logger.info('%s: I do not have contacts nor contact_groups' % self.get_name())
            state = False

        # If time_based or not, we do not check all properties
        if self.time_based:
            if not hasattr(self, 'first_notification_time'):
                logger.info('%s: I do not have first_notification_time' % self.get_name())
                state = False
            if not hasattr(self, 'last_notification_time'):
                logger.info('%s: I do not have last_notification_time' % self.get_name())
                state = False
        else:  # we check classical properties
            if not hasattr(self, 'first_notification'):
                logger.info('%s: I do not have first_notification' % self.get_name())
                state = False
            if not hasattr(self, 'last_notification'):
                logger.info('%s: I do not have last_notification' % self.get_name())
                state = False

        return state


class Escalations(Items):
    name_property = "escalation_name"
    inner_class = Escalation

    def linkify(self, timeperiods, contacts, services, hosts):
        self.linkify_with_timeperiods(timeperiods, 'escalation_period')
        self.linkify_with_contacts(contacts)
        self.linkify_es_by_s(services)
        self.linkify_es_by_h(hosts)

    def add_escalation(self, es):
        self.items[es.id] = es

    # Will register escalations into service.escalations
    def linkify_es_by_s(self, services):
        for es in self:
            # If no host, no hope of having a service
            if not (hasattr(es, 'host_name') and hasattr(es, 'service_description')):
                continue
            es_hname, sdesc = es.host_name, es.service_description
            if '' in (es_hname.strip(), sdesc.strip()):
                continue
            for hname in strip_and_uniq(es_hname.split(',')):
                for sname in strip_and_uniq(sdesc.split(',')):
                    s = services.find_srv_by_name_and_hostname(hname, sname)
                    if s is not None:
                        #print "Linking service", s.get_name(), 'with me', es.get_name()
                        s.escalations.append(es)
                                #print "Now service", s.get_name(), 'have', s.escalations


    # Will register escalations into host.escalations
    def linkify_es_by_h(self, hosts):
        for es in self:
            # If no host, no hope of having a service
            if (not hasattr(es, 'host_name') or es.host_name.strip() == ''
                    or (hasattr(es, 'service_description') and es.service_description.strip() != '')):
                continue
            # I must be NOT a escalation on for service
            for hname in strip_and_uniq(es.host_name.split(',')):
                h = hosts.find_by_name(hname)
                if h is not None:
                    #print "Linking host", h.get_name(), 'with me', es.get_name()
                    h.escalations.append(es)
                    #print "Now host", h.get_name(), 'have', h.escalations


    # We look for contacts property in contacts and
    def explode(self, hosts, hostgroups, contactgroups):

        # items::explode_host_groups_into_hosts
        # take all hosts from our hostgroup_name into our host_name property
        self.explode_host_groups_into_hosts(hosts, hostgroups)

        # items::explode_contact_groups_into_contacts
        # take all contacts from our contact_groups into our contact property
        self.explode_contact_groups_into_contacts(contactgroups)

########NEW FILE########
__FILENAME__ = host
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This is the main class for the Host. In fact it's mainly
about the configuration part. for the running one, it's better
to look at the schedulingitem class that manage all
scheduling/consume check smart things :)
"""

import time

from item import Items
from schedulingitem import SchedulingItem

from shinken.autoslots import AutoSlots
from shinken.util import format_t_into_dhms_format, to_hostnames_list, get_obj_name, to_svc_hst_distinct_lists, to_list_string_of_names, to_list_of_names, to_name_if_possible, strip_and_uniq
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.graph import Graph
from shinken.macroresolver import MacroResolver
from shinken.eventhandler import EventHandler
from shinken.log import logger, console_logger


class Host(SchedulingItem):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    id = 1  # zero is reserved for host (primary node for parents)
    ok_up = 'UP'
    my_type = 'host'

    # properties defined by configuration
    # *required: is required in conf
    # *default: default value if no set in conf
    # *pythonize: function to call when transforming string to python object
    # *fill_brok: if set, send to broker. there are two categories: full_status for initial and update status, check_result for check results
    # *no_slots: do not take this property for __slots__
    #  Only for the initial call
    # conf_send_preparation: if set, will pass the property to this function. It's used to "flatten"
    #  some dangerous properties like realms that are too 'linked' to be send like that.
    # brok_transformation: if set, will call the function with the value of the property
    #  the major times it will be to flatten the data (like realm_name instead of the realm object).
    properties = SchedulingItem.properties.copy()
    properties.update({
        'host_name':            StringProp(fill_brok=['full_status', 'check_result', 'next_schedule']),
        'alias':                StringProp(fill_brok=['full_status']),
        'display_name':         StringProp(default='', fill_brok=['full_status']),
        'address':              StringProp(fill_brok=['full_status']),
        'parents':              ListProp(brok_transformation=to_hostnames_list, default='', fill_brok=['full_status'], merging='join'),
        'hostgroups':           StringProp(brok_transformation=to_list_string_of_names, default='', fill_brok=['full_status'], merging='join'),
        'check_command':        StringProp(default='_internal_host_up', fill_brok=['full_status']),
        'initial_state':        CharProp(default='u', fill_brok=['full_status']),
        'max_check_attempts':   IntegerProp(default='1',fill_brok=['full_status']),
        'check_interval':       IntegerProp(default='0', fill_brok=['full_status']),
        'retry_interval':       IntegerProp(default='0', fill_brok=['full_status']),
        'active_checks_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'passive_checks_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'check_period':         StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'obsess_over_host':     BoolProp(default='0', fill_brok=['full_status'], retention=True),
        'check_freshness':      BoolProp(default='0', fill_brok=['full_status']),
        'freshness_threshold':  IntegerProp(default='0', fill_brok=['full_status']),
        'event_handler':        StringProp(default='', fill_brok=['full_status']),
        'event_handler_enabled': BoolProp(default='0', fill_brok=['full_status']),
        'low_flap_threshold':   IntegerProp(default='25', fill_brok=['full_status']),
        'high_flap_threshold':  IntegerProp(default='50', fill_brok=['full_status']),
        'flap_detection_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'flap_detection_options': ListProp(default='o,d,u', fill_brok=['full_status'], merging='join'),
        'process_perf_data':    BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'retain_status_information': BoolProp(default='1', fill_brok=['full_status']),
        'retain_nonstatus_information': BoolProp(default='1', fill_brok=['full_status']),
        'contacts':             StringProp(default='', brok_transformation=to_list_of_names, fill_brok=['full_status'], merging='join'),
        'contact_groups':       StringProp(default='', fill_brok=['full_status'], merging='join'),
        'notification_interval': IntegerProp(default='60', fill_brok=['full_status']),
        'first_notification_delay': IntegerProp(default='0', fill_brok=['full_status']),
        'notification_period':  StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'notification_options': ListProp(default='d,u,r,f', fill_brok=['full_status'], merging='join'),
        'notifications_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'stalking_options':     ListProp(default='', fill_brok=['full_status']),
        'notes':                StringProp(default='', fill_brok=['full_status']),
        'notes_url':            StringProp(default='', fill_brok=['full_status']),
        'action_url':           StringProp(default='', fill_brok=['full_status']),
        'icon_image':           StringProp(default='', fill_brok=['full_status']),
        'icon_image_alt':       StringProp(default='', fill_brok=['full_status']),
        'icon_set':             StringProp(default='', fill_brok=['full_status']),
        'vrml_image':           StringProp(default='', fill_brok=['full_status']),
        'statusmap_image':      StringProp(default='', fill_brok=['full_status']),

        # No slots for this 2 because begin property by a number seems bad
        # it's stupid!
        '2d_coords':            StringProp(default='', fill_brok=['full_status'], no_slots=True),
        '3d_coords':            StringProp(default='', fill_brok=['full_status'], no_slots=True),
        'failure_prediction_enabled': BoolProp(default='0', fill_brok=['full_status']),

        ### New to shinken
        # 'fill_brok' is ok because in scheduler it's already
        # a string from conf_send_preparation
        'realm':                StringProp(default=None, fill_brok=['full_status'], conf_send_preparation=get_obj_name),
        'poller_tag':           StringProp(default='None'),
        'reactionner_tag':      StringProp(default='None'),
        'resultmodulations':    StringProp(default='', merging='join'),
        'business_impact_modulations': StringProp(default='', merging='join'),
        'escalations':          StringProp(default='', fill_brok=['full_status'], merging='join'),
        'maintenance_period':   StringProp(default='', brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'time_to_orphanage':    IntegerProp(default='300', fill_brok=['full_status']),
        'service_overrides':    ListProp(default='', merging='duplicate', split_on_coma=False),
        'service_excludes':     ListProp(default='', merging='duplicate'),
        'labels':               ListProp(default='', fill_brok=['full_status'], merging='join'),

        # BUSINESS CORRELATOR PART
        # Business rules output format template
        'business_rule_output_template': StringProp(default='', fill_brok=['full_status']),
        # Business rules notifications mode
        'business_rule_smart_notifications': BoolProp(default='0', fill_brok=['full_status']),
        # Treat downtimes as acknowledgements in smart notifications
        'business_rule_downtime_as_ack': BoolProp(default='0', fill_brok=['full_status']),
        # Enforces child nodes notification options
        'business_rule_host_notification_options':    ListProp(default='', fill_brok=['full_status']),
        'business_rule_service_notification_options': ListProp(default='', fill_brok=['full_status']),

        # Business impact value
        'business_impact':      IntegerProp(default='2', fill_brok=['full_status']),

        # Load some triggers
        'trigger':         StringProp(default=''),
        'trigger_name':    ListProp(default=''),
        'trigger_broker_raise_enabled': BoolProp(default='0'),

        # Trending
        'trending_policies':    ListProp(default='', fill_brok=['full_status'], merging='join'),

        # Our modulations. By defualt void, but will filled by an inner if need
        'checkmodulations':       ListProp(default='', fill_brok=['full_status'], merging='join'),
        'macromodulations':       ListProp(default='', merging='join'),

        # Custom views
        'custom_views'     :    ListProp(default='', fill_brok=['full_status'], merging='join'),
    })

    # properties set only for running purpose
    # retention: save/load this property from retention
    running_properties = SchedulingItem.running_properties.copy()
    running_properties.update({
        'modified_attributes':  IntegerProp(default=0L, fill_brok=['full_status'], retention=True),
        'last_chk':             IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'next_chk':             IntegerProp(default=0, fill_brok=['full_status', 'next_schedule'], retention=True),
        'in_checking':          BoolProp(default=False, fill_brok=['full_status', 'check_result', 'next_schedule']),
        'in_maintenance':       IntegerProp(default=None, fill_brok=['full_status'], retention=True),
        'latency':              FloatProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'attempt':              IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'state':                StringProp(default='PENDING', fill_brok=['full_status', 'check_result'], retention=True),
        'state_id':             IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'state_type':           StringProp(default='HARD', fill_brok=['full_status', 'check_result'], retention=True),
        'state_type_id':        IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'current_event_id':     StringProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_event_id':        IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_state':           StringProp(default='PENDING', fill_brok=['full_status', 'check_result'], retention=True),
        'last_state_id':        IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_state_type':     StringProp(default='HARD', fill_brok=['full_status', 'check_result'],  retention=True),
        'last_state_change':    FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_hard_state_change': FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_hard_state':      StringProp(default='PENDING', fill_brok=['full_status'], retention=True),
        'last_hard_state_id':  IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'last_time_up':         IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_time_down':       IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_time_unreachable': IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'duration_sec':         IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'output':               StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'long_output':          StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'is_flapping':          BoolProp(default=False, fill_brok=['full_status'], retention=True),
        'flapping_comment_id':  IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        # No broks for _depend_of because of to much links to hosts/services
        # dependencies for actions like notif of event handler, so AFTER check return
        'act_depend_of':        StringProp(default=[]),

        # dependencies for checks raise, so BEFORE checks
        'chk_depend_of':        StringProp(default=[]),

        # elements that depend of me, so the reverse than just upper
        'act_depend_of_me':     StringProp(default=[]),

        # elements that depend of me
        'chk_depend_of_me':     StringProp(default=[]),
        'last_state_update':    StringProp(default=0, fill_brok=['full_status'], retention=True),

        # no brok ,to much links
        'services':             StringProp(default=[]),

        # No broks, it's just internal, and checks have too links
        'checks_in_progress':   StringProp(default=[]),

        # No broks, it's just internal, and checks have too links
        'notifications_in_progress': StringProp(default={}, retention=True),
        'downtimes':            StringProp(default=[], fill_brok=['full_status'], retention=True),
        'comments':             StringProp(default=[], fill_brok=['full_status'], retention=True),
        'flapping_changes':     StringProp(default=[], fill_brok=['full_status'], retention=True),
        'percent_state_change': FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'problem_has_been_acknowledged': BoolProp(default=False, fill_brok=['full_status'], retention=True),
        'acknowledgement':      StringProp(default=None, retention=True),
        'acknowledgement_type': IntegerProp(default=1, fill_brok=['full_status', 'check_result'], retention=True),
        'check_type':           IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'has_been_checked':     IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'should_be_scheduled':  IntegerProp(default=1, fill_brok=['full_status'], retention=True),
        'last_problem_id':      IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'current_problem_id':   IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'execution_time':       FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'u_time':               FloatProp(default=0.0),
        's_time':               FloatProp(default=0.0),
        'last_notification':    FloatProp(default=0.0, fill_brok=['full_status'], retention=True),
        'current_notification_number': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'current_notification_id': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'check_flapping_recovery_notification': BoolProp(default=True, fill_brok=['full_status'], retention=True),
        'scheduled_downtime_depth': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'pending_flex_downtime': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'timeout':              IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'start_time':           IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'end_time':             IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'early_timeout':        IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'return_code':          IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'perf_data':            StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'last_perf_data':       StringProp(default='', retention=True),
        'customs':              StringProp(default={}, fill_brok=['full_status']),
        'got_default_realm':   BoolProp(default=False),

        # use for having all contacts we have notified
        # Warning: for the notified_contacts retention save, we save only the names of the contacts, and we should RELINK
        # them when we load it.
        'notified_contacts':    StringProp(default=set(), retention=True, retention_preparation=to_list_of_names),

        'in_scheduled_downtime': BoolProp(default=False, fill_brok=['full_status'], retention=True),
        'in_scheduled_downtime_during_last_check': BoolProp(default=False, retention=True),

        # put here checks and notif raised
        'actions':              StringProp(default=[]),
        # and here broks raised
        'broks':                StringProp(default=[]),

        # For knowing with which elements we are in relation
        # of dep.
        # childs are the hosts that have US as parent, so
        # only a network dep
        'childs':               StringProp(brok_transformation=to_hostnames_list, default=[], fill_brok=['full_status']),
        # Here it's the elements we are depending on
        # so our parents as network relation, or a host
        # we are depending in a hostdependency
        # or even if we are business based.
        'parent_dependencies': StringProp(brok_transformation=to_svc_hst_distinct_lists, default=set(), fill_brok=['full_status']),
        # Here it's the guys that depend on us. So it's the total
        # opposite of the parent_dependencies
        'child_dependencies':   StringProp(
            brok_transformation=to_svc_hst_distinct_lists,
            default=set(),
            fill_brok=['full_status']),


        ### Problem/impact part
        'is_problem':           StringProp(default=False, fill_brok=['full_status']),
        'is_impact':            StringProp(default=False, fill_brok=['full_status']),

        # the save value of our business_impact for "problems"
        'my_own_business_impact':     IntegerProp(default=-1, fill_brok=['full_status']),

        # list of problems that make us an impact
        'source_problems':      StringProp(brok_transformation=to_svc_hst_distinct_lists, default=[], fill_brok=['full_status']),

        # list of the impact I'm the cause of
        'impacts':              StringProp(brok_transformation=to_svc_hst_distinct_lists, default=[], fill_brok=['full_status']),

        # keep a trace of the old state before being an impact
        'state_before_impact':  StringProp(default='PENDING'),
        # keep a trace of the old state id before being an impact
        'state_id_before_impact': StringProp(default=0),
        # if the state change, we know so we do not revert it
        'state_changed_since_impact': StringProp(default=False),

        # BUSINESS CORRELATOR PART
        # Say if we are business based rule or not
        'got_business_rule': BoolProp(default=False, fill_brok=['full_status']),
        # Previously processed business rule (with macro expanded)
        'processed_business_rule': StringProp(default="", fill_brok=['full_status']),
        # Our Dependency node for the business rule
        'business_rule': StringProp(default=None),

        # Manage the unknown/unreach during hard state
        # From now its not really used
        'in_hard_unknown_reach_phase': BoolProp(default=False, retention=True),
        'was_in_hard_unknown_reach_phase': BoolProp(default=False, retention=True),
        'state_before_hard_unknown_reach_phase': StringProp(default='UP', retention=True),

        # Set if the element just change its father/son topology
        'topology_change': BoolProp(default=False, fill_brok=['full_status']),

        # Keep in mind our pack id after the cutting phase
        'pack_id': IntegerProp(default=-1),

        # Trigger list
        'triggers':  StringProp(default=[]),
    })

    # Hosts macros and prop that give the information
    # the prop can be callable or not
    macros = {
        'HOSTNAME':          'host_name',
        'HOSTDISPLAYNAME':   'display_name',
        'HOSTALIAS':         'alias',
        'HOSTADDRESS':       'address',
        'HOSTSTATE':         'state',
        'HOSTSTATEID':       'state_id',
        'LASTHOSTSTATE':     'last_state',
        'LASTHOSTSTATEID':   'last_state_id',
        'HOSTSTATETYPE':     'state_type',
        'HOSTATTEMPT':       'attempt',
        'MAXHOSTATTEMPTS':   'max_check_attempts',
        'HOSTEVENTID':       'current_event_id',
        'LASTHOSTEVENTID':   'last_event_id',
        'HOSTPROBLEMID':     'current_problem_id',
        'LASTHOSTPROBLEMID': 'last_problem_id',
        'HOSTLATENCY':       'latency',
        'HOSTEXECUTIONTIME': 'execution_time',
        'HOSTDURATION':      'get_duration',
        'HOSTDURATIONSEC':   'get_duration_sec',
        'HOSTDOWNTIME':      'get_downtime',
        'HOSTPERCENTCHANGE': 'percent_state_change',
        'HOSTGROUPNAME':     'get_groupname',
        'HOSTGROUPNAMES':    'get_groupnames',
        'LASTHOSTCHECK':     'last_chk',
        'LASTHOSTSTATECHANGE': 'last_state_change',
        'LASTHOSTUP':        'last_time_up',
        'LASTHOSTDOWN':      'last_time_down',
        'LASTHOSTUNREACHABLE': 'last_time_unreachable',
        'HOSTOUTPUT':        'output',
        'LONGHOSTOUTPUT':    'long_output',
        'HOSTPERFDATA':      'perf_data',
        'LASTHOSTPERFDATA':  'last_perf_data',
        'HOSTCHECKCOMMAND':  'get_check_command',
        'HOSTACKAUTHOR':     'get_ack_author_name',
        'HOSTACKAUTHORNAME': 'get_ack_author_name',
        'HOSTACKAUTHORALIAS': 'get_ack_author_name',
        'HOSTACKCOMMENT':    'get_ack_comment',
        'HOSTACTIONURL':     'action_url',
        'HOSTNOTESURL':      'notes_url',
        'HOSTNOTES':         'notes',
        'HOSTREALM':         'get_realm',
        'TOTALHOSTSERVICES': 'get_total_services',
        'TOTALHOSTSERVICESOK': 'get_total_services_ok',
        'TOTALHOSTSERVICESWARNING': 'get_total_services_warning',
        'TOTALHOSTSERVICESUNKNOWN': 'get_total_services_unknown',
        'TOTALHOSTSERVICESCRITICAL': 'get_total_services_critical',
        'HOSTBUSINESSIMPACT':  'business_impact'
    }

    # Manage ADDRESSX macros by adding them dynamically
    for _i in range(32):
        macros['HOSTADDRESS%d'%_i] = 'address%d'% _i

    # This tab is used to transform old parameters name into new ones
    # so from Nagios2 format, to Nagios3 ones.
    # Or Shinken deprecated names like criticity
    old_properties = {
        'normal_check_interval': 'check_interval',
        'retry_check_interval': 'retry_interval',
        'criticity': 'business_impact',
        'hostgroup': 'hostgroups',
        ## 'criticitymodulations': 'business_impact_modulations',
    }

#######
#                   __ _                       _   _
#                  / _(_)                     | | (_)
#   ___ ___  _ __ | |_ _  __ _ _   _ _ __ __ _| |_ _  ___  _ __
#  / __/ _ \| '_ \|  _| |/ _` | | | | '__/ _` | __| |/ _ \| '_ \
# | (_| (_) | | | | | | | (_| | |_| | | | (_| | |_| | (_) | | | |
#  \___\___/|_| |_|_| |_|\__, |\__,_|_|  \__,_|\__|_|\___/|_| |_|
#                         __/ |
#                        |___/
######


    # Fill address with host_name if not already set
    def fill_predictive_missing_parameters(self):
        if hasattr(self, 'host_name') and not hasattr(self, 'address'):
            self.address = self.host_name
        if hasattr(self, 'host_name') and not hasattr(self, 'alias'):
            self.alias = self.host_name

    # Check is required prop are set:
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        source = getattr(self, 'imported_from', 'unknown')

        special_properties = ['check_period', 'notification_interval', 'check_period',
                              'notification_period']
        for prop, entry in cls.properties.items():
            if prop not in special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.error("[host::%s] %s property not set" % (self.get_name(), prop))
                    state = False  # Bad boy...

        # Then look if we have some errors in the conf
        # Juts print warnings, but raise errors
        for err in self.configuration_warnings:
            logger.warning("[host::%s] %s" % (self.get_name(), err))

        # Raised all previously saw errors like unknown contacts and co
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[host::%s] %s" % (self.get_name(), err))

        if not hasattr(self, 'notification_period'):
            self.notification_period = None

        # Ok now we manage special cases...
        if self.notifications_enabled and self.contacts == []:
            logger.warning("The host %s has no contacts nor contact_groups in (%s)" % (self.get_name(), source))

        if getattr(self, 'event_handler', None) and not self.event_handler.is_valid():
            logger.info("%s: my event_handler %s is invalid" % (self.get_name(), self.event_handler.command))
            state = False

        if getattr(self, 'check_command', None) is None:
            logger.info("%s: I've got no check_command" % self.get_name())
            state = False
        # Ok got a command, but maybe it's invalid
        else:
            if not self.check_command.is_valid():
                logger.info("%s: my check_command %s is invalid" % (self.get_name(), self.check_command.command))
                state = False
            if self.got_business_rule:
                if not self.business_rule.is_valid():
                    logger.error("%s: my business rule is invalid" % (self.get_name(),))
                    for bperror in self.business_rule.configuration_errors:
                        logger.error("[host::%s] %s" % (self.get_name(), bperror))
                    state = False

        if not hasattr(self, 'notification_interval') and self.notifications_enabled == True:
            logger.info("%s: I've got no notification_interval but I've got notifications enabled" % self.get_name())
            state = False

        # If active check is enabled with a check_interval!=0, we must have a check_period
        if (getattr(self, 'active_checks_enabled', False)
             and getattr(self, 'check_period', None) is None
             and getattr(self, 'check_interval', 1) != 0):
            logger.info("%s: check_period is not correct" % self.get_name())
            state = False

        if not hasattr(self, 'check_period'):
            self.check_period = None

        if hasattr(self, 'host_name'):
            for c in cls.illegal_object_name_chars:
                if c in self.host_name:
                    logger.info("%s: My host_name got the character %s that is not allowed." % (self.get_name(), c))
                    state = False

        return state

    # Search in my service if I've got the service
    def find_service_by_name(self, service_description):
        for s in self.services:
            if getattr(s, 'service_description', '__UNNAMED_SERVICE__') == service_description:
                return s
        return None

    # For get a nice name
    def get_name(self):
        if not self.is_tpl():
            try:
                return self.host_name
            except AttributeError:  # outch, no hostname
                return 'UNNAMEDHOST'
        else:
            try:
                return self.name
            except AttributeError:  # outch, no name for this template
                return 'UNNAMEDHOSTTEMPLATE'

    def get_groupname(self):
        groupname = ''
        for hg in self.hostgroups:
            # console_logger.info('get_groupname : %s %s %s' % (hg.id, hg.alias, hg.get_name()))
            # groupname = "%s [%s]" % (hg.alias, hg.get_name())
            groupname = "%s" % (hg.alias)
        return groupname

    def get_groupnames(self):
        groupnames = ''
        for hg in self.hostgroups:
            # console_logger.info('get_groupnames : %s' % (hg.get_name()))
            if groupnames == '':
                groupnames = hg.get_name()
            else:
                groupnames = "%s, %s" % (groupnames, hg.get_name())
        return groupnames

    # For debugging purpose only
    def get_dbg_name(self):
        return self.host_name

    # Same but for clean call, no debug
    def get_full_name(self):
        return self.host_name

    # Get our realm
    def get_realm(self):
        return self.realm

    def get_hostgroups(self):
        return self.hostgroups

    def get_host_tags(self):
        return self.tags

    # Say if we got the other in one of your dep list
    def is_linked_with_host(self, other):
        for (h, status, type, timeperiod, inherits_parent) in self.act_depend_of:
            if h == other:
                return True
        return False

    # Delete all links in the act_depend_of list of self and other
    def del_host_act_dependency(self, other):
        to_del = []
        # First we remove in my list
        for (h, status, type, timeperiod, inherits_parent) in self.act_depend_of:
            if h == other:
                to_del.append((h, status, type, timeperiod, inherits_parent))
        for t in to_del:
            self.act_depend_of.remove(t)

        # And now in the father part
        to_del = []
        for (h, status, type, timeperiod, inherits_parent) in other.act_depend_of_me:
            if h == self:
                to_del.append((h, status, type, timeperiod, inherits_parent))
        for t in to_del:
            other.act_depend_of_me.remove(t)

        # Remove in child/parents deps too
        # Me in father list
        other.child_dependencies.remove(self)
        # and father list in mine
        self.parent_dependencies.remove(other)

    # Add a dependency for action event handler, notification, etc)
    # and add ourself in it's dep list
    def add_host_act_dependency(self, h, status, timeperiod, inherits_parent):
        # I add him in MY list
        self.act_depend_of.append((h, status, 'logic_dep', timeperiod, inherits_parent))
        # And I add me in it's list
        h.act_depend_of_me.append((self, status, 'logic_dep', timeperiod, inherits_parent))

        # And the parent/child dep lists too
        h.register_son_in_parent_child_dependencies(self)

    # Register the dependency between 2 service for action (notification etc)
    # but based on a BUSINESS rule, so on fact:
    # ERP depend on database, so we fill just database.act_depend_of_me
    # because we will want ERP mails to go on! So call this
    # on the database service with the srv=ERP service
    def add_business_rule_act_dependency(self, h, status, timeperiod, inherits_parent):
        # first I add the other the I depend on in MY list
        # I only register so he know that I WILL be a impact
        self.act_depend_of_me.append((h, status, 'business_dep',
                                      timeperiod, inherits_parent))

        # And the parent/child dep lists too
        self.register_son_in_parent_child_dependencies(h)

    # Add a dependency for check (so before launch)
    def add_host_chk_dependency(self, h, status, timeperiod, inherits_parent):
        # I add him in MY list
        self.chk_depend_of.append((h, status, 'logic_dep', timeperiod, inherits_parent))
        # And I add me in it's list
        h.chk_depend_of_me.append((self, status, 'logic_dep', timeperiod, inherits_parent))

        # And we fill parent/childs dep for brok purpose
        # Here self depend on h
        h.register_son_in_parent_child_dependencies(self)

    # Add one of our service to services (at linkify)
    def add_service_link(self, service):
        self.services.append(service)

#####
#                         _
#                        (_)
#  _ __ _   _ _ __  _ __  _ _ __   __ _
# | '__| | | | '_ \| '_ \| | '_ \ / _` |
# | |  | |_| | | | | | | | | | | | (_| |
# |_|   \__,_|_| |_|_| |_|_|_| |_|\__, |
#                                  __/ |
#                                 |___/
####



    # Set unreachable: all our parents are down!
    # We have a special state, but state was already set, we just need to
    # update it. We are no DOWN, we are UNREACHABLE and
    # got a state id is 2
    def set_unreachable(self):
        now = time.time()
        self.state_id = 2
        self.state = 'UNREACHABLE'
        self.last_time_unreachable = int(now)

    # We just go an impact, so we go unreachable
    # But only if we enable this state change in the conf
    def set_impact_state(self):
        cls = self.__class__
        if cls.enable_problem_impacts_states_change:
            # Keep a trace of the old state (problem came back before
            # a new checks)
            self.state_before_impact = self.state
            self.state_id_before_impact = self.state_id
            # This flag will know if we override the impact state
            self.state_changed_since_impact = False
            self.state = 'UNREACHABLE'  # exit code UNDETERMINED
            self.state_id = 2

    # Ok, we are no more an impact, if no news checks
    # override the impact state, we came back to old
    # states
    # And only if impact state change is set in configuration
    def unset_impact_state(self):
        cls = self.__class__
        if cls.enable_problem_impacts_states_change and not self.state_changed_since_impact:
            self.state = self.state_before_impact
            self.state_id = self.state_id_before_impact

    # set the state in UP, DOWN, or UNDETERMINED
    # with the status of a check. Also update last_state
    def set_state_from_exit_status(self, status):
        now = time.time()
        self.last_state_update = now

        # we should put in last_state the good last state:
        # if not just change the state by an problem/impact
        # we can take current state. But if it's the case, the
        # real old state is self.state_before_impact (it's the TRUE
        # state in fact)
        # And only if we enable the impact state change
        cls = self.__class__
        if cls.enable_problem_impacts_states_change and self.is_impact and not self.state_changed_since_impact:
            self.last_state = self.state_before_impact
        else:
            self.last_state = self.state

        if status == 0 or (status == 1 and cls.use_aggressive_host_checking == 0):
            self.state = 'UP'
            self.state_id = 0
            self.last_time_up = int(self.last_state_update)
            state_code = 'u'
        elif status in (2, 3) or (status == 1 and cls.use_aggressive_host_checking == 1):
            self.state = 'DOWN'
            self.state_id = 1
            self.last_time_down = int(self.last_state_update)
            state_code = 'd'
        else:
            self.state = 'DOWN'  # exit code UNDETERMINED
            self.state_id = 1
            self.last_time_down = int(self.last_state_update)
            state_code = 'd'
        if state_code in self.flap_detection_options:
            self.add_flapping_change(self.state != self.last_state)
        if self.state != self.last_state:
            self.last_state_change = self.last_state_update
        self.duration_sec = now - self.last_state_change

    # See if status is status. Can be low of high format (o/UP, d/DOWN, ...)
    def is_state(self, status):
        if status == self.state:
            return True
        # Now low status
        elif status == 'o' and self.state == 'UP':
            return True
        elif status == 'd' and self.state == 'DOWN':
            return True
        elif status == 'u' and self.state == 'UNREACHABLE':
            return True
        return False

    # The last time when the state was not UP
    def last_time_non_ok_or_up(self):
        if self.last_time_down > self.last_time_up:
            last_time_non_up = self.last_time_down
        else:
            last_time_non_up = 0
        return last_time_non_up

    # Add a log entry with a HOST ALERT like:
    # HOST ALERT: server;DOWN;HARD;1;I don't know what to say...
    def raise_alert_log_entry(self):
        console_logger.alert('HOST ALERT: %s;%s;%s;%d;%s'
                            % (self.get_name(),
                               self.state, self.state_type,
                               self.attempt, self.output))

    # If the configuration allow it, raise an initial log like
    # CURRENT HOST STATE: server;DOWN;HARD;1;I don't know what to say...
    def raise_initial_state(self):
        if self.__class__.log_initial_states:
            console_logger.info('CURRENT HOST STATE: %s;%s;%s;%d;%s'
                                % (self.get_name(),
                                   self.state, self.state_type,
                                   self.attempt, self.output))

    # Add a log entry with a Freshness alert like:
    # Warning: The results of host 'Server' are stale by 0d 0h 0m 58s (threshold=0d 1h 0m 0s).
    # I'm forcing an immediate check of the host.
    def raise_freshness_log_entry(self, t_stale_by, t_threshold):
        logger.warning("The results of host '%s' are stale by %s "
                       "(threshold=%s).  I'm forcing an immediate check "
                       "of the host."
                       % (self.get_name(),
                          format_t_into_dhms_format(t_stale_by),
                          format_t_into_dhms_format(t_threshold)))

    # Raise a log entry with a Notification alert like
    # HOST NOTIFICATION: superadmin;server;UP;notify-by-rss;no output
    def raise_notification_log_entry(self, n):
        contact = n.contact
        command = n.command_call
        if n.type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'CUSTOM',
                      'ACKNOWLEDGEMENT', 'FLAPPINGSTART', 'FLAPPINGSTOP',
                      'FLAPPINGDISABLED'):
            state = '%s (%s)' % (n.type, self.state)
        else:
            state = self.state
        if self.__class__.log_notifications:
            console_logger.alert("HOST NOTIFICATION: %s;%s;%s;%s;%s"
                                % (contact.get_name(), self.get_name(),
                                   state, command.get_name(), self.output))

    # Raise a log entry with a Eventhandler alert like
    # HOST NOTIFICATION: superadmin;server;UP;notify-by-rss;no output
    def raise_event_handler_log_entry(self, command):
        if self.__class__.log_event_handlers:
            console_logger.alert("HOST EVENT HANDLER: %s;%s;%s;%s;%s"
                                % (self.get_name(),
                                   self.state, self.state_type,
                                   self.attempt, command.get_name()))

    # Raise a log entry with FLAPPING START alert like
    # HOST FLAPPING ALERT: server;STARTED; Host appears to have started flapping (50.6% change >= 50.0% threshold)
    def raise_flapping_start_log_entry(self, change_ratio, threshold):
        console_logger.alert("HOST FLAPPING ALERT: %s;STARTED; "
                            "Host appears to have started flapping "
                            "(%.1f%% change >= %.1f%% threshold)"
                            % (self.get_name(), change_ratio, threshold))

    # Raise a log entry with FLAPPING STOP alert like
    # HOST FLAPPING ALERT: server;STOPPED; host appears to have stopped flapping (23.0% change < 25.0% threshold)
    def raise_flapping_stop_log_entry(self, change_ratio, threshold):
        console_logger.alert("HOST FLAPPING ALERT: %s;STOPPED; "
                            "Host appears to have stopped flapping "
                            "(%.1f%% change < %.1f%% threshold)"
                            % (self.get_name(), change_ratio, threshold))

    # If there is no valid time for next check, raise a log entry
    def raise_no_next_check_log_entry(self):
        logger.warning("I cannot schedule the check for the host '%s' "
                       "because there is not future valid time"
                       % (self.get_name()))

    # Raise a log entry when a downtime begins
    # HOST DOWNTIME ALERT: test_host_0;STARTED; Host has entered a period of scheduled downtime
    def raise_enter_downtime_log_entry(self):
        console_logger.alert("HOST DOWNTIME ALERT: %s;STARTED; "
                            "Host has entered a period of scheduled downtime"
                            % (self.get_name()))

    # Raise a log entry when a downtime has finished
    # HOST DOWNTIME ALERT: test_host_0;STOPPED; Host has exited from a period of scheduled downtime
    def raise_exit_downtime_log_entry(self):
        console_logger.alert("HOST DOWNTIME ALERT: %s;STOPPED; Host has "
                            "exited from a period of scheduled downtime"
                            % (self.get_name()))

    # Raise a log entry when a downtime prematurely ends
    # HOST DOWNTIME ALERT: test_host_0;CANCELLED; Service has entered a period of scheduled downtime
    def raise_cancel_downtime_log_entry(self):
        console_logger.alert("HOST DOWNTIME ALERT: %s;CANCELLED; "
                            "Scheduled downtime for host has been cancelled."
                            % (self.get_name()))

    # Is stalking?
    # Launch if check is waitconsume==first time
    # and if c.status is in self.stalking_options
    def manage_stalking(self, c):
        need_stalk = False
        if c.status == 'waitconsume':
            if c.exit_status == 0 and 'o' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 1 and 'd' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 2 and 'd' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 3 and 'u' in self.stalking_options:
                need_stalk = True
            if c.output != self.output:
                need_stalk = False
        if need_stalk:
            logger.info("Stalking %s: %s" % (self.get_name(), self.output))

    # fill act_depend_of with my parents (so network dep)
    # and say parents they impact me, no timeperiod and follow parents of course
    def fill_parents_dependency(self):
        for parent in self.parents:
            if parent is not None:
                # I add my parent in my list
                self.act_depend_of.append((parent, ['d', 'u', 's', 'f'], 'network_dep', None, True))

                # And I register myself in my parent list too
                parent.register_child(self)

                # And add the parent/child dep filling too, for broking
                parent.register_son_in_parent_child_dependencies(self)

    # Register a child in our lists
    def register_child(self, child):
        # We've got 2 list: a list for our child
        # where we just put the pointer, it's just for broking
        # and another with all data, useful for 'running' part
        self.childs.append(child)
        self.act_depend_of_me.append((child, ['d', 'u', 's', 'f'], 'network_dep', None, True))

    # Give data for checks's macros
    def get_data_for_checks(self):
        return [self]

    # Give data for event handler's macro
    def get_data_for_event_handler(self):
        return [self]

    # Give data for notifications'n macros
    def get_data_for_notifications(self, contact, n):
        return [self, contact, n]

    # See if the notification is launchable (time is OK and contact is OK too)
    def notification_is_blocked_by_contact(self, n, contact):
        return not contact.want_host_notification(self.last_chk, self.state, n.type, self.business_impact, n.command_call)

    # MACRO PART
    def get_duration_sec(self):
        return str(int(self.duration_sec))

    def get_duration(self):
        m, s = divmod(self.duration_sec, 60)
        h, m = divmod(m, 60)
        return "%02dh %02dm %02ds" % (h, m, s)

    # Check if a notification for this host is suppressed at this time
    # This is a check at the host level. Do not look at contacts here
    def notification_is_blocked_by_item(self, type, t_wished=None):
        if t_wished is None:
            t_wished = time.time()

        # TODO
        # forced notification -> false
        # custom notification -> false

        # Block if notifications are program-wide disabled
        if not self.enable_notifications:
            return True

        # Does the notification period allow sending out this notification?
        if self.notification_period is not None and not self.notification_period.is_time_valid(t_wished):
            return True

        # Block if notifications are disabled for this host
        if not self.notifications_enabled:
            return True

        # Block if the current status is in the notification_options d,u,r,f,s
        if 'n' in self.notification_options:
            return True

        if type in ('PROBLEM', 'RECOVERY'):
            if self.state == 'DOWN' and not 'd' in self.notification_options:
                return True
            if self.state == 'UP' and not 'r' in self.notification_options:
                return True
            if self.state == 'UNREACHABLE' and not 'u' in self.notification_options:
                return True
        if (type in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED')
                and not 'f' in self.notification_options):
            return True
        if (type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'DOWNTIMECANCELLED')
                and not 's' in self.notification_options):
            return True

        # Acknowledgements make no sense when the status is ok/up
        if type == 'ACKNOWLEDGEMENT':
            if self.state == self.ok_up:
                return True

        # Flapping
        if type in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
        # todo    block if not notify_on_flapping
            if self.scheduled_downtime_depth > 0:
                return True

        # When in deep downtime, only allow end-of-downtime notifications
        # In depth 1 the downtime just started and can be notified
        if self.scheduled_downtime_depth > 1 and not type in ('DOWNTIMEEND', 'DOWNTIMECANCELLED'):
            return True

        # Block if in a scheduled downtime and a problem arises
        if self.scheduled_downtime_depth > 0 and type in ('PROBLEM', 'RECOVERY'):
            return True

        # Block if the status is SOFT
        if self.state_type == 'SOFT' and type == 'PROBLEM':
            return True

        # Block if the problem has already been acknowledged
        if self.problem_has_been_acknowledged and type != 'ACKNOWLEDGEMENT':
            return True

        # Block if flapping
        if self.is_flapping and type not in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            return True

        # Block if business rule smart notifications is enabled and all its
        # childs have been acknowledged or are under downtime.
        if self.got_business_rule is True \
                and self.business_rule_smart_notifications is True \
                and self.business_rule_notification_is_blocked() is True \
                and type == 'PROBLEM':
            return True

        return False

    # Get a oc*p command if item has obsess_over_*
    # command. It must be enabled locally and globally
    def get_obsessive_compulsive_processor_command(self):
        cls = self.__class__
        if not cls.obsess_over or not self.obsess_over_host:
            return

        m = MacroResolver()
        data = self.get_data_for_event_handler()
        cmd = m.resolve_command(cls.ochp_command, data)
        e = EventHandler(cmd, timeout=cls.ochp_timeout)

        # ok we can put it in our temp action queue
        self.actions.append(e)

    # Macro part
    def get_total_services(self):
        return str(len(self.services))

    def get_total_services_ok(self):
        return str(len([s for s in self.services if s.state_id == 0]))

    def get_total_services_warning(self):
        return str(len([s for s in self.services if s.state_id == 1]))

    def get_total_services_critical(self):
        return str(len([s for s in self.services if s.state_id == 2]))

    def get_total_services_unknown(self):
        return str(len([s for s in self.services if s.state_id == 3]))

    def get_ack_author_name(self):
        if self.acknowledgement is None:
            return ''
        return self.acknowledgement.author

    def get_ack_comment(self):
        if self.acknowledgement is None:
            return ''
        return self.acknowledgement.comment

    def get_check_command(self):
        return self.check_command.get_name()


# CLass for the hosts lists. It's mainly for configuration
# part
class Hosts(Items):
    name_property = "host_name"  # use for the search by name
    inner_class = Host  # use for know what is in items


    # Create link between elements:
    # hosts -> timeperiods
    # hosts -> hosts (parents, etc)
    # hosts -> commands (check_command)
    # hosts -> contacts
    def linkify(self, timeperiods=None, commands=None, contacts=None, realms=None, resultmodulations=None, businessimpactmodulations=None, escalations=None, hostgroups=None, triggers=None, checkmodulations=None, macromodulations=None):
        self.linkify_with_timeperiods(timeperiods, 'notification_period')
        self.linkify_with_timeperiods(timeperiods, 'check_period')
        self.linkify_with_timeperiods(timeperiods, 'maintenance_period')
        self.linkify_h_by_h()
        self.linkify_h_by_hg(hostgroups)
        self.linkify_one_command_with_commands(commands, 'check_command')
        self.linkify_one_command_with_commands(commands, 'event_handler')

        self.linkify_with_contacts(contacts)
        self.linkify_h_by_realms(realms)
        self.linkify_with_resultmodulations(resultmodulations)
        self.linkify_with_business_impact_modulations(businessimpactmodulations)
        # WARNING: all escalations will not be link here
        # (just the escalation here, not serviceesca or hostesca).
        # This last one will be link in escalations linkify.
        self.linkify_with_escalations(escalations)
        self.linkify_with_triggers(triggers)
        self.linkify_with_checkmodulations(checkmodulations)
        self.linkify_with_macromodulations(macromodulations)


    # Fill address by host_name if not set
    def fill_predictive_missing_parameters(self):
        for h in self:
            h.fill_predictive_missing_parameters()

    # Link host with hosts (parents)
    def linkify_h_by_h(self):
        for h in self:
            parents = h.parents
            # The new member list
            new_parents = []
            for parent in parents:
                parent = parent.strip()
                p = self.find_by_name(parent)
                if p is not None:
                    new_parents.append(p)
                else:
                    err = "the parent '%s' on host '%s' is unknown!" % (parent, h.get_name())
                    self.configuration_warnings.append(err)
            #print "Me,", h.host_name, "define my parents", new_parents
            # We find the id, we replace the names
            h.parents = new_parents


    # Link with realms and set a default realm if none
    def linkify_h_by_realms(self, realms):
        default_realm = None
        for r in realms:
            if getattr(r, 'default', False):
                default_realm = r
        # if default_realm is None:
        #    print "Error: there is no default realm defined!"
        for h in self:
            if h.realm is not None:
                p = realms.find_by_name(h.realm.strip())
                if p is None:
                    err = "the host %s got an invalid realm (%s)!" % (h.get_name(), h.realm)
                    h.configuration_errors.append(err)
                h.realm = p
            else:
                #print "Notice: applying default realm %s to host %s" % (default_realm.get_name(), h.get_name())
                h.realm = default_realm
                h.got_default_realm = True


    # We look for hostgroups property in hosts and
    # link them
    def linkify_h_by_hg(self, hostgroups):
        # Register host in the hostgroups
        for h in self:
            if not h.is_tpl():
                new_hostgroups = []
                if hasattr(h, 'hostgroups') and h.hostgroups != '':
                    hgs = h.hostgroups.split(',')
                    for hg_name in hgs:
                        hg_name = hg_name.strip()
                        hg = hostgroups.find_by_name(hg_name)
                        if hg is not None:
                            new_hostgroups.append(hg)
                        else:
                            err = "the hostgroup '%s' of the host '%s' is unknown" % (hg_name, h.host_name)
                            h.configuration_errors.append(err)
                h.hostgroups = new_hostgroups


    # We look for hostgroups property in hosts and
    def explode(self, hostgroups, contactgroups, triggers):

        # items::explode_trigger_string_into_triggers
        self.explode_trigger_string_into_triggers(triggers)

        # Register host in the hostgroups
        for h in self:
            if not h.is_tpl() and hasattr(h, 'host_name'):
                hname = h.host_name
                if hasattr(h, 'hostgroups'):
                    if isinstance(h.hostgroups, list):
                        h.hostgroups = ','.join(h.hostgroups)
                    hgs = h.hostgroups.split(',')
                    for hg in hgs:
                        hostgroups.add_member(hname, hg.strip())

        # items::explode_contact_groups_into_contacts
        # take all contacts from our contact_groups into our contact property
        self.explode_contact_groups_into_contacts(contactgroups)


    # In the scheduler we need to relink the commandCall with
    # the real commands
    def late_linkify_h_by_commands(self, commands):
        props = ['check_command', 'event_handler']
        for h in self:
            for prop in props:
                cc = getattr(h, prop, None)
                if cc:
                    cc.late_linkify_with_command(commands)

            # Ok also link checkmodulations
            for cw in h.checkmodulations:
                cw.late_linkify_cw_by_commands(commands)
                print cw


    # Create dependencies:
    # Dependencies at the host level: host parent
    def apply_dependencies(self):
        for h in self:
            h.fill_parents_dependency()


    # Parent graph: use to find quickly relations between all host, and loop
    # return True if there is a loop
    def no_loop_in_parents(self):
        # Ok, we say "from now, no loop :) "
        r = True

        # Create parent graph
        parents = Graph()

        # With all hosts as nodes
        for h in self:
            if h is not None:
                parents.add_node(h)

        # And now fill edges
        for h in self:
            for p in h.parents:
                if p is not None:
                    parents.add_edge(p, h)

        # Now get the list of all hosts in a loop
        host_in_loops = parents.loop_check()

        # and raise errors about it
        for h in host_in_loops:
            logger.error("The host '%s' is part of a circular parent/child chain!" % h.get_name())
            r = False

        return r

    # Return a list of the host_name of the hosts
    # that got the template with name=tpl_name or inherit from
    # a template that use it
    def find_hosts_that_use_template(self, tpl_name):
        res = set()
        tpl_name = tpl_name.strip()

        # First find the template
        tpl = None
        for h in self:
            # Look for template with the good name
            if h.is_tpl() and hasattr(h, 'name') and h.name.strip() == tpl_name:
                tpl = h

        # If we find none, we should manually lookup all hosts to find this 'tag'
        if tpl is None:
            for h in self:
                if not hasattr(h, 'host_name') or h.is_tpl():
                    continue
                # Manually lookup for the templates defines in use
                tnames = strip_and_uniq(getattr(h, 'use', '').split(','))
                if tpl_name in tnames:
                    res.add(h.host_name)

            return list(res)

        # Ok, we find the tpl. We should find its father template too
        for t in self.templates.values():
            t.dfs_loop_status = 'DFS_UNCHECKED'
        all_tpl_searched = self.templates_graph.dfs_get_all_childs(tpl)
        # Clean the search tag
        # TODO: better way?
        for t in self.templates.values():
            del t.dfs_loop_status

        # Now we got all the templates we are looking for (so the template
        # and all its own templates too, we search for the hosts that are
        # using them
        for h in self:
            # If the host is a not valid one, skip it
            if not hasattr(h, 'host_name'):
                continue
            # look if there is a match between host templates
            # and the ones we are looking for
            for t in h.templates:
                if t in all_tpl_searched:
                    res.add(h.host_name)
                    continue

        return list(res)

    # Will create all business tree for the
    # services
    def create_business_rules(self, hosts, services):
        for h in self:
            h.create_business_rules(hosts, services)

    # Will link all business service/host with theirs
    # dep for problem/impact link
    def create_business_rules_dependencies(self):
        for h in self:
            h.create_business_rules_dependencies()

########NEW FILE########
__FILENAME__ = hostdependency
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items

from shinken.property import BoolProp, StringProp, ListProp
from shinken.log import logger


class Hostdependency(Item):
    id = 0
    my_type = 'hostdependency'

    # F is dep of D
    # host_name                      Host B
    #       service_description             Service D
    #       dependent_host_name             Host C
    #       dependent_service_description   Service F
    #       execution_failure_criteria      o
    #       notification_failure_criteria   w,u
    #       inherits_parent         1
    #       dependency_period       24x7

    properties = Item.properties.copy()
    properties.update({
        'dependent_host_name':           StringProp(),
        'dependent_hostgroup_name':      StringProp(default=''),
        'host_name':                     StringProp(),
        'hostgroup_name':                StringProp(default=''),
        'inherits_parent':               BoolProp(default='0'),
        'execution_failure_criteria':    ListProp(default='n'),
        'notification_failure_criteria': ListProp(default='n'),
        'dependency_period':             StringProp(default='')
    })

    # Give a nice name output, for debugging purpose
    # (debugging happens more often than expected...)
    def get_name(self):
        dependent_host_name = 'unknown'
        if getattr(self, 'dependent_host_name', None):
            dependent_host_name = getattr(getattr(self, 'dependent_host_name'), 'host_name', 'unknown')
        host_name = 'unknown'
        if getattr(self, 'host_name', None):
            host_name = getattr(getattr(self, 'host_name'), 'host_name', 'unknown')
        return dependent_host_name + '/' + host_name


class Hostdependencies(Items):
    def delete_hostsdep_by_id(self, ids):
        for id in ids:
            del self[id]

    # We create new hostdep if necessary (host groups and co)
    def explode(self, hostgroups):
        # The "old" dependencies will be removed. All dependencies with
        # more than one host or a host group will be in it
        hstdep_to_remove = []

        # Then for every host create a copy of the dependency with just the host
        # because we are adding services, we can't just loop in it
        hostdeps = self.items.keys()
        for id in hostdeps:
            hd = self.items[id]
            if hd.is_tpl():  # Exploding template is useless
                continue

            # We explode first the dependent (son) part
            dephnames = []
            if hasattr(hd, 'dependent_hostgroup_name'):
                dephg_names = hd.dependent_hostgroup_name.split(',')
                dephg_names = [hg_name.strip() for hg_name in dephg_names]
                for dephg_name in dephg_names:
                    dephg = hostgroups.find_by_name(dephg_name)
                    if dephg is None:
                        err = "ERROR: the hostdependency got an unknown dependent_hostgroup_name '%s'" % dephg_name
                        hd.configuration_errors.append(err)
                        continue
                    dephnames.extend(dephg.members.split(','))

            if hasattr(hd, 'dependent_host_name'):
                dephnames.extend(hd.dependent_host_name.split(','))

            # Ok, and now the father part :)
            hnames = []
            if hasattr(hd, 'hostgroup_name'):
                hg_names = hd.hostgroup_name.split(',')
                hg_names = [hg_name.strip() for hg_name in hg_names]
                for hg_name in hg_names:
                    hg = hostgroups.find_by_name(hg_name)
                    if hg is None:
                        err = "ERROR: the hostdependency got an unknown hostgroup_name '%s'" % hg_name
                        hd.configuration_errors.append(err)
                        continue
                    hnames.extend(hg.members.split(','))

            if hasattr(hd, 'host_name'):
                hnames.extend(hd.host_name.split(','))

            # Loop over all sons and fathers to get S*F host deps
            for dephname in dephnames:
                dephname = dephname.strip()
                for hname in hnames:
                    new_hd = hd.copy()
                    new_hd.dependent_host_name = dephname
                    new_hd.host_name = hname
                    self.items[new_hd.id] = new_hd
            hstdep_to_remove.append(id)

        self.delete_hostsdep_by_id(hstdep_to_remove)

    def linkify(self, hosts, timeperiods):
        self.linkify_hd_by_h(hosts)
        self.linkify_hd_by_tp(timeperiods)
        self.linkify_h_by_hd()

    def linkify_hd_by_h(self, hosts):
        for hd in self:
            try:
                h_name = hd.host_name
                dh_name = hd.dependent_host_name
                h = hosts.find_by_name(h_name)
                if h is None:
                    err = "Error: the host dependency got a bad host_name definition '%s'" % h_name
                    hd.configuration_errors.append(err)
                dh = hosts.find_by_name(dh_name)
                if dh is None:
                    err = "Error: the host dependency got a bad dependent_host_name definition '%s'" % dh_name
                    hd.configuration_errors.append(err)
                hd.host_name = h
                hd.dependent_host_name = dh
            except AttributeError, exp:
                err = "Error: the host dependency miss a property '%s'" % exp
                hd.configuration_errors.append(err)

    # We just search for each hostdep the id of the host
    # and replace the name by the id
    def linkify_hd_by_tp(self, timeperiods):
        for hd in self:
            try:
                tp_name = hd.dependency_period
                tp = timeperiods.find_by_name(tp_name)
                hd.dependency_period = tp
            except AttributeError, exp:
                logger.error("[hostdependency] fail to linkify by timeperiod: %s" % exp)

    # We backport host dep to host. So HD is not need anymore
    def linkify_h_by_hd(self):
        for hd in self:
            # Link template is useless
            if hd.is_tpl():
                continue
            # if the host dep conf is bad, pass this one
            if getattr(hd, 'host_name', None) is None or getattr(hd, 'dependent_host_name', None) is None:
                continue
            # Ok, link!
            depdt_hname = hd.dependent_host_name
            dp = getattr(hd, 'dependency_period', None)
            depdt_hname.add_host_act_dependency(hd.host_name, hd.notification_failure_criteria, dp, hd.inherits_parent)
            depdt_hname.add_host_chk_dependency(hd.host_name, hd.execution_failure_criteria, dp, hd.inherits_parent)

    # Apply inheritance for all properties
    def apply_inheritance(self):
        # We check for all Host properties if the host has it
        # if not, it check all host templates for a value
        for prop in Hostdependency.properties:
            self.apply_partial_inheritance(prop)

        # Then implicit inheritance
        # self.apply_implicit_inheritance(hosts)
        for h in self:
            h.get_customs_properties_by_inheritance(self)

########NEW FILE########
__FILENAME__ = hostescalation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items
from escalation import Escalation

from shinken.property import IntegerProp, StringProp, ListProp


class Hostescalation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'hostescalation'

    properties = Item.properties.copy()
    properties.update({
        'host_name':             StringProp(),
        'hostgroup_name':        StringProp(),
        'first_notification':    IntegerProp(),
        'last_notification':     IntegerProp(),
        'notification_interval': IntegerProp(default='30'), # like Nagios value
        'escalation_period':     StringProp(default=''),
        'escalation_options':    ListProp(default='d,u,r,w,c'),
        'contacts':              StringProp(),
        'contact_groups':        StringProp(),
    })

    # For debugging purpose only (nice name)
    def get_name(self):
        return ''


class Hostescalations(Items):
    name_property = ""
    inner_class = Hostescalation

    # We look for contacts property in contacts and
    def explode(self, escalations):
        # Now we explode all escalations (host_name, service_description) to escalations
        for es in self:
            properties = es.__class__.properties
            name = getattr(es, 'host_name', getattr(es, 'hostgroup_name', ''))
            creation_dict = {'escalation_name': 'Generated-Hostescalation-%d-%s' % (es.id, name)}
            for prop in properties:
                if hasattr(es, prop):
                    creation_dict[prop] = getattr(es, prop)
            s = Escalation(creation_dict)
            escalations.add_escalation(s)

        #print "All escalations"
        #for es in escalations:
        #    print es

########NEW FILE########
__FILENAME__ = hostextinfo
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This is the main class for the Host ext info. In fact it's mainly
about the configuration part. Parameters are merged in Hosts so it's
no use in running part
"""

import time

from item import Item, Items

from shinken.autoslots import AutoSlots
from shinken.util import format_t_into_dhms_format, to_hostnames_list, get_obj_name, to_svc_hst_distinct_lists, to_list_string_of_names
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.macroresolver import MacroResolver
from shinken.eventhandler import EventHandler
from shinken.log import logger


class HostExtInfo(Item):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    id = 1  # zero is reserved for host (primary node for parents)
    my_type = 'hostextinfo'

    # properties defined by configuration
    # *required: is required in conf
    # *default: default value if no set in conf
    # *pythonize: function to call when transforming string to python object
    # *fill_brok: if set, send to broker. there are two categories: full_status for initial and update status, check_result for check results
    # *no_slots: do not take this property for __slots__
    #  Only for the initial call
    # conf_send_preparation: if set, will pass the property to this function. It's used to "flatten"
    #  some dangerous properties like realms that are too 'linked' to be send like that.
    # brok_transformation: if set, will call the function with the value of the property
    #  the major times it will be to flatten the data (like realm_name instead of the realm object).
    properties = Item.properties.copy()
    properties.update({
        'host_name':            ListProp(brok_transformation=to_hostnames_list),
        'notes':                StringProp(default=''),
        'notes_url':            StringProp(default=''),
        'icon_image':           StringProp(default=''),
        'icon_image_alt':       StringProp(default=''),
        'vrml_image':           StringProp(default=''),
        'statusmap_image':      StringProp(default=''),

        # No slots for this 2 because begin property by a number seems bad
        # it's stupid!
        '2d_coords':            StringProp(default='', no_slots=True),
        '3d_coords':            StringProp(default='', no_slots=True),
    })

    # Hosts macros and prop that give the information
    # the prop can be callable or not
    macros = {
        'HOSTNAME':          'host_name',
        'HOSTNOTESURL':      'notes_url',
        'HOSTNOTES':         'notes',
    }

#######
#                   __ _                       _   _
#                  / _(_)                     | | (_)
#   ___ ___  _ __ | |_ _  __ _ _   _ _ __ __ _| |_ _  ___  _ __
#  / __/ _ \| '_ \|  _| |/ _` | | | | '__/ _` | __| |/ _ \| '_ \
# | (_| (_) | | | | | | | (_| | |_| | | | (_| | |_| | (_) | | | |
#  \___\___/|_| |_|_| |_|\__, |\__,_|_|  \__,_|\__|_|\___/|_| |_|
#                         __/ |
#                        |___/
######


    # Check is required prop are set:
    # host_name is needed
    def is_correct(self):
        state = True
        cls = self.__class__

        return state

    # For get a nice name
    def get_name(self):
        if not self.is_tpl():
            try:
                return self.host_name
            except AttributeError:  # outch, no hostname
                return 'UNNAMEDHOST'
        else:
            try:
                return self.name
            except AttributeError:  # outch, no name for this template
                return 'UNNAMEDHOSTTEMPLATE'

    # For debugging purpose only
    def get_dbg_name(self):
        return self.host_name

    # Same but for clean call, no debug
    def get_full_name(self):
        return self.host_name


# Class for the hosts lists. It's mainly for configuration
# part
class HostsExtInfo(Items):
    name_property = "host_name"  # use for the search by name
    inner_class = HostExtInfo  # use for know what is in items

    # Merge extended host information into host
    def merge(self, hosts):
        for ei in self:
            hosts_names = ei.get_name().split(",")
            for host_name in hosts_names:
                h = hosts.find_by_name(host_name)
                if h is not None:
                    # FUUUUUUUUUUsion
                    self.merge_extinfo(h, ei)

    def merge_extinfo(self, host, extinfo):
        properties = ['notes', 'notes_url', 'icon_image', 'icon_image_alt', 'vrml_image', 'statusmap_image']
        # host properties have precedence over hostextinfo properties
        for p in properties:
            if getattr(host, p) == '' and getattr(extinfo, p) != '':
                setattr(host, p, getattr(extinfo, p))

########NEW FILE########
__FILENAME__ = hostgroup
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from itemgroup import Itemgroup, Itemgroups

from shinken.util import get_obj_name
from shinken.property import StringProp
from shinken.log import logger


class Hostgroup(Itemgroup):
    id = 1  # zero is always a little bit special... like in database
    my_type = 'hostgroup'

    properties = Itemgroup.properties.copy()
    properties.update({
        'id':             StringProp(default=0, fill_brok=['full_status']),
        'hostgroup_name': StringProp(fill_brok=['full_status']),
        'alias':          StringProp(fill_brok=['full_status']),
        'notes':          StringProp(default='', fill_brok=['full_status']),
        'notes_url':      StringProp(default='', fill_brok=['full_status']),
        'action_url':     StringProp(default='', fill_brok=['full_status']),
        'realm':          StringProp(default='', fill_brok=['full_status'], conf_send_preparation=get_obj_name),
    })

    macros = {
        'HOSTGROUPALIAS':     'alias',
        'HOSTGROUPMEMBERS':   'members',
        'HOSTGROUPNOTES':     'notes',
        'HOSTGROUPNOTESURL':  'notes_url',
        'HOSTGROUPACTIONURL': 'action_url'
    }

    def get_name(self):
        return self.hostgroup_name

    def get_hosts(self):
        return getattr(self, 'members', '')

    def get_hostgroup_members(self):
        if self.has('hostgroup_members'):
            return self.hostgroup_members.split(',')
        else:
            return []

    # We fillfull properties with template ones if need
    # Because hostgroup we call may not have it's members
    # we call get_hosts_by_explosion on it
    def get_hosts_by_explosion(self, hostgroups):
        # First we tag the hg so it will not be explode
        # if a son of it already call it
        self.already_explode = True

        # Now the recursive part
        # rec_tag is set to False every HG we explode
        # so if True here, it must be a loop in HG
        # calls... not GOOD!
        if self.rec_tag:
            logger.error("[hostgroup::%s] got a loop in hostgroup definition" % self.get_name())
            return self.get_hosts()

        # Ok, not a loop, we tag it and continue
        self.rec_tag = True

        hg_mbrs = self.get_hostgroup_members()
        for hg_mbr in hg_mbrs:
            hg = hostgroups.find_by_name(hg_mbr.strip())
            if hg is not None:
                value = hg.get_hosts_by_explosion(hostgroups)
                if value is not None:
                    self.add_string_member(value)

        return self.get_hosts()


class Hostgroups(Itemgroups):
    name_property = "hostgroup_name"  # is used for finding hostgroups
    inner_class = Hostgroup

    def get_members_by_name(self, hgname):
        hg = self.find_by_name(hgname)
        if hg is None:
            return []
        return hg.get_hosts()


    def linkify(self, hosts=None, realms=None):
        self.linkify_hg_by_hst(hosts)
        self.linkify_hg_by_realms(realms)


    # We just search for each hostgroup the id of the hosts
    # and replace the name by the id
    def linkify_hg_by_hst(self, hosts):
        for hg in self:
            mbrs = hg.get_hosts()
            # The new member list, in id
            new_mbrs = []

            for mbr in mbrs:
                if mbr == '*':
                    new_mbrs.extend(hosts)
                else:
                    h = hosts.find_by_name(mbr)
                    if h is not None:
                        new_mbrs.append(h)
                    else:
                        hg.unknown_members.append(mbr)

            # Make members uniq
            new_mbrs = list(set(new_mbrs))

            # We find the id, we replace the names
            hg.replace_members(new_mbrs)

            # Now register us in our members
            for h in hg.members:
                h.hostgroups.append(hg)
                # and be sure we are uniq in it
                h.hostgroups = list(set(h.hostgroups))

    # More than an explode function, but we need to already
    # have members so... Will be really linkify just after
    # And we explode realm in ours members, but do not override
    # a host realm value if it's already set
    def linkify_hg_by_realms(self, realms):
        # Now we explode the realm value if we've got one
        # The group realm must not override a host one (warning?)
        for hg in self:
            if not hasattr(hg, 'realm'):
                continue

            # Maybe the value is void?
            if not hg.realm.strip():
                continue

            r = realms.find_by_name(hg.realm.strip())
            if r is not None:
                hg.realm = r
                logger.debug("[hostgroups] %s is in %s realm" % (hg.get_name(), r.get_name()))
            else:
                err = "the hostgroup %s got an unknown realm '%s'" % (hg.get_name(), hg.realm)
                hg.configuration_errors.append(err)
                hg.realm = None
                continue

            for h in hg:
                if h is None:
                    continue
                if h.realm is None or h.got_default_realm:  # default value not hasattr(h, 'realm'):
                    logger.debug("[hostgroups] apply a realm %s to host %s from a hostgroup rule (%s)" % \
                        (hg.realm.get_name(), h.get_name(), hg.get_name()))
                    h.realm = hg.realm
                else:
                    if h.realm != hg.realm:
                        logger.warning("[hostgroups] host %s it not in the same realm than it's hostgroup %s" % \
                            (h.get_name(), hg.get_name()))

    # Add a host string to a hostgroup member
    # if the host group do not exist, create it
    def add_member(self, hname, hgname):
        hg = self.find_by_name(hgname)
        # if the id do not exist, create the hg
        if hg is None:
            hg = Hostgroup({'hostgroup_name': hgname, 'alias': hgname, 'members': hname})
            self.add(hg)
        else:
            hg.add_string_member(hname)

    # Use to fill members with hostgroup_members
    def explode(self):
        # We do not want a same hg to be explode again and again
        # so we tag it
        for tmp_hg in self.items.values():
            tmp_hg.already_explode = False
        for hg in self.items.values():
            if hg.has('hostgroup_members') and not hg.already_explode:
                # get_hosts_by_explosion is a recursive
                # function, so we must tag hg so we do not loop
                for tmp_hg in self.items.values():
                    tmp_hg.rec_tag = False
                hg.get_hosts_by_explosion(self)

        # We clean the tags
        for tmp_hg in self.items.values():
            if hasattr(tmp_hg, 'rec_tag'):
                del tmp_hg.rec_tag
            del tmp_hg.already_explode

########NEW FILE########
__FILENAME__ = item
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This class is a base class for nearly all configuration
 elements like service, hosts or contacts.
"""
import time
import cPickle  # for hashing compute

# Try to import md5 function
try:
    from hashlib import md5
except ImportError:
    from md5 import md5

from copy import copy

from shinken.graph import Graph
from shinken.commandcall import CommandCall
from shinken.property import StringProp, ListProp, BoolProp, IntegerProp
from shinken.brok import Brok
from shinken.util import strip_and_uniq
from shinken.acknowledge import Acknowledge
from shinken.comment import Comment
from shinken.complexexpression import ComplexExpressionFactory
from shinken.log import logger



class Item(object):

    properties = {
        'imported_from':            StringProp(default='unknown'),
        'use':                      ListProp(default=''),
        'name':                     StringProp(default=''),
        'definition_order':         IntegerProp(default='100'),
        # TODO: find why we can't uncomment this line below.
        #'register':                 BoolProp(default='1'),
    }

    running_properties = {
        # All errors and warning raised during the configuration parsing
        # and that will raised real warning/errors during the is_correct
        'configuration_warnings':   ListProp(default=[]),
        'configuration_errors':     ListProp(default=[]),
        'hash':   StringProp(default=''),
        # We save all template we asked us to load from
        'tags': ListProp(default=set(), fill_brok=['full_status']),
    }

    macros = {
    }

    def __init__(self, params={}):
        # We have our own id of My Class type :)
        # use set attr for going into the slots
        # instead of __dict__ :)
        cls = self.__class__
        self.id = cls.id
        cls.id += 1

        self.customs = {}  # for custom variables
        self.plus = {}  # for value with a +

        self.init_running_properties()

        # [0] = +  -> new key-plus
        # [0] = _  -> new custom entry in UPPER case
        for key in params:
            # delistify attributes if there is only one value
            params[key] = self.compact_unique_attr_value(params[key])
            # checks for attribute value special syntax (+ or _)
            if not isinstance(params[key], list) and \
               len(params[key]) >= 1 and params[key][0] == '+':
                # Special case: a _MACRO can be a plus. so add to plus
                # but upper the key for the macro name
                if key[0] == "_":
                    self.plus[key.upper()] = params[key][1:]  # we remove the +
                else:
                    self.plus[key] = params[key][1:]  # we remove the +
            elif key[0] == "_":
                if isinstance(params[key], list):
                    err = "no support for _ syntax in multiple valued attributes"
                    self.configuration_errors.append(err)
                    continue
                custom_name = key.upper()
                self.customs[custom_name] = params[key]
            else:
                setattr(self, key, params[key])


    # When values to set on attributes are unique (single element list),
    # return the value directly rather than setting list element.
    def compact_unique_attr_value(self, val):
        if isinstance(val, list):
            if len(val) > 1:
                return val
            elif len(val) == 0:
                return ''
            else:
                return val[0]
        else:
            return val

    def init_running_properties(self):
        for prop, entry in self.__class__.running_properties.items():
            # Copy is slow, so we check type
            # Type with __iter__ are list or dict, or tuple.
            # Item need it's own list, so we copy
            val = entry.default
            if hasattr(val, '__iter__'):
                setattr(self, prop, copy(val))
            else:
                setattr(self, prop, val)
            # each instance to have his own running prop!

    def copy(self):
        """ Return a copy of the item, but give him a new id """
        cls = self.__class__
        i = cls({})  # Dummy item but with it's own running properties
        for prop in cls.properties:
            if hasattr(self, prop):
                val = getattr(self, prop)
                setattr(i, prop, val)
        # Also copy the customs tab
        i.customs = copy(self.customs)
        return i

    def clean(self):
        """ Clean useless things not requested once item has been fully initialized&configured.
Like temporary attributes such as "imported_from", etc.. """
        for name in ('imported_from', 'use', 'plus', 'templates',):
            try:
                delattr(self, name)
            except AttributeError:
                pass

    def __str__(self):
        return str(self.__dict__) + '\n'

    def is_tpl(self):
        """ Return if the elements is a template """
        try:
            return self.register == '0'
        except Exception, exp:
            return False

    # If a prop is absent and is not required, put the default value
    def fill_default(self):
        """ Fill missing properties if they are missing """
        cls = self.__class__

        for prop, entry in cls.properties.items():
            if not hasattr(self, prop) and entry.has_default:
                setattr(self, prop, entry.default)

    # We load every useful parameter so no need to access global conf later
    # Must be called after a change in a global conf parameter
    def load_global_conf(cls, conf):
        """ Used to put global values in the sub Class like
        hosts or services """
        # conf have properties, if 'enable_notifications':
        # { [...] 'class_inherit': [(Host, None), (Service, None),
        #  (Contact, None)]}
        # get the name and put the value if None, put the Name
        # (not None) if not (not clear?)
        for prop, entry in conf.properties.items():
            # If we have a class_inherit, and the arbiter really send us it
            # if 'class_inherit' in entry and hasattr(conf, prop):
            if hasattr(conf, prop):
                for (cls_dest, change_name) in entry.class_inherit:
                    if cls_dest == cls:  # ok, we've got something to get
                        value = getattr(conf, prop)
                        if change_name is None:
                            setattr(cls, prop, value)
                        else:
                            setattr(cls, change_name, value)

    # Make this method a classmethod
    load_global_conf = classmethod(load_global_conf)

    # Use to make python properties
    def pythonize(self):
        cls = self.__class__
        for prop, tab in cls.properties.items():
            try:
                new_val = tab.pythonize(getattr(self, prop))
                setattr(self, prop, new_val)
            except AttributeError, exp:
                #print exp
                pass  # Will be catch at the is_correct moment
            except KeyError, exp:
                #print "Missing prop value", exp
                err = "the property '%s' of '%s' do not have value" % (prop, self.get_name())
                self.configuration_errors.append(err)
            except ValueError, exp:
                err = "incorrect type for property '%s' of '%s'" % (prop, self.get_name())
                self.configuration_errors.append(err)

    # Compute a hash of this element values. Should be launched
    # When we got all our values, but not linked with other objects
    def compute_hash(self):
        # ID will always changed between runs, so we remove it
        # for hash compute
        i = self.id
        del self.id
        m = md5()
        tmp = cPickle.dumps(self, cPickle.HIGHEST_PROTOCOL)
        m.update(tmp)
        self.hash = m.digest()
        # and put again our id
        self.id = i

    def get_templates(self):
        if hasattr(self, 'use') and self.use != '':
            if isinstance(self.use, list):
                return self.use
            else:
                return self.use.split(',')
        else:
            return []


    # We fillfull properties with template ones if need
    def get_property_by_inheritance(self, items, prop):

        # If I have the prop, I take mine but I check if I must
        # add a plus property
        if hasattr(self, prop):
            value = getattr(self, prop)
            # Maybe this value is 'null'. If so, we should NOT inherit
            # and just delete this entry, and hope of course.
            # Keep "null" values, because in "inheritance chaining" they must
            # be passed from one level to the next.
            #if value == 'null':
            #    delattr(self, prop)
            #    return None
            # Manage the additive inheritance for the property,
            # if property is in plus, add or replace it
            # Template should keep the '+' at the beginning of the chain
            if self.has_plus(prop):
                value = self.get_plus_and_delete(prop) + ',' + value
                if self.is_tpl():
                    value = '+' + value
            return value
        # Ok, I do not have prop, Maybe my templates do?
        # Same story for plus
        for i in self.templates:
            value = i.get_property_by_inheritance(items, prop)

            if value is not None:
                # If our template give us a '+' value, we should continue to loop
                still_loop = False
                if not isinstance(value, list) and value.startswith('+'):
                    # Templates should keep their + inherited from their parents
                    if not self.is_tpl():
                        value = value[1:]
                    still_loop = True

                # Maybe in the previous loop, we set a value, use it too
                if hasattr(self, prop):
                    # If the current value is strong, it will simplify the problem
                    if not isinstance(value, list) and value.startswith('+'):
                        # In this case we can remove the + from our current
                        # tpl because our value will be final
                        value = ','.join([getattr(self, prop), value[1:]])
                    else: # If not, se should keep the + sign of need
                        value = ','.join([getattr(self, prop), value])


                # Ok, we can set it
                setattr(self, prop, value)

                # If we only got some '+' values, we must still loop
                # for an end value without it
                if not still_loop:
                    # And set my own value in the end if need
                    if self.has_plus(prop):
                        value = ','.join([getattr(self, prop), self.get_plus_and_delete(prop)])
                        # Template should keep their '+'
                        if self.is_tpl() and not value.startswith('+'):
                            value = '+' + value
                        setattr(self, prop, value)
                    return value

        # Maybe templates only give us + values, so we didn't quit, but we already got a
        # self.prop value after all
        template_with_only_plus = hasattr(self, prop)

        # I do not have endingprop, my templates too... Maybe a plus?
        # warning: if all my templates gave me '+' values, do not forgot to
        # add the already set self.prop value
        if self.has_plus(prop):
            if template_with_only_plus:
                value = ','.join([getattr(self, prop), self.get_plus_and_delete(prop)])
            else:
                value = self.get_plus_and_delete(prop)
            # Template should keep their '+' chain
            # We must say it's a '+' value, so our son will now that it must
            # still loop
            if self.is_tpl() and not value.startswith('+'):
                value = '+' + value
            setattr(self, prop, value)
            return value

        # Ok so in the end, we give the value we got if we have one, or None
        # Not even a plus... so None :)
        return getattr(self, prop, None)


    # We fillfull properties with template ones if need
    def get_customs_properties_by_inheritance(self, items):
        for i in self.templates:
            tpl_cv = i.get_customs_properties_by_inheritance(items)
            if tpl_cv is not {}:
                for prop in tpl_cv:
                    if prop not in self.customs:
                        value = tpl_cv[prop]
                    else:
                        value = self.customs[prop]
                    if self.has_plus(prop):
                        value = self.get_plus_and_delete(prop) + ',' + value
                    self.customs[prop] = value
        for prop in self.customs:
            value = self.customs[prop]
            if self.has_plus(prop):
                value = self.get_plus_and_delete(prop) + ',' + value
                self.customs[prop] = value
        # We can get custom properties in plus, we need to get all
        # entires and put
        # them into customs
        cust_in_plus = self.get_all_plus_and_delete()
        for prop in cust_in_plus:
            self.customs[prop] = cust_in_plus[prop]
        return self.customs


    def has_plus(self, prop):
        try:
            self.plus[prop]
        except:
            return False
        return True


    def get_all_plus_and_delete(self):
        res = {}
        props = self.plus.keys()  # we delete entries, so no for ... in ...
        for prop in props:
            res[prop] = self.get_plus_and_delete(prop)
        return res


    def get_plus_and_delete(self, prop):
        val = self.plus[prop]
        del self.plus[prop]
        return val


    # Check is required prop are set:
    # template are always correct
    def is_correct(self):
        state = True
        properties = self.__class__.properties

        # Raised all previously saw errors like unknown contacts and co
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[item::%s] %s" % (self.get_name(), err))

        for prop, entry in properties.items():
            if not hasattr(self, prop) and entry.required:
                logger.warning("[item::%s] %s property is missing" % (self.get_name(), prop))
                state = False

        return state


    # This function is used by service and hosts
    # to transform Nagios2 parameters to Nagios3
    # ones, like normal_check_interval to
    # check_interval. There is a old_parameters tab
    # in Classes that give such modifications to do.
    def old_properties_names_to_new(self):
        old_properties = self.__class__.old_properties
        for old_name, new_name in old_properties.items():
            # Ok, if we got old_name and NO new name,
            # we switch the name
            if hasattr(self, old_name) and not hasattr(self, new_name):
                value = getattr(self, old_name)
                setattr(self, new_name, value)

    # The arbiter is asking us our raw value before all explode or linking
    def get_raw_import_values(self):
        r = {}
        properties = self.__class__.properties.keys()
        # Register is not by default in the properties
        if not 'register' in properties:
            properties.append('register')

        for prop in properties:
            if hasattr(self, prop):
                v = getattr(self, prop)
                #print prop, ":", v
                r[prop] = v
        return r


    def add_downtime(self, downtime):
        self.downtimes.append(downtime)

    def del_downtime(self, downtime_id):
        d_to_del = None
        for dt in self.downtimes:
            if dt.id == downtime_id:
                d_to_del = dt
                dt.can_be_deleted = True
        if d_to_del is not None:
            self.downtimes.remove(d_to_del)

    def add_comment(self, comment):
        self.comments.append(comment)

    def del_comment(self, comment_id):
        c_to_del = None
        for c in self.comments:
            if c.id == comment_id:
                c_to_del = c
                c.can_be_deleted = True
        if c_to_del is not None:
            self.comments.remove(c_to_del)

    def acknowledge_problem(self, sticky, notify, persistent, author, comment, end_time=0):
        if self.state != self.ok_up:
            if notify:
                self.create_notifications('ACKNOWLEDGEMENT')
            self.problem_has_been_acknowledged = True
            if sticky == 2:
                sticky = True
            else:
                sticky = False
            a = Acknowledge(self, sticky, notify, persistent, author, comment, end_time=end_time)
            self.acknowledgement = a
            if self.my_type == 'host':
                comment_type = 1
            else:
                comment_type = 2
            c = Comment(self, persistent, author, comment,
                        comment_type, 4, 0, False, 0)
            self.add_comment(c)
            self.broks.append(self.get_update_status_brok())

    # Look if we got an ack that is too old with an expire date and should
    # be delete
    def check_for_expire_acknowledge(self):
        if self.acknowledgement and self.acknowledgement.end_time != 0 and self.acknowledgement.end_time < time.time():
            self.unacknowledge_problem()

    #  Delete the acknowledgement object and reset the flag
    #  but do not remove the associated comment.
    def unacknowledge_problem(self):
        if self.problem_has_been_acknowledged:
            logger.debug("[item::%s] deleting acknowledge of %s" % (self.get_name(), self.get_dbg_name()))
            self.problem_has_been_acknowledged = False
            # Should not be deleted, a None is Good
            self.acknowledgement = None
            # del self.acknowledgement
            # find comments of non-persistent ack-comments and delete them too
            for c in self.comments:
                if c.entry_type == 4 and not c.persistent:
                    self.del_comment(c.id)
            self.broks.append(self.get_update_status_brok())

    # Check if we have an acknowledgement and if this is marked as sticky.
    # This is needed when a non-ok state changes
    def unacknowledge_problem_if_not_sticky(self):
        if hasattr(self, 'acknowledgement') and self.acknowledgement is not None:
            if not self.acknowledgement.sticky:
                self.unacknowledge_problem()

    # Will flatten some parameters tagged by the 'conf_send_preparation'
    # property because they are too "linked" to be send like that (like realms)
    def prepare_for_conf_sending(self):
        cls = self.__class__

        for prop, entry in cls.properties.items():
            # Is this property need preparation for sending?
            if entry.conf_send_preparation is not None:
                f = entry.conf_send_preparation
                if f is not None:
                    val = f(getattr(self, prop))
                    setattr(self, prop, val)

        if hasattr(cls, 'running_properties'):
            for prop, entry in cls.running_properties.items():
            # Is this property need preparation for sending?
                if entry.conf_send_preparation is not None:
                    f = entry.conf_send_preparation
                    if f is not None:
                        val = f(getattr(self, prop))
                        setattr(self, prop, val)

    # Get the property for an object, with good value
    # and brok_transformation if need
    def get_property_value_for_brok(self, prop, tab):
        entry = tab[prop]
        # Get the current value, or the default if need
        value = getattr(self, prop, entry.default)

        # Apply brok_transformation if need
        # Look if we must preprocess the value first
        pre_op = entry.brok_transformation
        if pre_op is not None:
            value = pre_op(self, value)

        return value

    # Fill data with info of item by looking at brok_type
    # in props of properties or running_properties
    def fill_data_brok_from(self, data, brok_type):
        cls = self.__class__
        # Now config properties
        for prop, entry in cls.properties.items():
            # Is this property intended for broking?
            if brok_type in entry.fill_brok:
                data[prop] = self.get_property_value_for_brok(prop, cls.properties)

        # Maybe the class do not have running_properties
        if hasattr(cls, 'running_properties'):
            # We've got prop in running_properties too
            for prop, entry in cls.running_properties.items():
                #if 'fill_brok' in cls.running_properties[prop]:
                if brok_type in entry.fill_brok:
                    data[prop] = self.get_property_value_for_brok(prop, cls.running_properties)

    # Get a brok with initial status
    def get_initial_status_brok(self):
        cls = self.__class__
        my_type = cls.my_type
        data = {'id': self.id}

        self.fill_data_brok_from(data, 'full_status')
        b = Brok('initial_' + my_type + '_status', data)
        return b

    # Get a brok with update item status
    def get_update_status_brok(self):
        cls = self.__class__
        my_type = cls.my_type

        data = {'id': self.id}
        self.fill_data_brok_from(data, 'full_status')
        b = Brok('update_' + my_type + '_status', data)
        return b

    # Get a brok with check_result
    def get_check_result_brok(self):
        cls = self.__class__
        my_type = cls.my_type

        data = {}
        self.fill_data_brok_from(data, 'check_result')
        b = Brok(my_type + '_check_result', data)
        return b

    # Get brok about the new schedule (next_check)
    def get_next_schedule_brok(self):
        cls = self.__class__
        my_type = cls.my_type

        data = {}
        self.fill_data_brok_from(data, 'next_schedule')
        b = Brok(my_type + '_next_schedule', data)
        return b

    # Link one command property to a class (for globals like oc*p_command)
    def linkify_one_command_with_commands(self, commands, prop):
        if hasattr(self, prop):
            command = getattr(self, prop).strip()
            if command != '':
                if hasattr(self, 'poller_tag'):
                    cmdCall = CommandCall(commands, command,
                                          poller_tag=self.poller_tag)
                elif hasattr(self, 'reactionner_tag'):
                    cmdCall = CommandCall(commands, command,
                                          reactionner_tag=self.reactionner_tag)
                else:
                    cmdCall = CommandCall(commands, command)
                    setattr(self, prop, cmdCall)
            else:
                setattr(self, prop, None)



    # We look at the 'trigger' prop and we create a trigger for it
    def explode_trigger_string_into_triggers(self, triggers):
        src = getattr(self, 'trigger', '')
        if src:
            # Change on the fly the characters
            src = src.replace(r'\n', '\n').replace(r'\t', '\t')
            t = triggers.create_trigger(src, 'inner-trigger-' + self.__class__.my_type + '' + str(self.id))
            if t:
                # Maybe the trigger factory give me a already existing trigger,
                # so my name can be dropped
                self.triggers.append(t.get_name())

    # Link with triggers. Can be with a "in source" trigger, or a file name
    def linkify_with_triggers(self, triggers):
        # Get our trigger string and trigger names in the same list
        self.triggers.extend(self.trigger_name)
        #print "I am linking my triggers", self.get_full_name(), self.triggers
        new_triggers = []
        for tname in self.triggers:
            t = triggers.find_by_name(tname)
            if t:
                setattr(t, 'trigger_broker_raise_enabled', self.trigger_broker_raise_enabled)
                new_triggers.append(t)
            else:
                self.configuration_errors.append('the %s %s does have a unknown trigger_name "%s"' % (self.__class__.my_type, self.get_full_name(), tname))
        self.triggers = new_triggers


class Items(object):
    def __init__(self, items):
        self.items = {}
        self.configuration_warnings = []
        self.configuration_errors = []
        for i in items:
            self.items[i.id] = i
        self.templates = {}
        # We should keep a graph of templates relations
        self.templates_graph = Graph()

    def __iter__(self):
        return self.items.itervalues()

    def __len__(self):
        return len(self.items)

    def __delitem__(self, key):
        try:
            del self.items[key]
        except KeyError:  # we don't want it, we do not have it. All is perfect
            pass

    def __setitem__(self, key, value):
        self.items[key] = value

    def __getitem__(self, key):
        return self.items[key]

    def __contains__(self, key):
        return key in self.items

    def compute_hash(self):
        for i in self:
            i.compute_hash()


    # We create the reversed list so search will be faster
    # We also create a twins list with id of twins (not the original
    # just the others, higher twins)
    def create_reversed_list(self):
        self.reversed_list = {}
        self.twins = []
        name_property = self.__class__.name_property
        for id in self.items:
            if hasattr(self.items[id], name_property):
                name = getattr(self.items[id], name_property)
                if name not in self.reversed_list:
                    self.reversed_list[name] = id
                else:
                    self.twins.append(id)


    def find_id_by_name(self, name):
        if hasattr(self, 'reversed_list'):
            if name in self.reversed_list:
                return self.reversed_list[name]
            else:
                return None
        else:  # ok, an early ask, with no reversed list from now...
            name_property = self.__class__.name_property
            for i in self:
                if hasattr(i, name_property):
                    i_name = getattr(i, name_property)
                    if i_name == name:
                        return i.id
            return None


    def find_by_name(self, name):
        id = self.find_id_by_name(name)
        if id is not None:
            return self.items[id]
        else:
            return None


    # Search items using a list of filter callbacks. Each callback is passed
    # the item instances and should return a boolean value indicating if it
    # matched the filter.
    # Returns a list of items matching all filters.
    def find_by_filter(self, filters):
        items = []
        for i in self:
            failed = False
            for f in filters:
                if not f(i):
                    failed = True
                    break
            if failed is False:
                items.append(i)
        return items


    # prepare_for_conf_sending to flatten some properties
    def prepare_for_sending(self):
        for i in self:
            i.prepare_for_conf_sending()

    
    # It's used to change old Nagios2 names to
    # Nagios3 ones
    def old_properties_names_to_new(self):
        for i in self:
            i.old_properties_names_to_new()

    def pythonize(self):
        for id in self.items:
            self.items[id].pythonize()

    def create_tpl_list(self):
        for id in self.items:
            i = self.items[id]
            if i.is_tpl():
                self.templates[id] = i

    def find_tpl_by_name(self, name):
        for i in self.templates.values():
            if hasattr(i, 'name') and i.name == name:
                return i
        return None

    # We will link all templates, and create the template
    # graph too
    def linkify_templates(self):
        # First we create a list of all templates
        self.create_tpl_list()
        for i in self:
            tpls = i.get_templates()
            new_tpls = []
            for tpl in tpls:
                tpl = tpl.strip()
                # We save this template in the 'tags' set
                i.tags.add(tpl)
                # Then we link it
                t = self.find_tpl_by_name(tpl)
                # If it's ok, add the template and update the
                # template graph too
                if t is not None:
                    # add the template object to us
                    new_tpls.append(t)
                else:  # not find? not good!
                    err = "the template '%s' defined for '%s' is unknown" % (tpl, i.get_name())
                    i.configuration_warnings.append(err)
            i.templates = new_tpls

        # Now we will create the template graph, so
        # we look only for templates here. First we should declare our nodes
        for tpl in self.templates.values():
            self.templates_graph.add_node(tpl)
        # And then really create our edge
        for tpl in self.templates.values():
            for father in tpl.templates:
                self.templates_graph.add_edge(father, tpl)

    def is_correct(self):
        # we are ok at the beginning. Hope we still ok at the end...
        r = True
        # Some class do not have twins, because they do not have names
        # like servicedependencies
        twins = getattr(self, 'twins', None)
        if twins is not None:
            # Ok, look at no twins (it's bad!)
            for id in twins:
                i = self.items[id]
                logger.warning("[items] %s.%s is duplicated from %s" %\
                    (i.__class__.my_type, i.get_name(), getattr(i, 'imported_from', "unknown source")))

        # Then look if we have some errors in the conf
        # Juts print warnings, but raise errors
        for err in self.configuration_warnings:
            logger.warning("[items] %s" % err)

        for err in self.configuration_errors:
            logger.error("[items] %s" % err)
            r = False

        # Then look for individual ok
        for i in self:
            # Alias and display_name hook hook
            prop_name = getattr(self.__class__, 'name_property', None)
            if prop_name and not hasattr(i, 'alias') and hasattr(i, prop_name):
                setattr(i, 'alias', getattr(i, prop_name))
            if prop_name and getattr(i, 'display_name', '') == '' and hasattr(i, prop_name):
                setattr(i, 'display_name', getattr(i, prop_name))

            # Now other checks
            if not i.is_correct():
                n = getattr(i, 'imported_from', "unknown source")
                logger.error("[items] In %s is incorrect ; from %s" % (i.get_name(), n))
                r = False

        return r

    def remove_templates(self):
        """ Remove useless templates (& properties) of our items ; otherwise we could get errors on config.is_correct() """
        tpls = [i for i in self if i.is_tpl()]
        for i in tpls:
            del self[i.id]
        del self.templates
        del self.templates_graph

    def clean(self):
        """ Request to remove the unnecessary attributes/others from our items """
        for i in self:
            i.clean()
        Item.clean(self)

    # If a prop is absent and is not required, put the default value
    def fill_default(self):
        for i in self:
            i.fill_default()

    def __str__(self):
        s = ''
        cls = self.__class__
        for id in self.items:
            s = s + str(cls) + ':' + str(id) + str(self.items[id]) + '\n'
        return s

    # Inheritance for just a property
    def apply_partial_inheritance(self, prop):
        for i in self:
            i.get_property_by_inheritance(self, prop)
            if not i.is_tpl():
                # If a "null" attribute was inherited, delete it
                try:
                    if getattr(i, prop) == 'null':
                        delattr(i, prop)
                except:
                    pass

    def apply_inheritance(self):
        # We check for all Class properties if the host has it
        # if not, it check all host templates for a value
        cls = self.inner_class
        for prop in cls.properties:
            self.apply_partial_inheritance(prop)
        for i in self:
            i.get_customs_properties_by_inheritance(self)

    # We remove twins
    # Remember: item id respect the order of conf. So if and item
    #  is defined multiple times,
    # we want to keep the first one.
    # Services are also managed here but they are specials:
    # We remove twins services with the same host_name/service_description
    # Remember: host service are take into account first before hostgroup service
    # Id of host service are lower than hostgroups one, so they are
    # in self.twins_services
    # and we can remove them.
    def remove_twins(self):
        for id in self.twins:
            i = self.items[id]
            type = i.__class__.my_type
            logger.warning("[items] %s.%s is already defined '%s'" % (type, i.get_name(), getattr(i, 'imported_from', "unknown source")))
            del self[id]  # bye bye
        # do not remove twins, we should look in it, but just void it
        self.twins = []
        #del self.twins #no more need



    # We've got a contacts property with , separated contacts names
    # and we want have a list of Contacts
    def linkify_with_contacts(self, contacts):
        for i in self:
            if hasattr(i, 'contacts'):
                contacts_tab = i.contacts.split(',')
                contacts_tab = strip_and_uniq(contacts_tab)
                new_contacts = []
                for c_name in contacts_tab:
                    if c_name != '':
                        c = contacts.find_by_name(c_name)
                        if c is not None:
                            new_contacts.append(c)
                        # Else: Add in the errors tab.
                        # will be raised at is_correct
                        else:
                            err = "the contact '%s' defined for '%s' is unknown" % (c_name, i.get_name())
                            i.configuration_errors.append(err)
                # Get the list, but first make elements uniq
                i.contacts = list(set(new_contacts))

    # Make link between an object and its escalations
    def linkify_with_escalations(self, escalations):
        for i in self:
            if hasattr(i, 'escalations'):
                escalations_tab = i.escalations.split(',')
                escalations_tab = strip_and_uniq(escalations_tab)
                new_escalations = []
                for es_name in [e for e in escalations_tab if e != '']:
                    es = escalations.find_by_name(es_name)
                    if es is not None:
                        new_escalations.append(es)
                    else:  # Escalation not find, not good!
                        err = "the escalation '%s' defined for '%s' is unknown" % (es_name, i.get_name())
                        i.configuration_errors.append(err)
                i.escalations = new_escalations

    # Make link between item and it's resultmodulations
    def linkify_with_resultmodulations(self, resultmodulations):
        for i in self:
            if hasattr(i, 'resultmodulations'):
                resultmodulations_tab = i.resultmodulations.split(',')
                resultmodulations_tab = strip_and_uniq(resultmodulations_tab)
                new_resultmodulations = []
                for rm_name in resultmodulations_tab:
                    rm = resultmodulations.find_by_name(rm_name)
                    if rm is not None:
                        new_resultmodulations.append(rm)
                    else:
                        err = "the result modulation '%s' defined on the %s '%s' do not exist" % (rm_name, i.__class__.my_type, i.get_name())
                        i.configuration_warnings.append(err)
                        continue
                i.resultmodulations = new_resultmodulations

    # Make link between item and it's business_impact_modulations
    def linkify_with_business_impact_modulations(self, business_impact_modulations):
        for i in self:
            if hasattr(i, 'business_impact_modulations'):
                business_impact_modulations_tab = i.business_impact_modulations.split(',')
                business_impact_modulations_tab = strip_and_uniq(business_impact_modulations_tab)
                new_business_impact_modulations = []
                for rm_name in business_impact_modulations_tab:
                    rm = business_impact_modulations.find_by_name(rm_name)
                    if rm is not None:
                        new_business_impact_modulations.append(rm)
                    else:
                        err = "the business impact modulation '%s' defined on the %s '%s' do not exist" % (rm_name, i.__class__.my_type, i.get_name())
                        i.configuration_errors.append(err)
                        continue
                i.business_impact_modulations = new_business_impact_modulations

    # If we've got a contact_groups properties, we search for all
    # theses groups and ask them their contacts, and then add them
    # all into our contacts property
    def explode_contact_groups_into_contacts(self, contactgroups):
        for i in self:
            if hasattr(i, 'contact_groups'):
                if isinstance(i.contact_groups, list):
                    cgnames = i.contact_groups
                else:
                    cgnames = i.contact_groups.split(',')
                cgnames = strip_and_uniq(cgnames)
                for cgname in cgnames:
                    cg = contactgroups.find_by_name(cgname)
                    if cg is None:
                        err = "The contact group '%s' defined on the %s '%s' do not exist" % (cgname, i.__class__.my_type, i.get_name())
                        i.configuration_errors.append(err)
                        continue
                    cnames = contactgroups.get_members_by_name(cgname)
                    # We add contacts into our contacts
                    if cnames != []:
                        if hasattr(i, 'contacts'):
                            i.contacts += ',' + cnames
                        else:
                            i.contacts = cnames

    # Link a timeperiod property (prop)
    def linkify_with_timeperiods(self, timeperiods, prop):
        for i in self:
            if hasattr(i, prop):
                tpname = getattr(i, prop).strip()
                # some default values are '', so set None
                if tpname == '':
                    setattr(i, prop, None)
                    continue

                # Ok, get a real name, search for it
                tp = timeperiods.find_by_name(tpname)
                # If not found, it's an error
                if tp is None:
                    err = "The %s of the %s '%s' named '%s' is unknown!" % (prop, i.__class__.my_type, i.get_name(), tpname)
                    i.configuration_errors.append(err)
                    continue
                # Got a real one, just set it :)
                setattr(i, prop, tp)

    def create_commandcall(self,prop, commands, command):
        comandcall = dict(commands=commands, call=command)
        if hasattr(prop, 'enable_environment_macros'):
            comandcall['enable_environment_macros'] = prop.enable_environment_macros

        if hasattr(prop, 'poller_tag'):
            comandcall['poller_tag']=prop.poller_tag
        elif hasattr(prop, 'reactionner_tag'):
            comandcall['reactionner_tag']=prop.reactionner_tag

        return CommandCall(**comandcall)

    # Link one command property
    def linkify_one_command_with_commands(self, commands, prop):
        for i in self:
            if hasattr(i, prop):
                command = getattr(i, prop).strip()
                if command != '':
                    cmdCall = self.create_commandcall(i, commands, command)

                    # TODO: catch None?
                    setattr(i, prop, cmdCall)
                else:

                    setattr(i, prop, None)

    # Link a command list (commands with , between) in real CommandCalls
    def linkify_command_list_with_commands(self, commands, prop):
        for i in self:
            if hasattr(i, prop):
                coms = getattr(i, prop).split(',')
                coms = strip_and_uniq(coms)
                com_list = []
                for com in coms:
                    if com != '':
                        cmdCall = self.create_commandcall(i, commands, com)
                        # TODO: catch None?
                        com_list.append(cmdCall)
                    else:  # TODO: catch?
                        pass
                setattr(i, prop, com_list)

    # Link with triggers. Can be with a "in source" trigger, or a file name
    def linkify_with_triggers(self, triggers):
        for i in self:
            i.linkify_with_triggers(triggers)


    # We've got a notificationways property with , separated contacts names
    # and we want have a list of NotificationWay
    def linkify_with_checkmodulations(self, checkmodulations):
        for i in self:
            if not hasattr(i, 'checkmodulations'):
                continue
            new_checkmodulations = []
            for cw_name in i.checkmodulations:
                cw = checkmodulations.find_by_name(cw_name)
                if cw is not None:
                    new_checkmodulations.append(cw)
                else:
                    err = "The checkmodulations of the %s '%s' named '%s' is unknown!" % (i.__class__.my_type, i.get_name(), cw_name)
                    i.configuration_errors.append(err)
            # Get the list, but first make elements uniq
            i.checkmodulations = new_checkmodulations


    # We've got list of macro modulations as list of names, and
    # we want real objects
    def linkify_with_macromodulations(self, macromodulations):
        for i in self:
            if not hasattr(i, 'macromodulations'):
                continue
            new_macromodulations = []
            for cw_name in i.macromodulations:
                cw = macromodulations.find_by_name(cw_name)
                if cw is not None:
                    new_macromodulations.append(cw)
                else:
                    err = "The macromodulations of the %s '%s' named '%s' is unknown!" % (i.__class__.my_type, i.get_name(), cw_name)
                    i.configuration_errors.append(err)
            # Get the list, but first make elements uniq
            i.macromodulations = new_macromodulations


    # Linkify with modules
    def linkify_s_by_plug(self, modules):
        for s in self:
            new_modules = []
            for plug_name in s.modules:
                plug_name = plug_name.strip()
                # don't tread void names
                if plug_name == '':
                    continue

                plug = modules.find_by_name(plug_name)
                print plug
                if plug is not None:
                    new_modules.append(plug)
                else:
                    err = "Error: the module %s is unknown for %s" % (plug_name, s.get_name())
                    s.configuration_errors.append(err)
            s.modules = new_modules


    def evaluate_hostgroup_expression(self, expr, hosts, hostgroups, look_in='hostgroups'):
        #print "\n"*10, "looking for expression", expr
        # Maybe exp is a list, like numerous hostgroups entries in a service, link them
        if isinstance(expr, list):
            expr = '|'.join(expr)
        #print "\n"*10, "looking for expression", expr        
        if look_in=='hostgroups':
            f = ComplexExpressionFactory(look_in, hostgroups, hosts)
        else: # templates
            f = ComplexExpressionFactory(look_in, hosts, hosts)
        expr_tree = f.eval_cor_pattern(expr)

        #print "RES of ComplexExpressionFactory"
        #print expr_tree

        #print "Try to resolve the Tree"
        set_res = expr_tree.resolve_elements()
        #print "R2d2 final is", set_res

        # HOOK DBG
        return list(set_res)



    # If we've got a hostgroup_name property, we search for all
    # theses groups and ask them their hosts, and then add them
    # all into our host_name property
    def explode_host_groups_into_hosts(self, hosts, hostgroups):
        for i in self:
            hnames_list = []
            if hasattr(i, 'hostgroup_name'):
                hnames_list.extend(self.evaluate_hostgroup_expression(i.hostgroup_name, hosts, hostgroups))

            # Maybe there is no host in the groups, and do not have any
            # host_name too, so tag is as template to do not look at
            if hnames_list == [] and not hasattr(i, 'host_name'):
                i.register = '0'

            if hasattr(i, 'host_name'):
                hst = i.host_name.split(',')
                for h in hst:
                    h = h.strip()
                    # If the host start with a !, it's to be removed from
                    # the hostgroup get list
                    if h.startswith('!'):
                        hst_to_remove = h[1:].strip()
                        try:
                            hnames_list.remove(hst_to_remove)
                        # was not in it
                        except ValueError:
                            pass
                    # Else it's an host to add, but maybe it's ALL
                    elif h == '*':
                        for newhost in  set(h.host_name for h in hosts.items.values() \
                                            if getattr(h, 'host_name', '') != '' and not h.is_tpl()):
                            hnames_list.append(newhost)
                            #print "DBG in item.explode_host_groups_into_hosts , added '%s' to group '%s'" % (newhost, i)
                    else:
                        hnames_list.append(h)

            i.host_name = ','.join(list(set(hnames_list)))

            # Ok, even with all of it, there is still no host, put it as a template
            if i.host_name == '':
                i.register = '0'


    # Take our trigger strings and create true objects with it
    def explode_trigger_string_into_triggers(self, triggers):
        for i in self:
            i.explode_trigger_string_into_triggers(triggers)


########NEW FILE########
__FILENAME__ = itemgroup
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.



# And itemgroup is like a item, but it's a group of items :)

from item import Item, Items

from shinken.brok import Brok
from shinken.property import StringProp
from shinken.log import logger


# TODO: subclass Item & Items for Itemgroup & Itemgroups?
class Itemgroup(Item):

    id = 0

    properties = Item.properties.copy()
    properties.update({
        'members': StringProp(fill_brok=['full_status']),
        # Shinken specific
        'unknown_members': StringProp(default=[]),
    })

    def __init__(self, params={}):
        self.id = self.__class__.id
        self.__class__.id += 1

        self.init_running_properties()

        for key in params:
            # delistify attributes if there is only one value
            params[key] = self.compact_unique_attr_value(params[key])
            setattr(self, key, params[key])


    # Copy the groups properties EXCEPT the members
    # members need to be fill after manually
    def copy_shell(self):
        cls = self.__class__
        old_id = cls.id
        new_i = cls()  # create a new group
        new_i.id = self.id  # with the same id
        cls.id = old_id  # Reset the Class counter

        # Copy all properties
        for prop in cls.properties:
            if prop is not 'members':
                if self.has(prop):
                    val = getattr(self, prop)
                    setattr(new_i, prop, val)
        # but no members
        new_i.members = []
        return new_i

    # Change the members like item1 ,item2 to ['item1' , 'item2']
    # so a python list :)
    # We also strip elements because spaces Stinks!
    def pythonize(self):
        v =  getattr(self, 'members', '')
        # Maybe it's a multi-property like multi hostgroups entries
        # if so "flatten" it
        if isinstance(v, list):
            v = ','.join(v)
        self.members = [mbr for mbr in
                            (m.strip() for m in v.split(','))
                        if mbr != '']

    def replace_members(self, members):
        self.members = members

    # If a prop is absent and is not required, put the default value
    def fill_default(self):
        cls = self.__class__
        for prop, entry in cls.properties.items():
            if not hasattr(self, prop) and not entry.required:
                value = entry.default
                setattr(self, prop, value)

    def add_string_member(self, member):
        if hasattr(self, 'members'):
            self.members += ',' + member
        else:
            self.members = member

    def __str__(self):
        return str(self.__dict__) + '\n'

    def __iter__(self):
        return self.members.__iter__()

    def __delitem__(self, i):
        try:
            self.members.remove(i)
        except ValueError:
            pass

    # a item group is correct if all members actually exists,
    # so if unknown_members is still []
    def is_correct(self):
        res = True

        if self.unknown_members != []:
            for m in self.unknown_members:
                logger.error("[itemgroup::%s] as %s, got unknown member %s" % (self.get_name(), self.__class__.my_type, m))
            res = False

        if self.configuration_errors != []:
            for err in self.configuration_errors:
                logger.error("[itemgroup] %s" % err)
            res = False

        return res


    def has(self, prop):
        return hasattr(self, prop)


    # Get a brok with hostgroup info (like id, name)
    # members is special: list of (id, host_name) for database info
    def get_initial_status_brok(self):
        cls = self.__class__
        data = {}
        # Now config properties
        for prop, entry in cls.properties.items():
            if entry.fill_brok != []:
                if self.has(prop):
                    data[prop] = getattr(self, prop)
        # Here members is just a bunch of host, I need name in place
        data['members'] = []
        for i in self.members:
            # it look like lisp! ((( ..))), sorry....
            data['members'].append((i.id, i.get_name()))
        b = Brok('initial_' + cls.my_type + '_status', data)
        return b


class Itemgroups(Items):

    # If a prop is absent and is not required, put the default value
    def fill_default(self):
        for i in self:
            i.fill_default()


    def add(self, ig):
        self.items[ig.id] = ig


    def get_members_by_name(self, gname):
        g = self.find_by_name(gname)
        if g is None:
            return []
        return getattr(g, 'members', [])

########NEW FILE########
__FILENAME__ = macromodulation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time

from item import Item, Items
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.util import to_name_if_possible
from shinken.log import logger


class MacroModulation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'macromodulation'

    properties = Item.properties.copy()
    properties.update({
        'macromodulation_name':       StringProp(fill_brok=['full_status']),
        'modulation_period' :         StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
    })

    running_properties = Item.running_properties.copy()

    _special_properties = ('modulation_period',)

    macros = {}

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.macromodulation_name


    # Will say if we are active or not
    def is_active(self):
        now = int(time.time())
        if not self.modulation_period or self.modulation_period.is_time_valid(now):
            return True
        return False


    # Should have all properties, or a void macro_period
    def is_correct(self):
        state = True
        cls = self.__class__

        # Raised all previously saw errors like unknown commands or timeperiods
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[item::%s] %s" % (self.get_name(), err))

        for prop, entry in cls.properties.items():
            if prop not in cls._special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.warning("[macromodulation::%s] %s property not set" % (self.get_name(), prop))
                    state = False  # Bad boy...

        # Ok just put None as modulation_period, means 24x7
        if not hasattr(self, 'modulation_period'):
            self.modulation_period = None

        return state



class MacroModulations(Items):
    name_property = "macromodulation_name"
    inner_class = MacroModulation


    def linkify(self, timeperiods):
        self.linkify_with_timeperiods(timeperiods, 'modulation_period')


########NEW FILE########
__FILENAME__ = matchingitem
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

'''
 This is a utility class for factorizing matching functions for
 discovery runners and rules.
'''

import re

from item import Item


class MatchingItem(Item):

    # Try to see if the key,value is matching one or
    # our rule. If value got ',' we must look for each value
    # If one match, we quit
    # We can find in matches or not_matches
    def is_matching(self, key, value, look_in='matches'):
        if look_in == 'matches':
            d = self.matches
        else:
            d = self.not_matches
        # If we do not even have the key, we bailout
        if not key.strip() in d:
            return False

        # Get my matching pattern
        m = d[key]
        if ',' in m:
            matchings = [mt.strip() for mt in m.split(',')]
        else:
            matchings = [m]

        # Split the value by , too
        values = value.split(',')
        for m in matchings:
            for v in values:
                print "Try to match", m, v
                # Maybe m is a list, if so should check one values
                if isinstance(m, list):
                    for _m in m:
                        if re.search(_m, v):
                            return True
                else:
                    if re.search(m, v):
                        return True
        return False

    # Look if we match all discovery data or not
    # a disco data look as a list of (key, values)
    def is_matching_disco_datas(self, datas):
        # If we got not data, no way we can match
        if len(datas) == 0:
            return False

        # First we look if it's possible to match
        # we must match All self.matches things
        for m in self.matches:
            #print "Compare to", m
            match_one = False
            for (k, v) in datas.iteritems():
                # We found at least one of our match key
                if m == k:
                    if self.is_matching(k, v):
                        #print "Got matching with", m, k, v
                        match_one = True
                        continue
            if not match_one:
                # It match none
                #print "Match none, False"
                return False
        #print "It's possible to be OK"

        # And now look if ANY of not_matches is reach. If so
        # it's False
        for m in self.not_matches:
            #print "Compare to NOT", m
            match_one = False
            for (k, v) in datas.iteritems():
                #print "K,V", k,v
                # We found at least one of our match key
                if m == k:
                    #print "Go loop"
                    if self.is_matching(k, v, look_in='not_matches'):
                        #print "Got matching with", m, k, v
                        match_one = True
                        continue
            if match_one:
                #print "I match one, I quit"
                return False

        # Ok we match ALL rules in self.matches
        # and NONE of self.not_matches, we can go :)
        return True

########NEW FILE########
__FILENAME__ = module
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items

from shinken.property import StringProp, ListProp
from shinken.util import strip_and_uniq
from shinken.log import logger


class Module(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'module'

    properties = Item.properties.copy()
    properties.update({
        'module_name': StringProp(),
        'module_type': StringProp(),
        'modules': ListProp(default=''),
    })

    macros = {}

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.module_name


class Modules(Items):
    name_property = "module_name"
    inner_class = Module

    def linkify(self):
        self.linkify_s_by_plug()

    def linkify_s_by_plug(self):
        for s in self:
            new_modules = []
            mods = s.modules.split(',')
            mods = strip_and_uniq(mods)
            for plug_name in mods:
                plug_name = plug_name.strip()

                # don't read void names
                if plug_name == '':
                    continue

                # We are the modules, we search them :)
                plug = self.find_by_name(plug_name)
                if plug is not None:
                    new_modules.append(plug)
                else:
                    err = "[module] unknown %s module from %s" % (plug_name, s.get_name())
                    logger.error(err)
                    s.configuration_errors.append(err)
            s.modules = new_modules
        

    # We look for contacts property in contacts and
    def explode(self):
        pass

########NEW FILE########
__FILENAME__ = notificationway
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items

from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.log import logger

_special_properties = ('service_notification_commands', 'host_notification_commands',
                        'service_notification_period', 'host_notification_period')


class NotificationWay(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'notificationway'

    properties = Item.properties.copy()
    properties.update({
        'notificationway_name':          StringProp(fill_brok=['full_status']),
        'host_notifications_enabled':    BoolProp(default='1', fill_brok=['full_status']),
        'service_notifications_enabled': BoolProp(default='1', fill_brok=['full_status']),
        'host_notification_period':      StringProp(fill_brok=['full_status']),
        'service_notification_period':   StringProp(fill_brok=['full_status']),
        'host_notification_options':     ListProp(fill_brok=['full_status']),
        'service_notification_options':  ListProp(fill_brok=['full_status']),
        'host_notification_commands':    StringProp(fill_brok=['full_status']),
        'service_notification_commands': StringProp(fill_brok=['full_status']),
        'min_business_impact':           IntegerProp(default='0', fill_brok=['full_status']),
    })

    running_properties = Item.running_properties.copy()

    # This tab is used to transform old parameters name into new ones
    # so from Nagios2 format, to Nagios3 ones.
    # Or Shinken deprecated names like criticity
    old_properties = {
        'min_criticity': 'min_business_impact',
    }

    macros = {}

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.notificationway_name

    # Search for notification_options with state and if t is
    # in service_notification_period
    def want_service_notification(self, t, state, type, business_impact, cmd=None):
        if not self.service_notifications_enabled:
            return False

        # Maybe the command we ask for are not for us, but for another notification ways
        # on the same contact. If so, bail out
        if cmd and not cmd in self.service_notification_commands:
            return False

        # If the business_impact is not high enough, we bail out
        if business_impact < self.min_business_impact:
            return False

        b = self.service_notification_period.is_time_valid(t)
        if 'n' in self.service_notification_options:
            return False
        t = {'WARNING': 'w', 'UNKNOWN': 'u', 'CRITICAL': 'c',
             'RECOVERY': 'r', 'FLAPPING': 'f', 'DOWNTIME': 's'}
        if type == 'PROBLEM':
            if state in t:
                return b and t[state] in self.service_notification_options
        elif type == 'RECOVERY':
            if type in t:
                return b and t[type] in self.service_notification_options
        elif type == 'ACKNOWLEDGEMENT':
            return b
        elif type in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            return b and 'f' in self.service_notification_options
        elif type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'DOWNTIMECANCELLED'):
            # No notification when a downtime was cancelled. Is that true??
            # According to the documentation we need to look at _host_ options
            return b and 's' in self.host_notification_options

        return False


    # Search for notification_options with state and if t is in
    # host_notification_period
    def want_host_notification(self, t, state, type, business_impact, cmd=None):
        if not self.host_notifications_enabled:
            return False

        # If the business_impact is not high enough, we bail out
        if business_impact < self.min_business_impact:
            return False

        # Maybe the command we ask for are not for us, but for another notification ways
        # on the same contact. If so, bail out
        if cmd and not cmd in self.host_notification_commands:
            return False

        b = self.host_notification_period.is_time_valid(t)
        if 'n' in self.host_notification_options:
            return False
        t = {'DOWN': 'd', 'UNREACHABLE': 'u', 'RECOVERY': 'r',
             'FLAPPING': 'f', 'DOWNTIME': 's'}
        if type == 'PROBLEM':
            if state in t:
                return b and t[state] in self.host_notification_options
        elif type == 'RECOVERY':
            if type in t:
                return b and t[type] in self.host_notification_options
        elif type == 'ACKNOWLEDGEMENT':
            return b
        elif type in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            return b and 'f' in self.host_notification_options
        elif type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'DOWNTIMECANCELLED'):
            return b and 's' in self.host_notification_options

        return False


    # Call to get our commands to launch a Notification
    def get_notification_commands(self, type):
        # service_notification_commands for service
        notif_commands_prop = type + '_notification_commands'
        notif_commands = getattr(self, notif_commands_prop)
        return notif_commands


    # Check is required prop are set:
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        # Raised all previously saw errors like unknown commands or timeperiods
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[item::%s] %s" % (self.get_name(), err))


        # A null notif way is a notif way that will do nothing (service = n, hot =n)
        is_null_notifway = False
        if hasattr(self, 'service_notification_options') and self.service_notification_options == ['n']:
            if hasattr(self, 'host_notification_options') and self.host_notification_options == ['n']:
                is_null_notifway = True
                return True

        for prop, entry in cls.properties.items():
            if prop not in _special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.warning("[notificationway::%s] %s property not set" % (self.get_name(), prop))
                    state = False  # Bad boy...

        # Ok now we manage special cases...
        # Service part
        if not hasattr(self, 'service_notification_commands'):
            logger.warning("[notificationway::%s] do not have any service_notification_commands defined" % self.get_name())
            state = False
        else:
            for cmd in self.service_notification_commands:
                if cmd is None:
                    logger.warning("[notificationway::%s] a service_notification_command is missing" % self.get_name())
                    state = False
                if not cmd.is_valid():
                    logger.warning("[notificationway::%s] a service_notification_command is invalid" % self.get_name())
                    state = False

        if getattr(self, 'service_notification_period', None) is None:
            logger.warning("[notificationway::%s] the service_notification_period is invalid" % self.get_name())
            state = False

        # Now host part
        if not hasattr(self, 'host_notification_commands'):
            logger.warning("[notificationway::%s] do not have any host_notification_commands defined" % self.get_name())
            state = False
        else:
            for cmd in self.host_notification_commands:
                if cmd is None:
                    logger.warning("[notificationway::%s] a host_notification_command is missing" % self.get_name())
                    state = False
                if not cmd.is_valid():
                    logger.warning("[notificationway::%s] a host_notification_command is invalid (%s)" % (cmd.get_name(), str(cmd.__dict__)))
                    state = False

        if getattr(self, 'host_notification_period', None) is None:
            logger.warning("[notificationway::%s] the host_notification_period is invalid" % self.get_name())
            state = False

        return state


    # In the scheduler we need to relink the commandCall with
    # the real commands
    def late_linkify_nw_by_commands(self, commands):
        props = ['service_notification_commands', 'host_notification_commands']
        for prop in props:
            for cc in getattr(self, prop, []):
                cc.late_linkify_with_command(commands)


class NotificationWays(Items):
    name_property = "notificationway_name"
    inner_class = NotificationWay


    def linkify(self, timeperiods, commands):
        self.linkify_with_timeperiods(timeperiods, 'service_notification_period')
        self.linkify_with_timeperiods(timeperiods, 'host_notification_period')
        self.linkify_command_list_with_commands(commands, 'service_notification_commands')
        self.linkify_command_list_with_commands(commands, 'host_notification_commands')


    def new_inner_member(self, name=None, params={}):
        if name is None:
            name = NotificationWay.id
        params['notificationway_name'] = name
        #print "Asking a new inner notificationway from name %s with params %s" % (name, params)
        nw = NotificationWay(params)
        self.items[nw.id] = nw

########NEW FILE########
__FILENAME__ = pack
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import os
import re
try:
    import json
except ImportError:
    json = None

from shinken.objects.item import Item, Items
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.log import logger


class Pack(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'pack'

    properties = Item.properties.copy()
    properties.update({'pack_name': StringProp(fill_brok=['full_status'])})

    running_properties = Item.running_properties.copy()
    running_properties.update({'macros': StringProp(default={})})

    # For debugging purpose only (nice name)
    def get_name(self):
        try:
            return self.pack_name
        except AttributeError:
            return 'UnnamedPack'


class Packs(Items):
    name_property = "pack_name"
    inner_class = Pack

    # We will dig into the path and load all .pack files
    def load_file(self, path):
        # Now walk for it
        for root, dirs, files in os.walk(path):
            for file in files:
                if re.search("\.pack$", file):
                    p = os.path.join(root, file)
                    try:
                        fd = open(p, 'rU')
                        buf = fd.read()
                        fd.close()
                    except IOError, exp:
                        logger.error("Cannot open pack file '%s' for reading: %s" % (p, exp))
                        # ok, skip this one
                        continue
                    self.create_pack(buf, file[:-5])

    # Create a pack from the string buf, and get a real object from it
    def create_pack(self, buf, name):
        if not json:
            logger.warning("[Pack] cannot load the pack file '%s': missing json lib" % name)
            return
        # Ok, go compile the code
        try:
            d = json.loads(buf)
            if not 'name' in d:
                logger.error("[Pack] no name in the pack '%s'" % name)
                return
            p = Pack({})
            p.pack_name = d['name']
            p.description = d.get('description', '')
            p.macros = d.get('macros', {})
            p.templates = d.get('templates', [p.pack_name])
            p.path = d.get('path', 'various/')
            p.doc_link = d.get('doc_link', '')
            p.services = d.get('services', {})
            p.commands = d.get('commands', [])
            if not p.path.endswith('/'):
                p.path += '/'
            # Ok, add it
            self[p.id] = p
        except ValueError, exp:
            logger.error("[Pack] error in loading pack file '%s': '%s'" % (name, exp))

########NEW FILE########
__FILENAME__ = realm
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import copy

from item import Item
from itemgroup import Itemgroup, Itemgroups
from shinken.property import BoolProp, IntegerProp, StringProp
from shinken.log import logger

# It change from hostgroup Class because there is no members
# properties, just the realm_members that we rewrite on it.

class Realm(Itemgroup):
    id = 1  # zero is always a little bit special... like in database
    my_type = 'realm'

    properties = Itemgroup.properties.copy()
    properties.update({
        'id':            IntegerProp(default=0, fill_brok=['full_status']),
        'realm_name':    StringProp(fill_brok=['full_status']),
        'realm_members': StringProp(default=''), # No status_broker_name because it put hosts, not host_name
        'higher_realms': StringProp(default=''),
        'default':       BoolProp(default='0'),
        'broker_complete_links':       BoolProp(default='0'),
        #'alias': {'required':  True, 'fill_brok': ['full_status']},
        #'notes': {'required': False, 'default':'', 'fill_brok': ['full_status']},
        #'notes_url': {'required': False, 'default':'', 'fill_brok': ['full_status']},
        #'action_url': {'required': False, 'default':'', 'fill_brok': ['full_status']},
    })

    running_properties = Item.running_properties.copy()
    running_properties.update({
            'serialized_confs': StringProp(default={}),
        })

    macros = {
        'REALMNAME': 'realm_name',
        'REALMMEMBERS': 'members',
    }


    def get_name(self):
        return self.realm_name


    def get_realms(self):
        return self.realm_members


    def add_string_member(self, member):
        self.realm_members += ',' + member


    def get_realm_members(self):
        if self.has('realm_members'):
            return [r.strip() for r in self.realm_members.split(',')]
        else:
            return []

    # Use to make python properties
    # TODO: change itemgroup function pythonize?
    def pythonize(self):
        cls = self.__class__
        for prop, tab in cls.properties.items():
            try:
                old_val = getattr(self, prop)
                new_val = tab.pythonize(old_val)
                #print "Changing ", old_val, "by", new_val
                setattr(self, prop, new_val)
            except AttributeError, exp:
                pass  # Will be catch at the is_correct moment


    # We fillfull properties with template ones if need
    # Because hostgroup we call may not have it's members
    # we call get_hosts_by_explosion on it
    def get_realms_by_explosion(self, realms):
        # First we tag the hg so it will not be explode
        # if a son of it already call it
        self.already_explode = True

        # Now the recursive part
        # rec_tag is set to False every HG we explode
        # so if True here, it must be a loop in HG
        # calls... not GOOD!
        if self.rec_tag:
            err = "Error: we've got a loop in realm definition %s" % self.get_name()
            self.configuration_errors.append(err)
            if self.has('members'):
                return self.members
            else:
                return ''

        # Ok, not a loop, we tag it and continue
        self.rec_tag = True

        p_mbrs = self.get_realm_members()
        for p_mbr in p_mbrs:
            p = realms.find_by_name(p_mbr.strip())
            if p is not None:
                value = p.get_realms_by_explosion(realms)
                if value is not None:
                    self.add_string_member(value)

        if self.has('members'):
            return self.members
        else:
            return ''


    def get_all_subs_pollers(self):
        r = copy.copy(self.pollers)
        for p in self.realm_members:
            tmps = p.get_all_subs_pollers()
            for s in tmps:
                r.append(s)
        return r


    def get_all_subs_reactionners(self):
        r = copy.copy(self.reactionners)
        for p in self.realm_members:
            tmps = p.get_all_subs_reactionners()
            for s in tmps:
                r.append(s)
        return r


    def count_reactionners(self):
        self.nb_reactionners = 0
        for reactionner in self.reactionners:
            if not reactionner.spare:
                self.nb_reactionners += 1
        for realm in self.higher_realms:
            for reactionner in realm.reactionners:
                if not reactionner.spare and reactionner.manage_sub_realms:
                    self.nb_reactionners += 1


    def fill_potential_reactionners(self):
        self.potential_reactionners = []
        for reactionner in self.reactionners:
            self.potential_reactionners.append(reactionner)
        for realm in self.higher_realms:
            for reactionner in realm.reactionners:
                if reactionner.manage_sub_realms:
                    self.potential_reactionners.append(reactionner)


    def count_pollers(self):
        self.nb_pollers = 0
        for poller in self.pollers:
            if not poller.spare:
                self.nb_pollers += 1
        for realm in self.higher_realms:
            for poller in realm.pollers:
                if not poller.spare and poller.manage_sub_realms:
                    self.nb_pollers += 1


    def fill_potential_pollers(self):
        self.potential_pollers = []
        for poller in self.pollers:
            self.potential_pollers.append(poller)
        for realm in self.higher_realms:
            for poller in realm.pollers:
                if poller.manage_sub_realms:
                    self.potential_pollers.append(poller)


    def count_brokers(self):
        self.nb_brokers = 0
        for broker in self.brokers:
            if not broker.spare:
                self.nb_brokers += 1
        for realm in self.higher_realms:
            for broker in realm.brokers:
                if not broker.spare and broker.manage_sub_realms:
                    self.nb_brokers += 1


    def fill_potential_brokers(self):
        self.potential_brokers = []
        for broker in self.brokers:
            self.potential_brokers.append(broker)
        for realm in self.higher_realms:
            for broker in realm.brokers:
                if broker.manage_sub_realms:
                    self.potential_brokers.append(broker)


    def count_receivers(self):
        self.nb_receivers = 0
        for receiver in self.receivers:
            if not receiver.spare:
                self.nb_receivers += 1
        for realm in self.higher_realms:
            for receiver in realm.receivers:
                if not receiver.spare and receiver.manage_sub_realms:
                    self.nb_receivers += 1


    def fill_potential_receivers(self):
        self.potential_receivers = []
        for broker in self.receivers:
            self.potential_receivers.append(broker)
        for realm in self.higher_realms:
            for broker in realm.receivers:
                if broker.manage_sub_realms:
                    self.potential_receivers.append(broker)


    # Return the list of satellites of a certain type
    # like reactionner -> self.reactionners
    def get_satellties_by_type(self, type):
        if hasattr(self, type + 's'):
            return getattr(self, type + 's')
        else:
            logger.debug("[realm] do not have this kind of satellites: %s" % type)
            return []


    # Return the list of potentials satellites of a certain type
    # like reactionner -> self.potential_reactionners
    def get_potential_satellites_by_type(self, type):
        if hasattr(self, 'potential_' + type + 's'):
            return getattr(self, 'potential_' + type + 's')
        else:
            logger.debug("[realm] do not have this kind of satellites: %s" % type)
            return []


    # Return the list of potentials satellites of a certain type
    # like reactionner -> self.nb_reactionners
    def get_nb_of_must_have_satellites(self, type):
        if hasattr(self, 'nb_' + type + 's'):
            return getattr(self, 'nb_' + type + 's')
        else:
            logger.debug("[realm] do not have this kind of satellites: %s" % type)
            return 0


    # Fill dict of realms for managing the satellites confs
    def prepare_for_satellites_conf(self):
        self.to_satellites = {}
        self.to_satellites['reactionner'] = {}
        self.to_satellites['poller'] = {}
        self.to_satellites['broker'] = {}
        self.to_satellites['receiver'] = {}

        self.to_satellites_need_dispatch = {}
        self.to_satellites_need_dispatch['reactionner'] = {}
        self.to_satellites_need_dispatch['poller'] = {}
        self.to_satellites_need_dispatch['broker'] = {}
        self.to_satellites_need_dispatch['receiver'] = {}

        self.to_satellites_managed_by = {}
        self.to_satellites_managed_by['reactionner'] = {}
        self.to_satellites_managed_by['poller'] = {}
        self.to_satellites_managed_by['broker'] = {}
        self.to_satellites_managed_by['receiver'] = {}

        self.count_reactionners()
        self.fill_potential_reactionners()
        self.count_pollers()
        self.fill_potential_pollers()
        self.count_brokers()
        self.fill_potential_brokers()
        self.count_receivers()
        self.fill_potential_receivers()

        s = "%s: (in/potential) (schedulers:%d) (pollers:%d/%d) (reactionners:%d/%d) (brokers:%d/%d) (receivers:%d/%d)" % \
            (self.get_name(),
             len(self.schedulers),
             self.nb_pollers, len(self.potential_pollers),
             self.nb_reactionners, len(self.potential_reactionners),
             self.nb_brokers, len(self.potential_brokers),
             self.nb_receivers, len(self.potential_receivers)
             )
        logger.info(s)


    # TODO: find a better name...
    # TODO: and if he goes active?
    def fill_broker_with_poller_reactionner_links(self, broker):
        # First we create/void theses links
        broker.cfg['pollers'] = {}
        broker.cfg['reactionners'] = {}

        # First our own level
        for p in self.pollers:
            cfg = p.give_satellite_cfg()
            broker.cfg['pollers'][p.id] = cfg

        for r in self.reactionners:
            cfg = r.give_satellite_cfg()
            broker.cfg['reactionners'][r.id] = cfg

        # Then sub if we must to it
        if broker.manage_sub_realms:
            # Now pollers
            for p in self.get_all_subs_pollers():
                cfg = p.give_satellite_cfg()
                broker.cfg['pollers'][p.id] = cfg

            # Now reactionners
            for r in self.get_all_subs_reactionners():
                cfg = r.give_satellite_cfg()
                broker.cfg['reactionners'][r.id] = cfg


    # Get a conf package of satellites links that can be useful for
    # a scheduler
    def get_satellites_links_for_scheduler(self):
        cfg = {}

        # First we create/void theses links
        cfg['pollers'] = {}
        cfg['reactionners'] = {}

        # First our own level
        for p in self.pollers:
            c = p.give_satellite_cfg()
            cfg['pollers'][p.id] = c

        for r in self.reactionners:
            c = r.give_satellite_cfg()
            cfg['reactionners'][r.id] = c

        #print "***** Preparing a satellites conf for a scheduler", cfg
        return cfg


class Realms(Itemgroups):
    name_property = "realm_name"  # is used for finding hostgroups
    inner_class = Realm

    def get_members_by_name(self, pname):
        realm = self.find_by_name(pname)
        if realm is None:
            return []
        return realm.get_realms()


    def linkify(self):
        self.linkify_p_by_p()

        # prepare list of satellites and confs
        for p in self:
            p.pollers = []
            p.schedulers = []
            p.reactionners = []
            p.brokers = []
            p.receivers = []
            p.packs = []
            p.confs = {}


    # We just search for each realm the others realms
    # and replace the name by the realm
    def linkify_p_by_p(self):
        for p in self.items.values():
            mbrs = p.get_realm_members()
            # The new member list, in id
            new_mbrs = []
            for mbr in mbrs:
                new_mbr = self.find_by_name(mbr)
                if new_mbr is not None:
                    new_mbrs.append(new_mbr)
            # We find the id, we replace the names
            p.realm_members = new_mbrs

        # Now put higher realm in sub realms
        # So after they can
        for p in self.items.values():
            p.higher_realms = []

        for p in self.items.values():
            for sub_p in p.realm_members:
                sub_p.higher_realms.append(p)


    # Use to fill members with hostgroup_members
    def explode(self):
        # We do not want a same hg to be explode again and again
        # so we tag it
        for tmp_p in self.items.values():
            tmp_p.already_explode = False
        for p in self:
            if p.has('realm_members') and not p.already_explode:
                # get_hosts_by_explosion is a recursive
                # function, so we must tag hg so we do not loop
                for tmp_p in self:
                    tmp_p.rec_tag = False
                p.get_realms_by_explosion(self)

        # We clean the tags
        for tmp_p in self.items.values():
            if hasattr(tmp_p, 'rec_tag'):
                del tmp_p.rec_tag
            del tmp_p.already_explode


    def get_default(self):
        for r in self:
            if getattr(r, 'default', False):
                return r
        return None


    def prepare_for_satellites_conf(self):
        for r in self:
            r.prepare_for_satellites_conf()

########NEW FILE########
__FILENAME__ = resultmodulation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


# The resultmodulation class is used for in scheduler modulation of results
# like the return code or the output.

import time

from item import Item, Items

from shinken.property import StringProp, ListProp


class Resultmodulation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'resultmodulation'

    properties = Item.properties.copy()
    properties.update({
        'resultmodulation_name': StringProp(),
        'exit_codes_match':      ListProp(default=''),
        'exit_code_modulation':  StringProp(default=None),
        'modulation_period':     StringProp(default=None),
    })

    # For debugging purpose only (nice name)
    def get_name(self):
        return self.resultmodulation_name

    # Make the return code modulation if need
    def module_return(self, return_code):
        # Only if in modulation_period of modulation_period == None
        if self.modulation_period is None or self.modulation_period.is_time_valid(time.time()):
            # Try to change the exit code only if a new one is defined
            if self.exit_code_modulation is not None:
                # First with the exit_code_match
                if return_code in self.exit_codes_match:
                    return_code = self.exit_code_modulation

        return return_code

    # We override the pythonize because we have special cases that we do not want
    # to be do at running
    def pythonize(self):
        # First apply Item pythonize
        super(self.__class__, self).pythonize()

        # Then very special cases
        # Intify the exit_codes_match, and make list
        self.exit_codes_match = [int(ec) for ec in getattr(self, 'exit_codes_match', [])]

        if hasattr(self, 'exit_code_modulation'):
            self.exit_code_modulation = int(self.exit_code_modulation)
        else:
            self.exit_code_modulation = None


class Resultmodulations(Items):
    name_property = "resultmodulation_name"
    inner_class = Resultmodulation

    def linkify(self, timeperiods):
        self.linkify_rm_by_tp(timeperiods)

    # We just search for each timeperiod the tp
    # and replace the name by the tp
    def linkify_rm_by_tp(self, timeperiods):
        for rm in self:
            mtp_name = rm.modulation_period.strip()

            # The new member list, in id
            mtp = timeperiods.find_by_name(mtp_name)

            if mtp_name != '' and mtp is None:
                err = "Error: the result modulation '%s' got an unknown modulation_period '%s'" % (rm.get_name(), mtp_name)
                rm.configuration_errors.append(err)

            rm.modulation_period = mtp

########NEW FILE########
__FILENAME__ = schedulingitem
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#    Thibault Cohen, thibault.cohen@savoirfairelinux.com
#    Francois Mikus, fmikus@acktomic.com
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This class is a common one for service/host. Here you
will find all scheduling related functions, like the schedule
or the consume_check. It's a very important class!
"""

import re
import random
import time
import traceback
from datetime import datetime

from item import Item

from shinken.check import Check
from shinken.notification import Notification
from shinken.macroresolver import MacroResolver
from shinken.eventhandler import EventHandler
from shinken.dependencynode import DependencyNodeFactory
from shinken.log import logger

# on system time change just reevaluate the following attributes:
on_time_change_update = ('last_notification', 'last_state_change', 'last_hard_state_change')


class SchedulingItem(Item):

    # global counters used for [current|last]_[host|service]_[event|problem]_id
    current_event_id = 0
    current_problem_id = 0

    # Call by pickle to data-ify the host
    # we do a dict because list are too dangerous for
    # retention save and co :( even if it's more
    # extensive
    # The setstate function do the inverse
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)
        for prop in cls.running_properties:
            if hasattr(self, prop):
                res[prop] = getattr(self, prop)
        return res


    # Inversed function of __getstate__
    def __setstate__(self, state):
        cls = self.__class__
        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])
        for prop in cls.running_properties:
            if prop in state:
                setattr(self, prop, state[prop])


    # Register the son in my child_dependencies, and
    # myself in its parent_dependencies
    def register_son_in_parent_child_dependencies(self, son):
        # So we register it in our list
        self.child_dependencies.add(son)

        # and us to its parents
        son.parent_dependencies.add(self)


    # Add a flapping change, but no more than 20 states
    # Then update the self.is_flapping bool by calling update_flapping
    def add_flapping_change(self, b):
        cls = self.__class__

        # If this element is not in flapping check, or
        # the flapping is globally disable, bailout
        if not self.flap_detection_enabled or not cls.enable_flap_detection:
            return

        self.flapping_changes.append(b)

        # Keep just 20 changes (global flap_history value)
        flap_history = cls.flap_history

        if len(self.flapping_changes) > flap_history:
            self.flapping_changes.pop(0)

        # Now we add a value, we update the is_flapping prop
        self.update_flapping()


    # We update the is_flapping prop with value in self.flapping_states
    # Old values have less weight than new ones
    def update_flapping(self):
        flap_history = self.__class__.flap_history
        # We compute the flapping change in %
        r = 0.0
        i = 0
        for b in self.flapping_changes:
            i += 1
            if b:
                r += i * (1.2 - 0.8) / flap_history + 0.8
        r = r / flap_history
        r *= 100

        # We can update our value
        self.percent_state_change = r

        # Look if we are full in our states, because if not
        # the value is not accurate
        is_full = len(self.flapping_changes) >= flap_history

        # Now we get the low_flap_threshold and high_flap_threshold values
        # They can be from self, or class
        (low_flap_threshold, high_flap_threshold) = (self.low_flap_threshold, self.high_flap_threshold)
        if low_flap_threshold == -1:
            cls = self.__class__
            low_flap_threshold = cls.global_low_flap_threshold
        if high_flap_threshold == -1:
            cls = self.__class__
            high_flap_threshold = cls.global_high_flap_threshold

        # Now we check is flapping change, but only if we got enough
        # states to look at the value accuracy
        if self.is_flapping and r < low_flap_threshold and is_full:
            self.is_flapping = False
            # We also raise a log entry
            self.raise_flapping_stop_log_entry(r, low_flap_threshold)
            # and a notification
            self.remove_in_progress_notifications()
            self.create_notifications('FLAPPINGSTOP')
            # And update our status for modules
            b = self.get_update_status_brok()
            self.broks.append(b)

        if not self.is_flapping and r >= high_flap_threshold and is_full:
            self.is_flapping = True
            # We also raise a log entry
            self.raise_flapping_start_log_entry(r, high_flap_threshold)
            # and a notification
            self.remove_in_progress_notifications()
            self.create_notifications('FLAPPINGSTART')
            # And update our status for modules
            b = self.get_update_status_brok()
            self.broks.append(b)


    # Add an attempt but cannot be more than max_check_attempts
    def add_attempt(self):
        self.attempt += 1
        self.attempt = min(self.attempt, self.max_check_attempts)


    # Return True if attempt is at max
    def is_max_attempts(self):
        return self.attempt >= self.max_check_attempts


    # Call by scheduler to see if last state is older than
    # freshness_threshold if check_freshness, then raise a check
    # even if active check is disabled
    def do_check_freshness(self):
        now = time.time()
        # Before, check if class (host or service) have check_freshness OK
        # Then check if item want freshness, then check freshness
        cls = self.__class__
        if not self.in_checking:
            if cls.global_check_freshness:
                if self.check_freshness and self.freshness_threshold != 0:
                    if self.last_state_update < now - (self.freshness_threshold + cls.additional_freshness_latency):
                        # Fred: Do not raise a check for passive only checked hosts when not in check period ...
                        if self.passive_checks_enabled and not self.active_checks_enabled:
                            if self.check_period is None or self.check_period.is_time_valid(now):
                                # Raise a log
                                self.raise_freshness_log_entry(int(now-self.last_state_update), int(now-self.freshness_threshold))
                                # And a new check
                                return self.launch_check(now)
                            else:
                                logger.debug("Should have checked freshness for passive only checked host:%s, but host is not in check period." % (self.host_name))
        return None


    # Raise all impact from my error. I'm setting myself
    # as a problem, and I register myself as this in all
    # hosts/services that depend_on_me. So they are now my
    # impacts
    def set_myself_as_problem(self):
        now = time.time()

        self.is_problem = True
        # we should warn potentials impact of our problem
        # and they should be cool to register them so I've got
        # my impacts list
        impacts = list(self.impacts)
        for (impact, status, dep_type, tp, inh_par) in self.act_depend_of_me:
            # Check if the status is ok for impact
            for s in status:
                if self.is_state(s):
                    # now check if we should bailout because of a
                    # not good timeperiod for dep
                    if tp is None or tp.is_time_valid(now):
                        new_impacts = impact.register_a_problem(self)
                        impacts.extend(new_impacts)

        # Only update impacts and create new brok if impacts changed.
        s_impacts = set(impacts)
        if s_impacts == set(self.impacts):
            return
        self.impacts = list(s_impacts)

        # We can update our business_impact value now
        self.update_business_impact_value()

        # And we register a new broks for update status
        b = self.get_update_status_brok()
        self.broks.append(b)


    # We update our 'business_impact' value with the max of
    # the impacts business_impact if we got impacts. And save our 'configuration'
    # business_impact if we do not have do it before
    # If we do not have impacts, we revert our value
    def update_business_impact_value(self):
        # First save our business_impact if not already do
        if self.my_own_business_impact == -1:
            self.my_own_business_impact = self.business_impact

        # We look at our crit modulations. If one apply, we take apply it
        # and it's done
        in_modulation = False
        for cm in self.business_impact_modulations:
            now = time.time()
            period = cm.modulation_period
            if period is None or period.is_time_valid(now):
                #print "My self", self.get_name(), "go from crit", self.business_impact, "to crit", cm.business_impact
                self.business_impact = cm.business_impact
                in_modulation = True
                # We apply the first available, that's all
                break

        # If we truly have impacts, we get the max business_impact
        # if it's huge than ourselves
        if len(self.impacts) != 0:
            self.business_impact = max(self.business_impact, max([e.business_impact for e in self.impacts]))
            return

        # If we are not a problem, we setup our own_crit if we are not in a
        # modulation period
        if self.my_own_business_impact != -1 and not in_modulation:
            self.business_impact = self.my_own_business_impact


    # Look for my impacts, and remove me from theirs problems list
    def no_more_a_problem(self):
        was_pb = self.is_problem
        if self.is_problem:
            self.is_problem = False

            # we warn impacts that we are no more a problem
            for impact in self.impacts:
                impact.deregister_a_problem(self)

            # we can just drop our impacts list
            self.impacts = []

        # We update our business_impact value, it's not a huge thing :)
        self.update_business_impact_value()

        # If we were a problem, we say to everyone
        # our new status, with good business_impact value
        if was_pb:
            # And we register a new broks for update status
            b = self.get_update_status_brok()
            self.broks.append(b)


    # Call recursively by potentials impacts so they
    # update their source_problems list. But do not
    # go below if the problem is not a real one for me
    # like If I've got multiple parents for examples
    def register_a_problem(self, pb):
        # Maybe we already have this problem? If so, bailout too
        if pb in self.source_problems:
            return []

        now = time.time()
        was_an_impact = self.is_impact
        # Our father already look of he impacts us. So if we are here,
        # it's that we really are impacted
        self.is_impact = True

        impacts = []
        # Ok, if we are impacted, we can add it in our
        # problem list
        # TODO: remove this unused check
        if self.is_impact:
            # Maybe I was a problem myself, now I can say: not my fault!
            if self.is_problem:
                self.no_more_a_problem()

            # Ok, we are now an impact, we should take the good state
            # but only when we just go in impact state
            if not was_an_impact:
                self.set_impact_state()

            # Ok now we can be a simple impact
            impacts.append(self)
            if pb not in self.source_problems:
                self.source_problems.append(pb)
            # we should send this problem to all potential impact that
            # depend on us
            for (impact, status, dep_type, tp, inh_par) in self.act_depend_of_me:
                # Check if the status is ok for impact
                for s in status:
                    if self.is_state(s):
                        # now check if we should bailout because of a
                        # not good timeperiod for dep
                        if tp is None or tp.is_time_valid(now):
                            new_impacts = impact.register_a_problem(pb)
                            impacts.extend(new_impacts)

            # And we register a new broks for update status
            b = self.get_update_status_brok()
            self.broks.append(b)

        # now we return all impacts (can be void of course)
        return impacts


    # Just remove the problem from our problems list
    # and check if we are still 'impacted'. It's not recursif because problem
    # got the list of all its impacts
    def deregister_a_problem(self, pb):
        self.source_problems.remove(pb)

        # For know if we are still an impact, maybe our dependencies
        # are not aware of the remove of the impact state because it's not ordered
        # so we can just look at if we still have some problem in our list
        if len(self.source_problems) == 0:
            self.is_impact = False
            # No more an impact, we can unset the impact state
            self.unset_impact_state()

        # And we register a new broks for update status
        b = self.get_update_status_brok()
        self.broks.append(b)


    # When all dep are resolved, this function say if
    # action can be raise or not by viewing dep status
    # network_dep have to be all raise to be no action
    # logic_dep: just one is enough
    def is_no_action_dependent(self):
        # Use to know if notif is raise or not
        # no_action = False
        parent_is_down = []
        # So if one logic is Raise, is dep
        # is one network is no ok, is not dep
        # at the end, raise no dep
        for (dep, status, type, tp, inh_par) in self.act_depend_of:
            # For logic_dep, only one state raise put no action
            if type == 'logic_dep':
                for s in status:
                    if dep.is_state(s):
                        return True
            # more complicated: if none of the states are match, the host is down
            # so -> network_dep
            else:
                p_is_down = False
                dep_match = [dep.is_state(s) for s in status]
                # check if the parent match a case, so he is down
                if True in dep_match:
                    p_is_down = True
                parent_is_down.append(p_is_down)
        # if a parent is not down, no dep can explain the pb
        if False in parent_is_down:
            return False
        else:  # every parents are dead, so... It's not my fault :)
            return True


    # We check if we are no action just because of ours parents (or host for
    # service)
    # TODO: factorize with previous check?
    def check_and_set_unreachability(self):
        parent_is_down = []
        # We must have all parents raised to be unreachable
        for (dep, status, type, tp, inh_par) in self.act_depend_of:
            # For logic_dep, only one state raise put no action
            if type == 'network_dep':
                p_is_down = False
                dep_match = [dep.is_state(s) for s in status]
                if True in dep_match:  # the parent match a case, so he is down
                    p_is_down = True
                parent_is_down.append(p_is_down)

        # if a parent is not down, no dep can explain the pb
        # or if we don't have any parents
        if len(parent_is_down) == 0 or False in parent_is_down:
            return
        else:  # every parents are dead, so... It's not my fault :)
            self.set_unreachable()
            return


    # Use to know if I raise dependency for someone else (with status)
    # If I do not raise dep, maybe my dep raise me. If so, I raise dep.
    # So it's a recursive function
    def do_i_raise_dependency(self, status, inherit_parents):
        # Do I raise dep?
        for s in status:
            if self.is_state(s):
                return True

        # If we do not inherit parent, we have no reason to be blocking
        if not inherit_parents:
            return False

        # Ok, I do not raise dep, but my dep maybe raise me
        now = time.time()
        for (dep, status, type, tp, inh_parent) in self.chk_depend_of:
            if dep.do_i_raise_dependency(status, inh_parent):
                if tp is None or tp.is_time_valid(now):
                    return True

        # No, I really do not raise...
        return False


    # Use to know if my dep force me not to be checked
    # So check the chk_depend_of if they raise me
    def is_no_check_dependent(self):
        now = time.time()
        for (dep, status, type, tp, inh_parent) in self.chk_depend_of:
            if tp is None or tp.is_time_valid(now):
                if dep.do_i_raise_dependency(status, inh_parent):
                    return True
        return False


    # call by a bad consume check where item see that he have dep
    # and maybe he is not in real fault.
    def raise_dependencies_check(self, ref_check):
        now = time.time()
        cls = self.__class__
        checks = []
        for (dep, status, type, tp, inh_par) in self.act_depend_of:
            # If the dep timeperiod is not valid, do notraise the dep,
            # None=everytime
            if tp is None or tp.is_time_valid(now):
                # if the update is 'fresh', do not raise dep,
                # cached_check_horizon = cached_service_check_horizon for service
                if dep.last_state_update < now - cls.cached_check_horizon:
                    # Fred : passive only checked host dependency ...
                    i = dep.launch_check(now, ref_check, dependent=True)
                    # i = dep.launch_check(now, ref_check)
                    if i is not None:
                        checks.append(i)
#                else:
#                    print "DBG: **************** The state is FRESH", dep.host_name, time.asctime(time.localtime(dep.last_state_update))
        return checks


    # Main scheduling function
    # If a check is in progress, or active check are disabled, do
    # not schedule a check.
    # The check interval change with HARD state or not:
    # SOFT: retry_interval
    # HARD: check_interval
    # The first scheduling is evenly distributed, so all checks
    # are not launched at the same time.
    def schedule(self, force=False, force_time=None):
        # if last_chk == 0 put in a random way so all checks
        # are not in the same time

        # next_chk il already set, do not change
        # unless we force the check or the time
        if self.in_checking and not (force or force_time):
            return None

        cls = self.__class__
        # if no active check and no force, no check
        if (not self.active_checks_enabled or not cls.execute_checks) and not force:
            return None

        now = time.time()

        # If check_interval is 0, we should not add it for a service
        # but suppose a 5min sched for hosts
        if self.check_interval == 0 and not force:
            if cls.my_type == 'service':
                return None
            else:  # host
                self.check_interval = 300 / cls.interval_length

        # Interval change is in a HARD state or not
        # If the retry is 0, take the normal value
        if self.state_type == 'HARD' or self.retry_interval == 0:
            interval = self.check_interval * cls.interval_length
        else:  # TODO: if no retry_interval?
            interval = self.retry_interval * cls.interval_length

        # Determine when a new check (randomize and distribute next check time)
        # or recurring check should happen.
        if self.next_chk == 0:
            # At the start, we cannot have an interval more than cls.max_check_spread
            # is service_max_check_spread or host_max_check_spread in config
            interval = min(interval, cls.max_check_spread * cls.interval_length)
            time_add = interval * random.uniform(0.0, 1.0)
        else:
            time_add = interval

        ## Do the actual Scheduling now

        # If not force_time, try to schedule
        if force_time is None:

            # Do not calculate next_chk based on current time, but based on the last check execution time.
            # Important for consistency of data for trending.
            if self.next_chk == 0 or self.next_chk is None:
                self.next_chk = now

            # If the neck_chk is already in the future, do not touch it.
            # But if ==0, means was 0 in fact, schedule it too
            if self.next_chk <= now:
                # maybe we do not have a check_period, if so, take always good (24x7)
                if self.check_period:
                    self.next_chk = self.check_period.get_next_valid_time_from_t(self.next_chk + time_add)
                else:
                    self.next_chk = int(self.next_chk + time_add)

            # Maybe we load next_chk from retention and  the value of the next_chk is still the past even
            # after add an interval
            if self.next_chk < now:
                interval = min(interval, cls.max_check_spread * cls.interval_length)
                time_add = interval * random.uniform(0.0, 1.0)

                # if we got a check period, use it, if now, use now
                if self.check_period:
                    self.next_chk = self.check_period.get_next_valid_time_from_t(now + time_add)
                else:
                    self.next_chk = int(now + time_add)
            # else: keep the self.next_chk value in the future
        else:
            self.next_chk = int(force_time)

        # If next time is None, do not go
        if self.next_chk is None:
            # Nagios do not raise it, I'm wondering if we should
            return None

        # Get the command to launch, and put it in queue
        self.launch_check(self.next_chk, force=force)


    # If we've got a system time change, we need to compensate it
    # If modify all past value. For active one like next_chk, it's the current
    # checks that will give us the new value
    def compensate_system_time_change(self, difference):
        # We only need to change some value
        for p in on_time_change_update:
            val = getattr(self, p)  # current value
            # Do not go below 1970 :)
            val = max(0, val + difference)  # diff may be negative
            setattr(self, p, val)


    # For disabling active checks, we need to set active_checks_enabled
    # to false, but also make a dummy current checks attempts so the
    # effect is immediate.
    def disable_active_checks(self):
        self.active_checks_enabled = False
        for c in self.checks_in_progress:
            c.status = 'waitconsume'
            c.exit_status = self.state_id
            c.output = self.output
            c.check_time = time.time()
            c.execution_time = 0
            c.perf_data = self.perf_data


    def remove_in_progress_check(self, c):
        # The check is consumed, update the in_checking properties
        if c in self.checks_in_progress:
            self.checks_in_progress.remove(c)
        self.update_in_checking()


    # Is in checking if and only if there are still checks not consumed
    def update_in_checking(self):
        self.in_checking = (len(self.checks_in_progress) != 0)


    # Del just a notification that is returned
    def remove_in_progress_notification(self, n):
        if n.id in self.notifications_in_progress:
            n.status = 'zombie'
            del self.notifications_in_progress[n.id]


    # We do not need ours currents pending notifications,
    # so we zombify them and clean our list
    def remove_in_progress_notifications(self):
        for n in self.notifications_in_progress.values():
            self.remove_in_progress_notification(n)


    # Get a event handler if item got an event handler
    # command. It must be enabled locally and globally
    def get_event_handlers(self, externalcmd=False):
        cls = self.__class__

        # The external command always pass
        # if not, only if we enable them (auto launch)
        if self.event_handler is None or ((not self.event_handler_enabled or not cls.enable_event_handlers) and not externalcmd):
            return

        # If we do not force and we are in downtime, bailout
        # if the no_event_handlers_during_downtimes is 1 in conf
        if cls.no_event_handlers_during_downtimes and not externalcmd and self.in_scheduled_downtime:
            return

        m = MacroResolver()
        data = self.get_data_for_event_handler()
        cmd = m.resolve_command(self.event_handler, data)
        rt = self.event_handler.reactionner_tag
        e = EventHandler(cmd, timeout=cls.event_handler_timeout, \
                             ref=self, reactionner_tag=rt)
        #print "DBG: Event handler call created"
        #print "DBG: ",e.__dict__
        self.raise_event_handler_log_entry(self.event_handler)

        # ok we can put it in our temp action queue
        self.actions.append(e)


    # Whenever a non-ok hard state is reached, we must check whether this
    # host/service has a flexible downtime waiting to be activated
    def check_for_flexible_downtime(self):
        status_updated = False
        for dt in self.downtimes:
            # activate flexible downtimes (do not activate triggered downtimes)
            if dt.fixed == False and dt.is_in_effect == False and dt.start_time <= self.last_chk and self.state_id != 0 and dt.trigger_id == 0:
                n = dt.enter()  # returns downtimestart notifications
                if n is not None:
                    self.actions.append(n)
                status_updated = True
        if status_updated == True:
            self.broks.append(self.get_update_status_brok())


    # UNKNOWN during a HARD state are not so important, and they should
    # not raise notif about it
    def update_hard_unknown_phase_state(self):
        self.was_in_hard_unknown_reach_phase = self.in_hard_unknown_reach_phase

        # We do not care about SOFT state at all
        # and we are sure we are no more in such a phase
        if self.state_type != 'HARD' or self.last_state_type != 'HARD':
            self.in_hard_unknown_reach_phase = False

        # So if we are not in already in such a phase, we check for
        # a start or not. So here we are sure to be in a HARD/HARD following
        # state
        if not self.in_hard_unknown_reach_phase:
            if self.state == 'UNKNOWN' and self.last_state != 'UNKNOWN' \
            or self.state == 'UNREACHABLE' and self.last_state != 'UNREACHABLE':
                self.in_hard_unknown_reach_phase = True
                # We also backup with which state we was before enter this phase
                self.state_before_hard_unknown_reach_phase = self.last_state
                return
        else:
            # if we were already in such a phase, look for its end
            if self.state != 'UNKNOWN' and self.state != 'UNREACHABLE':
                self.in_hard_unknown_reach_phase = False

        # If we just exit the phase, look if we exit with a different state
        # than we enter or not. If so, lie and say we were not in such phase
        # because we need so to raise a new notif
        if not self.in_hard_unknown_reach_phase and self.was_in_hard_unknown_reach_phase:
            if self.state != self.state_before_hard_unknown_reach_phase:
                self.was_in_hard_unknown_reach_phase = False


    # consume a check return and send action in return
    # main function of reaction of checks like raise notifications
    # Special case:
    # is_flapping: immediate notif when problem
    # is_in_scheduled_downtime: no notification
    # is_volatile: notif immediately (service only)
    def consume_result(self, c):
        OK_UP = self.__class__.ok_up  # OK for service, UP for host

        # Protect against bad type output
        # if str, go in unicode
        if isinstance(c.output, str):
            c.output = c.output.decode('utf8', 'ignore')
            c.long_output = c.long_output.decode('utf8', 'ignore')

        # Same for current output
        # TODO: remove in future version, this is need only for
        # migration from old shinken version, that got output as str
        # and not unicode
        # if str, go in unicode
        if isinstance(self.output, str):
            self.output = self.output.decode('utf8', 'ignore')
            self.long_output = self.long_output.decode('utf8', 'ignore')

        if isinstance(c.perf_data, str):
            c.perf_data = c.perf_data.decode('utf8', 'ignore')

        # We check for stalking if necessary
        # so if check is here
        self.manage_stalking(c)

        # Latency can be <0 is we get a check from the retention file
        # so if <0, set 0
        try:
            self.latency = max(0, c.check_time - c.t_to_go)
        except TypeError:
            pass

        # Ok, the first check is done
        self.has_been_checked = 1

        # Now get data from check
        self.execution_time = c.execution_time
        self.u_time = c.u_time
        self.s_time = c.s_time
        self.last_chk = int(c.check_time)

        # Get output and forgot bad UTF8 values for simple str ones
        # (we can get already unicode with external commands)
        self.output = c.output
        self.long_output = c.long_output

        # Set the check result type also in the host/service
        # 0 = result came from an active check
        # 1 = result came from a passive check
        self.check_type = c.check_type

        # Get the perf_data only if we want it in the configuration
        if self.__class__.process_performance_data and self.process_perf_data:
            self.last_perf_data = self.perf_data
            self.perf_data = c.perf_data

        # Before setting state, modulate them
        for rm in self.resultmodulations:
            if rm is not None:
                c.exit_status = rm.module_return(c.exit_status)

        # If we got a bad result on a normal check, and we have dep,
        # we raise dep checks
        # put the actual check in waitdep and we return all new checks
        if c.exit_status != 0 and c.status == 'waitconsume' and len(self.act_depend_of) != 0:
            c.status = 'waitdep'
            # Make sure the check know about his dep
            # C is my check, and he wants dependencies
            checks_id = self.raise_dependencies_check(c)
            for check_id in checks_id:
                # Get checks_id of dep
                c.depend_on.append(check_id)
            # Ok, no more need because checks are not
            # take by host/service, and not returned

        # remember how we was before this check
        self.last_state_type = self.state_type

        self.set_state_from_exit_status(c.exit_status)

        # we change the state, do whatever we are or not in
        # an impact mode, we can put it
        self.state_changed_since_impact = True

        # The check is consumed, update the in_checking properties
        self.remove_in_progress_check(c)

        # C is a check and someone wait for it
        if c.status == 'waitconsume' and c.depend_on_me != []:
            c.status = 'havetoresolvedep'

        # if finish, check need to be set to a zombie state to be removed
        # it can be change if necessary before return, like for dependencies
        if c.status == 'waitconsume' and c.depend_on_me == []:
            c.status = 'zombie'

        # Use to know if notif is raise or not
        no_action = False

        # C was waitdep, but now all dep are resolved, so check for deps
        if c.status == 'waitdep':
            if c.depend_on_me != []:
                c.status = 'havetoresolvedep'
            else:
                c.status = 'zombie'
            # Check deps
            no_action = self.is_no_action_dependent()
            # We recheck just for network_dep. Maybe we are just unreachable
            # and we need to override the state_id
            self.check_and_set_unreachability()

        # OK following a previous OK. perfect if we were not in SOFT
        if c.exit_status == 0 and self.last_state in (OK_UP, 'PENDING'):
            #print "Case 1 (OK following a previous OK): code:%s last_state:%s" % (c.exit_status, self.last_state)
            self.unacknowledge_problem()
            # action in return can be notification or other checks (dependencies)
            if (self.state_type == 'SOFT') and self.last_state != 'PENDING':
                if self.is_max_attempts() and self.state_type == 'SOFT':
                    self.state_type = 'HARD'
                else:
                    self.state_type = 'SOFT'
            else:
                self.attempt = 1
                self.state_type = 'HARD'

        # OK following a NON-OK.
        elif c.exit_status == 0 and self.last_state not in (OK_UP, 'PENDING'):
            self.unacknowledge_problem()
            #print "Case 2 (OK following a NON-OK): code:%s last_state:%s" % (c.exit_status, self.last_state)
            if self.state_type == 'SOFT':
                # OK following a NON-OK still in SOFT state
                if not c.is_dependent():
                    self.add_attempt()
                self.raise_alert_log_entry()
                # Eventhandler gets OK;SOFT;++attempt, no notification needed
                self.get_event_handlers()
                # Internally it is a hard OK
                self.state_type = 'HARD'
                self.attempt = 1
            elif self.state_type == 'HARD':
                # OK following a HARD NON-OK
                self.raise_alert_log_entry()
                # Eventhandler and notifications get OK;HARD;maxattempts
                # Ok, so current notifications are not needed, we 'zombie' them
                self.remove_in_progress_notifications()
                if not no_action:
                    self.create_notifications('RECOVERY')
                self.get_event_handlers()
                # Internally it is a hard OK
                self.state_type = 'HARD'
                self.attempt = 1

                #self.update_hard_unknown_phase_state()
                # I'm no more a problem if I was one
                self.no_more_a_problem()

        # Volatile part
        # Only for service
        elif c.exit_status != 0 and getattr(self, 'is_volatile', False):
            #print "Case 3 (volatile only)"
            # There are no repeated attempts, so the first non-ok results
            # in a hard state
            self.attempt = 1
            self.state_type = 'HARD'
            # status != 0 so add a log entry (before actions that can also raise log
            # it is smarter to log error before notification)
            self.raise_alert_log_entry()
            self.check_for_flexible_downtime()
            self.remove_in_progress_notifications()
            if not no_action:
                self.create_notifications('PROBLEM')
            # Ok, event handlers here too
            self.get_event_handlers()

            # PROBLEM/IMPACT
            # I'm a problem only if I'm the root problem,
            # so not no_action:
            if not no_action:
                self.set_myself_as_problem()

        # NON-OK follows OK. Everything was fine, but now trouble is ahead
        elif c.exit_status != 0 and self.last_state in (OK_UP, 'PENDING'):
            #print "Case 4: NON-OK follows OK: code:%s last_state:%s" % (c.exit_status, self.last_state)
            if self.is_max_attempts():
                # if max_attempts == 1 we're already in deep trouble
                self.state_type = 'HARD'
                self.raise_alert_log_entry()
                self.remove_in_progress_notifications()
                self.check_for_flexible_downtime()
                if not no_action:
                    self.create_notifications('PROBLEM')
                # Oh? This is the typical go for a event handler :)
                self.get_event_handlers()

                # PROBLEM/IMPACT
                # I'm a problem only if I'm the root problem,
                # so not no_action:
                if not no_action:
                    self.set_myself_as_problem()

            else:
                # This is the first NON-OK result. Initiate the SOFT-sequence
                # Also launch the event handler, he might fix it.
                self.attempt = 1
                self.state_type = 'SOFT'
                self.raise_alert_log_entry()
                self.get_event_handlers()

        # If no OK in a no OK: if hard, still hard, if soft,
        # check at self.max_check_attempts
        # when we go in hard, we send notification
        elif c.exit_status != 0 and self.last_state != OK_UP:
            #print "Case 5 (no OK in a no OK): code:%s last_state:%s state_type:%s" % (c.exit_status, self.last_state,self.state_type)
            if self.state_type == 'SOFT':
                if not c.is_dependent():
                    self.add_attempt()
                if self.is_max_attempts():
                    # Ok here is when we just go to the hard state
                    self.state_type = 'HARD'
                    self.raise_alert_log_entry()
                    self.remove_in_progress_notifications()
                    # There is a request in the Nagios trac to enter downtimes
                    # on soft states which does make sense. If this becomes
                    # the default behavior, just move the following line
                    # into the else-branch below.
                    self.check_for_flexible_downtime()
                    if not no_action:
                        self.create_notifications('PROBLEM')
                    # So event handlers here too
                    self.get_event_handlers()

                    # PROBLEM/IMPACT
                    # I'm a problem only if I'm the root problem,
                    # so not no_action:
                    if not no_action:
                        self.set_myself_as_problem()

                else:
                    self.raise_alert_log_entry()
                    # eventhandler is launched each time during the soft state
                    self.get_event_handlers()
            else:
                # Send notifications whenever the state has changed. (W -> C)
                # but not if the current state is UNKNOWN (hard C-> hard U -> hard C should
                # not restart notifications)
                if self.state != self.last_state:
                    self.update_hard_unknown_phase_state()
                    #print self.last_state, self.last_state_type, self.state_type, self.state
                    if not self.in_hard_unknown_reach_phase and not self.was_in_hard_unknown_reach_phase:
                        self.unacknowledge_problem_if_not_sticky()
                        self.raise_alert_log_entry()
                        self.remove_in_progress_notifications()
                        if not no_action:
                            self.create_notifications('PROBLEM')

                elif self.in_scheduled_downtime_during_last_check == True:
                    # during the last check i was in a downtime. but now
                    # the status is still critical and notifications
                    # are possible again. send an alert immediately
                    self.remove_in_progress_notifications()
                    if not no_action:
                        self.create_notifications('PROBLEM')

                # PROBLEM/IMPACT
                # Forces problem/impact registration even if no state change
                # was detected as we may have a non OK state restored from
                # retetion data. This way, we rebuild problem/impact hierarchy.
                # I'm a problem only if I'm the root problem,
                # so not no_action:
                if not no_action:
                    self.set_myself_as_problem()

        self.update_hard_unknown_phase_state()
        # Reset this flag. If it was true, actions were already taken
        self.in_scheduled_downtime_during_last_check = False

        # now is the time to update state_type_id
        # and our last_hard_state
        if self.state_type == 'HARD':
            self.state_type_id = 1
            self.last_hard_state = self.state
            self.last_hard_state_id = self.state_id
        else:
            self.state_type_id = 0

        # Fill last_hard_state_change to now
        # if we just change from SOFT->HARD or
        # in HARD we change of state (Warning->critical, or critical->ok, etc etc)
        if self.state_type == 'HARD' and (self.last_state_type == 'SOFT' or self.last_state != self.state):
            self.last_hard_state_change = int(time.time())

        # update event/problem-counters
        self.update_event_and_problem_id()

        # Now launch trigger if need. If it's from a trigger raised check,
        # do not raise a new one
        if not c.from_trigger:
            self.eval_triggers()
        if c.from_trigger or not c.from_trigger and len([t for t in self.triggers if t.trigger_broker_raise_enabled]) == 0 :
            self.broks.append(self.get_check_result_brok())

        self.get_obsessive_compulsive_processor_command()
        self.get_perfdata_command()


    def update_event_and_problem_id(self):
        OK_UP = self.__class__.ok_up  # OK for service, UP for host
        if (self.state != self.last_state and self.last_state != 'PENDING'
                or self.state != OK_UP and self.last_state == 'PENDING'):
            SchedulingItem.current_event_id += 1
            self.last_event_id = self.current_event_id
            self.current_event_id = SchedulingItem.current_event_id
            # now the problem_id
            if self.state != OK_UP and self.last_state == 'PENDING':
                # broken ever since i can remember
                SchedulingItem.current_problem_id += 1
                self.last_problem_id = self.current_problem_id
                self.current_problem_id = SchedulingItem.current_problem_id
            elif self.state != OK_UP and self.last_state != OK_UP:
                # State transitions between non-OK states
                # (e.g. WARNING to CRITICAL) do not cause
                # this problem id to increase.
                pass
            elif self.state == OK_UP:
                # If the service is currently in an OK state,
                # this macro will be set to zero (0).
                self.last_problem_id = self.current_problem_id
                self.current_problem_id = 0
            else:
                # Every time a service (or host) transitions from
                # an OK or UP state to a problem state, a global
                # problem ID number is incremented by one (1).
                SchedulingItem.current_problem_id += 1
                self.last_problem_id = self.current_problem_id
                self.current_problem_id = SchedulingItem.current_problem_id


    # Called by scheduler when a notification is
    # ok to be send (so fully prepared to be send
    # to reactionner). Here we update the command with
    # status of now, and we add the contact to set of
    # contact we notified. And we raise the log entry
    def prepare_notification_for_sending(self, n):
        if n.status == 'inpoller':
            self.update_notification_command(n)
            self.notified_contacts.add(n.contact)
            self.raise_notification_log_entry(n)


    # Just update the notification command by resolving Macros
    # And because we are just launching the notification, we can say
    # that this contact have been notified
    def update_notification_command(self, n):
        cls = self.__class__
        m = MacroResolver()
        data = self.get_data_for_notifications(n.contact, n)
        n.command = m.resolve_command(n.command_call, data)
        if cls.enable_environment_macros or n.enable_environment_macros:
            n.env = m.get_env_macros(data)


    # See if an escalation is eligible at t and notif nb=n
    def is_escalable(self, n):
        cls = self.__class__

        # We search since when we are in notification for escalations
        # that are based on time
        in_notif_time = time.time() - n.creation_time

        # Check is an escalation match the current_notification_number
        for es in self.escalations:
            if es.is_eligible(n.t_to_go, self.state, n.notif_nb, in_notif_time, cls.interval_length):
                return True

        return False


    # Give for a notification the next notification time
    # by taking the standard notification_interval or ask for
    # our escalation if one of them need a smaller value to escalade
    def get_next_notification_time(self, n):
        res = None
        now = time.time()
        cls = self.__class__

        # Look at the minimum notification interval
        notification_interval = self.notification_interval
        # and then look for currently active notifications, and take notification_interval
        # if filled and less than the self value
        in_notif_time = time.time() - n.creation_time
        for es in self.escalations:
            if es.is_eligible(n.t_to_go, self.state, n.notif_nb, in_notif_time, cls.interval_length):
                if es.notification_interval != -1 and es.notification_interval < notification_interval:
                    notification_interval = es.notification_interval

        # So take the by default time
        std_time = n.t_to_go + notification_interval * cls.interval_length

        # Maybe the notification comes from retention data and next notification alert is in the past
        # if so let use the now value instead
        if std_time < now:
            std_time = now + notification_interval * cls.interval_length

        # standard time is a good one
        res = std_time

        creation_time = n.creation_time
        in_notif_time = now - n.creation_time

        for es in self.escalations:
            # If the escalation was already raised, we do not look for a new "early start"
            if es.get_name() not in n.already_start_escalations:
                r = es.get_next_notif_time(std_time, self.state, creation_time, cls.interval_length)
                # If we got a real result (time base escalation), we add it
                if r is not None and now < r < res:
                    res = r

        # And we take the minimum of this result. Can be standard or escalation asked
        return res


    # Get all contacts (uniq) from eligible escalations
    def get_escalable_contacts(self, n):
        cls = self.__class__

        # We search since when we are in notification for escalations
        # that are based on this time
        in_notif_time = time.time() - n.creation_time

        contacts = set()
        for es in self.escalations:
            if es.is_eligible(n.t_to_go, self.state, n.notif_nb, in_notif_time, cls.interval_length):
                contacts.update(es.contacts)
                # And we tag this escalations as started now
                n.already_start_escalations.add(es.get_name())

        return list(contacts)

    # Create a "master" notification here, which will later
    # (immediately before the reactionner gets it) be split up
    # in many "child" notifications, one for each contact.
    def create_notifications(self, type, t_wished=None):
        cls = self.__class__
        # t_wished==None for the first notification launch after consume
        # here we must look at the self.notification_period
        if t_wished is None:
            now = time.time()
            t_wished = now
            # if first notification, we must add first_notification_delay
            if self.current_notification_number == 0 and type == 'PROBLEM':
                last_time_non_ok_or_up = self.last_time_non_ok_or_up()
                if last_time_non_ok_or_up == 0:
                    # this happens at initial
                    t_wished = now + self.first_notification_delay * cls.interval_length
                else:
                    t_wished = last_time_non_ok_or_up + self.first_notification_delay * cls.interval_length
            if self.notification_period is None:
                t = int(now)
            else:
                t = self.notification_period.get_next_valid_time_from_t(t_wished)
        else:
            # We follow our order
            t = t_wished

        if self.notification_is_blocked_by_item(type, t_wished) and self.first_notification_delay == 0 and self.notification_interval == 0:
            # If notifications are blocked on the host/service level somehow
            # and repeated notifications are not configured,
            # we can silently drop this one
            return

        if type == 'PROBLEM':
            # Create the notification with an incremented notification_number.
            # The current_notification_number  of the item itself will only
            # be incremented when this notification (or its children)
            # have actually be sent.
            next_notif_nb = self.current_notification_number + 1
        elif type == 'RECOVERY':
            # Recovery resets the notification counter to zero
            self.current_notification_number = 0
            next_notif_nb = self.current_notification_number
        else:
            # downtime/flap/etc do not change the notification number
            next_notif_nb = self.current_notification_number

        n = Notification(type, 'scheduled', 'VOID', None, self, None, t, \
            timeout=cls.notification_timeout, \
            notif_nb=next_notif_nb)

        # Keep a trace in our notifications queue
        self.notifications_in_progress[n.id] = n
        # and put it in the temp queue for scheduler
        self.actions.append(n)


    # In create_notifications we created a notification "template". When it's
    # time to hand it over to the reactionner, this master notification needs
    # to be split in several child notifications, one for each contact
    # To be more exact, one for each contact who is willing to accept
    # notifications of this type and at this time
    def scatter_notification(self, n):
        cls = self.__class__
        childnotifications = []

        if n.contact:
            # only master notifications can be split up
            return []
        if n.type == 'RECOVERY':
            if self.first_notification_delay != 0 and len(self.notified_contacts) == 0:
                # Recovered during first_notification_delay. No notifications
                # have been sent yet, so we keep quiet
                contacts = []
            else:
                # The old way. Only send recover notifications to those contacts
                # who also got problem notifications
                contacts = list(self.notified_contacts)
            self.notified_contacts.clear()
        else:
            # Check is an escalation match. If yes, get all contacts from escalations
            if self.is_escalable(n):
                contacts = self.get_escalable_contacts(n)
            # else take normal contacts
            else:
                contacts = self.contacts

        for contact in contacts:
            # We do not want to notify again a contact with notification interval == 0 that has been already
            # notified. Can happen when a service exit a dowtime and still in crit/warn (and not ack)
            if n.type == "PROBLEM" and self.notification_interval == 0 and contact in self.notified_contacts:
                continue
            # Get the property name for notif commands, like
            # service_notification_commands for service
            notif_commands = contact.get_notification_commands(cls.my_type)

            for cmd in notif_commands:
                rt = cmd.reactionner_tag
                child_n = Notification(n.type, 'scheduled', 'VOID', cmd, self,
                    contact, n.t_to_go, timeout=cls.notification_timeout,
                    notif_nb=n.notif_nb, reactionner_tag=rt, module_type=cmd.module_type,
                    enable_environment_macros=cmd.enable_environment_macros)
                if not self.notification_is_blocked_by_contact(child_n, contact):
                    # Update the notification with fresh status information
                    # of the item. Example: during the notification_delay
                    # the status of a service may have changed from WARNING to CRITICAL
                    self.update_notification_command(child_n)
                    self.raise_notification_log_entry(child_n)
                    self.notifications_in_progress[child_n.id] = child_n
                    childnotifications.append(child_n)

                    if n.type == 'PROBLEM':
                        # Remember the contacts. We might need them later in the
                        # recovery code some lines above
                        self.notified_contacts.add(contact)

        return childnotifications


    # return a check to check the host/service
    # and return id of the check
    # Fred : passive only checked host dependency
    def launch_check(self, t, ref_check=None, force=False, dependent=False):
    # def launch_check(self, t, ref_check=None, force=False):
        c = None
        cls = self.__class__

        # Look if we are in check or not
        self.update_in_checking()

        # the check is being forced, so we just replace next_chk time by now
        if force and self.in_checking:
            now = time.time()
            c_in_progress = self.checks_in_progress[0]
            c_in_progress.t_to_go = now
            return c_in_progress.id

        # If I'm already in checking, Why launch a new check?
        # If ref_check_id is not None , this is a dependency_ check
        # If none, it might be a forced check, so OK, I do a new

        # Dependency check, we have to create a new check that will be launched only once (now)
        # Otherwise it will delay the next real check. this can lead to an infinite SOFT state.
        if not force and (self.in_checking and ref_check is not None):

            c_in_progress = self.checks_in_progress[0]  # 0 is OK because in_checking is True

            # c_in_progress has almost everything we need but we cant copy.deepcopy() it
            # we need another c.id
            command_line = c_in_progress.command
            timeout = c_in_progress.timeout
            poller_tag = c_in_progress.poller_tag
            env = c_in_progress.env
            module_type = c_in_progress.module_type

            c = Check('scheduled', command_line, self, t, ref_check,
                      timeout=timeout,
                      poller_tag=poller_tag,
                      env=env,
                      module_type=module_type,
                      dependency_check=True)

            self.actions.append(c)
            #print "Creating new check with new id : %d, old id : %d" % (c.id, c_in_progress.id)
            return c.id

        if force or (not self.is_no_check_dependent()):
            # Fred : passive only checked host dependency
            if dependent and self.my_type == 'host' and self.passive_checks_enabled and not self.active_checks_enabled:
                logger.debug("Host check is for an host that is only passively checked (%s), do not launch the check !" % (self.host_name))
                return None
            
            # By default we will use our default check_command
            check_command = self.check_command
            # But if a checkway is available, use this one instead.
            # Take the first available
            for cw in self.checkmodulations:
                c_cw = cw.get_check_command(t)
                if c_cw:
                    check_command = c_cw
                    break

            # Get the command to launch
            m = MacroResolver()
            data = self.get_data_for_checks()
            command_line = m.resolve_command(check_command, data)
            # By default env is void
            env = {}

            # And get all environment variables only if needed
            if cls.enable_environment_macros or check_command.enable_environment_macros:
               env = m.get_env_macros(data)

            # By default we take the global timeout, but we use the command one if it
            # define it (by default it's -1)
            timeout = cls.check_timeout
            if check_command.timeout != -1:
                timeout = check_command.timeout

            # Make the Check object and put the service in checking
            # Make the check inherit poller_tag from the command
            # And reactionner_tag too
            c = Check('scheduled', command_line, self, t, ref_check, \
                      timeout=timeout, \
                      poller_tag=check_command.poller_tag, \
                      env=env, \
                      module_type=check_command.module_type)

            # We keep a trace of all checks in progress
            # to know if we are in checking_or not
            self.checks_in_progress.append(c)
        self.update_in_checking()

        # We need to put this new check in our actions queue
        # so scheduler can take it
        if c is not None:
            self.actions.append(c)
            return c.id
        # None mean I already take it into account
        return None


    # returns either 0 or a positive number
    # 0 == don't check for orphans
    # non-zero == number of secs that can pass before
    #             marking the check an orphan.
    def get_time_to_orphanage(self):
        # if disabled program-wide, disable it
        if not self.check_for_orphaned:
            return 0
        # otherwise, check what my local conf says
        if self.time_to_orphanage <= 0:
            return 0
        return self.time_to_orphanage


    # Get the perfdata command with macro resolved for this
    def get_perfdata_command(self):
        cls = self.__class__
        if not cls.process_performance_data or not self.process_perf_data:
            return

        if cls.perfdata_command is not None:
            m = MacroResolver()
            data = self.get_data_for_event_handler()
            cmd = m.resolve_command(cls.perfdata_command, data)
            reactionner_tag = cls.perfdata_command.reactionner_tag
            e = EventHandler(cmd, timeout=cls.perfdata_timeout,
                             ref=self, reactionner_tag=reactionner_tag)

            # ok we can put it in our temp action queue
            self.actions.append(e)


    # Create the whole business rule tree
    # if we need it
    def create_business_rules(self, hosts, services, running=False):
        cmdCall = getattr(self, 'check_command', None)

        # If we do not have a command, we bailout
        if cmdCall is None:
            return

        # we get our based command, like
        # check_tcp!80 -> check_tcp
        cmd = cmdCall.call
        elts = cmd.split('!')
        base_cmd = elts[0]

        # If it's bp_rule, we got a rule :)
        if base_cmd == 'bp_rule':
            #print "Got rule", elts, cmd
            self.got_business_rule = True
            rule = ''
            if len(elts) >= 2:
                rule = '!'.join(elts[1:])
            # Only (re-)evaluate the business rule if it has never been
            # evaluated before, or it contains a macro.
            if re.match(r"\$[\w\d_-]+\$", rule) or self.business_rule is None:
                data = self.get_data_for_checks()
                m = MacroResolver()
                rule = m.resolve_simple_macros_in_string(rule, data)
                prev = getattr(self, "processed_business_rule", "")

                if rule == prev:
                    # Business rule did not change (no macro was modulated)
                    return

                fact = DependencyNodeFactory(self)
                node = fact.eval_cor_pattern(rule, hosts, services, running)
                #print "got node", node
                self.processed_business_rule = rule
                self.business_rule = node


    # Returns a status string for business rules based services, formatted
    # using format string as output template.
    # The template may embed output formatting for itself, and for its child
    # (dependant) itmes. Childs format string is expanded into the $( and )$,
    # using the string between brackets as format string.
    # Output is generated by expanding the following macros.
    #
    #   SERVICE_DESCRIPTION:    The service name
    #   HOST_NAME:              The item name
    #   FULL_NAME:              The item name
    #   STATUS:                 The item status string (may be OK, WARN, CRIT,
    #                           UP, DOWN)
    #   SHORT_STATUS:           The item shorten status string (O, W, C, U, D)
    #
    # Caution: only childs in state not OK are displayed.
    #
    # Example:
    #   A business rule with a format string looking like
    #       "$STATUS$ [ $($STATUS$: $HOST_NAME$,$SERVICE_DESCRIPTION$ )$ ]"
    #   Would return
    #       "CRITICAL [ CRITICAL: host1,srv1 WARNING: host2,srv2  ]"
    def get_business_rule_output(self):
        got_business_rule = hasattr(self, 'got_business_rule') and \
                            self.got_business_rule is True
        # Checks that the service is a business rule.
        if got_business_rule is False or self.business_rule is None:
            return ""
        # Checks that the business rule has a format specified.
        output_template = self.business_rule_output_template
        if not output_template:
            return ""
        # Extracts template strings
        # Current service output format string
        service_template_string = re.sub("\$\(.*\)\$", "$CHILDS_OUTPUT$", output_template)
        # Childs output format string
        elts = re.findall(r"\$\((.*)\)\$", output_template)
        if not len(elts):
            child_template_string = ""
        else:
            child_template_string = elts[0]

        # Processes child services output
        childs_output = ""
        childs = self.business_rule.list_all_elements()
        ok_count = 0
        # Expands child items format string macros.
        for child in childs:
            # Do not display childs in OK state
            if child.last_hard_state_id == 0:
                ok_count += 1
                continue
            childs_output += self.expand_business_rule_item_macros(child_template_string, child)

        if ok_count == len(childs):
            childs_output = "all checks were successful."


        # Expands node's template string macros.
        # State has to be set manually, as the service state attribute is only
        # set on a next scheduler step.
        output = service_template_string
        mapping = {0: "OK", 1: "WARNING", 2: "CRITICAL", 3: "UNKNOWN"}
        status = mapping[self.business_rule.get_state()]
        output = re.sub(r"\$STATUS\$", status, output, flags=re.I)
        short_status = self.status_to_short_status(status)
        output = re.sub(r"\$SHORT_STATUS\$", short_status, output, flags=re.I)
        output = self.expand_business_rule_item_macros(output, self)
        output = re.sub("\$CHILDS_OUTPUT\$", childs_output, output)
        return output.strip()


    # Expands format string macros with item attributes
    def expand_business_rule_item_macros(self, template_string, item):
        output = template_string
        status = getattr(item, "state", "")
        output = re.sub(r"\$STATUS\$", status, output, flags=re.I)
        short_status = self.status_to_short_status(status)
        output = re.sub(r"\$SHORT_STATUS\$", short_status, output, flags=re.I)
        service_description = getattr(item, "service_description", "")
        output = re.sub(r"\$SERVICE_DESCRIPTION\$", service_description, output, flags=re.I)
        host_name = getattr(item, "host_name", "")
        output = re.sub(r"\$HOST_NAME\$", host_name, output, flags=re.I)
        full_name = item.get_full_name()
        output = re.sub(r"\$FULL_NAME\$", full_name, output, flags=re.I)
        return output


    # Returns status string shorten name.
    def status_to_short_status(self, status):
        mapping = {
            "OK": "O",
            "WARNING": "W",
            "CRITICAL": "C",
            "UNKNOWN": "U",
            "UP": "U",
            "DOWN": "D"
        }
        return mapping.get(status, status)


    # Processes business rule notifications behaviour. If all problems have
    # been acknowledged, no notifications should be sent if state is not OK.
    # By default, downtimes are ignored, unless explicitely told to be treated
    # as acknowledgements through with the business_rule_downtime_as_ack set.
    def business_rule_notification_is_blocked(self):
        # Walks through problems to check if all items in non ok are
        # acknowledged or in downtime period.
        acknowledged = 0
        for s in self.source_problems:
            if s.last_hard_state_id != 0:
                if s.problem_has_been_acknowledged:
                    # Problem hast been acknowledged
                    acknowledged += 1
                # Only check problems under downtime if we are
                # explicitely told to do so.
                elif self.business_rule_downtime_as_ack is True:
                    if s.scheduled_downtime_depth > 0:
                        # Problem is under downtime, and downtimes should be
                        # traeted as acknowledgements
                        acknowledged += 1
                    elif hasattr(s, "host") and s.host.scheduled_downtime_depth > 0:
                        # Host is under downtime, and downtimes should be
                        # traeted as acknowledgements
                        acknowledged += 1
        if acknowledged == len(self.source_problems):
            return True
        else:
            return False


    # We ask us to manage our own internal check,
    # like a business based one
    def manage_internal_check(self, hosts, services, c):
        #print "DBG, ask me to manage a check!"
        if c.command.startswith('bp_'):
            try:
                # Re evaluate the business rule to take into account macro
                # modulation.
                # Caution: We consider the that the macro modulation did not
                # change business rule dependency tree. Only Xof: values should
                # be modified by modulation.
                self.create_business_rules(hosts, services, running=True)
                state = self.business_rule.get_state()
                c.output = self.get_business_rule_output()
            except Exception, e:
                # Notifies the error, and return an UNKNOWN state.
                c.output = "Error while re-evaluating business rule: %s" % e
                logger.debug("[%s] Error while re-evaluating business rule:\n%s" %
                             (self.get_name(), traceback.format_exc()))
                state = 3
        # _internal_host_up is for putting host as UP
        elif c.command == '_internal_host_up':
            state = 0
            c.execution_time = 0
            c.output = 'Host assumed to be UP'
        # Echo is just putting the same state again
        elif c.command == '_echo':
            state = self.state
            c.execution_time = 0
            c.output = self.output
        c.long_output = c.output
        c.check_time = time.time()
        c.exit_status = state
        #print "DBG, setting state", state


    # If I'm a business rule service/host, I register myself to the
    # elements I will depend on, so They will have ME as an impact
    def create_business_rules_dependencies(self):
        if self.got_business_rule:
            #print "DBG: ask me to register me in my dependencies", self.get_name()
            elts = self.business_rule.list_all_elements()
            # I will register myself in this
            for e in elts:
                #print "I register to the element", e.get_name()
                # all states, every timeperiod, and inherit parents
                e.add_business_rule_act_dependency(self, ['d', 'u', 's', 'f', 'c', 'w'], None, True)
                # Enforces child hosts/services notification options if told to
                # do so (business_rule_(host|service)_notification_options)
                # set.
                if e.my_type == "host" and self.business_rule_host_notification_options:
                    e.notification_options = self.business_rule_host_notification_options
                if e.my_type == "service" and self.business_rule_service_notification_options:
                    e.notification_options = self.business_rule_service_notification_options


    def rebuild_ref(self):
        """ Rebuild the possible reference a schedulingitem can have """
        for g in self.comments, self.downtimes:
            for o in g:
                o.ref = self


    # Go launch all our triggers
    def eval_triggers(self):
        for t in self.triggers:
            try:
                t.eval(self)
            except Exception, exp:
                logger.error("We got an exception from a trigger on %s for %s" % (self.get_full_name().decode('utf8', 'ignore'), str(traceback.format_exc())))

########NEW FILE########
__FILENAME__ = service
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This Class is the service one, s it manage all service specific thing.
If you look at the scheduling part, look at the scheduling item class"""

import time
import re

try:
    from ClusterShell.NodeSet import NodeSet, NodeSetParseRangeError
except ImportError:
    NodeSet = None

from shinken.objects.item import Items
from shinken.objects.schedulingitem import SchedulingItem

from shinken.autoslots import AutoSlots
from shinken.util import strip_and_uniq, format_t_into_dhms_format, to_svc_hst_distinct_lists, \
    get_key_value_sequence, GET_KEY_VALUE_SEQUENCE_ERROR_SYNTAX, GET_KEY_VALUE_SEQUENCE_ERROR_NODEFAULT, \
    GET_KEY_VALUE_SEQUENCE_ERROR_NODE, to_list_string_of_names, to_list_of_names, to_name_if_possible
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.macroresolver import MacroResolver
from shinken.eventhandler import EventHandler
from shinken.log import logger, console_logger


class Service(SchedulingItem):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    # Every service have a unique ID, and 0 is always special in
    # database and co...
    id = 1
    # The host and service do not have the same 0 value, now yes :)
    ok_up = 'OK'
    # used by item class for format specific value like for Broks
    my_type = 'service'

    # properties defined by configuration
    # required: is required in conf
    # default: default value if no set in conf
    # pythonize: function to call when transforming string to python object
    # fill_brok: if set, send to broker. there are two categories:
    #  full_status for initial and update status, check_result for check results
    # no_slots: do not take this property for __slots__
    properties = SchedulingItem.properties.copy()
    properties.update({
        'host_name':              StringProp(fill_brok=['full_status', 'check_result', 'next_schedule']),
        'hostgroup_name':         StringProp(default='', fill_brok=['full_status'], merging='join'),
        'service_description':    StringProp(fill_brok=['full_status', 'check_result', 'next_schedule']),
        'display_name':           StringProp(default='', fill_brok=['full_status']),
        'servicegroups':          StringProp(default='', fill_brok=['full_status'], brok_transformation=to_list_string_of_names, merging='join'),
        'is_volatile':            BoolProp(default='0', fill_brok=['full_status']),
        'check_command':          StringProp(fill_brok=['full_status']),
        'initial_state':          CharProp(default='o', fill_brok=['full_status']),
        'max_check_attempts':     IntegerProp(default='1',fill_brok=['full_status']),
        'check_interval':         IntegerProp(fill_brok=['full_status']),
        'retry_interval':         IntegerProp(fill_brok=['full_status']),
        'active_checks_enabled':  BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'passive_checks_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'check_period':           StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'obsess_over_service':    BoolProp(default='0', fill_brok=['full_status'], retention=True),
        'check_freshness':        BoolProp(default='0', fill_brok=['full_status']),
        'freshness_threshold':    IntegerProp(default='0', fill_brok=['full_status']),
        'event_handler':          StringProp(default='', fill_brok=['full_status']),
        'event_handler_enabled':  BoolProp(default='0', fill_brok=['full_status'], retention=True),
        'low_flap_threshold':     IntegerProp(default='-1', fill_brok=['full_status']),
        'high_flap_threshold':    IntegerProp(default='-1', fill_brok=['full_status']),
        'flap_detection_enabled': BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'flap_detection_options': ListProp(default='o,w,c,u', fill_brok=['full_status']),
        'process_perf_data':      BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'retain_status_information': BoolProp(default='1', fill_brok=['full_status']),
        'retain_nonstatus_information': BoolProp(default='1', fill_brok=['full_status']),
        'notification_interval':  IntegerProp(default='60', fill_brok=['full_status']),
        'first_notification_delay': IntegerProp(default='0', fill_brok=['full_status']),
        'notification_period':    StringProp(brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'notification_options':   ListProp(default='w,u,c,r,f,s', fill_brok=['full_status']),
        'notifications_enabled':  BoolProp(default='1', fill_brok=['full_status'], retention=True),
        'contacts':               StringProp(default='', brok_transformation=to_list_of_names, fill_brok=['full_status'], merging='join'),
        'contact_groups':         StringProp(default='', fill_brok=['full_status'], merging='join'),
        'stalking_options':       ListProp(default='', fill_brok=['full_status'], merging='join'),
        'notes':                  StringProp(default='', fill_brok=['full_status']),
        'notes_url':              StringProp(default='', fill_brok=['full_status']),
        'action_url':             StringProp(default='', fill_brok=['full_status']),
        'icon_image':             StringProp(default='', fill_brok=['full_status']),
        'icon_image_alt':         StringProp(default='', fill_brok=['full_status']),
        'icon_set':               StringProp(default='', fill_brok=['full_status']),
        'failure_prediction_enabled': BoolProp(default='0', fill_brok=['full_status']),
        'parallelize_check':       BoolProp(default='1', fill_brok=['full_status']),

        # Shinken specific
        'poller_tag':              StringProp(default='None'),
        'reactionner_tag':         StringProp(default='None'),
        'resultmodulations':       StringProp(default='', merging='join'),
        'business_impact_modulations':    StringProp(default='', merging='join'),
        'escalations':             StringProp(default='', fill_brok=['full_status'], merging='join'),
        'maintenance_period':      StringProp(default='', brok_transformation=to_name_if_possible, fill_brok=['full_status']),
        'time_to_orphanage':       IntegerProp(default="300", fill_brok=['full_status']),
        'merge_host_contacts': 	   BoolProp(default='0', fill_brok=['full_status']),
        'labels':                  ListProp(default='', fill_brok=['full_status'], merging='join'),

        # BUSINESS CORRELATOR PART
        # Business rules output format template
        'business_rule_output_template': StringProp(default='', fill_brok=['full_status']),
        # Business rules notifications mode
        'business_rule_smart_notifications': BoolProp(default='0', fill_brok=['full_status']),
        # Treat downtimes as acknowledgements in smart notifications
        'business_rule_downtime_as_ack': BoolProp(default='0', fill_brok=['full_status']),
        # Enforces child nodes notification options
        'business_rule_host_notification_options':    ListProp(default='', fill_brok=['full_status']),
        'business_rule_service_notification_options': ListProp(default='', fill_brok=['full_status']),

        # Easy Service dep definition
        'service_dependencies':   ListProp(default='', merging='join'), # TODO: find a way to brok it?

        # service generator
        'duplicate_foreach':       StringProp(default=''),
        'default_value':           StringProp(default=''),

        # Business_Impact value
        'business_impact':         IntegerProp(default='2', fill_brok=['full_status']),

        # Load some triggers
        'trigger':         StringProp(default=''),
        'trigger_name':    ListProp(default=''),
        'trigger_broker_raise_enabled': BoolProp(default='0'),

        # Trending
        'trending_policies':    ListProp(default='', fill_brok=['full_status'], merging='join'),

        # Our check ways. By defualt void, but will filled by an inner if need
        'checkmodulations':       ListProp(default='', fill_brok=['full_status'], merging='join'),
        'macromodulations':       ListProp(default='', merging='join'),

        # Custom views
        'custom_views'     :    ListProp(default='', fill_brok=['full_status'], merging='join'),

        # UI aggregation
        'aggregation'      :    StringProp(default='', fill_brok=['full_status']),
    })

    # properties used in the running state
    running_properties = SchedulingItem.running_properties.copy()
    running_properties.update({
        'modified_attributes': IntegerProp(default=0L, fill_brok=['full_status'], retention=True),
        'last_chk':           IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'next_chk':           IntegerProp(default=0, fill_brok=['full_status', 'next_schedule'], retention=True),
        'in_checking':        BoolProp(default=False, fill_brok=['full_status', 'check_result', 'next_schedule'], retention=True),
        'in_maintenance':     IntegerProp(default=None, fill_brok=['full_status'], retention=True),
        'latency':            FloatProp(default=0, fill_brok=['full_status', 'check_result'], retention=True,),
        'attempt':            IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'state':              StringProp(default='PENDING', fill_brok=['full_status', 'check_result'], retention=True),
        'state_id':           IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'current_event_id':   IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_event_id':      IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_state':         StringProp(default='PENDING', fill_brok=['full_status', 'check_result'], retention=True),
        'last_state_type':    StringProp(default='HARD', fill_brok=['full_status', 'check_result'], retention=True),
        'last_state_id':      IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_state_change':  FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_hard_state_change': FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_hard_state':    StringProp(default='PENDING', fill_brok=['full_status'], retention=True),
        'last_hard_state_id': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'last_time_ok':       IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_time_warning':  IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_time_critical': IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'last_time_unknown':  IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'duration_sec':       IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'state_type':         StringProp(default='HARD', fill_brok=['full_status', 'check_result'], retention=True),
        'state_type_id':      IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'output':             StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'long_output':        StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'is_flapping':        BoolProp(default=False, fill_brok=['full_status'], retention=True),
        #  dependencies for actions like notif of event handler,
        # so AFTER check return
        'act_depend_of': ListProp(default=[]),
        # dependencies for checks raise, so BEFORE checks
        'chk_depend_of': ListProp(default=[]),
        # elements that depend of me, so the reverse than just upper
        'act_depend_of_me': ListProp(default=[]),
        # elements that depend of me
        'chk_depend_of_me': ListProp(default=[]),

        'last_state_update':  FloatProp(default=0.0, fill_brok=['full_status'], retention=True),
        'checks_in_progress': ListProp(default=[]), # no brok because checks are too linked
        'notifications_in_progress': ListProp(default={}, retention=True), # no broks because notifications are too linked
        'downtimes':          ListProp(default=[], fill_brok=['full_status'], retention=True),
        'comments':           ListProp(default=[], fill_brok=['full_status'], retention=True),
        'flapping_changes':   ListProp(default=[], fill_brok=['full_status'], retention=True),
        'flapping_comment_id': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'percent_state_change': FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'problem_has_been_acknowledged': BoolProp(default=False, fill_brok=['full_status'], retention=True),
        'acknowledgement':    StringProp(default=None, retention=True),
        'acknowledgement_type': IntegerProp(default=1, fill_brok=['full_status', 'check_result'], retention=True),
        'check_type':         IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'has_been_checked':   IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'should_be_scheduled': IntegerProp(default=1, fill_brok=['full_status'], retention=True),
        'last_problem_id':    IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'current_problem_id': IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'execution_time':     FloatProp(default=0.0, fill_brok=['full_status', 'check_result'], retention=True),
        'u_time':             FloatProp(default=0.0),
        's_time':             FloatProp(default=0.0),
        'last_notification':  FloatProp(default=0.0, fill_brok=['full_status'], retention=True),
        'current_notification_number': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'current_notification_id': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'check_flapping_recovery_notification': BoolProp(default=True, fill_brok=['full_status'], retention=True),
        'scheduled_downtime_depth': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'pending_flex_downtime': IntegerProp(default=0, fill_brok=['full_status'], retention=True),
        'timeout':            IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'start_time':         IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'end_time':           IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'early_timeout':      IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'return_code':        IntegerProp(default=0, fill_brok=['full_status', 'check_result'], retention=True),
        'perf_data':          StringProp(default='', fill_brok=['full_status', 'check_result'], retention=True),
        'last_perf_data':     StringProp(default='', retention=True),
        'host':               StringProp(default=None),
        'customs':            ListProp(default={}, fill_brok=['full_status']),
        # Warning: for the notified_contacts retention save, we save only the names of the contacts, and we should RELINK
        # them when we load it.
        'notified_contacts':  ListProp(default=set(), retention=True, retention_preparation=to_list_of_names), # use for having all contacts we have notified
        'in_scheduled_downtime': BoolProp(default=False, fill_brok=['full_status'], retention=True),
        'in_scheduled_downtime_during_last_check': BoolProp(default=False, retention=True),
        'actions':            ListProp(default=[]), # put here checks and notif raised
        'broks':              ListProp(default=[]), # and here broks raised


        # Problem/impact part
        'is_problem':         BoolProp(default=False, fill_brok=['full_status']),
        'is_impact':          BoolProp(default=False, fill_brok=['full_status']),
        # the save value of our business_impact for "problems"
        'my_own_business_impact':   IntegerProp(default=-1, fill_brok=['full_status']),
        # list of problems that make us an impact
        'source_problems':    ListProp(default=[], fill_brok=['full_status'], brok_transformation=to_svc_hst_distinct_lists),
        # list of the impact I'm the cause of
        'impacts':            ListProp(default=[], fill_brok=['full_status'], brok_transformation=to_svc_hst_distinct_lists),
        # keep a trace of the old state before being an impact
        'state_before_impact': StringProp(default='PENDING'),
        # keep a trace of the old state id before being an impact
        'state_id_before_impact': IntegerProp(default=0),
        # if the state change, we know so we do not revert it
        'state_changed_since_impact': BoolProp(default=False),

        # BUSINESS CORRELATOR PART
        # Say if we are business based rule or not
        'got_business_rule': BoolProp(default=False, fill_brok=['full_status']),
        # Previously processed business rule (with macro expanded)
        'processed_business_rule': StringProp(default="", fill_brok=['full_status']),
        # Our Dependency node for the business rule
        'business_rule': StringProp(default=None),


        # Here it's the elements we are depending on
        # so our parents as network relation, or a host
        # we are depending in a hostdependency
        # or even if we are business based.
        'parent_dependencies': StringProp(default=set(), brok_transformation=to_svc_hst_distinct_lists, fill_brok=['full_status']),
        # Here it's the guys that depend on us. So it's the total
        # opposite of the parent_dependencies
        'child_dependencies': StringProp(brok_transformation=to_svc_hst_distinct_lists, default=set(), fill_brok=['full_status']),

        # Manage the unknown/unreach during hard state
        'in_hard_unknown_reach_phase': BoolProp(default=False, retention=True),
        'was_in_hard_unknown_reach_phase': BoolProp(default=False, retention=True),
        'state_before_hard_unknown_reach_phase': StringProp(default='OK', retention=True),

        # Set if the element just change its father/son topology
        'topology_change': BoolProp(default=False, fill_brok=['full_status']),

        # Trigger list
        'triggers': StringProp(default=[])

    })

    # Mapping between Macros and properties (can be prop or a function)
    macros = {
        'SERVICEDESC':            'service_description',
        'SERVICEDISPLAYNAME':     'display_name',
        'SERVICESTATE':           'state',
        'SERVICESTATEID':         'state_id',
        'LASTSERVICESTATE':       'last_state',
        'LASTSERVICESTATEID':     'last_state_id',
        'SERVICESTATETYPE':       'state_type',
        'SERVICEATTEMPT':         'attempt',
        'MAXSERVICEATTEMPTS':     'max_check_attempts',
        'SERVICEISVOLATILE':      'is_volatile',
        'SERVICEEVENTID':         'current_event_id',
        'LASTSERVICEEVENTID':     'last_event_id',
        'SERVICEPROBLEMID':       'current_problem_id',
        'LASTSERVICEPROBLEMID':   'last_problem_id',
        'SERVICELATENCY':         'latency',
        'SERVICEEXECUTIONTIME':   'execution_time',
        'SERVICEDURATION':        'get_duration',
        'SERVICEDURATIONSEC':     'get_duration_sec',
        'SERVICEDOWNTIME':        'get_downtime',
        'SERVICEPERCENTCHANGE':   'percent_state_change',
        'SERVICEGROUPNAME':       'get_groupname',
        'SERVICEGROUPNAMES':      'get_groupnames',
        'LASTSERVICECHECK':       'last_chk',
        'LASTSERVICESTATECHANGE': 'last_state_change',
        'LASTSERVICEOK':          'last_time_ok',
        'LASTSERVICEWARNING':     'last_time_warning',
        'LASTSERVICEUNKNOWN':     'last_time_unknown',
        'LASTSERVICECRITICAL':    'last_time_critical',
        'SERVICEOUTPUT':          'output',
        'LONGSERVICEOUTPUT':      'long_output',
        'SERVICEPERFDATA':        'perf_data',
        'LASTSERVICEPERFDATA':    'last_perf_data',
        'SERVICECHECKCOMMAND':    'get_check_command',
        'SERVICEACKAUTHOR':       'get_ack_author_name',
        'SERVICEACKAUTHORNAME':   'get_ack_author_name',
        'SERVICEACKAUTHORALIAS':  'get_ack_author_name',
        'SERVICEACKCOMMENT':      'get_ack_comment',
        'SERVICEACTIONURL':       'action_url',
        'SERVICENOTESURL':        'notes_url',
        'SERVICENOTES':           'notes',
        'SERVICEBUSINESSIMPACT':  'business_impact'
    }

    # This tab is used to transform old parameters name into new ones
    # so from Nagios2 format, to Nagios3 ones.
    # Or Shinken deprecated names like criticity
    old_properties = {
        'normal_check_interval':    'check_interval',
        'retry_check_interval':    'retry_interval',
        'criticity':    'business_impact',
        'hostgroup':    'hostgroup_name',
        'hostgroups':    'hostgroup_name',
        ## 'criticitymodulations':    'business_impact_modulations',
    }

#######
#                   __ _                       _   _
#                  / _(_)                     | | (_)
#   ___ ___  _ __ | |_ _  __ _ _   _ _ __ __ _| |_ _  ___  _ __
#  / __/ _ \| '_ \|  _| |/ _` | | | | '__/ _` | __| |/ _ \| '_ \
# | (_| (_) | | | | | | | (_| | |_| | | | (_| | |_| | (_) | | | |
#  \___\___/|_| |_|_| |_|\__, |\__,_|_|  \__,_|\__|_|\___/|_| |_|
#                         __/ |
#                        |___/
######

    # Give a nice name output
    def get_name(self):
        if hasattr(self, 'service_description'):
            return self.service_description
        if hasattr(self, 'name'):
            return self.name
        return 'SERVICE-DESCRIPTION-MISSING'

    # Get the servicegroups names
    def get_groupnames(self):
        return ','.join([sg.get_name() for sg in self.servicegroups])

    # Need the whole name for debugging purpose
    def get_dbg_name(self):
        return "%s/%s" % (self.host.host_name, self.service_description)

    def get_full_name(self):
        if self.host and hasattr(self.host, 'host_name') and hasattr(self, 'service_description'):
            return "%s/%s" % (self.host.host_name, self.service_description)
        return 'UNKNOWN-SERVICE'

    # Get our realm, so in fact our host one
    def get_realm(self):
        if self.host is None:
            return None
        return self.host.get_realm()

    def get_hostgroups(self):
        return self.host.hostgroups

    def get_host_tags(self):
        return self.host.tags

    # Check is required prop are set:
    # template are always correct
    # contacts OR contactgroups is need
    def is_correct(self):
        state = True
        cls = self.__class__

        source = getattr(self, 'imported_from', 'unknown')

        desc = getattr(self, 'service_description', 'unnamed')
        hname = getattr(self, 'host_name', 'unnamed')

        special_properties = ('check_period', 'notification_interval', 'host_name',
                              'hostgroup_name', 'notification_period')

        for prop, entry in cls.properties.items():
            if prop not in special_properties:
                if not hasattr(self, prop) and entry.required:
                    logger.error("The service %s on host '%s' does not have %s" % (desc, hname, prop))
                    state = False  # Bad boy...

        # Then look if we have some errors in the conf
        # Juts print warnings, but raise errors
        for err in self.configuration_warnings:
            logger.warning("[service::%s] %s" % (desc, err))

        # Raised all previously saw errors like unknown contacts and co
        if self.configuration_errors != []:
            state = False
            for err in self.configuration_errors:
                logger.error("[service::%s] %s" % (self.get_full_name(), err))

        # If no notif period, set it to None, mean 24x7
        if not hasattr(self, 'notification_period'):
            self.notification_period = None

        # Ok now we manage special cases...
        if self.notifications_enabled and self.contacts == []:
            logger.warning("The service '%s' in the host '%s' does not have contacts nor contact_groups in '%s'" % (desc, hname, source))

        # Set display_name if need
        if getattr(self, 'display_name', '') == '':
            self.display_name = getattr(self, 'service_description', '')

        # If we got an event handler, it should be valid
        if getattr(self, 'event_handler', None) and not self.event_handler.is_valid():
            logger.info("%s: my event_handler %s is invalid" % (self.get_name(), self.event_handler.command))
            state = False

        if not hasattr(self, 'check_command'):
            logger.info("%s: I've got no check_command" % self.get_name())
            state = False
        # Ok got a command, but maybe it's invalid
        else:
            if not self.check_command.is_valid():
                logger.info("%s: my check_command %s is invalid" % (self.get_name(), self.check_command.command))
                state = False
            if self.got_business_rule:
                if not self.business_rule.is_valid():
                    logger.error("%s: my business rule is invalid" % (self.get_name(),))
                    for bperror in self.business_rule.configuration_errors:
                        logger.info("%s: %s" % (self.get_name(), bperror))
                    state = False
        if not hasattr(self, 'notification_interval') \
                and  self.notifications_enabled == True:
            logger.info("%s: I've got no notification_interval but I've got notifications enabled" % self.get_name())
            state = False
        if self.host is None:
            logger.warning("The service '%s' got an unknown host_name '%s'." % (desc, self.host_name))
            # do not set tis a a true error, only we will delete this after
        if not hasattr(self, 'check_period'):
            self.check_period = None
        if hasattr(self, 'service_description'):
            for c in cls.illegal_object_name_chars:
                if c in self.service_description:
                    logger.info("%s: My service_description got the character %s that is not allowed." % (self.get_name(), c))
                    state = False
        return state

    # The service is dependent of his father dep
    # Must be AFTER linkify
    def fill_daddy_dependency(self):
        #  Depend of host, all status, is a networkdep
        # and do not have timeperiod, and follow parents dep
        if self.host is not None:
            # I add the dep in MY list
            self.act_depend_of.append((self.host,
                                        ['d', 'u', 's', 'f'],
                                        'network_dep',
                                        None, True))
            # I add the dep in Daddy list
            self.host.act_depend_of_me.append((self,
                                                ['d', 'u', 's', 'f'],
                                                'network_dep',
                                                None, True))

            # And the parent/child dep lists too
            self.host.register_son_in_parent_child_dependencies(self)

    # Register the dependency between 2 service for action (notification etc)
    def add_service_act_dependency(self, srv, status, timeperiod, inherits_parent):
        # first I add the other the I depend on in MY list
        self.act_depend_of.append((srv, status, 'logic_dep',
                                    timeperiod, inherits_parent))
        # then I register myself in the other service dep list
        srv.act_depend_of_me.append((self, status, 'logic_dep',
                                      timeperiod, inherits_parent))

        # And the parent/child dep lists too
        srv.register_son_in_parent_child_dependencies(self)

    # Register the dependency between 2 service for action (notification etc)
    # but based on a BUSINESS rule, so on fact:
    # ERP depend on database, so we fill just database.act_depend_of_me
    # because we will want ERP mails to go on! So call this
    # on the database service with the srv=ERP service
    def add_business_rule_act_dependency(self, srv, status, timeperiod, inherits_parent):
        # I only register so he know that I WILL be a impact
        self.act_depend_of_me.append((srv, status, 'business_dep',
                                      timeperiod, inherits_parent))

        # And the parent/child dep lists too
        self.register_son_in_parent_child_dependencies(srv)

    # Register the dependency between 2 service for checks
    def add_service_chk_dependency(self, srv, status, timeperiod, inherits_parent):
        # first I add the other the I depend on in MY list
        self.chk_depend_of.append((srv, status, 'logic_dep',
                                    timeperiod, inherits_parent))
        # then I register myself in the other service dep list
        srv.chk_depend_of_me.append((self, status, 'logic_dep',
                                      timeperiod, inherits_parent))

        # And the parent/child dep lists too
        srv.register_son_in_parent_child_dependencies(self)

    # For a given host, look for all copy we must
    # create for for_each property
    def duplicate(self, host):
        duplicates = []

        # In macro, it's all in UPPER case
        prop = self.duplicate_foreach.strip().upper()

        # If I do not have the property, we bail out
        if prop in host.customs:
            # Get the list entry, and the not one if there is one
            entry = host.customs[prop]
            # Look at the list of the key we do NOT want maybe,
            # for _disks it will be _!disks
            not_entry = host.customs.get('_' + '!' + prop[1:], '').split(',')
            not_keys = strip_and_uniq(not_entry)

            default_value = getattr(self, 'default_value', '')
            # Transform the generator string to a list
            # Missing values are filled with the default value
            (key_values, errcode) = get_key_value_sequence(entry, default_value)

            if key_values:
                for key_value in key_values:
                    key = key_value['KEY']
                    # Maybe this key is in the NOT list, if so, skip it
                    if key in not_keys:
                        continue
                    value = key_value['VALUE']
                    new_s = self.copy()
                    new_s.host_name = host.get_name()
                    if self.is_tpl():  # if template, the new one is not
                        new_s.register = 1
                    for key in key_value:
                        if key == 'KEY':
                            if hasattr(self, 'service_description'):
                                # We want to change all illegal chars to a _ sign. We can't use class.illegal_obj_char
                                # because in the "explode" phase, we do not have access to this data! :(
                                safe_key_value = re.sub(r'[' + "`~!$%^&*\"|'<>?,()=" + ']+', '_', key_value[key])
                                new_s.service_description = self.service_description.replace('$' + key + '$', safe_key_value)
                        # Here is a list of property where we will expand the $KEY$ by the value
                        _the_expandables = ['check_command', 'aggregation', 'service_dependencies', 'event_handler']
                        for prop in _the_expandables:
                            if hasattr(self, prop):
                                # here we can replace VALUE, VALUE1, VALUE2,...
                                setattr(new_s, prop, getattr(new_s, prop).replace('$' + key + '$', key_value[key]))
                        if hasattr(self, 'aggregation'):
                            new_s.aggregation = new_s.aggregation.replace('$' + key + '$', key_value[key])
                    # And then add in our list this new service
                    duplicates.append(new_s)
            else:
                # If error, we should link the error to the host, because self is a template, and so won't be checked not print!
                if errcode == GET_KEY_VALUE_SEQUENCE_ERROR_SYNTAX:
                    err = "The custom property '%s' of the host '%s' is not a valid entry %s for a service generator" % (self.duplicate_foreach.strip(), host.get_name(), entry)
                    logger.warning(err)
                    host.configuration_errors.append(err)
                elif errcode == GET_KEY_VALUE_SEQUENCE_ERROR_NODEFAULT:
                    err = "The custom property '%s 'of the host '%s' has empty values %s but the service %s has no default_value" % (self.duplicate_foreach.strip(), host.get_name(), entry, self.service_description)
                    logger.warning(err)
                    host.configuration_errors.append(err)
                elif errcode == GET_KEY_VALUE_SEQUENCE_ERROR_NODE:
                    err = "The custom property '%s' of the host '%s' has an invalid node range %s" % (self.duplicate_foreach.strip(), host.get_name(), entry)
                    logger.warning(err)
                    host.configuration_errors.append(err)
        return duplicates

#####
#                         _
#                        (_)
#  _ __ _   _ _ __  _ __  _ _ __   __ _
# | '__| | | | '_ \| '_ \| | '_ \ / _` |
# | |  | |_| | | | | | | | | | | | (_| |
# |_|   \__,_|_| |_|_| |_|_|_| |_|\__, |
#                                  __/ |
#                                 |___/
####

    # Set unreachable: our host is DOWN, but it mean nothing for a service
    def set_unreachable(self):
        pass

    # We just go an impact, so we go unreachable
    # but only if it's enable in the configuration
    def set_impact_state(self):
        cls = self.__class__
        if cls.enable_problem_impacts_states_change:
            # Keep a trace of the old state (problem came back before
            # a new checks)
            self.state_before_impact = self.state
            self.state_id_before_impact = self.state_id
            # this flag will know if we override the impact state
            self.state_changed_since_impact = False
            self.state = 'UNKNOWN'  # exit code UNDETERMINED
            self.state_id = 3

    # Ok, we are no more an impact, if no news checks
    # override the impact state, we came back to old
    # states
    # And only if we enable the state change for impacts
    def unset_impact_state(self):
        cls = self.__class__
        if cls.enable_problem_impacts_states_change and not self.state_changed_since_impact:
            self.state = self.state_before_impact
            self.state_id = self.state_id_before_impact

    # Set state with status return by the check
    # and update flapping state
    def set_state_from_exit_status(self, status):
        now = time.time()
        self.last_state_update = now

        # we should put in last_state the good last state:
        # if not just change the state by an problem/impact
        # we can take current state. But if it's the case, the
        # real old state is self.state_before_impact (it's the TRUE
        # state in fact)
        # but only if the global conf have enable the impact state change
        cls = self.__class__
        if cls.enable_problem_impacts_states_change \
                and self.is_impact \
                and not self.state_changed_since_impact:
            self.last_state = self.state_before_impact
        else:  # standard case
            self.last_state = self.state

        if status == 0:
            self.state = 'OK'
            self.state_id = 0
            self.last_time_ok = int(self.last_state_update)
            state_code = 'o'
        elif status == 1:
            self.state = 'WARNING'
            self.state_id = 1
            self.last_time_warning = int(self.last_state_update)
            state_code = 'w'
        elif status == 2:
            self.state = 'CRITICAL'
            self.state_id = 2
            self.last_time_critical = int(self.last_state_update)
            state_code = 'c'
        elif status == 3:
            self.state = 'UNKNOWN'
            self.state_id = 3
            self.last_time_unknown = int(self.last_state_update)
            state_code = 'u'
        else:
            self.state = 'CRITICAL'  # exit code UNDETERMINED
            self.state_id = 2
            self.last_time_critical = int(self.last_state_update)
            state_code = 'c'

        if state_code in self.flap_detection_options:
            self.add_flapping_change(self.state != self.last_state)

        if self.state != self.last_state:
            self.last_state_change = self.last_state_update

        self.duration_sec = now - self.last_state_change

    # Return True if status is the state (like OK) or small form like 'o'
    def is_state(self, status):
        if status == self.state:
            return True
        # Now low status
        elif status == 'o' and self.state == 'OK':
            return True
        elif status == 'c' and self.state == 'CRITICAL':
            return True
        elif status == 'w' and self.state == 'WARNING':
            return True
        elif status == 'u' and self.state == 'UNKNOWN':
            return True
        return False

    # The last time when the state was not OK
    def last_time_non_ok_or_up(self):
        non_ok_times = filter(lambda x: x > self.last_time_ok, [self.last_time_warning,
                                                                self.last_time_critical,
                                                                self.last_time_unknown])
        if len(non_ok_times) == 0:
            last_time_non_ok = 0  # program_start would be better
        else:
            last_time_non_ok = min(non_ok_times)
        return last_time_non_ok

    # Add a log entry with a SERVICE ALERT like:
    # SERVICE ALERT: server;Load;UNKNOWN;HARD;1;I don't know what to say...
    def raise_alert_log_entry(self):
        console_logger.alert('SERVICE ALERT: %s;%s;%s;%s;%d;%s'
                            % (self.host.get_name(), self.get_name(),
                               self.state, self.state_type,
                               self.attempt, self.output))

    # If the configuration allow it, raise an initial log like
    # CURRENT SERVICE STATE: server;Load;UNKNOWN;HARD;1;I don't know what to say...
    def raise_initial_state(self):
        if self.__class__.log_initial_states:
            console_logger.info('CURRENT SERVICE STATE: %s;%s;%s;%s;%d;%s'
                                % (self.host.get_name(), self.get_name(),
                                   self.state, self.state_type,
                                   self.attempt, self.output))

    # Add a log entry with a Freshness alert like:
    # Warning: The results of host 'Server' are stale by 0d 0h 0m 58s (threshold=0d 1h 0m 0s).
    # I'm forcing an immediate check of the host.
    def raise_freshness_log_entry(self, t_stale_by, t_threshold):
        logger.warning("The results of service '%s' on host '%s' are stale "
                       "by %s (threshold=%s).  I'm forcing an immediate check "
                       "of the service."
                       % (self.get_name(), self.host.get_name(),
                          format_t_into_dhms_format(t_stale_by),
                          format_t_into_dhms_format(t_threshold)))

    # Raise a log entry with a Notification alert like
    # SERVICE NOTIFICATION: superadmin;server;Load;OK;notify-by-rss;no output
    def raise_notification_log_entry(self, n):
        contact = n.contact
        command = n.command_call
        if n.type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'DOWNTIMECANCELLED',
                      'CUSTOM', 'ACKNOWLEDGEMENT', 'FLAPPINGSTART',
                      'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            state = '%s (%s)' % (n.type, self.state)
        else:
            state = self.state
        if self.__class__.log_notifications:
            console_logger.alert("SERVICE NOTIFICATION: %s;%s;%s;%s;%s;%s"
                                % (contact.get_name(),
                                   self.host.get_name(), self.get_name(), state,
                                   command.get_name(), self.output))

    # Raise a log entry with a Eventhandler alert like
    # SERVICE EVENT HANDLER: test_host_0;test_ok_0;OK;SOFT;4;eventhandler
    def raise_event_handler_log_entry(self, command):
        if self.__class__.log_event_handlers:
            console_logger.alert("SERVICE EVENT HANDLER: %s;%s;%s;%s;%s;%s"
                                % (self.host.get_name(), self.get_name(),
                                   self.state, self.state_type,
                                   self.attempt, command.get_name()))

    # Raise a log entry with FLAPPING START alert like
    # SERVICE FLAPPING ALERT: server;LOAD;STARTED; Service appears to have started flapping (50.6% change >= 50.0% threshold)
    def raise_flapping_start_log_entry(self, change_ratio, threshold):
        console_logger.alert("SERVICE FLAPPING ALERT: %s;%s;STARTED; "
                            "Service appears to have started flapping "
                            "(%.1f%% change >= %.1f%% threshold)"
                            % (self.host.get_name(), self.get_name(),
                               change_ratio, threshold))

    # Raise a log entry with FLAPPING STOP alert like
    # SERVICE FLAPPING ALERT: server;LOAD;STOPPED; Service appears to have stopped flapping (23.0% change < 25.0% threshold)
    def raise_flapping_stop_log_entry(self, change_ratio, threshold):
        console_logger.alert("SERVICE FLAPPING ALERT: %s;%s;STOPPED; "
                            "Service appears to have stopped flapping "
                            "(%.1f%% change < %.1f%% threshold)"
                            % (self.host.get_name(), self.get_name(),
                               change_ratio, threshold))

    # If there is no valid time for next check, raise a log entry
    def raise_no_next_check_log_entry(self):
        logger.warning("I cannot schedule the check for the service '%s' on "
                       "host '%s' because there is not future valid time"
                       % (self.get_name(), self.host.get_name()))

    # Raise a log entry when a downtime begins
    # SERVICE DOWNTIME ALERT: test_host_0;test_ok_0;STARTED; Service has entered a period of scheduled downtime
    def raise_enter_downtime_log_entry(self):
        console_logger.alert("SERVICE DOWNTIME ALERT: %s;%s;STARTED; "
                            "Service has entered a period of scheduled "
                            "downtime"
                            % (self.host.get_name(), self.get_name()))

    # Raise a log entry when a downtime has finished
    # SERVICE DOWNTIME ALERT: test_host_0;test_ok_0;STOPPED; Service has exited from a period of scheduled downtime
    def raise_exit_downtime_log_entry(self):
        console_logger.alert("SERVICE DOWNTIME ALERT: %s;%s;STOPPED; Service "
                            "has exited from a period of scheduled downtime"
                            % (self.host.get_name(), self.get_name()))

    # Raise a log entry when a downtime prematurely ends
    # SERVICE DOWNTIME ALERT: test_host_0;test_ok_0;CANCELLED; Service has entered a period of scheduled downtime
    def raise_cancel_downtime_log_entry(self):
        console_logger.alert("SERVICE DOWNTIME ALERT: %s;%s;CANCELLED; "
                            "Scheduled downtime for service has been cancelled."
                            % (self.host.get_name(), self.get_name()))

    # Is stalking?
    # Launch if check is waitconsume==first time
    # and if c.status is in self.stalking_options
    def manage_stalking(self, c):
        need_stalk = False
        if c.status == 'waitconsume':
            if c.exit_status == 0 and 'o' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 1 and 'w' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 2 and 'c' in self.stalking_options:
                need_stalk = True
            elif c.exit_status == 3 and 'u' in self.stalking_options:
                need_stalk = True

            if c.output == self.output:
                need_stalk = False
        if need_stalk:
            logger.info("Stalking %s: %s" % (self.get_name(), c.output))

    # Give data for checks's macros
    def get_data_for_checks(self):
        return [self.host, self]

    # Give data for event handlers's macros
    def get_data_for_event_handler(self):
        return [self.host, self]

    # Give data for notifications'n macros
    def get_data_for_notifications(self, contact, n):
        return [self.host, self, contact, n]

    # See if the notification is launchable (time is OK and contact is OK too)
    def notification_is_blocked_by_contact(self, n, contact):
        return not contact.want_service_notification(self.last_chk, self.state, n.type, self.business_impact, n.command_call)

    def get_duration_sec(self):
        return str(int(self.duration_sec))

    def get_duration(self):
        m, s = divmod(self.duration_sec, 60)
        h, m = divmod(m, 60)
        return "%02dh %02dm %02ds" % (h, m, s)

    def get_ack_author_name(self):
        if self.acknowledgement is None:
            return ''
        return self.acknowledgement.author

    def get_ack_comment(self):
        if self.acknowledgement is None:
            return ''
        return self.acknowledgement.comment

    def get_check_command(self):
        return self.check_command.get_name()

    # Check if a notification for this service is suppressed at this time
    def notification_is_blocked_by_item(self, type, t_wished = None):
        if t_wished is None:
            t_wished = time.time()

        #  TODO
        # forced notification
        # pass if this is a custom notification

        # Block if notifications are program-wide disabled
        if not self.enable_notifications:
            return True

        # Does the notification period allow sending out this notification?
        if self.notification_period is not None and not self.notification_period.is_time_valid(t_wished):
            return True

        # Block if notifications are disabled for this service
        if not self.notifications_enabled:
            return True

        # Block if the current status is in the notification_options w,u,c,r,f,s
        if 'n' in self.notification_options:
            return True
        if type in ('PROBLEM', 'RECOVERY'):
            if self.state == 'UNKNOWN' and not 'u' in self.notification_options:
                return True
            if self.state == 'WARNING' and not 'w' in self.notification_options:
                return True
            if self.state == 'CRITICAL' and not 'c' in self.notification_options:
                return True
            if self.state == 'OK' and not 'r' in self.notification_options:
                return True
        if (type in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED')
                and not 'f' in self.notification_options):
            return True
        if (type in ('DOWNTIMESTART', 'DOWNTIMEEND', 'DOWNTIMECANCELLED')
                and not 's' in self.notification_options):
            return True

        # Acknowledgements make no sense when the status is ok/up
        if type == 'ACKNOWLEDGEMENT':
            if self.state == self.ok_up:
                return True

        # When in downtime, only allow end-of-downtime notifications
        if self.scheduled_downtime_depth > 1 and type not in ('DOWNTIMEEND', 'DOWNTIMECANCELLED'):
            return True

        # Block if host is in a scheduled downtime
        if self.host.scheduled_downtime_depth > 0:
            return True

        # Block if in a scheduled downtime and a problem arises, or flapping event
        if self.scheduled_downtime_depth > 0 and type in ('PROBLEM', 'RECOVERY', 'FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            return True

        # Block if the status is SOFT
        if self.state_type == 'SOFT' and type == 'PROBLEM':
            return True

        # Block if the problem has already been acknowledged
        if self.problem_has_been_acknowledged and type != 'ACKNOWLEDGEMENT':
            return True

        # Block if flapping
        if self.is_flapping and type not in ('FLAPPINGSTART', 'FLAPPINGSTOP', 'FLAPPINGDISABLED'):
            return True

        # Block if host is down
        if self.host.state != self.host.ok_up:
            return True

        # Block if business rule smart notifications is enabled and all its
        # childs have been acknowledged or are under downtime.
        if self.got_business_rule is True \
                and self.business_rule_smart_notifications is True \
                and self.business_rule_notification_is_blocked() is True \
                and type == 'PROBLEM':
            return True

        return False

    # Get a oc*p command if item has obsess_over_*
    # command. It must be enabled locally and globally
    def get_obsessive_compulsive_processor_command(self):
        cls = self.__class__
        if not cls.obsess_over or not self.obsess_over_service:
            return

        m = MacroResolver()
        data = self.get_data_for_event_handler()
        cmd = m.resolve_command(cls.ocsp_command, data)
        e = EventHandler(cmd, timeout=cls.ocsp_timeout)

        # ok we can put it in our temp action queue
        self.actions.append(e)


# Class for list of services. It's mainly, mainly for configuration part
class Services(Items):
    inner_class = Service  # use for know what is in items

    # Create the reversed list for speedup search by host_name/name
    # We also tag service already in list: they are twins. It's a a bad things.
    # Hostgroups service have an ID higher than host service. So it we tag
    # an id that already are in the list, this service is already
    # exist, and is a host,
    # or a previous hostgroup, but define before.
    def create_reversed_list(self):
        self.reversed_list = {}
        self.twins = []

        # Get a sorted list of all services, by definition_order
        all_services = [s for s in self]
        all_services.sort(key=lambda s:int(getattr(s, 'definition_order', '100')))

        # Now we sort them, we will have definition_order sorted like we want
        for s in all_services:
            if hasattr(s, 'service_description') and hasattr(s, 'host_name'):
                s_desc = getattr(s, 'service_description')
                s_host_name = getattr(s, 'host_name')
                key = (s_host_name, s_desc)
                if key not in self.reversed_list:
                    self.reversed_list[key] = s.id
                else:
                    self.twins.append(s.id)

        # For service, the reversed_list is not used for
        # search, so we del it
        del self.reversed_list

    # TODO: finish search to use reversed
    # Search a service id by it's name and host_name
    def find_srv_id_by_name_and_hostname(self, host_name, name):
        # key = (host_name, name)
        # if key in self.reversed_list:
        #     return self.reversed_list[key]

        # if not, maybe in the whole list?
        for s in self:
            # Runtime first, available only after linkify
            if hasattr(s, 'service_description') and hasattr(s, 'host'):
                if s.service_description == name and s.host == host_name:
                    return s.id
            # At config part, available before linkify
            if hasattr(s, 'service_description') and hasattr(s, 'host_name'):
                if s.service_description == name and s.host_name == host_name:
                    return s.id
        return None

    # Search a service by it's name and hot_name
    def find_srv_by_name_and_hostname(self, host_name, name):
        if hasattr(self, 'hosts'):
            h = self.hosts.find_by_name(host_name)
            if h is None:
                return None
            return h.find_service_by_name(name)

        id = self.find_srv_id_by_name_and_hostname(host_name, name)
        if id is not None:
            return self.items[id]
        else:
            return None

    # Removes service exceptions based on host configuration
    def remove_exclusions(self, hosts):
        # Looks for hosts having service_excludes attribute set
        have_excludes = [h for h in hosts if h.service_excludes]
        to_remove = []

        for host in have_excludes:
            for descr in host.service_excludes:
                # Deletes excluded service instances
                sid = None
                for service in self:
                    if service.service_description == descr and \
                       service.host_name == host.host_name:
                        sid = service.id
                        break

                if sid is not None:
                    to_remove.append(service.id)
                else:
                    err = "Error: exclusion contains an undefined service: %s" % descr
                    host.configuration_errors.append(err)

        for sid in to_remove:
            del self[sid]

        return len(to_remove)

    # Make link between elements:
    # service -> host
    # service -> command
    # service -> timeperiods
    # service -> contacts
    def linkify(self, hosts, commands, timeperiods, contacts,
                resultmodulations, businessimpactmodulations, escalations,
                servicegroups, triggers, checkmodulations, macromodulations):
        self.linkify_with_timeperiods(timeperiods, 'notification_period')
        self.linkify_with_timeperiods(timeperiods, 'check_period')
        self.linkify_with_timeperiods(timeperiods, 'maintenance_period')
        self.linkify_s_by_hst(hosts)
        self.linkify_s_by_sg(servicegroups)
        self.linkify_one_command_with_commands(commands, 'check_command')
        self.linkify_one_command_with_commands(commands, 'event_handler')
        self.linkify_with_contacts(contacts)
        self.linkify_with_resultmodulations(resultmodulations)
        self.linkify_with_business_impact_modulations(businessimpactmodulations)
        # WARNING: all escalations will not be link here
        # (just the escalation here, not serviceesca or hostesca).
        # This last one will be link in escalations linkify.
        self.linkify_with_escalations(escalations)
        self.linkify_with_triggers(triggers)
        self.linkify_with_checkmodulations(checkmodulations)
        self.linkify_with_macromodulations(macromodulations)

    def override_properties(self, hosts):
        for host in hosts:
            # We're only looking for hosts having service overrides defined
            if not hasattr(host, 'service_overrides') or not host.service_overrides:
                continue
            cache = {}
            if isinstance(host.service_overrides, list):
                service_overrides = host.service_overrides
            else:
                service_overrides = [host.service_overrides]
            for ovr in service_overrides:
                # Checks service override syntax
                match = re.match(r'^([^,]+),([^\s]+)\s+(.*)$', ovr)
                if match is None:
                    err = "Error: invalid service override syntax: %s" % ovr
                    host.configuration_errors.append(err)
                    continue
                name, prop, value = match.groups()
                # To speep up search if several properties are overriden on the
                # same service, we keep them in temporary cache
                key = "%s/%s" % (host.host_name, name)
                if key in cache:
                    service = cache[key]
                else:
                    # Looks for corresponding service
                    # As hosts and service are not yet linked, we have to walk
                    # through services to find which is associated to host.
                    service = None
                    for s in self:
                        if not hasattr(s, "host_name") or not hasattr(s, "service_description"):
                            # this is a template
                            continue
                        if s.host_name == host.host_name and s.service_description == name:
                            service = s
                            break
                    if service is None:
                        err = "Error: trying to override property '%s' on service '%s' but it's unknown for this host" % (prop, name)
                        host.configuration_errors.append(err)
                        continue
                    cache[key] = service
                # Checks if override is allowed
                excludes = ['host_name', 'service_description', 'use',
                            'servicegroups', 'trigger', 'trigger_name']
                if prop in excludes:
                    err = "Error: trying to override '%s', a forbidden property for service '%s'" % (prop, name)
                    host.configuration_errors.append(err)
                    continue
                setattr(service, prop, value)

    # We can link services with hosts so
    # We can search in O(hosts) instead
    # of O(services) for common cases
    def optimize_service_search(self, hosts):
        self.hosts = hosts

    # We just search for each host the id of the host
    # and replace the name by the id
    # + inform the host we are a service of him
    def linkify_s_by_hst(self, hosts):
        for s in self:
            # If we do not have an host_name, we set it as
            # a template element to delete. (like Nagios)
            if not hasattr(s, 'host_name'):
                s.host = None
                continue
            try:
                hst_name = s.host_name
                # The new member list, in id
                hst = hosts.find_by_name(hst_name)
                s.host = hst
                # Let the host know we are his service
                if s.host is not None:
                    hst.add_service_link(s)
                else:  # Ok, the host do not exists!
                    err = "Warning: the service '%s' got an invalid host_name '%s'" % (self.get_name(), hst_name)
                    s.configuration_warnings.append(err)
                    continue
            except AttributeError, exp:
                pass  # Will be catch at the is_correct moment

    # We look for servicegroups property in services and
    # link them
    def linkify_s_by_sg(self, servicegroups):
        for s in self:
            if not s.is_tpl():
                new_servicegroups = []
                if hasattr(s, 'servicegroups') and s.servicegroups != '':
                    sgs = s.servicegroups.split(',')
                    for sg_name in sgs:
                        sg_name = sg_name.strip()
                        sg = servicegroups.find_by_name(sg_name)
                        if sg is not None:
                            new_servicegroups.append(sg)
                        else:
                            err = "Error: the servicegroup '%s' of the service '%s' is unknown" % (sg_name, s.get_dbg_name())
                            s.configuration_errors.append(err)
                s.servicegroups = new_servicegroups

    # In the scheduler we need to relink the commandCall with
    # the real commands
    def late_linkify_s_by_commands(self, commands):
        props = ['check_command', 'event_handler']
        for s in self:
            for prop in props:
                cc = getattr(s, prop, None)
                if cc:
                    cc.late_linkify_with_command(commands)

    # Delete services by ids
    def delete_services_by_id(self, ids):
        for id in ids:
            del self[id]

    # Apply implicit inheritance for special properties:
    # contact_groups, notification_interval , notification_period
    # So service will take info from host if necessary
    def apply_implicit_inheritance(self, hosts):
        for prop in ('contacts', 'contact_groups', 'notification_interval',
                      'notification_period', 'resultmodulations', 'business_impact_modulations', 'escalations',
                      'poller_tag', 'reactionner_tag', 'check_period', 'business_impact', 'maintenance_period'):
            for s in self:
                if not s.is_tpl():
                    if not hasattr(s, prop) and hasattr(s, 'host_name'):
                        h = hosts.find_by_name(s.host_name)
                        if h is not None and hasattr(h, prop):
                            setattr(s, prop, getattr(h, prop))

    # Apply inheritance for all properties
    def apply_inheritance(self, hosts):
        # We check for all Host properties if the host has it
        # if not, it check all host templates for a value
        for prop in Service.properties:
            self.apply_partial_inheritance(prop)

        # Then implicit inheritance
        # self.apply_implicit_inheritance(hosts)
        for s in self:
            s.get_customs_properties_by_inheritance(self)

    # Create dependencies for services (daddy ones)
    def apply_dependencies(self):
        for s in self:
            s.fill_daddy_dependency()


    # For services the main clean is about service with bad hosts
    def clean(self):
        to_del = []
        for s in self:
            if not s.host:
                to_del.append(s.id)
        for sid in to_del:
            del self.items[sid]

    # Add in our queue a service create from another. Special case:
    # is a template: so hname is a name of template, so need to get all
    # hosts that inherit from it.
    def copy_create_service_from_another(self, hosts, s, hname):
        for_hosts_to_create = []
        # if we are not a template, it's easy: copy for all host_name
        # because they are our final host_name after all
        if not s.is_tpl():
            for_hosts_to_create.append(hname)
        else:
            # But for template it's more tricky: it's a template name
            # we've got, not a real host_name/ So we must get a list of host_name
            # that use this template
            # Use the complex expression manager for it, it will call find_hosts_that_use_template
            # for the templates it think it's useful
            hosts_from_tpl = self.evaluate_hostgroup_expression(hname.strip(), hosts, hosts.templates, look_in='templates')

            # And now copy our real services
            for n in hosts_from_tpl:
                for_hosts_to_create.append(n)

        if getattr(s, 'duplicate_foreach', '') == '':
            def _loop(name):
                new_s = s.copy()
                new_s.host_name = name
                if s.is_tpl():  # if template, the new one is not
                    new_s.register = 1
                self.items[new_s.id] = new_s
        else:
            def _loop(name):
                # the generator case, we must create several new services
                # we must find our host, and get all key:value we need
                h = hosts.find_by_name(name.strip())

                if h is not None:
                    for new_s in s.duplicate(h):
                        self.items[new_s.id] = new_s
                else:  # TODO: raise an error?
                    err = 'Error: The hostname %s is unknown for the service %s!' % (name, s.get_name())
                    s.configuration_errors.append(err)

        # Now really create the services
        for name in for_hosts_to_create:
            _loop(name)

    # We create new service if necessary (host groups and co)
    def explode(self, hosts, hostgroups, contactgroups,
                servicegroups, servicedependencies, triggers):
        # The "old" services will be removed. All services with
        # more than one host or a host group will be in it
        srv_to_remove = []

        # items::explode_trigger_string_into_triggers
        self.explode_trigger_string_into_triggers(triggers)

        # items::explode_host_groups_into_hosts
        # take all hosts from our hostgroup_name into our host_name property
        self.explode_host_groups_into_hosts(hosts, hostgroups)

        # items::explode_contact_groups_into_contacts
        # take all contacts from our contact_groups into our contact property
        self.explode_contact_groups_into_contacts(contactgroups)

        # Then for every host create a copy of the service with just the host
        # because we are adding services, we can't just loop in it
        service_to_check = self.items.keys()

        for id in service_to_check:
            s = self.items[id]
            duplicate_for_hosts = []  # get the list of our host_names if more than 1
            not_hosts = []  # the list of !host_name so we remove them after

            # If do not have an host_name, just delete it
            if not hasattr(s, 'host_name'):
                srv_to_remove.append(s.id)

            # if not s.is_tpl(): # Exploding template is useless
            # Explode for real service or template with a host_name
            if hasattr(s, 'host_name'):
                hnames = s.host_name.split(',')
                hnames = strip_and_uniq(hnames)
                # We will duplicate if we have multiple host_name
                # or if we are a template (so a clean service)
                if len(hnames) >= 2 or s.is_tpl() \
                        or (hasattr(s, 'duplicate_foreach') and s.duplicate_foreach != ''):
                    for hname in hnames:
                        hname = hname.strip()

                        # If the name begin with a !, we put it in
                        # the not list
                        if hname.startswith('!'):
                            not_hosts.append(hname[1:])
                        else:  # the standard list
                            duplicate_for_hosts.append(hname)

                    # remove duplicate items from duplicate_for_hosts:
                    duplicate_for_hosts = list(set(duplicate_for_hosts))

                    # Ok now we clean the duplicate_for_hosts with all hosts
                    # of the not
                    for hname in not_hosts:
                        if hname in duplicate_for_hosts:
                            duplicate_for_hosts.remove(hname)

                    # Now we duplicate the service for all host_names
                    for hname in duplicate_for_hosts:
                        self.copy_create_service_from_another(hosts, s, hname)

                    # Multiple host_name -> the original service
                    # must be delete. But template are clean else where
                    # and only the the service not got an error in it's conf
                    if not s.is_tpl() and s.configuration_errors == []:
                        srv_to_remove.append(id)

                else:  # Maybe the hnames was full of same host, so we must reset the name
                    for hname in hnames:  # So even if len == 0, we are protected
                        s.host_name = hname

        # We clean all service that was for multiple hosts.
        self.delete_services_by_id(srv_to_remove)

        # Servicegroups property need to be fullfill for got the informations
        # And then just register to this service_group
        for s in self:
            if not s.is_tpl() and hasattr(s, 'service_description'):
                sname = s.service_description
                shname = getattr(s, 'host_name', '')
                if hasattr(s, 'servicegroups'):
                    sgs = s.servicegroups.split(',')
                    for sg in sgs:
                        servicegroups.add_member(shname+','+sname, sg)


        # Now we explode service_dependencies into Servicedependency
        # We just create serviceDep with goods values (as STRING!),
        # the link pass will be done after
        for s in self:
            # Templates are useless here
            if not s.is_tpl():
                if hasattr(s, 'service_dependencies'):
                    if s.service_dependencies != '':
                        sdeps = s.service_dependencies.split(',')
                        # %2=0 are for hosts, !=0 are for service_description
                        i = 0
                        hname = ''
                        for elt in sdeps:
                            if i % 2 == 0:  # host
                                hname = elt.strip()
                            else:  # description
                                desc = elt.strip()
                                # we can register it (s) (depend on) -> (hname, desc)
                                # If we do not have enough data for s, it's no use
                                if hasattr(s, 'service_description') and hasattr(s, 'host_name'):
                                    if hname == '':
                                        hname = s.host_name
                                    servicedependencies.add_service_dependency(s.host_name, s.service_description, hname, desc)
                            i += 1


    # Will create all business tree for the
    # services
    def create_business_rules(self, hosts, services):
        for s in self:
            s.create_business_rules(hosts, services)


    # Will link all business service/host with theirs
    # dep for problem/impact link
    def create_business_rules_dependencies(self):
        for s in self:
            s.create_business_rules_dependencies()

########NEW FILE########
__FILENAME__ = servicedependency
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items
from shinken.property import BoolProp, StringProp, ListProp
from shinken.log import logger


class Servicedependency(Item):
    id = 0
    my_type = "servicedependency"

    # F is dep of D
    # host_name                      Host B
    #       service_description             Service D
    #       dependent_host_name             Host C
    #       dependent_service_description   Service F
    #       execution_failure_criteria      o
    #       notification_failure_criteria   w,u
    #       inherits_parent         1
    #       dependency_period       24x7

    properties = Item.properties.copy()
    properties.update({
        'dependent_host_name':           StringProp(),
        'dependent_hostgroup_name':      StringProp(default=''),
        'dependent_service_description': StringProp(),
        'host_name':                     StringProp(),
        'hostgroup_name':                StringProp(default=''),
        'service_description':           StringProp(),
        'inherits_parent':               BoolProp(default='0'),
        'execution_failure_criteria':    ListProp(default='n'),
        'notification_failure_criteria': ListProp(default='n'),
        'dependency_period':             StringProp(default=''),
        'explode_hostgroup':             BoolProp(default='0')
    })

    # Give a nice name output, for debugging purpose
    # (Yes, debugging CAN happen...)
    def get_name(self):
        return getattr(self, 'dependent_host_name', '') + '/' + getattr(self, 'dependent_service_description', '') + '..' + getattr(self, 'host_name', '') + '/' + getattr(self, 'service_description', '')


class Servicedependencies(Items):
    def delete_servicesdep_by_id(self, ids):
        for id in ids:
            del self[id]

    # Add a simple service dep from another (dep -> par)
    def add_service_dependency(self, dep_host_name, dep_service_description, par_host_name, par_service_description):
        # We create a "standard" service_dep
        prop = {
            'dependent_host_name':           dep_host_name,
            'dependent_service_description': dep_service_description,
            'host_name':                     par_host_name,
            'service_description':           par_service_description,
            'notification_failure_criteria': 'u,c,w',
            'inherits_parent': '1',
        }
        sd = Servicedependency(prop)
        self.items[sd.id] = sd

    # If we have explode_hostgroup parameter we have to create a service dependency for each host of the hostgroup
    def explode_hostgroup(self, sd, hostgroups):
        # We will create a service dependency for each host part of the host group

        # First get services
        snames = sd.service_description.split(',')

        # And dep services
        dep_snames = sd.dependent_service_description.split(',')

        # Now for each host into hostgroup we will create a service dependency object
        hg_names = sd.hostgroup_name.split(',')
        for hg_name in hg_names:
            hg = hostgroups.find_by_name(hg_name)
            if hg is None:
                err = "ERROR: the servicedependecy got an unknown hostgroup_name '%s'" % hg_name
                self.configuration_errors.append(err)
                continue
            hnames = []
            hnames.extend(hg.members.split(','))
            for hname in hnames:
                for dep_sname in dep_snames:
                    for sname in snames:
                        new_sd = sd.copy()
                        new_sd.host_name = hname
                        new_sd.service_description = sname
                        new_sd.dependent_host_name = hname
                        new_sd.dependent_service_description = dep_sname
                        self.items[new_sd.id] = new_sd

    # We create new servicedep if necessary (host groups and co)
    def explode(self, hostgroups):
        # The "old" services will be removed. All services with
        # more than one host or a host group will be in it
        srvdep_to_remove = []

        # Then for every host create a copy of the service with just the host
        # because we are adding services, we can't just loop in it
        servicedeps = self.items.keys()
        for id in servicedeps:
            sd = self.items[id]
            if sd.is_tpl():  # Exploding template is useless
                continue

            # Have we to explode the hostgroup into many service?
            if hasattr(sd, 'explode_hostgroup') and hasattr(sd, 'hostgroup_name'):
                self.explode_hostgroup(sd, hostgroups)
                srvdep_to_remove.append(id)
                continue

            # Get the list of all FATHER hosts and service deps
            hnames = []
            if hasattr(sd, 'hostgroup_name'):
                hg_names = sd.hostgroup_name.split(',')
                hg_names = [hg_name.strip() for hg_name in hg_names]
                for hg_name in hg_names:
                    hg = hostgroups.find_by_name(hg_name)
                    if hg is None:
                        err = "ERROR: the servicedependecy got an unknown hostgroup_name '%s'" % hg_name
                        hg.configuration_errors.append(err)
                        continue
                    hnames.extend(hg.members.split(','))

            if not hasattr(sd, 'host_name'):
                sd.host_name = ''

            if sd.host_name != '':
                hnames.extend(sd.host_name.split(','))
            snames = sd.service_description.split(',')
            couples = []
            for hname in hnames:
                for sname in snames:
                    couples.append((hname.strip(), sname.strip()))

            if not hasattr(sd, 'dependent_hostgroup_name') and hasattr(sd, 'hostgroup_name'):
                sd.dependent_hostgroup_name = sd.hostgroup_name

            # Now the dep part (the sons)
            dep_hnames = []
            if hasattr(sd, 'dependent_hostgroup_name'):
                hg_names = sd.dependent_hostgroup_name.split(',')
                hg_names = [hg_name.strip() for hg_name in hg_names]
                for hg_name in hg_names:
                    hg = hostgroups.find_by_name(hg_name)
                    if hg is None:
                        err = "ERROR: the servicedependecy got an unknown dependent_hostgroup_name '%s'" % hg_name
                        hg.configuration_errors.append(err)
                        continue
                    dep_hnames.extend(hg.members.split(','))

            if not hasattr(sd, 'dependent_host_name'):
                sd.dependent_host_name = getattr(sd, 'host_name', '')

            if sd.dependent_host_name != '':
                dep_hnames.extend(sd.dependent_host_name.split(','))
            dep_snames = sd.dependent_service_description.split(',')
            dep_couples = []
            for dep_hname in dep_hnames:
                for dep_sname in dep_snames:
                    dep_couples.append((dep_hname.strip(), dep_sname.strip()))

            # Create the new service deps from all of this.
            for (dep_hname, dep_sname) in dep_couples:  # the sons, like HTTP
                for (hname, sname) in couples:  # the fathers, like MySQL
                    new_sd = sd.copy()
                    new_sd.host_name = hname
                    new_sd.service_description = sname
                    new_sd.dependent_host_name = dep_hname
                    new_sd.dependent_service_description = dep_sname
                    self.items[new_sd.id] = new_sd
                # Ok so we can remove the old one
                srvdep_to_remove.append(id)

        self.delete_servicesdep_by_id(srvdep_to_remove)

    def linkify(self, hosts, services, timeperiods):
        self.linkify_sd_by_s(hosts, services)
        self.linkify_sd_by_tp(timeperiods)
        self.linkify_s_by_sd()

    # We just search for each srvdep the id of the srv
    # and replace the name by the id
    def linkify_sd_by_s(self, hosts, services):
        for sd in self:
            try:
                s_name = sd.dependent_service_description
                hst_name = sd.dependent_host_name

                # The new member list, in id
                s = services.find_srv_by_name_and_hostname(hst_name, s_name)
                if s is None:
                    self.configuration_errors.append("Service %s not found for host %s"
                                                     % (s_name, hst_name))
                sd.dependent_service_description = s

                s_name = sd.service_description
                hst_name = sd.host_name

                # The new member list, in id
                s = services.find_srv_by_name_and_hostname(hst_name, s_name)
                if s is None:
                    self.configuration_errors.append("Service %s not found for host %s"
                                                     % (s_name, hst_name))
                sd.service_description = s

            except AttributeError, exp:
                logger.error("[servicedependency] fail to linkify by service %s: %s" % (sd, exp))

    # We just search for each srvdep the id of the srv
    # and replace the name by the id
    def linkify_sd_by_tp(self, timeperiods):
        for sd in self:
            try:
                tp_name = sd.dependency_period
                tp = timeperiods.find_by_name(tp_name)
                sd.dependency_period = tp
            except AttributeError, exp:
                logger.error("[servicedependency] fail to linkify by timeperiods: %s" % exp)

    # We backport service dep to service. So SD is not need anymore
    def linkify_s_by_sd(self):
        for sd in self:
            if sd.is_tpl():
                continue
            dsc = sd.dependent_service_description
            sdval = sd.service_description
            if dsc is not None and sdval is not None:
                dp = getattr(sd, 'dependency_period', None)
                dsc.add_service_act_dependency(sdval, sd.notification_failure_criteria, dp, sd.inherits_parent)
                dsc.add_service_chk_dependency(sdval, sd.execution_failure_criteria, dp, sd.inherits_parent)

    # Apply inheritance for all properties
    def apply_inheritance(self, hosts):
        # We check for all Host properties if the host has it
        # if not, it check all host templates for a value
        for prop in Servicedependency.properties:
            self.apply_partial_inheritance(prop)

        # Then implicit inheritance
        # self.apply_implicit_inheritance(hosts)
        for s in self:
            s.get_customs_properties_by_inheritance(self)

########NEW FILE########
__FILENAME__ = serviceescalation
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from item import Item, Items
from escalation import Escalation

from shinken.property import IntegerProp, StringProp, ListProp


class Serviceescalation(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'serviceescalation'

    properties = Item.properties.copy()
    properties.update({
        'host_name':             StringProp(),
        'hostgroup_name':        StringProp(),
        'service_description':   StringProp(),
        'first_notification':    IntegerProp(),
        'last_notification':     IntegerProp(),
        'notification_interval': IntegerProp(default='30'), # like Nagios value
        'escalation_period':     StringProp(default=''),
        'escalation_options':    ListProp(default='d,u,r,w,c'),
        'contacts':              StringProp(),
        'contact_groups':        StringProp(),
    })

    # For debugging purpose only (nice name)
    def get_name(self):
        return ''


class Serviceescalations(Items):
    name_property = ""
    inner_class = Serviceescalation

    # We look for contacts property in contacts and
    def explode(self, escalations):
        # Now we explode all escalations (host_name, service_description) to escalations
        for es in self:
            properties = es.__class__.properties

            creation_dict = {'escalation_name': 'Generated-Serviceescalation-%d' % es.id}
            for prop in properties:
                if hasattr(es, prop):
                    creation_dict[prop] = getattr(es, prop)
            #print "Creation an escalation with:", creation_dict
            s = Escalation(creation_dict)
            escalations.add_escalation(s)

########NEW FILE########
__FILENAME__ = serviceextinfo
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""" This is the main class for the Service ext info. In fact it's mainly
about the configuration part. Parameters are merged in Service so it's
no use in running part
"""

import time

from item import Item, Items

from shinken.autoslots import AutoSlots
from shinken.util import format_t_into_dhms_format, to_hostnames_list, get_obj_name, to_svc_hst_distinct_lists, to_list_string_of_names
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.macroresolver import MacroResolver
from shinken.eventhandler import EventHandler
from shinken.log import logger


class ServiceExtInfo(Item):
    # AutoSlots create the __slots__ with properties and
    # running_properties names
    __metaclass__ = AutoSlots

    id = 1  # zero is reserved for host (primary node for parents)
    my_type = 'serviceextinfo'

    # properties defined by configuration
    # *required: is required in conf
    # *default: default value if no set in conf
    # *pythonize: function to call when transforming string to python object
    # *fill_brok: if set, send to broker. there are two categories: full_status for initial and update status, check_result for check results
    # *no_slots: do not take this property for __slots__
    #  Only for the initial call
    # conf_send_preparation: if set, will pass the property to this function. It's used to "flatten"
    #  some dangerous properties like realms that are too 'linked' to be send like that.
    # brok_transformation: if set, will call the function with the value of the property
    #  the major times it will be to flatten the data (like realm_name instead of the realm object).
    properties = Item.properties.copy()
    properties.update({
        'host_name':            ListProp(),
        'service_description':  StringProp(),
        'notes':                StringProp(default=''),
        'notes_url':            StringProp(default=''),
        'icon_image':           StringProp(default=''),
        'icon_image_alt':       StringProp(default=''),
    })

    # Hosts macros and prop that give the information
    # the prop can be callable or not
    macros = {
        'SERVICEDESC':            'service_description',
        'SERVICEACTIONURL':       'action_url',
        'SERVICENOTESURL':        'notes_url',
        'SERVICENOTES':           'notes'
    }

#######
#                   __ _                       _   _
#                  / _(_)                     | | (_)
#   ___ ___  _ __ | |_ _  __ _ _   _ _ __ __ _| |_ _  ___  _ __
#  / __/ _ \| '_ \|  _| |/ _` | | | | '__/ _` | __| |/ _ \| '_ \
# | (_| (_) | | | | | | | (_| | |_| | | | (_| | |_| | (_) | | | |
#  \___\___/|_| |_|_| |_|\__, |\__,_|_|  \__,_|\__|_|\___/|_| |_|
#                         __/ |
#                        |___/
######


    # Check is required prop are set:
    # host_name is needed
    def is_correct(self):
        state = True
        cls = self.__class__

        return state

    # For get a nice name
    def get_name(self):
        if not self.is_tpl():
            try:
                return self.host_name
            except AttributeError:  # outch, no hostname
                return 'UNNAMEDHOST'
        else:
            try:
                return self.name
            except AttributeError:  # outch, no name for this template
                return 'UNNAMEDHOSTTEMPLATE'

    # For debugging purpose only
    def get_dbg_name(self):
        return self.host_name

    # Same but for clean call, no debug
    def get_full_name(self):
        return self.host_name


# Class for the hosts lists. It's mainly for configuration
# part
class ServicesExtInfo(Items):
    name_property = "host_name"  # use for the search by name
    inner_class = ServiceExtInfo  # use for know what is in items

    # Merge extended host information into host
    def merge(self, services):
        for ei in self:
            if hasattr(ei, 'register') and getattr(ei, 'register') == '0':
                # We don't have to merge template
                continue
            hosts_names = ei.get_name().split(",")
            for host_name in hosts_names:
                s = services.find_srv_by_name_and_hostname(host_name, ei.service_description)
                if s is not None:
                    # FUUUUUUUUUUsion
                    self.merge_extinfo(s, ei)

    def merge_extinfo(self, service, extinfo):
        properties = ['notes', 'notes_url', 'icon_image', 'icon_image_alt']
        # service properties have precedence over serviceextinfo properties
        for p in properties:
            if getattr(service, p) == '' and getattr(extinfo, p) != '':
                setattr(service, p, getattr(extinfo, p))

########NEW FILE########
__FILENAME__ = servicegroup
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from itemgroup import Itemgroup, Itemgroups

from shinken.property import StringProp
from shinken.log import logger


class Servicegroup(Itemgroup):
    id = 1  # zero is always a little bit special... like in database
    my_type = 'servicegroup'

    properties = Itemgroup.properties.copy()
    properties.update({
        'id':                StringProp(default=0, fill_brok=['full_status']),
        'servicegroup_name': StringProp(fill_brok=['full_status']),
        'alias':             StringProp(fill_brok=['full_status']),
        'notes':             StringProp(default='', fill_brok=['full_status']),
        'notes_url':         StringProp(default='', fill_brok=['full_status']),
        'action_url':        StringProp(default='', fill_brok=['full_status']),
    })

    macros = {
        'SERVICEGROUPALIAS':     'alias',
        'SERVICEGROUPMEMBERS':   'members',
        'SERVICEGROUPNOTES':     'notes',
        'SERVICEGROUPNOTESURL':  'notes_url',
        'SERVICEGROUPACTIONURL': 'action_url'
    }

    def get_services(self):
        if self.has('members'):
            return self.members
        else:
            return ''

    def get_name(self):
        return self.servicegroup_name

    def get_servicegroup_members(self):
        if self.has('servicegroup_members'):
            return self.servicegroup_members.split(',')
        else:
            return []

    # We fillfull properties with template ones if need
    # Because hostgroup we call may not have it's members
    # we call get_hosts_by_explosion on it
    def get_services_by_explosion(self, servicegroups):
        # First we tag the hg so it will not be explode
        # if a son of it already call it
        self.already_explode = True

        # Now the recursive part
        # rec_tag is set to False every HG we explode
        # so if True here, it must be a loop in HG
        # calls... not GOOD!
        if self.rec_tag:
            logger.error("[servicegroup::%s] got a loop in servicegroup definition" % self.get_name())
            if self.has('members'):
                return self.members
            else:
                return ''
        # Ok, not a loop, we tag it and continue
        self.rec_tag = True

        sg_mbrs = self.get_servicegroup_members()
        for sg_mbr in sg_mbrs:
            sg = servicegroups.find_by_name(sg_mbr.strip())
            if sg is not None:
                value = sg.get_services_by_explosion(servicegroups)
                if value is not None:
                    self.add_string_member(value)

        if self.has('members'):
            return self.members
        else:
            return ''


class Servicegroups(Itemgroups):
    name_property = "servicegroup_name"  # is used for finding servicegroup
    inner_class = Servicegroup

    def linkify(self, services):
        self.linkify_sg_by_srv(services)

    # We just search for each host the id of the host
    # and replace the name by the id
    # TODO: very slow for hight services, so search with host list,
    # not service one
    def linkify_sg_by_srv(self, services):
        for sg in self:
            mbrs = sg.get_services()

            # The new member list, in id
            new_mbrs = []
            seek = 0
            host_name = ''
            if (len(mbrs) == 1):
                sg.unknown_members.append('%s' % mbrs[0])

            for mbr in mbrs:
                if seek % 2 == 0:
                    host_name = mbr.strip()
                else:
                    service_desc = mbr.strip()
                    find = services.find_srv_by_name_and_hostname(host_name, service_desc)
                    if find is not None:
                        new_mbrs.append(find)
                    else:
                        sg.unknown_members.append('%s,%s' % (host_name, service_desc))
                seek += 1

            # Make members uniq
            new_mbrs = list(set(new_mbrs))

            # We find the id, we replace the names
            sg.replace_members(new_mbrs)
            for s in sg.members:
                s.servicegroups.append(sg)
                # and make this uniq
                s.servicegroups = list(set(s.servicegroups))

    # Add a service string to a service member
    # if the service group do not exist, create it
    def add_member(self, cname, sgname):
        sg = self.find_by_name(sgname)
        # if the id do not exist, create the cg
        if sg is None:
            sg = Servicegroup({'servicegroup_name': sgname, 'alias': sgname, 'members': cname})
            self.add(sg)
        else:
            sg.add_string_member(cname)

    # Use to fill members with contactgroup_members
    def explode(self):
        # We do not want a same hg to be explode again and again
        # so we tag it
        for sg in self:
            sg.already_explode = False

        for sg in self:
            if sg.has('servicegroup_members') and not sg.already_explode:
                # get_services_by_explosion is a recursive
                # function, so we must tag hg so we do not loop
                for sg2 in self:
                    sg2.rec_tag = False
                sg.get_services_by_explosion(self)

        # We clean the tags
        for sg in self:
            try:
                del sg.rec_tag
            except AttributeError:
                pass
            del sg.already_explode

########NEW FILE########
__FILENAME__ = timeperiod
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


# Calendar date
# -------------
#  '(\d{4})-(\d{2})-(\d{2}) - (\d{4})-(\d{2})-(\d{2}) / (\d+) ([0-9:, -]+)'
#   => len = 8  => CALENDAR_DATE
#
#  '(\d{4})-(\d{2})-(\d{2}) / (\d+) ([0-9:, -]+)'
#   => len = 5 => CALENDAR_DATE
#
#  '(\d{4})-(\d{2})-(\d{2}) - (\d{4})-(\d{2})-(\d{2}) ([0-9:, -]+)'
#   => len = 7 => CALENDAR_DATE
#
#  '(\d{4})-(\d{2})-(\d{2}) ([0-9:, -]+)'
#   => len = 4 => CALENDAR_DATE
#
# Month week day
# --------------
#  '([a-z]*) (\d+) ([a-z]*) - ([a-z]*) (\d+) ([a-z]*) / (\d+) ([0-9:, -]+)'
#  => len = 8 => MONTH WEEK DAY
#  e.g.: wednesday 1 january - thursday 2 july / 3
#
#  '([a-z]*) (\d+) - ([a-z]*) (\d+) / (\d+) ([0-9:, -]+)' => len = 6
#  e.g.: february 1 - march 15 / 3 => MONTH DATE
#  e.g.: monday 2 - thusday 3 / 2 => WEEK DAY
#  e.g.: day 2 - day 6 / 3 => MONTH DAY
#
#  '([a-z]*) (\d+) - (\d+) / (\d+) ([0-9:, -]+)' => len = 6
#  e.g.: february 1 - 15 / 3 => MONTH DATE
#  e.g.: thursday 2 - 4 => WEEK DAY
#  e.g.: day 1 - 4 => MONTH DAY
#
#  '([a-z]*) (\d+) ([a-z]*) - ([a-z]*) (\d+) ([a-z]*) ([0-9:, -]+)' => len = 7
#  e.g.: wednesday 1 january - thursday 2 july => MONTH WEEK DAY
#
#  '([a-z]*) (\d+) - (\d+) ([0-9:, -]+)' => len = 7
#  e.g.: thursday 2 - 4 => WEEK DAY
#  e.g.: february 1 - 15 / 3 => MONTH DATE
#  e.g.: day 1 - 4 => MONTH DAY
#
#  '([a-z]*) (\d+) - ([a-z]*) (\d+) ([0-9:, -]+)' => len = 5
#  e.g.: february 1 - march 15  => MONTH DATE
#  e.g.: monday 2 - thusday 3  => WEEK DAY
#  e.g.: day 2 - day 6  => MONTH DAY
#
#  '([a-z]*) (\d+) ([0-9:, -]+)' => len = 3
#  e.g.: february 3 => MONTH DATE
#  e.g.: thursday 2 => WEEK DAY
#  e.g.: day 3 => MONTH DAY
#
#  '([a-z]*) (\d+) ([a-z]*) ([0-9:, -]+)' => len = 4
#  e.g.: thusday 3 february => MONTH WEEK DAY
#
#  '([a-z]*) ([0-9:, -]+)' => len = 6
#  e.g.: thusday => normal values
#
# Types: CALENDAR_DATE
#        MONTH WEEK DAY
#        WEEK DAY
#        MONTH DATE
#        MONTH DAY
#

import time
import re
import string

from item import Item, Items

from shinken.daterange import Daterange, CalendarDaterange
from shinken.daterange import StandardDaterange, MonthWeekDayDaterange
from shinken.daterange import MonthDateDaterange, WeekDayDaterange
from shinken.daterange import MonthDayDaterange
from shinken.brok import Brok
from shinken.property import IntegerProp, StringProp, ListProp, BoolProp
from shinken.log import logger, console_logger


class Timeperiod(Item):
    id = 1
    my_type = 'timeperiod'

    properties = Item.properties.copy()
    properties.update({
        'timeperiod_name':  StringProp(fill_brok=['full_status']),
        'alias':            StringProp(default='', fill_brok=['full_status']),
        'use':              StringProp(default=''),
        'register':         IntegerProp(default='1'),

        # These are needed if a broker module calls methods on timeperiod objects
        'dateranges':       ListProp(fill_brok=['full_status'], default=[]),
        'exclude':          ListProp(fill_brok=['full_status'], default=[]),
        'is_active':        BoolProp(default='0')
    })
    running_properties = Item.running_properties.copy()

    def __init__(self, params={}):
        self.id = Timeperiod.id
        Timeperiod.id = Timeperiod.id + 1
        self.unresolved = []
        self.dateranges = []
        self.exclude = ''
        self.customs = {}
        self.plus = {}
        self.invalid_entries = []
        for key in params:
            # timeperiod objects are too complicated to support multi valued
            # attributes. we do as usual, last set value wins.
            if isinstance(params[key], list):
                if params[key]:
                    params[key] = params[key][-1]
                else:
                    params[key] = ''

            if key in ['name', 'alias', 'timeperiod_name', 'exclude', 'use', 'register', 'imported_from', 'is_active', 'dateranges']:
                setattr(self, key, params[key])
            else:
                self.unresolved.append(key + ' ' + params[key])

        self.cache = {}  # For tunning purpose only
        self.invalid_cache = {}  # same but for invalid search
        self.configuration_errors = []
        self.configuration_warnings = []
        # By default the tp is None so we know we just start
        self.is_active = None
        self.tags = set()

    def get_name(self):
        return getattr(self, 'timeperiod_name', 'unknown_timeperiod')

    # We fillfull properties with template ones if need
    # for the unresolved values (like sunday ETCETC)
    def get_unresolved_properties_by_inheritance(self, items):
        # Ok, I do not have prop, Maybe my templates do?
        # Same story for plus
        for i in self.templates:
            self.unresolved.extend(i.unresolved)

    # Ok timeperiods are a bit different from classic items, because we do not have a real list
    # of our raw properties, like if we got february 1 - 15 / 3 for example
    def get_raw_import_values(self):
        properties = ['timeperiod_name', 'alias', 'use', 'register']
        r = {}
        for prop in properties:
            if hasattr(self, prop):
                v = getattr(self, prop)
                print prop, ":", v
                r[prop] = v
        # Now the unresolved one. The only way to get ride of same key things is to put
        # directly the full value as the key
        for other in self.unresolved:
            r[other] = ''
        return r


    def is_time_valid(self, t):
        if self.has('exclude'):
            for dr in self.exclude:
                if dr.is_time_valid(t):
                    return False
        for dr in self.dateranges:
            if dr.is_time_valid(t):
                return True
        return False

    # will give the first time > t which is valid
    def get_min_from_t(self, t):
        mins_incl = []
        for dr in self.dateranges:
            mins_incl.append(dr.get_min_from_t(t))
        return min(mins_incl)

    # will give the first time > t which is not valid
    def get_not_in_min_from_t(self, f):
        pass

    def find_next_valid_time_from_cache(self, t):
        try:
            return self.cache[t]
        except KeyError:
            return None

    def find_next_invalid_time_from_cache(self, t):
        try:
            return self.invalid_cache[t]
        except KeyError:
            return None

    # will look for active/un-active change. And log it
    # [1327392000] TIMEPERIOD TRANSITION: <name>;<from>;<to>
    # from is -1 on startup.  to is 1 if the timeperiod starts
    # and 0 if it ends.
    def check_and_log_activation_change(self):
        now = int(time.time())

        was_active = self.is_active
        self.is_active = self.is_time_valid(now)

        # If we got a change, log it!
        if self.is_active != was_active:
            _from = 0
            _to = 0
            # If it's the start, get a special value for was
            if was_active is None:
                _from = -1
            if was_active:
                _from = 1
            if self.is_active:
                _to = 1

            # Now raise the log
            console_logger.info('TIMEPERIOD TRANSITION: %s;%d;%d'
                                % (self.get_name(), _from, _to))

    # clean the get_next_valid_time_from_t cache
    # The entries are a dict on t. t < now are useless
    # Because we do not care about past anymore.
    # If not, it's not important, it's just a cache after all :)
    def clean_cache(self):
        now = int(time.time())
        t_to_del = []
        for t in self.cache:
            if t < now:
                t_to_del.append(t)
        for t in t_to_del:
            del self.cache[t]

        # same for the invalid cache
        t_to_del = []
        for t in self.invalid_cache:
            if t < now:
                t_to_del.append(t)
        for t in t_to_del:
            del self.invalid_cache[t]

    def get_next_valid_time_from_t(self, t):
        # first find from cache
        t = int(t)
        original_t = t

        #logger.debug("[%s] Check valid time for %s" % ( self.get_name(), time.asctime(time.localtime(t)))

        res_from_cache = self.find_next_valid_time_from_cache(t)
        if res_from_cache is not None:
            return res_from_cache

        still_loop = True

        # Loop for all minutes...
        while still_loop:
            local_min = None

            # Ok, not in cache...
            dr_mins = []
            s_dr_mins = []

            for dr in self.dateranges:
                dr_mins.append(dr.get_next_valid_time_from_t(t))

            s_dr_mins = sorted([d for d in dr_mins if d is not None])

            for t1 in s_dr_mins:
                if not self.exclude and still_loop is True:
                    # No Exclude so we are good
                    local_min = t1
                    still_loop = False
                else:
                    for tp in self.exclude:
                        if not tp.is_time_valid(t1) and still_loop is True:
                            # OK we found a date that is not valid in any exclude timeperiod
                            local_min = t1
                            still_loop = False

            if local_min is None:
                # print "Looking for next valid date"
                exc_mins = []
                if s_dr_mins != []:
                    for tp in self.exclude:
                        exc_mins.append(tp.get_next_invalid_time_from_t(s_dr_mins[0]))

                s_exc_mins = sorted([d for d in exc_mins if d is not None])

                if s_exc_mins != []:
                    local_min = s_exc_mins[0]

            if local_min is None:
                still_loop = False
            else:
                t = local_min
                # No loop more than one year
                if t > original_t + 3600*24*366 + 1:
                    still_loop = False
                    local_min = None

        # Ok, we update the cache...
        self.cache[original_t] = local_min
        return local_min

    def get_next_invalid_time_from_t(self, t):
        #print '\n\n', self.get_name(), 'Search for next invalid from', time.asctime(time.localtime(t)), t
        t = int(t)
        original_t = t
        still_loop = True

        # First try to find in cache
        res_from_cache = self.find_next_invalid_time_from_cache(t)
        if res_from_cache is not None:
            return res_from_cache

        # Then look, maybe t is already invalid
        if not self.is_time_valid(t):
            return t

        local_min = t
        res = None
        # Loop for all minutes...
        while still_loop:
            #print "Invalid loop with", time.asctime(time.localtime(local_min))

            dr_mins = []
            #val_valids = []
            #val_inval = []
            # But maybe we can find a better solution with next invalid of standard dateranges
            #print self.get_name(), "After valid of exclude, local_min =", time.asctime(time.localtime(local_min))
            for dr in self.dateranges:
                #print self.get_name(), "Search a next invalid from DR", time.asctime(time.localtime(local_min))
                #print dr.__dict__
                m = dr.get_next_invalid_time_from_t(local_min)

                #print self.get_name(), "Dr", dr.__dict__,  "give me next invalid", time.asctime(time.localtime(m))
                if m is not None:
                    # But maybe it's invalid for this dr, but valid for other ones.
                    #if not self.is_time_valid(m):
                    #     print "Final: Got a next invalid at", time.asctime(time.localtime(m))
                    dr_mins.append(m)
                    #if not self.is_time_valid(m):
                    #    val_inval.append(m)
                    #else:
                    #    val_valids.append(m)
                    #    print "Add a m", time.asctime(time.localtime(m))
                    #else:
                    #     print dr.__dict__
                    #     print "FUCK bad result\n\n\n"
            #print "Inval"
            #for v in val_inval:
            #    print "\t", time.asctime(time.localtime(v))
            #print "Valid"
            #for v in val_valids:
            #    print "\t", time.asctime(time.localtime(v))

            if dr_mins != []:
                local_min = min(dr_mins)
                # Take the minimum valid as lower for next search
                #local_min_valid = 0
                #if val_valids != []:
                #    local_min_valid = min(val_valids)
                #if local_min_valid != 0:
                #    local_min = local_min_valid
                #else:
                #    local_min = min(dr_mins)
                #print "UPDATE After dr: found invalid local min:", time.asctime(time.localtime(local_min)), "is valid", self.is_time_valid(local_min)

            #print self.get_name(), 'Invalid: local min', local_min #time.asctime(time.localtime(local_min))
            # We do not loop unless the local_min is not valid
            if not self.is_time_valid(local_min):
                still_loop = False
            else:  # continue until we reach too far..., in one minute
                # After one month, go quicker...
                if local_min > original_t + 3600*24*30:
                    local_min += 3600
                else:  # else search for 1min precision
                    local_min += 60
                # after one year, stop.
                if local_min > original_t + 3600*24*366 + 1:  # 60*24*366 + 1:
                    still_loop = False
            #print "Loop?", still_loop
            # if we've got a real value, we check it with the exclude
            if local_min is not None:
                # Now check if local_min is not valid
                for tp in self.exclude:
                    #print self.get_name(),"we check for invalid", time.asctime(time.localtime(local_min)), 'with tp', tp.name
                    if tp.is_time_valid(local_min):
                        still_loop = True
                        # local_min + 60
                        local_min = tp.get_next_invalid_time_from_t(local_min+60)
                        # No loop more than one year
                        if local_min > original_t + 60*24*366 + 1:
                            still_loop = False
                            res = None

            if not still_loop:  # We find a possible value
                # We take the result the minimal possible
                if res is None or local_min < res:
                    res = local_min

        #print "Finished Return the next invalid", time.asctime(time.localtime(local_min))
        # Ok, we update the cache...
        self.invalid_cache[original_t] = local_min
        return local_min

    def has(self, prop):
        return hasattr(self, prop)

    # We are correct only if our daterange are
    # and if we have no unmatch entries
    def is_correct(self):
        b = True
        for dr in self.dateranges:
            b &= dr.is_correct()

        # Warn about non correct entries
        for e in self.invalid_entries:
            logger.warning("[timeperiod::%s] invalid entry '%s'" % (self.get_name(), e))
        return b

    def __str__(self):
        s = ''
        s += str(self.__dict__) + '\n'
        for elt in self.dateranges:
            s += str(elt)
            (start, end) = elt.get_start_and_end_time()
            start = time.asctime(time.localtime(start))
            end = time.asctime(time.localtime(end))
            s += "\nStart and end:" + str((start, end))
        s += '\nExclude'
        for elt in self.exclude:
            s += str(elt)

        return s

    def resolve_daterange(self, dateranges, entry):
        #print "Trying to resolve ", entry

        res = re.search('(\d{4})-(\d{2})-(\d{2}) - (\d{4})-(\d{2})-(\d{2}) / (\d+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 1"
            (syear, smon, smday, eyear, emon, emday, skip_interval, other) = res.groups()
            dateranges.append(CalendarDaterange(syear, smon, smday, 0, 0, eyear, emon, emday, 0, 0, skip_interval, other))
            return

        res = re.search('(\d{4})-(\d{2})-(\d{2}) / (\d+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 2"
            (syear, smon, smday, skip_interval, other) = res.groups()
            eyear = syear
            emon = smon
            emday = smday
            dateranges.append(CalendarDaterange(syear, smon, smday, 0, 0, eyear, emon, emday, 0, 0, skip_interval, other))
            return

        res = re.search('(\d{4})-(\d{2})-(\d{2}) - (\d{4})-(\d{2})-(\d{2})[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 3"
            (syear, smon, smday, eyear, emon, emday, other) = res.groups()
            dateranges.append(CalendarDaterange(syear, smon, smday, 0, 0, eyear, emon, emday, 0, 0, 0, other))
            return

        res = re.search('(\d{4})-(\d{2})-(\d{2})[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 4"
            (syear, smon, smday, other) = res.groups()
            eyear = syear
            emon = smon
            emday = smday
            dateranges.append(CalendarDaterange(syear, smon, smday, 0, 0, eyear, emon, emday, 0, 0, 0, other))
            return

        res = re.search('([a-z]*) ([\d-]+) ([a-z]*) - ([a-z]*) ([\d-]+) ([a-z]*) / (\d+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 5"
            (swday, swday_offset, smon, ewday, ewday_offset, emon, skip_interval, other) = res.groups()
            dateranges.append(MonthWeekDayDaterange(0, smon, 0, swday, swday_offset, 0, emon, 0, ewday, ewday_offset, skip_interval, other))
            return

        res = re.search('([a-z]*) ([\d-]+) - ([a-z]*) ([\d-]+) / (\d+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 6"
            (t0, smday, t1, emday, skip_interval, other) = res.groups()
            if t0 in Daterange.weekdays and t1 in Daterange.weekdays:
                swday = t0
                ewday = t1
                swday_offset = smday
                ewday_offset = emday
                dateranges.append(WeekDayDaterange(0, 0, 0, swday, swday_offset, 0, 0, 0, ewday, ewday_offset, skip_interval, other))
                return
            elif t0 in Daterange.months and t1 in Daterange.months:
                smon = t0
                emon = t1
                dateranges.append(MonthDateDaterange(0, smon, smday, 0, 0, 0, emon, emday, 0, 0, skip_interval, other))
                return
            elif t0 == 'day' and t1 == 'day':
                dateranges.append(MonthDayDaterange(0, 0, smday, 0, 0, 0, 0, emday, 0, 0, skip_interval, other))
                return

        res = re.search('([a-z]*) ([\d-]+) - ([\d-]+) / (\d+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 7"
            (t0, smday, emday, skip_interval, other) = res.groups()
            if t0 in Daterange.weekdays:
                swday = t0
                swday_offset = smday
                ewday = swday
                ewday_offset = emday
                dateranges.append(WeekDayDaterange(0, 0, 0, swday, swday_offset, 0, 0, 0, ewday, ewday_offset, skip_interval, other))
                return
            elif t0 in Daterange.months:
                smon = t0
                emon = smon
                dateranges.append(MonthDateDaterange(0, smon, smday, 0, 0, 0, emon, emday, 0, 0, skip_interval, other))
                return
            elif t0 == 'day':
                dateranges.append(MonthDayDaterange(0, 0, smday, 0, 0, 0, 0, emday, 0, 0, skip_interval, other))
                return

        res = re.search('([a-z]*) ([\d-]+) ([a-z]*) - ([a-z]*) ([\d-]+) ([a-z]*) [\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 8"
            (swday, swday_offset, smon, ewday, ewday_offset, emon, other) = res.groups()
            #print "Debug:", (swday, swday_offset, smon, ewday, ewday_offset, emon, other)
            dateranges.append(MonthWeekDayDaterange(0, smon, 0, swday, swday_offset, 0, emon, 0, ewday, ewday_offset, 0, other))
            return

        res = re.search('([a-z]*) ([\d-]+) - ([\d-]+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 9"
            (t0, smday, emday, other) = res.groups()
            if t0 in Daterange.weekdays:
                swday = t0
                swday_offset = smday
                ewday = swday
                ewday_offset = emday
                dateranges.append(WeekDayDaterange(0, 0, 0, swday, swday_offset, 0, 0, 0, ewday, ewday_offset, 0, other))
                return
            elif t0 in Daterange.months:
                smon = t0
                emon = smon
                dateranges.append(MonthDateDaterange(0, smon, smday, 0, 0, 0, emon, emday, 0, 0, 0, other))
                return
            elif t0 == 'day':
                dateranges.append(MonthDayDaterange(0, 0, smday, 0, 0, 0, 0, emday, 0, 0, 0, other))
                return

        res = re.search('([a-z]*) ([\d-]+) - ([a-z]*) ([\d-]+)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 10"
            (t0, smday, t1, emday, other) = res.groups()
            if t0 in Daterange.weekdays and t1 in Daterange.weekdays:
                swday = t0
                ewday = t1
                swday_offset = smday
                ewday_offset = emday
                dateranges.append(WeekDayDaterange(0, 0, 0, swday, swday_offset, 0, 0, 0, ewday, ewday_offset, 0, other))
                return
            elif t0 in Daterange.months and t1 in Daterange.months:
                smon = t0
                emon = t1
                dateranges.append(MonthDateDaterange(0, smon, smday, 0, 0, 0, emon, emday, 0, 0, 0, other))
                return
            elif t0 == 'day' and t1 == 'day':
                dateranges.append(MonthDayDaterange(0, 0, smday, 0, 0, 0, 0, emday, 0, 0, 0, other))
                return

        res = re.search('([a-z]*) ([\d-]+) ([a-z]*)[\s\t]*([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 11"
            (t0, swday_offset, t1, other) = res.groups()
            if t0 in Daterange.weekdays and t1 in Daterange.months:
                swday = t0
                smon = t1
                emon = smon
                ewday = swday
                ewday_offset = swday_offset
                dateranges.append(MonthWeekDayDaterange(0, smon, 0, swday, swday_offset, 0, emon, 0, ewday, ewday_offset, 0, other))
                return

        res = re.search('([a-z]*) ([\d-]+)[\s\t]+([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 12"
            (t0, smday, other) = res.groups()
            if t0 in Daterange.weekdays:
                swday = t0
                swday_offset = smday
                ewday = swday
                ewday_offset = swday_offset
                dateranges.append(WeekDayDaterange(0, 0, 0, swday, swday_offset, 0, 0, 0, ewday, ewday_offset, 0, other))
                return
            if t0 in Daterange.months:
                smon = t0
                emon = smon
                emday = smday
                dateranges.append(MonthDateDaterange(0, smon, smday, 0, 0, 0, emon, emday, 0, 0, 0, other))
                return
            if t0 == 'day':
                emday = smday
                dateranges.append(MonthDayDaterange(0, 0, smday, 0, 0, 0, 0, emday, 0, 0, 0, other))
                return

        res = re.search('([a-z]*)[\s\t]+([0-9:, -]+)', entry)
        if res is not None:
            #print "Good catch 13"
            (t0, other) = res.groups()
            if t0 in Daterange.weekdays:
                day = t0
                dateranges.append(StandardDaterange(day, other))
                return
        logger.info("[timeentry::%s] no match for %s" % (self.get_name(), entry))
        self.invalid_entries.append(entry)

    def apply_inheritance(self):
        pass

    # create daterange from unresolved param
    def explode(self, timeperiods):
        for entry in self.unresolved:
            #print "Revolving entry", entry
            self.resolve_daterange(self.dateranges, entry)
        self.unresolved = []

    # Will make tp in exclude with id of the timeperiods
    def linkify(self, timeperiods):
        new_exclude = []
        if self.has('exclude') and self.exclude != '':
            logger.debug("[timeentry::%s] have excluded %s" % (self.get_name(), self.exclude))
            excluded_tps = self.exclude.split(',')
            #print "I will exclude from:", excluded_tps
            for tp_name in excluded_tps:
                tp = timeperiods.find_by_name(tp_name.strip())
                if tp is not None:
                    new_exclude.append(tp)
                else:
                    logger.error("[timeentry::%s] unknown %s timeperiod" % (self.get_name(), tp_name))
        self.exclude = new_exclude

    def check_exclude_rec(self):
        if self.rec_tag:
            logger.error("[timeentry::%s] is in a loop in exclude parameter" % self.get_name())
            return False
        self.rec_tag = True
        for tp in self.exclude:
            tp.check_exclude_rec()
        return True

    def fill_data_brok_from(self, data, brok_type):
        cls = self.__class__
        # Now config properties
        for prop, entry in cls.properties.items():
            # Is this property intended for broking?
            #if 'fill_brok' in entry:
            if brok_type in entry.fill_brok:
                if hasattr(self, prop):
                    data[prop] = getattr(self, prop)
                elif entry.has_default:
                    data[prop] = entry.default

    # Get a brok with initial status
    def get_initial_status_brok(self):
        cls = self.__class__
        my_type = cls.my_type
        data = {'id': self.id}

        self.fill_data_brok_from(data, 'full_status')
        b = Brok('initial_' + my_type + '_status', data)
        return b


class Timeperiods(Items):
    name_property = "timeperiod_name"
    inner_class = Timeperiod

    def explode(self):
        for id in self.items:
            tp = self.items[id]
            tp.explode(self)

    def linkify(self):
        for id in self.items:
            tp = self.items[id]
            tp.linkify(self)

    def apply_inheritance(self):
        # The only interesting property to inherit is exclude
        self.apply_partial_inheritance('exclude')
        for i in self:
            i.get_customs_properties_by_inheritance(self)

        # And now apply inheritance for unresolved properties
        # like the dateranges in fact
        for tp in self:
            tp.get_unresolved_properties_by_inheritance(self.items)

    # check for loop in definition
    def is_correct(self):
        r = True
        # We do not want a same hg to be explode again and again
        # so we tag it
        for tp in self.items.values():
            tp.rec_tag = False

        for tp in self.items.values():
            for tmp_tp in self.items.values():
                tmp_tp.rec_tag = False
            r &= tp.check_exclude_rec()

        # We clean the tags
        for tp in self.items.values():
            del tp.rec_tag

        # And check all timeperiods for correct (sunday is false)
        for tp in self:
            r &= tp.is_correct()

        return r



if __name__ == '__main__':
    t = Timeperiod()
    test = ['1999-01-28  00:00-24:00',
            'monday 3                   00:00-24:00             ',
            'day 2                      00:00-24:00',
            'february 10                00:00-24:00',
            'february -1 00:00-24:00',
            'friday -2                  00:00-24:00',
            'thursday -1 november 00:00-24:00',
            '2007-01-01 - 2008-02-01    00:00-24:00',
            'monday 3 - thursday 4      00:00-24:00',
            'day 1 - 15         00:00-24:00',
            'day 20 - -1                00:00-24:00',
            'july -10 - -1              00:00-24:00',
            'april 10 - may 15          00:00-24:00',
            'tuesday 1 april - friday 2 may 00:00-24:00',
            '2007-01-01 - 2008-02-01 / 3 00:00-24:00',
            '2008-04-01 / 7             00:00-24:00',
            'day 1 - 15 / 5             00:00-24:00',
            'july 10 - 15 / 2 00:00-24:00',
            'tuesday 1 april - friday 2 may / 6 00:00-24:00',
            'tuesday 1 october - friday 2 may / 6 00:00-24:00',
            'monday 3 - thursday 4 / 2 00:00-24:00',
            'monday 4 - thursday 3 / 2 00:00-24:00',
            'day -1 - 15 / 5            01:00-24:00,00:30-05:60',
            'tuesday 00:00-24:00',
            'sunday 00:00-24:00',
            'saturday 03:00-24:00,00:32-01:02',
            'wednesday 09:00-15:46,00:00-21:00',
            'may 7 - february 2 00:00-10:00',
            'day -1 - 5 00:00-10:00',
            'tuesday 1 february - friday 1 may 01:00-24:00,00:30-05:60',
            'december 2 - may -15               00:00-24:00',
            ]
    for entry in test:
        print "**********************"
        print entry
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, entry)
        #t.exclude = []
        #t.resolve_daterange(t.exclude, 'monday 00:00-19:00')
        #t.check_valid_for_today()
        now = time.time()
        #print "Is valid NOW?", t.is_time_valid(now)
        t_next = t.get_next_valid_time_from_t(now + 5*60)
        if t_next is not None:
            print "Get next valid for now + 5 min ==>", time.asctime(time.localtime(t_next)), "<=="
        else:
            print "===> No future time!!!"
        #print "End date:", t.get_end_time()
        #print "Next valid", time.asctime(time.localtime(t.get_next_valid_time()))
        print str(t) + '\n\n'

    print "*************************************************************"
    t3 = Timeperiod()
    t3.timeperiod_name = 't3'
    t3.resolve_daterange(t3.dateranges, 'day 1 - 10 10:30-15:00')
    t3.exclude = []

    t2 = Timeperiod()
    t2.timeperiod_name = 't2'
    t2.resolve_daterange(t2.dateranges, 'day 1 - 10 12:00-17:00')
    t2.exclude = [t3]

    t = Timeperiod()
    t.timeperiod_name = 't'
    t.resolve_daterange(t.dateranges, 'day 1 - 10 14:00-15:00')
    t.exclude = [t2]

    print "Mon T", str(t) + '\n\n'
    t_next = t.get_next_valid_time_from_t(now)
    t_no_next = t.get_next_invalid_time_from_t(now)
    print "Get next valid for now ==>", time.asctime(time.localtime(t_next)), "<=="
    print "Get next invalid for now ==>", time.asctime(time.localtime(t_no_next)), "<=="

########NEW FILE########
__FILENAME__ = trigger
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import os
import re

from shinken.objects.item import Item, Items
from shinken.misc.perfdata import PerfDatas
from shinken.property import BoolProp, IntegerProp, FloatProp, CharProp, StringProp, ListProp
from shinken.log import logger
from shinken.trigger_functions import objs, trigger_functions
#objs = {'hosts': [], 'services': []}


class Trigger(Item):
    id = 1  # zero is always special in database, so we do not take risk here
    my_type = 'trigger'

    properties = Item.properties.copy()
    properties.update({'trigger_name': StringProp(fill_brok=['full_status']),
                       'code_src': StringProp(default='', fill_brok=['full_status']),
                       })

    running_properties = Item.running_properties.copy()
    running_properties.update({'code_bin': StringProp(default=None),
                               'trigger_broker_raise_enabled': BoolProp(default='0')
                               })

    # For debugging purpose only (nice name)
    def get_name(self):
        try:
            return self.trigger_name
        except AttributeError:
            return 'UnnamedTrigger'

    def compile(self):
        self.code_bin = compile(self.code_src, "<irc>", "exec")

    # ctx is the object we are evaluating the code. In the code
    # it will be "self".
    def eval(myself, ctx):
        self = ctx

        # Ok we can declare for this trigger call our functions
        for (n, f) in trigger_functions.iteritems():
            locals()[n] = f

        code = myself.code_bin  # Comment? => compile(myself.code_bin, "<irc>", "exec")
        exec code in dict(locals())

    def __getstate__(self):
        return {'trigger_name': self.trigger_name, 'code_src': self.code_src, 'trigger_broker_raise_enabled': self.trigger_broker_raise_enabled}

    def __setstate__(self, d):
        self.trigger_name = d['trigger_name']
        self.code_src = d['code_src']
        self.trigger_broker_raise_enabled = d['trigger_broker_raise_enabled']


class Triggers(Items):
    name_property = "trigger_name"
    inner_class = Trigger

    # We will dig into the path and load all .trig files
    def load_file(self, path):
        # Now walk for it
        for root, dirs, files in os.walk(path):
            for file in files:
                if re.search("\.trig$", file):
                    p = os.path.join(root, file)
                    try:
                        fd = open(p, 'rU')
                        buf = fd.read()
                        fd.close()
                    except IOError, exp:
                        logger.error("Cannot open trigger file '%s' for reading: %s" % (p, exp))
                        # ok, skip this one
                        continue
                    self.create_trigger(buf, file[:-5])

    # Create a trigger from the string src, and with the good name
    def create_trigger(self, src, name):
        # Ok, go compile the code
        t = Trigger({'trigger_name': name, 'code_src': src})
        t.compile()
        # Ok, add it
        self[t.id] = t
        return t

    def compile(self):
        for i in self:
            i.compile()

    def load_objects(self, conf):
        global objs
        objs['hosts'] = conf.hosts
        objs['services'] = conf.services

########NEW FILE########
__FILENAME__ = pollerlink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp


class PollerLink(SatelliteLink):
    """This class is the link between Arbiter and Poller. With it, arbiter
    can see if a poller is alive, and can send it new configuration

    """

    id = 0
    my_type = 'poller'
    # To_send: send or not to satellite conf
    properties = SatelliteLink.properties.copy()
    properties.update({
        'poller_name':  StringProp(fill_brok=['full_status'], to_send=True),
        'port':         IntegerProp(default=7771, fill_brok=['full_status']),
        'passive':     BoolProp(default='0', fill_brok=['full_status'], to_send=True),
        'min_workers':  IntegerProp(default='0', fill_brok=['full_status'], to_send=True),
        'max_workers':  IntegerProp(default='30', fill_brok=['full_status'], to_send=True),
        'processes_by_worker': IntegerProp(default='256', fill_brok=['full_status'], to_send=True),
        'poller_tags':  ListProp(default='None', to_send=True),
    })

    def get_name(self):
        return self.poller_name

    def register_to_my_realm(self):
        self.realm.pollers.append(self)


class PollerLinks(SatelliteLinks):
    """Please Add a Docstring to describe the class here"""

    name_property = "poller_name"
    inner_class = PollerLink

########NEW FILE########
__FILENAME__ = property
#!/usr/bin/env python

# -*- mode: python ; coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import re

from shinken.util import to_float, to_split, to_char, to_int, unique_value
from shinken.log  import logger

__all__ = ['UnusedProp', 'BoolProp', 'IntegerProp', 'FloatProp',
           'CharProp', 'StringProp', 'ListProp',
           'FULL_STATUS', 'CHECK_RESULT']

# Suggestion
# Is this useful? see above
__author__ = "Hartmut Goebel <h.goebel@goebel-consult.de>"
__copyright__ = "Copyright 2010-2011 by Hartmut Goebel <h.goebel@goebel-consult.de>"
__licence__ = "GNU Affero General Public License version 3 (AGPL v3)"

FULL_STATUS = 'full_status'
CHECK_RESULT = 'check_result'

none_object = object()


class Property(object):
    """Baseclass of all properties.

    Same semantic for all subclasses (except UnusedProp): The property
    is required if, and only if, the default value is `None`.


    """

    def __init__(self, default=none_object, class_inherit=None,
                 unmanaged=False, help='', no_slots=False,
                 fill_brok=None, conf_send_preparation=None,
                 brok_transformation=None, retention=False,
                 retention_preparation=None, to_send=False,
                 override=False, managed=True, split_on_coma=True, merging='uniq'):

        """
        `default`: default value to be used if this property is not set.
                   If default is None, this property is required.

        `class_inherit`: List of 2-tuples, (Service, 'blabla'): must
                   set this property to the Service class with name
                   blabla. if (Service, None): must set this property
                   to the Service class with same name
        `unmanaged`: ....
        `help`: usage text
        `no_slots`: do not take this property for __slots__

        `fill_brok`: if set, send to broker. There are two categories:
                     FULL_STATUS for initial and update status,
                     CHECK_RESULT for check results
        `retention`: if set, we will save this property in the retention files
        `retention_preparation`: function, if set, will go this function before
                     being save to the retention data
        `split_on_coma`: indicates that list property value should not be
                     splitted on coma delimiter (values conain comas that
                     we want to keep).

        Only for the initial call:

        conf_send_preparation: if set, will pass the property to this
                     function. It's used to 'flatten' some dangerous
                     properties like realms that are too 'linked' to
                     be send like that.

        brok_transformation: if set, will call the function with the
                     value of the property when flattening
                     data is necessary (like realm_name instead of
                     the realm object).

        override: for scheduler, if the property must override the
                     value of the configuration we send it

        managed: property that is managed in Nagios but not in Shinken

        merging: for merging properties, should we take only one or we can
                     link with ,

        """

        self.default = default
        self.has_default = (default is not none_object)
        self.required = not self.has_default
        self.class_inherit = class_inherit or []
        self.help = help or ''
        self.unmanaged = unmanaged
        self.no_slots = no_slots
        self.fill_brok = fill_brok or []
        self.conf_send_preparation = conf_send_preparation
        self.brok_transformation = brok_transformation
        self.retention = retention
        self.retention_preparation = retention_preparation
        self.to_send = to_send
        self.override = override
        self.managed = managed
        self.unused = False
        self.merging = merging
        self.split_on_coma = split_on_coma


class UnusedProp(Property):
    """A unused Property. These are typically used by Nagios but
    no longer useful/used by Shinken.

    This is just to warn the user that the option he uses is no more used
    in Shinken.

    """

    # Since this property is not used, there is no use for other
    # parameters than 'text'.
    # 'text' a some usage text if present, will print it to explain
    # why it's no more useful
    def __init__(self, text=None):

        if text is None:
            text = ("This parameter is no longer useful in the "
                    "Shinken architecture.")
        self.text = text
        self.has_default = False
        self.class_inherit = []
        self.unused = True
        self.managed = True

_boolean_states = {'1': True, 'yes': True, 'true': True, 'on': True,
                   '0': False, 'no': False, 'false': False, 'off': False}


class BoolProp(Property):
    """A Boolean Property.

    Boolean values are currently case insensitively defined as 0,
    false, no, off for False, and 1, true, yes, on for True).
    """

    #@staticmethod
    def pythonize(self, val):
        val = unique_value(val)
        return _boolean_states[val.lower()]


class IntegerProp(Property):
    """Please Add a Docstring to describe the class here"""

    #@staticmethod
    def pythonize(self, val):
        val = unique_value(val)
        return to_int(val)


class FloatProp(Property):
    """Please Add a Docstring to describe the class here"""

    #@staticmethod
    def pythonize(self, val):
        val = unique_value(val)
        return to_float(val)


class CharProp(Property):
    """Please Add a Docstring to describe the class here"""

    #@staticmethod
    def pythonize(self, val):
        val = unique_value(val)
        return to_char(val)


class StringProp(Property):
    """Please Add a Docstring to describe the class here"""

    #@staticmethod
    def pythonize(self, val):
        val = unique_value(val)
        return val


class PathProp(StringProp):
    """ A string property representing a "running" (== VAR) file path """


class ConfigPathProp(StringProp):
    """ A string property representing a config file path """


class ListProp(Property):
    """Please Add a Docstring to describe the class here"""

    #@staticmethod
    def pythonize(self, val):
        return to_split(val, self.split_on_coma)


class LogLevelProp(StringProp):
    """ A string property representing a logging level """

    def pythonize(self, val):
        val = unique_value(val)
        return logger.get_level_id(val)


class DictProp(Property):
    def __init__(self, elts_prop=None, *args, **kwargs):
        """Dictionary of values.
             If elts_prop is not None, must be a Property subclass
             All dict values will be casted as elts_prop values when pythonized

            elts_prop = Property of dict members
        """
        super(DictProp, self).__init__(*args, **kwargs)

        if not elts_prop is None and not issubclass(elts_prop, Property):
            raise TypeError("DictProp constructor only accept Property sub-classes as elts_prop parameter")
        self.elts_prop = elts_prop()

    def pythonize(self, val):
        val = unique_value(val)
        #import traceback; traceback.print_stack()
        def split(kv):
            m = re.match("^\s*([^\s]+)\s*=\s*([^\s]+)\s*$", kv)
            if m is None:
                raise ValueError

            return (
                m.group(1),
                # >2.4 only. we keep it for later. m.group(2) if self.elts_prop is None else self.elts_prop.pythonize(m.group(2))
                (self.elts_prop.pythonize(m.group(2)), m.group(2))[self.elts_prop is None]
            )

        if val is None:
            return(dict())

        # val is in the form "key1=addr:[port],key2=addr:[port],..."
        print ">>>", dict([split(kv) for kv in to_split(val)])
        return dict([split(kv) for kv in to_split(val)])


class AddrProp(Property):
    """Address property (host + port)"""

    def pythonize(self, val):
        """
            i.e: val = "192.168.10.24:445"
            NOTE: port is optional
        """
        val = unique_value(val)
        m = re.match("^([^:]*)(?::(\d+))?$", val)
        if m is None:
            raise ValueError

        addr = {'address': m.group(1)}
        if m.group(2) is not None:
            addr['port'] = int(m.group(2))

        return addr

########NEW FILE########
__FILENAME__ = reactionnerlink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp


class ReactionnerLink(SatelliteLink):
    """Please Add a Docstring to describe the class here"""

    id = 0
    my_type = 'reactionner'
    properties = SatelliteLink.properties.copy()
    properties.update({
        'reactionner_name': StringProp(fill_brok=['full_status'], to_send=True),
        'port':             IntegerProp(default='7769', fill_brok=['full_status']),
        'passive':          BoolProp(default='0', fill_brok=['full_status'], to_send=True),
        'min_workers':      IntegerProp(default='1', fill_brok=['full_status'], to_send=True),
        'max_workers':      IntegerProp(default='30', fill_brok=['full_status'], to_send=True),
        'processes_by_worker': IntegerProp(default='256', fill_brok=['full_status'], to_send=True),
        'reactionner_tags':      ListProp(default='None', to_send=True),
    })

    def get_name(self):
        return self.reactionner_name

    def register_to_my_realm(self):
        self.realm.reactionners.append(self)


class ReactionnerLinks(SatelliteLinks): # (Items):
    """Please Add a Docstring to describe the class here"""

    name_property = "reactionner_name"
    inner_class = ReactionnerLink

########NEW FILE########
__FILENAME__ = receiverlink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.log import logger
from shinken.http_client import HTTPExceptions


class ReceiverLink(SatelliteLink):
    """Please Add a Docstring to describe the class here"""

    id = 0
    my_type = 'receiver'
    properties = SatelliteLink.properties.copy()
    properties.update({
        'receiver_name':      StringProp(fill_brok=['full_status'], to_send=True),
        'port':               IntegerProp(default='7772', fill_brok=['full_status']),
        'manage_sub_realms':  BoolProp(default='1', fill_brok=['full_status']),
        'manage_arbiters':    BoolProp(default='0', fill_brok=['full_status'], to_send=True),
        'direct_routing':     BoolProp(default='0', fill_brok=['full_status'], to_send=True),
    })


    def get_name(self):
        return self.receiver_name


    def register_to_my_realm(self):
        self.realm.receivers.append(self)


    def push_host_names(self, sched_id, hnames):
        try:
            if self.con is None:
                self.create_connection()
            logger.info(" (%s)" % self.uri)

            # If the connection failed to initialize, bail out
            if self.con is None:
                self.add_failed_check_attempt()
                return

            #r = self.con.push_host_names(sched_id, hnames)
            self.con.get('ping')
            self.con.post('push_host_names', {'sched_id':sched_id, 'hnames':hnames}, wait='long')
        except HTTPExceptions, exp:
            self.add_failed_check_attempt(reason=str(exp))



class ReceiverLinks(SatelliteLinks):
    """Please Add a Docstring to describe the class here"""

    name_property = "receiver_name"
    inner_class = ReceiverLink

########NEW FILE########
__FILENAME__ = satellite
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
This class is an interface for Reactionner and Poller daemons
A Reactionner listens to a port for the configuration from the Arbiter
The conf contains the schedulers where actionners will gather actions.

The Reactionner keeps on listening to the Arbiter
(one a timeout)

If Arbiter wants it to have a new conf, the satellite forgets the previous
 Schedulers (and actions into) and takes the new ones.
"""



# Try to see if we are in an android device or not
is_android = True
try:
    import android
except ImportError:
    is_android = False

from Queue import Empty

if not is_android:
    from multiprocessing import Queue, active_children, cpu_count
else:
    from Queue import Queue

import os
import copy
import time
import sys
import cPickle
import traceback
import socket
import zlib
import base64
import threading

from shinken.http_daemon import HTTPDaemon
from shinken.http_client import HTTPClient, HTTPExceptions

from shinken.message import Message
from shinken.worker import Worker
from shinken.load import Load
from shinken.daemon import Daemon, Interface
from shinken.log import logger



# Class to tell that we are facing a non worker module
# but a standard one
class NotWorkerMod(Exception):
    pass


# Interface for Arbiter, our big MASTER
# It gives us our conf
class IForArbiter(Interface):

    doc = 'Remove a scheduler connexion (internal)'
    # Arbiter ask us to do not manage a scheduler_id anymore
    # I do it and don't ask why
    def remove_from_conf(self, sched_id):
        try:
            del self.app.schedulers[sched_id]
        except KeyError:
            pass
    remove_from_conf.doc = doc
    
    
    doc = 'Return the managed configuration ids (internal)'
    # Arbiter ask me which sched_id I manage, If it is not ok with it
    # It will ask me to remove one or more sched_id
    def what_i_managed(self):
        logger.debug("The arbiter asked me what I manage. It's %s" % self.app.what_i_managed())
        return self.app.what_i_managed()
    what_i_managed.need_lock = False
    what_i_managed.doc = doc
    
    doc = 'Ask the daemon to drop its configuration and wait for a new one'
    # Call by arbiter if it thinks we are running but we must do not (like
    # if I was a spare that take a conf but the master returns, I must die
    # and wait a new conf)
    # Us: No please...
    # Arbiter: I don't care, hasta la vista baby!
    # Us: ... <- Nothing! We are dead! you don't get it or what??
    # Reading code is not a job for eyes only...
    def wait_new_conf(self):
        logger.debug("Arbiter wants me to wait for a new configuration")
        self.app.schedulers.clear()
        self.app.cur_conf = None
    wait_new_conf.doc = doc


    doc = 'Push broks objects to the daemon (internal)'
    # NB: following methods are only used by broker
    # Used by the Arbiter to push broks to broker
    def push_broks(self, broks):
        with self.app.arbiter_broks_lock:
            self.app.arbiter_broks.extend(broks.values())
    push_broks.method = 'post'
    # We are using a Lock just for NOT lock this call from the arbiter :)
    push_broks.need_lock = False
    push_broks.doc = doc
    
    
    doc = 'Get the external commands from the daemon (internal)'
    # The arbiter ask us our external commands in queue
    # Same than push_broks, we will not using Global lock here,
    # and only lock for external_commands
    def get_external_commands(self):
        with self.app.external_commands_lock:
            cmds = self.app.get_external_commands()
            raw = cPickle.dumps(cmds)
        return raw
    get_external_commands.need_lock = False
    get_external_commands.doc = doc

    
    doc = 'Does the daemon got configuration (receiver)'
    ### NB: only useful for receiver
    def got_conf(self):
        return self.app.cur_conf is not None
    got_conf.need_lock = False
    got_conf.doc = doc


    doc = 'Push hostname/scheduler links (receiver in direct routing)'
    # Use by the receivers to got the host names managed by the schedulers
    def push_host_names(self, sched_id, hnames):
        self.app.push_host_names(sched_id, hnames)
    push_host_names.method = 'post'
    push_host_names.doc = doc


class ISchedulers(Interface):
    """Interface for Schedulers
    If we are passive, they connect to this and send/get actions

    """
    
    doc = 'Push new actions to the scheduler (internal)'
    # A Scheduler send me actions to do
    def push_actions(self, actions, sched_id):
        self.app.add_actions(actions, int(sched_id))
    push_actions.method = 'post'
    push_actions.doc = doc

    doc = 'Get the returns of the actions (internal)'
    # A scheduler ask us the action return value
    def get_returns(self, sched_id):
        #print "A scheduler ask me the returns", sched_id
        ret = self.app.get_return_for_passive(int(sched_id))
        #print "Send mack", len(ret), "returns"
        return cPickle.dumps(ret)
    get_returns.doc = doc


class IBroks(Interface):
    """Interface for Brokers
    They connect here and get all broks (data for brokers)
    data must be ORDERED! (initial status BEFORE update...)

    """

    doc = 'Get broks from the daemon'
    # poller or reactionner ask us actions
    def get_broks(self, bname):
        res = self.app.get_broks()
        return base64.b64encode(zlib.compress(cPickle.dumps(res), 2))
    get_broks.doc = doc


class IStats(Interface):
    """ 
    Interface for various stats about poller/reactionner activity
    """

    doc = 'Get raw stats from the daemon'
    def get_raw_stats(self):
        app = self.app
        res = {}
        
        for sched_id in app.schedulers:
            sched = app.schedulers[sched_id]
            lst = []
            res[sched_id] = lst
            for mod in app.q_by_mod:
                # In workers we've got actions send to queue - queue size
                for (i, q) in app.q_by_mod[mod].items():
                    lst.append( {'scheduler_name' : sched['name'],
                                 'module' : mod,
                                 'queue_number' : i,
                                 'queue_size' :q.qsize(),
                                 'return_queue_len' : app.get_returns_queue_len() } )
        return res
    get_raw_stats.doc = doc




class BaseSatellite(Daemon):
    """Please Add a Docstring to describe the class here"""

    def __init__(self, name, config_file, is_daemon, do_replace, debug, debug_file):
        super(BaseSatellite, self).__init__(name, config_file, is_daemon, \
                                                do_replace, debug, debug_file)
        # Ours schedulers
        self.schedulers = {}

        # Now we create the interfaces
        self.interface = IForArbiter(self)
        self.istats = IStats(self)

        # Can have a queue of external_commands given by modules
        # will be taken by arbiter to process
        self.external_commands = []
        self.external_commands_lock = threading.RLock()


    # The arbiter can resent us new conf in the pyro_daemon port.
    # We do not want to loose time about it, so it's not a blocking
    # wait, timeout = 0s
    # If it send us a new conf, we reinit the connections of all schedulers
    def watch_for_new_conf(self, timeout):
        self.handleRequests(timeout)


    def do_stop(self):
        if self.http_daemon and self.interface:
            logger.info("[%s] Stopping all network connections" % self.name)
            self.http_daemon.unregister(self.interface)
        super(BaseSatellite, self).do_stop()


    # Give the arbiter the data about what I manage
    # for me it's the ids of my schedulers
    def what_i_managed(self):
        r = {}
        for (k, v) in self.schedulers.iteritems():
            r[k] = v['push_flavor']
        return r


    # Call by arbiter to get our external commands
    def get_external_commands(self):
        res = self.external_commands
        self.external_commands = []
        return res




class Satellite(BaseSatellite):
    """Our main APP class"""

    def __init__(self, name, config_file, is_daemon, do_replace, debug, debug_file):

        super(Satellite, self).__init__(name, config_file, is_daemon, do_replace,
                                        debug, debug_file)

        # Keep broks so they can be eaten by a broker
        self.broks = {}

        self.workers = {}   # dict of active workers

        # Init stats like Load for workers
        self.wait_ratio = Load(initial_value=1)

        self.brok_interface = IBroks(self)
        self.scheduler_interface = ISchedulers(self)

        # Just for having these attributes defined here. explicit > implicit ;)
        self.uri2 = None
        self.uri3 = None
        self.s = None

        self.returns_queue = None
        self.q_by_mod = {}


    
    # Initialize or re-initialize connection with scheduler """
    def pynag_con_init(self, id):
        sched = self.schedulers[id]

        # If sched is not active, I do not try to init
        # it is just useless
        if not sched['active']:
            return

        sname = sched['name']
        uri = sched['uri']
        running_id = sched['running_id']
        logger.info("[%s] Init connection with %s at %s" % (self.name, sname, uri))

        try:
            sch_con = sched['con'] = HTTPClient(uri=uri, strong_ssl=sched['hard_ssl_name_check'])
        except HTTPExceptions, exp:
            logger.warning("[%s] Scheduler %s is not initialized or has network problem: %s" % (self.name, sname, str(exp)))
            sched['con'] = None
            return

        # timeout of 120 s
        # and get the running id
        try:
            new_run_id = sch_con.get('get_running_id')
            new_run_id = float(new_run_id)
        except (HTTPExceptions, cPickle.PicklingError, KeyError), exp:
            logger.warning("[%s] Scheduler %s is not initialized or has network problem: %s" % (self.name, sname, str(exp)))
            sched['con'] = None
            return

        # The schedulers have been restarted: it has a new run_id.
        # So we clear all verifs, they are obsolete now.
        if sched['running_id'] != 0 and new_run_id != running_id:
            logger.info("[%s] The running id of the scheduler %s changed, we must clear its actions"
                        % (self.name, sname))
            sched['wait_homerun'].clear()
        sched['running_id'] = new_run_id
        logger.info("[%s] Connection OK with scheduler %s" % (self.name, sname))


    # Manage action returned from Workers
    # We just put them into the corresponding sched
    # and we clean unused properties like sched_id
    def manage_action_return(self, action):
       # Maybe our workers end us something else than an action
       # if so, just add this in other queues and return
        cls_type = action.__class__.my_type
        if cls_type not in ['check', 'notification', 'eventhandler']:
            self.add(action)
            return

        # Ok, it's a result. We get it, and fill verifs of the good sched_id
        sched_id = action.sched_id

        # Now we now where to put action, we do not need sched_id anymore
        del action.sched_id

        # Unset the tag of the worker_id too
        try:
            del action.worker_id
        except AttributeError:
            pass

        # And we remove it from the actions queue of the scheduler too
        try:
            del self.schedulers[sched_id]['actions'][action.get_id()]
        except KeyError:
            pass
        # We tag it as "return wanted", and move it in the wait return queue
        # Stop, if it is "timeout" we need this information later
        # in the scheduler
        #action.status = 'waitforhomerun'
        try:
            self.schedulers[sched_id]['wait_homerun'][action.get_id()] = action
        except KeyError:
            pass

    # Return the chk to scheduler and clean them
    # REF: doc/shinken-action-queues.png (6)
    def manage_returns(self):
        #return
        # For all schedulers, we check for waitforhomerun
        # and we send back results
        for sched_id in self.schedulers:
            sched = self.schedulers[sched_id]
            # If sched is not active, I do not try return
            if not sched['active']:
                continue
            # Now ret have all verifs, we can return them
            send_ok = False
            ret = sched['wait_homerun'].values()
            if ret is not []:
                try:
                    con = sched['con']
                    if con is not None:  # None = not initialized
                        send_ok = con.post('put_results', {'results':ret})

                # Not connected or sched is gone
                except (HTTPExceptions, KeyError), exp:
                    logger.error('manage_returns exception:: %s,%s ' % (type(exp), str(exp)))
                    self.pynag_con_init(sched_id)
                    return
                except AttributeError, exp:  # the scheduler must  not be initialized
                    logger.error('manage_returns exception:: %s,%s ' % (type(exp), str(exp)))
                except Exception, exp:
                    logger.error("A satellite raised an unknown exception: %s (%s)" % (exp, type(exp)))
                    raise

            # We clean ONLY if the send is OK
            if send_ok:
                sched['wait_homerun'].clear()
            else:
                self.pynag_con_init(sched_id)
                logger.warning("Sent failed!")

    # Get all returning actions for a call from a
    # scheduler
    def get_return_for_passive(self, sched_id):
        # I do not know this scheduler?
        if sched_id not in self.schedulers:
            logger.debug("I do not know this scheduler: %s" % sched_id)
            return []

        sched = self.schedulers[sched_id]
        logger.debug("Preparing to return %s" % str(sched['wait_homerun'].values()))

        # prepare our return
        ret = copy.copy(sched['wait_homerun'].values())

        # and clear our dict
        sched['wait_homerun'].clear()

        return ret

    # Create and launch a new worker, and put it into self.workers
    # It can be mortal or not
    def create_and_launch_worker(self, module_name='fork', mortal=True):
        # create the input queue of this worker
        try:
            if is_android:
                q = Queue()
            else:
                q = self.manager.Queue()
        # If we got no /dev/shm on linux, we can got problem here.
        # Must raise with a good message
        except OSError, exp:
            # We look for the "Function not implemented" under Linux
            if exp.errno == 38 and os.name == 'posix':
                logger.critical("Got an exception (%s). If you are under Linux, "
                                "please check that your /dev/shm directory exists and"
                                " is read-write." % (str(exp)))
            raise

        # If we are in the fork module, we do not specify a target
        target = None
        if module_name == 'fork':
            target = None
        else:
            for module in self.modules_manager.instances:
                if module.properties['type'] == module_name:
                    # First, see if the module is a 'worker' one or not
                    if not module.properties.get('worker_capable', False):
                        raise NotWorkerMod
                    target = module.work
            if target is None:
                return
        # We want to give to the Worker the name of the daemon (poller or reactionner)
        cls_name = self.__class__.__name__.lower()
        w = Worker(1, q, self.returns_queue, self.processes_by_worker, \
                   mortal=mortal, max_plugins_output_length=self.max_plugins_output_length, target=target, loaded_into=cls_name, http_daemon=self.http_daemon)
        w.module_name = module_name
        # save this worker
        self.workers[w.id] = w

        # And save the Queue of this worker, with key = worker id
        self.q_by_mod[module_name][w.id] = q
        logger.info("[%s] Allocating new %s Worker: %s" % (self.name, module_name, w.id))

        # Ok, all is good. Start it!
        w.start()

    # The main stop of this daemon. Stop all workers
    # modules and sockets
    def do_stop(self):
        logger.info("[%s] Stopping all workers" % (self.name))
        for w in self.workers.values():
            try:
                w.terminate()
                w.join(timeout=1)
            # A already dead worker or in a worker
            except (AttributeError, AssertionError):
                pass
        # Close the pyro server socket if it was opened
        if self.http_daemon:
            if self.brok_interface:
                self.http_daemon.unregister(self.brok_interface)
            if self.scheduler_interface:
                self.http_daemon.unregister(self.scheduler_interface)
        # And then call our master stop from satellite code
        super(Satellite, self).do_stop()



    # A simple function to add objects in self
    # like broks in self.broks, etc
    # TODO: better tag ID?
    def add(self, elt):
        cls_type = elt.__class__.my_type
        if cls_type == 'brok':
            # For brok, we TAG brok with our instance_id
            elt.instance_id = 0
            self.broks[elt.id] = elt
            return
        elif cls_type == 'externalcommand':
            logger.debug("Enqueuing an external command '%s'" % str(elt.__dict__))
            with self.external_commands_lock:
                self.external_commands.append(elt)


    # Someone ask us our broks. We send them, and clean the queue
    def get_broks(self):
        res = copy.copy(self.broks)
        self.broks.clear()
        return res


    # workers are processes, they can die in a numerous of ways
    # like:
    # *99.99%: bug in code, sorry:p
    # *0.005 %: a mix between a stupid admin (or an admin without coffee),
    # and a kill command
    # *0.005%: alien attack
    # So they need to be detected, and restart if need
    def check_and_del_zombie_workers(self):
        # In android, we are using threads, so there is not active_children call
        if not is_android:
            # Active children make a join with everyone, useful :)
            active_children()

        w_to_del = []
        for w in self.workers.values():
            # If a worker goes down and we did not ask him, it's not
            # good: we can think that we have a worker and it's not True
            # So we del it
            if not w.is_alive():
                logger.warning("[%s] The worker %s goes down unexpectedly!" % (self.name, w.id))
                # Terminate immediately
                w.terminate()
                w.join(timeout=1)
                w_to_del.append(w.id)

        # OK, now really del workers from queues
        # And requeue the actions it was managed
        for id in w_to_del:
            w = self.workers[id]

            # Del the queue of the module queue
            del self.q_by_mod[w.module_name][w.id]

            for sched_id in self.schedulers:
                sched = self.schedulers[sched_id]
                for a in sched['actions'].values():
                    if a.status == 'queue' and a.worker_id == id:
                        # Got a check that will NEVER return if we do not
                        # restart it
                        self.assign_to_a_queue(a)

            # So now we can really forgot it
            del self.workers[id]


    # Here we create new workers if the queue load (len of verifs) is too long
    def adjust_worker_number_by_load(self):
        to_del = []
        logger.debug("[%s] Trying to adjust worker number."
                     " Actual number : %d, min per module : %d, max per module : %d"
                     % (self.name, len(self.workers), self.min_workers, self.max_workers))

        # I want at least min_workers by module then if I can, I add worker for load balancing
        for mod in self.q_by_mod:
            #At least min_workers
            while len(self.q_by_mod[mod]) < self.min_workers:
                try:
                    self.create_and_launch_worker(module_name=mod)
                # Maybe this modules is not a true worker one.
                # if so, just delete if from q_by_mod
                except NotWorkerMod:
                    to_del.append(mod)
                    break
            """
            # Try to really adjust load if necessary
            if self.get_max_q_len(mod) > self.max_q_size:
                if len(self.q_by_mod[mod]) >= self.max_workers:
                    logger.info("Cannot add a new %s worker, even if load is high. "
                                "Consider changing your max_worker parameter") % mod
                else:
                    try:
                        self.create_and_launch_worker(module_name=mod)
                    # Maybe this modules is not a true worker one.
                    # if so, just delete if from q_by_mod
                    except NotWorkerMod:
                        to_del.append(mod)
            """

        for mod in to_del:
            logger.debug("[%s] The module %s is not a worker one, "
                         "I remove it from the worker list" % (self.name, mod))
            del self.q_by_mod[mod]
        # TODO: if len(workers) > 2*wish, maybe we can kill a worker?

    # Get the Queue() from an action by looking at which module
    # it wants with a round robin way to scale the load between
    # workers
    def _got_queue_from_action(self, a):
        # get the module name, if not, take fork
        mod = getattr(a, 'module_type', 'fork')
        queues = self.q_by_mod[mod].items()

        # Maybe there is no more queue, it's very bad!
        if len(queues) == 0:
            return (0, None)

        # if not get a round robin index to get a queue based
        # on the action id
        rr_idx = a.id % len(queues)
        (i, q) = queues[rr_idx]

        # return the id of the worker (i), and its queue
        return (i, q)


    # Add a list of actions to our queues
    def add_actions(self, lst, sched_id):
        for a in lst:
            # First we look if we do not already have it, if so
            # do nothing, we are already working!
            if a.id in self.schedulers[sched_id]['actions']:
                continue
            a.sched_id = sched_id
            a.status = 'queue'
            self.assign_to_a_queue(a)


    # Take an action and put it into one queue
    def assign_to_a_queue(self, a):
        msg = Message(id=0, type='Do', data=a)
        (i, q) = self._got_queue_from_action(a)
        # Tag the action as "in the worker i"
        a.worker_id = i
        if q is not None:
            q.put(msg)


    # We get new actions from schedulers, we create a Message and we
    # put it in the s queue (from master to slave)
    # REF: doc/shinken-action-queues.png (1)
    def get_new_actions(self):
        #now = time.time()  #Unused

        # Here are the differences between a
        # poller and a reactionner:
        # Poller will only do checks,
        # reactionner do actions (notif + event handlers)
        do_checks = self.__class__.do_checks
        do_actions = self.__class__.do_actions

        # We check for new check in each schedulers and put the result in new_checks
        for sched_id in self.schedulers:
            sched = self.schedulers[sched_id]
            # If sched is not active, I do not try return
            if not sched['active']:
                continue

            try:
                try:
                    con = sched['con']
                except KeyError:
                    con = None
                if con is not None:  # None = not initialized
                    #pyro.set_timeout(con, 120)
                    # OK, go for it :)
                    # Before ask a call that can be long, do a simple ping to be sure it is alive
                    con.get('ping')
                    tmp = con.get('get_checks', {'do_checks':do_checks, 'do_actions':do_actions,
                                                          'poller_tags':self.poller_tags,
                                                          'reactionner_tags':self.reactionner_tags,
                                                          'worker_name':self.name,
                                                          'module_types':self.q_by_mod.keys()}, wait='long')
                    # Explicit pickle load
                    tmp = base64.b64decode(tmp)
                    tmp = zlib.decompress(tmp)
                    tmp = cPickle.loads(str(tmp))
                    logger.debug("Ask actions to %d, got %d" % (sched_id, len(tmp)))
                    # We 'tag' them with sched_id and put into queue for workers
                    # REF: doc/shinken-action-queues.png (2)
                    self.add_actions(tmp, sched_id)
                else:  # no con? make the connection
                    self.pynag_con_init(sched_id)
            # Ok, con is unknown, so we create it
            # Or maybe is the connection lost, we recreate it
            except (HTTPExceptions, KeyError), exp:
                logger.debug('get_new_actions exception:: %s,%s ' % (type(exp), str(exp)))
                self.pynag_con_init(sched_id)
            # scheduler must not be initialized
            # or scheduler must not have checks
            except AttributeError, exp:
                logger.debug('get_new_actions exception:: %s,%s ' % (type(exp), str(exp)))
            # What the F**k? We do not know what happened,
            # log the error message if possible.
            except Exception, exp:
                logger.error("A satellite raised an unknown exception: %s (%s)" % (exp, type(exp)))
                raise

    # In android we got a Queue, and a manager list for others
    def get_returns_queue_len(self):
        return self.returns_queue.qsize()

    # In android we got a Queue, and a manager list for others
    def get_returns_queue_item(self):
        return self.returns_queue.get()

    # Get 'objects' from external modules
    # from now nobody use it, but it can be useful
    # for a module like livestatus to raise external
    # commands for example
    def get_objects_from_from_queues(self):
        for f in self.modules_manager.get_external_from_queues():
            full_queue = True
            while full_queue:
                try:
                    o = f.get(block=False)
                    self.add(o)
                except Empty:
                    full_queue = False

    # An arbiter ask us to wait a new conf, so we must clean
    # all the mess we did, and close modules too
    def clean_previous_run(self):
        # Clean all lists
        self.schedulers.clear()
        self.broks.clear()
        with self.external_commands_lock:
            self.external_commands = self.external_commands[:]


    def do_loop_turn(self):
        logger.debug("Loop turn")
        # Maybe the arbiter ask us to wait for a new conf
        # If true, we must restart all...
        if self.cur_conf is None:
            # Clean previous run from useless objects
            # and close modules
            self.clean_previous_run()

            self.wait_for_initial_conf()
            # we may have been interrupted or so; then
            # just return from this loop turn
            if not self.new_conf:
                return
            self.setup_new_conf()

        # Now we check if arbiter speak to us in the pyro_daemon.
        # If so, we listen to it
        # When it push a conf, we reinit connections
        # Sleep in waiting a new conf :)
        # TODO: manage the diff again.
        while self.timeout > 0:
            begin = time.time()
            self.watch_for_new_conf(self.timeout)
            end = time.time()
            if self.new_conf:
                self.setup_new_conf()
            self.timeout = self.timeout - (end - begin)

        logger.debug(" ======================== ")

        self.timeout = self.polling_interval

        # Check if zombies workers are among us :)
        # If so: KILL THEM ALL!!!
        self.check_and_del_zombie_workers()

        # But also modules
        self.check_and_del_zombie_modules()

        # Print stats for debug
        for sched_id in self.schedulers:
            sched = self.schedulers[sched_id]
            for mod in self.q_by_mod:
                # In workers we've got actions send to queue - queue size
                for (i, q) in self.q_by_mod[mod].items():
                    logger.debug("[%d][%s][%s] Stats: Workers:%d (Queued:%d TotalReturnWait:%d)" %
                                (sched_id, sched['name'], mod,
                                 i, q.qsize(), self.get_returns_queue_len()))

        # Before return or get new actions, see how we manage
        # old ones: are they still in queue (s)? If True, we
        # must wait more or at least have more workers
        wait_ratio = self.wait_ratio.get_load()
        total_q = 0
        for mod in self.q_by_mod:
            for q in self.q_by_mod[mod].values():
                total_q += q.qsize()
        if total_q != 0 and wait_ratio < 2 * self.polling_interval:
            logger.debug("I decide to up wait ratio")
            self.wait_ratio.update_load(wait_ratio * 2)
            #self.wait_ratio.update_load(self.polling_interval)
        else:
            # Go to self.polling_interval on normal run, if wait_ratio
            # was >2*self.polling_interval,
            # it make it come near 2 because if < 2, go up :)
            self.wait_ratio.update_load(self.polling_interval)
        wait_ratio = self.wait_ratio.get_load()
        logger.debug("Wait ratio: %f" % wait_ratio)

        # We can wait more than 1s if needed,
        # no more than 5s, but no less than 1
        timeout = self.timeout * wait_ratio
        timeout = max(self.polling_interval, timeout)
        self.timeout = min(5 * self.polling_interval, timeout)

        # Maybe we do not have enough workers, we check for it
        # and launch the new ones if needed
        self.adjust_worker_number_by_load()

        # Manage all messages we've got in the last timeout
        # for queue in self.return_messages:
        while self.get_returns_queue_len() != 0:
            self.manage_action_return(self.get_returns_queue_item())

        # If we are passive, we do not initiate the check getting
        # and return
        if not self.passive:
            # Now we can get new actions from schedulers
            self.get_new_actions()

            # We send all finished checks
            # REF: doc/shinken-action-queues.png (6)
            self.manage_returns()

        # Get objects from our modules that are not worker based
        self.get_objects_from_from_queues()

        # Say to modules it's a new tick :)
        self.hook_point('tick')

    # Do this satellite (poller or reactionner) post "daemonize" init:
    # we must register our interfaces for 3 possible callers: arbiter,
    # schedulers or brokers.
    def do_post_daemon_init(self):

        # And we register them
        self.uri2 = self.http_daemon.register(self.interface)#, "ForArbiter")
        self.uri3 = self.http_daemon.register(self.brok_interface)#, "Broks")
        self.uri4 = self.http_daemon.register(self.scheduler_interface)#, "Schedulers")
        self.uri5 = self.http_daemon.register(self.istats)

        # self.s = Queue() # Global Master -> Slave
        # We can open the Queue for fork AFTER
        self.q_by_mod['fork'] = {}

        # Under Android, we do not have multiprocessing lib
        # so use standard Queue threads things
        # but in multiprocess, we are also using a Queue(). It's just
        # not the same
        if is_android:
            self.returns_queue = Queue()
        else:
            self.returns_queue = self.manager.Queue()

        # For multiprocess things, we should not have
        # socket timeouts.
        import socket
        socket.setdefaulttimeout(None)


    # Setup the new received conf from arbiter
    def setup_new_conf(self):
        conf = self.new_conf
        logger.debug("[%s] Sending us a configuration %s" % (self.name, conf))
        self.new_conf = None
        self.cur_conf = conf
        g_conf = conf['global']

        # Got our name from the globals
        if 'poller_name' in g_conf:
            name = g_conf['poller_name']
        elif 'reactionner_name' in g_conf:
            name = g_conf['reactionner_name']
        else:
            name = 'Unnamed satellite'
        self.name = name

        self.passive = g_conf['passive']
        if self.passive:
            logger.info("[%s] Passive mode enabled." % self.name)

        # If we've got something in the schedulers, we do not want it anymore
        for sched_id in conf['schedulers']:

            already_got = False

            # We can already got this conf id, but with another address
            if sched_id in self.schedulers:
                new_addr = conf['schedulers'][sched_id]['address']
                old_addr = self.schedulers[sched_id]['address']
                new_port = conf['schedulers'][sched_id]['port']
                old_port = self.schedulers[sched_id]['port']

                # Should got all the same to be ok :)
                if new_addr == old_addr and new_port == old_port:
                    already_got = True

            if already_got:
                logger.info("[%s] We already got the conf %d (%s)"
                            % (self.name, sched_id, conf['schedulers'][sched_id]['name']))
                wait_homerun = self.schedulers[sched_id]['wait_homerun']
                actions = self.schedulers[sched_id]['actions']

            s = conf['schedulers'][sched_id]
            self.schedulers[sched_id] = s

            if s['name'] in g_conf['satellitemap']:
                s.update(g_conf['satellitemap'][s['name']])
            proto = 'http'
            if s['use_ssl']:
                proto = 'https'
            uri = '%s://%s:%s/' % (proto, s['address'], s['port'])

            self.schedulers[sched_id]['uri'] = uri
            if already_got:
                self.schedulers[sched_id]['wait_homerun'] = wait_homerun
                self.schedulers[sched_id]['actions'] = actions
            else:
                self.schedulers[sched_id]['wait_homerun'] = {}
                self.schedulers[sched_id]['actions'] = {}
            self.schedulers[sched_id]['running_id'] = 0
            self.schedulers[sched_id]['active'] = s['active']

            # Do not connect if we are a passive satellite
            if not self.passive and not already_got:
                # And then we connect to it :)
                self.pynag_con_init(sched_id)

        # Now the limit part, 0 mean: number of cpu of this machine :)
        # if not available, use 4 (modern hardware)
        self.max_workers = g_conf['max_workers']
        if self.max_workers == 0 and not is_android:
            try:
                self.max_workers = cpu_count()
            except NotImplementedError:
                self.max_workers = 4
        logger.info("[%s] Using max workers: %s" % (self.name, self.max_workers))
        self.min_workers = g_conf['min_workers']
        if self.min_workers == 0 and not is_android:
            try:
                self.min_workers = cpu_count()
            except NotImplementedError:
                self.min_workers = 4
        logger.info("[%s] Using min workers: %s" % (self.name, self.min_workers))

        self.processes_by_worker = g_conf['processes_by_worker']
        self.polling_interval = g_conf['polling_interval']
        self.timeout = self.polling_interval

        # Now set tags
        # ['None'] is the default tags
        self.poller_tags = g_conf.get('poller_tags', ['None'])
        self.reactionner_tags = g_conf.get('reactionner_tags', ['None'])
        self.max_plugins_output_length = g_conf.get('max_plugins_output_length', 8192)

        # Set our giving timezone from arbiter
        use_timezone = g_conf['use_timezone']
        if use_timezone != 'NOTSET':
            logger.info("[%s] Setting our timezone to %s" % (self.name, use_timezone))
            os.environ['TZ'] = use_timezone
            time.tzset()

        logger.info("We have our schedulers: %s" % (str(self.schedulers)))

        # Now manage modules
        # TODO: check how to better handle this with modules_manager..
        mods = g_conf['modules']
        for module in mods:
            # If we already got it, bypass
            if not module.module_type in self.q_by_mod:
                logger.debug("Add module object %s" % str(module))
                self.modules_manager.modules.append(module)
                logger.info("[%s] Got module: %s " % (self.name, module.module_type))
                self.q_by_mod[module.module_type] = {}

    def main(self):
        try:
            for line in self.get_header():
                logger.info(line)

            self.load_config_file()

            # Look if we are enabled or not. If ok, start the daemon mode
            self.look_for_early_exit()
            self.do_daemon_init_and_start()

            self.do_post_daemon_init()

            self.load_modules_manager()

            # We wait for initial conf
            self.wait_for_initial_conf()
            if not self.new_conf:  # we must have either big problem or was requested to shutdown
                return
            self.setup_new_conf()

            # We can load our modules now
            self.modules_manager.set_modules(self.modules_manager.modules)
            self.do_load_modules()
            # And even start external ones
            self.modules_manager.start_external_instances()

            # Allocate Mortal Threads
            for _ in xrange(1, self.min_workers):
                to_del = []
                for mod in self.q_by_mod:
                    try:
                        self.create_and_launch_worker(module_name=mod)
                    # Maybe this modules is not a true worker one.
                    # if so, just delete if from q_by_mod
                    except NotWorkerMod:
                        to_del.append(mod)

                for mod in to_del:
                    logger.debug("The module %s is not a worker one, "
                                 "I remove it from the worker list" % mod)
                    del self.q_by_mod[mod]

            # Now main loop
            self.do_mainloop()
        except Exception:
            logger.critical("I got an unrecoverable error. I have to exit")
            logger.critical("You can log a bug ticket at "
                            "https://github.com/naparuba/shinken/issues/new to get help")
            logger.critical("Back trace of it: %s" % (traceback.format_exc()))
            raise

########NEW FILE########
__FILENAME__ = satellitelink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import socket

import json
import zlib
import cPickle

from shinken.util import get_obj_name_two_args_and_void
from shinken.objects.item import Item, Items
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp, DictProp, AddrProp
from shinken.log import logger
from shinken.http_client import HTTPClient, HTTPExceptions

        


class SatelliteLink(Item):
    """SatelliteLink is a common Class for link to satellite for
    Arbiter with Conf Dispatcher.

    """
    # id = 0 each Class will have it's own id

    properties = Item.properties.copy()
    properties.update({
        'address':         StringProp(fill_brok=['full_status']),
        'timeout':         IntegerProp(default='3', fill_brok=['full_status']),
        'data_timeout':    IntegerProp(default='120', fill_brok=['full_status']),
        'check_interval':  IntegerProp(default='60', fill_brok=['full_status']),
        'max_check_attempts': IntegerProp(default='3', fill_brok=['full_status']),
        'spare':              BoolProp(default='0', fill_brok=['full_status']),
        'manage_sub_realms':  BoolProp(default='1', fill_brok=['full_status']),
        'manage_arbiters':    BoolProp(default='0', fill_brok=['full_status'], to_send=True),
        'modules':            ListProp(default='', to_send=True),
        'polling_interval':   IntegerProp(default='1', fill_brok=['full_status'], to_send=True),
        'use_timezone':       StringProp(default='NOTSET', to_send=True),
        'realm':              StringProp(default='', fill_brok=['full_status'], brok_transformation=get_obj_name_two_args_and_void),
        'satellitemap':       DictProp(default=None, elts_prop=AddrProp, to_send=True, override=True),
        'use_ssl':            BoolProp(default='0', fill_brok=['full_status']),
        'hard_ssl_name_check':BoolProp(default='0', fill_brok=['full_status']),
    })

    running_properties = Item.running_properties.copy()
    running_properties.update({
        'con':                  StringProp(default=None),
        'alive':                StringProp(default=True, fill_brok=['full_status']),
        'broks':                StringProp(default=[]),
        'attempt':              StringProp(default=0, fill_brok=['full_status']), # the number of failed attempt
        'reachable':            StringProp(default=False, fill_brok=['full_status']), # can be network ask or not (dead or check in timeout or error)
        'last_check':           IntegerProp(default=0, fill_brok=['full_status']),
        'managed_confs':        StringProp(default={}),
    })
    
    def __init__(self, *args, **kwargs):
        super(SatelliteLink, self).__init__(*args, **kwargs)
        
        self.arb_satmap = {'address': '0.0.0.0', 'port': 0}
        if hasattr(self, 'address'):
            self.arb_satmap['address'] = self.address
        if hasattr(self, 'port'):
            try:
                self.arb_satmap['port'] = int(self.port)
            except:
                pass

    
    def set_arbiter_satellitemap(self, satellitemap):
        """
            arb_satmap is the satellitemap in current context:
                - A SatelliteLink is owned by an Arbiter
                - satellitemap attribute of SatelliteLink is the map defined IN THE satellite configuration
                  but for creating connections, we need the have the satellitemap of the Arbiter
        """
        self.arb_satmap = {'address': self.address, 'port': self.port, 'use_ssl':self.use_ssl, 'hard_ssl_name_check':self.hard_ssl_name_check}
        self.arb_satmap.update(satellitemap)


    def create_connection(self):
        self.con = HTTPClient(address=self.arb_satmap['address'], port=self.arb_satmap['port'],
                              timeout=self.timeout, data_timeout=self.data_timeout, use_ssl=self.use_ssl,
                              strong_ssl=self.hard_ssl_name_check
                              )
        self.uri = self.con.uri
        

    def put_conf(self, conf):
        if self.con is None:
            self.create_connection()

        # Maybe the connexion was not ok, bail out
        if not self.con:
            return False

        try:
            #pyro.set_timeout(self.con, self.data_timeout)
            self.con.get('ping')
            self.con.post('put_conf', {'conf':conf}, wait='long')
            #pyro.set_timeout(self.con, self.timeout)
            print "PUT CONF SUCESS", self.get_name()
            return True
        except HTTPExceptions, exp:
            self.con = None
            logger.error("Failed sending configuration for %s: %s" % (self.get_name(), str(exp)))
            return False
            

    # Get and clean all of our broks
    def get_all_broks(self):
        res = self.broks
        self.broks = []
        return res


    # Set alive, reachable, and reset attempts.
    # If we change state, raise a status brok update
    def set_alive(self):
        was_alive = self.alive
        self.alive = True
        self.attempt = 0
        self.reachable = True

        # We came from dead to alive
        # so we must add a brok update
        if not was_alive:
            b = self.get_update_status_brok()
            self.broks.append(b)

    def set_dead(self):
        was_alive = self.alive
        self.alive = False
        self.con = None

        # We are dead now. Must raise
        # a brok to say it
        if was_alive:
            logger.warning("Setting the satellite %s to a dead state." % self.get_name())
            b = self.get_update_status_brok()
            self.broks.append(b)

    # Go in reachable=False and add a failed attempt
    # if we reach the max, go dead
    def add_failed_check_attempt(self, reason=''):
        self.reachable = False
        self.attempt += 1
        self.attempt = min(self.attempt, self.max_check_attempts)
        # Don't need to warn again and again if the satellite is already dead
        if self.alive:
            logger.warning("Add failed attempt to %s (%d/%d) %s" % (self.get_name(), self.attempt, self.max_check_attempts, reason))

        # check when we just go HARD (dead)
        if self.attempt == self.max_check_attempts:
            self.set_dead()

    # Update satellite info each self.check_interval seconds
    # so we smooth arbiter actions for just useful actions
    # and not cry for a little timeout
    def update_infos(self):
        # First look if it's not too early to ping
        now = time.time()
        since_last_check = now - self.last_check
        if since_last_check < self.check_interval:
            return

        self.last_check = now

        # We ping and update the managed list
        self.ping()
        self.update_managed_list()

        # Update the state of this element
        b = self.get_update_status_brok()
        self.broks.append(b)


    # The elements just got a new conf_id, we put it in our list
    # because maybe the satellite is too busy to answer now
    def known_conf_managed_push(self, cfg_id, push_flavor):
        self.managed_confs[cfg_id] = push_flavor


    def ping(self):
        logger.debug("Pinging %s" % self.get_name())
        try:
            if self.con is None:
                self.create_connection()
            logger.debug(" (%s)" % (self.uri))

            # If the connection failed to initialize, bail out
            if self.con is None:
                self.add_failed_check_attempt()
                return
            
            r = self.con.get('ping')

            # Should return us pong string
            if r == 'pong':
                self.set_alive()
            else:
                self.add_failed_check_attempt()
        except HTTPExceptions, exp:
            self.add_failed_check_attempt(reason=str(exp))


    def wait_new_conf(self):
        if self.con is None:
            self.create_connection()
        try:
            r = self.con.get('wait_new_conf')
            return True
        except HTTPExceptions, exp:
            self.con = None
            return False
        

    # To know if the satellite have a conf (magic_hash = None)
    # OR to know if the satellite have THIS conf (magic_hash != None)
    # Magic_hash is for arbiter check only
    def have_conf(self, magic_hash=None):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            return False

        try:
            if magic_hash is None:
                r = self.con.get('have_conf')
            else:
                r = self.con.get('have_conf', {'magic_hash':magic_hash})
            print "have_conf RAW CALL", r, type(r)
            if not isinstance(r, bool):
                return False
            return r
        except HTTPExceptions, exp:
            self.con = None
            return False


    # To know if a receiver got a conf or not
    def got_conf(self):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            return False

        try:
            r = self.con.get('got_conf')
            # Protect against bad return
            if not isinstance(r, bool):
                return False
            return r
        except HTTPExceptions, exp:
            self.con = None
            return False


    def remove_from_conf(self, sched_id):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            return

        try:
            self.con.get('remove_from_conf', {'sched_id':sched_id})
            return True
        except HTTPExceptions, exp:
            self.con = None
            return False


    def update_managed_list(self):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            self.managed_confs = {}
            return

        try:
            tab = self.con.get('what_i_managed')
            print "[%s]What i managed raw value is %s" % (self.get_name(), tab)

            # Protect against bad return
            if not isinstance(tab, dict):
                print "[%s]What i managed: Got exception: bad what_i_managed returns" % self.get_name(), tab
                self.con = None
                self.managed_confs = {}
                return

            # Ok protect against json that is chaning keys as string instead of int
            tab_cleaned = {}
            for (k,v) in tab.iteritems():
                try:
                    tab_cleaned[int(k)] = v
                except ValueError:
                    print "[%s]What i managed: Got exception: bad what_i_managed returns" % self.get_name(), tab
            # We can update our list now
            self.managed_confs = tab_cleaned
        except HTTPExceptions, exp:
            print "EXCEPTION INwhat_i_managed", str(exp)
            # A timeout is not a crime, put this case aside
            #TODO : fix the timeout part?
            self.con = None
            print "[%s]What i managed: Got exception: %s %s %s" % (self.get_name(), exp, type(exp), exp.__dict__)
            self.managed_confs = {}


    # Return True if the satellite said to managed a configuration
    def do_i_manage(self, cfg_id, push_flavor):
        # If not even the cfg_id in the managed_conf, bail out
        if not cfg_id in self.managed_confs:
            return False
        # maybe it's in but with a false push_flavor. check it :)
        return self.managed_confs[cfg_id] == push_flavor


    def push_broks(self, broks):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            return False

        try:
            # Always do a simple ping to avoid a LOOOONG lock
            self.con.get('ping')
            self.con.post('push_broks', {'broks':broks}, wait='long')
            return True
        except HTTPExceptions, exp:
            self.con = None
            return False
            

    def get_external_commands(self):
        if self.con is None:
            self.create_connection()

        # If the connection failed to initialize, bail out
        if self.con is None:
            return []

        try:
            self.con.get('ping')
            tab = self.con.get('get_external_commands', wait='long')
            tab = cPickle.loads(str(tab))
            # Protect against bad return
            if not isinstance(tab, list):
                self.con = None
                return []
            return tab
        except HTTPExceptions, exp:
            self.con = None
            return []
        except AttributeError:
            self.con = None
            return []


    def prepare_for_conf(self):
        self.cfg = {'global': {}, 'schedulers': {}, 'arbiters': {}}
        properties = self.__class__.properties
        for prop, entry in properties.items():
            if entry.to_send:
                self.cfg['global'][prop] = getattr(self, prop)


    # Some parameters for satellites are not defined in the satellites conf
    # but in the global configuration. We can pass them in the global
    # property
    def add_global_conf_parameters(self, params):
        for prop in params:
            self.cfg['global'][prop] = params[prop]


    def get_my_type(self):
        return self.__class__.my_type


    # Here for poller and reactionner. Scheduler have its own function
    def give_satellite_cfg(self):
        return {'port': self.port,
                'address': self.address,
                'use_ssl':self.use_ssl,
                'hard_ssl_name_check':self.hard_ssl_name_check,
                'name': self.get_name(),
                'instance_id': self.id,
                'active': True,
                'passive': self.passive,
                'poller_tags': getattr(self, 'poller_tags', []),
                'reactionner_tags': getattr(self, 'reactionner_tags', [])}


    # Call by pickle for dataify the downtime
    # because we DO NOT WANT REF in this pickleisation!
    def __getstate__(self):
        cls = self.__class__
        # id is not in *_properties
        res = {'id': self.id}
        for prop in cls.properties:
            if prop != 'realm':
                if hasattr(self, prop):
                    res[prop] = getattr(self, prop)
        for prop in cls.running_properties:
            if prop != 'con':
                if hasattr(self, prop):
                    res[prop] = getattr(self, prop)
        return res

    # Inverted function of getstate
    def __setstate__(self, state):
        cls = self.__class__

        self.id = state['id']
        for prop in cls.properties:
            if prop in state:
                setattr(self, prop, state[prop])
        for prop in cls.running_properties:
            if prop in state:
                setattr(self, prop, state[prop])
        # con needs to be explicitly set:
        self.con = None


class SatelliteLinks(Items):
    """Please Add a Docstring to describe the class here"""

    # name_property = "name"
    # inner_class = SchedulerLink

    # We must have a realm property, so we find our realm
    def linkify(self, realms, modules):
        self.linkify_s_by_p(realms)
        self.linkify_s_by_plug(modules)

    def linkify_s_by_p(self, realms):
        for s in self:
            p_name = s.realm.strip()
            # If no realm name, take the default one
            if p_name == '':
                p = realms.get_default()
                s.realm = p
            else:  # find the realm one
                p = realms.find_by_name(p_name)
                s.realm = p
            # Check if what we get is OK or not
            if p is not None:
                s.register_to_my_realm()
            else:
                err = "The %s %s got a unknown realm '%s'" % (s.__class__.my_type, s.get_name(), p_name)
                s.configuration_errors.append(err)


########NEW FILE########
__FILENAME__ = scheduler
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import os
import cStringIO
import sys
import tempfile
import traceback
import json
import cPickle
import zlib
from Queue import Empty

from shinken.external_command import ExternalCommand
from shinken.check import Check
from shinken.notification import Notification
from shinken.eventhandler import EventHandler
from shinken.brok import Brok
from shinken.downtime import Downtime
from shinken.contactdowntime import ContactDowntime
from shinken.comment import Comment
from shinken.acknowledge import Acknowledge
from shinken.log import logger
from shinken.util import nighty_five_percent
from shinken.load import Load
from shinken.http_client import HTTPClient, HTTPExceptions


class Scheduler:
    """Please Add a Docstring to describe the class here"""

    def __init__(self, scheduler_daemon):
        self.sched_daemon = scheduler_daemon
        # When set to false by us, we die and arbiter launch a new Scheduler
        self.must_run = True

        self.waiting_results = []  # satellites returns us results
        # and to not wait for them, we put them here and
        # use them later

        # Every N seconds we call functions like consume, del zombies
        # etc. All of theses functions are in recurrent_works with the
        # every tick to run. So must be an integer > 0
        # The order is important, so make key an int.
        # TODO: at load, change value by configuration one (like reaper time, etc)
        self.recurrent_works = {
            0: ('update_downtimes_and_comments', self.update_downtimes_and_comments, 1),
            1: ('schedule', self.schedule, 1), # just schedule
            2: ('consume_results', self.consume_results, 1), # incorporate checks and dependencies
            3: ('get_new_actions', self.get_new_actions, 1), # now get the news actions (checks, notif) raised
            4: ('get_new_broks', self.get_new_broks, 1), # and broks
            5: ('scatter_master_notifications', self.scatter_master_notifications, 1),
            6: ('delete_zombie_checks', self.delete_zombie_checks, 1),
            7: ('delete_zombie_actions', self.delete_zombie_actions, 1),
            # 3: (self.delete_unwanted_notifications, 1),
            8: ('check_freshness', self.check_freshness, 10),
            9: ('clean_caches', self.clean_caches, 1),
            10: ('update_retention_file', self.update_retention_file, 3600),
            11: ('check_orphaned', self.check_orphaned, 60),
            # For NagVis like tools: update our status every 10s
            12: ('get_and_register_update_program_status_brok', self.get_and_register_update_program_status_brok, 10),
            # Check for system time change. And AFTER get new checks
            # so they are changed too.
            13: ('check_for_system_time_change', self.sched_daemon.check_for_system_time_change, 1),
            # launch if need all internal checks
            14: ('manage_internal_checks', self.manage_internal_checks, 1),
            # clean some times possible overridden Queues, to do not explode in memory usage
            # every 1/4 of hour
            15: ('clean_queues', self.clean_queues, 1),
            # Look for new business_impact change by modulation every minute
            16: ('update_business_values', self.update_business_values, 60),
            # Reset the topology change flag if need
            17: ('reset_topology_change_flag', self.reset_topology_change_flag, 1),
            18: ('check_for_expire_acknowledge', self.check_for_expire_acknowledge, 1),
            19: ('send_broks_to_modules', self.send_broks_to_modules, 1),
            20: ('get_objects_from_from_queues', self.get_objects_from_from_queues, 1),
        }

        # stats part
        self.nb_checks_send = 0
        self.nb_actions_send = 0
        self.nb_broks_send = 0
        self.nb_check_received = 0

        # Log init
        logger.load_obj(self)

        self.instance_id = 0  # Temporary set. Will be erase later

        # Ours queues
        self.checks = {}
        self.actions = {}
        self.downtimes = {}
        self.contact_downtimes = {}
        self.comments = {}
        self.broks = {}

        # Some flags
        self.has_full_broks = False  # have a initial_broks in broks queue?
        self.need_dump_memory = False  # set by signal 1
        self.need_objects_dump = False #set by signal 2

        # And a dummy push flavor
        self.push_flavor = 0

        # Now fake initialize for our satellites
        self.brokers = {}
        self.pollers = {}
        self.reactionners = {}


    def reset(self):
        self.must_run = True
        del self.waiting_results[:]
        for o in self.checks, self.actions, self.downtimes, self.contact_downtimes, self.comments, self.broks, self.brokers:
            o.clear()



    # Load conf for future use
    # we are in_test if the data are from an arbiter object like,
    # so only for tests
    def load_conf(self, conf, in_test=False):
        self.program_start = int(time.time())
        self.conf = conf
        self.hostgroups = conf.hostgroups
        self.hostgroups.create_reversed_list()
        self.services = conf.services
        # We need reversed list for search in the retention
        # file read
        self.services.create_reversed_list()
        self.services.optimize_service_search(conf.hosts)
        self.hosts = conf.hosts
        self.hosts.create_reversed_list()

        self.notificationways = conf.notificationways
        self.checkmodulations = conf.checkmodulations
        self.macromodulations = conf.macromodulations
        self.contacts = conf.contacts
        self.contacts.create_reversed_list()
        self.contactgroups = conf.contactgroups
        self.contactgroups.create_reversed_list()
        self.servicegroups = conf.servicegroups
        self.servicegroups.create_reversed_list()
        self.timeperiods = conf.timeperiods
        self.timeperiods.create_reversed_list()
        self.commands = conf.commands
        self.commands.create_reversed_list()
        self.triggers = conf.triggers
        self.triggers.create_reversed_list()
        self.triggers.compile()
        self.triggers.load_objects(self)


        if not in_test:
            # Commands in the host/services/contacts are not real one
            # we must relink them
            t0 = time.time()
            self.conf.late_linkify()
            logger.debug("Late command relink in %d" % (time.time() - t0))

        # self.status_file = StatusFile(self)        # External status file
        self.instance_id = conf.instance_id  # From Arbiter. Use for
                                            # Broker to differentiate
                                            # schedulers
        # Tag our hosts with our instance_id
        for h in self.hosts:
            h.instance_id = conf.instance_id
        for s in self.services:
            s.instance_id = conf.instance_id
        # self for instance_name
        self.instance_name = conf.instance_name
        # and push flavor
        self.push_flavor = conf.push_flavor

        # Now we can update our 'ticks' for special calls
        # like the retention one, etc
        self.update_recurrent_works_tick('update_retention_file', self.conf.retention_update_interval * 60)
        self.update_recurrent_works_tick('clean_queues', self.conf.cleaning_queues_interval)


    # Update the 'tick' for a function call in our
    # recurrent work
    def update_recurrent_works_tick(self, f_name, new_tick):
        for i in self.recurrent_works:
            (name, f, old_tick) = self.recurrent_works[i]
            if name == f_name:
                logger.debug("Changing the tick to %d for the function %s" % (new_tick, name))
                self.recurrent_works[i] = (name, f, new_tick)


    # Load the pollers from our app master
    def load_satellites(self, pollers, reactionners):
        self.pollers = pollers
        self.reactionners = reactionners


    # Oh... Arbiter want us to die... To launch a new Scheduler
    # "Mais qu'a-t-il de plus que je n'ais pas?"
    # "But.. On which point it is better than me?"
    def die(self):
        self.must_run = False


    def dump_objects(self):
        d = tempfile.gettempdir()
        p = os.path.join(d, 'scheduler-obj-dump-%d' % time.time())
        logger.info('Opening the DUMP FILE %s' % (p))
        try:
            f = open(p, 'w')
            f.write('Scheduler DUMP at %d\n' % time.time())
            for c in self.checks.values():
                s = 'CHECK: %s:%s:%s:%s:%s:%s\n' % (c.id, c.status, c.t_to_go, c.poller_tag, c.command, c.worker)
                f.write(s)
            for a in self.actions.values():
                s = '%s: %s:%s:%s:%s:%s:%s\n' % (a.__class__.my_type.upper(), a.id, a.status, a.t_to_go, a.reactionner_tag, a.command, a.worker)
                f.write(s)
            for b in self.broks.values():
                s = 'BROK: %s:%s\n' % (b.id, b.type)
                f.write(s)
            f.close()
        except Exception, exp:
            logger.error("Error in writing the dump file %s : %s" % (p, str(exp)))


    # Load the external command
    def load_external_command(self, e):
        self.external_command = e


    # We've got activity in the fifo, we get and run commands
    def run_external_commands(self, cmds):
        for command in cmds:
            self.run_external_command(command)


    def run_external_command(self, command):
        logger.debug("scheduler resolves command '%s'" % command)
        ext_cmd = ExternalCommand(command)
        self.external_command.resolve_command(ext_cmd)


    # Add_Brok is a bit more complex than the others, because
    # on starting, the broks are put in a global queue : self.broks
    # then when the first broker connect, it will generate initial_broks
    # in it's own queue (so bname != None).
    # and when in "normal" run, we just need to put the brok to all queues
    def add_Brok(self, brok, bname=None):
        # For brok, we TAG brok with our instance_id
        brok.instance_id = self.instance_id
        # Maybe it's just for one broker
        if bname:
            broks = self.brokers[bname]['broks']
            broks[brok.id] = brok
        else:
            # If there are known brokers, give it to them
            if len(self.brokers) > 0:
                # Or maybe it's for all
                for bname in self.brokers:
                    broks = self.brokers[bname]['broks']
                    broks[brok.id] = brok
            else: # no brokers? maybe at startup for logs
                # we will put in global queue, that the first broker
                # connexion will get all
                self.broks[brok.id] = brok


    def add_Notification(self, notif):
        self.actions[notif.id] = notif
        # A notification ask for a brok
        if notif.contact is not None:
            b = notif.get_initial_status_brok()
            self.add(b)


    def add_Check(self, c):
        self.checks[c.id] = c
        # A new check means the host/service changes its next_check
        # need to be refreshed
        b = c.ref.get_next_schedule_brok()
        self.add(b)


    def add_EventHandler(self, action):
        # print "Add an event Handler", elt.id
        self.actions[action.id] = action


    def add_Downtime(self, dt):
        self.downtimes[dt.id] = dt
        if dt.extra_comment:
            self.add_Comment(dt.extra_comment)


    def add_ContactDowntime(self, contact_dt):
        self.contact_downtimes[contact_dt.id] = contact_dt


    def add_Comment(self, comment):
        self.comments[comment.id] = comment
        b = comment.ref.get_update_status_brok()
        self.add(b)


    # Ok one of our modules send us a command? just run it!
    def add_ExternalCommand(self, ext_cmd):
        self.external_command.resolve_command(ext_cmd)


    # Schedulers have some queues. We can simplify call by adding
    # elements into the proper queue just by looking at their type
    # Brok -> self.broks
    # Check -> self.checks
    # Notification -> self.actions
    # Downtime -> self.downtimes
    # ContactDowntime -> self.contact_downtimes
    def add(self, elt):
        f = self.__add_actions.get(elt.__class__, None)
        if f:
            #print("found action for %s: %s" % (elt.__class__.__name__, f.__name__))
            f(self, elt)

    __add_actions = {
        Check:              add_Check,
        Brok:               add_Brok,
        Notification:       add_Notification,
        EventHandler:       add_EventHandler,
        Downtime:           add_Downtime,
        ContactDowntime:    add_ContactDowntime,
        Comment:            add_Comment,
        ExternalCommand:    add_ExternalCommand,
    }

    # We call the function of modules that got the
    # hook function
    # TODO: find a way to merge this and the version in daemon.py
    def hook_point(self, hook_name):
        for inst in self.sched_daemon.modules_manager.instances:
            full_hook_name = 'hook_' + hook_name
            logger.debug("hook_point: %s: %s %s" % (inst.get_name(), str(hasattr(inst, full_hook_name)), hook_name))

            if hasattr(inst, full_hook_name):
                f = getattr(inst, full_hook_name)
                try:
                    f(self)
                except Exception, exp:
                    logger.error("The instance %s raise an exception %s. I disable it and set it to restart it later" % (inst.get_name(), str(exp)))
                    output = cStringIO.StringIO()
                    traceback.print_exc(file=output)
                    logger.error("Exception trace follows: %s" % (output.getvalue()))
                    output.close()
                    self.sched_daemon.modules_manager.set_to_restart(inst)

    # Ours queues may explode if no one ask us for elements
    # It's very dangerous: you can crash your server... and it's a bad thing :)
    # So we 'just' keep last elements: 5 of max is a good overhead
    def clean_queues(self):
        # if we set the interval at 0, we bail out
        if self.conf.cleaning_queues_interval == 0:
            return

        max_checks = 5 * (len(self.hosts) + len(self.services))
        max_broks = 5 * (len(self.hosts) + len(self.services))
        max_actions = 5 * len(self.contacts) * (len(self.hosts) + len(self.services))

        # For checks, it's not very simple:
        # For checks, they may be referred to their host/service
        # We do not just del them in the check list, but also in their service/host
        # We want id of lower than max_id - 2*max_checks
        if len(self.checks) > max_checks:
            id_max = self.checks.keys()[-1]  # The max id is the last id
                                            #: max is SO slow!
            to_del_checks = [c for c in self.checks.values() if c.id < id_max - max_checks]
            nb_checks_drops = len(to_del_checks)
            if nb_checks_drops > 0:
                logger.info("I have to del some checks (%d)..., sorry" % nb_checks_drops)
            for c in to_del_checks:
                i = c.id
                elt = c.ref
                # First remove the link in host/service
                elt.remove_in_progress_check(c)
                # Then in dependent checks (I depend on, or check
                # depend on me)
                for dependent_checks in c.depend_on_me:
                    dependent_checks.depend_on.remove(c.id)
                for c_temp in c.depend_on:
                    c_temp.depen_on_me.remove(c)
                del self.checks[i]  # Final Bye bye ...
        else:
            nb_checks_drops = 0

        # For broks and actions, it's more simple
        # or brosk, manage global but also all brokers queue
        b_lists = [self.broks]
        for (bname, e) in self.brokers.iteritems():
            b_lists.append(e['broks'])
        for broks in b_lists:
            if len(broks) > max_broks:
                id_max = broks.keys()[-1]
                id_to_del_broks = [i for i in broks if i < id_max - max_broks]
                nb_broks_drops = len(id_to_del_broks)
                for i in id_to_del_broks:
                    del broks[i]
            else:
                nb_broks_drops = 0

        if len(self.actions) > max_actions:
            id_max = self.actions.keys()[-1]
            id_to_del_actions = [i for i in self.actions if i < id_max - max_actions]
            nb_actions_drops = len(id_to_del_actions)
            for i in id_to_del_actions:
                # Remember to delete reference of notification in service/host
                a = self.actions[i]
                if a.is_a == 'notification':
                    a.ref.remove_in_progress_notification(a)
                del self.actions[i]
        else:
            nb_actions_drops = 0

        if nb_checks_drops != 0 or nb_broks_drops != 0 or nb_actions_drops != 0:
            logger.warning("We drop %d checks, %d broks and %d actions" % (nb_checks_drops, nb_broks_drops, nb_actions_drops))

    # For tunning purpose we use caches but we do not want them to explode
    # So we clean them
    def clean_caches(self):
        for tp in self.timeperiods:
            tp.clean_cache()

    # Ask item (host or service) an update_status
    # and add it to our broks queue
    def get_and_register_status_brok(self, item):
        b = item.get_update_status_brok()
        self.add(b)

    # Ask item (host or service) a check_result_brok
    # and add it to our broks queue
    def get_and_register_check_result_brok(self, item):
        b = item.get_check_result_brok()
        self.add(b)

    # We do not want this downtime id
    def del_downtime(self, dt_id):
        if dt_id in self.downtimes:
            self.downtimes[dt_id].ref.del_downtime(dt_id)
            del self.downtimes[dt_id]

    # We do not want this downtime id
    def del_contact_downtime(self, dt_id):
        if dt_id in self.contact_downtimes:
            self.contact_downtimes[dt_id].ref.del_downtime(dt_id)
            del self.contact_downtimes[dt_id]

    # We do not want this comment id
    def del_comment(self, c_id):
        if c_id in self.comments:
            self.comments[c_id].ref.del_comment(c_id)
            del self.comments[c_id]

    # We are looking for outdated acks, and if so, remove them
    def check_for_expire_acknowledge(self):
        for t in [self.hosts, self.services]:
            for i in t:
                i.check_for_expire_acknowledge()

    # We update all business_impact to look at new modulation
    # start for impacts, and so update broks status and
    # problems value too
    def update_business_values(self):
        for t in [self.hosts, self.services]:
            # We first update impacts and classic elements
            for i in [i for i in t if not i.is_problem]:
                was = i.business_impact
                i.update_business_impact_value()
                new = i.business_impact
                # Ok, the business_impact change, we can update the broks
                if new != was:
                    #print "The elements", i.get_name(), "change it's business_impact value"
                    self.get_and_register_status_brok(i)

        # When all impacts and classic elements are updated,
        # we can update problems (their value depend on impacts, so
        # they must be done after)
        for t in [self.hosts, self.services]:
            # We first update impacts and classic elements
            for i in [i for i in t if i.is_problem]:
                was = i.business_impact
                i.update_business_impact_value()
                new = i.business_impact
                # Maybe one of the impacts change it's business_impact to a high value
                # and so ask for the problem to raise too
                if new != was:
                    #print "The elements", i.get_name(), "change it's business_impact value from", was, "to", new
                    self.get_and_register_status_brok(i)


    # Each second we search for master notification that are scatterisable and we do the job
    # we take the sons and we put them into our actions queue
    def scatter_master_notifications(self):
            now = time.time()
            for a in self.actions.values():
                # We only want notifications
                if a.is_a != 'notification':
                    continue
                if a.status == 'scheduled' and a.is_launchable(now):
                    if not a.contact:
                        # This is a "master" notification created by create_notifications.
                        # It wont sent itself because it has no contact.
                        # We use it to create "child" notifications (for the contacts and
                        # notification_commands) which are executed in the reactionner.
                        item = a.ref
                        childnotifications = []
                        if not item.notification_is_blocked_by_item(a.type, now):
                            # If it is possible to send notifications of this type at the current time, then create
                            # a single notification for each contact of this item.
                            childnotifications = item.scatter_notification(a)
                            for c in childnotifications:
                                c.status = 'scheduled'
                                self.add(c)  # this will send a brok

                        # If we have notification_interval then schedule the next notification (problems only)
                        if a.type == 'PROBLEM':
                            # Update the ref notif number after raise the one of the notification
                            if len(childnotifications) != 0:
                                # notif_nb of the master notification was already current_notification_number+1.
                                # If notifications were sent, then host/service-counter will also be incremented
                                item.current_notification_number = a.notif_nb

                            if item.notification_interval != 0 and a.t_to_go is not None:
                                # We must continue to send notifications.
                                # Just leave it in the actions list and set it to "scheduled" and it will be found again later
                                # Ask the service/host to compute the next notif time. It can be just
                                # a.t_to_go + item.notification_interval * item.__class__.interval_length
                                # or maybe before because we have an escalation that need to raise up before
                                a.t_to_go = item.get_next_notification_time(a)

                                a.notif_nb = item.current_notification_number + 1
                                a.status = 'scheduled'
                            else:
                                # Wipe out this master notification. One problem notification is enough.
                                item.remove_in_progress_notification(a)
                                self.actions[a.id].status = 'zombie'

                        else:
                            # Wipe out this master notification. We don't repeat recover/downtime/flap/etc...
                            item.remove_in_progress_notification(a)
                            self.actions[a.id].status = 'zombie'


    # Called by poller to get checks
    # Can get checks and actions (notifications and co)
    def get_to_run_checks(self, do_checks=False, do_actions=False,
                          poller_tags=['None'], reactionner_tags=['None'], \
                              worker_name='none', module_types=['fork']
                          ):
        res = []
        now = time.time()

        # If poller want to do checks
        if do_checks:
            for c in self.checks.values():
                #  If the command is untagged, and the poller too, or if both are tagged
                #  with same name, go for it
                # if do_check, call for poller, and so poller_tags by default is ['None']
                # by default poller_tag is 'None' and poller_tags is ['None']
                # and same for module_type, the default is the 'fork' type
                if c.poller_tag in poller_tags and c.module_type in module_types:
                    # must be ok to launch, and not an internal one (business rules based)
                    if c.status == 'scheduled' and c.is_launchable(now) and not c.internal:
                        c.status = 'inpoller'
                        c.worker = worker_name
                        # We do not send c, because it is a link (c.ref) to
                        # host/service and poller do not need it. It only
                        # need a shell with id, command and defaults
                        # parameters. It's the goal of copy_shell
                        res.append(c.copy_shell())

        # If reactionner want to notify too
        if do_actions:
            for a in self.actions.values():
                is_master = (a.is_a == 'notification' and not a.contact)

                if not is_master:
                    # if do_action, call the reactionner, and so reactionner_tags by default is ['None']
                    # by default reactionner_tag is 'None' and reactionner_tags is ['None'] too
                    # So if not the good one, loop for next :)
                    if not a.reactionner_tag in reactionner_tags:
                        continue

                    # same for module_type
                    if not a.module_type in module_types:
                        continue

                # And now look for can launch or not :)
                if a.status == 'scheduled' and a.is_launchable(now):
                    a.status = 'inpoller'
                    a.worker = worker_name
                    if not is_master:
                        # This is for child notifications and eventhandlers
                        new_a = a.copy_shell()
                        res.append(new_a)
        return res
    
    
    # Called by poller and reactionner to send result
    def put_results(self, c):
        if c.is_a == 'notification':
            # We will only see childnotifications here
            try:
                timeout = False
                if c.status == 'timeout':
                    # Unfortunately the remove_in_progress_notification
                    # sets the status to zombie, so we need to save it here.
                    timeout = True
                    execution_time = c.execution_time

                # Add protection for strange charset
                if isinstance(c.output, str):
                    c.output = c.output.decode('utf8', 'ignore')

                self.actions[c.id].get_return_from(c)
                item = self.actions[c.id].ref
                item.remove_in_progress_notification(c)
                self.actions[c.id].status = 'zombie'
                item.last_notification = c.check_time

                # And we ask the item to update it's state
                self.get_and_register_status_brok(item)

                # If we' ve got a problem with the notification, raise a Warning log
                if timeout:
                    logger.warning("Contact %s %s notification command '%s ' timed out after %d seconds" %
                                   (self.actions[c.id].contact.contact_name,
                                    self.actions[c.id].ref.__class__.my_type,
                                    self.actions[c.id].command,
                                    int(execution_time)))
                elif c.exit_status != 0:
                    logger.warning("The notification command '%s' raised an error (exit code=%d): '%s'" % (c.command, c.exit_status, c.output))

            except KeyError, exp:  # bad number for notif, not that bad
                logger.warning('put_results:: get unknown notification : %s ' % str(exp))

            except AttributeError, exp:  # bad object, drop it
                logger.warning('put_results:: get bad notification : %s ' % str(exp))



        elif c.is_a == 'check':
            try:
                if c.status == 'timeout':
                    c.output = "(%s Check Timed Out)" % self.checks[c.id].ref.__class__.my_type.capitalize()
                    c.long_output = c.output
                    c.exit_status = self.conf.timeout_exit_status
                self.checks[c.id].get_return_from(c)
                self.checks[c.id].status = 'waitconsume'
            except KeyError, exp:
                pass


        elif c.is_a == 'eventhandler':
            # It just die
            try:
                if c.status == 'timeout':
                    logger.warning("%s event handler command '%s ' timed out after %d seconds" %
                                   (self.actions[c.id].ref.__class__.my_type.capitalize(),
                                    self.actions[c.id].command,
                                    int(c.execution_time)))
                self.actions[c.id].status = 'zombie'
            # Maybe we got a return of a old even handler, so we can forget it
            except KeyError, exp:
                logger.warning('put_results:: get unknown event handler : %s ' % str(exp))
                pass
        else:
            logger.error("The received result type in unknown! %s" % str(c.is_a))

    # Get the good tabs for links regarding to the kind. If unknown, return None
    def get_links_from_type(self, type):
        t = {'poller': self.pollers, 'reactionner': self.reactionners}
        if type in t:
            return t[type]
        return None

    # Check if we do not connect to often to this
    def is_connection_try_too_close(self, elt):
        now = time.time()
        last_connection = elt['last_connection']
        if now - last_connection < 5:
            return  True
        return False

    # initialize or re-initialize connection with a poller
    # or a reactionner
    def pynag_con_init(self, id, type='poller'):
        # Get good links tab for looping..
        links = self.get_links_from_type(type)
        if links is None:
            logger.debug("Unknown '%s' type for connection!" % type)
            return

        # We want only to initiate connections to the passive
        # pollers and reactionners
        passive = links[id]['passive']
        if not passive:
            return

        # If we try to connect too much, we slow down our tests
        if self.is_connection_try_too_close(links[id]):
            return

        # Ok, we can now update it
        links[id]['last_connection'] = time.time()

        logger.debug("Init connection with %s" % links[id]['uri'])

        uri = links[id]['uri']
        try:
            links[id]['con'] = HTTPClient(uri=uri, strong_ssl=links[id]['hard_ssl_name_check'])
            con = links[id]['con']
        except HTTPExceptions, exp:
            logger.warning("Connection problem to the %s %s: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            return

        try:
            # initial ping must be quick
            con.get('ping')
        except HTTPExceptions, exp:
            logger.warning("Connection problem to the %s %s: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            return
        except KeyError, exp:
            logger.warning("The %s '%s' is not initialized: %s" % (type, links[id]['name'], str(exp)))
            links[id]['con'] = None
            return

        logger.info("Connection OK to the %s %s" % (type, links[id]['name']))


    # We should push actions to our passives satellites
    def push_actions_to_passives_satellites(self):
        # We loop for our passive pollers or reactionners
        for p in filter(lambda p: p['passive'], self.pollers.values()):
            logger.debug("I will send actions to the poller %s" % str(p))
            con = p['con']
            poller_tags = p['poller_tags']
            if con is not None:
                # get actions
                lst = self.get_to_run_checks(True, False, poller_tags, worker_name=p['name'])
                try:
                    # initial ping must be quick
                    #pyro.set_timeout(con, 120)
                    logger.debug("Sending %s actions" % len(lst))
                    #con.push_actions(lst, self.instance_id)
                    con.post('push_actions', {'actions':lst, 'sched_id':self.instance_id})
                    self.nb_checks_send += len(lst)
                except HTTPExceptions, exp:
                    logger.warning("Connection problem to the %s %s: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                except KeyError, exp:
                    logger.warning("The %s '%s' is not initialized: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                # we come back to normal timeout
                #pyro.set_timeout(con, 5)
            else:  # no connection? try to reconnect
                self.pynag_con_init(p['instance_id'], type='poller')

        # TODO:factorize
        # We loop for our passive reactionners
        for p in filter(lambda p: p['passive'], self.reactionners.values()):
            logger.debug("I will send actions to the reactionner %s" % str(p))
            con = p['con']
            reactionner_tags = p['reactionner_tags']
            if con is not None:
            # get actions
                lst = self.get_to_run_checks(False, True, reactionner_tags=reactionner_tags, worker_name=p['name'])
                try:
                    # initial ping must be quick
                    #pyro.set_timeout(con, 120)
                    logger.debug("Sending %d actions" % len(lst))
                    #con.push_actions(lst, self.instance_id)
                    con.post('push_actions', {'actions':lst, 'sched_id':self.instance_id})
                    self.nb_checks_send += len(lst)
                except HTTPExceptions, exp:
                    logger.warning("Connection problem to the %s %s: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                except KeyError, exp:
                    logger.warning("The %s '%s' is not initialized: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                # we come back to normal timeout
                #pyro.set_timeout(con, 5)
            else:  # no connection? try to reconnect
                self.pynag_con_init(p['instance_id'], type='reactionner')


    # We should get returns from satellites
    def get_actions_from_passives_satellites(self):
        # We loop for our passive pollers
        for p in filter(lambda p: p['passive'], self.pollers.values()):
            logger.debug("I will get actions from the poller %s" % str(p))
            con = p['con']
            poller_tags = p['poller_tags']
            if con is not None:
                try:
                    # initial ping must be quick
                    #pyro.set_timeout(con, 120)
                    #results = con.get_returns(self.instance_id)
                    # Before ask a call that can be long, do a simple ping to be sure it is alive
                    con.get('ping')
                    results = con.get('get_returns', {'sched_id':self.instance_id}, wait='long')
                    results = cPickle.loads(str(results))
                    nb_received = len(results)
                    self.nb_check_received += nb_received
                    logger.debug("Received %d passive results" % nb_received)
                    for result in results:
                        result.set_type_passive()
                    self.waiting_results.extend(results)
                except HTTPExceptions, exp:
                    logger.warning("Connection problem to the %s %s: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                except KeyError, exp:
                    logger.warning("The %s '%s' is not initialized: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                # we come back to normal timeout
                #pyro.set_timeout(con, 5)
            else:  # no connection, try reinit
                self.pynag_con_init(p['instance_id'], type='poller')

        # We loop for our passive reactionners
        for p in filter(lambda p: p['passive'], self.reactionners.values()):
            logger.debug("I will get actions from the reactionner %s" % str(p))
            con = p['con']
            reactionner_tags = p['reactionner_tags']
            if con is not None:
                try:
                    # initial ping must be quick
                    #pyro.set_timeout(con, 120)
                    #results = con.get_returns(self.instance_id)
                    # Before ask a call that can be long, do a simple ping to be sure it is alive
                    con.get('ping')
                    results = con.get('get_returns', {'sched_id':self.instance_id}, wait='long')
                    results = cPickle.loads(str(results))
                    nb_received = len(results)
                    self.nb_check_received += nb_received
                    logger.debug("Received %d passive results" % nb_received)
                    for result in results:
                        result.set_type_passive()
                    self.waiting_results.extend(results)
                except HTTPExceptions, exp:
                    logger.warning("Connection problem to the %s %s: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                except KeyError, exp:
                    logger.warning("The %s '%s' is not initialized: %s" % (type, p['name'], str(exp)))
                    p['con'] = None
                    return
                # we come back to normal timeout
                #pyro.set_timeout(con, 5)
            else:  # no connection, try reinit
                self.pynag_con_init(p['instance_id'], type='reactionner')


    # Some checks are purely internal, like business based one
    # simply ask their ref to manage it when it's ok to run
    def manage_internal_checks(self):
        now = time.time()
        for c in self.checks.values():
            # must be ok to launch, and not an internal one (business rules based)
            if c.internal and c.status == 'scheduled' and c.is_launchable(now):
                c.ref.manage_internal_check(self.hosts, self.services, c)
                # it manage it, now just ask to consume it
                # like for all checks
                c.status = 'waitconsume'

    # Call by brokers to have broks
    # We give them, and clean them!
    def get_broks(self, bname):
        # If we are here, we are sure the broker entry exists
        res = self.brokers[bname]['broks']
        # They are gone, we keep none!
        self.brokers[bname]['broks'] = {}

        # Also put in the result the possible first log broks if so
        res.update(self.broks)
        # and clean the global broks too now
        self.broks.clear()

        return res


    # An element can have its topology changed by an external command
    # if so a brok will be generated with this flag. No need to reset all of
    # them.
    def reset_topology_change_flag(self):
        for i in self.hosts:
            i.topology_change = False
        for i in  self.services:
            i.topology_change = False

    # Update the retention file and give all te data in
    # a dict so the read function can pickup what it wants
    # For now compression is not used, but it can be added easily
    # just uncomment :)
    def update_retention_file(self, forced=False):
        # If we set the update to 0, we do not want of this
        # if we do not forced (like at stopping)
        if self.conf.retention_update_interval == 0 and not forced:
            return

        self.hook_point('save_retention')

    # Load the retention file and get status from it. It does not get all checks in progress
    # for the moment, just the status and the notifications.
    def retention_load(self):
        self.hook_point('load_retention')

    # Helper function for module, will give the host and service
    # data
    def get_retention_data(self):
        # We create an all_data dict with list of useful retention data dicts
        # of our hosts and services
        all_data = {'hosts': {}, 'services': {}}
        for h in self.hosts:
            d = {}
            running_properties = h.__class__.running_properties
            for prop, entry in running_properties.items():
                if entry.retention:
                    v = getattr(h, prop)
                    # Maybe we should "prepare" the data before saving it
                    # like get only names instead of the whole objects
                    f = entry.retention_preparation
                    if f:
                        v = f(h, v)
                    d[prop] = v
            # and some properties are also like this, like
            # active checks enabled or not
            properties = h.__class__.properties
            for prop, entry in properties.items():
                if entry.retention:
                    v = getattr(h, prop)
                    # Maybe we should "prepare" the data before saving it
                    # like get only names instead of the whole objects
                    f = entry.retention_preparation
                    if f:
                        v = f(h, v)
                    d[prop] = v
            all_data['hosts'][h.host_name] = d

        # Same for services
        for s in self.services:
            d = {}
            running_properties = s.__class__.running_properties
            for prop, entry in running_properties.items():
                if entry.retention:
                    v = getattr(s, prop)
                    # Maybe we should "prepare" the data before saving it
                    # like get only names instead of the whole objects
                    f = entry.retention_preparation
                    if f:
                        v = f(s, v)
                    d[prop] = v
            # Same for properties, like active checks enabled or not
            properties = s.__class__.properties
            for prop, entry in properties.items():
                if entry.retention:
                    v = getattr(s, prop)
                    # Maybe we should "prepare" the data before saving it
                    # like get only names instead of the whole objects
                    f = entry.retention_preparation
                    if f:
                        v = f(s, v)
                    d[prop] = v
            all_data['services'][(s.host.host_name, s.service_description)] = d
        return all_data

    # Get back our broks from a retention module :)
    def restore_retention_data(self, data):
        # Now load interesting properties in hosts/services
        # Tagging retention=False prop that not be directly load
        # Items will be with theirs status, but not in checking, so
        # a new check will be launched like with a normal beginning (random distributed
        # scheduling)

        ret_hosts = data['hosts']
        for ret_h_name in ret_hosts:
            # We take the dict of our value to load
            d = data['hosts'][ret_h_name]
            h = self.hosts.find_by_name(ret_h_name)
            if h is not None:
                # First manage all running properties
                running_properties = h.__class__.running_properties
                for prop, entry in running_properties.items():
                    if entry.retention:
                        # Maybe the saved one was not with this value, so
                        # we just bypass this
                        if prop in d:
                            setattr(h, prop, d[prop])
                # Ok, some are in properties too (like active check enabled
                # or not. Will OVERRIDE THE CONFIGURATION VALUE!
                properties = h.__class__.properties
                for prop, entry in properties.items():
                    if entry.retention:
                        # Maybe the saved one was not with this value, so
                        # we just bypass this
                        if prop in d:
                            setattr(h, prop, d[prop])
                # Now manage all linked objects load from previous run
                for a in h.notifications_in_progress.values():
                    a.ref = h
                    self.add(a)
                    # Also raises the action id, so do not overlap ids
                    a.assume_at_least_id(a.id)
                h.update_in_checking()
                # And also add downtimes and comments
                for dt in h.downtimes:
                    dt.ref = h
                    if hasattr(dt, 'extra_comment'):
                        dt.extra_comment.ref = h
                    else:
                        dt.extra_comment = None
                    # raises the downtime id to do not overlap
                    Downtime.id = max(Downtime.id, dt.id + 1)
                    self.add(dt)
                for c in h.comments:
                    c.ref = h
                    self.add(c)
                    # raises comment id to do not overlap ids
                    Comment.id = max(Comment.id, c.id + 1)
                if h.acknowledgement is not None:
                    h.acknowledgement.ref = h
                    # Raises the id of future ack so we don't overwrite
                    # these one
                    Acknowledge.id = max(Acknowledge.id, h.acknowledgement.id + 1)
                # Relink the notified_contacts as a set() of true contacts objects
                # it it was load from the retention, it's now a list of contacts
                # names
                if 'notified_contacts' in d:
                    new_notified_contacts = set()
                    for cname in h.notified_contacts:
                        c = self.contacts.find_by_name(cname)
                        # Maybe the contact is gone. Skip it
                        if c:
                            new_notified_contacts.add(c)
                    h.notified_contacts = new_notified_contacts

        # SAme for services
        ret_services = data['services']
        for (ret_s_h_name, ret_s_desc) in ret_services:
            # We take our dict to load
            d = data['services'][(ret_s_h_name, ret_s_desc)]
            s = self.services.find_srv_by_name_and_hostname(ret_s_h_name, ret_s_desc)

            if s is not None:
                # Load the major values from running properties
                running_properties = s.__class__.running_properties
                for prop, entry in running_properties.items():
                    if entry.retention:
                        # Maybe the saved one was not with this value, so
                        # we just bypass this
                        if prop in d:
                            setattr(s, prop, d[prop])
                # And some others from properties dict too
                properties = s.__class__.properties
                for prop, entry in properties.items():
                    if entry.retention:
                        # Maybe the saved one was not with this value, so
                        # we just bypass this
                        if prop in d:
                            setattr(s, prop, d[prop])
                # Ok now manage all linked objects
                for a in s.notifications_in_progress.values():
                    a.ref = s
                    self.add(a)
                    # Also raises the action id, so do not overlap id
                    a.assume_at_least_id(a.id)
                s.update_in_checking()
                # And also add downtimes and comments
                for dt in s.downtimes:
                    dt.ref = s
                    if hasattr(dt, 'extra_comment'):
                        dt.extra_comment.ref = s
                    else:
                        dt.extra_comment = None
                    # raises the downtime id to do not overlap
                    Downtime.id = max(Downtime.id, dt.id + 1)
                    self.add(dt)
                for c in s.comments:
                    c.ref = s
                    self.add(c)
                    # raises comment id to do not overlap ids
                    Comment.id = max(Comment.id, c.id + 1)
                if s.acknowledgement is not None:
                    s.acknowledgement.ref = s
                    # Raises the id of future ack so we don't overwrite
                    # these one
                    Acknowledge.id = max(Acknowledge.id, s.acknowledgement.id + 1)
                # Relink the notified_contacts as a set() of true contacts objects
                # it it was load from the retention, it's now a list of contacts
                # names
                if 'notified_contacts' in d:
                    new_notified_contacts = set()
                    for cname in s.notified_contacts:
                        c = self.contacts.find_by_name(cname)
                        # Maybe the contact is gone. Skip it
                        if c:
                            new_notified_contacts.add(c)
                    s.notified_contacts = new_notified_contacts


    # Fill the self.broks with broks of self (process id, and co)
    # broks of service and hosts (initial status)
    def fill_initial_broks(self, bname, with_logs=False):
        # First a Brok for delete all from my instance_id
        b = Brok('clean_all_my_instance_id', {'instance_id': self.instance_id})
        self.add_Brok(b, bname)

        # first the program status
        b = self.get_program_status_brok()
        self.add_Brok(b, bname)

        #  We can't call initial_status from all this types
        #  The order is important, service need host...
        initial_status_types = (self.timeperiods, self.commands,
                          self.contacts, self.contactgroups,
                          self.hosts, self.hostgroups,
                          self.services, self.servicegroups)

        self.conf.skip_initial_broks = getattr(self.conf,'skip_initial_broks', False)
        logger.debug("Skipping initial broks? %s" % str(self.conf.skip_initial_broks))
        if not self.conf.skip_initial_broks:
            for tab in initial_status_types:
                for i in tab:
                    b = i.get_initial_status_brok()
                    self.add_Brok(b, bname)

        # Only raises the all logs at the scheduler startup
        if with_logs:
            # Ask for INITIAL logs for services and hosts
            for i in self.hosts:
                i.raise_initial_state()
            for i in self.services:
                i.raise_initial_state()

        # Add a brok to say that we finished all initial_pass
        b = Brok('initial_broks_done', {'instance_id': self.instance_id})
        self.add_Brok(b, bname)

        # We now have all full broks
        self.has_full_broks = True

        logger.info("[%s] Created %d initial Broks for broker %s" % (self.instance_name, len(self.brokers[bname]['broks']), bname))


    # Crate a brok with program status info
    def get_and_register_program_status_brok(self):
        b = self.get_program_status_brok()
        self.add(b)


    # Crate a brok with program status info
    def get_and_register_update_program_status_brok(self):
        b = self.get_program_status_brok()
        b.type = 'update_program_status'
        self.add(b)


    # Get a brok with program status
    # TODO: GET REAL VALUES
    def get_program_status_brok(self):
        now = int(time.time())
        data = {"is_running": 1,
                "instance_id": self.instance_id,
                "instance_name": self.instance_name,
                "last_alive": now,
                "interval_length": self.conf.interval_length,
                "program_start": self.program_start,
                "pid": os.getpid(),
                "daemon_mode": 1,
                "last_command_check": now,
                "last_log_rotation": now,
                "notifications_enabled": self.conf.enable_notifications,
                "active_service_checks_enabled": self.conf.execute_service_checks,
                "passive_service_checks_enabled": self.conf.accept_passive_service_checks,
                "active_host_checks_enabled": self.conf.execute_host_checks,
                "passive_host_checks_enabled": self.conf.accept_passive_host_checks,
                "event_handlers_enabled": self.conf.enable_event_handlers,
                "flap_detection_enabled": self.conf.enable_flap_detection,
                "failure_prediction_enabled": 0,
                "process_performance_data": self.conf.process_performance_data,
                "obsess_over_hosts": self.conf.obsess_over_hosts,
                "obsess_over_services": self.conf.obsess_over_services,
                "modified_host_attributes": 0,
                "modified_service_attributes": 0,
                "global_host_event_handler": self.conf.global_host_event_handler,
                'global_service_event_handler': self.conf.global_service_event_handler,
                'check_external_commands': self.conf.check_external_commands,
                'check_service_freshness': self.conf.check_service_freshness,
                'check_host_freshness': self.conf.check_host_freshness,
                'command_file': self.conf.command_file
                }
        b = Brok('program_status', data)
        return b

    # Called every 1sec to consume every result in services or hosts
    # with these results, they are OK, CRITICAL, UP/DOWN, etc...
    def consume_results(self):
        # All results are in self.waiting_results
        # We need to get them first
        for c in self.waiting_results:
            self.put_results(c)
        self.waiting_results = []

        # Then we consume them
        #print "**********Consume*********"
        for c in self.checks.values():
            if c.status == 'waitconsume':
                item = c.ref
                item.consume_result(c)


        # All 'finished' checks (no more dep) raise checks they depends on
        for c in self.checks.values():
            if c.status == 'havetoresolvedep':
                for dependent_checks in c.depend_on_me:
                    # Ok, now dependent will no more wait c
                    dependent_checks.depend_on.remove(c.id)
                # REMOVE OLD DEP CHECK -> zombie
                c.status = 'zombie'

        # Now, reinteger dep checks
        for c in self.checks.values():
            if c.status == 'waitdep' and len(c.depend_on) == 0:
                item = c.ref
                item.consume_result(c)

    # Called every 1sec to delete all checks in a zombie state
    # zombie = not useful anymore
    def delete_zombie_checks(self):
        #print "**********Delete zombies checks****"
        id_to_del = []
        for c in self.checks.values():
            if c.status == 'zombie':
                id_to_del.append(c.id)
        # une petite tape dans le dos et tu t'en vas, merci...
        # *pat pat* GFTO, thks :)
        for id in id_to_del:
            del self.checks[id]  # ZANKUSEN!

    # Called every 1sec to delete all actions in a zombie state
    # zombie = not useful anymore
    def delete_zombie_actions(self):
        #print "**********Delete zombies actions****"
        id_to_del = []
        for a in self.actions.values():
            if a.status == 'zombie':
                id_to_del.append(a.id)
        # une petite tape dans le dos et tu t'en vas, merci...
        # *pat pat* GFTO, thks :)
        for id in id_to_del:
            del self.actions[id]  # ZANKUSEN!

    # Check for downtimes start and stop, and register
    # them if needed
    def update_downtimes_and_comments(self):
        broks = []
        now = time.time()

        # Look for in objects comments, and look if we already got them
        for elt in [y for y in [x for x in self.hosts] + [x for x in self.services]]:
            for c in elt.comments:
                if not c.id in self.comments:
                    self.comments[c.id] = c

        # Check maintenance periods
        for elt in [y for y in [x for x in self.hosts] + [x for x in self.services] if y.maintenance_period is not None]:

            if elt.in_maintenance is None:
                if elt.maintenance_period.is_time_valid(now):
                    start_dt = elt.maintenance_period.get_next_valid_time_from_t(now)
                    end_dt = elt.maintenance_period.get_next_invalid_time_from_t(start_dt + 1) - 1
                    dt = Downtime(elt, start_dt, end_dt, 1, 0, 0, "system", "this downtime was automatically scheduled through a maintenance_period")
                    elt.add_downtime(dt)
                    self.add(dt)
                    self.get_and_register_status_brok(elt)
                    elt.in_maintenance = dt.id
            else:
                if not elt.in_maintenance in self.downtimes:
                    # the main downtimes has expired or was manually deleted
                    elt.in_maintenance = None

        #  Check the validity of contact downtimes
        for elt in self.contacts:
            for dt in elt.downtimes:
                dt.check_activation()

        # A loop where those downtimes are removed
        # which were marked for deletion (mostly by dt.exit())
        for dt in self.downtimes.values():
            if dt.can_be_deleted == True:
                ref = dt.ref
                self.del_downtime(dt.id)
                broks.append(ref.get_update_status_brok())

        # Same for contact downtimes:
        for dt in self.contact_downtimes.values():
            if dt.can_be_deleted == True:
                ref = dt.ref
                self.del_contact_downtime(dt.id)
                broks.append(ref.get_update_status_brok())

        # Downtimes are usually accompanied by a comment.
        # An exiting downtime also invalidates it's comment.
        for c in self.comments.values():
            if c.can_be_deleted == True:
                ref = c.ref
                self.del_comment(c.id)
                broks.append(ref.get_update_status_brok())

        # Check start and stop times
        for dt in self.downtimes.values():
            if dt.real_end_time < now:
                # this one has expired
                broks.extend(dt.exit())  # returns downtimestop notifications
            elif now >= dt.start_time and dt.fixed and not dt.is_in_effect:
                # this one has to start now
                broks.extend(dt.enter())  # returns downtimestart notifications
                broks.append(dt.ref.get_update_status_brok())

        for b in broks:
            self.add(b)

    # Main schedule function to make the regular scheduling
    def schedule(self):
        # ask for service and hosts their next check
        for type_tab in [self.services, self.hosts]:
            for i in type_tab:
                i.schedule()

    # Main actions reaper function: it get all new checks,
    # notification and event handler from hosts and services
    def get_new_actions(self):
        self.hook_point('get_new_actions')
        # ask for service and hosts their next check
        for type_tab in [self.services, self.hosts]:
            for i in type_tab:
                for a in i.actions:
                    self.add(a)
                # We take all, we can clear it
                i.actions = []

    # Similar as above, but for broks
    def get_new_broks(self):
        # ask for service and hosts their broks waiting
        # be eaten
        for type_tab in [self.services, self.hosts]:
            for i in type_tab:
                for b in i.broks:
                    self.add(b)
                # We take all, we can clear it
                i.broks = []

    # Raises checks for no fresh states for services and hosts
    def check_freshness(self):
        #print "********** Check freshness******"
        for type_tab in [self.services, self.hosts]:
            for i in type_tab:
                c = i.do_check_freshness()
                if c is not None:
                    self.add(c)

    # Check for orphaned checks: checks that never returns back
    # so if inpoller and t_to_go < now - 300s: pb!
    # Warn only one time for each "worker"
    # XXX I think we should make "time_to_orphanage" configurable
    #     each action type, each for notification, event_handler & check
    #     I think it will be a little more useful that way, not sure tho
    def check_orphaned(self):
        worker_names = {}
        now = int(time.time())
        for c in self.checks.values():
            time_to_orphanage = c.ref.get_time_to_orphanage()
            if time_to_orphanage:
                if c.status == 'inpoller' and c.t_to_go < now - time_to_orphanage:
                    c.status = 'scheduled'
                    if c.worker not in worker_names:
                        worker_names[c.worker] = 1
                        continue
                    worker_names[c.worker] += 1
        for a in self.actions.values():
            time_to_orphanage = a.ref.get_time_to_orphanage()
            if time_to_orphanage:
                if a.status == 'inpoller' and a.t_to_go < now - time_to_orphanage:
                    a.status = 'scheduled'
                    if a.worker not in worker_names:
                        worker_names[a.worker] = 1
                        continue
                    worker_names[a.worker] += 1

        for w in worker_names:
            logger.warning("%d actions never came back for the satellite '%s'. I reenable them for polling" % (worker_names[w], w))

    # Each loop we are going to send our broks to our modules (if need)
    def send_broks_to_modules(self):
        t0 = time.time()
        nb_sent = 0
        for mod in self.sched_daemon.modules_manager.get_external_instances():
            logger.debug("Look for sending to module %s" % mod.get_name())
            q = mod.to_q
            to_send = [b for b in self.broks.values() if not getattr(b, 'sent_to_sched_externals', False) and mod.want_brok(b)]
            q.put(to_send)
            nb_sent += len(to_send)

        # No more need to send them
        for b in self.broks.values():
            b.sent_to_sched_externals = True
        logger.debug("Time to send %s broks (after %d secs)" % (nb_sent, time.time() - t0))

    # Get 'objects' from external modules
    # right now on nobody uses it, but it can be useful
    # for a module like livestatus to raise external
    # commands for example
    def get_objects_from_from_queues(self):
        for f in self.sched_daemon.modules_manager.get_external_from_queues():
            full_queue = True
            while full_queue:
                try:
                    o = f.get(block=False)
                    self.add(o)
                except Empty:
                    full_queue = False

    # Main function
    def run(self):
        # Then we see if we've got info in the retention file
        self.retention_load()

        # Finally start the external modules now we got our data
        self.hook_point('pre_scheduler_mod_start')
        self.sched_daemon.modules_manager.start_external_instances(late_start=True)

        # Ok, now all is initialized, we can make the initial broks
        logger.info("[%s] First scheduling launched" % self.instance_name)
        self.schedule()
        logger.info("[%s] First scheduling done" % self.instance_name)

        # Now connect to the passive satellites if needed
        for p_id in self.pollers:
            self.pynag_con_init(p_id, type='poller')

        # Ticks are for recurrent function call like consume
        # del zombies etc
        ticks = 0
        timeout = 1.0  # For the select

        gogogo = time.time()

        # We must reset it if we received a new conf from the Arbiter.
        # Otherwise, the stat check average won't be correct
        self.nb_check_received = 0

        self.load_one_min = Load(initial_value=1)
        logger.debug("First loop at %d" % time.time())
        while self.must_run:
            #print "Loop"
            # Before answer to brokers, we send our broks to modules
            # Ok, go to send our broks to our external modules
            #self.send_broks_to_modules()

            elapsed, _, _ = self.sched_daemon.handleRequests(timeout)
            if elapsed:
                timeout -= elapsed
                if timeout > 0:
                    continue

            self.load_one_min.update_load(self.sched_daemon.sleep_time)

            # load of the scheduler is the percert of time it is waiting
            l = min(100, 100.0 - self.load_one_min.get_load() * 100)
            logger.debug("Load: (sleep) %.2f (average: %.2f) -> %d%%" % (self.sched_daemon.sleep_time, self.load_one_min.get_load(), l))

            self.sched_daemon.sleep_time = 0.0

            # Timeout or time over
            timeout = 1.0
            ticks += 1

            # Do recurrent works like schedule, consume
            # delete_zombie_checks
            for i in self.recurrent_works:
                (name, f, nb_ticks) = self.recurrent_works[i]
                # A 0 in the tick will just disable it
                if nb_ticks != 0:
                    if ticks % nb_ticks == 0:
                        # print "I run function:", name
                        f()

            # DBG: push actions to passives?
            self.push_actions_to_passives_satellites()
            self.get_actions_from_passives_satellites()

            #if  ticks % 10 == 0:
            #    self.conf.quick_debug()

            # stats
            nb_scheduled = len([c for c in self.checks.values() if c.status == 'scheduled'])
            nb_inpoller = len([c for c in self.checks.values() if c.status == 'inpoller'])
            nb_zombies = len([c for c in self.checks.values() if c.status == 'zombie'])
            nb_notifications = len(self.actions)

            logger.debug("Checks: total %s, scheduled %s, inpoller %s, zombies %s, notifications %s" %\
                (len(self.checks), nb_scheduled, nb_inpoller, nb_zombies, nb_notifications))

            # Get a overview of the latencies with just
            # a 95 percentile view, but lso min/max values
            latencies = [s.latency for s in self.services]
            lat_avg, lat_min, lat_max = nighty_five_percent(latencies)
            if lat_avg is not None:
                logger.debug("Latency (avg/min/max): %.2f/%.2f/%.2f" % (lat_avg, lat_min, lat_max))

            # print "Notifications:", nb_notifications
            now = time.time()

            if self.nb_checks_send != 0:
                logger.debug("Nb checks/notifications/event send: %s" % self.nb_checks_send)
            self.nb_checks_send = 0
            if self.nb_broks_send != 0:
                logger.debug("Nb Broks send: %s" % self.nb_broks_send)
            self.nb_broks_send = 0

            time_elapsed = now - gogogo
            logger.debug("Check average = %d checks/s" % int(self.nb_check_received / time_elapsed))

            if self.need_dump_memory:
                self.sched_daemon.dump_memory()
                self.need_dump_memory = False

            if self.need_objects_dump:
                logger.debug('I need to dump my objects!')
                self.dump_objects()
                self.need_objects_dump = False




        # WE must save the retention at the quit BY OURSELF
        # because our daemon will not be able to do it for us
        self.update_retention_file(True)

########NEW FILE########
__FILENAME__ = schedulerlink
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from shinken.satellitelink import SatelliteLink, SatelliteLinks
from shinken.property import BoolProp, IntegerProp, StringProp, ListProp
from shinken.log  import logger
from shinken.http_client import HTTPExceptions

class SchedulerLink(SatelliteLink):
    """Please Add a Docstring to describe the class here"""

    id = 0

    # Ok we lie a little here because we are a mere link in fact
    my_type = 'scheduler'

    properties = SatelliteLink.properties.copy()
    properties.update({
        'scheduler_name':     StringProp(fill_brok=['full_status']),
        'port':               IntegerProp(default='7768', fill_brok=['full_status']),
        'weight':             IntegerProp(default='1', fill_brok=['full_status']),
        'skip_initial_broks': BoolProp(default='0', fill_brok=['full_status']),
    })

    running_properties = SatelliteLink.running_properties.copy()
    running_properties.update({
        'conf': StringProp(default=None),
        'need_conf': StringProp(default=True),
        'external_commands': StringProp(default=[]),
        'push_flavor': IntegerProp(default=0),
    })

    def get_name(self):
        return self.scheduler_name

    def run_external_commands(self, commands):
        if self.con is None:
            self.create_connection()
        if not self.alive:
            return None
        logger.debug("[SchedulerLink] Sending %d commands" % len(commands))
        try:
            self.con.post('run_external_commands', {'cmds' : commands})
        except HTTPExceptions, exp:
            self.con = None
            logger.debug(exp)
            return False

    def register_to_my_realm(self):
        self.realm.schedulers.append(self)


    def give_satellite_cfg(self):
        return {'port': self.port, 'address': self.address, 'name': self.scheduler_name, 'instance_id': self.id, 'active': self.conf is not None,
                'push_flavor': self.push_flavor, 'use_ssl':self.use_ssl, 'hard_ssl_name_check':self.hard_ssl_name_check}


    # Some parameters can give as 'overridden parameters' like use_timezone
    # so they will be mixed (in the scheduler) with the standard conf sent by the arbiter
    def get_override_configuration(self):
        r = {}
        properties = self.__class__.properties
        for prop, entry in properties.items():
            if entry.override:
                r[prop] = getattr(self, prop)
        return r


class SchedulerLinks(SatelliteLinks):
    """Please Add a Docstring to describe the class here"""

    name_property = "scheduler_name"
    inner_class = SchedulerLink

########NEW FILE########
__FILENAME__ = singleton
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


class Singleton(type):
    """The classic Singleton class. So all instance of this class will be the same
    instance in fact.

    """

    def __init__(cls, name, bases, dict):
        super(Singleton, cls).__init__(name, bases, dict)
        cls.instance = None

    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(Singleton, cls).__call__(*args, **kw)
            return cls.instance

########NEW FILE########
__FILENAME__ = sorteddict
#!/usr/bin/env python
#
# sorteddict.py
# Sorted dictionary (implementation for Python 2.x)
#
# Copyright (c) 2010 Jan Kaliszewski (zuo)
#
# The MIT License:
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

from bisect import bisect_left, insort
from itertools import izip, repeat


def dictdoc(method):
    "A decorator making reuse of the ordinary dict's docstrings more concise."
    dict_method = getattr(dict, method.__name__)
    if hasattr(dict_method, '__doc__'):
        method.__doc__ = dict_method.__doc__
    return method


class SortedDict(dict):
    '''Dictionary with sorted keys.

    The interface is similar to the ordinary dict's one, but:
    * methods: __repr__(), __str__(), __iter__(), iterkeys(), itervalues(),
      iteritems(), keys(), values(), items() and popitem() -- return results
      taking into consideration sorted keys order;
    * new methods: largest_key(), largest_item(), smallest_key(),
      smallest_item() added.
    '''

    def __init__(self, *args, **kwargs):
        '''Like with the ordinary dict: from a mapping, from an iterable
        of (key, value) pairs, or from keyword arguments.'''
        dict.__init__(self, *args, **kwargs)
        self._sorted_keys = sorted(dict.iterkeys(self))

    @dictdoc
    def __repr__(self):
        return 'SortedDict({%s})' % ', '.join('%r: %r' % item
                                              for item in self.iteritems())

    @dictdoc
    def __str__(self):
        return repr(self)

    @dictdoc
    def __setitem__(self, key, value):
        key_is_new = key not in self
        dict.__setitem__(self, key, value)
        if key_is_new:
            insort(self._sorted_keys, key)

    @dictdoc
    def __delitem__(self, key):
        dict.__delitem__(self, key)
        del self._sorted_keys[bisect_left(self._sorted_keys, key)]

    def __iter__(self, reverse=False):
        '''D.__iter__() <==> iter(D) <==> D.iterkeys() -> an iterator over
        sorted keys (add reverse=True for reverse ordering).'''
        if reverse:
            return reversed(self._sorted_keys)
        else:
            return iter(self._sorted_keys)

    iterkeys = __iter__

    def itervalues(self, reverse=False):
        '''D.itervalues() -> an iterator over values sorted by keys
        (add reverse=True for reverse ordering).'''
        return (self[key] for key in self.iterkeys(reverse))

    def iteritems(self, reverse=False):
        '''D.iteritems() -> an iterator over (key, value) pairs sorted by keys
        (add reverse=True for reverse ordering).'''
        return ((key, self[key]) for key in self.iterkeys(reverse))

    def keys(self, reverse=False):
        '''D.keys() -> a sorted list of keys
        (add reverse=True for reverse ordering).'''
        return list(self.iterkeys(reverse))

    def values(self, reverse=False):
        '''D.values() -> a list of values sorted by keys
        (add reverse=True for reverse ordering).'''
        return list(self.itervalues(reverse))

    def items(self, reverse=False):
        '''D.items() -> a list of (key, value) pairs sorted by keys
        (add reverse=True for reverse ordering).'''
        return list(self.iteritems(reverse))

    @dictdoc
    def clear(self):
        dict.clear(self)
        del self._sorted_keys[:]

    def copy(self):
        '''D.copy() -> a shallow copy of D (still as a SortedDict).'''
        return self.__class__(self)

    @classmethod
    @dictdoc
    def fromkeys(cls, seq, value=None):
        return cls(izip(seq, repeat(value)))

    @dictdoc
    def pop(self, key, *args, **kwargs):
        if key in self:
            del self._sorted_keys[bisect_left(self._sorted_keys, key)]
        return dict.pop(self, key, *args, **kwargs)

    def popitem(self):
        '''D.popitem() -> (k, v). Remove and return a (key, value) pair with
        the largest key; raise KeyError if D is empty.'''
        try:
            key = self._sorted_keys.pop()
        except IndexError:
            raise KeyError('popitem(): dictionary is empty')
        else:
            return key, dict.pop(self, key)

    @dictdoc
    def setdefault(self, key, default=None):
        if key not in self:
            insort(self._sorted_keys, key)
        return dict.setdefault(self, key, default)

    @dictdoc
    def update(self, other=()):
        if hasattr(other, 'keys') and hasattr(other, 'values'):
            # mapping
            newkeys = [key for key in other if key not in self]
        else:
            # iterator/sequence of pairs
            other = list(other)
            newkeys = [key for key, _ in other if key not in self]
        dict.update(self, other)
        for key in newkeys:
            insort(self._sorted_keys, key)

    def largest_key(self):
        '''D.largest_key() -> the largest key; raise KeyError if D is empty.'''
        try:
            return self._sorted_keys[-1]
        except IndexError:
            raise KeyError('largest_key(): dictionary is empty')

    def largest_item(self):
        '''D.largest_item() -> a (key, value) pair with the largest key;
        raise KeyError if D is empty.'''
        key = self.largest_key()
        return key, self[key]

    def smallest_key(self):
        '''D.smallest_key() -> the smallest key; raise KeyError if D is empty.'''
        try:
            return self._sorted_keys[0]
        except IndexError:
            raise KeyError('smallest_key(): dictionary is empty')

    def smallest_item(self):
        '''D.smallest_item() -> a (key, value) pair with the smallest key;
        raise KeyError if D is empty.'''
        key = self.smallest_key()
        return key, self[key]

########NEW FILE########
__FILENAME__ = trigger_functions
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import os
import re

from shinken.objects.item import Item, Items
from shinken.misc.perfdata import PerfDatas
from shinken.property import (BoolProp, IntegerProp, FloatProp,
                              CharProp, StringProp, ListProp)
from shinken.log import logger

objs = {'hosts': [], 'services': []}
trigger_functions = {}


class declared(object):
    """ Decorator to add function in trigger environnement
    """
    def __init__(self, f):
        self.f = f
        global functions
        n = f.func_name
        #logger.debug("Initializing function %s %s" % (n, f))
        trigger_functions[n] = f

    def __call__(self, *args):
        logger.debug("Calling %s with arguments %s" % (self.f.func_name, args))
        return self.f(*args)

@declared
def up(obj, output):
    """ Set a host in UP state
    """
    set_value(obj, output, None, 0)


@declared
def down(obj, output):
    """ Set a host in DOWN state
    """
    set_value(obj, output, None, 1)


@declared
def ok(obj, output):
    """ Set a service in OK state
    """
    set_value(obj, output, None, 0)


@declared
def warning(obj, output):
    """ Set a service in WARNING state
    """
    set_value(obj, output, None, 1)


@declared
def critical(obj, output):
    """ Set a service in CRITICAL state
    """
    set_value(obj, output, None, 2)


@declared
def unknown(obj, output):
    """ Set a service in UNKNOWN state
    """
    set_value(obj, output, None, 3)


@declared
def set_value(obj_ref, output=None, perfdata=None, return_code=None):
    """ Set output, state and perfdata to a service or host
    """
    obj = get_object(obj_ref)
    if not obj:
        return
    output = output or obj.output
    perfdata = perfdata or obj.perf_data
    if return_code is None:
        return_code = obj.state_id

    logger.debug("[trigger] Setting %s %s %s for object %s" % (output,
                                                               perfdata,
                                                               return_code,
                                                               obj.get_full_name()))

    if perfdata:
        output = output + ' | ' + perfdata

    now = time.time()
    cls = obj.__class__
    i = obj.launch_check(now, force=True)
    for chk in obj.checks_in_progress:
        if chk.id == i:
            logger.debug("[trigger] I found the check I want to change")
            c = chk
            # Now we 'transform the check into a result'
            # So exit_status, output and status is eaten by the host
            c.exit_status = return_code
            c.get_outputs(output, obj.max_plugins_output_length)
            c.status = 'waitconsume'
            c.check_time = now
            # IMPORTANT: tag this check as from a trigger, so we will not
            # loop in an infinite way for triggers checks!
            c.from_trigger = True
            # Ok now this result will be read by scheduler the next loop


@declared
def perf(obj_ref, metric_name):
    """ Get perf data from a service
    """
    obj = get_object(obj_ref)
    p = PerfDatas(obj.perf_data)
    if metric_name in p:
        logger.debug("[trigger] I found the perfdata")
        return p[metric_name].value
    logger.debug("[trigger] I am in perf command")
    return None


@declared
def get_custom(obj_ref, cname, default=None):
    """ Get custom varialbe from a service or a host
    """
    obj = get_objects(obj_ref)
    if not obj:
        return default
    cname = cname.upper().strip()
    if not cname.startswith('_'):
        cname = '_' + cname
    return obj.customs.get(cname, default)


@declared
def perfs(objs_ref, metric_name):
    """ TODO: check this description
        Get perfdatas from multiple services/hosts
    """
    objs = get_objects(objs_ref)
    r = []
    for o in objs:
        v = perf(o, metric_name)
        r.append(v)
    return r


@declared
def allperfs(obj_ref):
    """ Get all perfdatas from a service or a host
    """
    obj = get_object(obj_ref)
    p = PerfDatas(obj.perf_data)
    logger.debug("[trigger] I get all perfdatas")
    return dict([(metric.name, p[metric.name]) for metric in p])


@declared
def get_object(ref):
    """ Retrive object (service/host) from name
    """
    # Maybe it's already a real object, if so, return it :)
    if not isinstance(ref, basestring):
        return ref

    # Ok it's a string
    name = ref
    if not '/' in name:
        return objs['hosts'].find_by_name(name)
    else:
        elts = name.split('/', 1)
        return objs['services'].find_srv_by_name_and_hostname(elts[0], elts[1])


@declared
def get_objects(ref):
    """ TODO: check this description
        Retrive objects (service/host) from names
    """
    # Maybe it's already a real object, if so, return it :)
    if not isinstance(ref, basestring):
        return ref

    name = ref
    # Maybe there is no '*'? if so, it's one element
    if not '*' in name:
        return get_object(name)

    # Ok we look for spliting the host or service thing
    hname = ''
    sdesc = ''
    if not '/' in name:
        hname = name
    else:
        elts = name.split('/', 1)
        hname = elts[0]
        sdesc = elts[1]
    logger.debug("[trigger get_objects] Look for %s %s" % (hname, sdesc))
    res = []
    hosts = []
    services = []

    # Look for host, and if need, look for service
    if not '*' in hname:
        h = objs['hosts'].find_by_name(hname)
        if h:
            hosts.append(h)
    else:
        hname = hname.replace('*', '.*')
        p = re.compile(hname)
        for h in objs['hosts']:
            logger.debug("[trigger] Compare %s with %s" % (hname, h.get_name()))
            if p.search(h.get_name()):
                hosts.append(h)

    # Maybe the user ask for justs hosts :)
    if not sdesc:
        return hosts

    for h in hosts:
        if not '*' in sdesc:
            s = h.find_service_by_name(sdesc)
            if s:
                services.append(s)
        else:
            sdesc = sdesc.replace('*', '.*')
            p = re.compile(sdesc)
            for s in h.services:
                logger.debug("[trigger] Compare %s with %s" % (s.service_description, sdesc))
                if p.search(s.service_description):
                    services.append(s)

    logger.debug("Found the following services: %s" % (services))
    return services

########NEW FILE########
__FILENAME__ = util
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import time
import re
import copy
import sys
#import shutil
import os
import json
try:
    from ClusterShell.NodeSet import NodeSet, NodeSetParseRangeError
except ImportError:
    NodeSet = None

from shinken.macroresolver import MacroResolver
from shinken.log import logger

#from memoized import memoized
try:
    stdout_encoding = sys.stdout.encoding
    safe_stdout = (stdout_encoding == 'UTF-8')
except Exception, exp:
    logger.error('Encoding detection error= %s' % (exp))
    safe_stdout = False


########### Strings #############
# Try to print strings, but if there is an utf8 error, go in simple ascii mode
# (Like if the terminal do not have en_US.UTF8 as LANG for example)
def safe_print(*args):
    l = []
    for e in args:
        # If we got an str, go in unicode, and if we cannot print
        # utf8, go in ascii mode
        if isinstance(e, str):
            if safe_stdout:
                s = unicode(e, 'utf8', errors='ignore')
            else:
                s = e.decode('ascii', 'replace').encode('ascii', 'replace').decode('ascii', 'replace')
            l.append(s)
        # Same for unicode, but skip the unicode pass
        elif isinstance(e, unicode):
            if safe_stdout:
                s = e
            else:
                s = e.encode('ascii', 'replace')
            l.append(s)
        # Other types can be directly convert in unicode
        else:
            l.append(unicode(e))
    # Ok, now print it :)
    print u' '.join(l)


def split_semicolon(line, maxsplit=None):
    """Split a line on semicolons characters but not on the escaped semicolons
    """
    # Split on ';' character
    splitted_line = line.split(';')

    splitted_line_size = len(splitted_line)

    # if maxsplit is not specified, we set it to the number of part
    if maxsplit is None or 0 > maxsplit:
        maxsplit = splitted_line_size

    # Join parts  to the next one, if ends with a '\'
    # because we mustn't split if the semicolon is escaped
    i = 0
    while i < splitted_line_size - 1:

        # for each part, check if its ends with a '\'
        ends = splitted_line[i].endswith('\\')

        if ends:
            # remove the last character '\'
            splitted_line[i] = splitted_line[i][:-1]

        # append the next part to the current if it is not the last and the current
        # ends with '\' or if there is more than maxsplit parts
        if (ends or i >= maxsplit) and i < splitted_line_size - 1:

            splitted_line[i] = ";".join([splitted_line[i], splitted_line[i + 1]])

            # delete the next part
            del splitted_line[i + 1]
            splitted_line_size -= 1

        # increase i only if we don't have append because after append the new
        # string can end with '\'
        else:
            i += 1

    return splitted_line



# Json-ify the objects
def jsonify_r(obj):
    res = {}
    cls = obj.__class__
    if not hasattr(cls, 'properties'):
        try:
            json.dumps(obj)
            return obj
        except Exception, exp:
            return None
    properties = cls.properties.keys()
    if hasattr(cls, 'running_properties'):
        properties += cls.running_properties.keys()
    for prop in properties:
        if not hasattr(obj, prop):
            continue
        v = getattr(obj, prop)
        # Maybe the property is not jsonable
        try:
            if isinstance(v, set):
                v = list(v)
            json.dumps(v)
            res[prop] = v
        except Exception, exp:
            if isinstance(v, list):
                lst = []
                for _t in v:
                    t = getattr(_t.__class__, 'my_type', '')
                    if t == 'CommandCall':
                        try:
                            lst.append(_t.call)
                        except:
                            pass
                        continue
                    if t and hasattr(_t, t+'_name'):
                        lst.append(getattr(_t, t+'_name'))
                    else:
                        print "CANNOT MANAGE OBJECT", _t, type(_t), t
                res[prop] = lst
            else:
                t = getattr(v.__class__, 'my_type', '')
                if t == 'CommandCall':
                    try:
                        res[prop] = v.call
                    except:
                        pass
                    continue
                if t and hasattr(v, t+'_name'):
                    res[prop] = getattr(v, t+'_name')
                else:
                    print "CANNOT MANAGE OBJECT", v, type(v), t
    return res

################################### TIME ##################################
# @memoized
def get_end_of_day(year, month_id, day):
    end_time = (year, month_id, day, 23, 59, 59, 0, 0, -1)
    end_time_epoch = time.mktime(end_time)
    return end_time_epoch


# @memoized
def print_date(t):
    return time.asctime(time.localtime(t))


# @memoized
def get_day(t):
    return int(t - get_sec_from_morning(t))


# Same but for week day
def get_wday(t):
    t_lt = time.localtime(t)
    return  t_lt.tm_wday


# @memoized
def get_sec_from_morning(t):
    t_lt = time.localtime(t)
    h = t_lt.tm_hour
    m = t_lt.tm_min
    s = t_lt.tm_sec
    return h * 3600 + m * 60 + s


# @memoized
def get_start_of_day(year, month_id, day):
    start_time = (year, month_id, day, 00, 00, 00, 0, 0, -1)
    try:
        start_time_epoch = time.mktime(start_time)
    except OverflowError:
        # Windows mktime sometimes crashes on (1970, 1, 1, ...)
        start_time_epoch = 0.0

    return start_time_epoch


# change a time in seconds like 3600 into a format: 0d 1h 0m 0s
def format_t_into_dhms_format(t):
    s = t
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)
    d, h = divmod(h, 24)
    return '%sd %sh %sm %ss' % (d, h, m, s)


################################# Pythonization ###########################
# first change to float so manage for example 25.0 to 25
def to_int(val):
    return int(float(val))


def to_float(val):
    return float(val)


def to_char(val):
    return val[0]


def to_split(val, split_on_coma=True):
    if isinstance(val, list):
        return val
    if not split_on_coma:
        return [val]
    val = val.split(',')
    if val == ['']:
        val = []
    return val


def to_best_int_float(val):
    i = int(float(val))
    f = float(val)
    # If the f is a .0 value,
    # best match is int
    if i == f:
        return i
    return f


# bool('0') = true, so...
def to_bool(val):
    if val == '1' or val == 'on' or val == 'true' or val == 'True':
        return True
    else:
        return False


def from_bool_to_string(b):
    if b:
        return '1'
    else:
        return '0'


def from_bool_to_int(b):
    if b:
        return 1
    else:
        return 0


def from_list_to_split(val):
    val = ','.join(['%s' % v for v in val])
    return val


def from_float_to_int(val):
    val = int(val)
    return val


### Functions for brok_transformations
### They take 2 parameters: ref, and a value
### ref is the item like a service, and value
### if the value to preprocess

# Just a string list of all names, with ,
def to_list_string_of_names(ref, tab):
    return ",".join([e.get_name() for e in tab])


# Just a list of names
def to_list_of_names(ref, tab):
    return [e.get_name() for e in tab]


# This will give a string if the value exists
# or '' if not
def to_name_if_possible(ref, value):
    if value:
        return value.get_name()
    return ''


# take a list of hosts and return a list
# of all host_names
def to_hostnames_list(ref, tab):
    r = []
    for h in tab:
        if hasattr(h, 'host_name'):
            r.append(h.host_name)
    return r


# Will create a dict with 2 lists:
# *services: all services of the tab
# *hosts: all hosts of the tab
def to_svc_hst_distinct_lists(ref, tab):
    r = {'hosts': [], 'services': []}
    for e in tab:
        cls = e.__class__
        if cls.my_type == 'service':
            name = e.get_dbg_name()
            r['services'].append(name)
        else:
            name = e.get_dbg_name()
            r['hosts'].append(name)
    return r


# Will expand the value with macros from the
# host/service ref before brok it
def expand_with_macros(ref, value):
    return MacroResolver().resolve_simple_macros_in_string(value, ref.get_data_for_checks())


# Just get the string name of the object
# (like for realm)
def get_obj_name(obj):
    # Maybe we do not have a real object but already a string. If so
    # return the string
    if isinstance(obj, basestring):
        return obj
    return obj.get_name()


# Same as before, but call with object,prop instead of just value
# But if we got an attribute error, return ''
def get_obj_name_two_args_and_void(obj, value):
    try:
        return value.get_name()
    except AttributeError:
        return ''


# Get the full name if there is one
def get_obj_full_name(obj):
    try:
        return obj.get_full_name()
    except Exception:
        return obj.get_name()


# return the list of keys of the custom dict
# but without the _ before
def get_customs_keys(d):
    return [k[1:] for k in d.keys()]


# return the values of the dict
def get_customs_values(d):
    return d.values()


# Checks that a parameter has an unique value. If it's a list, the last
# value set wins.
def unique_value(val):
    if isinstance(val, list):
        if val:
            return val[-1]
        else:
            return ''
    else:
        return val


###################### Sorting ################
def scheduler_no_spare_first(x, y):
    if x.spare and not y.spare:
        return 1
    elif x.spare and y.spare:
        return 0
    else:
        return -1


#-1 is x first, 0 equal, 1 is y first
def alive_then_spare_then_deads(x, y):
    # First are alive
    if x.alive and not y.alive:
        return -1
    if y.alive and not x.alive:
        return 0
    # if not alive both, I really don't care...
    if not x.alive and not y.alive:
        return -1
    # Ok, both are alive... now spare after no spare
    if not x.spare:
        return -1
    # x is a spare, so y must be before, even if
    # y is a spare
    if not y.spare:
        return 1
    return 0


#-1 is x first, 0 equal, 1 is y first
def sort_by_ids(x, y):
    if x.id < y.id:
        return -1
    if x.id > y.id:
        return 1
    # So is equal
    return 0


# From a tab, get the avg, min, max
# for the tab values, but not the lower ones
# and higher ones that are too distinct
# than major ones
def nighty_five_percent(t):
    t2 = copy.copy(t)
    t2.sort()

    l = len(t)

    # If void tab, wtf??
    if l == 0:
        return (None, None, None)

    t_reduce = t2
    # only take a part if we got more
    # than 100 elements, or it's a non sense
    if l > 100:
        offset = int(l * 0.05)
        t_reduce = t_reduce[offset:-offset]

    reduce_len = len(t_reduce)
    reduce_sum = sum(t_reduce)

    reduce_avg = float(reduce_sum) / reduce_len
    reduce_max = max(t_reduce)
    reduce_min = min(t_reduce)

    return (reduce_avg, reduce_min, reduce_max)


##################### Cleaning ##############
def strip_and_uniq(tab):
    new_tab = set()
    for elt in tab:
        val = elt.strip()
        if (val != ''):
            new_tab.add(val)
    return list(new_tab)


#################### Pattern change application (mainly for host) #######


def expand_xy_pattern(pattern):
    ns = NodeSet(str(pattern))
    if len(ns) > 1:
        for elem in ns:
            for a in expand_xy_pattern(elem):
                yield a
    else:
        yield pattern


# This function is used to generate all pattern change as
# recursive list.
# for example, for a [(1,3),(1,4),(1,5)] xy_couples,
# it will generate a 60 item list with:
# Rule: [1, '[1-5]', [1, '[1-4]', [1, '[1-3]', []]]]
# Rule: [1, '[1-5]', [1, '[1-4]', [2, '[1-3]', []]]]
# ...
def got_generation_rule_pattern_change(xy_couples):
    res = []
    xy_cpl = xy_couples
    if xy_couples == []:
        return []
    (x, y) = xy_cpl[0]
    for i in xrange(x, y + 1):
        n = got_generation_rule_pattern_change(xy_cpl[1:])
        if n != []:
            for e in n:
                res.append([i, '[%d-%d]' % (x, y), e])
        else:
            res.append([i, '[%d-%d]' % (x, y), []])
    return res


# this function apply a recursive pattern change
# generate by the got_generation_rule_pattern_change
# function.
# It take one entry of this list, and apply
# recursively the change to s like:
# s = "Unit [1-3] Port [1-4] Admin [1-5]"
# rule = [1, '[1-5]', [2, '[1-4]', [3, '[1-3]', []]]]
# output = Unit 3 Port 2 Admin 1
def apply_change_recursive_pattern_change(s, rule):
    #print "Try to change %s" % s, 'with', rule
    #new_s = s
    (i, m, t) = rule
    #print "replace %s by %s" % (r'%s' % m, str(i)), 'in', s
    s = s.replace(r'%s' % m, str(i))
    #print "And got", s
    if t == []:
        return s
    return apply_change_recursive_pattern_change(s, t)

# For service generator, get dict from a _custom properties
# as _disks   C$(80%!90%),D$(80%!90%)$,E$(80%!90%)$
#return {'C': '80%!90%', 'D': '80%!90%', 'E': '80%!90%'}
# And if we have a key that look like [X-Y] we will expand it
# into Y-X+1 keys
GET_KEY_VALUE_SEQUENCE_ERROR_NOERROR = 0
GET_KEY_VALUE_SEQUENCE_ERROR_SYNTAX = 1
GET_KEY_VALUE_SEQUENCE_ERROR_NODEFAULT = 2
GET_KEY_VALUE_SEQUENCE_ERROR_NODE = 3


def get_key_value_sequence(entry, default_value=None):
    array1 = []
    array2 = []
    conf_entry = entry

    # match a key$(value1..n)$
    keyval_pattern_txt = r"""
\s*(?P<key>[^,]+?)(?P<values>(\$\(.*?\)\$)*)(?:[,]|$)
"""
    keyval_pattern = re.compile('(?x)' + keyval_pattern_txt)
    # match a whole sequence of key$(value1..n)$
    all_keyval_pattern = re.compile('(?x)^(' + keyval_pattern_txt + ')+$')
    # match a single value
    value_pattern = re.compile('(?:\$\((?P<val>.*?)\)\$)')
    # match a sequence of values
    all_value_pattern = re.compile('^(?:\$\(.*?\)\$)+$')

    if all_keyval_pattern.match(conf_entry):
        for mat in re.finditer(keyval_pattern, conf_entry):
            r = {'KEY': mat.group('key')}
            # The key is in mat.group('key')
            # If there are also value(s)...
            if mat.group('values'):
                if all_value_pattern.match(mat.group('values')):
                    # If there are multiple values, loop over them
                    valnum = 1
                    for val in re.finditer(value_pattern, mat.group('values')):
                        r['VALUE' + str(valnum)] = val.group('val')
                        valnum += 1
                else:
                    # Value syntax error
                    return (None, GET_KEY_VALUE_SEQUENCE_ERROR_SYNTAX)
            else:
                r['VALUE1'] = None
            array1.append(r)
    else:
        # Something is wrong with the values. (Maybe unbalanced '$(')
        # TODO: count opening and closing brackets in the pattern
        return (None, GET_KEY_VALUE_SEQUENCE_ERROR_SYNTAX)

    # now fill the empty values with the default value
    for r in array1:
        if r['VALUE1'] is None:
            if default_value is None:
                return (None, GET_KEY_VALUE_SEQUENCE_ERROR_NODEFAULT)
            else:
                r['VALUE1'] = default_value
        r['VALUE'] = r['VALUE1']

    # Now create new one but for [X-Y] matchs
    #  array1 holds the original entries. Some of the keys may contain wildcards
    #  array2 is filled with originals and inflated wildcards

    if NodeSet is None:
        # The pattern that will say if we have a [X-Y] key.
        pat = re.compile('\[(\d*)-(\d*)\]')

    for r in array1:

        key = r['KEY']
        orig_key = r['KEY']

        # We have no choice, we cannot use NodeSet, so we use the
        # simple regexp
        if NodeSet is None:
            m = pat.search(key)
            got_xy = (m is not None)
        else:  # Try to look with a nodeset check directly
            try:
                ns = NodeSet(str(key))
                # If we have more than 1 element, we have a xy thing
                got_xy = (len(ns) != 1)
            except NodeSetParseRangeError:
                return (None, GET_KEY_VALUE_SEQUENCE_ERROR_NODE)
                pass  # go in the next key

        # Now we've got our couples of X-Y. If no void,
        # we were with a "key generator"

        if got_xy:
            # Ok 2 cases: we have the NodeSet lib or not.
            # if not, we use the dumb algo (quick, but manage less
            # cases like /N or , in patterns)
            if NodeSet is None:  # us the old algo
                still_loop = True
                xy_couples = []  # will get all X-Y couples
                while still_loop:
                    m = pat.search(key)
                    if m is not None:  # we've find one X-Y
                        (x, y) = m.groups()
                        (x, y) = (int(x), int(y))
                        xy_couples.append((x, y))
                        # We must search if we've gotother X-Y, so
                        # we delete this one, and loop
                        key = key.replace('[%d-%d]' % (x, y), 'Z' * 10)
                    else:  # no more X-Y in it
                        still_loop = False

                # Now we have our xy_couples, we can manage them

                # We search all pattern change rules
                rules = got_generation_rule_pattern_change(xy_couples)

                # Then we apply them all to get ours final keys
                for rule in rules:
                    res = apply_change_recursive_pattern_change(orig_key, rule)
                    new_r = {}
                    for key in r:
                        new_r[key] = r[key]
                    new_r['KEY'] = res
                    array2.append(new_r)

            else:
                # The key was just a generator, we can remove it
                # keys_to_del.append(orig_key)

                # We search all pattern change rules
                #rules = got_generation_rule_pattern_change(xy_couples)
                nodes_set = expand_xy_pattern(orig_key)
                new_keys = list(nodes_set)

                # Then we apply them all to get ours final keys
                for new_key in new_keys:
                #res = apply_change_recursive_pattern_change(orig_key, rule)
                    new_r = {}
                    for key in r:
                        new_r[key] = r[key]
                    new_r['KEY'] = new_key
                    array2.append(new_r)
        else:
            # There were no wildcards
            array2.append(r)
    #t1 = time.time()
    #print "***********Diff", t1 -t0

    return (array2, GET_KEY_VALUE_SEQUENCE_ERROR_NOERROR)


############################### Files management #######################
# We got a file like /tmp/toto/toto2/bob.png And we want to be sure the dir
# /tmp/toto/toto2/ will really exists so we can copy it. Try to make if if need
# and return True/False if succeed
def expect_file_dirs(root, path):
    dirs = os.path.normpath(path).split('/')
    dirs = [d for d in dirs if d != '']
    # We will create all directory until the last one
    # so we are doing a mkdir -p .....
    # TODO: and windows????
    tmp_dir = root
    for d in dirs:
        _d = os.path.join(tmp_dir, d)
        logger.info('Verify the existence of file %s' % (_d))
        if not os.path.exists(_d):
            try:
                os.mkdir(_d)
            except:
                return False
        tmp_dir = _d
    return True


######################## Services/hosts search filters  #######################
# Filters used in services or hosts find_by_filter method
# Return callback functions which are passed host or service instances, and
# should return a boolean value that indicates if the inscance mached the
# filter
def filter_any(name):

    def inner_filter(host):
        return True

    return inner_filter


def filter_none(name):

    def inner_filter(host):
        return False

    return inner_filter


def filter_host_by_name(name):

    def inner_filter(host):
        if host is None:
            return False
        return host.host_name == name

    return inner_filter


def filter_host_by_regex(regex):
    host_re = re.compile(regex)

    def inner_filter(host):
        if host is None:
            return False
        return host_re.match(host.host_name) is not None

    return inner_filter


def filter_host_by_group(group):

    def inner_filter(host):
        if host is None:
            return False
        return group in [g.hostgroup_name for g in host.hostgroups]

    return inner_filter


def filter_host_by_template(tpl):

    def inner_filter(host):
        if host is None:
            return False
        return tpl in [t.strip() for t in host.use]

    return inner_filter



def filter_service_by_name(name):

    def inner_filter(service):
        if service is None:
            return False
        return service.service_description == name

    return inner_filter


def filter_service_by_regex_name(regex):
    host_re = re.compile(regex)

    def inner_filter(service):
        if service is None:
            return False
        return host_re.match(service.service_description) is not None

    return inner_filter


def filter_service_by_host_name(host_name):

    def inner_filter(service):
        if service is None or service.host is None:
            return False
        return service.host.host_name == host_name

    return inner_filter


def filter_service_by_regex_host_name(regex):
    host_re = re.compile(regex)

    def inner_filter(service):
        if service is None or service.host is None:
            return False
        return host_re.match(service.host.host_name) is not None

    return inner_filter


def filter_service_by_hostgroup_name(group):

    def inner_filter(service):
        if service is None or service.host is None:
            return False
        return group in [g.hostgroup_name for g in service.host.hostgroups]

    return inner_filter


def filter_service_by_host_template_name(tpl):

    def inner_filter(service):
        if service is None or service.host is None:
            return False
        return tpl in [t.strip() for t in service.host.use]
        

    return inner_filter


def filter_service_by_servicegroup_name(group):

    def inner_filter(service):
        if service is None:
            return False
        return group in [g.servicegroup_name for g in service.servicegroups]

    return inner_filter


def filter_host_by_bp_rule_label(label):

    def inner_filter(host):
        if host is None:
            return False
        return label in host.labels

    return inner_filter


def filter_service_by_host_bp_rule_label(label):

    def inner_filter(service):
        if service is None or service.host is None:
            return False
        return label in service.host.labels

    return inner_filter


def filter_service_by_bp_rule_label(label):
    def inner_filter(service):
        if service is None:
            return False
        return label in service.labels

    return inner_filter

########NEW FILE########
__FILENAME__ = bottlecore
# -*- coding: utf-8 -*-
"""
Bottle is a fast and simple micro-framework for small web applications. It
offers request dispatching (Routes) with url parameter support, templates,
a built-in HTTP Server and adapters for many third party WSGI/HTTP-server and
template engines - all in a single file and with no dependencies other than the
Python Standard Library.

Homepage and documentation: http://bottlepy.org/

Copyright (c) 2011, Marcel Hellkamp.
License: MIT (see LICENSE.txt for details)
"""

from __future__ import with_statement

__author__ = 'Marcel Hellkamp'
__version__ = '0.10.dev'
__license__ = 'MIT'

import base64
import cgi
import email.utils
import functools
import hmac
import httplib
import imp
import itertools
import mimetypes
import os
import re
import subprocess
import sys
import tempfile
import thread
import threading
import time
import warnings

from Cookie import SimpleCookie
from tempfile import TemporaryFile
from traceback import format_exc
from urllib import urlencode, quote as urlquote
from urlparse import urljoin, SplitResult as UrlSplitResult

try:
    from collections import MutableMapping as DictMixin
except ImportError:  # pragma: no cover
    from UserDict import DictMixin

try:
    from urlparse import parse_qs
except ImportError:  # pragma: no cover
    from cgi import parse_qs

try:
    import cPickle as pickle
except ImportError:  # pragma: no cover
    import pickle

try:
    from json import dumps as json_dumps, loads as json_lds
except ImportError:  # pragma: no cover
    try:
        from simplejson import dumps as json_dumps, loads as json_lds
    except ImportError:  # pragma: no cover
        try:
            from django.utils.simplejson import dumps as json_dumps, loads as json_lds
        except ImportError:  # pragma: no cover
            def json_dumps(data):
                raise ImportError("JSON support requires Python 2.6 or simplejson.")
            json_lds = json_dumps

py3k = sys.version_info >= (3, 0, 0)
NCTextIOWrapper = None

if py3k:  # pragma: no cover
    json_loads = lambda s: json_lds(touni(s))
    # See Request.POST
    from io import BytesIO


    def touni(x, enc='utf8', err='strict'):
        """ Convert anything to unicode """
        return str(x, enc, err) if isinstance(x, bytes) else str(x)
    if sys.version_info < (3, 2, 0):
        from io import TextIOWrapper


        class NCTextIOWrapper(TextIOWrapper):
            ''' Garbage collecting an io.TextIOWrapper(buffer) instance closes
                the wrapped buffer. This subclass keeps it open. '''

            def close(self): pass

else:
    json_loads = json_lds
    from StringIO import StringIO as BytesIO
    bytes = str


    def touni(x, enc='utf8', err='strict'):
        """ Convert anything to unicode """
        return x if isinstance(x, unicode) else unicode(str(x), enc, err)


def tob(data, enc='utf8'):
    """ Convert anything to bytes """
    return data.encode(enc) if isinstance(data, unicode) else bytes(data)

# Convert strings and unicode to native strings
if  py3k:
    tonat = touni
else:
    tonat = tob
tonat.__doc__ = """ Convert anything to native strings """


# Backward compatibility
def depr(message, critical=False):
    if critical:
        raise DeprecationWarning(message)
    warnings.warn(message, DeprecationWarning, stacklevel=3)


# Small helpers
def makelist(data):
    if isinstance(data, (tuple, list, set, dict)):
        return list(data)
    elif data:
        return [data]
    else:
        return []


class DictProperty(object):
    ''' Property that maps to a key in a local dict-like attribute. '''

    def __init__(self, attr, key=None, read_only=False):
        self.attr, self.key, self.read_only = attr, key, read_only

    def __call__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter, self.key = func, self.key or func.__name__
        return self

    def __get__(self, obj, cls):
        if obj is None:
            return self
        key, storage = self.key, getattr(obj, self.attr)
        if key not in storage:
            storage[key] = self.getter(obj)
        return storage[key]

    def __set__(self, obj, value):
        if self.read_only:
            raise AttributeError("Read-Only property.")
        getattr(obj, self.attr)[self.key] = value

    def __delete__(self, obj):
        if self.read_only:
            raise AttributeError("Read-Only property.")
        del getattr(obj, self.attr)[self.key]


def cached_property(func):
    ''' A property that, if accessed, replaces itself with the computed
        value. Subsequent accesses won't call the getter again. '''
    return DictProperty('__dict__')(func)


class lazy_attribute(object):  # Does not need configuration -> lower-case name
    ''' A property that caches itself to the class object. '''

    def __init__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter = func

    def __get__(self, obj, cls):
        value = self.getter(cls)
        setattr(cls, self.__name__, value)
        return value


class HeaderProperty(object):
    def __init__(self, name, reader=None, writer=str, default=''):
        self.name, self.reader, self.writer, self.default = name, reader, writer, default
        self.__doc__ = 'Current value of the %r header.' % name.title()

    def __get__(self, obj, cls):
        if obj is None:
            return self
        value = obj.headers.get(self.name)
        return self.reader(value) if (value and self.reader) else (value or self.default)

    def __set__(self, obj, value):
        if self.writer:
            value = self.writer(value)
        obj.headers[self.name] = value

    def __delete__(self, obj):
        if self.name in obj.headers:
            del obj.headers[self.name]

###############################################################################
# Exceptions and Events ########################################################
###############################################################################

class BottleException(Exception):
    """ A base class for exceptions used by bottle. """
    pass


class HTTPResponse(BottleException):
    """ Used to break execution and immediately finish the response """

    def __init__(self, output='', status=200, header=None):
        super(BottleException, self).__init__("HTTP Response %d" % status)
        self.status = int(status)
        self.output = output
        self.headers = HeaderDict(header) if header else None

    def apply(self, response):
        if self.headers:
            for key, value in self.headers.iterallitems():
                response.headers[key] = value
        response.status = self.status


class HTTPError(HTTPResponse):
    """ Used to generate an error page """

    def __init__(self, code=500, output='Unknown Error', exception=None,
                 traceback=None, header=None):
        super(HTTPError, self).__init__(output, code, header)
        self.exception = exception
        self.traceback = traceback

    def __repr__(self):
        return template(ERROR_PAGE_TEMPLATE, e=self)

###############################################################################
# Routing ######################################################################
###############################################################################

class RouteError(BottleException):
    """ This is a base class for all routing related exceptions """


class RouteReset(BottleException):
    """ If raised by a plugin or request handler, the route is reset and all
        plugins are re-applied. """


class RouteSyntaxError(RouteError):
    """ The route parser found something not supported by this router """


class RouteBuildError(RouteError):
    """ The route could not been built """


class Router(object):
    ''' A Router is an ordered collection of route->target pairs. It is used to
        efficiently match WSGI requests against a number of routes and return
        the first target that satisfies the request. The target may be anything,
        usually a string, ID or callable object. A route consists of a path-rule
        and a HTTP method.

        The path-rule is either a static path (e.g. `/contact`) or a dynamic
        path that contains wildcards (e.g. `/wiki/:page`). By default, wildcards
        consume characters up to the next slash (`/`). To change that, you may
        add a regular expression pattern (e.g. `/wiki/:page#[a-z]+#`).

        For performance reasons, static routes (rules without wildcards) are
        checked first. Dynamic routes are searched in order. Try to avoid
        ambiguous or overlapping rules.

        The HTTP method string matches only on equality, with two exceptions:
          * GET routes also match HEAD requests if there is no appropriate
            HEAD route installed.
          * ANY routes do match if there is no other suitable route installed.

        An optional ``name`` parameter is used by :meth:`build` to identify
        routes.
    '''

    default = '[^/]+'

    @lazy_attribute
    def syntax(cls):
        return re.compile(r'(?<!\\):([a-zA-Z_][a-zA-Z_0-9]*)?(?:#(.*?)#)?')

    def __init__(self):
        self.routes = {}   # A {rule: {method: target}} mapping
        self.rules  = []   # An ordered list of rules
        self.named  = {}   # A name->(rule, build_info) mapping
        self.static = {}   # Cache for static routes: {path: {method: target}}
        self.dynamic = []  # Cache for dynamic routes. See _compile()

    def add(self, rule, method, target, name=None):
        ''' Add a new route or replace the target for an existing route. '''

        if rule in self.routes:
            self.routes[rule][method.upper()] = target
        else:
            self.routes[rule] = {method.upper(): target}
            self.rules.append(rule)
            if self.static or self.dynamic:  # Clear precompiler cache.
                self.static, self.dynamic = {}, {}
        if name:
            self.named[name] = (rule, None)

    def build(self, _name, *anon, **args):
        ''' Return a string that matches a named route. Use keyword arguments
            to fill out named wildcards. Remaining arguments are appended as a
            query string. Raises RouteBuildError or KeyError.'''
        if _name not in self.named:
            raise RouteBuildError("No route with that name.", _name)
        rule, pairs = self.named[_name]
        if not pairs:
            token = self.syntax.split(rule)
            parts = [p.replace('\\:', ':') for p in token[::3]]
            names = token[1::3]
            if len(parts) > len(names):
                names.append(None)
            pairs = zip(parts, names)
            self.named[_name] = (rule, pairs)
        try:
            anon = list(anon)
            url = [s if k is None
                   else s + str(args.pop(k)) if k else s + str(anon.pop())
                   for s, k in pairs]
        except IndexError:
            msg = "Not enough arguments to fill out anonymous wildcards."
            raise RouteBuildError(msg)
        except KeyError, e:
            raise RouteBuildError(*e.args)

        if args:
            url += ['?', urlencode(args)]
        return ''.join(url)

    def match(self, environ):
        ''' Return a (target, url_agrs) tuple or raise HTTPError(404/405). '''
        targets, urlargs = self._match_path(environ)
        if not targets:
            raise HTTPError(404, "Not found: " + repr(environ['PATH_INFO']))
        method = environ['REQUEST_METHOD'].upper()
        if method in targets:
            return targets[method], urlargs
        if method == 'HEAD' and 'GET' in targets:
            return targets['GET'], urlargs
        if 'ANY' in targets:
            return targets['ANY'], urlargs
        allowed = [verb for verb in targets if verb != 'ANY']
        if 'GET' in allowed and 'HEAD' not in allowed:
            allowed.append('HEAD')
        raise HTTPError(405, "Method not allowed.",
                        header=[('Allow', ",".join(allowed))])

    def _match_path(self, environ):
        ''' Optimized PATH_INFO matcher. '''
        path = environ['PATH_INFO'] or '/'
        # Assume we are in a warm state. Search compiled rules first.
        match = self.static.get(path)
        if match:
            return match, {}
        for combined, rules in self.dynamic:
            match = combined.match(path)
            if not match:
                continue
            gpat, match = rules[match.lastindex - 1]
            return match, gpat(path).groupdict() if gpat else {}

        # Lazy-check if we are really in a warm state. If yes, stop here.
        if self.static or self.dynamic or not self.routes:
            return None, {}

        # Cold state: We have not compiled any rules yet. Do so and try again.
        if not environ.get('wsgi.run_once'):
            self._compile()
            return self._match_path(environ)

        # For run_once (CGI) environments, don't compile. Just check one by one.
        epath = path.replace(':', '\\:')  # Turn path into its own static rule.
        match = self.routes.get(epath)  # This returns static rule only.
        if match:
            return match, {}
        for rule in self.rules:
            #: Skip static routes to reduce re.compile() calls.
            if rule.count(':') < rule.count('\\:'):
                continue
            match = self._compile_pattern(rule).match(path)
            if match:
                return self.routes[rule], match.groupdict()
        return None, {}

    def _compile(self):
        ''' Prepare static and dynamic search structures. '''
        self.static = {}
        self.dynamic = []

        def fpat_sub(m):
            return m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:'
        for rule in self.rules:
            target = self.routes[rule]
            if not self.syntax.search(rule):
                self.static[rule.replace('\\:', ':')] = target
                continue
            gpat = self._compile_pattern(rule)
            fpat = re.sub(r'(\\*)(\(\?P<[^>]*>|\((?!\?))', fpat_sub, gpat.pattern)
            gpat = gpat.match if gpat.groupindex else None
            try:
                combined = '%s|(%s)' % (self.dynamic[-1][0].pattern, fpat)
                self.dynamic[-1] = (re.compile(combined), self.dynamic[-1][1])
                self.dynamic[-1][1].append((gpat, target))
            except (AssertionError, IndexError), e:  # AssertionError: Too many groups
                self.dynamic.append((re.compile('(^%s$)' % fpat),
                                    [(gpat, target)]))
            except re.error, e:
                raise RouteSyntaxError("Could not add Route: %s (%s)" % (rule, e))

    def _compile_pattern(self, rule):
        ''' Return a regular expression with named groups for each wildcard. '''
        out = ''
        for i, part in enumerate(self.syntax.split(rule)):
            if i % 3 == 0:
                out += re.escape(part.replace('\\:', ':'))
            elif i % 3 == 1:
                out += '(?P<%s>' % part if part else '(?:'
            else:
                out += '%s)' % (part or '[^/]+')
        return re.compile('^%s$' % out)

###############################################################################
# Application Object ###########################################################
###############################################################################

class Bottle(object):
    """ WSGI application """

    def __init__(self, catchall=True, autojson=True, config=None):
        """ Create a new bottle instance.
            You usually don't do that. Use `bottle.app.push()` instead.
        """
        self.routes = []  # List of installed routes including metadata.
        self.router = Router()  # Maps requests to self.route indices.
        self.ccache = {}  # Cache for callbacks with plugins applied.

        self.plugins = []  # List of installed plugins.

        self.mounts = {}
        self.error_handler = {}
        #: If true, most exceptions are caught and returned as :exc:`HTTPError`
        self.catchall = catchall
        self.config = config or {}
        self.serve = True
        # Default plugins
        self.hooks = self.install(HooksPlugin())
        if autojson:
            self.install(JSONPlugin())
        self.install(TemplatePlugin())

    def mount(self, app, prefix, **options):
        ''' Mount an application to a specific URL prefix. The prefix is added
            to SCIPT_PATH and removed from PATH_INFO before the sub-application
            is called.:param app: an instance of :class:`Bottle`.:param prefix: path prefix used as a mount-point.

            All other parameters are passed to the underlying :meth:`route` call.
        '''
        if not isinstance(app, Bottle):
            raise TypeError('Only Bottle instances are supported for now.')
        prefix = '/'.join(filter(None, prefix.split('/')))
        if not prefix:
            raise TypeError('Empty prefix. Perhaps you want a merge()?')
        for other in self.mounts:
            if other.startswith(prefix):
                raise TypeError('Conflict with existing mount: %s' % other)
        path_depth = prefix.count('/') + 1
        options.setdefault('method', 'ANY')
        options.setdefault('skip', True)
        self.mounts[prefix] = app

        @self.route('/%s/:#.*#' % prefix, **options)
        def mountpoint():
            request.path_shift(path_depth)
            return app._handle(request.environ)

    def install(self, plugin):
        ''' Add a plugin to the list of plugins and prepare it for being
            applied to all routes of this application. A plugin may be a simple
            decorator or an object that implements the :class:`Plugin` API.
        '''
        if hasattr(plugin, 'setup'):
            plugin.setup(self)
        if not callable(plugin) and not hasattr(plugin, 'apply'):
            raise TypeError("Plugins must be callable or implement .apply()")
        self.plugins.append(plugin)
        self.reset()
        return plugin

    def uninstall(self, plugin):
        ''' Uninstall plugins. Pass an instance to remove a specific plugin.
            Pass a type object to remove all plugins that match that type.
            Subclasses are not removed. Pass a string to remove all plugins with
            a matching ``name`` attribute. Pass ``True`` to remove all plugins.
            The list of affected plugins is returned. '''
        removed, remove = [], plugin
        for i, plugin in list(enumerate(self.plugins))[::-1]:
            if remove is True or remove is plugin or remove is type(plugin) \
            or getattr(plugin, 'name', True) == remove:
                removed.append(plugin)
                del self.plugins[i]
                if hasattr(plugin, 'close'):
                    plugin.close()
        if removed:
            self.reset()
        return removed

    def reset(self, id=None):
        ''' Reset all routes (force plugins to be re-applied) and clear all
            caches. If an ID is given, only that specific route is affected. '''
        if id is None:
            self.ccache.clear()
        else:
            self.ccache.pop(id, None)
        if DEBUG:
            for route in self.routes:
                if route['id'] not in self.ccache:
                    self.ccache[route['id']] = self._build_callback(route)

    def close(self):
        ''' Close the application and all installed plugins. '''
        for plugin in self.plugins:
            if hasattr(plugin, 'close'):
                plugin.close()
        self.stopped = True

    def match(self, environ):
        """ (deprecated) Search for a matching route and return a
            (callback, urlargs) tuple.
            The first element is the associated route callback with plugins
            applied. The second value is a dictionary with parameters extracted
            from the URL. The :class:`Router` raises :exc:`HTTPError` (404/405)
            on a non-match."""
        depr("This method will change semantics in 0.10.")
        return self._match(environ)

    def _match(self, environ):
        handle, args = self.router.match(environ)
        environ['route.handle'] = handle  # TODO move to router?
        environ['route.url_args'] = args
        try:
            return self.ccache[handle], args
        except KeyError:
            config = self.routes[handle]
            callback = self.ccache[handle] = self._build_callback(config)
            return callback, args

    def _build_callback(self, config):
        ''' Apply plugins to a route and return a new callable. '''
        wrapped = config['callback']
        plugins = self.plugins + config['apply']
        skip = config['skip']
        try:
            for plugin in reversed(plugins):
                if True in skip:
                    break
                if plugin in skip or type(plugin) in skip:
                    continue
                if getattr(plugin, 'name', True) in skip:
                    continue
                if hasattr(plugin, 'apply'):
                    wrapped = plugin.apply(wrapped, config)
                else:
                    wrapped = plugin(wrapped)
                if not wrapped:
                    break
                functools.update_wrapper(wrapped, config['callback'])
            return wrapped
        except RouteReset:  # A plugin may have changed the config dict inplace.
            return self._build_callback(config)  # Apply all plugins again.

    def get_url(self, routename, **kargs):
        """ Return a string that matches a named route """
        scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'
        location = self.router.build(routename, **kargs).lstrip('/')
        return urljoin(urljoin('/', scriptname), location)

    def route(self, path=None, method='GET', callback=None, name=None,
              apply=None, skip=None, **config):
        """ A decorator to bind a function to a request URL. Example::

                @app.route('/hello/:name')
                def hello(name):
                    return 'Hello %s' % name

            The ``:name`` part is a wildcard. See :class:`Router` for syntax
            details.:param path: Request path or a list of paths to listen to. If no
              path is specified, it is automatically generated from the
              signature of the function.:param method: HTTP method (`GET`, `POST`, `PUT`, ...) or a list of
              methods to listen to. (default: `GET`):param callback: An optional shortcut to avoid the decorator
              syntax. ``route(..., callback=func)`` equals ``route(...)(func)``:param name: The name for this route. (default: None):param apply: A decorator or plugin or a list of plugins. These are
              applied to the route callback in addition to installed plugins.:param skip: A list of plugins, plugin classes or names. Matching
              plugins are not installed to this route. ``True`` skips all.

            Any additional keyword arguments are stored as route-specific
            configuration and passed to plugins (see :meth:`Plugin.apply`).
        """
        if callable(path):
            path, callback = None, path

        plugins = makelist(apply)
        skiplist = makelist(skip)

        def decorator(callback):
            for rule in makelist(path) or yieldroutes(callback):
                for verb in makelist(method):
                    verb = verb.upper()
                    cfg = dict(rule=rule, method=verb, callback=callback,
                               name=name, app=self, config=config,
                               apply=plugins, skip=skiplist)
                    self.routes.append(cfg)
                    cfg['id'] = self.routes.index(cfg)
                    self.router.add(rule, verb, cfg['id'], name=name)
                    if DEBUG:
                        self.ccache[cfg['id']] = self._build_callback(cfg)
            return callback

        return decorator(callback) if callback else decorator

    def get(self, path=None, method='GET', **options):
        """ Equals :meth:`route`. """
        return self.route(path, method, **options)

    def post(self, path=None, method='POST', **options):
        """ Equals :meth:`route` with a ``POST`` method parameter. """
        return self.route(path, method, **options)

    def put(self, path=None, method='PUT', **options):
        """ Equals :meth:`route` with a ``PUT`` method parameter. """
        return self.route(path, method, **options)

    def delete(self, path=None, method='DELETE', **options):
        """ Equals :meth:`route` with a ``DELETE`` method parameter. """
        return self.route(path, method, **options)

    def error(self, code=500):
        """ Decorator: Register an output handler for a HTTP error code"""

        def wrapper(handler):
            self.error_handler[int(code)] = handler
            return handler
        return wrapper

    def hook(self, name):
        """ Return a decorator that attaches a callback to a hook. """

        def wrapper(func):
            self.hooks.add(name, func)
            return func
        return wrapper

    def handle(self, path, method='GET'):
        """ (deprecated) Execute the first matching route callback and return
            the result. :exc:`HTTPResponse` exceptions are caught and returned.
            If :attr:`Bottle.catchall` is true, other exceptions are caught as
            well and returned as :exc:`HTTPError` instances (500).
        """
        depr("This method will change semantics in 0.10. Try to avoid it.")
        if isinstance(path, dict):
            return self._handle(path)
        return self._handle({'PATH_INFO': path, 'REQUEST_METHOD': method.upper()})

    def _handle(self, environ):
        if not self.serve:
            depr("Bottle.serve will be removed in 0.10.")
            return HTTPError(503, "Server stopped")
        try:
            callback, args = self._match(environ)
            return callback(**args)
        except HTTPResponse, r:
            return r
        except RouteReset:  # Route reset requested by the callback or a plugin.
            del self.ccache[environ['route.handle']]
            return self._handle(environ)  # Try again.
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except Exception, e:
            if not self.catchall:
                raise
            stacktrace = format_exc(10)
            environ['wsgi.errors'].write(stacktrace)
            return HTTPError(500, "Internal Server Error", e, stacktrace)

    def _cast(self, out, request, response, peek=None):
        """ Try to convert the parameter into something WSGI compatible and set
        correct HTTP headers when possible.
        Support: False, str, unicode, dict, HTTPResponse, HTTPError, file-like,
        iterable of strings and iterable of unicodes
        """

        # Empty output is done here
        if not out:
            response['Content-Length'] = 0
            return []
        # Join lists of byte or unicode strings. Mixed lists are NOT supported
        if isinstance(out, (tuple, list))\
        and isinstance(out[0], (bytes, unicode)):
            out = out[0][0:0].join(out)  # b'abc'[0:0] -> b''
        # Encode unicode strings
        if isinstance(out, unicode):
            out = out.encode(response.charset)
        # Byte Strings are just returned
        if isinstance(out, bytes):
            response['Content-Length'] = len(out)
            return [out]
        # HTTPError or HTTPException (recursive, because they may wrap anything)
        # TODO: Handle these explicitly in handle() or make them iterable.
        if isinstance(out, HTTPError):
            out.apply(response)
            out = self.error_handler.get(out.status, repr)(out)
            if isinstance(out, HTTPResponse):
                depr('Error handlers must not return :exc:`HTTPResponse`.')  # 0.9
            return self._cast(out, request, response)
        if isinstance(out, HTTPResponse):
            out.apply(response)
            return self._cast(out.output, request, response)

        # File-like objects.
        if hasattr(out, 'read'):
            if 'wsgi.file_wrapper' in request.environ:
                return request.environ['wsgi.file_wrapper'](out)
            elif hasattr(out, 'close') or not hasattr(out, '__iter__'):
                return WSGIFileWrapper(out)

        # Handle Iterables. We peek into them to detect their inner type.
        try:
            out = iter(out)
            first = out.next()
            while not first:
                first = out.next()
        except StopIteration:
            return self._cast('', request, response)
        except HTTPResponse, e:
            first = e
        except Exception, e:
            first = HTTPError(500, 'Unhandled exception', e, format_exc(10))
            if isinstance(e, (KeyboardInterrupt, SystemExit, MemoryError))\
            or not self.catchall:
                raise
        # These are the inner types allowed in iterator or generator objects.
        if isinstance(first, HTTPResponse):
            return self._cast(first, request, response)
        if isinstance(first, bytes):
            return itertools.chain([first], out)
        if isinstance(first, unicode):
            return itertools.imap(lambda x: x.encode(response.charset),
                                  itertools.chain([first], out))
        return self._cast(HTTPError(500, 'Unsupported response type: %s'\
                                         % type(first)), request, response)

    def wsgi(self, environ, start_response):
        """ The bottle WSGI-interface. """
        try:
            environ['bottle.app'] = self
            request.bind(environ)
            response.bind()
            out = self._cast(self._handle(environ), request, response)
            # rfc2616 section 4.3
            if response.status_code in (100, 101, 204, 304)\
            or request.method == 'HEAD':
                if hasattr(out, 'close'):
                    out.close()
                out = []
            start_response(response.status_line, list(response.iter_headers()))
            return out
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except Exception, e:
            if not self.catchall:
                raise
            err = '<h1>Critical error while processing request: %s</h1>' \
                  % environ.get('PATH_INFO', '/')
            if DEBUG:
                err += '<h2>Error:</h2>\n<pre>%s</pre>\n' % repr(e)
                err += '<h2>Traceback:</h2>\n<pre>%s</pre>\n' % format_exc(10)
            environ['wsgi.errors'].write(err)  # TODO: wsgi.error should not get html
            start_response('500 INTERNAL SERVER ERROR', [('Content-Type', 'text/html')])
            return [tob(err)]

    def __call__(self, environ, start_response):
        return self.wsgi(environ, start_response)

###############################################################################
# HTTP and WSGI Tools ##########################################################
###############################################################################

class BaseRequest(DictMixin):
    """ A wrapper for WSGI environment dictionaries that adds a lot of
        convenient access methods and properties. Most of them are read-only."""

    #: Maximum size of memory buffer for :attr:`body` in bytes.
    # SHINKEN: *1000
    MEMFILE_MAX = 1024000000

    def __init__(self, environ):
        """ Wrap a WSGI environ dictionary. """
        #: The wrapped WSGI environ dictionary. This is the only real attribute.
        #: All other attributes actually are read-only properties.
        self.environ = environ
        environ['bottle.request'] = self

    @property
    def path(self):
        ''' The value of ``PATH_INFO`` with exactly one prefixed slash (to fix
            broken clients and avoid the "empty path" edge case). '''
        return '/' + self.environ.get('PATH_INFO', '').lstrip('/')

    @property
    def method(self):
        ''' The ``REQUEST_METHOD`` value as an uppercase string. '''
        return self.environ.get('REQUEST_METHOD', 'GET').upper()

    @DictProperty('environ', 'bottle.request.headers', read_only=True)
    def headers(self):
        ''' A :class:`WSGIHeaderDict` that provides case-insensitive access to
            HTTP request headers. '''
        return WSGIHeaderDict(self.environ)

    @DictProperty('environ', 'bottle.request.cookies', read_only=True)
    def cookies(self):
        """ Cookies parsed into a dictionary. Signed cookies are NOT decoded.
            Use :meth:`get_cookie` if you expect signed cookies. """
        raw_dict = SimpleCookie(self.environ.get('HTTP_COOKIE', ''))
        cookies = {}
        for cookie in raw_dict.itervalues():
            cookies[cookie.key] = cookie.value
        return cookies

    def get_cookie(self, key, default=None, secret=None):
        """ Return the content of a cookie. To read a `Signed Cookie`, the
            `secret` must match the one used to create the cookie (see
            :meth:`BaseResponse.set_cookie`). If anything goes wrong (missing
            cookie or wrong signature), return a default value. """
        value = self.cookies.get(key)
        if secret and value:
            dec = cookie_decode(value, secret)  # (key, value) tuple or None
            return dec[1] if dec and dec[0] == key else default
        return value or default

    @DictProperty('environ', 'bottle.request.query', read_only=True)
    def query(self):
        ''' The :attr:`query_string` parsed into a :class:`MultiDict`. These
            values are sometimes called "URL arguments" or "GET parameters", but
            not to be confused with "URL wildcards" as they are provided by the
            :class:`Router`. '''
        data = parse_qs(self.query_string, keep_blank_values=True)
        get = self.environ['bottle.get'] = MultiDict()
        for key, values in data.iteritems():
            for value in values:
                get[key] = value
        return get

    @DictProperty('environ', 'bottle.request.forms', read_only=True)
    def forms(self):
        """ Form values parsed from an `url-encoded` or `multipart/form-data`
            encoded POST or PUT request body. The result is returned as a
            :class:`MultiDict`. All keys and values are strings. File uploads
            are stored separately in :attr:`files`. """
        forms = MultiDict()
        for name, item in self.POST.iterallitems():
            if not hasattr(item, 'filename'):
                forms[name] = item
        return forms

    @DictProperty('environ', 'bottle.request.params', read_only=True)
    def params(self):
        """ A :class:`MultiDict` with the combined values of :attr:`query` and
            :attr:`forms`. File uploads are stored in :attr:`files`. """
        params = MultiDict()
        for key, value in self.query.iterallitems():
            params[key] = value
        for key, value in self.forms.iterallitems():
            params[key] = value
        return params

    @DictProperty('environ', 'bottle.request.files', read_only=True)
    def files(self):
        """ File uploads parsed from an `url-encoded` or `multipart/form-data`
            encoded POST or PUT request body. The values are instances of
            :class:`cgi.FieldStorage`. The most important attributes are:

            filename
                The filename, if specified; otherwise None; this is the client
                side filename, *not* the file name on which it is stored (that's
                a temporary file you don't deal with)
            file
                The file(-like) object from which you can read the data.
            value
                The value as a *string*; for file uploads, this transparently
                reads the file every time you request the value. Do not do this
                on big files.
        """
        files = MultiDict()
        for name, item in self.POST.iterallitems():
            if hasattr(item, 'filename'):
                files[name] = item
        return files

    @DictProperty('environ', 'bottle.request.json', read_only=True)
    def json(self):
        ''' If the ``Content-Type`` header is ``application/json``, this
            property holds the parsed content of the request body. Only requests
            smaller than :attr:`MEMFILE_MAX` are processed to avoid memory
            exhaustion. '''
        if self.environ.get('CONTENT_TYPE') == 'application/json' \
        and 0 < self.content_length < self.MEMFILE_MAX:
            return json_loads(self.body.read(self.MEMFILE_MAX))
        return None

    @DictProperty('environ', 'bottle.request.body', read_only=True)
    def _body(self):
        maxread = max(0, self.content_length)
        stream = self.environ['wsgi.input']
        body = BytesIO() if maxread < self.MEMFILE_MAX else TemporaryFile(mode='w+b')
        while maxread > 0:
            part = stream.read(min(maxread, self.MEMFILE_MAX))
            if not part:
                break
            body.write(part)
            maxread -= len(part)
        self.environ['wsgi.input'] = body
        body.seek(0)
        return body

    @property
    def body(self):
        """ The HTTP request body as a seek-able file-like object. Depending on
            :attr:`MEMFILE_MAX`, this is either a temporary file or a
            :class:`io.BytesIO` instance. Accessing this property for the first
            time reads and replaces the ``wsgi.input`` environ variable.
            Subsequent accesses just do a `seek(0)` on the file object. """
        self._body.seek(0)
        return self._body

    #: An alias for :attr:`query`.
    GET = query

    @DictProperty('environ', 'bottle.request.post', read_only=True)
    def POST(self):
        """ The values of :attr:`forms` and :attr:`files` combined into a single
            :class:`MultiDict`. Values are either strings (form values) or
            instances of :class:`cgi.FieldStorage` (file uploads).
        """
        post = MultiDict()
        safe_env = {'QUERY_STRING': ''}  # Build a safe environment for cgi
        for key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):
            if key in self.environ:
                safe_env[key] = self.environ[key]
        if NCTextIOWrapper:
            fb = NCTextIOWrapper(self.body, encoding='ISO-8859-1', newline='\n')
        else:
            fb = self.body
        data = cgi.FieldStorage(fp=fb, environ=safe_env, keep_blank_values=True)
        for item in data.list or []:
            post[item.name] = item if item.filename else item.value
        return post

    @property
    def COOKIES(self):
        ''' Alias for :attr:`cookies` (deprecated). '''
        depr('BaseRequest.COOKIES was renamed to BaseRequest.cookies (lowercase).')
        return self.cookies

    @property
    def url(self):
        """ The full request URI including hostname and scheme. If your app
            lives behind a reverse proxy or load balancer and you get confusing
            results, make sure that the ``X-Forwarded-Host`` header is set
            correctly. """
        return self.urlparts.geturl()

    @DictProperty('environ', 'bottle.request.urlparts', read_only=True)
    def urlparts(self):
        ''' The :attr:`url` string as an :class:`urlparse.SplitResult` tuple.
            The tuple contains (scheme, host, path, query_string and fragment),
            but the fragment is always empty because it is not visible to the
            server. '''
        env = self.environ
        http = env.get('wsgi.url_scheme', 'http')
        host = env.get('HTTP_X_FORWARDED_HOST') or env.get('HTTP_HOST')
        if not host:
            # HTTP 1.1 requires a Host-header. This is for HTTP/1.0 clients.
            host = env.get('SERVER_NAME', '127.0.0.1')
            port = env.get('SERVER_PORT')
            if port and port != ('80' if http == 'http' else '443'):
                host += ':' + port
        path = urlquote(self.fullpath)
        return UrlSplitResult(http, host, path, env.get('QUERY_STRING'), '')

    @property
    def fullpath(self):
        """ Request path including :attr:`script_name` (if present). """
        return urljoin(self.script_name, self.path.lstrip('/'))

    @property
    def query_string(self):
        """ The raw :attr:`query` part of the URL (everything in between ``?``
            and ``#``) as a string. """
        return self.environ.get('QUERY_STRING', '')

    @property
    def script_name(self):
        ''' The initial portion of the URL's `path` that was removed by a higher
            level (server or routing middleware) before the application was
            called. This property returns an empty string, or a path with
            leading and tailing slashes. '''
        script_name = self.environ.get('SCRIPT_NAME', '').strip('/')
        return '/' + script_name + '/' if script_name else '/'

    def path_shift(self, shift=1):
        ''' Shift path segments from :attr:`path` to :attr:`script_name` and
            vice versa.:param shift: The number of path segments to shift. May be negative
                         to change the shift direction. (default: 1)
        '''
        script = self.environ.get('SCRIPT_NAME', '/')
        self['SCRIPT_NAME'], self['PATH_INFO'] = path_shift(script, self.path, shift)

    @property
    def content_length(self):
        ''' The request body length as an integer. The client is responsible to
            set this header. Otherwise, the real length of the body is unknown
            and -1 is returned. In this case, :attr:`body` will be empty. '''
        return int(self.environ.get('CONTENT_LENGTH') or -1)

    @property
    def is_xhr(self):
        ''' True if the request was triggered by a XMLHttpRequest. This only
            works with JavaScript libraries that support the `X-Requested-With`
            header (most of the popular libraries do). '''
        requested_with = self.environ.get('HTTP_X_REQUESTED_WITH', '')
        return requested_with.lower() == 'xmlhttprequest'

    @property
    def is_ajax(self):
        ''' Alias for :attr:`is_xhr`. "Ajax" is not the right term. '''
        return self.is_xhr

    @property
    def auth(self):
        """ HTTP authentication data as a (user, password) tuple. This
            implementation currently supports basic (not digest) authentication
            only. If the authentication happened at a higher level (e.g. in the
            front web-server or a middleware), the password field is None, but
            the user field is looked up from the ``REMOTE_USER`` environ
            variable. On any errors, None is returned. """
        basic = parse_auth(self.environ.get('HTTP_AUTHORIZATION', ''))
        if basic:
            return basic
        ruser = self.environ.get('REMOTE_USER')
        if ruser:
            return (ruser, None)
        return None

    @property
    def remote_route(self):
        """ A list of all IPs that were involved in this request, starting with
            the client IP and followed by zero or more proxies. This does only
            work if all proxies support the ```X-Forwarded-For`` header. Note
            that this information can be forged by malicious clients. """
        proxy = self.environ.get('HTTP_X_FORWARDED_FOR')
        if proxy:
            return [ip.strip() for ip in proxy.split(',')]
        remote = self.environ.get('REMOTE_ADDR')
        return [remote] if remote else []

    @property
    def remote_addr(self):
        """ The client IP as a string. Note that this information can be forged
            by malicious clients. """
        route = self.remote_route
        return route[0] if route else None

    def copy(self):
        """ Return a new :class:`Request` with a shallow :attr:`environ` copy. """
        return Request(self.environ.copy())

    def __getitem__(self, key): return self.environ[key]
    def __delitem__(self, key): self[key] = ""; del(self.environ[key])
    def __iter__(self): return iter(self.environ)
    def __len__(self): return len(self.environ)
    def keys(self): return self.environ.keys()

    def __setitem__(self, key, value):
        """ Change an environ value and clear all caches that depend on it. """

        if self.environ.get('bottle.request.readonly'):
            raise KeyError('The environ dictionary is read-only.')

        self.environ[key] = value
        todelete = ()

        if key == 'wsgi.input':
            todelete = ('body', 'forms', 'files', 'params', 'post', 'json')
        elif key == 'QUERY_STRING':
            todelete = ('query', 'params')
        elif key.startswith('HTTP_'):
            todelete = ('headers', 'cookies')

        for key in todelete:
            self.environ.pop('bottle.request.' + key, None)


class LocalRequest(BaseRequest, threading.local):
    ''' A thread-local subclass of :class:`BaseRequest`. '''

    def __init__(self): pass
    bind = BaseRequest.__init__

Request = LocalRequest


def _hkey(s):
    return s.title().replace('_', '-')


class BaseResponse(object):
    """ Storage class for a response body as well as headers and cookies.

        This class does support dict-like case-insensitive item-access to
        headers, but is NOT a dict. Most notably, iterating over a response
        yields parts of the body and not the headers.
    """

    default_status = 200
    default_content_type = 'text/html; charset=UTF-8'

    #: Header blacklist for specific response codes
    #: (rfc2616 section 10.2.3 and 10.3.5)
    bad_headers = {
        204: set(('Content-Type',)),
        304: set(('Allow', 'Content-Encoding', 'Content-Language',
                  'Content-Length', 'Content-Range', 'Content-Type',
                  'Content-Md5', 'Last-Modified'))}

    def __init__(self, body='', status=None, **headers):
        #: The HTTP status code as an integer (e.g. 404).
        #: Do not change it directly, see :attr:`status`.
        self.status_code = None
        #: The HTTP status line as a string (e.g. "404 Not Found").
        #: Do not change it directly, see :attr:`status`.
        self.status_line = None
        #: The response body as one of the supported data types.
        self.body = body
        self._cookies = None
        self._headers = {'Content-Type': [self.default_content_type]}
        self.status = status or self.default_status
        if headers:
            for name, value in headers.items():
                self[name] = value

    def copy(self):
        ''' Returns a copy of self. '''
        copy = Response()
        copy.status = self.status
        copy._headers = dict((k, v[:]) for (k, v) in self._headers.items())
        return copy

    def __iter__(self):
        return iter(self.body)

    def close(self):
        if hasattr(self.body, 'close'):
            self.body.close()

    def _set_status(self, status):
        if isinstance(status, int):
            code, status = status, _HTTP_STATUS_LINES.get(status)
        elif ' ' in status:
            status = status.strip()
            code = int(status.split()[0])
        else:
            raise ValueError('String status line without a reason phrase.')
        if not 100 <= code <= 999:
            raise ValueError('Status code out of range.')
        self.status_code = code
        self.status_line = status or ('%d Unknown' % code)

    status = property(lambda self: self.status_code, _set_status, None,
        ''' A writeable property to change the HTTP response status. It accepts
            either a numeric code (100-999) or a string with a custom reason
            phrase (e.g. "404 Brain not found"). Both :data:`status_line` and
            :data:`status_line` are updates accordingly. The return value is
            always a numeric code. ''')
    del _set_status

    @property
    def headers(self):
        ''' An instance of :class:`HeaderDict`, a case-insensitive dict-like
            view on the response headers. '''
        self.__dict__['headers'] = hdict = HeaderDict()
        hdict.dict = self._headers
        return hdict

    def __contains__(self, name): return _hkey(name) in self._headers
    def __delitem__(self, name): del self._headers[_hkey(name)]
    def __getitem__(self, name): return self._headers[_hkey(name)][-1]
    def __setitem__(self, name, value): self._headers[_hkey(name)] = [str(value)]

    def get_header(self, name, default=None):
        ''' Return the value of a previously defined header. If there is no
            header with that name, return a default value. '''
        return self._headers.get(_hkey(name), [default])[-1]

    def set_header(self, name, value, append=False):
        ''' Create a new response header, replacing any previously defined
            headers with the same name. This equals ``response[name] = value``.:param append: Do not delete previously defined headers. This can
                           result in two (or more) headers having the same name.
        '''
        if append:
            self._headers.setdefault(_hkey(name), []).append(str(value))
        else:
            self._headers[_hkey(name)] = [str(value)]

    def iter_headers(self):
        ''' Yield (header, value) tuples, skipping headers that are not
            allowed with the current response status code. '''
        headers = self._headers.iteritems()
        bad_headers = self.bad_headers.get(self.status_code)
        if bad_headers:
            headers = (h for h in headers if h[0] not in bad_headers)
        for name, values in headers:
            for value in values:
                yield name, value
        if self._cookies:
            for c in self._cookies.values():
                yield 'Set-Cookie', c.OutputString()

    def wsgiheader(self):
        depr('The wsgiheader method is deprecated. See headerlist.')  # 0.10
        return self.headerlist

    @property
    def headerlist(self):
        ''' WSGI conform list of (header, value) tuples. '''
        return list(self.iter_headers())

    content_type = HeaderProperty('Content-Type')
    content_length = HeaderProperty('Content-Length', reader=int)

    @property
    def charset(self):
        """ Return the charset specified in the content-type header (default: utf8). """
        if 'charset=' in self.content_type:
            return self.content_type.split('charset=')[-1].split(';')[0].strip()
        return 'UTF-8'

    @property
    def COOKIES(self):
        """ A dict-like SimpleCookie instance. This should not be used directly.
            See :meth:`set_cookie`. """
        depr('The COOKIES dict is deprecated. Use `set_cookie()` instead.')  # 0.10
        if not self._cookies:
            self._cookies = SimpleCookie()
        return self._cookies

    def set_cookie(self, key, value, secret=None, **options):
        ''' Create a new cookie or replace an old one. If the `secret` parameter is
            set, create a `Signed Cookie` (described below).:param key: the name of the cookie.:param value: the value of the cookie.:param secret: a signature key required for signed cookies.

            Additionally, this method accepts all RFC 2109 attributes that are
            supported by :class:`cookie.Morsel`, including::param max_age: maximum age in seconds. (default: None):param expires: a datetime object or UNIX timestamp. (default: None):param domain: the domain that is allowed to read the cookie.
              (default: current domain):param path: limits the cookie to a given path (default: ``/``):param secure: limit the cookie to HTTPS connections (default: off).:param httponly: prevents client-side javascript to read this cookie
              (default: off, requires Python 2.6 or newer).

            If neither `expires` nor `max_age` is set (default), the cookie will
            expire at the end of the browser session (as soon as the browser
            window is closed).

            Signed cookies may store any pickle-able object and are
            cryptographically signed to prevent manipulation. Keep in mind that
            cookies are limited to 4kb in most browsers.

            Warning: Signed cookies are not encrypted (the client can still see
            the content) and not copy-protected (the client can restore an old
            cookie). The main intention is to make pickling and unpickling
            save, not to store secret information at client side.
        '''
        if not self._cookies:
            self._cookies = SimpleCookie()

        if secret:
            value = touni(cookie_encode((key, value), secret))
        elif not isinstance(value, basestring):
            raise TypeError('Secret key missing for non-string Cookie.')

        self._cookies[key] = value
        for k, v in options.iteritems():
            self._cookies[key][k.replace('_', '-')] = v

    def delete_cookie(self, key, **kwargs):
        ''' Delete a cookie. Be sure to use the same `domain` and `path`
            settings as used to create the cookie. '''
        kwargs['max_age'] = -1
        kwargs['expires'] = 0
        self.set_cookie(key, '', **kwargs)


class LocalResponse(BaseResponse, threading.local):
    ''' A thread-local subclass of :class:`BaseResponse`. '''
    bind = BaseResponse.__init__

Response = LocalResponse

###############################################################################
# Plugins ######################################################################
###############################################################################

class JSONPlugin(object):
    name = 'json'

    def __init__(self, json_dumps=json_dumps):
        self.json_dumps = json_dumps

    def apply(self, callback, context):
        dumps = self.json_dumps
        if not dumps:
            return callback

        def wrapper(*a, **ka):
            rv = callback(*a, **ka)
            if isinstance(rv, dict):
                response.content_type = 'application/json'
                return dumps(rv)
            return rv
        return wrapper


class HooksPlugin(object):
    name = 'hooks'

    def __init__(self):
        self.hooks = {'before_request': [], 'after_request': []}
        self.app = None

    def _empty(self):
        return not (self.hooks['before_request'] or self.hooks['after_request'])

    def setup(self, app):
        self.app = app

    def add(self, name, func):
        ''' Attach a callback to a hook. '''
        if name not in self.hooks:
            raise ValueError("Unknown hook name %s" % name)
        was_empty = self._empty()
        self.hooks[name].append(func)
        if self.app and was_empty and not self._empty():
            self.app.reset()

    def remove(self, name, func):
        ''' Remove a callback from a hook. '''
        if name not in self.hooks:
            raise ValueError("Unknown hook name %s" % name)
        was_empty = self._empty()
        self.hooks[name].remove(func)
        if self.app and not was_empty and self._empty():
            self.app.reset()

    def apply(self, callback, context):
        if self._empty():
            return callback
        before_request = self.hooks['before_request']
        after_request = self.hooks['after_request']

        def wrapper(*a, **ka):
            for hook in before_request:
                hook()
            rv = callback(*a, **ka)
            for hook in after_request[::-1]:
                hook()
            return rv
        return wrapper


class TypeFilterPlugin(object):
    def __init__(self):
        self.filter = []
        self.app = None

    def setup(self, app):
        self.app = app

    def add(self, ftype, func):
        if not isinstance(ftype, type):
            raise TypeError("Expected type object, got %s" % type(ftype))
        self.filter = [(t, f) for (t, f) in self.filter if t != ftype]
        self.filter.append((ftype, func))
        if len(self.filter) == 1 and self.app:
            self.app.reset()

    def apply(self, callback, context):
        filter = self.filter
        if not filter:
            return callback

        def wrapper(*a, **ka):
            rv = callback(*a, **ka)
            for testtype, filterfunc in filter:
                if isinstance(rv, testtype):
                    rv = filterfunc(rv)
            return rv
        return wrapper


class TemplatePlugin(object):
    ''' This plugin applies the :func:`view` decorator to all routes with a
        `template` config parameter. If the parameter is a tuple, the second
        element must be a dict with additional options (e.g. `template_engine`)
        or default variables for the template. '''
    name = 'template'

    def apply(self, callback, context):
        conf = context['config'].get('template')
        if isinstance(conf, (tuple, list)) and len(conf) == 2:
            return view(conf[0], **conf[1])(callback)
        elif isinstance(conf, str) and 'template_opts' in context['config']:
            depr('The `template_opts` parameter is deprecated.')  # 0.9
            return view(conf, **context['config']['template_opts'])(callback)
        elif isinstance(conf, str):
            return view(conf)(callback)
        else:
            return callback


#: Not a plugin, but part of the plugin API. TODO: Find a better place.
class _ImportRedirect(object):
    def __init__(self, name, impmask):
        ''' Create a virtual package that redirects imports (see PEP 302). '''
        self.name = name
        self.impmask = impmask
        self.module = sys.modules.setdefault(name, imp.new_module(name))
        self.module.__dict__.update({'__file__': '<virtual>', '__path__': [],
                                    '__all__': [], '__loader__': self})
        sys.meta_path.append(self)

    def find_module(self, fullname, path=None):
        if '.' not in fullname:
            return
        packname, modname = fullname.rsplit('.', 1)
        if packname != self.name:
            return
        return self

    def load_module(self, fullname):
        if fullname in sys.modules:
            return sys.modules[fullname]
        packname, modname = fullname.rsplit('.', 1)
        realname = self.impmask % modname
        __import__(realname)
        module = sys.modules[fullname] = sys.modules[realname]
        setattr(self.module, modname, module)
        module.__loader__ = self
        return module

###############################################################################
# Common Utilities #############################################################
###############################################################################

class MultiDict(DictMixin):
    """ This dict stores multiple values per key, but behaves exactly like a
        normal dict in that it returns only the newest value for any given key.
        There are special methods available to access the full list of values.
    """

    def __init__(self, *a, **k):
        self.dict = dict((k, [v]) for k, v in dict(*a, **k).iteritems())

    def __len__(self): return len(self.dict)
    def __iter__(self): return iter(self.dict)
    def __contains__(self, key): return key in self.dict
    def __delitem__(self, key): del self.dict[key]
    def __getitem__(self, key): return self.dict[key][-1]
    def __setitem__(self, key, value): self.append(key, value)
    def iterkeys(self): return self.dict.iterkeys()
    def itervalues(self): return (v[-1] for v in self.dict.itervalues())
    def iteritems(self): return ((k, v[-1]) for (k, v) in self.dict.iteritems())

    def iterallitems(self):
        for key, values in self.dict.iteritems():
            for value in values:
                yield key, value

    # 2to3 is not able to fix these automatically.
    keys     = iterkeys     if py3k else lambda self: list(self.iterkeys())
    values   = itervalues   if py3k else lambda self: list(self.itervalues())
    items    = iteritems    if py3k else lambda self: list(self.iteritems())
    allitems = iterallitems if py3k else lambda self: list(self.iterallitems())

    def get(self, key, default=None, index=-1):
        ''' Return the current value for a key. The third `index` parameter
            defaults to -1 (last value). '''
        if key in self.dict or default is KeyError:
            return self.dict[key][index]
        return default

    def append(self, key, value):
        ''' Add a new value to the list of values for this key. '''
        self.dict.setdefault(key, []).append(value)

    def replace(self, key, value):
        ''' Replace the list of values with a single value. '''
        self.dict[key] = [value]

    def getall(self, key):
        ''' Return a (possibly empty) list of values for a key. '''
        return self.dict.get(key) or []


class HeaderDict(MultiDict):
    """ A case-insensitive version of :class:`MultiDict` that defaults to
        replace the old value instead of appending it. """

    def __init__(self, *a, **ka):
        self.dict = {}
        if a or ka:
            self.update(*a, **ka)

    def __contains__(self, key): return _hkey(key) in self.dict
    def __delitem__(self, key): del self.dict[_hkey(key)]
    def __getitem__(self, key): return self.dict[_hkey(key)][-1]
    def __setitem__(self, key, value): self.dict[_hkey(key)] = [str(value)]

    def append(self, key, value):
        self.dict.setdefault(_hkey(key), []).append(str(value))

    def replace(self, key, value):
        self.dict[_hkey(key)] = [str(value)]

    def getall(self, key):
        return self.dict.get(_hkey(key)) or []

    def get(self, key, default=None, index=-1):
        return MultiDict.get(self, _hkey(key), default, index)

    def filter(self, names):
        for name in map(_hkey, names):
            if name in self.dict:
                del self.dict[name]


class WSGIHeaderDict(DictMixin):
    ''' This dict-like class wraps a WSGI environ dict and provides convenient
        access to HTTP_* fields. Keys and values are native strings
        (2.x bytes or 3.x unicode) and keys are case-insensitive. If the WSGI
        environment contains non-native string values, these are de- or encoded
        using a lossless 'latin1' character set.

        The API will remain stable even on changes to the relevant PEPs.
        Currently PEP 333, 444 and 3333 are supported. (PEP 444 is the only one
        that uses non-native strings.)
    '''
    #: List of keys that do not have a 'HTTP_' prefix.
    cgikeys = ('CONTENT_TYPE', 'CONTENT_LENGTH')

    def __init__(self, environ):
        self.environ = environ

    def _ekey(self, key):
        ''' Translate header field name to CGI/WSGI environ key. '''
        key = key.replace('-', '_').upper()
        if key in self.cgikeys:
            return key
        return 'HTTP_' + key

    def raw(self, key, default=None):
        ''' Return the header value as is (may be bytes or unicode). '''
        return self.environ.get(self._ekey(key), default)

    def __getitem__(self, key):
        return tonat(self.environ[self._ekey(key)], 'latin1')

    def __setitem__(self, key, value):
        raise TypeError("%s is read-only." % self.__class__)

    def __delitem__(self, key):
        raise TypeError("%s is read-only." % self.__class__)

    def __iter__(self):
        for key in self.environ:
            if key[:5] == 'HTTP_':
                yield key[5:].replace('_', '-').title()
            elif key in self.cgikeys:
                yield key.replace('_', '-').title()

    def keys(self): return list(self)
    def __len__(self): return len(list(self))
    def __contains__(self, key): return self._ekey(key) in self.environ


class AppStack(list):
    """ A stack-like list. Calling it returns the head of the stack. """

    def __call__(self):
        """ Return the current default application. """
        return self[-1]

    def push(self, value=None):
        """ Add a new :class:`Bottle` instance to the stack """
        if not isinstance(value, Bottle):
            value = Bottle()
        self.append(value)
        return value


class WSGIFileWrapper(object):

    def __init__(self, fp, buffer_size=1024*64):
        self.fp, self.buffer_size = fp, buffer_size
        for attr in ('fileno', 'close', 'read', 'readlines'):
            if hasattr(fp, attr):
                setattr(self, attr, getattr(fp, attr))

    def __iter__(self):
        read, buff = self.fp.read, self.buffer_size
        while True:
            part = read(buff)
            if not part:
                break
            yield part

###############################################################################
# Application Helper ###########################################################
###############################################################################

def abort(code=500, text='Unknown Error: Application stopped.'):
    """ Aborts execution and causes a HTTP error. """
    raise HTTPError(code, text)


def redirect(url, code=303):
    """ Aborts execution and causes a 303 redirect. """
    location = urljoin(request.url, url)
    raise HTTPResponse("", status=code, header=dict(Location=location))


def static_file(filename, root, mimetype='auto', download=False):
    """ Open a file in a safe way and return :exc:`HTTPResponse` with status
        code 200, 305, 401 or 404. Set Content-Type, Content-Encoding,
        Content-Length and Last-Modified header. Obey If-Modified-Since header
        and HEAD requests.
    """
    root = os.path.abspath(root) + os.sep
    filename = os.path.abspath(os.path.join(root, filename.strip('/\\')))
    header = dict()

    if not filename.startswith(root):
        return HTTPError(403, "Access denied.")
    if not os.path.exists(filename) or not os.path.isfile(filename):
        return HTTPError(404, "File does not exist.")
    if not os.access(filename, os.R_OK):
        return HTTPError(403, "You do not have permission to access this file.")

    if mimetype == 'auto':
        mimetype, encoding = mimetypes.guess_type(filename)
        if mimetype:
            header['Content-Type'] = mimetype
        if encoding:
            header['Content-Encoding'] = encoding
    elif mimetype:
        header['Content-Type'] = mimetype

    if download:
        download = os.path.basename(filename if download == True else download)
        header['Content-Disposition'] = 'attachment; filename="%s"' % download

    stats = os.stat(filename)
    header['Content-Length'] = stats.st_size
    lm = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(stats.st_mtime))
    header['Last-Modified'] = lm

    ims = request.environ.get('HTTP_IF_MODIFIED_SINCE')
    if ims:
        ims = parse_date(ims.split(";")[0].strip())
    if ims is not None and ims >= int(stats.st_mtime):
        header['Date'] = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime())
        return HTTPResponse(status=304, header=header)

    body = '' if request.method == 'HEAD' else open(filename, 'rb')
    return HTTPResponse(body, header=header)

###############################################################################
# HTTP Utilities and MISC (TODO) ###############################################
###############################################################################

def debug(mode=True):
    """ Change the debug level.
    There is only one debug level supported at the moment."""
    global DEBUG
    DEBUG = bool(mode)


def parse_date(ims):
    """ Parse rfc1123, rfc850 and asctime timestamps and return UTC epoch. """
    try:
        ts = email.utils.parsedate_tz(ims)
        return time.mktime(ts[:8] + (0,)) - (ts[9] or 0) - time.timezone
    except (TypeError, ValueError, IndexError, OverflowError):
        return None


def parse_auth(header):
    """ Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None"""
    try:
        method, data = header.split(None, 1)
        if method.lower() == 'basic':
            # TODO: Add 2to3 save base64[encode/decode] functions.
            user, pwd = touni(base64.b64decode(tob(data))).split(':', 1)
            return user, pwd
    except (KeyError, ValueError):
        return None


def _lscmp(a, b):
    ''' Compares two strings in a cryptographically save way:
        Runtime is not affected by length of common prefix. '''
    return not sum(0 if x==y else 1 for x, y in zip(a, b)) and len(a) == len(b)


def cookie_encode(data, key):
    ''' Encode and sign a pickle-able object. Return a (byte) string '''
    msg = base64.b64encode(pickle.dumps(data, -1))
    sig = base64.b64encode(hmac.new(key, msg).digest())
    return tob('!') + sig + tob('?') + msg


def cookie_decode(data, key):
    ''' Verify and decode an encoded string. Return an object or None.'''
    data = tob(data)
    if cookie_is_encoded(data):
        sig, msg = data.split(tob('?'), 1)
        if _lscmp(sig[1:], base64.b64encode(hmac.new(key, msg).digest())):
            return pickle.loads(base64.b64decode(msg))
    return None


def cookie_is_encoded(data):
    ''' Return True if the argument looks like a encoded cookie.'''
    return bool(data.startswith(tob('!')) and tob('?') in data)


def yieldroutes(func):
    """ Return a generator for routes that match the signature (name, args)
    of the func parameter. This may yield more than one route if the function
    takes optional keyword arguments. The output is best described by example::

        a()         -> '/a'
        b(x, y)     -> '/b/:x/:y'
        c(x, y=5)   -> '/c/:x' and '/c/:x/:y'
        d(x=5, y=6) -> '/d' and '/d/:x' and '/d/:x/:y'
    """
    import inspect  # Expensive module. Only import if necessary.
    path = '/' + func.__name__.replace('__', '/').lstrip('/')
    spec = inspect.getargspec(func)
    argc = len(spec[0]) - len(spec[3] or [])
    path += ('/:%s' * argc) % tuple(spec[0][:argc])
    yield path
    for arg in spec[0][argc:]:
        path += '/:%s' % arg
        yield path


def path_shift(script_name, path_info, shift=1):
    ''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.

        :return: The modified paths.:param script_name: The SCRIPT_NAME path.:param script_name: The PATH_INFO path.:param shift: The number of path fragments to shift. May be negative to
          change the shift direction. (default: 1)
    '''
    if shift == 0:
        return script_name, path_info
    pathlist = path_info.strip('/').split('/')
    scriptlist = script_name.strip('/').split('/')
    if pathlist and pathlist[0] == '':
        pathlist = []
    if scriptlist and scriptlist[0] == '':
        scriptlist = []
    if shift > 0 and shift <= len(pathlist):
        moved = pathlist[:shift]
        scriptlist = scriptlist + moved
        pathlist = pathlist[shift:]
    elif shift < 0 and shift >= -len(scriptlist):
        moved = scriptlist[shift:]
        pathlist = moved + pathlist
        scriptlist = scriptlist[:shift]
    else:
        empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'
        raise AssertionError("Cannot shift. Nothing left from %s" % empty)
    new_script_name = '/' + '/'.join(scriptlist)
    new_path_info = '/' + '/'.join(pathlist)
    if path_info.endswith('/') and pathlist:
        new_path_info += '/'
    return new_script_name, new_path_info

# Decorators
# TODO: Replace default_app() with app()

def validate(**vkargs):
    """
    Validates and manipulates keyword arguments by user defined callables.
    Handles ValueError and missing arguments by raising HTTPError(403).
    """

    def decorator(func):
        def wrapper(**kargs):
            for key, value in vkargs.iteritems():
                if key not in kargs:
                    abort(403, 'Missing parameter: %s' % key)
                try:
                    kargs[key] = value(kargs[key])
                except ValueError:
                    abort(403, 'Wrong parameter format for: %s' % key)
            return func(**kargs)
        return wrapper
    return decorator


def auth_basic(check, realm="private", text="Access denied"):
    ''' Callback decorator to require HTTP auth (basic).
        TODO: Add route(check_auth=...) parameter. '''

    def decorator(func):
        def wrapper(*a, **ka):
            user, password = request.auth or (None, None)
            if user is None or not check(user, password):
                response.headers['WWW-Authenticate'] = 'Basic realm="%s"' % realm
                return HTTPError(401, text)
            return func(*a, **ka)
        return wrapper
    return decorator


def make_default_app_wrapper(name):
    ''' Return a callable that relays calls to the current default app. '''

    @functools.wraps(getattr(Bottle, name))
    def wrapper(*a, **ka):
        return getattr(app(), name)(*a, **ka)
    return wrapper


for name in '''route get post put delete error mount
               hook install uninstall'''.split():
    globals()[name] = make_default_app_wrapper(name)
url = make_default_app_wrapper('get_url')
del name

###############################################################################
# Server Adapter ###############################################################
###############################################################################

class ServerAdapter(object):
    quiet = False

    def __init__(self, host='127.0.0.1', port=8080, **config):
        self.options = config
        self.host = host
        self.port = int(port)

    def run(self, handler):  # pragma: no cover
        pass

    def __repr__(self):
        args = ', '.join(['%s=%s' % (k, repr(v)) for k, v in self.options.items()])
        return "%s(%s)" % (self.__class__.__name__, args)


class CGIServer(ServerAdapter):
    quiet = True

    def run(self, handler):  # pragma: no cover
        from wsgiref.handlers import CGIHandler

        def fixed_environ(environ, start_response):
            environ.setdefault('PATH_INFO', '')
            return handler(environ, start_response)
        CGIHandler().run(fixed_environ)


class FlupFCGIServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        import flup.server.fcgi
        kwargs = {'bindAddress': (self.host, self.port)}
        kwargs.update(self.options)  # allow to override bindAddress and others
        flup.server.fcgi.WSGIServer(handler, **kwargs).run()

class FlupSCGIServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        import flup.server.scgi
        kwargs = {'bindAddress': (self.host, self.port)}
        kwargs.update(self.options)  # allow to override bindAddress and others
        flup.server.scgi.WSGIServer(handler, **kwargs).run()


class WSGIRefServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from wsgiref.simple_server import make_server, WSGIRequestHandler
        print "Launching Swsgi backend"
        if self.quiet:
            class QuietHandler(WSGIRequestHandler):
                def log_request(*args, **kw): pass
            self.options['handler_class'] = QuietHandler
        srv = make_server(self.host, self.port, handler, **self.options)
        srv.serve_forever()


## Shinken: add WSGIRefServerSelect
class WSGIRefServerSelect(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from wsgiref.simple_server import make_server, WSGIRequestHandler
        if self.quiet:
            class QuietHandler(WSGIRequestHandler):
                def log_request(*args, **kw): pass
            self.options['handler_class'] = QuietHandler
        srv = make_server(self.host, self.port, handler, **self.options)
        # srv.serve_forever()
        return srv


class CherryPyServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from cherrypy import wsgiserver
        print "Launching CherryPy backend"
        server = wsgiserver.CherryPyWSGIServer((self.host, self.port), handler)
        try:
            server.start()
        finally:
            server.stop()


class PasteServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from paste import httpserver
        print "Launching Paste backend"
        if not self.quiet:
            from paste.translogger import TransLogger
            handler = TransLogger(handler)
        httpserver.serve(handler, host=self.host, port=str(self.port),
                         **self.options)


class MeinheldServer(ServerAdapter):
    def run(self, handler):
        from meinheld import server
        server.listen((self.host, self.port))
        server.run(handler)


class FapwsServer(ServerAdapter):
    """ Extremely fast webserver using libev. See http://www.fapws.org/ """

    def run(self, handler):  # pragma: no cover
        import fapws._evwsgi as evwsgi
        from fapws import base, config
        port = self.port
        if float(config.SERVER_IDENT[-2:]) > 0.4:
            # fapws3 silently changed its API in 0.5
            port = str(port)
        evwsgi.start(self.host, port)
        # fapws3 never releases the GIL. Complain upstream. I tried. No luck.
        if 'BOTTLE_CHILD' in os.environ and not self.quiet:
            print "WARNING: Auto-reloading does not work with Fapws3."
            print "         (Fapws3 breaks python thread support)"
        evwsgi.set_base_module(base)

        def app(environ, start_response):
            environ['wsgi.multiprocess'] = False
            return handler(environ, start_response)
        evwsgi.wsgi_cb(('', app))
        evwsgi.run()


class TornadoServer(ServerAdapter):
    """ The super hyped asynchronous server by facebook. Untested. """

    def run(self, handler):  # pragma: no cover
        import tornado.wsgi
        import tornado.httpserver
        import tornado.ioloop
        container = tornado.wsgi.WSGIContainer(handler)
        server = tornado.httpserver.HTTPServer(container)
        server.listen(port=self.port)
        tornado.ioloop.IOLoop.instance().start()


class AppEngineServer(ServerAdapter):
    """ Adapter for Google App Engine. """
    quiet = True

    def run(self, handler):
        from google.appengine.ext.webapp import util
        # A main() function in the handler script enables 'App Caching'.
        # Lets makes sure it is there. This _really_ improves performance.
        module = sys.modules.get('__main__')
        if module and not hasattr(module, 'main'):
            module.main = lambda: util.run_wsgi_app(handler)
        util.run_wsgi_app(handler)


class TwistedServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from twisted.web import server, wsgi
        from twisted.python.threadpool import ThreadPool
        from twisted.internet import reactor
        thread_pool = ThreadPool()
        thread_pool.start()
        reactor.addSystemEventTrigger('after', 'shutdown', thread_pool.stop)
        factory = server.Site(wsgi.WSGIResource(reactor, thread_pool, handler))
        reactor.listenTCP(self.port, factory, interface=self.host)
        reactor.run()


class DieselServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from diesel.protocols.wsgi import WSGIApplication
        app = WSGIApplication(handler, port=self.port)
        app.run()


class GeventServer(ServerAdapter):
    """ Untested. Options:

        * `monkey` (default: True) fixes the stdlib to use greenthreads.
        * `fast` (default: False) uses libevent's http server, but has some
          issues: No streaming, no pipelining, no SSL.
    """

    def run(self, handler):
        from gevent import wsgi as wsgi_fast, pywsgi, monkey
        if self.options.get('monkey', True):
            monkey.patch_all()
        wsgi = wsgi_fast if self.options.get('fast') else pywsgi
        wsgi.WSGIServer((self.host, self.port), handler).serve_forever()


class GunicornServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from gunicorn.arbiter import Arbiter
        from gunicorn.config import Config
        handler.cfg = Config({'bind': "%s:%d" % (self.host, self.port), 'workers': 4})
        arbiter = Arbiter(handler)
        arbiter.run()


class EventletServer(ServerAdapter):
    """ Untested """

    def run(self, handler):
        from eventlet import wsgi, listen
        wsgi.server(listen((self.host, self.port)), handler)


class RocketServer(ServerAdapter):
    """ Untested. As requested in issue 63
        https://github.com/defnull/bottle/issues/#issue/63 """

    def run(self, handler):
        from rocket import Rocket
        server = Rocket((self.host, self.port), 'wsgi', {'wsgi_app': handler})
        server.start()


class BjoernServer(ServerAdapter):
    """ Screamingly fast server written in C: https://github.com/jonashaag/bjoern """

    def run(self, handler):
        from bjoern import run
        run(handler, self.host, self.port)


class AutoServer(ServerAdapter):
    """ Untested. """
    adapters = [PasteServer, CherryPyServer, TwistedServer, WSGIRefServer]

    def run(self, handler):
        for sa in self.adapters:
            try:
                return sa(self.host, self.port, **self.options).run(handler)
            except ImportError:
                pass

## Shinken: add 'wsgirefselect': WSGIRefServerSelect,
server_names = {
    'cgi': CGIServer,
    'flup': FlupFCGIServer,
    'flupscgi': FlupSCGIServer,
    'wsgiref': WSGIRefServer,
    'wsgirefselect': WSGIRefServerSelect,
    'cherrypy': CherryPyServer,
    'paste': PasteServer,
    'fapws3': FapwsServer,
    'tornado': TornadoServer,
    'gae': AppEngineServer,
#    'twisted': TwistedServer,
    'diesel': DieselServer,
    'meinheld': MeinheldServer,
    'gunicorn': GunicornServer,
    'eventlet': EventletServer,
    'gevent': GeventServer,
    'rocket': RocketServer,
    'bjoern': BjoernServer,
    'auto': AutoServer,
}

###############################################################################
# Application Control ##########################################################
###############################################################################

def _load(target, **vars):
    """ Fetch something from a module. The exact behavior depends on the
        target string:

        If the target is a valid python import path (e.g. `package.module`),
        the rightmost part is returned as a module object.
        If the target contains a colon (e.g. `package.module:var`) the module
        variable specified after the colon is returned.
        If the part after the colon contains any non-alphanumeric characters
        (e.g. `package.module:func(var)`) the result of the expression
        is returned. The expression has access to keyword arguments supplied
        to this function.

        Example::
        >>> _load('bottle')
        <module 'bottle' from 'bottle.py'>
        >>> _load('bottle:Bottle')
        <class 'bottle.Bottle'>
        >>> _load('bottle:cookie_encode(v, secret)', v='foo', secret='bar')
        '!F+hN4dQxaDJ4QxxaZ+Z3jw==?gAJVA2Zvb3EBLg=='

    """
    module, target = target.split(":", 1) if ':' in target else (target, None)
    if module not in sys.modules:
        __import__(module)
    if not target:
        return sys.modules[module]
    if target.isalnum():
        return getattr(sys.modules[module], target)
    package_name = module.split('.')[0]
    vars[package_name] = sys.modules[package_name]
    return eval('%s.%s' % (module, target), vars)


def load_app(target):
    """ Load a bottle application based on a target string and return the
        application object.

        If the target is an import path (e.g. package.module), the application
        stack is used to isolate the routes defined in that module.
        If the target contains a colon (e.g. package.module:myapp) the
        module variable specified after the colon is returned instead.
    """
    tmp = app.push()  # Create a new "default application"
    rv = _load(target)  # Import the target module
    app.remove(tmp)  # Remove the temporary added default application
    return rv if isinstance(rv, Bottle) else tmp


## Shinken: add the return of the server
def run(app=None, server='wsgiref', host='127.0.0.1', port=8080,
        interval=1, reloader=False, quiet=False, **kargs):
    """ Start a server instance. This method blocks until the server terminates.:param app: WSGI application or target string supported by
               :func:`load_app`. (default: :func:`default_app`):param server: Server adapter to use. See :data:`server_names` keys
               for valid names or pass a :class:`ServerAdapter` subclass.
               (default: `wsgiref`):param host: Server address to bind to. Pass ``0.0.0.0`` to listens on
               all interfaces including the external one. (default: 127.0.0.1):param port: Server port to bind to. Values below 1024 require root
               privileges. (default: 8080):param reloader: Start auto-reloading server? (default: False):param interval: Auto-reloader interval in seconds (default: 1):param quiet: Suppress output to stdout and stderr? (default: False):param options: Options passed to the server adapter.
     """
    # Shinken
    res = None
    app = app or default_app()
    if isinstance(app, basestring):
        app = load_app(app)
    if isinstance(server, basestring):
        server = server_names.get(server)
    if isinstance(server, type):
        server = server(host=host, port=port, **kargs)
    if not isinstance(server, ServerAdapter):
        raise RuntimeError("Server must be a subclass of ServerAdapter")
    server.quiet = server.quiet or quiet
    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):
        print "Bottle server starting up (using %s)..." % repr(server)
        print "Listening on http://%s:%d/" % (server.host, server.port)
        print "Use Ctrl-C to quit."
        print
    try:
        if reloader:
            interval = min(interval, 1)
            if os.environ.get('BOTTLE_CHILD'):
                _reloader_child(server, app, interval)
            else:
                _reloader_observer(server, app, interval)
        else:
            # Shinken
            res = server.run(app)
    except KeyboardInterrupt:
        pass
    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):
        print "Shutting down..."
    # Shinken
    return res


class FileCheckerThread(threading.Thread):
    ''' Thread that periodically checks for changed module files. '''

    def __init__(self, lockfile, interval):
        threading.Thread.__init__(self)
        self.lockfile, self.interval = lockfile, interval
        # 1: lockfile to old; 2: lockfile missing
        # 3: module file changed; 5: external exit
        self.status = 0

    def run(self):
        exists = os.path.exists
        mtime = lambda path: os.stat(path).st_mtime
        files = dict()
        for module in sys.modules.values():
            path = getattr(module, '__file__', '')
            if path[-4:] in ('.pyo', '.pyc'):
                path = path[:-1]
            if path and exists(path):
                files[path] = mtime(path)
        while not self.status:
            for path, lmtime in files.iteritems():
                if not exists(path) or mtime(path) > lmtime:
                    self.status = 3
            if not exists(self.lockfile):
                self.status = 2
            elif mtime(self.lockfile) < time.time() - self.interval - 5:
                self.status = 1
            if not self.status:
                time.sleep(self.interval)
        if self.status != 5:
            thread.interrupt_main()


def _reloader_child(server, app, interval):
    ''' Start the server and check for modified files in a background thread.
        As soon as an update is detected, KeyboardInterrupt is thrown in
        the main thread to exit the server loop. The process exists with status
        code 3 to request a reload by the observer process. If the lockfile
        is not modified in 2*interval second or missing, we assume that the
        observer process died and exit with status code 1 or 2.
    '''
    lockfile = os.environ.get('BOTTLE_LOCKFILE')
    bgcheck = FileCheckerThread(lockfile, interval)
    try:
        bgcheck.start()
        server.run(app)
    except KeyboardInterrupt:
        pass
    bgcheck.status, status = 5, bgcheck.status
    bgcheck.join()  # bgcheck.status == 5 --> silent exit
    if status:
        sys.exit(status)


def _reloader_observer(server, app, interval):
    ''' Start a child process with identical commandline arguments and restart
        it as long as it exists with status code 3. Also create a lockfile and
        touch it (update mtime) every interval seconds.
    '''
    fd, lockfile = tempfile.mkstemp(prefix='bottle-reloader.', suffix='.lock')
    os.close(fd)  # We only need this file to exist. We never write to it
    try:
        while os.path.exists(lockfile):
            args = [sys.executable] + sys.argv
            environ = os.environ.copy()
            environ['BOTTLE_CHILD'] = 'true'
            environ['BOTTLE_LOCKFILE'] = lockfile
            p = subprocess.Popen(args, env=environ)
            while p.poll() is None:  # Busy wait...
                os.utime(lockfile, None)  # I am alive!
                time.sleep(interval)
            if p.poll() != 3:
                if os.path.exists(lockfile):
                    os.unlink(lockfile)
                sys.exit(p.poll())
            elif not server.quiet:
                print "Reloading server..."
    except KeyboardInterrupt:
        pass
    if os.path.exists(lockfile):
        os.unlink(lockfile)

###############################################################################
# Template Adapters ############################################################
###############################################################################

class TemplateError(HTTPError):
    def __init__(self, message):
        HTTPError.__init__(self, 500, message)


class BaseTemplate(object):
    """ Base class and minimal API for template adapters """
    extentions = ['tpl', 'html', 'thtml', 'stpl']
    settings = {}  # used in prepare()
    defaults = {}  # used in render()

    def __init__(self, source=None, name=None, lookup=[], encoding='utf8', **settings):
        """ Create a new template.
        If the source parameter (str or buffer) is missing, the name argument
        is used to guess a template filename. Subclasses can assume that
        self.source and/or self.filename are set. Both are strings.
        The lookup, encoding and settings parameters are stored as instance
        variables.
        The lookup parameter stores a list containing directory paths.
        The encoding parameter should be used to decode byte strings or files.
        The settings parameter contains a dict for engine-specific settings.
        """
        self.name = name
        self.source = source.read() if hasattr(source, 'read') else source
        self.filename = source.filename if hasattr(source, 'filename') else None
        self.lookup = map(os.path.abspath, lookup)
        self.encoding = encoding
        self.settings = self.settings.copy()  # Copy from class variable
        self.settings.update(settings)  # Apply
        if not self.source and self.name:
            self.filename = self.search(self.name, self.lookup)
            if not self.filename:
                raise TemplateError('Template %s not found.' % repr(name))
        if not self.source and not self.filename:
            raise TemplateError('No template specified.')
        self.prepare(**self.settings)

    @classmethod
    def search(cls, name, lookup=[]):
        """ Search name in all directories specified in lookup.
        First without, then with common extensions. Return first hit. """
        if os.path.isfile(name):
            return name
        for spath in lookup:
            fname = os.path.join(spath, name)
            if os.path.isfile(fname):
                return fname
            for ext in cls.extentions:
                if os.path.isfile('%s.%s' % (fname, ext)):
                    return '%s.%s' % (fname, ext)

    @classmethod
    def global_config(cls, key, *args):
        ''' This reads or sets the global settings stored in class.settings. '''
        if args:
            cls.settings[key] = args[0]
        else:
            return cls.settings[key]

    def prepare(self, **options):
        """ Run preparations (parsing, caching, ...).
        It should be possible to call this again to refresh a template or to
        update settings.
        """
        raise NotImplementedError

    def render(self, *args, **kwargs):
        """ Render the template with the specified local variables and return
        a single byte or unicode string. If it is a byte string, the encoding
        must match self.encoding. This method must be thread-safe!
        Local variables may be provided in dictionaries (*args)
        or directly, as keywords (**kwargs).
        """
        raise NotImplementedError


class MakoTemplate(BaseTemplate):
    def prepare(self, **options):
        from mako.template import Template
        from mako.lookup import TemplateLookup
        options.update({'input_encoding': self.encoding})
        options.setdefault('format_exceptions', bool(DEBUG))
        lookup = TemplateLookup(directories=self.lookup, **options)
        if self.source:
            self.tpl = Template(self.source, lookup=lookup, **options)
        else:
            self.tpl = Template(uri=self.name, filename=self.filename, lookup=lookup, **options)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)


class CheetahTemplate(BaseTemplate):
    def prepare(self, **options):
        from Cheetah.Template import Template
        self.context = threading.local()
        self.context.vars = {}
        options['searchList'] = [self.context.vars]
        if self.source:
            self.tpl = Template(source=self.source, **options)
        else:
            self.tpl = Template(file=self.filename, **options)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        self.context.vars.update(self.defaults)
        self.context.vars.update(kwargs)
        out = str(self.tpl)
        self.context.vars.clear()
        return out


class Jinja2Template(BaseTemplate):
    def prepare(self, filters=None, tests=None, **kwargs):
        from jinja2 import Environment, FunctionLoader
        if 'prefix' in kwargs:  # TODO: to be removed after a while
            raise RuntimeError('The keyword argument `prefix` has been removed. '
                'Use the full jinja2 environment name line_statement_prefix instead.')
        self.env = Environment(loader=FunctionLoader(self.loader), **kwargs)
        if filters:
            self.env.filters.update(filters)
        if tests:
            self.env.tests.update(tests)
        if self.source:
            self.tpl = self.env.from_string(self.source)
        else:
            self.tpl = self.env.get_template(self.filename)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)

    def loader(self, name):
        fname = self.search(name, self.lookup)
        if fname:
            with open(fname, "rb") as f:
                return f.read().decode(self.encoding)


class SimpleTALTemplate(BaseTemplate):
    ''' Untested! '''

    def prepare(self, **options):
        from simpletal import simpleTAL
        # TODO: add option to load METAL files during render
        if self.source:
            self.tpl = simpleTAL.compileHTMLTemplate(self.source)
        else:
            with open(self.filename, 'rb') as fp:
                self.tpl = simpleTAL.compileHTMLTemplate(tonat(fp.read()))

    def render(self, *args, **kwargs):
        from simpletal import simpleTALES
        from StringIO import StringIO
        for dictarg in args:
            kwargs.update(dictarg)
        # TODO: maybe reuse a context instead of always creating one
        context = simpleTALES.Context()
        for k, v in self.defaults.items():
            context.addGlobal(k, v)
        for k, v in kwargs.items():
            context.addGlobal(k, v)
        output = StringIO()
        self.tpl.expand(context, output)
        return output.getvalue()


class SimpleTemplate(BaseTemplate):
    blocks = ('if', 'elif', 'else', 'try', 'except', 'finally', 'for', 'while',
              'with', 'def', 'class')
    dedent_blocks = ('elif', 'else', 'except', 'finally')

    @lazy_attribute
    def re_pytokens(cls):
        ''' This matches comments and all kinds of quoted strings but does
            NOT match comments (#...) within quoted strings. (trust me) '''
        return re.compile(r'''
            (''(?!')|""(?!")|'{6}|"{6}    # Empty strings (all 4 types)
             |'(?:[^\\']|\\.)+?'          # Single quotes (')
             |"(?:[^\\"]|\\.)+?"          # Double quotes (")
             |'{3}(?:[^\\]|\\.|\n)+?'{3}  # Triple-quoted strings (')
             |"{3}(?:[^\\]|\\.|\n)+?"{3}  # Triple-quoted strings (")
             |\#.*                        # Comments
            )''', re.VERBOSE)

    def prepare(self, escape_func=cgi.escape, noescape=False):
        self.cache = {}
        enc = self.encoding
        self._str = lambda x: touni(x, enc)
        self._escape = lambda x: escape_func(touni(x, enc))
        if noescape:
            self._str, self._escape = self._escape, self._str

    @classmethod
    def split_comment(cls, code):
        """ Removes comments (#...) from python code. """
        if '#' not in code:
            return code
        #: Remove comments only (leave quoted strings as they are)
        subf = lambda m: '' if m.group(0)[0] == '#' else m.group(0)
        return re.sub(cls.re_pytokens, subf, code)

    @cached_property
    def co(self):
        return compile(self.code, self.filename or '<string>', 'exec')

    @cached_property
    def code(self):
        stack = []  # Current Code indentation
        lineno = 0  # Current line of code
        ptrbuffer = []  # Buffer for printable strings and token tuple instances
        codebuffer = []  # Buffer for generated python code
        multiline = dedent = oneline = False
        template = self.source if self.source else open(self.filename).read()

        def yield_tokens(line):
            for i, part in enumerate(re.split(r'\{\{(.*?)\}\}', line)):
                if i % 2:
                    if part.startswith('!'):
                        yield 'RAW', part[1:]
                    else:
                        yield 'CMD', part
                else:
                    yield 'TXT', part

        def flush():  # Flush the ptrbuffer
            if not ptrbuffer:
                return
            cline = ''
            for line in ptrbuffer:
                for token, value in line:
                    if token == 'TXT':
                        cline += repr(value)
                    elif token == 'RAW':
                        cline += '_str(%s)' % value
                    elif token == 'CMD':
                        cline += '_escape(%s)' % value
                    cline += ', '
                cline = cline[:-2] + '\\\n'
            cline = cline[:-2]
            if cline[:-1].endswith('\\\\\\\\\\n'):
                cline = cline[:-7] + cline[-1]  # 'nobr\\\\\n' --> 'nobr'
            cline = '_printlist([' + cline + '])'
            del ptrbuffer[:]  # Do this before calling code() again
            code(cline)

        def code(stmt):
            for line in stmt.splitlines():
                codebuffer.append('  ' * len(stack) + line.strip())

        for line in template.splitlines(True):
            lineno += 1
            line = line if isinstance(line, unicode)\
                        else unicode(line, encoding=self.encoding)
            if lineno <= 2:
                m = re.search(r"%.*coding[:=]\s*([-\w\.]+)", line)
                if m:
                    self.encoding = m.group(1)
                if m:
                    line = line.replace('coding', 'coding (removed)')
            if line.strip()[:2].count('%') == 1:
                line = line.split('%', 1)[1].lstrip()  # Full line following the %
                cline = self.split_comment(line).strip()
                cmd = re.split(r'[^a-zA-Z0-9_]', cline)[0]
                flush() ## encoding (TODO: why?)
                if cmd in self.blocks or multiline:
                    cmd = multiline or cmd
                    dedent = cmd in self.dedent_blocks  # "else:"
                    if dedent and not oneline and not multiline:
                        cmd = stack.pop()
                    code(line)
                    oneline = not cline.endswith(':')  # "if 1: pass"
                    multiline = cmd if cline.endswith('\\') else False
                    if not oneline and not multiline:
                        stack.append(cmd)
                elif cmd == 'end' and stack:
                    code('#end(%s) %s' % (stack.pop(), line.strip()[3:]))
                elif cmd == 'include':
                    p = cline.split(None, 2)[1:]
                    if len(p) == 2:
                        code("_=_include(%s, _stdout, %s)" % (repr(p[0]), p[1]))
                    elif p:
                        code("_=_include(%s, _stdout)" % repr(p[0]))
                    else:  # Empty %include -> reverse of %rebase
                        code("_printlist(_base)")
                elif cmd == 'rebase':
                    p = cline.split(None, 2)[1:]
                    if len(p) == 2:
                        code("globals()['_rebase']=(%s, dict(%s))" % (repr(p[0]), p[1]))
                    elif p:
                        code("globals()['_rebase']=(%s, {})" % repr(p[0]))
                else:
                    code(line)
            else:  # Line starting with text (not '%') or '%%' (escaped)
                if line.strip().startswith('%%'):
                    line = line.replace('%%', '%', 1)
                ptrbuffer.append(yield_tokens(line))
        flush()
        return '\n'.join(codebuffer) + '\n'

    def subtemplate(self, _name, _stdout, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        if _name not in self.cache:
            self.cache[_name] = self.__class__(name=_name, lookup=self.lookup)
        return self.cache[_name].execute(_stdout, kwargs)

    def execute(self, _stdout, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        env = self.defaults.copy()
        env.update({'_stdout': _stdout, '_printlist': _stdout.extend,
               '_include': self.subtemplate, '_str': self._str,
               '_escape': self._escape})
        env.update(kwargs)
        eval(self.co, env)
        if '_rebase' in env:
            subtpl, rargs = env['_rebase']
            subtpl = self.__class__(name=subtpl, lookup=self.lookup)
            rargs['_base'] = _stdout[:]  # copy stdout
            del _stdout[:]  # clear stdout
            return subtpl.execute(_stdout, rargs)
        return env

    def render(self, *args, **kwargs):
        """ Render the template using keyword arguments as local variables. """
        for dictarg in args:
            kwargs.update(dictarg)
        stdout = []
        self.execute(stdout, kwargs)
        return ''.join(stdout)


def template(*args, **kwargs):
    '''
    Get a rendered template as a string iterator.
    You can use a name, a filename or a template string as first parameter.
    Template rendering arguments can be passed as dictionaries
    or directly (as keyword arguments).
    '''
    tpl = args[0] if args else None
    template_adapter = kwargs.pop('template_adapter', SimpleTemplate)
    if tpl not in TEMPLATES or DEBUG:
        settings = kwargs.pop('template_settings', {})
        lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)
        if isinstance(tpl, template_adapter):
            TEMPLATES[tpl] = tpl
            if settings:
                TEMPLATES[tpl].prepare(**settings)
        elif "\n" in tpl or "{" in tpl or "%" in tpl or '$' in tpl:
            TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, **settings)
        else:
            TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, **settings)
    if not TEMPLATES[tpl]:
        abort(500, 'Template (%s) not found' % tpl)
    for dictarg in args[1:]:
        kwargs.update(dictarg)
    return TEMPLATES[tpl].render(kwargs)

mako_template = functools.partial(template, template_adapter=MakoTemplate)
cheetah_template = functools.partial(template, template_adapter=CheetahTemplate)
jinja2_template = functools.partial(template, template_adapter=Jinja2Template)
simpletal_template = functools.partial(template, template_adapter=SimpleTALTemplate)


def view(tpl_name, **defaults):
    ''' Decorator: renders a template for a handler.
        The handler can control its behavior like that:

          - return a dict of template vars to fill out the template
          - return something other than a dict and the view decorator will not
            process the template, but return the handler result as is.
            This includes returning a HTTPResponse(dict) to get,
            for instance, JSON with autojson or other castfilters.
    '''

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            if isinstance(result, (dict, DictMixin)):
                tplvars = defaults.copy()
                tplvars.update(result)
                return template(tpl_name, **tplvars)
            return result
        return wrapper
    return decorator

mako_view = functools.partial(view, template_adapter=MakoTemplate)
cheetah_view = functools.partial(view, template_adapter=CheetahTemplate)
jinja2_view = functools.partial(view, template_adapter=Jinja2Template)
simpletal_view = functools.partial(view, template_adapter=SimpleTALTemplate)
###############################################################################
# Constants and Globals ########################################################
###############################################################################

TEMPLATE_PATH = ['./', './views/']
TEMPLATES = {}
DEBUG = False

#: A dict to map HTTP status codes (e.g. 404) to phrases (e.g. 'Not Found')
HTTP_CODES = httplib.responses
HTTP_CODES[418] = "I'm a teapot"  # RFC 2324
_HTTP_STATUS_LINES = dict((k, '%d %s' % (k, v)) for (k, v) in HTTP_CODES.iteritems())

#: The default template used for error pages. Override with @error()
### SHINKEN MOD: change from bottle import DEBUG to from shinken.webui.bottle import DEBUG,...
ERROR_PAGE_TEMPLATE = """
%try:
    %from shinken.webui.bottlecore import DEBUG, HTTP_CODES, request, touni
    %status_name = HTTP_CODES.get(e.status, 'Unknown').title()
    <!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
    <html>
        <head>
            <title>Error {{e.status}}: {{status_name}}</title>
            <style type="text/css">
              html {background-color: #eee; font-family: sans;}
              body {background-color: #fff; border: 1px solid #ddd;
                    padding: 15px; margin: 15px;}
              pre {background-color: #eee; border: 1px solid #ddd; padding: 5px;}
            </style>
        </head>
        <body>
            <h1>Error {{e.status}}: {{status_name}}</h1>
            <p>Sorry, the requested URL <tt>{{repr(request.url)}}</tt>
               caused an error:</p>
            <pre>{{e.output}}</pre>
            %if DEBUG and e.exception:
              <h2>Exception:</h2>
              <pre>{{repr(e.exception)}}</pre>
            %end
            %if DEBUG and e.traceback:
              <h2>Traceback:</h2>
              <pre>{{e.traceback}}</pre>
            %end
        </body>
    </html>
%except ImportError:
    <b>ImportError:</b> Could not generate the error page. Please add bottle to
    the import path.
%end
"""

#: A thread-save instance of :class:`Request` representing the `current` request.
request = Request()

#: A thread-save instance of :class:`Response` used to build the HTTP response.
response = Response()

#: A thread-save namespace. Not used by Bottle.
local = threading.local()

# Initialize app stack (create first empty Bottle app)
# BC: 0.6.4 and needed for run()
app = default_app = AppStack()
app.push()

#: A virtual package that redirects import statements.
#: Example: ``import bottle.ext.sqlite`` actually imports `bottle_sqlite`.
ext = _ImportRedirect(__name__ + '.ext', 'bottle_%s').module

########NEW FILE########
__FILENAME__ = bottlewebui
# -*- coding: utf-8 -*-
"""
Bottle is a fast and simple micro-framework for small web applications. It
offers request dispatching (Routes) with url parameter support, templates,
a built-in HTTP Server and adapters for many third party WSGI/HTTP-server and
template engines - all in a single file and with no dependencies other than the
Python Standard Library.

Homepage and documentation: http://bottlepy.org/

Copyright (c) 2011, Marcel Hellkamp.
License: MIT (see LICENSE.txt for details)
"""

from __future__ import with_statement

__author__ = 'Marcel Hellkamp'
__version__ = '0.10.dev'
__license__ = 'MIT'

import base64
import cgi
import email.utils
import functools
import hmac
import httplib
import imp
import itertools
import mimetypes
import os
import re
import subprocess
import sys
import tempfile
import thread
import threading
import time
import warnings

from Cookie import SimpleCookie
from tempfile import TemporaryFile
from traceback import format_exc
from urllib import urlencode, quote as urlquote
from urlparse import urljoin, SplitResult as UrlSplitResult

try:
    from collections import MutableMapping as DictMixin
except ImportError:  # pragma: no cover
    from UserDict import DictMixin

try:
    from urlparse import parse_qs
except ImportError:  # pragma: no cover
    from cgi import parse_qs

try:
    import cPickle as pickle
except ImportError:  # pragma: no cover
    import pickle

try:
    from json import dumps as json_dumps, loads as json_lds
except ImportError:  # pragma: no cover
    try:
        from simplejson import dumps as json_dumps, loads as json_lds
    except ImportError:  # pragma: no cover
        try:
            from django.utils.simplejson import dumps as json_dumps, loads as json_lds
        except ImportError:  # pragma: no cover
            def json_dumps(data):
                raise ImportError("JSON support requires Python 2.6 or simplejson.")
            json_lds = json_dumps

py3k = sys.version_info >= (3, 0, 0)
NCTextIOWrapper = None

if py3k:  # pragma: no cover
    json_loads = lambda s: json_lds(touni(s))
    # See Request.POST
    from io import BytesIO


    def touni(x, enc='utf8', err='strict'):
        """ Convert anything to unicode """
        return str(x, enc, err) if isinstance(x, bytes) else str(x)
    if sys.version_info < (3, 2, 0):
        from io import TextIOWrapper


        class NCTextIOWrapper(TextIOWrapper):
            ''' Garbage collecting an io.TextIOWrapper(buffer) instance closes
                the wrapped buffer. This subclass keeps it open. '''

            def close(self): pass

else:
    json_loads = json_lds
    from StringIO import StringIO as BytesIO
    bytes = str


    def touni(x, enc='utf8', err='strict'):
        """ Convert anything to unicode """
        return x if isinstance(x, unicode) else unicode(str(x), enc, err)


def tob(data, enc='utf8'):
    """ Convert anything to bytes """
    return data.encode(enc) if isinstance(data, unicode) else bytes(data)

# Convert strings and unicode to native strings
if  py3k:
    tonat = touni
else:
    tonat = tob
tonat.__doc__ = """ Convert anything to native strings """


# Backward compatibility
def depr(message, critical=False):
    if critical:
        raise DeprecationWarning(message)
    warnings.warn(message, DeprecationWarning, stacklevel=3)


# Small helpers
def makelist(data):
    if isinstance(data, (tuple, list, set, dict)):
        return list(data)
    elif data:
        return [data]
    else:
        return []


class DictProperty(object):
    ''' Property that maps to a key in a local dict-like attribute. '''

    def __init__(self, attr, key=None, read_only=False):
        self.attr, self.key, self.read_only = attr, key, read_only

    def __call__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter, self.key = func, self.key or func.__name__
        return self

    def __get__(self, obj, cls):
        if obj is None:
            return self
        key, storage = self.key, getattr(obj, self.attr)
        if key not in storage:
            storage[key] = self.getter(obj)
        return storage[key]

    def __set__(self, obj, value):
        if self.read_only:
            raise AttributeError("Read-Only property.")
        getattr(obj, self.attr)[self.key] = value

    def __delete__(self, obj):
        if self.read_only:
            raise AttributeError("Read-Only property.")
        del getattr(obj, self.attr)[self.key]


def cached_property(func):
    ''' A property that, if accessed, replaces itself with the computed
        value. Subsequent accesses won't call the getter again. '''
    return DictProperty('__dict__')(func)


class lazy_attribute(object):  # Does not need configuration -> lower-case name
    ''' A property that caches itself to the class object. '''

    def __init__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter = func

    def __get__(self, obj, cls):
        value = self.getter(cls)
        setattr(cls, self.__name__, value)
        return value


class HeaderProperty(object):
    def __init__(self, name, reader=None, writer=str, default=''):
        self.name, self.reader, self.writer, self.default = name, reader, writer, default
        self.__doc__ = 'Current value of the %r header.' % name.title()

    def __get__(self, obj, cls):
        if obj is None:
            return self
        value = obj.headers.get(self.name)
        return self.reader(value) if (value and self.reader) else (value or self.default)

    def __set__(self, obj, value):
        if self.writer:
            value = self.writer(value)
        obj.headers[self.name] = value

    def __delete__(self, obj):
        if self.name in obj.headers:
            del obj.headers[self.name]

###############################################################################
# Exceptions and Events ########################################################
###############################################################################

class BottleException(Exception):
    """ A base class for exceptions used by bottle. """
    pass


class HTTPResponse(BottleException):
    """ Used to break execution and immediately finish the response """

    def __init__(self, output='', status=200, header=None):
        super(BottleException, self).__init__("HTTP Response %d" % status)
        self.status = int(status)
        self.output = output
        self.headers = HeaderDict(header) if header else None

    def apply(self, response):
        if self.headers:
            for key, value in self.headers.iterallitems():
                response.headers[key] = value
        response.status = self.status


class HTTPError(HTTPResponse):
    """ Used to generate an error page """

    def __init__(self, code=500, output='Unknown Error', exception=None,
                 traceback=None, header=None):
        super(HTTPError, self).__init__(output, code, header)
        self.exception = exception
        self.traceback = traceback

    def __repr__(self):
        return template(ERROR_PAGE_TEMPLATE, e=self)

###############################################################################
# Routing ######################################################################
###############################################################################

class RouteError(BottleException):
    """ This is a base class for all routing related exceptions """


class RouteReset(BottleException):
    """ If raised by a plugin or request handler, the route is reset and all
        plugins are re-applied. """


class RouteSyntaxError(RouteError):
    """ The route parser found something not supported by this router """


class RouteBuildError(RouteError):
    """ The route could not been built """


class Router(object):
    ''' A Router is an ordered collection of route->target pairs. It is used to
        efficiently match WSGI requests against a number of routes and return
        the first target that satisfies the request. The target may be anything,
        usually a string, ID or callable object. A route consists of a path-rule
        and a HTTP method.

        The path-rule is either a static path (e.g. `/contact`) or a dynamic
        path that contains wildcards (e.g. `/wiki/:page`). By default, wildcards
        consume characters up to the next slash (`/`). To change that, you may
        add a regular expression pattern (e.g. `/wiki/:page#[a-z]+#`).

        For performance reasons, static routes (rules without wildcards) are
        checked first. Dynamic routes are searched in order. Try to avoid
        ambiguous or overlapping rules.

        The HTTP method string matches only on equality, with two exceptions:
          * GET routes also match HEAD requests if there is no appropriate
            HEAD route installed.
          * ANY routes do match if there is no other suitable route installed.

        An optional ``name`` parameter is used by :meth:`build` to identify
        routes.
    '''

    default = '[^/]+'

    @lazy_attribute
    def syntax(cls):
        return re.compile(r'(?<!\\):([a-zA-Z_][a-zA-Z_0-9]*)?(?:#(.*?)#)?')

    def __init__(self):
        self.routes = {}   # A {rule: {method: target}} mapping
        self.rules  = []   # An ordered list of rules
        self.named  = {}   # A name->(rule, build_info) mapping
        self.static = {}   # Cache for static routes: {path: {method: target}}
        self.dynamic = []  # Cache for dynamic routes. See _compile()

    def add(self, rule, method, target, name=None):
        ''' Add a new route or replace the target for an existing route. '''

        if rule in self.routes:
            self.routes[rule][method.upper()] = target
        else:
            self.routes[rule] = {method.upper(): target}
            self.rules.append(rule)
            if self.static or self.dynamic:  # Clear precompiler cache.
                self.static, self.dynamic = {}, {}
        if name:
            self.named[name] = (rule, None)

    def build(self, _name, *anon, **args):
        ''' Return a string that matches a named route. Use keyword arguments
            to fill out named wildcards. Remaining arguments are appended as a
            query string. Raises RouteBuildError or KeyError.'''
        if _name not in self.named:
            raise RouteBuildError("No route with that name.", _name)
        rule, pairs = self.named[_name]
        if not pairs:
            token = self.syntax.split(rule)
            parts = [p.replace('\\:', ':') for p in token[::3]]
            names = token[1::3]
            if len(parts) > len(names):
                names.append(None)
            pairs = zip(parts, names)
            self.named[_name] = (rule, pairs)
        try:
            anon = list(anon)
            url = [s if k is None
                   else s + str(args.pop(k)) if k else s + str(anon.pop())
                   for s, k in pairs]
        except IndexError:
            msg = "Not enough arguments to fill out anonymous wildcards."
            raise RouteBuildError(msg)
        except KeyError, e:
            raise RouteBuildError(*e.args)

        if args:
            url += ['?', urlencode(args)]
        return ''.join(url)

    def match(self, environ):
        ''' Return a (target, url_agrs) tuple or raise HTTPError(404/405). '''
        targets, urlargs = self._match_path(environ)
        if not targets:
            raise HTTPError(404, "Not found: " + repr(environ['PATH_INFO']))
        method = environ['REQUEST_METHOD'].upper()
        if method in targets:
            return targets[method], urlargs
        if method == 'HEAD' and 'GET' in targets:
            return targets['GET'], urlargs
        if 'ANY' in targets:
            return targets['ANY'], urlargs
        allowed = [verb for verb in targets if verb != 'ANY']
        if 'GET' in allowed and 'HEAD' not in allowed:
            allowed.append('HEAD')
        raise HTTPError(405, "Method not allowed.",
                        header=[('Allow', ",".join(allowed))])

    def _match_path(self, environ):
        ''' Optimized PATH_INFO matcher. '''
        path = environ['PATH_INFO'] or '/'
        # Assume we are in a warm state. Search compiled rules first.
        match = self.static.get(path)
        if match:
            return match, {}
        for combined, rules in self.dynamic:
            match = combined.match(path)
            if not match:
                continue
            gpat, match = rules[match.lastindex - 1]
            return match, gpat(path).groupdict() if gpat else {}

        # Lazy-check if we are really in a warm state. If yes, stop here.
        if self.static or self.dynamic or not self.routes:
            return None, {}

        # Cold state: We have not compiled any rules yet. Do so and try again.
        if not environ.get('wsgi.run_once'):
            self._compile()
            return self._match_path(environ)

        # For run_once (CGI) environments, don't compile. Just check one by one.
        epath = path.replace(':', '\\:')  # Turn path into its own static rule.
        match = self.routes.get(epath)  # This returns static rule only.
        if match:
            return match, {}
        for rule in self.rules:
            #: Skip static routes to reduce re.compile() calls.
            if rule.count(':') < rule.count('\\:'):
                continue
            match = self._compile_pattern(rule).match(path)
            if match:
                return self.routes[rule], match.groupdict()
        return None, {}

    def _compile(self):
        ''' Prepare static and dynamic search structures. '''
        self.static = {}
        self.dynamic = []

        def fpat_sub(m):
            return m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:'
        for rule in self.rules:
            target = self.routes[rule]
            if not self.syntax.search(rule):
                self.static[rule.replace('\\:', ':')] = target
                continue
            gpat = self._compile_pattern(rule)
            fpat = re.sub(r'(\\*)(\(\?P<[^>]*>|\((?!\?))', fpat_sub, gpat.pattern)
            gpat = gpat.match if gpat.groupindex else None
            try:
                combined = '%s|(%s)' % (self.dynamic[-1][0].pattern, fpat)
                self.dynamic[-1] = (re.compile(combined), self.dynamic[-1][1])
                self.dynamic[-1][1].append((gpat, target))
            except (AssertionError, IndexError), e:  # AssertionError: Too many groups
                self.dynamic.append((re.compile('(^%s$)' % fpat),
                                    [(gpat, target)]))
            except re.error, e:
                raise RouteSyntaxError("Could not add Route: %s (%s)" % (rule, e))

    def _compile_pattern(self, rule):
        ''' Return a regular expression with named groups for each wildcard. '''
        out = ''
        for i, part in enumerate(self.syntax.split(rule)):
            if i % 3 == 0:
                out += re.escape(part.replace('\\:', ':'))
            elif i % 3 == 1:
                out += '(?P<%s>' % part if part else '(?:'
            else:
                out += '%s)' % (part or '[^/]+')
        return re.compile('^%s$' % out)

###############################################################################
# Application Object ###########################################################
###############################################################################

class Bottle(object):
    """ WSGI application """

    def __init__(self, catchall=True, autojson=True, config=None):
        """ Create a new bottle instance.
            You usually don't do that. Use `bottle.app.push()` instead.
        """
        self.routes = []  # List of installed routes including metadata.
        self.router = Router()  # Maps requests to self.route indices.
        self.ccache = {}  # Cache for callbacks with plugins applied.

        self.plugins = []  # List of installed plugins.

        self.mounts = {}
        self.error_handler = {}
        #: If true, most exceptions are caught and returned as :exc:`HTTPError`
        self.catchall = catchall
        self.config = config or {}
        self.serve = True
        # Default plugins
        self.hooks = self.install(HooksPlugin())
        if autojson:
            self.install(JSONPlugin())
        self.install(TemplatePlugin())

    def mount(self, app, prefix, **options):
        ''' Mount an application to a specific URL prefix. The prefix is added
            to SCIPT_PATH and removed from PATH_INFO before the sub-application
            is called.:param app: an instance of :class:`Bottle`.:param prefix: path prefix used as a mount-point.

            All other parameters are passed to the underlying :meth:`route` call.
        '''
        if not isinstance(app, Bottle):
            raise TypeError('Only Bottle instances are supported for now.')
        prefix = '/'.join(filter(None, prefix.split('/')))
        if not prefix:
            raise TypeError('Empty prefix. Perhaps you want a merge()?')
        for other in self.mounts:
            if other.startswith(prefix):
                raise TypeError('Conflict with existing mount: %s' % other)
        path_depth = prefix.count('/') + 1
        options.setdefault('method', 'ANY')
        options.setdefault('skip', True)
        self.mounts[prefix] = app

        @self.route('/%s/:#.*#' % prefix, **options)
        def mountpoint():
            request.path_shift(path_depth)
            return app._handle(request.environ)

    def install(self, plugin):
        ''' Add a plugin to the list of plugins and prepare it for being
            applied to all routes of this application. A plugin may be a simple
            decorator or an object that implements the :class:`Plugin` API.
        '''
        if hasattr(plugin, 'setup'):
            plugin.setup(self)
        if not callable(plugin) and not hasattr(plugin, 'apply'):
            raise TypeError("Plugins must be callable or implement .apply()")
        self.plugins.append(plugin)
        self.reset()
        return plugin

    def uninstall(self, plugin):
        ''' Uninstall plugins. Pass an instance to remove a specific plugin.
            Pass a type object to remove all plugins that match that type.
            Subclasses are not removed. Pass a string to remove all plugins with
            a matching ``name`` attribute. Pass ``True`` to remove all plugins.
            The list of affected plugins is returned. '''
        removed, remove = [], plugin
        for i, plugin in list(enumerate(self.plugins))[::-1]:
            if remove is True or remove is plugin or remove is type(plugin) \
            or getattr(plugin, 'name', True) == remove:
                removed.append(plugin)
                del self.plugins[i]
                if hasattr(plugin, 'close'):
                    plugin.close()
        if removed:
            self.reset()
        return removed

    def reset(self, id=None):
        ''' Reset all routes (force plugins to be re-applied) and clear all
            caches. If an ID is given, only that specific route is affected. '''
        if id is None:
            self.ccache.clear()
        else:
            self.ccache.pop(id, None)
        if DEBUG:
            for route in self.routes:
                if route['id'] not in self.ccache:
                    self.ccache[route['id']] = self._build_callback(route)

    def close(self):
        ''' Close the application and all installed plugins. '''
        for plugin in self.plugins:
            if hasattr(plugin, 'close'):
                plugin.close()
        self.stopped = True

    def match(self, environ):
        """ (deprecated) Search for a matching route and return a
            (callback, urlargs) tuple.
            The first element is the associated route callback with plugins
            applied. The second value is a dictionary with parameters extracted
            from the URL. The :class:`Router` raises :exc:`HTTPError` (404/405)
            on a non-match."""
        depr("This method will change semantics in 0.10.")
        return self._match(environ)

    def _match(self, environ):
        handle, args = self.router.match(environ)
        environ['route.handle'] = handle  # TODO move to router?
        environ['route.url_args'] = args
        try:
            return self.ccache[handle], args
        except KeyError:
            config = self.routes[handle]
            callback = self.ccache[handle] = self._build_callback(config)
            return callback, args

    def _build_callback(self, config):
        ''' Apply plugins to a route and return a new callable. '''
        wrapped = config['callback']
        plugins = self.plugins + config['apply']
        skip = config['skip']
        try:
            for plugin in reversed(plugins):
                if True in skip:
                    break
                if plugin in skip or type(plugin) in skip:
                    continue
                if getattr(plugin, 'name', True) in skip:
                    continue
                if hasattr(plugin, 'apply'):
                    wrapped = plugin.apply(wrapped, config)
                else:
                    wrapped = plugin(wrapped)
                if not wrapped:
                    break
                functools.update_wrapper(wrapped, config['callback'])
            return wrapped
        except RouteReset:  # A plugin may have changed the config dict inplace.
            return self._build_callback(config)  # Apply all plugins again.

    def get_url(self, routename, **kargs):
        """ Return a string that matches a named route """
        scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'
        location = self.router.build(routename, **kargs).lstrip('/')
        return urljoin(urljoin('/', scriptname), location)

    def route(self, path=None, method='GET', callback=None, name=None,
              apply=None, skip=None, **config):
        """ A decorator to bind a function to a request URL. Example::

                @app.route('/hello/:name')
                def hello(name):
                    return 'Hello %s' % name

            The ``:name`` part is a wildcard. See :class:`Router` for syntax
            details.:param path: Request path or a list of paths to listen to. If no
              path is specified, it is automatically generated from the
              signature of the function.:param method: HTTP method (`GET`, `POST`, `PUT`, ...) or a list of
              methods to listen to. (default: `GET`):param callback: An optional shortcut to avoid the decorator
              syntax. ``route(..., callback=func)`` equals ``route(...)(func)``:param name: The name for this route. (default: None):param apply: A decorator or plugin or a list of plugins. These are
              applied to the route callback in addition to installed plugins.:param skip: A list of plugins, plugin classes or names. Matching
              plugins are not installed to this route. ``True`` skips all.

            Any additional keyword arguments are stored as route-specific
            configuration and passed to plugins (see :meth:`Plugin.apply`).
        """
        if callable(path):
            path, callback = None, path

        plugins = makelist(apply)
        skiplist = makelist(skip)

        def decorator(callback):
            for rule in makelist(path) or yieldroutes(callback):
                for verb in makelist(method):
                    verb = verb.upper()
                    cfg = dict(rule=rule, method=verb, callback=callback,
                               name=name, app=self, config=config,
                               apply=plugins, skip=skiplist)
                    self.routes.append(cfg)
                    cfg['id'] = self.routes.index(cfg)
                    self.router.add(rule, verb, cfg['id'], name=name)
                    if DEBUG:
                        self.ccache[cfg['id']] = self._build_callback(cfg)
            return callback

        return decorator(callback) if callback else decorator

    def get(self, path=None, method='GET', **options):
        """ Equals :meth:`route`. """
        return self.route(path, method, **options)

    def post(self, path=None, method='POST', **options):
        """ Equals :meth:`route` with a ``POST`` method parameter. """
        return self.route(path, method, **options)

    def put(self, path=None, method='PUT', **options):
        """ Equals :meth:`route` with a ``PUT`` method parameter. """
        return self.route(path, method, **options)

    def delete(self, path=None, method='DELETE', **options):
        """ Equals :meth:`route` with a ``DELETE`` method parameter. """
        return self.route(path, method, **options)

    def error(self, code=500):
        """ Decorator: Register an output handler for a HTTP error code"""

        def wrapper(handler):
            self.error_handler[int(code)] = handler
            return handler
        return wrapper

    def hook(self, name):
        """ Return a decorator that attaches a callback to a hook. """

        def wrapper(func):
            self.hooks.add(name, func)
            return func
        return wrapper

    def handle(self, path, method='GET'):
        """ (deprecated) Execute the first matching route callback and return
            the result. :exc:`HTTPResponse` exceptions are caught and returned.
            If :attr:`Bottle.catchall` is true, other exceptions are caught as
            well and returned as :exc:`HTTPError` instances (500).
        """
        depr("This method will change semantics in 0.10. Try to avoid it.")
        if isinstance(path, dict):
            return self._handle(path)
        return self._handle({'PATH_INFO': path, 'REQUEST_METHOD': method.upper()})

    def _handle(self, environ):
        if not self.serve:
            depr("Bottle.serve will be removed in 0.10.")
            return HTTPError(503, "Server stopped")
        try:
            callback, args = self._match(environ)
            return callback(**args)
        except HTTPResponse, r:
            return r
        except RouteReset:  # Route reset requested by the callback or a plugin.
            del self.ccache[environ['route.handle']]
            return self._handle(environ)  # Try again.
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except Exception, e:
            if not self.catchall:
                raise
            stacktrace = format_exc(10)
            environ['wsgi.errors'].write(stacktrace)
            return HTTPError(500, "Internal Server Error", e, stacktrace)

    def _cast(self, out, request, response, peek=None):
        """ Try to convert the parameter into something WSGI compatible and set
        correct HTTP headers when possible.
        Support: False, str, unicode, dict, HTTPResponse, HTTPError, file-like,
        iterable of strings and iterable of unicodes
        """

        # Empty output is done here
        if not out:
            response['Content-Length'] = 0
            return []
        # Join lists of byte or unicode strings. Mixed lists are NOT supported
        if isinstance(out, (tuple, list))\
        and isinstance(out[0], (bytes, unicode)):
            out = out[0][0:0].join(out)  # b'abc'[0:0] -> b''
        # Encode unicode strings
        if isinstance(out, unicode):
            out = out.encode(response.charset)
        # Byte Strings are just returned
        if isinstance(out, bytes):
            response['Content-Length'] = len(out)
            return [out]
        # HTTPError or HTTPException (recursive, because they may wrap anything)
        # TODO: Handle these explicitly in handle() or make them iterable.
        if isinstance(out, HTTPError):
            out.apply(response)
            out = self.error_handler.get(out.status, repr)(out)
            if isinstance(out, HTTPResponse):
                depr('Error handlers must not return :exc:`HTTPResponse`.')  # 0.9
            return self._cast(out, request, response)
        if isinstance(out, HTTPResponse):
            out.apply(response)
            return self._cast(out.output, request, response)

        # File-like objects.
        if hasattr(out, 'read'):
            if 'wsgi.file_wrapper' in request.environ:
                return request.environ['wsgi.file_wrapper'](out)
            elif hasattr(out, 'close') or not hasattr(out, '__iter__'):
                return WSGIFileWrapper(out)

        # Handle Iterables. We peek into them to detect their inner type.
        try:
            out = iter(out)
            first = out.next()
            while not first:
                first = out.next()
        except StopIteration:
            return self._cast('', request, response)
        except HTTPResponse, e:
            first = e
        except Exception, e:
            first = HTTPError(500, 'Unhandled exception', e, format_exc(10))
            if isinstance(e, (KeyboardInterrupt, SystemExit, MemoryError))\
            or not self.catchall:
                raise
        # These are the inner types allowed in iterator or generator objects.
        if isinstance(first, HTTPResponse):
            return self._cast(first, request, response)
        if isinstance(first, bytes):
            return itertools.chain([first], out)
        if isinstance(first, unicode):
            return itertools.imap(lambda x: x.encode(response.charset),
                                  itertools.chain([first], out))
        return self._cast(HTTPError(500, 'Unsupported response type: %s'\
                                         % type(first)), request, response)

    def wsgi(self, environ, start_response):
        """ The bottle WSGI-interface. """
        try:
            environ['bottle.app'] = self
            request.bind(environ)
            response.bind()
            out = self._cast(self._handle(environ), request, response)
            # rfc2616 section 4.3
            if response.status_code in (100, 101, 204, 304)\
            or request.method == 'HEAD':
                if hasattr(out, 'close'):
                    out.close()
                out = []
            start_response(response.status_line, list(response.iter_headers()))
            return out
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except Exception, e:
            if not self.catchall:
                raise
            err = '<h1>Critical error while processing request: %s</h1>' \
                  % environ.get('PATH_INFO', '/')
            if DEBUG:
                err += '<h2>Error:</h2>\n<pre>%s</pre>\n' % repr(e)
                err += '<h2>Traceback:</h2>\n<pre>%s</pre>\n' % format_exc(10)
            environ['wsgi.errors'].write(err)  # TODO: wsgi.error should not get html
            start_response('500 INTERNAL SERVER ERROR', [('Content-Type', 'text/html')])
            return [tob(err)]

    def __call__(self, environ, start_response):
        return self.wsgi(environ, start_response)

###############################################################################
# HTTP and WSGI Tools ##########################################################
###############################################################################

class BaseRequest(DictMixin):
    """ A wrapper for WSGI environment dictionaries that adds a lot of
        convenient access methods and properties. Most of them are read-only."""

    #: Maximum size of memory buffer for :attr:`body` in bytes.
    MEMFILE_MAX = 102400

    def __init__(self, environ):
        """ Wrap a WSGI environ dictionary. """
        #: The wrapped WSGI environ dictionary. This is the only real attribute.
        #: All other attributes actually are read-only properties.
        self.environ = environ
        environ['bottle.request'] = self

    @property
    def path(self):
        ''' The value of ``PATH_INFO`` with exactly one prefixed slash (to fix
            broken clients and avoid the "empty path" edge case). '''
        return '/' + self.environ.get('PATH_INFO', '').lstrip('/')

    @property
    def method(self):
        ''' The ``REQUEST_METHOD`` value as an uppercase string. '''
        return self.environ.get('REQUEST_METHOD', 'GET').upper()

    @DictProperty('environ', 'bottle.request.headers', read_only=True)
    def headers(self):
        ''' A :class:`WSGIHeaderDict` that provides case-insensitive access to
            HTTP request headers. '''
        return WSGIHeaderDict(self.environ)

    @DictProperty('environ', 'bottle.request.cookies', read_only=True)
    def cookies(self):
        """ Cookies parsed into a dictionary. Signed cookies are NOT decoded.
            Use :meth:`get_cookie` if you expect signed cookies. """
        raw_dict = SimpleCookie(self.environ.get('HTTP_COOKIE', ''))
        cookies = {}
        for cookie in raw_dict.itervalues():
            cookies[cookie.key] = cookie.value
        return cookies

    def get_cookie(self, key, default=None, secret=None):
        """ Return the content of a cookie. To read a `Signed Cookie`, the
            `secret` must match the one used to create the cookie (see
            :meth:`BaseResponse.set_cookie`). If anything goes wrong (missing
            cookie or wrong signature), return a default value. """
        value = self.cookies.get(key)
        if secret and value:
            dec = cookie_decode(value, secret)  # (key, value) tuple or None
            return dec[1] if dec and dec[0] == key else default
        return value or default

    @DictProperty('environ', 'bottle.request.query', read_only=True)
    def query(self):
        ''' The :attr:`query_string` parsed into a :class:`MultiDict`. These
            values are sometimes called "URL arguments" or "GET parameters", but
            not to be confused with "URL wildcards" as they are provided by the
            :class:`Router`. '''
        data = parse_qs(self.query_string, keep_blank_values=True)
        get = self.environ['bottle.get'] = MultiDict()
        for key, values in data.iteritems():
            for value in values:
                get[key] = value
        return get

    @DictProperty('environ', 'bottle.request.forms', read_only=True)
    def forms(self):
        """ Form values parsed from an `url-encoded` or `multipart/form-data`
            encoded POST or PUT request body. The result is returned as a
            :class:`MultiDict`. All keys and values are strings. File uploads
            are stored separately in :attr:`files`. """
        forms = MultiDict()
        for name, item in self.POST.iterallitems():
            if not hasattr(item, 'filename'):
                forms[name] = item
        return forms

    @DictProperty('environ', 'bottle.request.params', read_only=True)
    def params(self):
        """ A :class:`MultiDict` with the combined values of :attr:`query` and
            :attr:`forms`. File uploads are stored in :attr:`files`. """
        params = MultiDict()
        for key, value in self.query.iterallitems():
            params[key] = value
        for key, value in self.forms.iterallitems():
            params[key] = value
        return params

    @DictProperty('environ', 'bottle.request.files', read_only=True)
    def files(self):
        """ File uploads parsed from an `url-encoded` or `multipart/form-data`
            encoded POST or PUT request body. The values are instances of
            :class:`cgi.FieldStorage`. The most important attributes are:

            filename
                The filename, if specified; otherwise None; this is the client
                side filename, *not* the file name on which it is stored (that's
                a temporary file you don't deal with)
            file
                The file(-like) object from which you can read the data.
            value
                The value as a *string*; for file uploads, this transparently
                reads the file every time you request the value. Do not do this
                on big files.
        """
        files = MultiDict()
        for name, item in self.POST.iterallitems():
            if hasattr(item, 'filename'):
                files[name] = item
        return files

    @DictProperty('environ', 'bottle.request.json', read_only=True)
    def json(self):
        ''' If the ``Content-Type`` header is ``application/json``, this
            property holds the parsed content of the request body. Only requests
            smaller than :attr:`MEMFILE_MAX` are processed to avoid memory
            exhaustion. '''
        if self.environ.get('CONTENT_TYPE') == 'application/json' \
        and 0 < self.content_length < self.MEMFILE_MAX:
            return json_loads(self.body.read(self.MEMFILE_MAX))
        return None

    @DictProperty('environ', 'bottle.request.body', read_only=True)
    def _body(self):
        maxread = max(0, self.content_length)
        stream = self.environ['wsgi.input']
        body = BytesIO() if maxread < self.MEMFILE_MAX else TemporaryFile(mode='w+b')
        while maxread > 0:
            part = stream.read(min(maxread, self.MEMFILE_MAX))
            if not part:
                break
            body.write(part)
            maxread -= len(part)
        self.environ['wsgi.input'] = body
        body.seek(0)
        return body

    @property
    def body(self):
        """ The HTTP request body as a seek-able file-like object. Depending on
            :attr:`MEMFILE_MAX`, this is either a temporary file or a
            :class:`io.BytesIO` instance. Accessing this property for the first
            time reads and replaces the ``wsgi.input`` environ variable.
            Subsequent accesses just do a `seek(0)` on the file object. """
        self._body.seek(0)
        return self._body

    #: An alias for :attr:`query`.
    GET = query

    @DictProperty('environ', 'bottle.request.post', read_only=True)
    def POST(self):
        """ The values of :attr:`forms` and :attr:`files` combined into a single
            :class:`MultiDict`. Values are either strings (form values) or
            instances of :class:`cgi.FieldStorage` (file uploads).
        """
        post = MultiDict()
        safe_env = {'QUERY_STRING': ''}  # Build a safe environment for cgi
        for key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):
            if key in self.environ:
                safe_env[key] = self.environ[key]
        if NCTextIOWrapper:
            fb = NCTextIOWrapper(self.body, encoding='ISO-8859-1', newline='\n')
        else:
            fb = self.body
        data = cgi.FieldStorage(fp=fb, environ=safe_env, keep_blank_values=True)
        for item in data.list or []:
            post[item.name] = item if item.filename else item.value
        return post

    @property
    def COOKIES(self):
        ''' Alias for :attr:`cookies` (deprecated). '''
        depr('BaseRequest.COOKIES was renamed to BaseRequest.cookies (lowercase).')
        return self.cookies

    @property
    def url(self):
        """ The full request URI including hostname and scheme. If your app
            lives behind a reverse proxy or load balancer and you get confusing
            results, make sure that the ``X-Forwarded-Host`` header is set
            correctly. """
        return self.urlparts.geturl()

    @DictProperty('environ', 'bottle.request.urlparts', read_only=True)
    def urlparts(self):
        ''' The :attr:`url` string as an :class:`urlparse.SplitResult` tuple.
            The tuple contains (scheme, host, path, query_string and fragment),
            but the fragment is always empty because it is not visible to the
            server. '''
        env = self.environ
        http = env.get('wsgi.url_scheme', 'http')
        host = env.get('HTTP_X_FORWARDED_HOST') or env.get('HTTP_HOST')
        if not host:
            # HTTP 1.1 requires a Host-header. This is for HTTP/1.0 clients.
            host = env.get('SERVER_NAME', '127.0.0.1')
            port = env.get('SERVER_PORT')
            if port and port != ('80' if http == 'http' else '443'):
                host += ':' + port
        path = urlquote(self.fullpath)
        return UrlSplitResult(http, host, path, env.get('QUERY_STRING'), '')

    @property
    def fullpath(self):
        """ Request path including :attr:`script_name` (if present). """
        return urljoin(self.script_name, self.path.lstrip('/'))

    @property
    def query_string(self):
        """ The raw :attr:`query` part of the URL (everything in between ``?``
            and ``#``) as a string. """
        return self.environ.get('QUERY_STRING', '')

    @property
    def script_name(self):
        ''' The initial portion of the URL's `path` that was removed by a higher
            level (server or routing middleware) before the application was
            called. This property returns an empty string, or a path with
            leading and tailing slashes. '''
        script_name = self.environ.get('SCRIPT_NAME', '').strip('/')
        return '/' + script_name + '/' if script_name else '/'

    def path_shift(self, shift=1):
        ''' Shift path segments from :attr:`path` to :attr:`script_name` and
            vice versa.:param shift: The number of path segments to shift. May be negative
                         to change the shift direction. (default: 1)
        '''
        script = self.environ.get('SCRIPT_NAME', '/')
        self['SCRIPT_NAME'], self['PATH_INFO'] = path_shift(script, self.path, shift)

    @property
    def content_length(self):
        ''' The request body length as an integer. The client is responsible to
            set this header. Otherwise, the real length of the body is unknown
            and -1 is returned. In this case, :attr:`body` will be empty. '''
        return int(self.environ.get('CONTENT_LENGTH') or -1)

    @property
    def is_xhr(self):
        ''' True if the request was triggered by a XMLHttpRequest. This only
            works with JavaScript libraries that support the `X-Requested-With`
            header (most of the popular libraries do). '''
        requested_with = self.environ.get('HTTP_X_REQUESTED_WITH', '')
        return requested_with.lower() == 'xmlhttprequest'

    @property
    def is_ajax(self):
        ''' Alias for :attr:`is_xhr`. "Ajax" is not the right term. '''
        return self.is_xhr

    @property
    def auth(self):
        """ HTTP authentication data as a (user, password) tuple. This
            implementation currently supports basic (not digest) authentication
            only. If the authentication happened at a higher level (e.g. in the
            front web-server or a middleware), the password field is None, but
            the user field is looked up from the ``REMOTE_USER`` environ
            variable. On any errors, None is returned. """
        basic = parse_auth(self.environ.get('HTTP_AUTHORIZATION', ''))
        if basic:
            return basic
        ruser = self.environ.get('REMOTE_USER')
        if ruser:
            return (ruser, None)
        return None

    @property
    def remote_route(self):
        """ A list of all IPs that were involved in this request, starting with
            the client IP and followed by zero or more proxies. This does only
            work if all proxies support the ```X-Forwarded-For`` header. Note
            that this information can be forged by malicious clients. """
        proxy = self.environ.get('HTTP_X_FORWARDED_FOR')
        if proxy:
            return [ip.strip() for ip in proxy.split(',')]
        remote = self.environ.get('REMOTE_ADDR')
        return [remote] if remote else []

    @property
    def remote_addr(self):
        """ The client IP as a string. Note that this information can be forged
            by malicious clients. """
        route = self.remote_route
        return route[0] if route else None

    def copy(self):
        """ Return a new :class:`Request` with a shallow :attr:`environ` copy. """
        return Request(self.environ.copy())

    def __getitem__(self, key): return self.environ[key]
    def __delitem__(self, key): self[key] = ""; del(self.environ[key])
    def __iter__(self): return iter(self.environ)
    def __len__(self): return len(self.environ)
    def keys(self): return self.environ.keys()

    def __setitem__(self, key, value):
        """ Change an environ value and clear all caches that depend on it. """

        if self.environ.get('bottle.request.readonly'):
            raise KeyError('The environ dictionary is read-only.')

        self.environ[key] = value
        todelete = ()

        if key == 'wsgi.input':
            todelete = ('body', 'forms', 'files', 'params', 'post', 'json')
        elif key == 'QUERY_STRING':
            todelete = ('query', 'params')
        elif key.startswith('HTTP_'):
            todelete = ('headers', 'cookies')

        for key in todelete:
            self.environ.pop('bottle.request.' + key, None)


class LocalRequest(BaseRequest, threading.local):
    ''' A thread-local subclass of :class:`BaseRequest`. '''

    def __init__(self): pass
    bind = BaseRequest.__init__

Request = LocalRequest


def _hkey(s):
    return s.title().replace('_', '-')


class BaseResponse(object):
    """ Storage class for a response body as well as headers and cookies.

        This class does support dict-like case-insensitive item-access to
        headers, but is NOT a dict. Most notably, iterating over a response
        yields parts of the body and not the headers.
    """

    default_status = 200
    default_content_type = 'text/html; charset=UTF-8'

    #: Header blacklist for specific response codes
    #: (rfc2616 section 10.2.3 and 10.3.5)
    bad_headers = {
        204: set(('Content-Type',)),
        304: set(('Allow', 'Content-Encoding', 'Content-Language',
                  'Content-Length', 'Content-Range', 'Content-Type',
                  'Content-Md5', 'Last-Modified'))}

    def __init__(self, body='', status=None, **headers):
        #: The HTTP status code as an integer (e.g. 404).
        #: Do not change it directly, see :attr:`status`.
        self.status_code = None
        #: The HTTP status line as a string (e.g. "404 Not Found").
        #: Do not change it directly, see :attr:`status`.
        self.status_line = None
        #: The response body as one of the supported data types.
        self.body = body
        self._cookies = None
        self._headers = {'Content-Type': [self.default_content_type]}
        self.status = status or self.default_status
        if headers:
            for name, value in headers.items():
                self[name] = value

    def copy(self):
        ''' Returns a copy of self. '''
        copy = Response()
        copy.status = self.status
        copy._headers = dict((k, v[:]) for (k, v) in self._headers.items())
        return copy

    def __iter__(self):
        return iter(self.body)

    def close(self):
        if hasattr(self.body, 'close'):
            self.body.close()

    def _set_status(self, status):
        if isinstance(status, int):
            code, status = status, _HTTP_STATUS_LINES.get(status)
        elif ' ' in status:
            status = status.strip()
            code = int(status.split()[0])
        else:
            raise ValueError('String status line without a reason phrase.')
        if not 100 <= code <= 999:
            raise ValueError('Status code out of range.')
        self.status_code = code
        self.status_line = status or ('%d Unknown' % code)

    status = property(lambda self: self.status_code, _set_status, None,
        ''' A writeable property to change the HTTP response status. It accepts
            either a numeric code (100-999) or a string with a custom reason
            phrase (e.g. "404 Brain not found"). Both :data:`status_line` and
            :data:`status_line` are updates accordingly. The return value is
            always a numeric code. ''')
    del _set_status

    @property
    def headers(self):
        ''' An instance of :class:`HeaderDict`, a case-insensitive dict-like
            view on the response headers. '''
        self.__dict__['headers'] = hdict = HeaderDict()
        hdict.dict = self._headers
        return hdict

    def __contains__(self, name): return _hkey(name) in self._headers
    def __delitem__(self, name): del self._headers[_hkey(name)]
    def __getitem__(self, name): return self._headers[_hkey(name)][-1]
    def __setitem__(self, name, value): self._headers[_hkey(name)] = [str(value)]

    def get_header(self, name, default=None):
        ''' Return the value of a previously defined header. If there is no
            header with that name, return a default value. '''
        return self._headers.get(_hkey(name), [default])[-1]

    def set_header(self, name, value, append=False):
        ''' Create a new response header, replacing any previously defined
            headers with the same name. This equals ``response[name] = value``.:param append: Do not delete previously defined headers. This can
                           result in two (or more) headers having the same name.
        '''
        if append:
            self._headers.setdefault(_hkey(name), []).append(str(value))
        else:
            self._headers[_hkey(name)] = [str(value)]

    def iter_headers(self):
        ''' Yield (header, value) tuples, skipping headers that are not
            allowed with the current response status code. '''
        headers = self._headers.iteritems()
        bad_headers = self.bad_headers.get(self.status_code)
        if bad_headers:
            headers = (h for h in headers if h[0] not in bad_headers)
        for name, values in headers:
            for value in values:
                yield name, value
        if self._cookies:
            for c in self._cookies.values():
                yield 'Set-Cookie', c.OutputString()

    def wsgiheader(self):
        depr('The wsgiheader method is deprecated. See headerlist.')  # 0.10
        return self.headerlist

    @property
    def headerlist(self):
        ''' WSGI conform list of (header, value) tuples. '''
        return list(self.iter_headers())

    content_type = HeaderProperty('Content-Type')
    content_length = HeaderProperty('Content-Length', reader=int)

    @property
    def charset(self):
        """ Return the charset specified in the content-type header (default: utf8). """
        if 'charset=' in self.content_type:
            return self.content_type.split('charset=')[-1].split(';')[0].strip()
        return 'UTF-8'

    @property
    def COOKIES(self):
        """ A dict-like SimpleCookie instance. This should not be used directly.
            See :meth:`set_cookie`. """
        depr('The COOKIES dict is deprecated. Use `set_cookie()` instead.')  # 0.10
        if not self._cookies:
            self._cookies = SimpleCookie()
        return self._cookies

    def set_cookie(self, key, value, secret=None, **options):
        ''' Create a new cookie or replace an old one. If the `secret` parameter is
            set, create a `Signed Cookie` (described below).:param key: the name of the cookie.:param value: the value of the cookie.:param secret: a signature key required for signed cookies.

            Additionally, this method accepts all RFC 2109 attributes that are
            supported by :class:`cookie.Morsel`, including::param max_age: maximum age in seconds. (default: None):param expires: a datetime object or UNIX timestamp. (default: None):param domain: the domain that is allowed to read the cookie.
              (default: current domain):param path: limits the cookie to a given path (default: ``/``):param secure: limit the cookie to HTTPS connections (default: off).:param httponly: prevents client-side javascript to read this cookie
              (default: off, requires Python 2.6 or newer).

            If neither `expires` nor `max_age` is set (default), the cookie will
            expire at the end of the browser session (as soon as the browser
            window is closed).

            Signed cookies may store any pickle-able object and are
            cryptographically signed to prevent manipulation. Keep in mind that
            cookies are limited to 4kb in most browsers.

            Warning: Signed cookies are not encrypted (the client can still see
            the content) and not copy-protected (the client can restore an old
            cookie). The main intention is to make pickling and unpickling
            save, not to store secret information at client side.
        '''
        if not self._cookies:
            self._cookies = SimpleCookie()

        if secret:
            value = touni(cookie_encode((key, value), secret))
        elif not isinstance(value, basestring):
            raise TypeError('Secret key missing for non-string Cookie.')

        self._cookies[key] = value
        for k, v in options.iteritems():
            self._cookies[key][k.replace('_', '-')] = v

    def delete_cookie(self, key, **kwargs):
        ''' Delete a cookie. Be sure to use the same `domain` and `path`
            settings as used to create the cookie. '''
        kwargs['max_age'] = -1
        kwargs['expires'] = 0
        self.set_cookie(key, '', **kwargs)


class LocalResponse(BaseResponse, threading.local):
    ''' A thread-local subclass of :class:`BaseResponse`. '''
    bind = BaseResponse.__init__

Response = LocalResponse

###############################################################################
# Plugins ######################################################################
###############################################################################

class JSONPlugin(object):
    name = 'json'

    def __init__(self, json_dumps=json_dumps):
        self.json_dumps = json_dumps

    def apply(self, callback, context):
        dumps = self.json_dumps
        if not dumps:
            return callback

        def wrapper(*a, **ka):
            rv = callback(*a, **ka)
            if isinstance(rv, dict):
                response.content_type = 'application/json'
                return dumps(rv)
            return rv
        return wrapper


class HooksPlugin(object):
    name = 'hooks'

    def __init__(self):
        self.hooks = {'before_request': [], 'after_request': []}
        self.app = None

    def _empty(self):
        return not (self.hooks['before_request'] or self.hooks['after_request'])

    def setup(self, app):
        self.app = app

    def add(self, name, func):
        ''' Attach a callback to a hook. '''
        if name not in self.hooks:
            raise ValueError("Unknown hook name %s" % name)
        was_empty = self._empty()
        self.hooks[name].append(func)
        if self.app and was_empty and not self._empty():
            self.app.reset()

    def remove(self, name, func):
        ''' Remove a callback from a hook. '''
        if name not in self.hooks:
            raise ValueError("Unknown hook name %s" % name)
        was_empty = self._empty()
        self.hooks[name].remove(func)
        if self.app and not was_empty and self._empty():
            self.app.reset()

    def apply(self, callback, context):
        if self._empty():
            return callback
        before_request = self.hooks['before_request']
        after_request = self.hooks['after_request']

        def wrapper(*a, **ka):
            for hook in before_request:
                hook()
            rv = callback(*a, **ka)
            for hook in after_request[::-1]:
                hook()
            return rv
        return wrapper


class TypeFilterPlugin(object):
    def __init__(self):
        self.filter = []
        self.app = None

    def setup(self, app):
        self.app = app

    def add(self, ftype, func):
        if not isinstance(ftype, type):
            raise TypeError("Expected type object, got %s" % type(ftype))
        self.filter = [(t, f) for (t, f) in self.filter if t != ftype]
        self.filter.append((ftype, func))
        if len(self.filter) == 1 and self.app:
            self.app.reset()

    def apply(self, callback, context):
        filter = self.filter
        if not filter:
            return callback

        def wrapper(*a, **ka):
            rv = callback(*a, **ka)
            for testtype, filterfunc in filter:
                if isinstance(rv, testtype):
                    rv = filterfunc(rv)
            return rv
        return wrapper


class TemplatePlugin(object):
    ''' This plugin applies the :func:`view` decorator to all routes with a
        `template` config parameter. If the parameter is a tuple, the second
        element must be a dict with additional options (e.g. `template_engine`)
        or default variables for the template. '''
    name = 'template'

    def apply(self, callback, context):
        conf = context['config'].get('template')
        if isinstance(conf, (tuple, list)) and len(conf) == 2:
            return view(conf[0], **conf[1])(callback)
        elif isinstance(conf, str) and 'template_opts' in context['config']:
            depr('The `template_opts` parameter is deprecated.')  # 0.9
            return view(conf, **context['config']['template_opts'])(callback)
        elif isinstance(conf, str):
            return view(conf)(callback)
        else:
            return callback


#: Not a plugin, but part of the plugin API. TODO: Find a better place.
class _ImportRedirect(object):
    def __init__(self, name, impmask):
        ''' Create a virtual package that redirects imports (see PEP 302). '''
        self.name = name
        self.impmask = impmask
        self.module = sys.modules.setdefault(name, imp.new_module(name))
        self.module.__dict__.update({'__file__': '<virtual>', '__path__': [],
                                    '__all__': [], '__loader__': self})
        sys.meta_path.append(self)

    def find_module(self, fullname, path=None):
        if '.' not in fullname:
            return
        packname, modname = fullname.rsplit('.', 1)
        if packname != self.name:
            return
        return self

    def load_module(self, fullname):
        if fullname in sys.modules:
            return sys.modules[fullname]
        packname, modname = fullname.rsplit('.', 1)
        realname = self.impmask % modname
        __import__(realname)
        module = sys.modules[fullname] = sys.modules[realname]
        setattr(self.module, modname, module)
        module.__loader__ = self
        return module

###############################################################################
# Common Utilities #############################################################
###############################################################################

class MultiDict(DictMixin):
    """ This dict stores multiple values per key, but behaves exactly like a
        normal dict in that it returns only the newest value for any given key.
        There are special methods available to access the full list of values.
    """

    def __init__(self, *a, **k):
        self.dict = dict((k, [v]) for k, v in dict(*a, **k).iteritems())

    def __len__(self): return len(self.dict)
    def __iter__(self): return iter(self.dict)
    def __contains__(self, key): return key in self.dict
    def __delitem__(self, key): del self.dict[key]
    def __getitem__(self, key): return self.dict[key][-1]
    def __setitem__(self, key, value): self.append(key, value)
    def iterkeys(self): return self.dict.iterkeys()
    def itervalues(self): return (v[-1] for v in self.dict.itervalues())
    def iteritems(self): return ((k, v[-1]) for (k, v) in self.dict.iteritems())

    def iterallitems(self):
        for key, values in self.dict.iteritems():
            for value in values:
                yield key, value

    # 2to3 is not able to fix these automatically.
    keys     = iterkeys     if py3k else lambda self: list(self.iterkeys())
    values   = itervalues   if py3k else lambda self: list(self.itervalues())
    items    = iteritems    if py3k else lambda self: list(self.iteritems())
    allitems = iterallitems if py3k else lambda self: list(self.iterallitems())

    def get(self, key, default=None, index=-1):
        ''' Return the current value for a key. The third `index` parameter
            defaults to -1 (last value). '''
        if key in self.dict or default is KeyError:
            return self.dict[key][index]
        return default

    def append(self, key, value):
        ''' Add a new value to the list of values for this key. '''
        self.dict.setdefault(key, []).append(value)

    def replace(self, key, value):
        ''' Replace the list of values with a single value. '''
        self.dict[key] = [value]

    def getall(self, key):
        ''' Return a (possibly empty) list of values for a key. '''
        return self.dict.get(key) or []


class HeaderDict(MultiDict):
    """ A case-insensitive version of :class:`MultiDict` that defaults to
        replace the old value instead of appending it. """

    def __init__(self, *a, **ka):
        self.dict = {}
        if a or ka:
            self.update(*a, **ka)

    def __contains__(self, key): return _hkey(key) in self.dict
    def __delitem__(self, key): del self.dict[_hkey(key)]
    def __getitem__(self, key): return self.dict[_hkey(key)][-1]
    def __setitem__(self, key, value): self.dict[_hkey(key)] = [str(value)]

    def append(self, key, value):
        self.dict.setdefault(_hkey(key), []).append(str(value))

    def replace(self, key, value):
        self.dict[_hkey(key)] = [str(value)]

    def getall(self, key):
        return self.dict.get(_hkey(key)) or []

    def get(self, key, default=None, index=-1):
        return MultiDict.get(self, _hkey(key), default, index)

    def filter(self, names):
        for name in map(_hkey, names):
            if name in self.dict:
                del self.dict[name]


class WSGIHeaderDict(DictMixin):
    ''' This dict-like class wraps a WSGI environ dict and provides convenient
        access to HTTP_* fields. Keys and values are native strings
        (2.x bytes or 3.x unicode) and keys are case-insensitive. If the WSGI
        environment contains non-native string values, these are de- or encoded
        using a lossless 'latin1' character set.

        The API will remain stable even on changes to the relevant PEPs.
        Currently PEP 333, 444 and 3333 are supported. (PEP 444 is the only one
        that uses non-native strings.)
    '''
    #: List of keys that do not have a 'HTTP_' prefix.
    cgikeys = ('CONTENT_TYPE', 'CONTENT_LENGTH')

    def __init__(self, environ):
        self.environ = environ

    def _ekey(self, key):
        ''' Translate header field name to CGI/WSGI environ key. '''
        key = key.replace('-', '_').upper()
        if key in self.cgikeys:
            return key
        return 'HTTP_' + key

    def raw(self, key, default=None):
        ''' Return the header value as is (may be bytes or unicode). '''
        return self.environ.get(self._ekey(key), default)

    def __getitem__(self, key):
        return tonat(self.environ[self._ekey(key)], 'latin1')

    def __setitem__(self, key, value):
        raise TypeError("%s is read-only." % self.__class__)

    def __delitem__(self, key):
        raise TypeError("%s is read-only." % self.__class__)

    def __iter__(self):
        for key in self.environ:
            if key[:5] == 'HTTP_':
                yield key[5:].replace('_', '-').title()
            elif key in self.cgikeys:
                yield key.replace('_', '-').title()

    def keys(self): return list(self)
    def __len__(self): return len(list(self))
    def __contains__(self, key): return self._ekey(key) in self.environ


class AppStack(list):
    """ A stack-like list. Calling it returns the head of the stack. """

    def __call__(self):
        """ Return the current default application. """
        return self[-1]

    def push(self, value=None):
        """ Add a new :class:`Bottle` instance to the stack """
        if not isinstance(value, Bottle):
            value = Bottle()
        self.append(value)
        return value


class WSGIFileWrapper(object):

    def __init__(self, fp, buffer_size=1024*64):
        self.fp, self.buffer_size = fp, buffer_size
        for attr in ('fileno', 'close', 'read', 'readlines'):
            if hasattr(fp, attr):
                setattr(self, attr, getattr(fp, attr))

    def __iter__(self):
        read, buff = self.fp.read, self.buffer_size
        while True:
            part = read(buff)
            if not part:
                break
            yield part

###############################################################################
# Application Helper ###########################################################
###############################################################################

def abort(code=500, text='Unknown Error: Application stopped.'):
    """ Aborts execution and causes a HTTP error. """
    raise HTTPError(code, text)


def redirect(url, code=303):
    """ Aborts execution and causes a 303 redirect. """
    location = urljoin(request.url, url)
    raise HTTPResponse("", status=code, header=dict(Location=location))


def static_file(filename, root, mimetype='auto', download=False):
    """ Open a file in a safe way and return :exc:`HTTPResponse` with status
        code 200, 305, 401 or 404. Set Content-Type, Content-Encoding,
        Content-Length and Last-Modified header. Obey If-Modified-Since header
        and HEAD requests.
    """
    root = os.path.abspath(root) + os.sep
    filename = os.path.abspath(os.path.join(root, filename.strip('/\\')))
    header = dict()

    if not filename.startswith(root):
        return HTTPError(403, "Access denied.")
    if not os.path.exists(filename) or not os.path.isfile(filename):
        return HTTPError(404, "File does not exist.")
    if not os.access(filename, os.R_OK):
        return HTTPError(403, "You do not have permission to access this file.")

    if mimetype == 'auto':
        mimetype, encoding = mimetypes.guess_type(filename)
        if mimetype:
            header['Content-Type'] = mimetype
        if encoding:
            header['Content-Encoding'] = encoding
    elif mimetype:
        header['Content-Type'] = mimetype

    if download:
        download = os.path.basename(filename if download == True else download)
        header['Content-Disposition'] = 'attachment; filename="%s"' % download

    stats = os.stat(filename)
    header['Content-Length'] = stats.st_size
    lm = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(stats.st_mtime))
    header['Last-Modified'] = lm

    ims = request.environ.get('HTTP_IF_MODIFIED_SINCE')
    if ims:
        ims = parse_date(ims.split(";")[0].strip())
    if ims is not None and ims >= int(stats.st_mtime):
        header['Date'] = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime())
        return HTTPResponse(status=304, header=header)

    body = '' if request.method == 'HEAD' else open(filename, 'rb')
    return HTTPResponse(body, header=header)

###############################################################################
# HTTP Utilities and MISC (TODO) ###############################################
###############################################################################

def debug(mode=True):
    """ Change the debug level.
    There is only one debug level supported at the moment."""
    global DEBUG
    DEBUG = bool(mode)


def parse_date(ims):
    """ Parse rfc1123, rfc850 and asctime timestamps and return UTC epoch. """
    try:
        ts = email.utils.parsedate_tz(ims)
        return time.mktime(ts[:8] + (0,)) - (ts[9] or 0) - time.timezone
    except (TypeError, ValueError, IndexError, OverflowError):
        return None


def parse_auth(header):
    """ Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None"""
    try:
        method, data = header.split(None, 1)
        if method.lower() == 'basic':
            # TODO: Add 2to3 save base64[encode/decode] functions.
            user, pwd = touni(base64.b64decode(tob(data))).split(':', 1)
            return user, pwd
    except (KeyError, ValueError):
        return None


def _lscmp(a, b):
    ''' Compares two strings in a cryptographically save way:
        Runtime is not affected by length of common prefix. '''
    return not sum(0 if x==y else 1 for x, y in zip(a, b)) and len(a) == len(b)


def cookie_encode(data, key):
    ''' Encode and sign a pickle-able object. Return a (byte) string '''
    msg = base64.b64encode(pickle.dumps(data, -1))
    sig = base64.b64encode(hmac.new(key, msg).digest())
    return tob('!') + sig + tob('?') + msg


def cookie_decode(data, key):
    ''' Verify and decode an encoded string. Return an object or None.'''
    data = tob(data)
    if cookie_is_encoded(data):
        sig, msg = data.split(tob('?'), 1)
        if _lscmp(sig[1:], base64.b64encode(hmac.new(key, msg).digest())):
            return pickle.loads(base64.b64decode(msg))
    return None


def cookie_is_encoded(data):
    ''' Return True if the argument looks like a encoded cookie.'''
    return bool(data.startswith(tob('!')) and tob('?') in data)


def yieldroutes(func):
    """ Return a generator for routes that match the signature (name, args)
    of the func parameter. This may yield more than one route if the function
    takes optional keyword arguments. The output is best described by example::

        a()         -> '/a'
        b(x, y)     -> '/b/:x/:y'
        c(x, y=5)   -> '/c/:x' and '/c/:x/:y'
        d(x=5, y=6) -> '/d' and '/d/:x' and '/d/:x/:y'
    """
    import inspect  # Expensive module. Only import if necessary.
    path = '/' + func.__name__.replace('__', '/').lstrip('/')
    spec = inspect.getargspec(func)
    argc = len(spec[0]) - len(spec[3] or [])
    path += ('/:%s' * argc) % tuple(spec[0][:argc])
    yield path
    for arg in spec[0][argc:]:
        path += '/:%s' % arg
        yield path


def path_shift(script_name, path_info, shift=1):
    ''' Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.

        :return: The modified paths.:param script_name: The SCRIPT_NAME path.:param script_name: The PATH_INFO path.:param shift: The number of path fragments to shift. May be negative to
          change the shift direction. (default: 1)
    '''
    if shift == 0:
        return script_name, path_info
    pathlist = path_info.strip('/').split('/')
    scriptlist = script_name.strip('/').split('/')
    if pathlist and pathlist[0] == '':
        pathlist = []
    if scriptlist and scriptlist[0] == '':
        scriptlist = []
    if shift > 0 and shift <= len(pathlist):
        moved = pathlist[:shift]
        scriptlist = scriptlist + moved
        pathlist = pathlist[shift:]
    elif shift < 0 and shift >= -len(scriptlist):
        moved = scriptlist[shift:]
        pathlist = moved + pathlist
        scriptlist = scriptlist[:shift]
    else:
        empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'
        raise AssertionError("Cannot shift. Nothing left from %s" % empty)
    new_script_name = '/' + '/'.join(scriptlist)
    new_path_info = '/' + '/'.join(pathlist)
    if path_info.endswith('/') and pathlist:
        new_path_info += '/'
    return new_script_name, new_path_info

# Decorators
# TODO: Replace default_app() with app()

def validate(**vkargs):
    """
    Validates and manipulates keyword arguments by user defined callables.
    Handles ValueError and missing arguments by raising HTTPError(403).
    """

    def decorator(func):
        def wrapper(**kargs):
            for key, value in vkargs.iteritems():
                if key not in kargs:
                    abort(403, 'Missing parameter: %s' % key)
                try:
                    kargs[key] = value(kargs[key])
                except ValueError:
                    abort(403, 'Wrong parameter format for: %s' % key)
            return func(**kargs)
        return wrapper
    return decorator


def auth_basic(check, realm="private", text="Access denied"):
    ''' Callback decorator to require HTTP auth (basic).
        TODO: Add route(check_auth=...) parameter. '''

    def decorator(func):
        def wrapper(*a, **ka):
            user, password = request.auth or (None, None)
            if user is None or not check(user, password):
                response.headers['WWW-Authenticate'] = 'Basic realm="%s"' % realm
                return HTTPError(401, text)
            return func(*a, **ka)
        return wrapper
    return decorator


def make_default_app_wrapper(name):
    ''' Return a callable that relays calls to the current default app. '''

    @functools.wraps(getattr(Bottle, name))
    def wrapper(*a, **ka):
        return getattr(app(), name)(*a, **ka)
    return wrapper


for name in '''route get post put delete error mount
               hook install uninstall'''.split():
    globals()[name] = make_default_app_wrapper(name)
url = make_default_app_wrapper('get_url')
del name

###############################################################################
# Server Adapter ###############################################################
###############################################################################

class ServerAdapter(object):
    quiet = False

    def __init__(self, host='127.0.0.1', port=8080, **config):
        self.options = config
        self.host = host
        self.port = int(port)

    def run(self, handler):  # pragma: no cover
        pass

    def __repr__(self):
        args = ', '.join(['%s=%s' % (k, repr(v)) for k, v in self.options.items()])
        return "%s(%s)" % (self.__class__.__name__, args)


class CGIServer(ServerAdapter):
    quiet = True

    def run(self, handler):  # pragma: no cover
        from wsgiref.handlers import CGIHandler

        def fixed_environ(environ, start_response):
            environ.setdefault('PATH_INFO', '')
            return handler(environ, start_response)
        CGIHandler().run(fixed_environ)


class FlupFCGIServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        import flup.server.fcgi
        kwargs = {'bindAddress': (self.host, self.port)}
        kwargs.update(self.options)  # allow to override bindAddress and others
        flup.server.fcgi.WSGIServer(handler, **kwargs).run()

class FlupSCGIServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        import flup.server.scgi
        kwargs = {'bindAddress': (self.host, self.port)}
        kwargs.update(self.options)  # allow to override bindAddress and others
        flup.server.scgi.WSGIServer(handler, **kwargs).run()


class WSGIRefServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from wsgiref.simple_server import make_server, WSGIRequestHandler
        print "Launching Swsgi backend"
        if self.quiet:
            class QuietHandler(WSGIRequestHandler):
                def log_request(*args, **kw): pass
            self.options['handler_class'] = QuietHandler
        srv = make_server(self.host, self.port, handler, **self.options)
        srv.serve_forever()


## Shinken: add WSGIRefServerSelect
class WSGIRefServerSelect(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        print "Call the Select version"
        from wsgiref.simple_server import make_server, WSGIRequestHandler
        if self.quiet:
            class QuietHandler(WSGIRequestHandler):
                def log_request(*args, **kw): pass
            self.options['handler_class'] = QuietHandler
        srv = make_server(self.host, self.port, handler, **self.options)
        # srv.serve_forever()
        return srv


class CherryPyServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from cherrypy import wsgiserver
        print "Launching CherryPy backend"
        server = wsgiserver.CherryPyWSGIServer((self.host, self.port), handler)
        try:
            server.start()
        finally:
            server.stop()


class PasteServer(ServerAdapter):
    def run(self, handler):  # pragma: no cover
        from paste import httpserver
        print "Launching Paste backend"
        if not self.quiet:
            from paste.translogger import TransLogger
            handler = TransLogger(handler)
        httpserver.serve(handler, host=self.host, port=str(self.port),
                         **self.options)


class MeinheldServer(ServerAdapter):
    def run(self, handler):
        from meinheld import server
        server.listen((self.host, self.port))
        server.run(handler)


class FapwsServer(ServerAdapter):
    """ Extremely fast webserver using libev. See http://www.fapws.org/ """

    def run(self, handler):  # pragma: no cover
        import fapws._evwsgi as evwsgi
        from fapws import base, config
        port = self.port
        if float(config.SERVER_IDENT[-2:]) > 0.4:
            # fapws3 silently changed its API in 0.5
            port = str(port)
        evwsgi.start(self.host, port)
        # fapws3 never releases the GIL. Complain upstream. I tried. No luck.
        if 'BOTTLE_CHILD' in os.environ and not self.quiet:
            print "WARNING: Auto-reloading does not work with Fapws3."
            print "         (Fapws3 breaks python thread support)"
        evwsgi.set_base_module(base)

        def app(environ, start_response):
            environ['wsgi.multiprocess'] = False
            return handler(environ, start_response)
        evwsgi.wsgi_cb(('', app))
        evwsgi.run()


class TornadoServer(ServerAdapter):
    """ The super hyped asynchronous server by facebook. Untested. """

    def run(self, handler):  # pragma: no cover
        import tornado.wsgi
        import tornado.httpserver
        import tornado.ioloop
        container = tornado.wsgi.WSGIContainer(handler)
        server = tornado.httpserver.HTTPServer(container)
        server.listen(port=self.port)
        tornado.ioloop.IOLoop.instance().start()


class AppEngineServer(ServerAdapter):
    """ Adapter for Google App Engine. """
    quiet = True

    def run(self, handler):
        from google.appengine.ext.webapp import util
        # A main() function in the handler script enables 'App Caching'.
        # Lets makes sure it is there. This _really_ improves performance.
        module = sys.modules.get('__main__')
        if module and not hasattr(module, 'main'):
            module.main = lambda: util.run_wsgi_app(handler)
        util.run_wsgi_app(handler)


class TwistedServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from twisted.web import server, wsgi
        from twisted.python.threadpool import ThreadPool
        from twisted.internet import reactor
        thread_pool = ThreadPool()
        thread_pool.start()
        reactor.addSystemEventTrigger('after', 'shutdown', thread_pool.stop)
        factory = server.Site(wsgi.WSGIResource(reactor, thread_pool, handler))
        reactor.listenTCP(self.port, factory, interface=self.host)
        reactor.run()


class DieselServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from diesel.protocols.wsgi import WSGIApplication
        app = WSGIApplication(handler, port=self.port)
        app.run()


class GeventServer(ServerAdapter):
    """ Untested. Options:

        * `monkey` (default: True) fixes the stdlib to use greenthreads.
        * `fast` (default: False) uses libevent's http server, but has some
          issues: No streaming, no pipelining, no SSL.
    """

    def run(self, handler):
        from gevent import wsgi as wsgi_fast, pywsgi, monkey
        if self.options.get('monkey', True):
            monkey.patch_all()
        wsgi = wsgi_fast if self.options.get('fast') else pywsgi
        wsgi.WSGIServer((self.host, self.port), handler).serve_forever()


class GunicornServer(ServerAdapter):
    """ Untested. """

    def run(self, handler):
        from gunicorn.arbiter import Arbiter
        from gunicorn.config import Config
        handler.cfg = Config({'bind': "%s:%d" % (self.host, self.port), 'workers': 4})
        arbiter = Arbiter(handler)
        arbiter.run()


class EventletServer(ServerAdapter):
    """ Untested """

    def run(self, handler):
        from eventlet import wsgi, listen
        wsgi.server(listen((self.host, self.port)), handler)


class RocketServer(ServerAdapter):
    """ Untested. As requested in issue 63
        https://github.com/defnull/bottle/issues/#issue/63 """

    def run(self, handler):
        from rocket import Rocket
        server = Rocket((self.host, self.port), 'wsgi', {'wsgi_app': handler})
        server.start()


class BjoernServer(ServerAdapter):
    """ Screamingly fast server written in C: https://github.com/jonashaag/bjoern """

    def run(self, handler):
        from bjoern import run
        run(handler, self.host, self.port)


class AutoServer(ServerAdapter):
    """ Untested. """
    adapters = [PasteServer, CherryPyServer, TwistedServer, WSGIRefServer]

    def run(self, handler):
        for sa in self.adapters:
            try:
                return sa(self.host, self.port, **self.options).run(handler)
            except ImportError:
                pass

## Shinken: add 'wsgirefselect': WSGIRefServerSelect,
server_names = {
    'cgi': CGIServer,
    'flup': FlupFCGIServer,
    'flupscgi': FlupSCGIServer,
    'wsgiref': WSGIRefServer,
    'wsgirefselect': WSGIRefServerSelect,
    'cherrypy': CherryPyServer,
    'paste': PasteServer,
    'fapws3': FapwsServer,
    'tornado': TornadoServer,
    'gae': AppEngineServer,
#    'twisted': TwistedServer,
    'diesel': DieselServer,
    'meinheld': MeinheldServer,
    'gunicorn': GunicornServer,
    'eventlet': EventletServer,
    'gevent': GeventServer,
    'rocket': RocketServer,
    'bjoern': BjoernServer,
    'auto': AutoServer,
}

###############################################################################
# Application Control ##########################################################
###############################################################################

def _load(target, **vars):
    """ Fetch something from a module. The exact behavior depends on the
        target string:

        If the target is a valid python import path (e.g. `package.module`),
        the rightmost part is returned as a module object.
        If the target contains a colon (e.g. `package.module:var`) the module
        variable specified after the colon is returned.
        If the part after the colon contains any non-alphanumeric characters
        (e.g. `package.module:func(var)`) the result of the expression
        is returned. The expression has access to keyword arguments supplied
        to this function.

        Example::
        >>> _load('bottle')
        <module 'bottle' from 'bottle.py'>
        >>> _load('bottle:Bottle')
        <class 'bottle.Bottle'>
        >>> _load('bottle:cookie_encode(v, secret)', v='foo', secret='bar')
        '!F+hN4dQxaDJ4QxxaZ+Z3jw==?gAJVA2Zvb3EBLg=='

    """
    module, target = target.split(":", 1) if ':' in target else (target, None)
    if module not in sys.modules:
        __import__(module)
    if not target:
        return sys.modules[module]
    if target.isalnum():
        return getattr(sys.modules[module], target)
    package_name = module.split('.')[0]
    vars[package_name] = sys.modules[package_name]
    return eval('%s.%s' % (module, target), vars)


def load_app(target):
    """ Load a bottle application based on a target string and return the
        application object.

        If the target is an import path (e.g. package.module), the application
        stack is used to isolate the routes defined in that module.
        If the target contains a colon (e.g. package.module:myapp) the
        module variable specified after the colon is returned instead.
    """
    tmp = app.push()  # Create a new "default application"
    rv = _load(target)  # Import the target module
    app.remove(tmp)  # Remove the temporary added default application
    return rv if isinstance(rv, Bottle) else tmp


## Shinken: add the return of the server
def run(app=None, server='wsgiref', host='127.0.0.1', port=8080,
        interval=1, reloader=False, quiet=False, **kargs):
    """ Start a server instance. This method blocks until the server terminates.:param app: WSGI application or target string supported by
               :func:`load_app`. (default: :func:`default_app`):param server: Server adapter to use. See :data:`server_names` keys
               for valid names or pass a :class:`ServerAdapter` subclass.
               (default: `wsgiref`):param host: Server address to bind to. Pass ``0.0.0.0`` to listens on
               all interfaces including the external one. (default: 127.0.0.1):param port: Server port to bind to. Values below 1024 require root
               privileges. (default: 8080):param reloader: Start auto-reloading server? (default: False):param interval: Auto-reloader interval in seconds (default: 1):param quiet: Suppress output to stdout and stderr? (default: False):param options: Options passed to the server adapter.
     """
    # Shinken
    res = None
    app = app or default_app()
    if isinstance(app, basestring):
        app = load_app(app)
    if isinstance(server, basestring):
        server = server_names.get(server)
    if isinstance(server, type):
        server = server(host=host, port=port, **kargs)
    if not isinstance(server, ServerAdapter):
        raise RuntimeError("Server must be a subclass of ServerAdapter")
    server.quiet = server.quiet or quiet
    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):
        print "Bottle server starting up (using %s)..." % repr(server)
        print "Listening on http://%s:%d/" % (server.host, server.port)
        print "Use Ctrl-C to quit."
        print
    try:
        if reloader:
            interval = min(interval, 1)
            if os.environ.get('BOTTLE_CHILD'):
                _reloader_child(server, app, interval)
            else:
                _reloader_observer(server, app, interval)
        else:
            # Shinken
            res = server.run(app)
    except KeyboardInterrupt:
        pass
    if not server.quiet and not os.environ.get('BOTTLE_CHILD'):
        print "Shutting down..."
    # Shinken
    return res


class FileCheckerThread(threading.Thread):
    ''' Thread that periodically checks for changed module files. '''

    def __init__(self, lockfile, interval):
        threading.Thread.__init__(self)
        self.lockfile, self.interval = lockfile, interval
        # 1: lockfile to old; 2: lockfile missing
        # 3: module file changed; 5: external exit
        self.status = 0

    def run(self):
        exists = os.path.exists
        mtime = lambda path: os.stat(path).st_mtime
        files = dict()
        for module in sys.modules.values():
            path = getattr(module, '__file__', '')
            if path[-4:] in ('.pyo', '.pyc'):
                path = path[:-1]
            if path and exists(path):
                files[path] = mtime(path)
        while not self.status:
            for path, lmtime in files.iteritems():
                if not exists(path) or mtime(path) > lmtime:
                    self.status = 3
            if not exists(self.lockfile):
                self.status = 2
            elif mtime(self.lockfile) < time.time() - self.interval - 5:
                self.status = 1
            if not self.status:
                time.sleep(self.interval)
        if self.status != 5:
            thread.interrupt_main()


def _reloader_child(server, app, interval):
    ''' Start the server and check for modified files in a background thread.
        As soon as an update is detected, KeyboardInterrupt is thrown in
        the main thread to exit the server loop. The process exists with status
        code 3 to request a reload by the observer process. If the lockfile
        is not modified in 2*interval second or missing, we assume that the
        observer process died and exit with status code 1 or 2.
    '''
    lockfile = os.environ.get('BOTTLE_LOCKFILE')
    bgcheck = FileCheckerThread(lockfile, interval)
    try:
        bgcheck.start()
        server.run(app)
    except KeyboardInterrupt:
        pass
    bgcheck.status, status = 5, bgcheck.status
    bgcheck.join()  # bgcheck.status == 5 --> silent exit
    if status:
        sys.exit(status)


def _reloader_observer(server, app, interval):
    ''' Start a child process with identical commandline arguments and restart
        it as long as it exists with status code 3. Also create a lockfile and
        touch it (update mtime) every interval seconds.
    '''
    fd, lockfile = tempfile.mkstemp(prefix='bottle-reloader.', suffix='.lock')
    os.close(fd)  # We only need this file to exist. We never write to it
    try:
        while os.path.exists(lockfile):
            args = [sys.executable] + sys.argv
            environ = os.environ.copy()
            environ['BOTTLE_CHILD'] = 'true'
            environ['BOTTLE_LOCKFILE'] = lockfile
            p = subprocess.Popen(args, env=environ)
            while p.poll() is None:  # Busy wait...
                os.utime(lockfile, None)  # I am alive!
                time.sleep(interval)
            if p.poll() != 3:
                if os.path.exists(lockfile):
                    os.unlink(lockfile)
                sys.exit(p.poll())
            elif not server.quiet:
                print "Reloading server..."
    except KeyboardInterrupt:
        pass
    if os.path.exists(lockfile):
        os.unlink(lockfile)

###############################################################################
# Template Adapters ############################################################
###############################################################################

class TemplateError(HTTPError):
    def __init__(self, message):
        HTTPError.__init__(self, 500, message)


class BaseTemplate(object):
    """ Base class and minimal API for template adapters """
    extentions = ['tpl', 'html', 'thtml', 'stpl']
    settings = {}  # used in prepare()
    defaults = {}  # used in render()

    def __init__(self, source=None, name=None, lookup=[], encoding='utf8', **settings):
        """ Create a new template.
        If the source parameter (str or buffer) is missing, the name argument
        is used to guess a template filename. Subclasses can assume that
        self.source and/or self.filename are set. Both are strings.
        The lookup, encoding and settings parameters are stored as instance
        variables.
        The lookup parameter stores a list containing directory paths.
        The encoding parameter should be used to decode byte strings or files.
        The settings parameter contains a dict for engine-specific settings.
        """
        self.name = name
        self.source = source.read() if hasattr(source, 'read') else source
        self.filename = source.filename if hasattr(source, 'filename') else None
        self.lookup = map(os.path.abspath, lookup)
        self.encoding = encoding
        self.settings = self.settings.copy()  # Copy from class variable
        self.settings.update(settings)  # Apply
        if not self.source and self.name:
            self.filename = self.search(self.name, self.lookup)
            if not self.filename:
                raise TemplateError('Template %s not found.' % repr(name))
        if not self.source and not self.filename:
            raise TemplateError('No template specified.')
        self.prepare(**self.settings)

    @classmethod
    def search(cls, name, lookup=[]):
        """ Search name in all directories specified in lookup.
        First without, then with common extensions. Return first hit. """
        if os.path.isfile(name):
            return name
        for spath in lookup:
            fname = os.path.join(spath, name)
            if os.path.isfile(fname):
                return fname
            for ext in cls.extentions:
                if os.path.isfile('%s.%s' % (fname, ext)):
                    return '%s.%s' % (fname, ext)

    @classmethod
    def global_config(cls, key, *args):
        ''' This reads or sets the global settings stored in class.settings. '''
        if args:
            cls.settings[key] = args[0]
        else:
            return cls.settings[key]

    def prepare(self, **options):
        """ Run preparations (parsing, caching, ...).
        It should be possible to call this again to refresh a template or to
        update settings.
        """
        raise NotImplementedError

    def render(self, *args, **kwargs):
        """ Render the template with the specified local variables and return
        a single byte or unicode string. If it is a byte string, the encoding
        must match self.encoding. This method must be thread-safe!
        Local variables may be provided in dictionaries (*args)
        or directly, as keywords (**kwargs).
        """
        raise NotImplementedError


class MakoTemplate(BaseTemplate):
    def prepare(self, **options):
        from mako.template import Template
        from mako.lookup import TemplateLookup
        options.update({'input_encoding': self.encoding})
        options.setdefault('format_exceptions', bool(DEBUG))
        lookup = TemplateLookup(directories=self.lookup, **options)
        if self.source:
            self.tpl = Template(self.source, lookup=lookup, **options)
        else:
            self.tpl = Template(uri=self.name, filename=self.filename, lookup=lookup, **options)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)


class CheetahTemplate(BaseTemplate):
    def prepare(self, **options):
        from Cheetah.Template import Template
        self.context = threading.local()
        self.context.vars = {}
        options['searchList'] = [self.context.vars]
        if self.source:
            self.tpl = Template(source=self.source, **options)
        else:
            self.tpl = Template(file=self.filename, **options)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        self.context.vars.update(self.defaults)
        self.context.vars.update(kwargs)
        out = str(self.tpl)
        self.context.vars.clear()
        return out


class Jinja2Template(BaseTemplate):
    def prepare(self, filters=None, tests=None, **kwargs):
        from jinja2 import Environment, FunctionLoader
        if 'prefix' in kwargs:  # TODO: to be removed after a while
            raise RuntimeError('The keyword argument `prefix` has been removed. '
                'Use the full jinja2 environment name line_statement_prefix instead.')
        self.env = Environment(loader=FunctionLoader(self.loader), **kwargs)
        if filters:
            self.env.filters.update(filters)
        if tests:
            self.env.tests.update(tests)
        if self.source:
            self.tpl = self.env.from_string(self.source)
        else:
            self.tpl = self.env.get_template(self.filename)

    def render(self, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)

    def loader(self, name):
        fname = self.search(name, self.lookup)
        if fname:
            with open(fname, "rb") as f:
                return f.read().decode(self.encoding)


class SimpleTALTemplate(BaseTemplate):
    ''' Untested! '''

    def prepare(self, **options):
        from simpletal import simpleTAL
        # TODO: add option to load METAL files during render
        if self.source:
            self.tpl = simpleTAL.compileHTMLTemplate(self.source)
        else:
            with open(self.filename, 'rb') as fp:
                self.tpl = simpleTAL.compileHTMLTemplate(tonat(fp.read()))

    def render(self, *args, **kwargs):
        from simpletal import simpleTALES
        from StringIO import StringIO
        for dictarg in args:
            kwargs.update(dictarg)
        # TODO: maybe reuse a context instead of always creating one
        context = simpleTALES.Context()
        for k, v in self.defaults.items():
            context.addGlobal(k, v)
        for k, v in kwargs.items():
            context.addGlobal(k, v)
        output = StringIO()
        self.tpl.expand(context, output)
        return output.getvalue()


class SimpleTemplate(BaseTemplate):
    blocks = ('if', 'elif', 'else', 'try', 'except', 'finally', 'for', 'while',
              'with', 'def', 'class')
    dedent_blocks = ('elif', 'else', 'except', 'finally')

    @lazy_attribute
    def re_pytokens(cls):
        ''' This matches comments and all kinds of quoted strings but does
            NOT match comments (#...) within quoted strings. (trust me) '''
        return re.compile(r'''
            (''(?!')|""(?!")|'{6}|"{6}    # Empty strings (all 4 types)
             |'(?:[^\\']|\\.)+?'          # Single quotes (')
             |"(?:[^\\"]|\\.)+?"          # Double quotes (")
             |'{3}(?:[^\\]|\\.|\n)+?'{3}  # Triple-quoted strings (')
             |"{3}(?:[^\\]|\\.|\n)+?"{3}  # Triple-quoted strings (")
             |\#.*                        # Comments
            )''', re.VERBOSE)

    def prepare(self, escape_func=cgi.escape, noescape=False):
        self.cache = {}
        enc = self.encoding
        self._str = lambda x: touni(x, enc)
        self._escape = lambda x: escape_func(touni(x, enc))
        if noescape:
            self._str, self._escape = self._escape, self._str

    @classmethod
    def split_comment(cls, code):
        """ Removes comments (#...) from python code. """
        if '#' not in code:
            return code
        #: Remove comments only (leave quoted strings as they are)
        subf = lambda m: '' if m.group(0)[0] == '#' else m.group(0)
        return re.sub(cls.re_pytokens, subf, code)

    @cached_property
    def co(self):
        return compile(self.code, self.filename or '<string>', 'exec')

    @cached_property
    def code(self):
        stack = []  # Current Code indentation
        lineno = 0  # Current line of code
        ptrbuffer = []  # Buffer for printable strings and token tuple instances
        codebuffer = []  # Buffer for generated python code
        multiline = dedent = oneline = False
        template = self.source if self.source else open(self.filename).read()

        def yield_tokens(line):
            for i, part in enumerate(re.split(r'\{\{(.*?)\}\}', line)):
                if i % 2:
                    if part.startswith('!'):
                        yield 'RAW', part[1:]
                    else:
                        yield 'CMD', part
                else:
                    yield 'TXT', part

        def flush():  # Flush the ptrbuffer
            if not ptrbuffer:
                return
            cline = ''
            for line in ptrbuffer:
                for token, value in line:
                    if token == 'TXT':
                        cline += repr(value)
                    elif token == 'RAW':
                        cline += '_str(%s)' % value
                    elif token == 'CMD':
                        cline += '_escape(%s)' % value
                    cline += ', '
                cline = cline[:-2] + '\\\n'
            cline = cline[:-2]
            if cline[:-1].endswith('\\\\\\\\\\n'):
                cline = cline[:-7] + cline[-1]  # 'nobr\\\\\n' --> 'nobr'
            cline = '_printlist([' + cline + '])'
            del ptrbuffer[:]  # Do this before calling code() again
            code(cline)

        def code(stmt):
            for line in stmt.splitlines():
                codebuffer.append('  ' * len(stack) + line.strip())

        for line in template.splitlines(True):
            lineno += 1
            line = line if isinstance(line, unicode)\
                        else unicode(line, encoding=self.encoding)
            if lineno <= 2:
                m = re.search(r"%.*coding[:=]\s*([-\w\.]+)", line)
                if m:
                    self.encoding = m.group(1)
                if m:
                    line = line.replace('coding', 'coding (removed)')
            if line.strip()[:2].count('%') == 1:
                line = line.split('%', 1)[1].lstrip()  # Full line following the %
                cline = self.split_comment(line).strip()
                cmd = re.split(r'[^a-zA-Z0-9_]', cline)[0]
                flush() ## encoding (TODO: why?)
                if cmd in self.blocks or multiline:
                    cmd = multiline or cmd
                    dedent = cmd in self.dedent_blocks  # "else:"
                    if dedent and not oneline and not multiline:
                        cmd = stack.pop()
                    code(line)
                    oneline = not cline.endswith(':')  # "if 1: pass"
                    multiline = cmd if cline.endswith('\\') else False
                    if not oneline and not multiline:
                        stack.append(cmd)
                elif cmd == 'end' and stack:
                    code('#end(%s) %s' % (stack.pop(), line.strip()[3:]))
                elif cmd == 'include':
                    p = cline.split(None, 2)[1:]
                    if len(p) == 2:
                        code("_=_include(%s, _stdout, %s)" % (repr(p[0]), p[1]))
                    elif p:
                        code("_=_include(%s, _stdout)" % repr(p[0]))
                    else:  # Empty %include -> reverse of %rebase
                        code("_printlist(_base)")
                elif cmd == 'rebase':
                    p = cline.split(None, 2)[1:]
                    if len(p) == 2:
                        code("globals()['_rebase']=(%s, dict(%s))" % (repr(p[0]), p[1]))
                    elif p:
                        code("globals()['_rebase']=(%s, {})" % repr(p[0]))
                else:
                    code(line)
            else:  # Line starting with text (not '%') or '%%' (escaped)
                if line.strip().startswith('%%'):
                    line = line.replace('%%', '%', 1)
                ptrbuffer.append(yield_tokens(line))
        flush()
        return '\n'.join(codebuffer) + '\n'

    def subtemplate(self, _name, _stdout, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        if _name not in self.cache:
            self.cache[_name] = self.__class__(name=_name, lookup=self.lookup)
        return self.cache[_name].execute(_stdout, kwargs)

    def execute(self, _stdout, *args, **kwargs):
        for dictarg in args:
            kwargs.update(dictarg)
        env = self.defaults.copy()
        env.update({'_stdout': _stdout, '_printlist': _stdout.extend,
               '_include': self.subtemplate, '_str': self._str,
               '_escape': self._escape})
        env.update(kwargs)
        eval(self.co, env)
        if '_rebase' in env:
            subtpl, rargs = env['_rebase']
            subtpl = self.__class__(name=subtpl, lookup=self.lookup)
            rargs['_base'] = _stdout[:]  # copy stdout
            del _stdout[:]  # clear stdout
            return subtpl.execute(_stdout, rargs)
        return env

    def render(self, *args, **kwargs):
        """ Render the template using keyword arguments as local variables. """
        for dictarg in args:
            kwargs.update(dictarg)
        stdout = []
        self.execute(stdout, kwargs)
        return ''.join(stdout)


def template(*args, **kwargs):
    '''
    Get a rendered template as a string iterator.
    You can use a name, a filename or a template string as first parameter.
    Template rendering arguments can be passed as dictionaries
    or directly (as keyword arguments).
    '''
    tpl = args[0] if args else None
    template_adapter = kwargs.pop('template_adapter', SimpleTemplate)
    if tpl not in TEMPLATES or DEBUG:
        settings = kwargs.pop('template_settings', {})
        lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)
        if isinstance(tpl, template_adapter):
            TEMPLATES[tpl] = tpl
            if settings:
                TEMPLATES[tpl].prepare(**settings)
        elif "\n" in tpl or "{" in tpl or "%" in tpl or '$' in tpl:
            TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, **settings)
        else:
            TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, **settings)
    if not TEMPLATES[tpl]:
        abort(500, 'Template (%s) not found' % tpl)
    for dictarg in args[1:]:
        kwargs.update(dictarg)
    return TEMPLATES[tpl].render(kwargs)

mako_template = functools.partial(template, template_adapter=MakoTemplate)
cheetah_template = functools.partial(template, template_adapter=CheetahTemplate)
jinja2_template = functools.partial(template, template_adapter=Jinja2Template)
simpletal_template = functools.partial(template, template_adapter=SimpleTALTemplate)


def view(tpl_name, **defaults):
    ''' Decorator: renders a template for a handler.
        The handler can control its behavior like that:

          - return a dict of template vars to fill out the template
          - return something other than a dict and the view decorator will not
            process the template, but return the handler result as is.
            This includes returning a HTTPResponse(dict) to get,
            for instance, JSON with autojson or other castfilters.
    '''

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            if isinstance(result, (dict, DictMixin)):
                tplvars = defaults.copy()
                tplvars.update(result)
                return template(tpl_name, **tplvars)
            return result
        return wrapper
    return decorator

mako_view = functools.partial(view, template_adapter=MakoTemplate)
cheetah_view = functools.partial(view, template_adapter=CheetahTemplate)
jinja2_view = functools.partial(view, template_adapter=Jinja2Template)
simpletal_view = functools.partial(view, template_adapter=SimpleTALTemplate)
###############################################################################
# Constants and Globals ########################################################
###############################################################################

TEMPLATE_PATH = ['./', './views/']
TEMPLATES = {}
DEBUG = False

#: A dict to map HTTP status codes (e.g. 404) to phrases (e.g. 'Not Found')
HTTP_CODES = httplib.responses
HTTP_CODES[418] = "I'm a teapot"  # RFC 2324
_HTTP_STATUS_LINES = dict((k, '%d %s' % (k, v)) for (k, v) in HTTP_CODES.iteritems())

#: The default template used for error pages. Override with @error()
### SHINKEN MOD: change from bottle import DEBUG to from shinken.webui.bottle import DEBUG,...
ERROR_PAGE_TEMPLATE = """
%try:
    %from shinken.webui.bottlewebui import DEBUG, HTTP_CODES, request, touni
    %status_name = HTTP_CODES.get(e.status, 'Unknown').title()
    <!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
    <html>
        <head>
            <title>Error {{e.status}}: {{status_name}}</title>
            <style type="text/css">
              html {background-color: #eee; font-family: sans;}
              body {background-color: #fff; border: 1px solid #ddd;
                    padding: 15px; margin: 15px;}
              pre {background-color: #eee; border: 1px solid #ddd; padding: 5px;}
            </style>
        </head>
        <body>
            <h1>Error {{e.status}}: {{status_name}}</h1>
            <p>Sorry, the requested URL <tt>{{repr(request.url)}}</tt>
               caused an error:</p>
            <pre>{{e.output}}</pre>
            %if DEBUG and e.exception:
              <h2>Exception:</h2>
              <pre>{{repr(e.exception)}}</pre>
            %end
            %if DEBUG and e.traceback:
              <h2>Traceback:</h2>
              <pre>{{e.traceback}}</pre>
            %end
        </body>
    </html>
%except ImportError:
    <b>ImportError:</b> Could not generate the error page. Please add bottle to
    the import path.
%end
"""

#: A thread-save instance of :class:`Request` representing the `current` request.
request = Request()

#: A thread-save instance of :class:`Response` used to build the HTTP response.
response = Response()

#: A thread-save namespace. Not used by Bottle.
local = threading.local()

# Initialize app stack (create first empty Bottle app)
# BC: 0.6.4 and needed for run()
app = default_app = AppStack()
app.push()

#: A virtual package that redirects import statements.
#: Example: ``import bottle.ext.sqlite`` actually imports `bottle_sqlite`.
ext = _ImportRedirect(__name__ + '.ext', 'bottle_%s').module

########NEW FILE########
__FILENAME__ = worker
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#     Gabes Jean, naparuba@gmail.com
#     Gerhard Lausser, Gerhard.Lausser@consol.de
#     Gregory Starck, g.starck@gmail.com
#     Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

from Queue import Empty

# In android, we should use threads, not process
is_android = True
try:
    import android
except ImportError:
    is_android = False

if not is_android:
    from multiprocessing import Process, Queue
else:
    from Queue import Queue
    from threading import Thread as Process

import os
import time
import sys
import signal
import traceback
import cStringIO


from log import logger


class Worker:
    """This class is used for poller and reactionner to work.
    The worker is a process launch by theses process and read Message in a Queue
    (self.s) (slave)
    They launch the Check and then send the result in the Queue self.m (master)
    they can die if they do not do anything (param timeout)

    """

    id = 0  # None
    _process = None
    _mortal = None
    _idletime = None
    _timeout = None
    _c = None

    def __init__(self, id, s, returns_queue, processes_by_worker, mortal=True, timeout=300, max_plugins_output_length=8192, target=None, loaded_into='unknown', http_daemon=None):
        self.id = self.__class__.id
        self.__class__.id += 1

        self._mortal = mortal
        self._idletime = 0
        self._timeout = timeout
        self.s = None
        self.processes_by_worker = processes_by_worker
        self._c = Queue()  # Private Control queue for the Worker
        # By default, take our own code
        if target is None:
            target = self.work
        self._process = Process(target=target, args=(s, returns_queue, self._c))
        self.returns_queue = returns_queue
        self.max_plugins_output_length = max_plugins_output_length
        self.i_am_dying = False
        # Keep a trace where the worker is launch from (poller or reactionner?)
        self.loaded_into = loaded_into
        if os.name != 'nt':
            self.http_daemon = http_daemon
        else: #windows forker do not like pickle http/lock
            self.http_daemon = None
        

    def is_mortal(self):
        return self._mortal


    def start(self):
        self._process.start()


    # Kill the background process
    # AND close correctly the queues (input and output)
    # each queue got a thread, so close it too....
    def terminate(self):
        # We can just terminate process, not threads
        if not is_android:
            self._process.terminate()
        # Is we are with a Manager() way
        # there should be not such functions
        if hasattr(self._c, 'close'):
            self._c.close()
            self._c.join_thread()
        if hasattr(self.s, 'close'):
            self.s.close()
            self.s.join_thread()

    def join(self, timeout=None):
        self._process.join(timeout)

    def is_alive(self):
        return self._process.is_alive()

    def is_killable(self):
        return self._mortal and self._idletime > self._timeout

    def add_idletime(self, time):
        self._idletime = self._idletime + time

    def reset_idle(self):
        self._idletime = 0

    def send_message(self, msg):
        self._c.put(msg)

    # A zombie is immortal, so kill not be kill anymore
    def set_zombie(self):
        self._mortal = False

    # Get new checks if less than nb_checks_max
    # If no new checks got and no check in queue,
    # sleep for 1 sec
    # REF: doc/shinken-action-queues.png (3)
    def get_new_checks(self):
        try:
            while(len(self.checks) < self.processes_by_worker):
                #print "I", self.id, "wait for a message"
                msg = self.s.get(block=False)
                if msg is not None:
                    self.checks.append(msg.get_data())
                #print "I", self.id, "I've got a message!"
        except Empty, exp:
            if len(self.checks) == 0:
                self._idletime = self._idletime + 1
                time.sleep(1)
        # Maybe the Queue() is not available, if so, just return
        # get back to work :)
        except IOError, exp:
            return


    # Launch checks that are in status
    # REF: doc/shinken-action-queues.png (4)
    def launch_new_checks(self):
        # queue
        for chk in self.checks:
            if chk.status == 'queue':
                self._idletime = 0
                r = chk.execute()
                # Maybe we got a true big problem in the
                # action launching
                if r == 'toomanyopenfiles':
                    # We should die as soon as we return all checks
                    logger.error("[%d] I am dying Too many open files %s ... " % (self.id, chk))
                    self.i_am_dying = True


    # Check the status of checks
    # if done, return message finished :)
    # REF: doc/shinken-action-queues.png (5)
    def manage_finished_checks(self):
        to_del = []
        wait_time = 1
        now = time.time()
        for action in self.checks:
            if action.status == 'launched' and action.last_poll < now - action.wait_time:
                action.check_finished(self.max_plugins_output_length)
                wait_time = min(wait_time, action.wait_time)
                # If action done, we can launch a new one
            if action.status in ('done', 'timeout'):
                to_del.append(action)
                # We answer to the master
                #msg = Message(id=self.id, type='Result', data=action)
                try:
                    self.returns_queue.put(action)
                except IOError, exp:
                    logger.error("[%d] Exiting: %s" % (self.id, exp))
                    sys.exit(2)

        # Little sleep
        self.wait_time = wait_time

        for chk in to_del:
            self.checks.remove(chk)

        # Little sleep
        time.sleep(wait_time)

    # Check if our system time change. If so, change our
    def check_for_system_time_change(self):
        now = time.time()
        difference = now - self.t_each_loop

        # Now set the new value for the tick loop
        self.t_each_loop = now

        # return the diff if it need, of just 0
        if abs(difference) > 900:
            return difference
        else:
            return 0


    # Wrapper function for work in order to catch the exception
    # to see the real work, look at do_work
    def work(self, s, returns_queue, c):
        try:
            self.do_work(s, returns_queue, c)
        # Catch any exception, try to print it and exit anyway
        except Exception, exp:
            output = cStringIO.StringIO()
            traceback.print_exc(file=output)
            logger.error("Worker '%d' exit with an unmanaged exception : %s" % (self.id, output.getvalue()))
            output.close()
            # Ok I die now
            raise


    # id = id of the worker
    # s = Global Queue Master->Slave
    # m = Queue Slave->Master
    # return_queue = queue managed by manager
    # c = Control Queue for the worker
    def do_work(self, s, returns_queue, c):
        ## restore default signal handler for the workers:
        # but on android, we are a thread, so don't do it
        if not is_android:
            signal.signal(signal.SIGTERM, signal.SIG_DFL)

        self.set_proctitle()

        print "I STOP THE http_daemon", self.http_daemon
        if self.http_daemon:
            self.http_daemon.shutdown()

        timeout = 1.0
        self.checks = []
        self.returns_queue = returns_queue
        self.s = s
        self.t_each_loop = time.time()
        while True:
            begin = time.time()
            msg = None
            cmsg = None

            # If we are dying (big problem!) we do not
            # take new jobs, we just finished the current one
            if not self.i_am_dying:
                # REF: doc/shinken-action-queues.png (3)
                self.get_new_checks()
                # REF: doc/shinken-action-queues.png (4)
                self.launch_new_checks()
            # REF: doc/shinken-action-queues.png (5)
            self.manage_finished_checks()

            # Now get order from master
            try:
                cmsg = c.get(block=False)
                if cmsg.get_type() == 'Die':
                    logger.debug("[%d] Dad say we are dying..." % self.id)
                    break
            except:
                pass

            # Look if we are dying, and if we finish all current checks
            # if so, we really die, our master poller will launch a new
            # worker because we were too weak to manage our job :(
            if len(self.checks) == 0 and self.i_am_dying:
                logger.warning("[%d] I DIE because I cannot do my job as I should (too many open files?)... forgot me please." % self.id)
                break

            # Manage a possible time change (our avant will be change with the diff)
            diff = self.check_for_system_time_change()
            begin += diff

            timeout -= time.time() - begin
            if timeout < 0:
                timeout = 1.0

    def set_proctitle(self):
        try:
            from setproctitle import setproctitle
            setproctitle("shinken-%s worker" % self.loaded_into)
        except:
            pass


########NEW FILE########
__FILENAME__ = test_spaces_in_commands
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_spaces_in_commands.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_port_2")
        ## for a in host.actions:
        ##     a.t_to_go = 0
        svc.schedule()
        for a in svc.actions:
            a.t_to_go = 0
        # the scheduler need to get this new checks in its own queues
        self.sched.get_new_actions()
        untaggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['None'])
        cc = untaggued_checks[0]
        # There must still be a sequence of 10 blanks
        self.assert_(cc.command.find("Port 2          ") != -1)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = hot_dep_export
#!/usr/bin/env python

import os
import sys
try:
    import json
except ImportError:
    # For old Python version, load
    # simple json (it can be hard json?! It's 2 functions guy!)
    try:
        import simplejson as json
    except ImportError:
        print "Error: you need the json or simplejson module for this script"
        sys.exit(0)

print "Argv", sys.argv

# Case 1 mean host0 is the father of host1
if sys.argv[1] == 'case1':
    d = [[["host", "test_host_0"], ["host", "test_host_1"]]]
if sys.argv[1] == 'case2':
    d = [[["host", "test_host_2"], ["host", "test_host_1"]]]

f = open(sys.argv[2], 'wb')
f.write(json.dumps(d))
f.close()

########NEW FILE########
__FILENAME__ = nmap_wrapper
#!/usr/bin/env python

s = """
srv1::os=windows
srv1::osversion=2003
srv1::macvendor=Hewlett Packard
srv1::openports=135,139,445,80
srv2::os=windows
srv2::osversion=7
srv2::macvendor=VMware
srv2::openports=80,135,139,445
"""
print s

########NEW FILE########
__FILENAME__ = shinken_modules
#!/usr/bin/env python


#
from  shinken_test import *
import shutil

#define_modules_dir("../modules")
modulesctx.set_modulesdir(modules_dir)

# Special Livestatus module opening since the module rename
#from shinken.modules.livestatus import module as livestatus_broker
livestatus_broker = modulesctx.get_module('livestatus')
LiveStatus_broker = livestatus_broker.LiveStatus_broker
LiveStatus = livestatus_broker.LiveStatus
LiveStatusRegenerator = livestatus_broker.LiveStatusRegenerator
LiveStatusQueryCache = livestatus_broker.LiveStatusQueryCache

Logline = livestatus_broker.Logline
LiveStatusLogStoreMongoDB = modulesctx.get_module('logstore-mongodb').LiveStatusLogStoreMongoDB
LiveStatusLogStoreSqlite = modulesctx.get_module('logstore-sqlite').LiveStatusLogStoreSqlite

from shinken.misc.datamanager import datamgr

livestatus_modconf = Module()
livestatus_modconf.module_name = "livestatus"
livestatus_modconf.module_type = livestatus_broker.properties['type']
livestatus_modconf.properties = livestatus_broker.properties.copy()

class ShinkenModulesTest(ShinkenTest):

    def do_load_modules(self):
        self.modules_manager.load_and_init()
        self.log.log("I correctly loaded the modules: [%s]" % (','.join([inst.get_name() for inst in self.modules_manager.instances])))



    def update_broker(self, dodeepcopy=False):
        # The brok should be manage in the good order
        ids = self.sched.brokers['Default-Broker']['broks'].keys()
        ids.sort()
        for brok_id in ids:
            brok = self.sched.brokers['Default-Broker']['broks'][brok_id]
            #print "Managing a brok type", brok.type, "of id", brok_id
            #if brok.type == 'update_service_status':
            #    print "Problem?", brok.data['is_problem']
            if dodeepcopy:
                brok = copy.deepcopy(brok)
            brok.prepare()
            self.livestatus_broker.manage_brok(brok)
        self.sched.brokers['Default-Broker']['broks'] = {}


    def init_livestatus(self, modconf=None, dbmodconf=None, needcache=False):
        self.livelogs = 'tmp/livelogs.db' + self.testid

        if modconf is None:
            modconf = Module({'module_name': 'LiveStatus',
                'module_type': 'livestatus',
                'port': str(50000 + os.getpid()),
                'pnp_path': 'tmp/pnp4nagios_test' + self.testid,
                'host': '127.0.0.1',
                'socket': 'live',
                'name': 'test', #?
            })

        if dbmodconf is None:
            dbmodconf = Module({'module_name': 'LogStore',
                'module_type': 'logstore_sqlite',
                'use_aggressive_sql': "0",
                'database_file': self.livelogs,
                'archive_path': os.path.join(os.path.dirname(self.livelogs), 'archives'),
            })

        modconf.modules = [dbmodconf]
        self.livestatus_broker = LiveStatus_broker(modconf)
        self.livestatus_broker.create_queues()

        #--- livestatus_broker.main
        self.livestatus_broker.log = logger
        # this seems to damage the logger so that the scheduler can't use it
        #self.livestatus_broker.log.load_obj(self.livestatus_broker)
        self.livestatus_broker.debug_output = []
        self.livestatus_broker.modules_manager = ModulesManager('livestatus', modules_dir, [])
        self.livestatus_broker.modules_manager.set_modules(self.livestatus_broker.modules)
        # We can now output some previouly silented debug ouput
        self.livestatus_broker.do_load_modules()
        for inst in self.livestatus_broker.modules_manager.instances:
            if inst.properties["type"].startswith('logstore'):
                f = getattr(inst, 'load', None)
                if f and callable(f):
                    f(self.livestatus_broker)  # !!! NOT self here !!!!
                break
        for s in self.livestatus_broker.debug_output:
            print "errors during load", s
        del self.livestatus_broker.debug_output
        self.livestatus_broker.rg = LiveStatusRegenerator()
        self.livestatus_broker.datamgr = datamgr
        datamgr.load(self.livestatus_broker.rg)
        self.livestatus_broker.query_cache = LiveStatusQueryCache()
        if not needcache:
            self.livestatus_broker.query_cache.disable()
        self.livestatus_broker.rg.register_cache(self.livestatus_broker.query_cache)
        #--- livestatus_broker.main

        self.livestatus_broker.init()
        self.livestatus_broker.db = self.livestatus_broker.modules_manager.instances[0]
        self.livestatus_broker.livestatus = LiveStatus(self.livestatus_broker.datamgr, self.livestatus_broker.query_cache, self.livestatus_broker.db, self.livestatus_broker.pnp_path, self.livestatus_broker.from_q)

        #--- livestatus_broker.do_main
        self.livestatus_broker.db.open()
        #--- livestatus_broker.do_main


class TestConfig(ShinkenModulesTest):

    def tearDown(self):
        self.stop_nagios()
        self.livestatus_broker.db.commit()
        self.livestatus_broker.db.close()
        if os.path.exists(self.livelogs):
            os.remove(self.livelogs)
        if os.path.exists(self.livelogs + "-journal"):
            os.remove(self.livelogs + "-journal")
        if os.path.exists(self.livestatus_broker.pnp_path):
            shutil.rmtree(self.livestatus_broker.pnp_path)
        if os.path.exists('var/shinken.log'):
            os.remove('var/shinken.log')
        if os.path.exists('var/retention.dat'):
            os.remove('var/retention.dat')
        if os.path.exists('var/status.dat'):
            os.remove('var/status.dat')
        self.livestatus_broker = None
    def contains_line(self, text, pattern):
        regex = re.compile(pattern)
        for line in text.splitlines():
            if re.search(regex, line):
                return True
        return False

    def scheduler_loop(self, count, reflist, do_sleep=False, sleep_time=61):
        super(TestConfig, self).scheduler_loop(count, reflist, do_sleep, sleep_time)
        if self.nagios_installed() and hasattr(self, 'nagios_started'):
            self.nagios_loop(1, reflist)

    def update_broker(self, dodeepcopy=False):
        # The brok should be manage in the good order
        ids = self.sched.brokers['Default-Broker']['broks'].keys()
        ids.sort()
        for brok_id in ids:
            brok = self.sched.brokers['Default-Broker']['broks'][brok_id]
            #print "Managing a brok type", brok.type, "of id", brok_id
            #if brok.type == 'update_service_status':
            #    print "Problem?", brok.data['is_problem']
            if dodeepcopy:
                brok = copy.deepcopy(brok)
            brok.prepare()
            self.livestatus_broker.manage_brok(brok)
        self.sched.brokers['Default-Broker']['broks'] = {}

    def lines_equal(self, text1, text2):
        # gets two multiline strings and compares the contents
        # lifestatus output may not be in alphabetical order, so this
        # function is used to compare unordered output with unordered
        # expected output
        # sometimes mklivestatus returns 0 or 1 on an empty result
        text1 = text1.replace("200           1", "200           0")
        text2 = text2.replace("200           1", "200           0")
        text1 = text1.rstrip()
        text2 = text2.rstrip()
        #print "text1 //%s//" % text1
        #print "text2 //%s//" % text2
        sorted1 = "\n".join(sorted(text1.split("\n")))
        sorted2 = "\n".join(sorted(text2.split("\n")))
        len1 = len(text1.split("\n"))
        len2 = len(text2.split("\n"))
        #print "%s == %s text cmp %s" % (len1, len2, sorted1 == sorted2)
        #print "text1 //%s//" % sorted(text1.split("\n"))
        #print "text2 //%s//" % sorted(text2.split("\n"))
        if sorted1 == sorted2 and len1 == len2:
            return True
        else:
            # Maybe list members are different
            # allhosts;test_host_0;test_ok_0;servicegroup_02,servicegroup_01,ok
            # allhosts;test_host_0;test_ok_0;servicegroup_02,ok,servicegroup_01
            # break it up to
            # [['allhosts'], ['test_host_0'], ['test_ok_0'],
            #     ['ok', 'servicegroup_01', 'servicegroup_02']]
            [line for line in sorted(text1.split("\n"))]
            data1 = [[sorted(c.split(',')) for c in columns] for columns in [line.split(';') for line in sorted(text1.split("\n")) if line]]
            data2 = [[sorted(c.split(',')) for c in columns] for columns in [line.split(';') for line in sorted(text2.split("\n")) if line]]
            #print "text1 //%s//" % data1
            #print "text2 //%s//" % data2
            # cmp is clever enough to handle nested arrays
            return cmp(data1, data2) == 0

    def show_broks(self, title):
        print
        print "--- ", title
        for brok in sorted(self.sched.broks.values(), lambda x, y: x.id - y.id):
            if re.compile('^service_').match(brok.type):
                pass
                #print "BROK:", brok.type
                #print "BROK   ", brok.data['in_checking']
        self.update_broker()
        request = 'GET services\nColumns: service_description is_executing\n'
        response, keepalive = self.livestatus_broker.livestatus.handle_request(request)
        print response

    def nagios_installed(self, path='/usr/local/nagios/bin/nagios', livestatus='/usr/local/nagios/lib/mk-livestatus/livestatus.o'):
        return False
        raise
        if os.path.exists(path) and os.access(path, os.X_OK) and os.path.exists(livestatus):
            self.nagios_path = path
            self.livestatus_path = livestatus
            return True
        else:
            return False

    # shinkenize_nagios_config('nagios_1r_1h_1s')
    # We assume that there is a nagios_1r_1h_1s.cfg and a nagios_1r_1h_1s directory for the objects
    def unshinkenize_config(self, configname):
        new_configname = configname + '_' + str(os.getpid())
        config = open('etc/shinken_' + configname + '.cfg')
        text = config.readlines()
        config.close()

        newconfig = open('etc/shinken_' + new_configname + '.cfg', 'w')
        for line in text:
            if re.search('^resource_file=', line):
                newconfig.write("resource_file=etc/resource.cfg\n")
            elif re.search('shinken\-specific\.cfg', line):
                pass
            elif re.search('enable_problem_impacts_states_change', line):
                pass
            elif re.search('cfg_dir=', line):
                newconfig.write(re.sub(configname, new_configname, line))
            elif re.search('cfg_file=', line):
                newconfig.write(re.sub(configname, new_configname, line))
            elif re.search('execute_host_checks=', line):
                newconfig.write("execute_host_checks=0\n")
            elif re.search('execute_service_checks=', line):
                newconfig.write("execute_service_checks=0\n")
            elif re.search('^debug_level=', line):
                newconfig.write("debug_level=0\n")
            elif re.search('^debug_verbosity=', line):
                newconfig.write("debug_verbosity=0\n")
            elif re.search('^status_update_interval=', line):
                newconfig.write("status_update_interval=30\n")
            elif re.search('^command_file=', line):
                newconfig.write("command_file=var/shinken.cmd\n")
            elif re.search('^command_check_interval=', line):
                newconfig.write("command_check_interval=1s\n")
            else:
                newconfig.write(line)
        newconfig.write('broker_module=/usr/local/nagios/lib/mk-livestatus/livestatus.o var/live' + "\n")
        newconfig.close()
        for dirfile in os.walk('etc/' + configname):
            dirpath, dirlist, filelist = dirfile
            newdirpath = re.sub(configname, new_configname, dirpath)
            os.mkdir(newdirpath)
            for file in [f for f in filelist if re.search('\.cfg$', f)]:
                config = open(dirpath + '/' + file)
                text = config.readlines()
                config.close()
                newconfig = open(newdirpath + '/' + file, 'w')
                for line in text:
                    if re.search('^\s*criticity', line):
                        pass
                    elif re.search('^\s*business_impact', line):
                        pass
                    elif re.search('enable_problem_impacts_states_change', line):
                        pass
                    else:
                        newconfig.write(line)
                newconfig.close()
        return new_configname

    def start_nagios(self, config):
        if os.path.exists('var/spool/checkresults'):
            # Cleanup leftover checkresults
            shutil.rmtree('var/spool/checkresults')
        for dir in ['tmp', 'var/tmp', 'var/spool', 'var/spool/checkresults', 'var/archives']:
            if not os.path.exists(dir):
                os.mkdir(dir)
        self.nagios_config = self.unshinkenize_config(config)
        if os.path.exists('var/shinken.log'):
            os.remove('var/shinken.log')
        if os.path.exists('var/retention.dat'):
            os.remove('var/retention.dat')
        if os.path.exists('var/status.dat'):
            os.remove('var/status.dat')
        self.nagios_proc = subprocess.Popen([self.nagios_path, 'etc/shinken_' + self.nagios_config + '.cfg'], close_fds=True)
        self.nagios_started = time.time()
        time.sleep(2)

    def stop_nagios(self):
        if self.nagios_installed():
            print "i stop nagios!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
            time.sleep(5)
            if hasattr(self, 'nagios_proc'):
                attempt = 1
                while self.nagios_proc.poll() is None and attempt < 4:
                    self.nagios_proc.terminate()
                    attempt += 1
                    time.sleep(1)
                if self.nagios_proc.poll() is None:
                    self.nagios_proc.kill()
                if os.path.exists('etc/' + self.nagios_config):
                    shutil.rmtree('etc/' + self.nagios_config)
                if os.path.exists('etc/shinken_' + self.nagios_config + '.cfg'):
                    os.remove('etc/shinken_' + self.nagios_config + '.cfg')

    def ask_nagios(self, request):
        if time.time() - self.nagios_started < 2:
            time.sleep(1)
        if not request.endswith("\n"):
            request = request + "\n"
        unixcat = subprocess.Popen([os.path.dirname(self.nagios_path) + '/' + 'unixcat', 'var/live'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        tic = time.clock()
        out, err = unixcat.communicate(request)
        tac = time.clock()
        print "mklivestatus duration %f" % (tac - tic)
        attempt = 1
        while unixcat.poll() is None and attempt < 4:
            unixcat.terminate()
            attempt += 1
            time.sleep(1)
        if unixcat.poll() is None:
            unixcat.kill()
        print "unixcat says", out
        return out

    def nagios_loop(self, count, reflist, do_sleep=False, sleep_time=61):
        now = time.time()
        buffer = open('var/pipebuffer', 'w')
        for ref in reflist:
            (obj, exit_status, output) = ref
            if obj.my_type == 'service':
                cmd = "[%lu] PROCESS_SERVICE_CHECK_RESULT;%s;%s;%d;%s\n" % (now, obj.host_name, obj.service_description, exit_status, output)
                print cmd
                buffer.write(cmd)
            else:
                cmd = "[%lu] PROCESS_HOST_CHECK_RESULT;%s;%d;%s\n" % (now, obj.host_name, exit_status, output)
                buffer.write(cmd)
        buffer.close()
        print "open pipe", self.conf.command_file
        fifo = open('var/shinken.cmd', 'w')
        cmd = "[%lu] PROCESS_FILE;%s;0\n" % (now, 'var/pipebuffer')
        fifo.write(cmd)
        fifo.flush()
        fifo.close()
        time.sleep(5)

    def nagios_extcmd(self, cmd):
        fifo = open('var/shinken.cmd', 'w')
        fifo.write(cmd)
        fifo.flush()
        fifo.close()
        time.sleep(5)

########NEW FILE########
__FILENAME__ = shinken_test
#!/usr/bin/env python

#
# This file is used to test host- and service-downtimes.
#

import sys
import time
import datetime
import os
import string
import re
import random
import unittest
import copy

# import the shinken library from the parent directory
import __import_shinken ; del __import_shinken

import shinken
from shinken.objects.config import Config
from shinken.objects.command import Command
from shinken.objects.module import Module

from shinken.dispatcher import Dispatcher
from shinken.log import logger
from shinken.modulesctx import modulesctx
from shinken.scheduler import Scheduler
from shinken.macroresolver import MacroResolver
from shinken.external_command import ExternalCommandManager, ExternalCommand
from shinken.check import Check
from shinken.message import Message
from shinken.arbiterlink import ArbiterLink
from shinken.schedulerlink import SchedulerLink
from shinken.pollerlink import PollerLink
from shinken.reactionnerlink import ReactionnerLink
from shinken.brokerlink import BrokerLink
from shinken.satellitelink import SatelliteLink
from shinken.notification import Notification
from shinken.modulesmanager import ModulesManager
from shinken.basemodule import BaseModule

from shinken.brok import Brok

from shinken.daemons.schedulerdaemon import Shinken
from shinken.daemons.brokerdaemon import Broker
from shinken.daemons.arbiterdaemon import Arbiter

# Modules are by default on the ../modules
myself = os.path.abspath(__file__)

global modules_dir
modules_dir = "modules"

def define_modules_dir(val):
    global modules_dir
    modules_dir = val

class __DUMMY:
    def add(self, obj):
        pass

logger.load_obj(__DUMMY())
#logger.set_level(logger.ERROR)


# We overwrite the functions time() and sleep()
# This way we can modify sleep() so that it immediately returns although
# for a following time() it looks like thee was actually a delay.
# This massively speeds up the tests.
class TimeHacker(object):

    def __init__(self):
        self.my_offset = 0
        self.my_starttime = time.time()
        self.my_oldtime = time.time
        self.original_time_time = time.time
        self.original_time_sleep = time.sleep
        self.in_real_time = True

    def my_time_time(self):
        return self.my_oldtime() + self.my_offset

    def my_time_sleep(self, delay):
        self.my_offset += delay

    def time_warp(self, duration):
        self.my_offset += duration

    def set_my_time(self):
        if self.in_real_time:
            time.time = self.my_time_time
            time.sleep = self.my_time_sleep
            self.in_real_time = False

# If external processes or time stamps for files are involved, we must
# revert the fake timing routines, because these externals cannot be fooled.
# They get their times from the operating system.
    def set_real_time(self):
        if not self.in_real_time:
            time.time = self.original_time_time
            time.sleep = self.original_time_sleep
            self.in_real_time = True


#Time hacking for every test!
time_hacker = TimeHacker()
time_hacker.set_my_time()


class Pluginconf(object):
    pass


class _Unittest2CompatMixIn:
    """
    Mixin for simulating methods new in unittest2 resp. Python 2.7.

    Every test-case should inherit this *after* unittest.TestCase to
    make the compatiblity-methods available if they are not defined in
    unittest.TestCase already. Example::

       class MyTestCase(unittest.TestCase, Unittest2CompatMixIn):
           ...
    In our case, it's better to always inherit from ShinkenTest

    """
    def assertNotIn(self, member, container, msg=None):
       self.assertTrue(member not in container)

    def assertIn(self, member, container, msg=None):
        self.assertTrue(member in container)

    def assertIsInstance(self, obj, cls, msg=None):
        self.assertTrue(isinstance(obj, cls))

    def assertRegexpMatches(self, line, pattern):
        r = re.search(pattern, line)
        self.assertTrue(r is not None)

    def assertIs(self, obj, cmp, msg=None):
        self.assertTrue(obj is cmp)


class ShinkenTest(unittest.TestCase, _Unittest2CompatMixIn):
    def setUp(self):
        self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    def setup_with_file(self, path):
        self.print_header()
        # i am arbiter-like
        self.broks = {}
        self.me = None
        self.log = logger
        self.log.load_obj(self)
        self.config_files = [path]
        self.conf = Config()
        buf = self.conf.read_config(self.config_files)
        raw_objects = self.conf.read_config_buf(buf)
        self.conf.create_objects_for_type(raw_objects, 'arbiter')
        self.conf.create_objects_for_type(raw_objects, 'module')
        self.conf.early_arbiter_linking()
        self.conf.create_objects(raw_objects)
        self.conf.old_properties_names_to_new()
        self.conf.instance_id = 0
        self.conf.instance_name = 'test'
        # Hack push_flavor, that is set by the dispatcher
        self.conf.push_flavor = 0
        self.conf.load_triggers()
        self.conf.linkify_templates()
        self.conf.apply_inheritance()
        self.conf.explode()
        #print "Aconf.services has %d elements" % len(self.conf.services)
        self.conf.create_reversed_list()
        self.conf.remove_twins()
        self.conf.apply_implicit_inheritance()
        self.conf.fill_default()
        self.conf.remove_templates()
        self.conf.compute_hash()
        #print "conf.services has %d elements" % len(self.conf.services)
        self.conf.create_reversed_list()
        self.conf.override_properties()
        self.conf.pythonize()
        count = self.conf.remove_exclusions()
        if count > 0:
            self.conf.create_reversed_list()
        self.conf.linkify()
        self.conf.apply_dependencies()
        self.conf.explode_global_conf()
        self.conf.propagate_timezone_option()
        self.conf.create_business_rules()
        self.conf.create_business_rules_dependencies()
        self.conf.is_correct()
        if not self.conf.conf_is_correct:
            print "The conf is not correct, I stop here"
            return
        self.conf.clean()

        self.confs = self.conf.cut_into_parts()
        self.conf.prepare_for_sending()
        self.conf.show_errors()
        self.dispatcher = Dispatcher(self.conf, self.me)

        scheddaemon = Shinken(None, False, False, False, None, None)
        self.sched = Scheduler(scheddaemon)

        scheddaemon.sched = self.sched
        scheddaemon.modules_dir = modules_dir
        scheddaemon.load_modules_manager()
        # Remember to clean the logs we just created before launching tests
        self.clear_logs()
        m = MacroResolver()
        m.init(self.conf)
        self.sched.load_conf(self.conf, in_test=True)
        e = ExternalCommandManager(self.conf, 'applyer')
        self.sched.external_command = e
        e.load_scheduler(self.sched)
        e2 = ExternalCommandManager(self.conf, 'dispatcher')
        e2.load_arbiter(self)
        self.external_command_dispatcher = e2

        self.sched.schedule()

    def add(self, b):
        if isinstance(b, Brok):
            self.broks[b.id] = b
            return
        if isinstance(b, ExternalCommand):
            self.sched.run_external_command(b.cmd_line)

    def fake_check(self, ref, exit_status, output="OK"):
        #print "fake", ref
        now = time.time()
        ref.schedule(force=True)
        # now checks are schedule and we get them in
        # the action queue
        #check = ref.actions.pop()
        check = ref.checks_in_progress[0]
        self.sched.add(check)  # check is now in sched.checks[]

        # Allows to force check scheduling without setting its status nor
        # output. Useful for manual business rules rescheduling, for instance.
        if exit_status is None:
            return

        # fake execution
        check.check_time = now

        # and lie about when we will launch it because
        # if not, the schedule call for ref
        # will not really reschedule it because there
        # is a valid value in the future
        ref.next_chk = now - 0.5

        check.get_outputs(output, 9000)
        check.exit_status = exit_status
        check.execution_time = 0.001
        check.status = 'waitconsume'
        self.sched.waiting_results.append(check)


    def scheduler_loop(self, count, reflist, do_sleep=False, sleep_time=61, verbose=True):
        for ref in reflist:
            (obj, exit_status, output) = ref
            obj.checks_in_progress = []
        for loop in range(1, count + 1):
            if verbose is True:
                print "processing check", loop
            for ref in reflist:
                (obj, exit_status, output) = ref
                obj.update_in_checking()
                self.fake_check(obj, exit_status, output)
            self.sched.manage_internal_checks()

            self.sched.consume_results()
            self.sched.get_new_actions()
            self.sched.get_new_broks()
            self.sched.scatter_master_notifications()
            self.worker_loop(verbose)
            for ref in reflist:
                (obj, exit_status, output) = ref
                obj.checks_in_progress = []
            self.sched.update_downtimes_and_comments()
            #time.sleep(ref.retry_interval * 60 + 1)
            if do_sleep:
                time.sleep(sleep_time)


    def worker_loop(self, verbose=True):
        self.sched.delete_zombie_checks()
        self.sched.delete_zombie_actions()
        checks = self.sched.get_to_run_checks(True, False, worker_name='tester')
        actions = self.sched.get_to_run_checks(False, True, worker_name='tester')
        #print "------------ worker loop checks ----------------"
        #print checks
        #print "------------ worker loop actions ----------------"
        if verbose is True:
            self.show_actions()
        #print "------------ worker loop new ----------------"
        for a in actions:
            a.status = 'inpoller'
            a.check_time = time.time()
            a.exit_status = 0
            self.sched.put_results(a)
        if verbose is True:
            self.show_actions()
        #print "------------ worker loop end ----------------"


    def show_logs(self):
        print "--- logs <<<----------------------------------"
        for brok in sorted(self.sched.broks.values(), lambda x, y: x.id - y.id):
            if brok.type == 'log':
                brok.prepare()
                print "LOG:", brok.data['log']
        print "--- logs >>>----------------------------------"


    def show_actions(self):
        print "--- actions <<<----------------------------------"
        for a in sorted(self.sched.actions.values(), lambda x, y: x.id - y.id):
            if a.is_a == 'notification':
                if a.ref.my_type == "host":
                    ref = "host: %s" % a.ref.get_name()
                else:
                    ref = "host: %s svc: %s" % (a.ref.host.get_name(), a.ref.get_name())
                print "NOTIFICATION %d %s %s %s %s" % (a.id, ref, a.type, time.asctime(time.localtime(a.t_to_go)), a.status)
            elif a.is_a == 'eventhandler':
                print "EVENTHANDLER:", a
        print "--- actions >>>----------------------------------"


    def show_and_clear_logs(self):
        self.show_logs()
        self.clear_logs()


    def show_and_clear_actions(self):
        self.show_actions()
        self.clear_actions()


    def count_logs(self):
        return len([b for b in self.sched.broks.values() if b.type == 'log'])


    def count_actions(self):
        return len(self.sched.actions.values())


    def clear_logs(self):
        id_to_del = []
        for b in self.sched.broks.values():
            if b.type == 'log':
                id_to_del.append(b.id)
        for id in id_to_del:
            del self.sched.broks[id]


    def clear_actions(self):
        self.sched.actions = {}


    def log_match(self, index, pattern):
        # log messages are counted 1...n, so index=1 for the first message
        if index > self.count_logs():
            return False
        else:
            regex = re.compile(pattern)
            lognum = 1
            for brok in sorted(self.sched.broks.values(), lambda x, y: x.id - y.id):
                if brok.type == 'log':
                    brok.prepare()
                    if index == lognum:
                        if re.search(regex, brok.data['log']):
                            return True
                    lognum += 1
        return False

    def any_log_match(self, pattern):
        regex = re.compile(pattern)
        for brok in sorted(self.sched.broks.values(), lambda x, y: x.id - y.id):
            if brok.type == 'log':
                brok.prepare()
                if re.search(regex, brok.data['log']):
                    return True
        return False

    def get_log_match(self, pattern):
        regex = re.compile(pattern)
        res = []
        for brok in sorted(self.sched.broks.values(), lambda x, y: x.id - y.id):
            if brok.type == 'log':
                if re.search(regex, brok.data['log']):
                    res.append(brok.data['log'])
        return res

    def print_header(self):
        print "\n" + "#" * 80 + "\n" + "#" + " " * 78 + "#"
        print "#" + string.center(self.id(), 78) + "#"
        print "#" + " " * 78 + "#\n" + "#" * 80 + "\n"

    def xtest_conf_is_correct(self):
        self.print_header()
        self.assert_(self.conf.conf_is_correct)





if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_acknowledge
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test acknowledge of problems
#

from shinken_test import *


class TestAcks(ShinkenTest):

    def test_ack_soft_service(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        # clean up
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.clear_logs()
        self.clear_actions()

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # someone acknowledges the problem before a notification goes out
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=False)
        #self.sched.get_new_actions()
        #self.worker_loop()
        self.assert_(svc.problem_has_been_acknowledged)
        self.assert_(self.log_match(3, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.show_and_clear_logs()
        self.show_actions()
        self.sched.update_downtimes_and_comments()
        self.assert_(len(svc.comments) == 1)

        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created but blocked
        # log for alert hard and log for eventhandler
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        self.assert_(self.count_logs() == 2)
        self.assert_(self.count_actions() == 2)
        self.assert_(svc.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)  # alert, eventhndlr
        self.assert_(self.count_actions() == 1)  # evt zombie
        self.assert_(not svc.problem_has_been_acknowledged)
        self.assert_(svc.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()

    def test_ack_hard_service(self):
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 2)

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.scheduler_loop(1, [], do_sleep=False)
        #self.worker_loop()
        self.assert_(svc.problem_has_been_acknowledged)
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.assert_(self.count_logs() == 1)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # remove acknowledgement
        # now notifications are sent again
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] REMOVE_SVC_ACKNOWLEDGEMENT;test_host_0;test_ok_0" % now
        self.sched.run_external_command(cmd)
        
        self.scheduler_loop(1, [], do_sleep=False)
        #elf.sched.get_new_actions()
        #self.worker_loop()
        self.show_logs()
        self.show_actions()
        # the contact notification was sent immediately (t_to_go)
        self.assert_(not svc.problem_has_been_acknowledged)
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'SERVICE NOTIFICATION'))
        self.assert_(self.log_match(2, 'SERVICE NOTIFICATION'))
        self.assert_(self.count_logs() == 2)
        self.assert_(self.count_actions() == 2)  # master sched, contact zombie
        self.assert_(svc.current_notification_number == 4)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        # recover notifications are only sent to contacts which
        # received a critical/warning notification
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 3)  # alert, eventhndlr, notif
        self.show_actions()
        print self.count_actions()
        self.assert_(self.count_actions() == 2)  # evt, recovery notif zombie
        self.assert_(not svc.problem_has_been_acknowledged)
        self.assert_(svc.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()


    def test_ack_nonsticky_changing_service(self):
        # acknowledge is not sticky
        # service goes from critical to warning
        # this means, the acknowledge deletes itself
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 2)

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;1;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.scheduler_loop(1, [], do_sleep=False)
        #self.worker_loop()
        self.assert_(svc.problem_has_been_acknowledged)
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.assert_(self.count_logs() == 1)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # now become warning
        # ack is deleted automatically and notifications are sent again
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[svc, 1, 'NOT REALLY BAD']], do_sleep=True)
        self.assert_(not svc.problem_has_been_acknowledged)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'SERVICE ALERT.*WARNING'))
        self.assert_(self.log_match(2, 'SERVICE NOTIFICATION'))
        self.assert_(self.log_match(3, 'SERVICE NOTIFICATION'))
        self.assert_(self.count_logs() == 3)
        self.assert_(self.count_actions() == 2)  # master sched, contact zombie
        self.assert_(svc.current_notification_number == 4)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 3)  # alert, eventhndlr, notification

        self.show_actions()
        self.assert_(self.count_actions() == 2)  # evt, one notif zombie left
        self.assert_(not svc.problem_has_been_acknowledged)
        self.assert_(svc.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        

    def test_ack_sticky_changing_service(self):
        # acknowledge is sticky
        # service goes from critical to warning
        # still acknowledged
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 2)

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;0;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=True)
        #self.sched.get_new_actions()
        #self.worker_loop()
        self.assert_(svc.problem_has_been_acknowledged)
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.assert_(self.count_logs() == 1)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # now become warning
        # ack remains set
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[svc, 1, 'NOT REALLY BAD']], do_sleep=True)
        self.assert_(svc.problem_has_been_acknowledged)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'SERVICE ALERT.*WARNING'))
        self.assert_(self.count_logs() == 1)  # alert
        self.assert_(svc.current_notification_number == 2)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(len(svc.comments) == 1)
        self.assert_(svc.comments[0].comment == 'blablub')

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 3)  # alert, eventhndlr, notification
        self.assert_(self.count_actions() == 2)  # evt, master notif
        self.assert_(not svc.problem_has_been_acknowledged)
        self.assert_(svc.current_notification_number == 0)
        self.assert_(len(svc.comments) == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()

    def test_ack_soft_host(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(host.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 3 x DOWN get hard -------------------------------------"
        self.scheduler_loop(3, [[host, 2, 'DOWN']])
        self.assert_(host.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(7, 'HOST NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        # clean up
        self.scheduler_loop(1, [[host, 0, 'UP']])
        self.clear_logs()
        self.clear_actions()

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[host, 2, 'DOWN']])
        self.assert_(host.current_notification_number == 0)

        #--------------------------------------------------------------
        # someone acknowledges the problem before a notification goes out
        #--------------------------------------------------------------
        self.assert_(not host.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_HOST_PROBLEM;test_host_0;1;1;0;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=False)
        #self.sched.get_new_actions()
        #self.worker_loop()
        self.assert_(host.problem_has_been_acknowledged)
        self.assert_(self.log_match(3, 'ACKNOWLEDGEMENT \(DOWN\)'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created but blocked
        # log for alert soft2, hard3 and log for eventhandler soft2, hard3
        # eventhandler hard3 (eventhandler soft2 is already zombied when
        # the workerloop is finished
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[host, 2, 'DOWN']])
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 4)
        self.assert_(self.count_actions() == 2)
        self.assert_(host.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        # recover notifications are only sent to contacts which
        # received a critical/warning notification
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)  # alert, eventhndlr, notification
        self.show_actions()
        
        print self.count_actions()
        self.assert_(self.count_actions() == 1)  # evt, no more notif
        self.assert_(not host.problem_has_been_acknowledged)
        self.assert_(host.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()

        
    def test_ack_hard_host(self):
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(host.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(3, [[host, 2, 'DOWN']])
        self.assert_(host.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(7, 'HOST NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[host, 2, 'DOWN']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(host.current_notification_number == 2)

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not host.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_HOST_PROBLEM;test_host_0;1;1;0;lausser;blablub" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.scheduler_loop(1, [], do_sleep=False)
        #self.worker_loop()
        self.assert_(host.problem_has_been_acknowledged)
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(DOWN\)'))
        self.scheduler_loop(2, [[host, 2, 'DOWN']], do_sleep=True)
        self.assert_(self.count_logs() == 1)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # remove acknowledgement
        # now notifications are sent again
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] REMOVE_HOST_ACKNOWLEDGEMENT;test_host_0" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.scheduler_loop(1, [], do_sleep=False)
        #self.worker_loop()
        # the contact notification was sent immediately (t_to_go)
        self.assert_(not host.problem_has_been_acknowledged)
        self.scheduler_loop(2, [[host, 2, 'DOWN']], do_sleep=True)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'HOST NOTIFICATION'))
        self.assert_(self.log_match(2, 'HOST NOTIFICATION'))
        self.assert_(self.count_logs() == 2)
        self.assert_(self.count_actions() == 2)  # master sched, contact zombie
        self.assert_(host.current_notification_number == 4)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # recover
        # the acknowledgement must have been removed automatically
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'GOOD']])
        print "- 1 x OK recover"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 3)  # alert, eventhndlr, notification
        print self.count_actions()
        self.show_actions()
        self.assert_(self.count_actions() == 2)  # evt,  recovery notif zombie
        self.assert_(not host.problem_has_been_acknowledged)
        self.assert_(host.current_notification_number == 0)
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        

    def test_unack_removes_comments(self):
        # critical
        # ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;1;test_contact_alias;ackweb6
        # ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;1;test_contact_alias;ackweb6
        # ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;0;test_contact_alias;acknull
        # now remove the ack
        # the first two comments remain. So persistent not only means "survice a reboot"
        # but also "stay after the ack has been deleted"
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 2)

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;1;lausser;blablub1" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=True)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;1;lausser;blablub2" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=True)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;2;1;0;lausser;blablub3" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=True)

        self.assert_(svc.problem_has_been_acknowledged)
        self.show_logs()
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.assert_(self.log_match(2, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.assert_(self.log_match(3, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(len(svc.comments) == 3)
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 2)

        #--------------------------------------------------------------
        # remove the ack. the 2 persistent comments must remain
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] REMOVE_SVC_ACKNOWLEDGEMENT;test_host_0;test_ok_0" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.scheduler_loop(1, [], do_sleep=False)
        #self.worker_loop()
        self.assert_(not svc.problem_has_been_acknowledged)
        self.assert_(len(svc.comments) == 2)
        self.assert_(svc.comments[0].comment == 'blablub1')
        self.assert_(svc.comments[1].comment == 'blablub2')


# service is critical, notification is out
# click on ack without setting the sticky checkbox in the webinterface
# EXTERNAL COMMAND: ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;test_contact_alias;weback
# now service is acknowledged and has a comment
# silence...
# service is warning
# notification is sent
# acknowledgement and comment have disappeared

# service is critical, notification is out
# send external command through the pipe 3 times
# ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;test_contact_alias;weback
# ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;test_contact_alias;weback
# ACKNOWLEDGE_SVC_PROBLEM;test_host_0;test_ok_0;1;1;0;test_contact_alias;weback
# now service is acknowledged and has 3 comments
# silence...
# service is warning
# notification is sent
# acknowledgement and comments have disappeared





if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_acknowledge_with_expire
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test acknowledge of problems
#

from shinken_test import *

# Restore sleep functions
time_hacker.set_real_time()



class TestAcksWithExpire(ShinkenTest):

    def test_ack_hard_service_with_expire(self):
        self.print_header()
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']])
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(svc.current_notification_number == 0)

        #--------------------------------------------------------------
        # first check the normal behavior
        # service reaches hard;2
        # at the end there must be 3 actions: eventhandler hard,
        #   master notification and contact notification
        #--------------------------------------------------------------
        print "- 2 x BAD get hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']])
        self.assert_(svc.current_notification_number == 1)
        self.assert_(self.count_actions() == 3)
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION'))
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # stay hard and wait for the second notification (notification_interval)
        #--------------------------------------------------------------
        print "- 2 x BAD stay hard -------------------------------------"
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=False)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # admin wakes up and acknowledges the problem
        # the ACK is the only log message
        # a master notification is still around, but can't be sent
        #--------------------------------------------------------------
        self.assert_(not svc.problem_has_been_acknowledged)
        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM_EXPIRE;test_host_0;test_ok_0;1;1;0;%d;lausser;blablub" % (now, int(now) + 5)
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [], do_sleep=False)
        self.assert_(svc.problem_has_been_acknowledged)
        self.assert_(self.log_match(1, 'ACKNOWLEDGEMENT \(CRITICAL\)'))
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=False)
        self.assert_(self.count_logs() == 1)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_actions()

        #--------------------------------------------------------------
        # Wait 4 remove acknowledgement
        # now notifications are sent again
        #--------------------------------------------------------------
        time.sleep(5)
        # Wait a bit
        self.sched.check_for_expire_acknowledge()
        self.assert_(not svc.problem_has_been_acknowledged)

        #now = time.time()
        #cmd = "[%lu] REMOVE_SVC_ACKNOWLEDGEMENT;test_host_0;test_ok_0" % now
        #self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.show_logs()
        self.show_actions()
        # the contact notification was sent immediately (t_to_go)
        self.assert_(not svc.problem_has_been_acknowledged)
        self.show_logs()
        self.show_actions()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_action
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import os
import sys

from shinken_test import *
from shinken.action import Action

time_hacker.set_real_time()

class TestAction(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def wait_finished(self, a, size=8012):
        start = time.time()
        while True:
            # Do the job
            if a.status == 'launched':
                #print a.process.poll()
                a.check_finished(size)
                time.sleep(0.01)
            #print a.status
            if a.status != 'launched':
                #print "Finish", a.status
                return
            # 20s timeout
            if time.time() - start > 20:
                print "COMMAND TIMEOUT AT 20s"
                return


    def test_action(self):
        a = Action()
        a.timeout = 10
        a.env = {}

        if os.name == 'nt':
            a.command = r'libexec\\dummy_command.cmd'
        else:
            a.command = "libexec/dummy_command.sh"
        self.assert_(a.got_shell_characters() == False)
        a.execute()
        self.assert_(a.status == 'launched')
        # Give also the max output we want for the command
        self.wait_finished(a)
        self.assert_(a.exit_status == 0)
        self.assert_(a.status == 'done')
        print a.output
        self.assert_(a.output == "Hi, I'm for testing only. Please do not use me directly, really")
        self.assert_(a.perf_data == "Hip=99% Bob=34mm")

    def test_echo_environment_variables(self):
        if os.name == 'nt':
            return

        a = Action()
        a.timeout = 10
        a.env = {}  # :fixme: this sould be pre-set in Action.__init__()

        a.command = "echo $TITI"

        self.assertNotIn('TITI', a.get_local_environnement())
        a.env = {'TITI': 'est en vacance'}
        self.assertIn('TITI', a.get_local_environnement())
        self.assertEqual(a.get_local_environnement()['TITI'],
                         'est en vacance' )
        a.execute()
        self.wait_finished(a)
        self.assertEqual(a.output, 'est en vacance')

    def test_grep_for_environment_variables(self):
        if os.name == 'nt':
            return

        a = Action()
        a.timeout = 10
        a.env = {}  # :fixme: this sould be pre-set in Action.__init__()

        a.command = "/usr/bin/env | grep TITI"

        self.assertNotIn('TITI', a.get_local_environnement())
        a.env = {'TITI': 'est en vacance'}
        self.assertIn('TITI', a.get_local_environnement())
        self.assertEqual(a.get_local_environnement()['TITI'],
                         'est en vacance' )
        a.execute()
        self.wait_finished(a)
        self.assertEqual(a.output, 'TITI=est en vacance')


    def test_environment_variables(self):

        class ActionWithoutPerfData(Action):
            def get_outputs(self, out, max_len):
                # do not cut the outputs into perf_data to avoid
                # problems with enviroments containing a dash like in
                # `LESSOPEN=|/usr/bin/lesspipe.sh %s`
                self.output = out

        if os.name == 'nt':
            return

        a = ActionWithoutPerfData()
        a.timeout = 10
        a.command = "/usr/bin/env"

        a.env = {}  # :fixme: this sould be pre-set in Action.__init__()
        self.assertNotIn('TITI', a.get_local_environnement())

        a.env = {'TITI': 'est en vacance'}

        self.assert_(a.got_shell_characters() == False)

        self.assertIn('TITI', a.get_local_environnement())
        self.assertEqual(a.get_local_environnement()['TITI'],
                         'est en vacance' )
        a.execute()

        self.assert_(a.status == 'launched')
        # Give also the max output we want for the command
        self.wait_finished(a, size=20*1024)
        titi_found = False
        for l in a.output.splitlines():
            if l == 'TITI=est en vacance':
                titi_found = True
        self.assertTrue(titi_found)



    # Some commands are shell without bangs! (like in Centreon...)
    # We can show it in the launch, and it should be managed
    def test_noshell_bang_command(self):
        a = Action()
        a.timeout = 10
        a.command = "libexec/dummy_command_nobang.sh"
        a.env = {}
        if os.name == 'nt':
            return
        self.assert_(a.got_shell_characters() == False)
        a.execute()

        self.assert_(a.status == 'launched')
        self.wait_finished(a)
        print "FUck", a.status, a.output
        self.assert_(a.exit_status == 0)
        self.assert_(a.status == 'done')

    def test_got_shell_characters(self):
        a = Action()
        a.timeout = 10
        a.command = "libexec/dummy_command_nobang.sh && echo finished ok"
        a.env = {}
        if os.name == 'nt':
            return
        self.assert_(a.got_shell_characters() == True)
        a.execute()

        self.assert_(a.status == 'launched')
        self.wait_finished(a)
        print "FUck", a.status, a.output
        self.assert_(a.exit_status == 0)
        self.assert_(a.status == 'done')

    def test_got_pipe_shell_characters(self):
        a = Action()
        a.timeout = 10
        a.command = "libexec/dummy_command_nobang.sh | grep 'Please do not use me directly'"
        a.env = {}
        if os.name == 'nt':
            return
        self.assert_(a.got_shell_characters() == True)
        a.execute()

        self.assert_(a.status == 'launched')
        self.wait_finished(a)
        print "FUck", a.status, a.output
        self.assert_(a.exit_status == 0)
        self.assert_(a.status == 'done')

    def test_got_unclosed_quote(self):
        # https://github.com/naparuba/shinken/issues/155
        a = Action()
        a.timeout = 10
        a.command = "libexec/dummy_command_nobang.sh -a 'wwwwzzzzeeee"
        a.env = {}
        if os.name == 'nt':
            return
        a.execute()

        self.wait_finished(a)
        self.assert_(a.status == 'done')
        print "FUck", a.status, a.output
        if sys.version_info < (2, 7):
            # cygwin: /bin/sh: -c: line 0: unexpected EOF while looking for matching'
            # ubuntu: /bin/sh: Syntax error: Unterminated quoted string
            self.assert_(a.output.startswith("/bin/sh"))
            self.assert_(a.exit_status == 3)
        else:
            self.assert_(a.output == 'Not a valid shell command: No closing quotation')
            self.assert_(a.exit_status == 3)

    # We got problems on LARGE output, more than 64K in fact.
    # We try to solve it with the fcntl and non blocking read
    # instead of "communicate" mode. So here we try to get a 100K
    # output. Should NOT be in a timeout
    def test_huge_output(self):
        a = Action()
        a.timeout = 5
        a.env = {}

        if os.name == 'nt':
            a.command = r"""python -c 'print "A"*1000000'"""
            # FROM NOW IT4S FAIL ON WINDOWS :(
            return
        else:
            a.command = r"""python -u -c 'print "A"*100000'"""
        print "EXECUTE"
        a.execute()
        print "EXECUTE FINISE"
        self.assert_(a.status == 'launched')
        # Give also the max output we want for the command
        self.wait_finished(a, 10000000000)
        print "Status?", a.exit_status
        self.assert_(a.exit_status == 0)
        print "Output", len(a.output)
        self.assert_(a.exit_status == 0)
        self.assert_(a.status == 'done')
        self.assert_(a.output == "A"*100000)
        self.assert_(a.perf_data == "")



    def test_execve_fail_with_utf8(self):
        if os.name == 'nt':
            return

        a = Action()
        a.timeout = 10
        a.env = {}  # :fixme: this sould be pre-set in Action.__init__()

        a.command = u"/bin/echo Wiadomo\u015b\u0107"

        a.execute()
        self.wait_finished(a)
        #print a.output
        self.assertEqual(a.output.decode('utf8'), u"Wiadomo\u015b\u0107")



if __name__ == '__main__':
    import sys

    #os.chdir(os.path.dirname(sys.argv[0]))
    unittest.main()

########NEW FILE########
__FILENAME__ = test_antivirg
#!/usr/bin/env python
# -*- coding: utf-8 -*

from shinken_test import *

class TestConfig(ShinkenTest):

    def setUp(self):
        # load the configuration from file
        self.setup_with_file('etc/shinken_antivirg.cfg')

    def test_hostname_antivirg(self):
        """Check that it is allowed to have a host with the "__ANTI-VIRG__" substring in its hostname"""

        # the global configuration must be valid
        self.assert_(
                       True == self.conf.conf_is_correct
                     ,("config is not correct")
                    )

        # try to get the host
        # if it is not possible to get the host, it is probably because
        # "__ANTI-VIRG__" has been replaced by ";"
        hst = self.conf.hosts.find_by_name('test__ANTI-VIRG___0')
        self.assert_(
                      hst is not None
                     ,("host 'test__ANTI-VIRG___0' not found")
                    )

        # Check that the host has a valid configuration
        self.assert_(
                      True == hst.is_correct()
                     ,("config of host '%s' is not true"
                       % (hst.get_name()))
                    )

    def test_parsing_comment(self):
        """Check that the semicolon is a comment delimiter"""

        # the global configuration must be valid
        self.assert_(
                       True == self.conf.conf_is_correct
                     ,("config is not correct")
                    )

        # try to get the host
        hst = self.conf.hosts.find_by_name('test_host_1')
        self.assert_(
                      hst is not None
                     ,("host 'test_host_1' not found")
                    )

        # Check that the host has a valid configuration
        self.assert_(
                      True == hst.is_correct()
                     ,("config of host '%s' is not true"
                       % (hst.get_name()))
                    )

    def test_escaped_semicolon(self):
        """Check that it is possible to have a host with a semicolon in its hostname
           
           The consequences of this aren't tested. We try just to send a command but 
           I think that others programs which send commands don't think to escape 
           the semicolon.

        """

        # the global configuration must be valid
        self.assert_(
                       True == self.conf.conf_is_correct
                     ,("config is not correct")
                    )

        # try to get the host
        hst = self.conf.hosts.find_by_name('test_host_2;with_semicolon')
        self.assert_(
                      hst is not None
                     ,("host 'test_host_2;with_semicolon' not found")
                    )

        # Check that the host has a valid configuration
        self.assert_(
                      True == hst.is_correct()
                     ,("config of host '%s' is not true"
                       % (hst.get_name()))
                    )

        # We can send a command by escaping the semicolon.


        command = '[%lu] PROCESS_HOST_CHECK_RESULT;test_host_2\;with_semicolon;2;down' % (time.time())
        self.sched.run_external_command(command)

        # can need 2 run for get the consum (I don't know why)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])

if '__main__' == __name__:
    unittest.main()


########NEW FILE########
__FILENAME__ = test_bad_contact_call
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_contact_call.cfg')

    def test_bad_contact_call(self):
        # The service got a unknow contact. It should raise an error
        svc = self.conf.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        print "Contacts:", svc.contacts
        self.assert_(svc.is_correct() == False)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_escalation_on_groups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBadEscaOnGroups(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_escalation_on_groups.cfg')

    def test_escalation_inheritance(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        print svc.escalations

        self.assert_(len(svc.escalations) > 0)
        es = svc.escalations.pop()
        self.assert_(es.is_correct())


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_notification_character
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_notification_character.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        for n in svc.notifications_in_progress.values():
            print "HEHE"
            print n.__dict__
            n.execute()
            print n.exit_status
            n.output = u'I love myself $'
            self.sched.put_results(n)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_notification_period
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBadNotificationPeriod(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_notification_period.cfg')

    # if a notif period is bad, should be catched!
    def test_bad_notification_period(self):
        self.assert_(self.conf.conf_is_correct == False)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_realm_conf
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBadRealmConf(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_realm_conf.cfg')

    def test_bad_conf(self):
        self.assert_(not self.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_sat_realm_conf
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBadSatRealmConf(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_sat_realm_conf.cfg')

    def test_badconf(self):
        self.assert_(not self.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_servicedependencies
#!/usr/bin/env python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Sebastien Coavoux, s.coavoux@free.fr
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBadServiceDependencies(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_bad_servicedependencies.cfg')

    def test_bad_conf(self):
        #
        # Config is not correct because of a wrong inexisting service in dependency
        # in the host configuration
        #
        self.assert_(not self.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = test_bad_start
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import os
import tempfile

from shinken_test import *

import shinken.log as shinken_log

from shinken.daemon import InvalidPidFile, InvalidWorkDir
from shinken.http_daemon import PortNotFree

from shinken.daemons.pollerdaemon import Poller
from shinken.daemons.brokerdaemon import Broker
from shinken.daemons.schedulerdaemon import Shinken
from shinken.daemons.reactionnerdaemon import Reactionner
from shinken.daemons.arbiterdaemon import Arbiter
try:
    import pwd, grp
    from pwd import getpwnam
    from grp import getgrnam


    def get_cur_user():
        return pwd.getpwuid(os.getuid()).pw_name

    def get_cur_group():
        return grp.getgrgid(os.getgid()).gr_name

except ImportError, exp:  # Like in nt system or Android
    # temporary workaround:
    def get_cur_user():
        return os.getlogin()

    def get_cur_group():
        return os.getlogin()

curdir = os.getcwd()

daemons_config = {
    Broker:       "etc/core/daemons/brokerd.ini",
    Poller:       "etc/core/daemons/pollerd.ini",
    Reactionner:  "etc/core/daemons/reactionnerd.ini",
    Shinken:      "etc/core/daemons/schedulerd.ini",
    Arbiter:    ["etc/core/shinken.cfg"]
}

import random

HIGH_PORT = random.randint(30000,65000)
run = 0   # We will open some ports but not close them (yes it's not good) and
# so we will open a range from a high port

class template_Daemon_Bad_Start():

    def get_login_and_group(self, p):
        try:
            p.user = get_cur_user()
            p.group = get_cur_group()
        except OSError:  # on some rare case, we can have a problem here
            # so bypass it and keep default value
            return

    def create_daemon(self):
        cls = self.daemon_cls
        return cls(daemons_config[cls], False, True, False, None, '')


    def get_daemon(self):
        global run
        os.chdir(curdir)
        shinken_log.local_log = None  # otherwise get some "trashs" logs..
        d = self.create_daemon()

        d.load_config_file()
        d.http_backend = 'wsgiref'
        d.port = HIGH_PORT + run  # random high port, I hope no one is using it :)
        run += 1
        self.get_login_and_group(d)
        return d

    def test_bad_piddir(self):
        print "Testing bad pidfile ..."
        d = self.get_daemon()
        d.workdir = tempfile.mkdtemp()
        d.pidfile = os.path.join('/DONOTEXISTS', "daemon.pid")
        #f = open(d.pidfile, "w")
        #f.close()
        #os.chmod(d.pidfile, 0)
        self.assertRaises(InvalidPidFile, d.do_daemon_init_and_start)
        #os.unlink(d.pidfile)
        os.rmdir(d.workdir)

    def test_bad_workdir(self):
        print("Testing bad workdir ... mypid=%d" % (os.getpid()))
        d = self.get_daemon()
        d.workdir = '/DONOTEXISTS'
        #os.chmod(d.workdir, 0)
        self.assertRaises(InvalidWorkDir, d.do_daemon_init_and_start)
        d.do_stop()
        #os.rmdir(d.workdir)

    def test_port_not_free(self):
        print("Testing port not free ... mypid=%d" % (os.getpid()))
        d1 = self.get_daemon()
        d1.workdir = tempfile.mkdtemp()
        d1.do_daemon_init_and_start()
        os.unlink(d1.pidfile)  ## so that second poller will not see first started poller
        d2 = self.get_daemon()
        d2.workdir = d1.workdir
        # TODO: find a way in Pyro4 to get the port
        if hasattr(d1.http_daemon, 'port'):
            d2.port = d1.http_daemon.port
            self.assertRaises(PortNotFree, d2.do_daemon_init_and_start)
            d2.do_stop()
        d1.do_stop()
        try:
            os.unlink(d1.pidfile)
        except:
            pass
        if hasattr(d1, 'local_log'):
            os.unlink(os.path.join(d1.workdir, d1.local_log))
        os.rmdir(d1.workdir)


class Test_Broker_Bad_Start(template_Daemon_Bad_Start, ShinkenTest):
    daemon_cls = Broker


class Test_Scheduler_Bad_Start(template_Daemon_Bad_Start, ShinkenTest):
    daemon_cls = Shinken


class Test_Poller_Bad_Start(template_Daemon_Bad_Start, ShinkenTest):
    daemon_cls = Poller


class Test_Reactionner_Bad_Start(template_Daemon_Bad_Start, ShinkenTest):
    daemon_cls = Reactionner


class Test_Arbiter_Bad_Start(template_Daemon_Bad_Start, ShinkenTest):
    daemon_cls = Arbiter

    def create_daemon(self):
        """ arbiter is always a bit special .. """
        cls = self.daemon_cls
        #Arbiter(config_files, is_daemon, do_replace, verify_only, debug, debug_file, profile)
        return cls(daemons_config[cls], False, True, False, False, None, '')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bad_timeperiods
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_bad_timeperiods.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the bad timeperiod"
        tp = self.conf.timeperiods.find_by_name("24x7")
        self.assert_(tp.is_correct() == True)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_business_correlator
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import re
from shinken_test import *


class TestBusinesscorrel(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator.cfg')

    # We will try a simple bd1 OR db2
    def test_simple_or_business_correlator(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Simple_Or")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '|')

        # We check for good parent/childs links
        # So svc_cor should be a son of svc_bd1 and svc_bd2
        # and bd1 and bd2 should be parents of svc_cor
        self.assert_(svc_cor in svc_bd1.child_dependencies)
        self.assert_(svc_cor in svc_bd2.child_dependencies)
        self.assert_(svc_bd1 in svc_cor.parent_dependencies)
        self.assert_(svc_bd2 in svc_cor.parent_dependencies)

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == svc_bd1)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must still be a 0 (or inside)
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we also set bd2 as CRITICAL/HARD... byebye 0 :)
        self.scheduler_loop(2, [[svc_bd2, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 2)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set one WARNING?
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # Must be WARNING (better no 0 value)
        state = bp_rule.get_state()
        self.assert_(state == 1)




    # We will try a simple bd1 AND db2
    def test_simple_and_business_correlator(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Simple_And")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '&')

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == svc_bd1)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        # becase we want HARD states
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must go CRITICAL
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # Now we also set bd2 as WARNING/HARD...
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set one WARNING too?
        self.scheduler_loop(2, [[svc_bd1, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'WARNING')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 1)

        # Must be WARNING (worse no 0 value for both)
        state = bp_rule.get_state()
        self.assert_(state == 1)

    # We will try a simple 1of: bd1 OR/AND db2
    def test_simple_1of_business_correlator(self):
        self.run_simple_1of_business_correlator()

    # We will try a simple -1of: bd1 OR/AND db2
    def test_simple_1of_neg_business_correlator(self):
        self.run_simple_1of_business_correlator(with_neg=True)

    # We will try a simple 50%of: bd1 OR/AND db2
    def test_simple_1of_pct_business_correlator(self):
        self.run_simple_1of_business_correlator(with_pct=True)

    # We will try a simple -50%of: bd1 OR/AND db2
    def test_simple_1of_pct_neg_business_correlator(self):
        self.run_simple_1of_business_correlator(with_pct=True, with_neg=True)


    def run_simple_1of_business_correlator(self, with_pct=False, with_neg=False):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        if with_pct is True:
            if with_neg is True:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_pct_neg")
            else:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_pct")
        else:
            if with_neg is True:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_neg")
            else:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        # Simple 1of: so in fact a triple ('1','2','2') (1of and MAX,MAX
        if with_pct is True:
            if with_neg is True:
                self.assert_(bp_rule.of_values == ('-50%', '2', '2'))
            else:
                self.assert_(bp_rule.of_values == ('50%', '2', '2'))
        else:
            if with_neg is True:
                self.assert_(bp_rule.of_values == ('-1', '2', '2'))
            else:
                self.assert_(bp_rule.of_values == ('1', '2', '2'))

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == svc_bd1)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        # becase we want HARD states
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule still be OK
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we also set bd2 as CRITICAL/HARD...
        self.scheduler_loop(2, [[svc_bd2, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 2)

        # And now the state of the rule must be 2 now
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set one WARNING now?
        self.scheduler_loop(2, [[svc_bd1, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'WARNING')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 1)

        # Must be WARNING (worse no 0 value for both, like for AND rule)
        state = bp_rule.get_state()
        self.assert_(state == 1)

    # We will try a simple 1of: test_router_0 OR/AND test_host_0
    def test_simple_1of_business_correlator_with_hosts(self):
        self.run_simple_1of_business_correlator_with_hosts()

    # We will try a simple -1of: test_router_0 OR/AND test_host_0
    def test_simple_1of_neg_business_correlator_with_hosts(self):
        self.run_simple_1of_business_correlator_with_hosts(with_neg=True)

    # We will try a simple 50%of: test_router_0 OR/AND test_host_0
    def test_simple_1of_pct_business_correlator_with_hosts(self):
        self.run_simple_1of_business_correlator_with_hosts(with_pct=True)

    # We will try a simple -50%of: test_router_0 OR/AND test_host_0
    def test_simple_1of_pct_neg_business_correlator_with_hosts(self):
        self.run_simple_1of_business_correlator_with_hosts(with_pct=True, with_neg=True)

    def run_simple_1of_business_correlator_with_hosts(self, with_pct=False, with_neg=False):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        if with_pct is True:
            if with_neg is True:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_with_host_pct_neg")
            else:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_with_host_pct")
        else:
            if with_neg is True:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_with_host_neg")
            else:
                svc_cor = self.sched.services.find_srv_by_name_and_hostname(
                        "test_host_0", "Simple_1Of_with_host")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        # Simple 1of: so in fact a triple ('1','2','2') (1of and MAX,MAX
        if with_pct is True:
            if with_neg is True:
                self.assert_(bp_rule.of_values == ('-50%', '2', '2'))
            else:
                self.assert_(bp_rule.of_values == ('50%', '2', '2'))
        else:
            if with_neg is True:
                self.assert_(bp_rule.of_values == ('-1', '2', '2'))
            else:
                self.assert_(bp_rule.of_values == ('1', '2', '2'))

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'host')
        self.assert_(sons[0].sons[0] == host)
        self.assert_(sons[1].operand == 'host')
        self.assert_(sons[1].sons[0] == router)

    # We will try a simple bd1 OR db2, but this time we will
    # schedule a real check and see if it's good
    def test_simple_or_business_correlator_with_schedule(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Simple_Or")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '|')

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == svc_bd1)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must still be a 0 (or inside)
        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we also set bd2 as CRITICAL/HARD... byebye 0 :)
        self.scheduler_loop(2, [[svc_bd2, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 2)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And now we must be CRITICAL/SOFT!
        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'CRITICAL')
        self.assert_(svc_cor.state_type == 'SOFT')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # OK, re recheck again, GO HARD!
        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'CRITICAL')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 2)

        # And If we set one WARNING?
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # Must be WARNING (better no 0 value)
        state = bp_rule.get_state()
        self.assert_(state == 1)

        # And in a HARD
        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'WARNING')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 1)

        print "All elements", bp_rule.list_all_elements()

        print "IMPACT:", svc_bd2.impacts
        for i in svc_bd2.impacts:
            print i.get_name()

        # Assert that Simple_Or Is an impact of the problem bd2
        self.assert_(svc_cor in svc_bd2.impacts)
        # and bd1 too
        self.assert_(svc_cor in svc_bd1.impacts)

    def test_dep_node_list_elements(self):
        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Simple_Or")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '|')

        print "All elements", bp_rule.list_all_elements()
        all_elt = bp_rule.list_all_elements()

        self.assert_(svc_bd2 in all_elt)
        self.assert_(svc_bd1 in all_elt)

        print "DBG: bd2 depend_on_me", svc_bd2.act_depend_of_me

    # We will try a full ERP rule and
    # schedule a real check and see if it's good
    def test_full_erp_rule_with_schedule(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_web1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "web1")
        self.assert_(svc_web1.got_business_rule == False)
        self.assert_(svc_web1.business_rule is None)
        svc_web2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "web2")
        self.assert_(svc_web2.got_business_rule == False)
        self.assert_(svc_web2.business_rule is None)
        svc_lvs1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "lvs1")
        self.assert_(svc_lvs1.got_business_rule == False)
        self.assert_(svc_lvs1.business_rule is None)
        svc_lvs2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "lvs2")
        self.assert_(svc_lvs2.got_business_rule == False)
        self.assert_(svc_lvs2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "ERP")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '&')

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 3 sons, each 3 rules
        self.assert_(len(sons) == 3)
        bd_node = sons[0]
        self.assert_(bd_node.operand == '|')
        self.assert_(bd_node.sons[0].sons[0] == svc_bd1)
        self.assert_(bd_node.sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must still be a 0 (or inside)
        state = bp_rule.get_state()
        self.assert_(state == 0)

        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Now we also set bd2 as CRITICAL/HARD... byebye 0 :)
        self.scheduler_loop(2, [[svc_bd2, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 2)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And now we must be CRITICAL/SOFT!
        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'CRITICAL')
        self.assert_(svc_cor.state_type == 'SOFT')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # OK, re recheck again, GO HARD!
        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'CRITICAL')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 2)

        # And If we set one WARNING?
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # Must be WARNING (better no 0 value)
        state = bp_rule.get_state()
        self.assert_(state == 1)

        # And in a HARD
        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'WARNING')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 1)

        print "All elements", bp_rule.list_all_elements()

        print "IMPACT:", svc_bd2.impacts
        for i in svc_bd2.impacts:
            print i.get_name()

        # Assert that Simple_Or Is an impact of the problem bd2
        self.assert_(svc_cor in svc_bd2.impacts)
        # and bd1 too
        self.assert_(svc_cor in svc_bd1.impacts)

        # And now all is green :)
        self.scheduler_loop(2, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | value1=1 value2=2']])

        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

        # And no more in impact
        self.assert_(svc_cor not in svc_bd2.impacts)
        self.assert_(svc_cor not in svc_bd1.impacts)

        # And what if we set 2 service from distant rule CRITICAL?
        # ERP should be still OK
        # And now all is green :)
        self.scheduler_loop(2, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2'], [svc_web1, 2, 'CRITICAL | value1=1 value2=2']])

        print "ERP: Launch internal check"
        svc_cor.launch_check(now-1)
        c = svc_cor.actions[0]
        self.assert_(c.internal == True)
        self.assert_(c.is_launchable(now))

        # ask the scheduler to launch this check
        # and ask 2 loops: one for launch the check
        # and another to integer the result
        self.scheduler_loop(2, [])

        # We should have no more the check
        self.assert_(len(svc_cor.actions) == 0)

        print "ERP: Look at svc_cor state", svc_cor.state
        # What is the svc_cor state now?
        self.assert_(svc_cor.state == 'OK')
        self.assert_(svc_cor.state_type == 'HARD')
        self.assert_(svc_cor.last_hard_state_id == 0)

    # We will try a simple 1of: bd1 OR/AND db2
    def test_complex_ABCof_business_correlator(self):
        self.run_complex_ABCof_business_correlator(with_pct=False)

    # We will try a simple 1of: bd1 OR/AND db2
    def test_complex_ABCof_pct_business_correlator(self):
        self.run_complex_ABCof_business_correlator(with_pct=True)

    def run_complex_ABCof_business_correlator(self, with_pct=False):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        A = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "A")
        self.assert_(A.got_business_rule == False)
        self.assert_(A.business_rule is None)
        B = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "B")
        self.assert_(B.got_business_rule == False)
        self.assert_(B.business_rule is None)
        C = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "C")
        self.assert_(C.got_business_rule == False)
        self.assert_(C.business_rule is None)
        D = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "D")
        self.assert_(D.got_business_rule == False)
        self.assert_(D.business_rule is None)
        E = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "E")
        self.assert_(E.got_business_rule == False)
        self.assert_(E.business_rule is None)

        if with_pct == False:
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Complex_ABCOf")
        else:
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Complex_ABCOf_pct")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        if with_pct == False:
            self.assert_(bp_rule.of_values == ('5', '1', '1'))
        else:
            self.assert_(bp_rule.of_values == ('100%', '20%', '20%'))

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 5)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == A)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == B)
        self.assert_(sons[2].operand == 'service')
        self.assert_(sons[2].sons[0] == C)
        self.assert_(sons[3].operand == 'service')
        self.assert_(sons[3].sons[0] == D)
        self.assert_(sons[4].operand == 'service')
        self.assert_(sons[4].sons[0] == E)

        # Now state working on the states
        self.scheduler_loop(1, [[A, 0, 'OK'], [B, 0, 'OK'], [C, 0, 'OK'], [D, 0, 'OK'], [E, 0, 'OK']])
        self.assert_(A.state == 'OK')
        self.assert_(A.state_type == 'HARD')
        self.assert_(B.state == 'OK')
        self.assert_(B.state_type == 'HARD')
        self.assert_(C.state == 'OK')
        self.assert_(C.state_type == 'HARD')
        self.assert_(D.state == 'OK')
        self.assert_(D.state_type == 'HARD')
        self.assert_(E.state == 'OK')
        self.assert_(E.state_type == 'HARD')

        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the A as soft/CRITICAL
        self.scheduler_loop(1, [[A, 2, 'CRITICAL']])
        self.assert_(A.state == 'CRITICAL')
        self.assert_(A.state_type == 'SOFT')
        self.assert_(A.last_hard_state_id == 0)

        # The business rule must still be 0
        # becase we want HARD states
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get A CRITICAL/HARD
        self.scheduler_loop(1, [[A, 2, 'CRITICAL']])
        self.assert_(A.state == 'CRITICAL')
        self.assert_(A.state_type == 'HARD')
        self.assert_(A.last_hard_state_id == 2)

        # The rule still be OK
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # Now we also set B as CRITICAL/HARD...
        self.scheduler_loop(2, [[B, 2, 'CRITICAL']])
        self.assert_(B.state == 'CRITICAL')
        self.assert_(B.state_type == 'HARD')
        self.assert_(B.last_hard_state_id == 2)

        # And now the state of the rule must be 2 now
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set A dn B WARNING now?
        self.scheduler_loop(2, [[A, 1, 'WARNING'], [B, 1, 'WARNING']])
        self.assert_(A.state == 'WARNING')
        self.assert_(A.state_type == 'HARD')
        self.assert_(A.last_hard_state_id == 1)
        self.assert_(B.state == 'WARNING')
        self.assert_(B.state_type == 'HARD')
        self.assert_(B.last_hard_state_id == 1)

        # Must be WARNING (worse no 0 value for both, like for AND rule)
        state = bp_rule.get_state()
        print "state", state
        self.assert_(state == 1)

        # Ok now more fun, with changing of_values and states

        ### W O O O O
        # 4 of: -> Ok (we got 4 OK, and not 4 warn or crit, so it's OK)
        # 5,1,1 -> Warning (at least one warning, and no crit -> warning)
        # 5,2,1 -> OK (we want warning only if we got 2 bad states, so not here)
        self.scheduler_loop(2, [[A, 1, 'WARNING'], [B, 0, 'OK']])
        # 4 of: -> 4,5,5
        if with_pct == False:
            bp_rule.of_values = ('4', '5', '5')
        else:
            bp_rule.of_values = ('80%', '100%', '100%')
        bp_rule.is_of_mul = False
        self.assert_(bp_rule.get_state() == 0)
        # 5,1,1
        if with_pct == False:
            bp_rule.of_values = ('5', '1', '1')
        else:
            bp_rule.of_values = ('100%', '20%', '20%')
        bp_rule.is_of_mul = True
        self.assert_(bp_rule.get_state() == 1)
        # 5,2,1
        if with_pct == False:
            bp_rule.of_values = ('5', '2', '1')
        else:
            bp_rule.of_values = ('100%', '40%', '20%')
        bp_rule.is_of_mul = True
        self.assert_(bp_rule.get_state() == 0)

        ###* W C O O O
        # 4 of: -> Crtitical (not 4 ok, so we take the worse state, the critical)
        # 4,1,1 -> Critical (2 states raise the waring, but on raise critical, so worse state is critical)
        self.scheduler_loop(2, [[A, 1, 'WARNING'], [B, 2, 'Crit']])
        # 4 of: -> 4,5,5
        if with_pct == False:
            bp_rule.of_values = ('4', '5', '5')
        else:
            bp_rule.of_values = ('80%', '100%', '100%')
        bp_rule.is_of_mul = False
        self.assert_(bp_rule.get_state() == 2)
        # 4,1,1
        if with_pct == False:
            bp_rule.of_values = ('4', '1', '1')
        else:
            bp_rule.of_values = ('40%', '20%', '20%')
        bp_rule.is_of_mul = True
        self.assert_(bp_rule.get_state() == 2)

        ##* W C C O O
        # * 2 of: OK
        # * 4,1,1 -> Critical (same as before)
        # * 4,1,3 -> warning (the warning rule is raised, but the critical is not)
        self.scheduler_loop(2, [[A, 1, 'WARNING'], [B, 2, 'Crit'], [C, 2, 'Crit']])
        # * 2 of: 2,5,5
        if with_pct == False:
            bp_rule.of_values = ('2', '5', '5')
        else:
            bp_rule.of_values = ('40%', '100%', '100%')
        bp_rule.is_of_mul = False
        self.assert_(bp_rule.get_state() == 0)
        # * 4,1,1
        if with_pct == False:
            bp_rule.of_values = ('4', '1', '1')
        else:
            bp_rule.of_values = ('80%', '20%', '20%')
        bp_rule.is_of_mul = True
        self.assert_(bp_rule.get_state() == 2)
        # * 4,1,3
        if with_pct == False:
            bp_rule.of_values = ('4', '1', '3')
        else:
            bp_rule.of_values = ('80%', '20%', '60%')
        bp_rule.is_of_mul = True
        self.assert_(bp_rule.get_state() == 1)

    # We will try a simple bd1 AND NOT db2
    def test_simple_and_not_business_correlator(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Simple_And_not")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '&')

        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        self.assert_(sons[0].operand == 'service')
        self.assert_(sons[0].sons[0] == svc_bd1)
        self.assert_(sons[1].operand == 'service')
        self.assert_(sons[1].sons[0] == svc_bd2)

        # Now state working on the states
        self.scheduler_loop(2, [[svc_bd1, 0, 'OK | value1=1 value2=2'], [svc_bd2, 2, 'CRITICAL | rtt=10']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')

        # We are a NOT, so should be OK here
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        # becase we want HARD states
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must go CRITICAL
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # Now we also set bd2 as WARNING/HARD...
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set one WARNING too?
        self.scheduler_loop(2, [[svc_bd1, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'WARNING')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 1)

        # Must be WARNING (worse no 0 value for both)
        state = bp_rule.get_state()
        self.assert_(state == 1)

        # Now try to get ok in both place, should be bad :)
        self.scheduler_loop(2, [[svc_bd1, 0, 'OK | value1=1 value2=2'], [svc_bd2, 0, 'OK | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 0)
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 0)

        # Must be CRITICAL (ok and not ok IS no OK :) )
        state = bp_rule.get_state()
        self.assert_(state == 2)






    # We will try a simple bd1 OR db2
    def test_multi_layers(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # THE RULE IS (test_host_0,db1| (test_host_0,db2 & (test_host_0,lvs1|test_host_0,lvs2) ) ) & test_router_0
        svc_lvs1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "lvs1")
        self.assert_(svc_lvs1 is not None)
        svc_lvs2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "lvs2")
        self.assert_(svc_lvs2 is not None)

        svc_bd1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db1")
        self.assert_(svc_bd1.got_business_rule == False)
        self.assert_(svc_bd1.business_rule is None)
        svc_bd2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "db2")
        self.assert_(svc_bd2.got_business_rule == False)
        self.assert_(svc_bd2.business_rule is None)
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Multi_levels")
        self.assert_(svc_cor.got_business_rule == True)
        self.assert_(svc_cor.business_rule is not None)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == '&')

        # We check for good parent/childs links
        # So svc_cor should be a son of svc_bd1 and svc_bd2
        # and bd1 and bd2 should be parents of svc_cor
        self.assert_(svc_cor in svc_bd1.child_dependencies)
        self.assert_(svc_cor in svc_bd2.child_dependencies)
        self.assert_(svc_cor in router.child_dependencies)
        self.assert_(svc_bd1 in svc_cor.parent_dependencies)
        self.assert_(svc_bd2 in svc_cor.parent_dependencies)
        self.assert_(router in svc_cor.parent_dependencies)


        sons = bp_rule.sons
        print "Sons,", sons
        # We've got 2 sons, 2 services nodes
        self.assert_(len(sons) == 2)
        # Son0 is (test_host_0,db1| (test_host_0,db2 & (test_host_0,lvs1|test_host_0,lvs2) ) )
        son0 = sons[0]
        self.assert_(son0.operand == '|')
        # Son1 is test_router_0
        self.assert_(sons[1].operand == 'host')
        self.assert_(sons[1].sons[0] == router)

        # Son0_0 is test_host_0,db1
        # Son0_1 is test_host_0,db2 & (test_host_0,lvs1|test_host_0,lvs2)
        son0_0 = son0.sons[0]
        son0_1 = son0.sons[1]
        self.assert_(son0_0.operand == 'service')
        self.assert_(son0_0.sons[0] == svc_bd1)
        self.assert_(son0_1.operand == '&')

        # Son0_1_0 is test_host_0,db2
        # Son0_1_1 is test_host_0,lvs1|test_host_0,lvs2
        son0_1_0 = son0_1.sons[0]
        son0_1_1 = son0_1.sons[1]
        self.assert_(son0_1_0.operand == 'service')
        self.assert_(son0_1_0.sons[0] == svc_bd2)
        self.assert_(son0_1_1.operand == '|')

        # Son0_1_1_0 is test_host_0,lvs1
        # Son0_1_1_1 is test_host_0,lvs2
        son0_1_1_0 = son0_1_1.sons[0]
        son0_1_1_1 = son0_1_1.sons[1]


        self.assert_(son0_1_1_0.operand == 'service')
        self.assert_(son0_1_1_0.sons[0] == svc_lvs1)
        self.assert_(son0_1_1_1.operand == 'service')
        self.assert_(son0_1_1_1.sons[0] == svc_lvs2)


        # Now state working on the states
        self.scheduler_loop(1, [[svc_bd2, 0, 'OK | value1=1 value2=2'], [svc_bd1, 0, 'OK | rtt=10'],
                                [svc_lvs1, 0, 'OK'], [svc_lvs2, 0, 'OK'], [router, 0, 'UP'] ])
        self.assert_(svc_bd1.state == 'OK')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd2.state == 'OK')
        self.assert_(svc_bd2.state_type == 'HARD')

        # All is green, the rule should be green too
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we set the bd1 as soft/CRITICAL
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'SOFT')
        self.assert_(svc_bd1.last_hard_state_id == 0)

        # The business rule must still be 0
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get bd1 CRITICAL/HARD
        self.scheduler_loop(1, [[svc_bd1, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd1.state == 'CRITICAL')
        self.assert_(svc_bd1.state_type == 'HARD')
        self.assert_(svc_bd1.last_hard_state_id == 2)

        # The rule must still be a 0 (or inside)
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we also set bd2 as CRITICAL/HARD... byebye 0 :)
        self.scheduler_loop(2, [[svc_bd2, 2, 'CRITICAL | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'CRITICAL')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 2)

        # And now the state of the rule must be 2
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # And If we set one WARNING?
        self.scheduler_loop(2, [[svc_bd2, 1, 'WARNING | value1=1 value2=2']])
        self.assert_(svc_bd2.state == 'WARNING')
        self.assert_(svc_bd2.state_type == 'HARD')
        self.assert_(svc_bd2.last_hard_state_id == 1)

        # Must be WARNING (better no 0 value)
        state = bp_rule.get_state()
        self.assert_(state == 1)

        # We should got now svc_bd2 and svc_bd1 as root problems
        print "Root problems"
        for p in svc_cor.source_problems:
            print p.get_full_name()
        self.assert_(svc_bd1 in svc_cor.source_problems)
        self.assert_(svc_bd2 in svc_cor.source_problems)



        # What about now with the router in DOWN?
        self.scheduler_loop(5, [[router, 2, 'DOWN']])
        self.assert_(router.state == 'DOWN')
        self.assert_(router.state_type == 'HARD')
        self.assert_(router.last_hard_state_id == 1)

        # Must be CRITICAL (CRITICAL VERSUS DOWN -> DOWN)
        state = bp_rule.get_state()
        self.assert_(state == 2)

        # Now our root problem is router
        print "Root problems"
        for p in svc_cor.source_problems:
            print p.get_full_name()
        self.assert_(router in svc_cor.source_problems)












    # We will try a strange rule that ask UP&UP -> DOWN&DONW-> OK
    def test_darthelmet_rule(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_darthelmet")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        A = self.sched.hosts.find_by_name("test_darthelmet_A")
        B = self.sched.hosts.find_by_name("test_darthelmet_B")

        self.assert_(host.got_business_rule == True)
        self.assert_(host.business_rule is not None)
        bp_rule = host.business_rule
        self.assert_(bp_rule.operand == '|')

        # Now state working on the states
        self.scheduler_loop(3, [[host, 0, 'UP'], [A, 0, 'UP'], [B, 0, 'UP'] ] )
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        self.assert_(A.state == 'UP')
        self.assert_(A.state_type == 'HARD')

        state = bp_rule.get_state()
        print "WTF0", state
        self.assert_(state == 0)

        # Now we set the A as soft/DOWN
        self.scheduler_loop(1, [[A, 2, 'DOWN']])
        self.assert_(A.state == 'DOWN')
        self.assert_(A.state_type == 'SOFT')
        self.assert_(A.last_hard_state_id == 0)

        # The business rule must still be 0
        state = bp_rule.get_state()
        self.assert_(state == 0)

        # Now we get A DOWN/HARD
        self.scheduler_loop(3, [[A, 2, 'DOWN']])
        self.assert_(A.state == 'DOWN')
        self.assert_(A.state_type == 'HARD')
        self.assert_(A.last_hard_state_id == 1)

        # The rule must still be a 2 (or inside)
        state = bp_rule.get_state()
        print "WFT", state
        self.assert_(state == 2)

        # Now we also set B as DOWN/HARD, should get back to 0!
        self.scheduler_loop(3, [[B, 2, 'DOWN']])
        self.assert_(B.state == 'DOWN')
        self.assert_(B.state_type == 'HARD')
        self.assert_(B.last_hard_state_id == 1)

        # And now the state of the rule must be 0 again! (strange rule isn't it?)
        state = bp_rule.get_state()
        self.assert_(state == 0)





class TestConfigBroken(ShinkenTest):
    """A class with a broken configuration, where business rules reference unknown hosts/services"""

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator_broken.cfg')


    def test_conf_is_correct(self):
        #
        # Business rules use services which don't exist. We want
        # the arbiter to output an error message and exit
        # in a controlled manner.
        #
        print "conf_is_correct", self.conf.conf_is_correct
        self.assert_(not self.conf.conf_is_correct)

        # Get the arbiter's log broks
        [b.prepare() for b in self.broks.values()]
        logs = [b.data['log'] for b in self.broks.values() if b.type == 'log']

        # Info: Simple_1Of_1unk_svc: my business rule is invalid
        # Info: Simple_1Of_1unk_svc: Business rule uses unknown service test_host_0/db3
        # Error: [items] In Simple_1Of_1unk_svc is incorrect ; from etc/business_correlator_broken/services.cfg
        self.assert_(len([log for log in logs if re.search('Simple_1Of_1unk_svc', log)]) == 3)
        self.assert_(len([log for log in logs if re.search('service test_host_0/db3', log)]) == 1)
        self.assert_(len([log for log in logs if re.search('Simple_1Of_1unk_svc.+from etc.+business_correlator_broken.+services.cfg', log)]) == 1)
        # Info: ERP_unk_svc: my business rule is invalid
        # Info: ERP_unk_svc: Business rule uses unknown service test_host_0/web100
        # Info: ERP_unk_svc: Business rule uses unknown service test_host_0/lvs100
        # Error: [items] In ERP_unk_svc is incorrect ; from etc/business_correlator_broken/services.cfg
        self.assert_(len([log for log in logs if re.search('ERP_unk_svc', log)]) == 4)
        self.assert_(len([log for log in logs if re.search('service test_host_0/web100', log)]) == 1)
        self.assert_(len([log for log in logs if re.search('service test_host_0/lvs100', log)]) == 1)
        self.assert_(len([log for log in logs if re.search('ERP_unk_svc.+from etc.+business_correlator_broken.+services.cfg', log)]) == 1)
        # Info: Simple_1Of_1unk_host: my business rule is invalid
        # Info: Simple_1Of_1unk_host: Business rule uses unknown host test_host_9
        # Error: [items] In Simple_1Of_1unk_host is incorrect ; from etc/business_correlator_broken/services.cfg
        self.assert_(len([log for log in logs if re.search('Simple_1Of_1unk_host', log)]) == 3)
        self.assert_(len([log for log in logs if re.search('host test_host_9', log)]) == 1)
        self.assert_(len([log for log in logs if re.search('Simple_1Of_1unk_host.+from etc.+business_correlator_broken.+services.cfg', log)]) == 1)

        # Now the number of all failed business rules.
        self.assert_(len([log for log in logs if re.search('my business rule is invalid', log)]) == 3)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_business_correlator_expand_expression
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test hostgroups and regex expressions expansion in
# business rules.
#

import re
from shinken_test import unittest, ShinkenTest

# Set this variable False to disable profiling test
PROFILE_BP_RULE_RE_PROCESSING = False


class TestBusinesscorrelExpand(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator_expand_expression.cfg')

    def test_hostgroup_expansion_bprule_simple_host_srv(self):
        for name in ("bprule_00", "bprule_01", "bprule_02", "bprule_03", "bprule_04", "bprule_05", "bprule_06"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == '&')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('2', '2', '2'))

            srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv1")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'service')
            self.assert_(sons[1].operand == 'service')

            self.assert_(srv1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(srv2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_simple_xof_host_srv(self):
        for name in ("bprule_10", "bprule_11", "bprule_12", "bprule_13", "bprule_14", "bprule_15", "bprule_16", "bprule_17"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == 'of:')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('1', '2', '2'))

            srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv1")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'service')
            self.assert_(sons[1].operand == 'service')

            self.assert_(srv1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(srv2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_combined_and(self):
        for name in ("bprule_20", "bprule_21", "bprule_22", "bprule_23", "bprule_24", "bprule_25", "bprule_26"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == '&')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('2', '2', '2'))

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)

            for son in sons:
                self.assert_(son.operand == '&')
                self.assert_(son.not_value is False)
                self.assert_(son.of_values == ('2', '2', '2'))
                self.assert_(len(son.sons) == 2)
                self.assert_(son.sons[0].operand == 'service')
                self.assert_(son.sons[1].operand == 'service')

            hst1_srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            hst2_srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv1")
            hst1_srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv2")
            hst2_srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

            self.assert_(hst1_srv1 in (sons[0].sons[0].sons[0], sons[0].sons[1].sons[0]))
            self.assert_(hst2_srv1 in (sons[0].sons[0].sons[0], sons[0].sons[1].sons[0]))
            self.assert_(hst1_srv2 in (sons[1].sons[0].sons[0], sons[1].sons[1].sons[0]))
            self.assert_(hst2_srv2 in (sons[1].sons[0].sons[0], sons[1].sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_combined_or(self):
        for name in ("bprule_30", "bprule_31", "bprule_32", "bprule_33", "bprule_34", "bprule_35", "bprule_36"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == '|')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('2', '2', '2'))

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)

            for son in sons:
                self.assert_(son.operand == '&')
                self.assert_(son.not_value is False)
                self.assert_(son.of_values == ('2', '2', '2'))
                self.assert_(len(son.sons) == 2)
                self.assert_(son.sons[0].operand == 'service')
                self.assert_(son.sons[1].operand == 'service')

            hst1_srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            hst2_srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv1")
            hst1_srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv2")
            hst2_srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

            self.assert_(hst1_srv1 in (sons[0].sons[0].sons[0], sons[0].sons[1].sons[0]))
            self.assert_(hst2_srv1 in (sons[0].sons[0].sons[0], sons[0].sons[1].sons[0]))
            self.assert_(hst1_srv2 in (sons[1].sons[0].sons[0], sons[1].sons[1].sons[0]))
            self.assert_(hst2_srv2 in (sons[1].sons[0].sons[0], sons[1].sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_simple_hosts(self):
        for name in ("bprule_40", "bprule_41", "bprule_42", "bprule_43"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == '&')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('2', '2', '2'))

            hst1 = self.sched.hosts.find_by_name("test_host_01")
            hst2 = self.sched.hosts.find_by_name("test_host_02")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'host')
            self.assert_(sons[1].operand == 'host')

            self.assert_(hst1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(hst2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_xof_hosts(self):
        for name in ("bprule_50", "bprule_51", "bprule_52", "bprule_53", "bprule_54"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == 'of:')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('1', '2', '2'))

            hst1 = self.sched.hosts.find_by_name("test_host_01")
            hst2 = self.sched.hosts.find_by_name("test_host_02")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'host')
            self.assert_(sons[1].operand == 'host')

            self.assert_(hst1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(hst2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_same_host_srv(self):
        for name in ("bprule_60", "bprule_61"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_01", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == '&')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('2', '2', '2'))

            srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv2")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'service')
            self.assert_(sons[1].operand == 'service')

            self.assert_(srv1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(srv2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_hostgroup_expansion_bprule_xof_same_host_srv(self):
        for name in ("bprule_70", "bprule_71"):
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("test_host_01", name)
            self.assert_(svc_cor.got_business_rule is True)
            self.assert_(svc_cor.business_rule is not None)
            bp_rule = svc_cor.business_rule
            self.assert_(bp_rule.operand == 'of:')
            self.assert_(bp_rule.not_value is False)
            self.assert_(bp_rule.of_values == ('1', '2', '2'))

            srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
            srv2 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv2")

            sons = bp_rule.sons
            self.assert_(len(sons) == 2)
            self.assert_(sons[0].operand == 'service')
            self.assert_(sons[1].operand == 'service')

            self.assert_(srv1 in (sons[0].sons[0], sons[1].sons[0]))
            self.assert_(srv2 in (sons[0].sons[0], sons[1].sons[0]))

    def test_macro_expansion_bprule_no_macro(self):
        # Tests macro expansion
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bprule_no_macro")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.processed_business_rule == "1 of: test_host_01,srv1 & test_host_02,srv2")
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        self.assert_(bp_rule.of_values == ('1', '2', '2'))

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        # Setting dependent services status
        self.scheduler_loop(1, [
            [svc1, 0, 'UP | value1=1 value2=2'],
            [svc2, 0, 'UP | value1=1 value2=2']])

        self.assert_(svc1.state == 'OK')
        self.assert_(svc1.state_type == 'HARD')
        self.assert_(svc2.state == 'OK')
        self.assert_(svc2.state_type == 'HARD')

        self.scheduler_loop(1, [[svc1, 2, 'CRITICAL | value1=1 value2=2']])

        self.assert_(svc1.state == 'CRITICAL')
        self.assert_(svc1.state_type == 'HARD')

        # Forces business rule evaluation.
        self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True)

        # Business rule should not have been re-evaluated (no macro in the
        # bp_rule)
        self.assert_(svc_cor.business_rule is bp_rule)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.get_state() == 0)
        self.assert_(svc_cor.last_hard_state_id == 0)

    def test_macro_expansion_bprule_macro_expand(self):
        # Tests macro expansion
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bprule_macro_expand")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.processed_business_rule == "1 of: test_host_01,srv1 & test_host_02,srv2")
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        self.assert_(bp_rule.of_values == ('1', '2', '2'))

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        # Setting dependent services status
        self.scheduler_loop(1, [
            [svc1, 0, 'UP | value1=1 value2=2'],
            [svc2, 0, 'UP | value1=1 value2=2']])

        self.assert_(svc1.state == 'OK')
        self.assert_(svc1.state_type == 'HARD')
        self.assert_(svc2.state == 'OK')
        self.assert_(svc2.state_type == 'HARD')

        self.scheduler_loop(1, [[svc1, 2, 'CRITICAL | value1=1 value2=2']])

        self.assert_(svc1.state == 'CRITICAL')
        self.assert_(svc1.state_type == 'HARD')

        # Forces business rule evaluation.
        self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True)

        # Business rule should not have been re-evaluated (macro did not change
        # value)
        self.assert_(svc_cor.business_rule is bp_rule)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.get_state() == 0)
        self.assert_(svc_cor.last_hard_state_id == 0)

    def test_macro_expansion_bprule_macro_modulated(self):
        # Tests macro modulation
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy_modulated", "bprule_macro_modulated")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.processed_business_rule == "2 of: test_host_01,srv1 & test_host_02,srv2")
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        self.assert_(bp_rule.of_values == ('2', '2', '2'))

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        # Setting dependent services status
        self.scheduler_loop(1, [
            [svc1, 0, 'UP | value1=1 value2=2'],
            [svc2, 0, 'UP | value1=1 value2=2']])

        self.assert_(svc1.state == 'OK')
        self.assert_(svc1.state_type == 'HARD')
        self.assert_(svc2.state == 'OK')
        self.assert_(svc2.state_type == 'HARD')

        self.scheduler_loop(1, [[svc1, 2, 'CRITICAL | value1=1 value2=2']])

        self.assert_(svc1.state == 'CRITICAL')
        self.assert_(svc1.state_type == 'HARD')

        # Forces business rule evaluation.
        self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True)

        # Business rule should not have been re-evaluated (macro did not change
        # value)
        self.assert_(svc_cor.business_rule is bp_rule)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.get_state() == 2)
        self.assert_(svc_cor.last_hard_state_id == 2)

        # Tests modulated value
        mod = self.sched.macromodulations.find_by_name("xof_modulation")
        mod.customs['_XOF'] = '1'

        # Forces business rule evaluation.
        self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True)

        self.assert_(svc_cor.processed_business_rule == "1 of: test_host_01,srv1 & test_host_02,srv2")
        self.assert_(svc_cor.business_rule is not bp_rule)
        bp_rule = svc_cor.business_rule
        self.assert_(bp_rule.operand == 'of:')
        self.assert_(bp_rule.of_values == ('1', '2', '2'))
        self.assert_(bp_rule.get_state() == 0)
        self.assert_(svc_cor.last_hard_state_id == 0)

        # Tests wrongly written macro modulation (inserts invalid string)
        mod.customs['_XOF'] = 'fake'

        # Forces business rule evaluation.
        self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True)

        # Business rule should have been re-evaluated (macro was modulated)
        self.assert_(svc_cor.business_rule is bp_rule)
        self.assert_(svc_cor.last_hard_state_id == 3)
        self.assert_(svc_cor.output.startswith("Error while re-evaluating business rule"))

    def test_macro_expansion_bprule_macro_profile(self):
        if PROFILE_BP_RULE_RE_PROCESSING is False:
            return

        import cProfile as profile

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        # Setting dependent services status
        self.scheduler_loop(1, [
            [svc1, 0, 'UP | value1=1 value2=2'],
            [svc2, 0, 'UP | value1=1 value2=2']], verbose=False)

        self.assert_(svc1.state == 'OK')
        self.assert_(svc1.state_type == 'HARD')
        self.assert_(svc2.state == 'OK')
        self.assert_(svc2.state_type == 'HARD')

        self.scheduler_loop(1, [[svc1, 2, 'CRITICAL | value1=1 value2=2']], verbose=False)

        self.assert_(svc1.state == 'CRITICAL')
        self.assert_(svc1.state_type == 'HARD')

        print "Profiling without macro"

        def profile_bp_rule_without_macro():
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bprule_no_macro")
            for i in range(1000):
                self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True, verbose=False)

        profile.runctx('profile_bp_rule_without_macro()', globals(), locals())

        print "Profiling with macro"

        def profile_bp_rule_macro_expand():
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bprule_macro_expand")
            for i in range(1000):
                self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True, verbose=False)

        profile.runctx('profile_bp_rule_macro_expand()', globals(), locals())

        print "Profiling with macro modulation"

        def profile_bp_rule_macro_modulated():
            svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy_modulated", "bprule_macro_modulated")
            for i in range(1000):
                self.scheduler_loop(2, [[svc_cor, None, None]], do_sleep=True, verbose=False)

        profile.runctx('profile_bp_rule_macro_modulated()', globals(), locals())


class TestConfigBroken(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator_expand_expression_broken.cfg')

    def test_hostgroup_expansion_errors(self):
        self.assert_(not self.conf.conf_is_correct)

        # Get the arbiter's log broks
        [b.prepare() for b in self.broks.values()]
        logs = [b.data['log'] for b in self.broks.values() if b.type == 'log']

        self.assert_(len([log for log in logs if re.search('Business rule uses invalid regex', log)]) == 1)
        self.assert_(len([log for log in logs if re.search('Business rule got an empty result', log)]) == 3)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_business_correlator_notifications
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test business rules smart notifications behaviour.
#

import time
from shinken_test import unittest, ShinkenTest


class TestBusinesscorrelNotifications(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator_notifications.cfg')

    def test_bprule_standard_notifications(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bp_rule_default")
        svc_cor.act_depend_of = []
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_smart_notifications is False)

        dummy = self.sched.hosts.find_by_name("dummy")
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        self.scheduler_loop(2, [
            [dummy, 0, 'UP dummy'],
            [svc1, 0, 'OK test_host_01/srv1'],
            [svc2, 2, 'CRITICAL test_host_02/srv2']], do_sleep=True)

        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_02;srv2;2;1;1;lausser;blablub" % (now)
        self.sched.run_external_command(cmd)
        self.assert_(svc2.problem_has_been_acknowledged is True)

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])

        self.assert_(svc_cor.business_rule.get_state() == 2)
        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

    def test_bprule_smart_notifications_ack(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bp_rule_smart_notif")
        svc_cor.act_depend_of = []
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_smart_notifications is True)

        dummy = self.sched.hosts.find_by_name("dummy")
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        self.scheduler_loop(2, [
            [dummy, 0, 'UP dummy'],
            [svc1, 0, 'OK test_host_01/srv1'],
            [svc2, 2, 'CRITICAL test_host_02/srv2']], do_sleep=True)

        self.assert_(svc_cor.business_rule.get_state() == 2)
        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

        now = time.time()
        cmd = "[%lu] ACKNOWLEDGE_SVC_PROBLEM;test_host_02;srv2;2;1;1;lausser;blablub" % (now)
        self.sched.run_external_command(cmd)
        self.assert_(svc2.problem_has_been_acknowledged is True)

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])

        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is True)

    def test_bprule_smart_notifications_svc_ack_downtime(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bp_rule_smart_notif")
        svc_cor.act_depend_of = []
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_smart_notifications is True)
        self.assert_(svc_cor.business_rule_downtime_as_ack is False)

        dummy = self.sched.hosts.find_by_name("dummy")
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")

        self.scheduler_loop(2, [
            [dummy, 0, 'UP dummy'],
            [svc1, 0, 'OK test_host_01/srv1'],
            [svc2, 2, 'CRITICAL test_host_02/srv2']], do_sleep=True)

        self.assert_(svc_cor.business_rule.get_state() == 2)
        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

        duration = 600
        now = time.time()
        # fixed downtime valid for the next 10 minutes
        cmd = "[%lu] SCHEDULE_SVC_DOWNTIME;test_host_02;srv2;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])
        self.assert_(svc2.scheduled_downtime_depth > 0)

        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

        svc_cor.business_rule_downtime_as_ack = True

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])

        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is True)

    def test_bprule_smart_notifications_hst_ack_downtime(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bp_rule_smart_notif")
        svc_cor.act_depend_of = []
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_smart_notifications is True)
        self.assert_(svc_cor.business_rule_downtime_as_ack is False)

        dummy = self.sched.hosts.find_by_name("dummy")
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")
        hst2 = self.sched.hosts.find_by_name("test_host_02")

        self.scheduler_loop(2, [
            [dummy, 0, 'UP dummy'],
            [svc1, 0, 'OK test_host_01/srv1'],
            [svc2, 2, 'CRITICAL test_host_02/srv2']], do_sleep=True)

        self.assert_(svc_cor.business_rule.get_state() == 2)
        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

        duration = 600
        now = time.time()
        # fixed downtime valid for the next 10 minutes
        cmd = "[%lu] SCHEDULE_HOST_DOWNTIME;test_host_02;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])
        self.assert_(hst2.scheduled_downtime_depth > 0)

        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is False)

        svc_cor.business_rule_downtime_as_ack = True

        self.scheduler_loop(1, [[svc_cor, None, None]], do_sleep=True)
        self.scheduler_loop(1, [[svc_cor, None, None]])

        self.assert_(svc_cor.notification_is_blocked_by_item('PROBLEM') is True)

    def test_bprule_child_notification_options(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "bp_rule_child_notif")
        svc_cor.act_depend_of = []
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        hst2 = self.sched.hosts.find_by_name("test_host_02")

        self.assert_(svc1.notification_options == ['w', 'u', 'c', 'r', 's'])
        self.assert_(hst2.notification_options == ['d', 'u', 'r', 's'])

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_business_correlator_output
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test business rules output based on template expansion.
#

import time
from shinken_test import unittest, ShinkenTest


class TestBusinesscorrelOutput(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_correlator_output.cfg')

    def test_service_shorten_status(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "empty_bp_rule_output")
        self.assert_(svc_cor.status_to_short_status("OK") == "O")
        self.assert_(svc_cor.status_to_short_status("WARNING") == "W")
        self.assert_(svc_cor.status_to_short_status("CRITICAL") == "C")
        self.assert_(svc_cor.status_to_short_status("UNKNOWN") == "U")
        self.assert_(svc_cor.status_to_short_status("UP") == "U")
        self.assert_(svc_cor.status_to_short_status("DOWN") == "D")
        self.assert_(svc_cor.status_to_short_status("FAKE") == "FAKE")

    def test_bprule_empty_output(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "empty_bp_rule_output")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.get_business_rule_output() == "")

    def test_bprule_expand_template_macros(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "formatted_bp_rule_output")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_host_03", "srv3")
        hst4 = self.sched.hosts.find_by_name("test_host_04")

        for i in range(2):
            self.scheduler_loop(1, [
                [svc1, 0, 'OK test_host_01/srv1'],
                [svc2, 1, 'WARNING test_host_02/srv2'],
                [svc3, 2, 'CRITICAL test_host_03/srv3'],
                [hst4, 2, 'DOWN test_host_04']])

        time.sleep(61)
        self.sched.manage_internal_checks()
        self.sched.consume_results()

        # Performs checks
        template = "$STATUS$,$SHORT_STATUS$,$HOST_NAME$,$SERVICE_DESCRIPTION$,$FULL_NAME$"
        output = svc_cor.expand_business_rule_item_macros(template, svc1)
        self.assert_(output == "OK,O,test_host_01,srv1,test_host_01/srv1")
        output = svc_cor.expand_business_rule_item_macros(template, svc2)
        self.assert_(output == "WARNING,W,test_host_02,srv2,test_host_02/srv2")
        output = svc_cor.expand_business_rule_item_macros(template, svc3)
        self.assert_(output == "CRITICAL,C,test_host_03,srv3,test_host_03/srv3")
        output = svc_cor.expand_business_rule_item_macros(template, hst4)
        self.assert_(output == "DOWN,D,test_host_04,,test_host_04")
        output = svc_cor.expand_business_rule_item_macros(template, svc_cor)
        self.assert_(output == "CRITICAL,C,dummy,formatted_bp_rule_output,dummy/formatted_bp_rule_output")

    def test_bprule_output(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "formatted_bp_rule_output")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_output_template == "$STATUS$ $([$STATUS$: $FULL_NAME$] )$")

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_host_03", "srv3")
        hst4 = self.sched.hosts.find_by_name("test_host_04")

        for i in range(2):
            self.scheduler_loop(1, [
                [svc1, 0, 'OK test_host_01/srv1'],
                [svc2, 1, 'WARNING test_host_02/srv2'],
                [svc3, 2, 'CRITICAL test_host_03/srv3'],
                [hst4, 2, 'DOWN test_host_04']])

        time.sleep(61)
        self.sched.manage_internal_checks()
        self.sched.consume_results()

        # Performs checks
        output = svc_cor.output
        self.assert_(output.find("[WARNING: test_host_02/srv2]") > 0)
        self.assert_(output.find("[CRITICAL: test_host_03/srv3]") > 0)
        self.assert_(output.find("[DOWN: test_host_04]") > 0)
        # Should not display OK state checks
        self.assert_(output.find("[OK: test_host_01/srv1]") == -1)
        self.assert_(output.startswith("CRITICAL"))

    def test_bprule_xof_one_critical_output(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "formatted_bp_rule_xof_output")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_output_template == "$STATUS$ $([$STATUS$: $FULL_NAME$] )$")

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_host_03", "srv3")
        hst4 = self.sched.hosts.find_by_name("test_host_04")

        for i in range(2):
            self.scheduler_loop(1, [
                [svc1, 0, 'OK test_host_01/srv1'],
                [svc2, 0, 'OK test_host_02/srv2'],
                [svc3, 2, 'CRITICAL test_host_03/srv3'],
                [hst4, 0, 'UP test_host_04']])

        time.sleep(61)
        self.sched.manage_internal_checks()
        self.sched.consume_results()

        # Performs checks
        self.assert_(svc_cor.business_rule.get_state() == 0)
        self.assert_(svc_cor.output == "OK [CRITICAL: test_host_03/srv3]")

    def test_bprule_xof_all_ok_output(self):
        svc_cor = self.sched.services.find_srv_by_name_and_hostname("dummy", "formatted_bp_rule_xof_output")
        self.assert_(svc_cor.got_business_rule is True)
        self.assert_(svc_cor.business_rule is not None)
        self.assert_(svc_cor.business_rule_output_template == "$STATUS$ $([$STATUS$: $FULL_NAME$] )$")

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv2")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_host_03", "srv3")
        hst4 = self.sched.hosts.find_by_name("test_host_04")

        for i in range(2):
            self.scheduler_loop(1, [
                [svc1, 0, 'OK test_host_01/srv1'],
                [svc2, 0, 'OK test_host_02/srv2'],
                [svc3, 0, 'OK test_host_03/srv3'],
                [hst4, 0, 'UP test_host_04']])

        time.sleep(61)
        self.sched.manage_internal_checks()
        self.sched.consume_results()

        # Performs checks
        self.assert_(svc_cor.business_rule.get_state() == 0)
        self.assert_(svc_cor.output == "OK all checks were successful.")


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_business_rules_with_bad_realm_conf
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestBusinessRulesBadRealmConf(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_business_rules_bad_realm_conf.cfg')

    def test_bad_conf(self):
        self.assert_(not self.conf.conf_is_correct)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_checkmodulations
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestCheckModulations(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_checkmodulations.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("host_modulated")
        self.assert_(host is not None)
        print host.checkmodulations

        mod = self.sched.checkmodulations.find_by_name("MODULATION")
        self.assert_(mod is not None)

        self.assert_(mod in host.checkmodulations)

        c = None
        for c in host.checks_in_progress:
            print c.command
            self.assert_(c.command == 'plugins/nothing VALUE')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_clean_sched_queues
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestSchedCleanQueues(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_clean_sched_queues.cfg')

    # Try to generate a bunch of external commands
    # and see if they are drop like it should
    def test_sched_clean_queues(self):
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        #host.__class__.obsess_over = True
        #host.obsess_over_host = True
        for i in xrange(1, 1001):
            host.get_obsessive_compulsive_processor_command()
        print "New len", len(host.actions)
        self.assert_(len(host.actions) >= 1000)
        self.sched.get_new_actions()
        print len(self.sched.actions)
        # So get our 1000 external commands
        self.assert_(len(self.sched.actions) >= 1000)

        # Try to call the clean, they are just too many!
        self.sched.clean_queues()
        # Should have something like 16 event handler
        print len(self.sched.actions)
        self.assert_(len(self.sched.actions) < 30)

        # Now for Notifications and co
        for i in xrange(1, 1001):
            host.create_notifications('PROBLEM')
        self.sched.get_new_actions()
        print len(self.sched.actions)
        # So get our 1000 notifications
        self.assert_(len(self.sched.actions) >= 1000)

        # Try to call the clean, they are just too many!
        self.sched.clean_queues()
        print len(self.sched.actions)
        self.assert_(len(self.sched.actions) < 30)

        #####  And now broks
        l = []
        for i in xrange(1, 1001):
            b = host.get_update_status_brok()
            l.append(b)
        host.broks = l

        self.sched.get_new_broks()
        print "LEn broks", len(self.sched.broks)
        self.assert_(len(self.sched.broks) >= 1000)
        self.sched.clean_queues()
        print "LEn broks", len(self.sched.broks)
        self.assert_(len(self.sched.broks) < 30)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_command
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *

from shinken.commandcall import CommandCall
from shinken.objects import Command, Commands


class TestCommand(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def test_command(self):
        t = {'command_name': 'check_command_test',
             'command_line': '/tmp/dummy_command.sh $ARG1$ $ARG2$',
             'poller_tag': 'DMZ'
             }
        c = Command(t)
        self.assert_(c.command_name == 'check_command_test')
        b = c.get_initial_status_brok()
        self.assert_(b.type == 'initial_command_status')

        # now create a commands packs
        cs = Commands([c])
        dummy_call = "check_command_test!titi!toto"
        cc = CommandCall(cs, dummy_call)
        self.assert_(cc.is_valid() == True)
        self.assert_(cc.command == c)
        self.assert_(cc.poller_tag == 'DMZ')



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_commands_perfdata
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test acknowledge of problems
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_commands_perfdata.cfg')

    def test_service_perfdata_command(self):
        self.print_header()

        # We want an eventhandelr (the perfdata command) to be put in the actions dict
        # after we got a service check
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        print "Service perfdata command", svc.__class__.perfdata_command, type(svc.__class__.perfdata_command)
        # We do not want to be just a string but a real command
        self.assert_(not isinstance(svc.__class__.perfdata_command, str))
        print svc.__class__.perfdata_command.__class__.my_type
        self.assert_(svc.__class__.perfdata_command.__class__.my_type == 'CommandCall')
        self.scheduler_loop(1, [[svc, 0, 'OK | bibi=99%']])
        print "Actions", self.sched.actions
        self.assert_(self.count_actions() == 1)

        # Ok now I disable the perfdata
        now = time.time()
        cmd = "[%lu] DISABLE_PERFORMANCE_DATA" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [[svc, 0, 'OK | bibi=99%']])
        print "Actions", self.sched.actions
        self.assert_(self.count_actions() == 0)

    def test_host_perfdata_command(self):
        # We want an eventhandelr (the perfdata command) to be put in the actions dict
        # after we got a service check
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        print "Host perfdata command", host.__class__.perfdata_command, type(host.__class__.perfdata_command)
        # We do not want to be just a string but a real command
        self.assert_(not isinstance(host.__class__.perfdata_command, str))
        print host.__class__.perfdata_command.__class__.my_type
        self.assert_(host.__class__.perfdata_command.__class__.my_type == 'CommandCall')
        self.scheduler_loop(1, [[host, 0, 'UP | bibi=99%']])
        print "Actions", self.sched.actions
        self.assert_(self.count_actions() == 1)

        # Ok now I disable the perfdata
        now = time.time()
        cmd = "[%lu] DISABLE_PERFORMANCE_DATA" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [[host, 0, 'UP | bibi=99%']])
        print "Actions", self.sched.actions
        self.assert_(self.count_actions() == 0)

    def test_multiline_perfdata(self):
        self.print_header()

        # We want an eventhandelr (the perfdata command) to be put in the actions dict
        # after we got a service check
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        print "Service perfdata command", svc.__class__.perfdata_command, type(svc.__class__.perfdata_command)
        # We do not want to be just a string but a real command
        self.assert_(not isinstance(svc.__class__.perfdata_command, str))
        print svc.__class__.perfdata_command.__class__.my_type
        self.assert_(svc.__class__.perfdata_command.__class__.my_type == 'CommandCall')
        output = """DISK OK - free space: / 3326 MB (56%); | /=2643MB;5948;5958;0;5968
/ 15272 MB (77%);
/boot 68 MB (69%);
/home 69357 MB (27%);
/var/log 819 MB (84%); | /boot=68MB;88;93;0;98
/home=69357MB;253404;253409;0;253414
/var/log=818MB;970;975;0;980
        """
        self.scheduler_loop(1, [[svc, 0, output]])
        print "Actions", self.sched.actions
        print 'Output', svc.output
        print 'long', svc.long_output
        print 'perf', svc.perf_data

        self.assert_(svc.output.strip() == 'DISK OK - free space: / 3326 MB (56%);')
        self.assert_(svc.perf_data.strip() == u'/=2643MB;5948;5958;0;5968 /boot=68MB;88;93;0;98 /home=69357MB;253404;253409;0;253414 /var/log=818MB;970;975;0;980')
        print svc.long_output.split('\n')
        self.assert_(svc.long_output == u"""/ 15272 MB (77%);
/boot 68 MB (69%);
/home 69357 MB (27%);
/var/log 819 MB (84%);""")



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_complex_hostgroups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestComplexHostgroups(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_complex_hostgroups.cfg')

    def get_svc(self):
        return self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

    def find_service(self, name, desc):
        return self.sched.services.find_srv_by_name_and_hostname(name, desc)

    def find_host(self, name):
        return self.sched.hosts.find_by_name(name)

    def find_hostgroup(self, name):
        return self.sched.hostgroups.find_by_name(name)

    def dump_hosts(self, svc):
        for h in svc.host_name:
            print h

    # check if service exist in hst, but NOT in others
    def srv_define_only_on(self, desc, hsts):
        r = True
        # first hsts
        for h in hsts:
            svc = self.find_service(h.host_name, desc)
            if svc is None:
                print "Error: the host %s is missing service %s!!" % (h.host_name, desc)
                r = False

        for h in self.sched.hosts:
            if h not in hsts:
                svc = self.find_service(h.host_name, desc)
                if svc is not None:
                    print "Error: the host %s got the service %s!!" % (h.host_name, desc)
                    r = False
        return r

    def test_complex_hostgroups(self):
        print self.sched.services.items
        svc = self.get_svc()
        print "Service", svc
        #print self.conf.hostgroups

        # All our hosts
        test_linux_web_prod_0 = self.find_host('test_linux_web_prod_0')
        test_linux_web_qual_0 = self.find_host('test_linux_web_qual_0')
        test_win_web_prod_0 = self.find_host('test_win_web_prod_0')
        test_win_web_qual_0 = self.find_host('test_win_web_qual_0')
        test_linux_file_prod_0 = self.find_host('test_linux_file_prod_0')

        hg_linux = self.find_hostgroup('linux')
        hg_web = self.find_hostgroup('web')
        hg_win = self.find_hostgroup('win')
        hg_file = self.find_hostgroup('file')
        print "HG Linux", hg_linux
        for h in hg_linux:
            print "H", h.get_name()

        self.assert_(test_linux_web_prod_0 in hg_linux.members)
        self.assert_(test_linux_web_prod_0 not in hg_file.members)

        # First the service define for linux only
        svc = self.find_service('test_linux_web_prod_0', 'linux_0')
        print "Service Linux only", svc.get_dbg_name()
        r = self.srv_define_only_on('linux_0', [test_linux_web_prod_0, test_linux_web_qual_0, test_linux_file_prod_0])
        self.assert_(r == True)

        print "Service Linux,web"
        r = self.srv_define_only_on('linux_web_0', [test_linux_web_prod_0, test_linux_web_qual_0, test_linux_file_prod_0, test_win_web_prod_0, test_win_web_qual_0])
        self.assert_(r == True)

        ### Now the real complex things :)
        print "Service Linux&web"
        r = self.srv_define_only_on('linux_AND_web_0', [test_linux_web_prod_0, test_linux_web_qual_0])
        self.assert_(r == True)

        print "Service Linux|web"
        r = self.srv_define_only_on('linux_OR_web_0', [test_linux_web_prod_0, test_linux_web_qual_0, test_win_web_prod_0, test_win_web_qual_0, test_linux_file_prod_0])
        self.assert_(r == True)

        print "(linux|web),file"
        r = self.srv_define_only_on('linux_OR_web_PAR_file0', [test_linux_web_prod_0, test_linux_web_qual_0, test_win_web_prod_0, test_win_web_qual_0, test_linux_file_prod_0, test_linux_file_prod_0])
        self.assert_(r == True)

        print "(linux|web)&prod"
        r = self.srv_define_only_on('linux_OR_web_PAR_AND_prod0', [test_linux_web_prod_0, test_win_web_prod_0, test_linux_file_prod_0])
        self.assert_(r == True)

        print "(linux|web)&(*&!prod)"
        r = self.srv_define_only_on('linux_OR_web_PAR_AND_NOT_prod0', [test_linux_web_qual_0, test_win_web_qual_0])
        self.assert_(r == True)

        print "Special minus problem"
        r = self.srv_define_only_on('name-with-minus-in-it', [test_linux_web_prod_0])
        self.assert_(r == True)

        print "(linux|web)&prod AND not test_linux_file_prod_0"
        r = self.srv_define_only_on('linux_OR_web_PAR_AND_prod0_AND_NOT_test_linux_file_prod_0', [test_linux_web_prod_0, test_win_web_prod_0])
        self.assert_(r == True)

        print "win&((linux|web)&prod) AND not test_linux_file_prod_0"
        r = self.srv_define_only_on('WINDOWS_AND_linux_OR_web_PAR_AND_prod0_AND_NOT_test_linux_file_prod_0', [test_win_web_prod_0])
        self.assert_(r == True)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_config
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_broken_1.cfg')

    def test_conf_is_correct(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        self.assert_(not self.conf.conf_is_correct)
        #self.show_logs()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_conf_in_symlinks
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#
import os
import sys
from shinken_test import *


class TestConfigWithSymlinks(ShinkenTest):

    def setUp(self):
        if os.name == 'nt':
            return
        self.setup_with_file('etc/shinken_conf_in_symlinks.cfg')

    def test_symlinks(self):
        if os.name == 'nt':
            return
        if sys.version_info < (2 , 6):
            print "************* WARNING********"*200
            print "On python 2.4 and 2.5, the symlinks following is NOT managed"
            return
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_HIDDEN")
        self.assert_(svc is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_contactdowntimes
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test host- and service-downtimes.
#

from shinken_test import *


class TestContactDowntime(ShinkenTest):

    def test_contact_downtime(self):
        self.print_header()
        # schedule a 2-minute downtime
        # downtime must be active
        # consume a good result, sleep for a minute
        # downtime must be active
        # consume a bad result
        # downtime must be active
        # no notification must be found in broks
        duration = 600
        now = time.time()
        # downtime valid for the next 2 minutes
        test_contact = self.sched.contacts.find_by_name('test_contact')
        cmd = "[%lu] SCHEDULE_CONTACT_DOWNTIME;test_contact;%d;%d;lausser;blablub" % (now, now, now + duration)
        self.sched.run_external_command(cmd)

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # Change the notif interval, so we can notify as soon as we want
        svc.notification_interval = 0.001

        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router

        #time.sleep(20)
        # We loop, the downtime wil be check and activate
        self.scheduler_loop(1, [[svc, 0, 'OK'], [host, 0, 'UP']])

        self.assert_(self.any_log_match('CONTACT DOWNTIME ALERT.*;STARTED'))
        self.show_and_clear_logs()

        print "downtime was scheduled. check its activity and the comment\n"*5
        self.assert_(len(self.sched.contact_downtimes) == 1)
        self.assert_(len(test_contact.downtimes) == 1)
        self.assert_(test_contact.downtimes[0] in self.sched.contact_downtimes.values())

        self.assert_(test_contact.downtimes[0].is_in_effect)
        self.assert_(not test_contact.downtimes[0].can_be_deleted)

        # Ok, we define the downtime like we should, now look at if it does the job: do not
        # raise notif during a downtime for this contact
        self.scheduler_loop(3, [[svc, 2, 'CRITICAL']])

        # We should NOT see any service notification
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        # Now we short the downtime a lot so it will be stop at now + 1 sec.
        test_contact.downtimes[0].end_time = time.time() + 1

        time.sleep(2)

        # We invalidate it with a scheduler loop
        self.scheduler_loop(1, [])

        # So we should be out now, with a log
        self.assert_(self.any_log_match('CONTACT DOWNTIME ALERT.*;STOPPED'))
        self.show_and_clear_logs()

        print "\n\nDowntime was ended. Check it is really stopped"
        self.assert_(len(self.sched.contact_downtimes) == 0)
        self.assert_(len(test_contact.downtimes) == 0)

        for n in svc.notifications_in_progress.values():
            print "NOTIF", n, n.t_to_go, time.time()

        # Now we want this contact to be really notify!
        # Ok, we define the downtime like we should, now look at if it does the job: do not
        # raise notif during a downtime for this contact
        time.sleep(1)
        self.scheduler_loop(3, [[svc, 2, 'CRITICAL']])
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        for n in svc.notifications_in_progress.values():
            print "NOTIF", n, n.t_to_go, time.time(), time.time() - n.t_to_go


    def test_contact_downtime_and_cancel(self):
        self.print_header()
        # schedule a 2-minute downtime
        # downtime must be active
        # consume a good result, sleep for a minute
        # downtime must be active
        # consume a bad result
        # downtime must be active
        # no notification must be found in broks
        duration = 600
        now = time.time()
        # downtime valid for the next 2 minutes
        test_contact = self.sched.contacts.find_by_name('test_contact')
        cmd = "[%lu] SCHEDULE_CONTACT_DOWNTIME;test_contact;%d;%d;lausser;blablub" % (now, now, now + duration)
        self.sched.run_external_command(cmd)

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # Change the notif interval, so we can notify as soon as we want
        svc.notification_interval = 0.001

        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router

        #time.sleep(20)
        # We loop, the downtime wil be check and activate
        self.scheduler_loop(1, [[svc, 0, 'OK'], [host, 0, 'UP']])

        self.assert_(self.any_log_match('CONTACT DOWNTIME ALERT.*;STARTED'))
        self.show_and_clear_logs()

        print "downtime was scheduled. check its activity and the comment"
        self.assert_(len(self.sched.contact_downtimes) == 1)
        self.assert_(len(test_contact.downtimes) == 1)
        self.assert_(test_contact.downtimes[0] in self.sched.contact_downtimes.values())

        self.assert_(test_contact.downtimes[0].is_in_effect)
        self.assert_(not test_contact.downtimes[0].can_be_deleted)

        time.sleep(1)
        # Ok, we define the downtime like we should, now look at if it does the job: do not
        # raise notif during a downtime for this contact
        self.scheduler_loop(3, [[svc, 2, 'CRITICAL']])

        # We should NOT see any service notification
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        downtime_id = test_contact.downtimes[0].id
        # OK, Now we cancel this downtime, we do not need it anymore
        cmd = "[%lu] DEL_CONTACT_DOWNTIME;%d" % (now, downtime_id)
        self.sched.run_external_command(cmd)

        # We check if the downtime is tag as to remove
        self.assert_(test_contact.downtimes[0].can_be_deleted)

        # We really delete it
        self.scheduler_loop(1, [])

        # So we should be out now, with a log
        self.assert_(self.any_log_match('CONTACT DOWNTIME ALERT.*;CANCELLED'))
        self.show_and_clear_logs()

        print "Downtime was cancelled"
        self.assert_(len(self.sched.contact_downtimes) == 0)
        self.assert_(len(test_contact.downtimes) == 0)

        time.sleep(1)
        # Now we want this contact to be really notify!
        # Ok, we define the downtime like we should, now look at if it does the job: do not
        # raise notif during a downtime for this contact
        self.scheduler_loop(3, [[svc, 2, 'CRITICAL']])
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_contactgroups_plus_inheritance
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test attribute inheritance and the right order
#

from shinken_test import *


class TestPlusInInheritance(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_contactgroups_plus_inheritance.cfg')

    def _dump(self, h):
        print "Dumping host", h.get_name()
        print h.contact_groups
        for c in h.contacts:
            print "->",c.get_name()
                        

    def test_contactgroups_plus_inheritance(self):
        host0 = self.sched.hosts.find_by_name("test_host_0")
        # HOST 1 should have 2 group of contacts
        # WARNING, it's a string, not the real objects!
        self._dump(host0)

        self.assert_("test_contact_1" in [c .get_name() for c in host0.contacts])
        self.assert_("test_contact_2" in [c .get_name() for c in host0.contacts])

        host2 = self.sched.hosts.find_by_name("test_host_2")
        self._dump(host2)
        self.assert_("test_contact_1" in [c .get_name() for c in host2.contacts])

        host3 = self.sched.hosts.find_by_name("test_host_3")
        self._dump(host3)
        self.assert_("test_contact_1" in [c .get_name() for c in host3.contacts])
        self.assert_("test_contact_2" in [c .get_name() for c in host3.contacts])

        host4 = self.sched.hosts.find_by_name("test_host_4")
        self._dump(host4)
        self.assert_("test_contact_1" in [c .get_name() for c in host4.contacts])

        host5 = self.sched.hosts.find_by_name("test_host_5")
        self._dump(host5)
        self.assert_("test_contact_1" in [c .get_name() for c in host5.contacts])
        self.assert_("test_contact_2" in [c .get_name() for c in host5.contacts])

        
        host6 = self.sched.hosts.find_by_name("test_host_6")
        self._dump(host6)
        self.assert_("test_contact_1" in [c .get_name() for c in host6.contacts])
        self.assert_("test_contact_2" in [c .get_name() for c in host6.contacts])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_contactgroup_nomembers
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestContactgroupWitoutMembers(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_contactgroup_nomembers.cfg')

    # It seems that a contact group with no member cause some crash for the arbiter.
    # should fix it :)
    def test_contactgroup_nomember(self):
        # Look for the members of the test_contact_nomember
        cg = self.sched.conf.contactgroups.find_by_name('test_contact_nomember')
        self.assert_(cg is not None)
        print cg.members
        self.assert_(cg.members == [])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_link_from_ext_cmd
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestCreateLinkFromExtCmd(ShinkenTest):
    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    def test_simple_host_link(self):
        now = int(time.time())
        h = self.sched.hosts.find_by_name('test_host_0')
        self.assert_(h is not None)
        h.act_depend_of = []
        r = self.sched.hosts.find_by_name('test_router_0')
        self.assert_(r is not None)
        r.act_depend_of = []
        e = ExternalCommandManager(self.conf, 'dispatcher')
        cmd = "[%lu] ADD_SIMPLE_HOST_DEPENDENCY;test_host_0;test_router_0" % now
        self.sched.run_external_command(cmd)
        self.assert_(h.is_linked_with_host(r))

        # Now we remove this link
        cmd = "[%lu] DEL_HOST_DEPENDENCY;test_host_0;test_router_0" % now
        self.sched.run_external_command(cmd)
        self.assert_(not h.is_linked_with_host(r))



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_critmodulation
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestCritMod(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_critmodulation.cfg')

    def test_critmodulation_def(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get our criticity modulation"
        cm = self.sched.conf.businessimpactmodulations.find_by_name('CritMod')
        self.assert_(cm is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        print svc.business_impact_modulations
        self.assert_(cm in svc.business_impact_modulations)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_css_in_command
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestCssInCommands(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_css_in_command.cfg')

    def test_dummy(self):
        r = self.conf.conf_is_correct
        self.assert_(r)
        print r


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_customs_on_service_hosgroups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestCustomsonservicehosgroups(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_customs_on_service_hosgroups.cfg')

    # We look for 3 services: on defined as direct on 1 hosts, on other
    # on 2 hsots, and a last one on a hostgroup
    def test_check_for_custom_copy_on_serice_hostgroups(self):
        # The one host service
        svc_one_host = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_on_1_host")
        self.assert_(svc_one_host is not None)
        # The 2 hosts service(s)
        svc_two_hosts_1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_on_2_hosts")
        self.assert_(svc_two_hosts_1 is not None)
        svc_two_hosts_2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "test_on_2_hosts")
        self.assert_(svc_two_hosts_2 is not None)
        # Then the one defined on a hostgroup
        svc_on_group = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "test_on_group")
        self.assert_(svc_on_group is not None)

        # Each one should have customs
        self.assert_(svc_one_host.customs['_CUSTNAME'] == 'custvalue')
        self.assert_(svc_two_hosts_1.customs['_CUSTNAME'] == 'custvalue')
        self.assert_(svc_two_hosts_2.customs['_CUSTNAME'] == 'custvalue')
        self.assert_(svc_on_group.customs['_CUSTNAME'] == 'custvalue')






if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_db
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
from shinken.db import DB


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def create_db(self):
        self.db = DB(table_prefix='test_')

    def test_create_insert_query(self):
        self.create_db()
        data = {'id': "1", "is_master": True, 'plop': "master of the universe"}
        q = self.db.create_insert_query('instances', data)
        self.assert_(q == "INSERT INTO test_instances  (is_master , id , plop  ) VALUES ('1' , '1' , 'master of the universe'  )")

        # Now some UTF8 funny characters
        data = {'id': "1", "is_master": True, 'plop': u''}
        q = self.db.create_insert_query('instances', data)
        #print "Q", q
        c = u"INSERT INTO test_instances  (is_master , id , plop  ) VALUES ('1' , '1' , ''  )"
        print type(q), type(c)
        print len(q), len(c)

        self.assert_(q == c)

    def test_update_query(self):
        self.create_db()
        data = {'id': "1", "is_master": True, 'plop': "master of the universe"}
        where = {'id': "1", "is_master": True}
        q = self.db.create_update_query('instances', data, where)
        # beware of the last space
        print "Q", q
        self.assert_(q == "UPDATE test_instances set plop='master of the universe'  WHERE is_master='1' and id='1' ")

        # Now some UTF8 funny characters
        data = {'id': "1", "is_master": True, 'plop': u''}
        where = {'id': "", "is_master": True}
        q = self.db.create_update_query('instances', data, where)
        #print "Q", q
        c = u"UPDATE test_instances set plop=''  WHERE is_master='1' and id=''"
        self.assert_(q.strip() == c.strip())




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_db_mysql
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
try:
    from shinken.db_mysql import DBMysql
except ImportError:
    # Oups this server do not have mysql installed, skip this test
    DBMysql = None


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def create_db(self):
        self.db = DBMysql(host='localhost', user='root', password='root', database='merlin', character_set='utf8')

    def test_connect_database(self):
        if not DBMysql:
            return
        self.create_db()
        try:
            self.db.connect_database()
        except Exception:  # arg, no database here? sic!
            pass

    def test_execute_query(self):
        if not DBMysql:
            return
        self.create_db()
        try:
            self.db.connect_database()
            q = "DELETE FROM service WHERE instance_id = '0'"
            self.db.execute_query(q)
        except Exception:
            pass



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_define_with_space
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestDefineWithSpaces(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_define_with_space.cfg')

    # We got a problem with define    host for example, the type read was "" and not host
    def testdefine_with_spaces(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        self.assert_(host is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_definition_order
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestDefinitionOrder(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_definition_order.cfg')

    def test_definition_order(self):
        print "Get the hosts and services"
        now = time.time()
        svc_specific = self.sched.services.find_srv_by_name_and_hostname("test_host_specific", "ZE-SERVICE")
        svc_generic  = self.sched.services.find_srv_by_name_and_hostname("test_host_generic", "ZE-SERVICE")
        
        self.assert_(svc_specific is not None)
        self.assert_(svc_generic is not None)

        print svc_generic.check_command.command.command_name
        self.assert_(svc_generic.check_command.command.command_name == 'general')
        
        print svc_specific.check_command.command.command_name
        self.assert_(svc_specific.check_command.command.command_name == 'specific')
        
        

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dependencies
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test host- and service-downtimes.
#

from shinken_test import *
sys.setcheckinterval(10000)


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_dependencies.cfg')

    def test_service_dependencies(self):
        self.print_header()
        now = time.time()
        test_host_0 = self.sched.hosts.find_by_name("test_host_0")
        test_host_1 = self.sched.hosts.find_by_name("test_host_1")
        test_host_0.checks_in_progress = []
        test_host_1.checks_in_progress = []
        test_host_0.act_depend_of = []  # ignore the router
        test_host_1.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore other routers
        test_host_0_test_ok_0 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        test_host_0_test_ok_1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_1")
        test_host_1_test_ok_0 = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_ok_0")
        test_host_1_test_ok_1 = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_ok_1")
        # the most important: test_ok_0 is in the chk_depend_of-list of test_ok_1
        self.assert_(test_host_0_test_ok_0 in [x[0] for x in test_host_0_test_ok_1.chk_depend_of])
        self.assert_(test_host_1_test_ok_0 in [x[0] for x in test_host_1_test_ok_1.chk_depend_of])

        # and not vice versa
        self.assert_(test_host_0_test_ok_1 not in [x[0] for x in test_host_0_test_ok_0.chk_depend_of])
        self.assert_(test_host_1_test_ok_1 not in [x[0] for x in test_host_1_test_ok_0.chk_depend_of])

        # test_ok_0 is also in the act_depend_of-list of test_ok_1
        self.assert_(test_host_0_test_ok_0 in [x[0] for x in test_host_0_test_ok_1.chk_depend_of])
        self.assert_(test_host_1_test_ok_0 in [x[0] for x in test_host_1_test_ok_1.chk_depend_of])

        # check the criteria
        # execution_failure_criteria      u,c
        # notification_failure_criteria   u,c,w
        self.assert_([['u', 'c']] == [x[1] for x in test_host_0_test_ok_1.chk_depend_of if x[0] is test_host_0_test_ok_0])
        self.assert_([['u', 'c']] == [x[1] for x in test_host_1_test_ok_1.chk_depend_of if x[0] is test_host_1_test_ok_0])
        self.assert_([['u', 'c', 'w']] == [x[1] for x in test_host_0_test_ok_1.act_depend_of if x[0] is test_host_0_test_ok_0])
        self.assert_([['u', 'c', 'w']] == [x[1] for x in test_host_1_test_ok_1.act_depend_of if x[0] is test_host_1_test_ok_0])

        # and every service has the host in it's act_depend_of-list
        self.assert_(test_host_0 in [x[0] for x in test_host_0_test_ok_0.act_depend_of])
        self.assert_(test_host_0 in [x[0] for x in test_host_0_test_ok_1.act_depend_of])
        self.assert_(test_host_1 in [x[0] for x in test_host_1_test_ok_0.act_depend_of])
        self.assert_(test_host_1 in [x[0] for x in test_host_1_test_ok_1.act_depend_of])

        # and final count the masters
        self.assert_(len(test_host_0_test_ok_0.chk_depend_of) == 0)
        self.assert_(len(test_host_0_test_ok_1.chk_depend_of) == 1)
        self.assert_(len(test_host_1_test_ok_0.chk_depend_of) == 0)
        self.assert_(len(test_host_1_test_ok_1.chk_depend_of) == 1)
        self.assert_(len(test_host_0_test_ok_0.act_depend_of) == 1)  # same, plus the host
        self.assert_(len(test_host_0_test_ok_1.act_depend_of) == 2)
        self.assert_(len(test_host_1_test_ok_0.act_depend_of) == 1)
        self.assert_(len(test_host_1_test_ok_1.act_depend_of) == 2)

    def test_host_dependencies(self):
        self.print_header()
        now = time.time()
        #
        #   A  <------  B  <--
        #   ^                 \---  C
        #   |---------------------
        #
        host_A = self.sched.hosts.find_by_name("test_host_A")
        host_B = self.sched.hosts.find_by_name("test_host_B")
        host_C = self.sched.hosts.find_by_name("test_host_C")
        host_D = self.sched.hosts.find_by_name("test_host_D")

        # the most important: test_ok_0 is in the chk_depend_of-list of test_ok_1
        #self.assert_(host_A in [x[0] for x in host_C.chk_depend_of])
        print host_C.act_depend_of
        print host_C.chk_depend_of
        print host_C.chk_depend_of_me
        self.assert_(host_B in [x[0] for x in host_C.act_depend_of])
        self.assert_(host_A in [x[0] for x in host_C.act_depend_of])
        self.assert_(host_A in [x[0] for x in host_B.act_depend_of])
        self.assert_(host_A.act_depend_of == [])
        self.assert_(host_B in [x[0] for x in host_C.chk_depend_of])
        self.assert_(host_A in [x[0] for x in host_C.chk_depend_of])
        self.assert_(host_A in [x[0] for x in host_B.chk_depend_of])
        self.assert_(host_A.act_depend_of == [])
        self.assert_(host_B in [x[0] for x in host_A.act_depend_of_me])
        self.assert_(host_C in [x[0] for x in host_A.act_depend_of_me])
        self.assert_(host_C in [x[0] for x in host_B.act_depend_of_me])
        #self.assert_(host_C.act_depend_of_me == []) # D in here
        self.assert_(host_B in [x[0] for x in host_A.chk_depend_of_me])
        self.assert_(host_C in [x[0] for x in host_A.chk_depend_of_me])
        self.assert_(host_C in [x[0] for x in host_B.chk_depend_of_me])
        self.assert_(host_D in [x[0] for x in host_C.chk_depend_of_me])

        # check the notification/execution criteria
        self.assert_([['d', 'u']] == [x[1] for x in host_C.act_depend_of if x[0] is host_B])
        self.assert_([['d']] == [x[1] for x in host_C.chk_depend_of if x[0] is host_B])
        self.assert_([['d', 'u']] == [x[1] for x in host_C.act_depend_of if x[0] is host_A])
        self.assert_([['d']] == [x[1] for x in host_C.chk_depend_of if x[0] is host_A])
        self.assert_([['d', 'u']] == [x[1] for x in host_B.act_depend_of if x[0] is host_A])
        self.assert_([['n']] == [x[1] for x in host_B.chk_depend_of if x[0] is host_A])

    def test_host_inherits_dependencies(self):
        self.print_header()
        now = time.time()
        #
        #   A  <------  B  <--
        #   ^                 \---  C   <--  D
        #   |---------------------
        #
        host_A = self.sched.hosts.find_by_name("test_host_A")
        host_B = self.sched.hosts.find_by_name("test_host_B")
        host_C = self.sched.hosts.find_by_name("test_host_C")
        host_D = self.sched.hosts.find_by_name("test_host_D")

        print "A depends on", ",".join([x[0].get_name() for x in host_A.chk_depend_of])
        print "B depends on", ",".join([x[0].get_name() for x in host_B.chk_depend_of])
        print "C depends on", ",".join([x[0].get_name() for x in host_C.chk_depend_of])
        print "D depends on", ",".join([x[0].get_name() for x in host_D.chk_depend_of])

        self.assert_(host_A.act_depend_of == [])
        self.assert_(host_A in [x[0] for x in host_B.act_depend_of])
        self.assert_(host_A in [x[0] for x in host_C.act_depend_of])
        self.assert_(host_B in [x[0] for x in host_C.act_depend_of])
        self.assert_(host_C in [x[0] for x in host_D.act_depend_of])

        # and through inherits_parent....
        #self.assert_(host_A in [x[0] for x in host_D.act_depend_of])
        #self.assert_(host_B in [x[0] for x in host_D.act_depend_of])


    # Now test a in service service_dep definition. More easierto use than create a full new object
    def test_in_servicedef_dep(self):
        svc_parent = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_parent_svc")
        svc_son = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_son_svc")

        print "DumP", self.conf.servicedependencies

        # the most important: test_parent is in the chk_depend_of-list of test_son
        print "Dep: ", svc_son.act_depend_of
        self.assert_([['u', 'c', 'w']] == [x[1] for x in svc_son.act_depend_of if x[0] is svc_parent])

    def test_host_non_inherits_dependencies(self):
        #
        #   A  <------  B  <--
        #   ^                 \NOT/---  C   <--  D
        #   |---------------------
        #
        host_A = self.sched.hosts.find_by_name("test_host_A")
        host_B = self.sched.hosts.find_by_name("test_host_B")
        host_C = self.sched.hosts.find_by_name("test_host_C")
        host_D = self.sched.hosts.find_by_name("test_host_D")
        host_E = self.sched.hosts.find_by_name("test_host_E")

        print "A depends on", ",".join([x[0].get_name() for x in host_A.chk_depend_of])
        print "B depends on", ",".join([x[0].get_name() for x in host_B.chk_depend_of])
        print "C depends on", ",".join([x[0].get_name() for x in host_C.chk_depend_of])
        print "D depends on", ",".join([x[0].get_name() for x in host_D.chk_depend_of])
        print "E depends on", ",".join([x[0].get_name() for x in host_E.chk_depend_of])

        host_C.state = 'DOWN'
        print "D state", host_D.state
        print "E dep", host_E.chk_depend_of
        print "I raise?", host_D.do_i_raise_dependency('d', inherit_parents=False)
        # If I ask D for dep, he should raise Nothing if we do not want parents.
        self.assert_(host_D.do_i_raise_dependency('d', inherit_parents=False) == False)
        # But he should raise a problem (C here) of we ask for its parents
        self.assert_(host_D.do_i_raise_dependency('d', inherit_parents=True) == True)


    def test_check_dependencies(self):
        self.print_header()
        now = time.time()
        test_host_0 = self.sched.hosts.find_by_name("test_host_0")
        test_host_0.checks_in_progress = []
        test_host_0.act_depend_of = []  # ignore the router

        test_host_0_test_ok_0 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        # The pending state is always different. Let assume it OK
        test_host_0.state = 'OK'

        # Create a fake check already done for service
        cs = Check('waitconsume', 'foo', test_host_0_test_ok_0, now)
        cs.exit_status = 2
        cs.output = 'BAD'
        cs.check_time = now
        cs.execution_time = now

        # Create a fake check for the host (so that it is in checking)
        ch = Check('scheduled', 'foo', test_host_0, now)
        test_host_0.checks_in_progress.append(ch)


        # This service should have his host dep
        assert(len(test_host_0_test_ok_0.act_depend_of) != 0)

        # Ok we are at attempt 0 (we should have a 1 with the OK state, but nervermind)
        assert(test_host_0.attempt == 0)

        # Add the check to sched queue
        self.sched.add(cs)
        self.sched.add(ch)
        # This should raise a log entry and schedule the host check now
        self.sched.consume_results()

        # Take the host check. The one generated by dependency not the usual one
        c_dep = test_host_0.actions[1]
        assert(c_dep.dependency_check)

        # Hack it to consider it as down and returning critical state
        c_dep.status = 'waitconsume'
        c_dep.exit_status = 2
        c_dep.output = 'BAD'
        c_dep.check_time = now
        c_dep.execution_time = now

        # Add and process result
        self.sched.add(c_dep)
        self.sched.consume_results()

        # We should not have a new attempt as it was a depency check.
        assert(test_host_0.attempt == 0)






if __name__ == '__main__':
    import cProfile
    command = """unittest.main()"""
    unittest.main()
    #cProfile.runctx( command, globals(), locals(), filename="Thruk.profile" )

########NEW FILE########
__FILENAME__ = test_disable_active_checks
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestDisableActiveChecks(ShinkenTest):

    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_disable_active_checks.cfg')


    # We try to disable the actie checks and see if it's really done
    # with a dummy check, so we need to get the same state and output
    def test_disable_active_checks(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")

        print "Checks in progress", host.checks_in_progress
        c = host.checks_in_progress.pop()
        print c.__dict__
        print c.status

        self.scheduler_loop(1, [[host, 0, 'I set this host UP | value1=1 value2=2']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        last_output = host.output

        host.schedule()
        self.sched.external_command.DISABLE_HOST_CHECK(host)

        c = host.checks_in_progress.pop()
        print c.__dict__
        print c.status
        self.assert_(c.status == 'waitconsume')
        self.scheduler_loop(2, [])

        print host.state
        print host.output
        self.assert_(host.output == last_output)

        print len(host.checks_in_progress)
        print host.in_checking
        self.assert_(host.in_checking == False)




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_discovery_def
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestDiscoveryConf(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_discovery_def.cfg')

    def test_look_for_discorule(self):
        genhttp = self.sched.conf.discoveryrules.find_by_name('GenHttp')
        self.assert_(genhttp != None)
        self.assert_(genhttp.creation_type == 'service')
        self.assert_(genhttp.matches['openports'] == '80,443')
        self.assert_(genhttp.matches['os'] == 'windows')

        key = 'osversion'
        value = '2003'
        # Should not match this
        self.assert_(genhttp.is_matching(key, value) == False)
        # But should match this one
        key = 'openports'
        value = '80'
        self.assert_(genhttp.is_matching(key, value) == True)

        # Low look for a list of matchings
        l = {'openports': '80', 'os': 'windows'}
        # should match this
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # Match this one too
        l = {'openports': '80', 'os': 'windows', 'super': 'man'}
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # But not this one
        l = {'openports': '80'}
        self.assert_(genhttp.is_matching_disco_datas(l) == False)

        # Now search the NOT rule
        genhttpnowin = self.sched.conf.discoveryrules.find_by_name('GenHttpNotWindows')

        # Should manage this
        l = {'openports': '80', 'os': 'linux'}
        self.assert_(genhttpnowin.is_matching_disco_datas(l) == True)

        # But NOT this one
        l = {'openports': '80', 'os': 'windows'}
        print "Should NOT match"
        self.assert_(genhttpnowin.is_matching_disco_datas(l) == False)

        # Now look for strict rule application
        genhttpstrict = self.sched.conf.discoveryrules.find_by_name('GenHttpStrict')
        self.assert_(genhttpstrict is not None)
        key = 'openports'
        value = '80,443'
        self.assert_(genhttpstrict.is_matching(key, value) == True)

        # But NOT this one
        key = 'openports'
        value = '800'
        self.assert_(genhttpstrict.is_matching(key, value) == False)


    # Look for good definition and call of a discoveryrun
    def test_look_for_discorun(self):
        nmap = self.sched.conf.discoveryruns.find_by_name('nmap')
        self.assert_(nmap != None)
        nmapcmd = self.sched.conf.commands.find_by_name('nmap_runner')
        self.assert_(nmapcmd != None)
        self.assert_(nmap.discoveryrun_command != None)
        # Launch it
        nmap.launch()
        for i in xrange(1, 5):
            nmap.check_finished()
            if nmap.is_finished():
                break
            time.sleep(1)
        print "Exit status", nmap.current_launch.exit_status
        print "Output", nmap.current_launch.output
        print "LongOutput", nmap.current_launch.long_output


    def test_look_for_host_discorule(self):
        genhttp = self.sched.conf.discoveryrules.find_by_name('GenHttpHost')
        self.assert_(genhttp != None)
        self.assert_(genhttp.creation_type == 'host')
        self.assert_(genhttp.matches['openports'] == '^80$')

        key = 'osversion'
        value = '2003'
        # Should not match this
        self.assert_(genhttp.is_matching(key, value) == False)
        # But should match this one
        key = 'openports'
        value = '80'
        self.assert_(genhttp.is_matching(key, value) == True)

        # Low look for a list of matchings
        l = {'openports': '80', 'os': 'windows'}
        # should match this
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # Match this one too
        l = {'openports': '80', 'os': 'windows', 'super': 'man'}
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # And this last one
        l = {'openports': '80'}
        self.assert_(genhttp.is_matching_disco_datas(l) == True)

        print "Writing properties"
        print genhttp.writing_properties




    def test_look_for_host_discorule_and_delete(self):
        genhttp = self.sched.conf.discoveryrules.find_by_name('GenHttpHostRemoveLinux')
        self.assert_(genhttp != None)
        self.assert_(genhttp.creation_type == 'host')
        self.assert_(genhttp.matches['openports'] == '^80$')

        key = 'os'
        value = 'linux'

        # Should not match this
        self.assert_(genhttp.is_matching(key, value) == False)
        
        # But should match this one
        key = 'openports'
        value = '80'
        self.assert_(genhttp.is_matching(key, value) == True)

        # Low look for a list of matchings
        l = {'openports': '80', 'os': 'linux'}
        # should match this
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # Match this one too
        l = {'openports': '80', 'os': 'linux', 'super': 'man'}
        self.assert_(genhttp.is_matching_disco_datas(l) == True)
        # And this last one
        l = {'openports': '80'}
        self.assert_(genhttp.is_matching_disco_datas(l) == True)

        print "Writing properties"
        print genhttp.writing_properties
        
        


    def test_discorun_matches(self):
        linux = self.sched.conf.discoveryruns.find_by_name('linux')
        self.assert_(linux != None)
        print linux.__dict__
        self.assert_(linux.matches == {u'osvendor': u'linux'})

        key = 'osvendor'
        value = 'microsoft'
        # Should not match this
        self.assert_(linux.is_matching(key, value) == False)

        key = 'osvendor'
        value = 'linux'
        # Should match this
        self.assert_(linux.is_matching(key, value) == True)

        # Low look for a list of matchings
        l = {'openports': '80', 'osvendor': 'linux'}
        # should match this
        self.assert_(linux.is_matching_disco_datas(l) == True)


    


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dispatcher
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class GoodArbiter(ArbiterLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def do_not_run(self):
        pass


class GoodScheduler(SchedulerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def put_conf(self, conf):
        return True


class BadScheduler(SchedulerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()

    def have_conf(self, i):
        return False


class GoodPoller(PollerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadPoller(PollerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodReactionner(ReactionnerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadReactionner(ReactionnerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodBroker(BrokerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadBroker(BrokerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class TestDispatcher(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_dispatcher.cfg')

    def test_simple_dispatch(self):
        for r in self.conf.realms:
            print r.get_name()
        r = self.conf.realms.find_by_name('All')
        print "The dispatcher", self.dispatcher
        # dummy for the arbiter
        for a in self.conf.arbiters:
            a.__class__ = GoodArbiter
        elts_types = ['schedulers', 'pollers', 'reactionners', 'brokers', 'receivers']
        for t in elts_types:
            lst = getattr(self.conf, t)
            for s in lst:
                print "TAG s", s
                s.realm = r

        print "Preparing schedulers"
        scheduler1 = self.conf.schedulers.find_by_name('scheduler-all-1')
        self.assert_(scheduler1 is not None)
        scheduler1.__class__ = GoodScheduler
        scheduler2 = self.conf.schedulers.find_by_name('scheduler-all-2')
        self.assert_(scheduler2 is not None)
        scheduler2.__class__ = BadScheduler

        print "Preparing pollers"
        poller1 = self.conf.pollers.find_by_name('poller-all-1')
        self.assert_(poller1 is not None)
        poller1.__class__ = GoodPoller
        poller2 = self.conf.pollers.find_by_name('poller-all-2')
        self.assert_(poller2 is not None)
        poller2.__class__ = BadPoller

        print "Preparing reactionners"
        reactionner1 = self.conf.reactionners.find_by_name('reactionner-all-1')
        self.assert_(reactionner1 is not None)
        reactionner1.__class__ = GoodReactionner
        reactionner2 = self.conf.reactionners.find_by_name('reactionner-all-2')
        self.assert_(reactionner2 is not None)
        reactionner2.__class__ = BadReactionner

        print "Preparing brokers"
        broker1 = self.conf.brokers.find_by_name('broker-all-1')
        self.assert_(broker1 is not None)
        broker1.__class__ = GoodBroker
        broker2 = self.conf.brokers.find_by_name('broker-all-2')
        self.assert_(broker2 is not None)
        broker2.__class__ = BadBroker

        # Ping all elements. Should have 1 as OK, 2 as
        # one bad attempt (3 max)
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 1)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 1)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 1)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 1)
        self.assert_(broker2.reachable == False)
        ### Now add another attempt, still alive, but attemp=2/3

        print "CheckAlive " * 10
        # We reset check time for the test
        elts = [scheduler1, scheduler2, poller1, poller2, broker1, broker2, reactionner1, reactionner2]
        for i in elts:
            i.last_check = 0.0

        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 2)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 2)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 2)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 2)
        self.assert_(broker2.reachable == False)

        ### Now we get BAD, We go DEAD for N2!
        print "CheckAlive " * 10
        # We reset check time for the test
        elts = [scheduler1, scheduler2, poller1, poller2, broker1, broker2, reactionner1, reactionner2]
        for i in elts:
            i.last_check = 0.0
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == False)
        self.assert_(scheduler2.attempt == 3)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == False)
        self.assert_(poller2.attempt == 3)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == False)
        self.assert_(reactionner2.attempt == 3)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == False)
        self.assert_(broker2.attempt == 3)
        self.assert_(broker2.reachable == False)

        # Now we check how we should dispatch confs
        self.dispatcher.check_dispatch()
        # the conf should not be in a good shape
        self.assert_(self.dispatcher.dispatch_ok == False)

        # Now we really dispatch them!
        self.dispatcher.dispatch()
        self.assert_(self.any_log_match('Dispatch OK of conf in scheduler scheduler-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to reactionner reactionner-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to poller poller-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to broker broker-all-1'))
        self.clear_logs()

        # And look if we really dispatch conf as we should
        for r in self.conf.realms:
            for cfg in r.confs.values():
                self.assert_(cfg.is_assigned == True)
                self.assert_(cfg.assigned_to == scheduler1)


class TestDispatcherMultiBroker(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_dispatcher_multibrokers.cfg')

    def test_simple_dispatch(self):
        print "The dispatcher", self.dispatcher
        # dummy for the arbiter
        for a in self.conf.arbiters:
            a.__class__ = GoodArbiter
        print "Preparing schedulers"
        scheduler1 = self.conf.schedulers.find_by_name('scheduler-all-1')
        self.assert_(scheduler1 is not None)
        scheduler1.__class__ = GoodScheduler
        scheduler2 = self.conf.schedulers.find_by_name('scheduler-all-2')
        self.assert_(scheduler2 is not None)
        scheduler2.__class__ = GoodScheduler

        print "Preparing pollers"
        poller1 = self.conf.pollers.find_by_name('poller-all-1')
        self.assert_(poller1 is not None)
        poller1.__class__ = GoodPoller
        poller2 = self.conf.pollers.find_by_name('poller-all-2')
        self.assert_(poller2 is not None)
        poller2.__class__ = BadPoller

        print "Preparing reactionners"
        reactionner1 = self.conf.reactionners.find_by_name('reactionner-all-1')
        self.assert_(reactionner1 is not None)
        reactionner1.__class__ = GoodReactionner
        reactionner2 = self.conf.reactionners.find_by_name('reactionner-all-2')
        self.assert_(reactionner2 is not None)
        reactionner2.__class__ = BadReactionner

        print "Preparing brokers"
        broker1 = self.conf.brokers.find_by_name('broker-all-1')
        self.assert_(broker1 is not None)
        broker1.__class__ = GoodBroker
        broker2 = self.conf.brokers.find_by_name('broker-all-2')
        self.assert_(broker2 is not None)
        broker2.__class__ = GoodBroker

        # Ping all elements. Should have 1 as OK, 2 as
        # one bad attempt (3 max)
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 0)
        self.assert_(scheduler2.reachable == True)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 1)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 1)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 0)
        self.assert_(broker2.reachable == True)

        ### Now add another attempt, still alive, but attemp=2/3
        print "CheckAlive " * 10
        # We reset check time for the test
        elts = [scheduler1, scheduler2, poller1, poller2, broker1, broker2, reactionner1, reactionner2]
        for i in elts:
            i.last_check = 0.0
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 0)
        self.assert_(scheduler2.reachable == True)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 2)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 2)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 0)
        self.assert_(broker2.reachable == True)

        ### Now we get BAD, We go DEAD for N2!
        print "CheckAlive " * 10
        # We reset check time for the test
        elts = [scheduler1, scheduler2, poller1, poller2, broker1, broker2, reactionner1, reactionner2]
        for i in elts:
            i.last_check = 0.0
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 0)
        self.assert_(scheduler2.reachable == True)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == False)
        self.assert_(poller2.attempt == 3)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == False)
        self.assert_(reactionner2.attempt == 3)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 0)
        self.assert_(broker2.reachable == True)

        # Now we check how we should dispatch confs
        self.dispatcher.check_dispatch()
        # the conf should not be in a good shape
        self.assert_(self.dispatcher.dispatch_ok == False)

        # Now we really dispatch them!
        self.dispatcher.dispatch()
        self.assert_(self.any_log_match('Dispatch OK of conf in scheduler scheduler-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to reactionner reactionner-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to poller poller-all-1'))

        self.assert_(self.any_log_match('Dispatch OK of configuration 1 to broker broker-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to broker broker-all-2'))
        self.clear_logs()


        # And look if we really dispatch conf as we should
        for r in self.conf.realms:
            for cfg in r.confs.values():
                self.assert_(cfg.is_assigned == True)
                self.assert_(cfg.assigned_to in [scheduler1, scheduler2])




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dot_virg_in_command
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_dot_virg_in_command.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        print svc.event_handler.args
        self.assert_('sudo -s pkill toto ; cd /my/path && ./toto' in svc.event_handler.args)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_downtimes
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test host- and service-downtimes.
#

from shinken_test import *

#time.time = original_time_time
#time.sleep = original_time_sleep

class TestDowntime(ShinkenTest):

    def test_schedule_fixed_svc_downtime(self):
        self.print_header()
        # schedule a 2-minute downtime
        # downtime must be active
        # consume a good result, sleep for a minute
        # downtime must be active
        # consume a bad result
        # downtime must be active
        # no notification must be found in broks
        duration = 600
        now = time.time()
        # downtime valid for the next 2 minutes
        cmd = "[%lu] SCHEDULE_SVC_DOWNTIME;test_host_0;test_ok_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        time.sleep(20)
        self.scheduler_loop(1, [[svc, 0, 'OK']])

        print "downtime was scheduled. check its activity and the comment"
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc.downtimes[0].fixed)
        self.assert_(svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(svc.comments) == 1)
        self.assert_(svc.comments[0] in self.sched.comments.values())
        self.assert_(svc.downtimes[0].comment_id == svc.comments[0].id)

        self.scheduler_loop(1, [[svc, 0, 'OK']])

        print "good check was launched, downtime must be active"
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc.in_scheduled_downtime)
        self.assert_(svc.downtimes[0].fixed)
        self.assert_(svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)

        self.scheduler_loop(1, [[svc, 2, 'BAD']])

        print "bad check was launched (SOFT;1), downtime must be active"
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc.in_scheduled_downtime)
        self.assert_(svc.downtimes[0].fixed)
        self.assert_(svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)

        # now the state changes to hard
        self.scheduler_loop(1, [[svc, 2, 'BAD']])

        print "bad check was launched (HARD;2), downtime must be active"
        print svc.downtimes[0]
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc.in_scheduled_downtime)
        self.assert_(svc.downtimes[0].fixed)
        self.assert_(svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)

        scheduled_downtime_depth = svc.scheduled_downtime_depth
        cmd = "[%lu] DEL_SVC_DOWNTIME;%d" % (now, svc.downtimes[0].id)
        self.sched.run_external_command(cmd)
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(not svc.in_scheduled_downtime)
        self.assert_(svc.scheduled_downtime_depth < scheduled_downtime_depth)
        self.assert_(svc.downtimes[0].fixed)
        self.assert_(not svc.downtimes[0].is_in_effect)
        self.assert_(svc.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(svc.comments) == 1)

        # now a notification must be sent
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        # downtimes must have been deleted now
        self.assert_(len(self.sched.downtimes) == 0)
        self.assert_(len(svc.downtimes) == 0)
        self.assert_(len(self.sched.comments) == 0)
        self.assert_(len(svc.comments) == 0)

    def test_schedule_flexible_svc_downtime(self):
        self.print_header()
        #----------------------------------------------------------------
        # schedule a flexible downtime of 3 minutes for the host
        #----------------------------------------------------------------
        duration = 180
        now = time.time()
        cmd = "[%lu] SCHEDULE_SVC_DOWNTIME;test_host_0;test_ok_0;%d;%d;0;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        time.sleep(20)
        #----------------------------------------------------------------
        # check if a downtime object exists (scheduler and service)
        # check if the downtime is still inactive
        #----------------------------------------------------------------
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(not svc.downtimes[0].fixed)
        self.assert_(not svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(svc.comments) == 1)
        self.assert_(svc.comments[0] in self.sched.comments.values())
        self.assert_(svc.downtimes[0].comment_id == svc.comments[0].id)
        #----------------------------------------------------------------
        # run the service and return an OK status
        # check if the downtime is still inactive
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(not svc.in_scheduled_downtime)
        self.assert_(not svc.downtimes[0].fixed)
        self.assert_(not svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)
        time.sleep(61)
        #----------------------------------------------------------------
        # run the service twice to get a soft critical status
        # check if the downtime is still inactive
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(not svc.in_scheduled_downtime)
        self.assert_(not svc.downtimes[0].fixed)
        self.assert_(not svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)
        time.sleep(61)
        #----------------------------------------------------------------
        # run the service again to get a hard critical status
        # check if the downtime is active now
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(svc.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc.in_scheduled_downtime)
        self.assert_(not svc.downtimes[0].fixed)
        self.assert_(svc.downtimes[0].is_in_effect)
        self.assert_(not svc.downtimes[0].can_be_deleted)
        #----------------------------------------------------------------
        # cancel the downtime
        # check if the downtime is inactive now and can be deleted
        #----------------------------------------------------------------
        scheduled_downtime_depth = svc.scheduled_downtime_depth
        cmd = "[%lu] DEL_SVC_DOWNTIME;%d" % (now, svc.downtimes[0].id)
        self.sched.run_external_command(cmd)
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 1)
        self.assert_(not svc.in_scheduled_downtime)
        self.assert_(svc.scheduled_downtime_depth < scheduled_downtime_depth)
        self.assert_(not svc.downtimes[0].fixed)
        self.assert_(not svc.downtimes[0].is_in_effect)
        self.assert_(svc.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(svc.comments) == 1)
        time.sleep(61)
        #----------------------------------------------------------------
        # run the service again with a critical status
        # the downtime must have disappeared
        # a notification must be sent
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']])
        self.assert_(len(self.sched.downtimes) == 0)
        self.assert_(len(svc.downtimes) == 0)
        self.assert_(len(self.sched.comments) == 0)
        self.assert_(len(svc.comments) == 0)
        self.show_logs()
        self.show_actions()

    def test_schedule_fixed_host_downtime(self):
        self.print_header()
        # schedule a 2-minute downtime
        # downtime must be active
        # consume a good result, sleep for a minute
        # downtime must be active
        # consume a bad result
        # downtime must be active
        # no notification must be found in broks
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        print "test_schedule_fixed_host_downtime initialized"
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 0)
        self.assert_(self.count_actions() == 0)
        #----------------------------------------------------------------
        # schedule a downtime of 10 minutes for the host
        #----------------------------------------------------------------
        duration = 600
        now = time.time()
        # fixed downtime valid for the next 10 minutes
        cmd = "[%lu] SCHEDULE_HOST_DOWNTIME;test_host_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)

        self.sched.run_external_command(cmd)
        self.sched.update_downtimes_and_comments()
        print "Launch scheduler loop"
        self.scheduler_loop(1, [], do_sleep=False)  # push the downtime notification
        self.show_actions()
        print "Launch worker loop"
        #self.worker_loop()
        self.show_actions()
        print "After both launchs"
        time.sleep(20)
        #----------------------------------------------------------------
        # check if a downtime object exists (scheduler and host)
        #----------------------------------------------------------------
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(host.downtimes) == 1)
        self.assert_(host.downtimes[0] in self.sched.downtimes.values())
        self.assert_(host.downtimes[0].fixed)
        self.assert_(host.downtimes[0].is_in_effect)
        self.assert_(not host.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(host.comments) == 1)
        self.assert_(host.comments[0] in self.sched.comments.values())
        self.assert_(host.downtimes[0].comment_id == host.comments[0].id)
        self.show_logs()
        self.show_actions()
        print "*****************************************************************************************************************************************************************Log matching:", self.get_log_match("STARTED*")
        self.show_actions()
        self.assert_(self.count_logs() == 2)    # start downt, notif downt
        print self.count_actions() # notif" down is removed, so only donwtime
        self.assert_(self.count_actions() == 1)
        self.scheduler_loop(1, [], do_sleep=False)
        self.show_logs()
        self.show_actions()
        
        self.assert_(self.count_logs() == 2)    # start downt, notif downt
        self.clear_logs()
        self.clear_actions()
        #----------------------------------------------------------------
        # send the host to a hard DOWN state
        # check log messages, (no) notifications and eventhandlers
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[host, 2, 'DOWN']])
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)    # soft1, evt1
        self.assert_(self.count_actions() == 1)  # evt1
        self.clear_logs()
        #--
        self.scheduler_loop(1, [[host, 2, 'DOWN']])
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)    # soft2, evt2
        self.assert_(self.count_actions() == 1)  # evt2
        self.clear_logs()
        #--
        self.scheduler_loop(1, [[host, 2, 'DOWN']])
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)    # hard3, evt3
        self.assert_(self.count_actions() == 2)  # evt3, notif"
        self.clear_logs()
        #--
        # we have a notification, but this is blocked. it will stay in
        # the actions queue because we have a notification_interval.
        # it's called notif" because it is a master notification
        print "DBG: host", host.state, host.state_type
        self.scheduler_loop(1, [[host, 2, 'DOWN']], do_sleep=True)
        print "DBG2: host", host.state, host.state_type
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 0)     #
        self.assert_(self.count_actions() == 1)  # notif"
        self.clear_logs()
        #----------------------------------------------------------------
        # the host comes UP again
        # check log messages, (no) notifications and eventhandlers
        # a (recovery) notification was created, but has been blocked.
        # should be a zombie, but was deteleted
        #----------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True)
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 2)    # hard3ok, evtok
        self.assert_(self.count_actions() == 1)  # evtok, notif"
        self.clear_logs()
        self.clear_actions()

    def test_schedule_fixed_host_downtime_with_service(self):
        self.print_header()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        host.notification_interval = 0
        svc.notification_interval = 0
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 0)
        self.assert_(self.count_actions() == 0)
        #----------------------------------------------------------------
        # schedule a downtime of 10 minutes for the host
        #----------------------------------------------------------------
        duration = 600
        now = time.time()
        cmd = "[%lu] SCHEDULE_HOST_DOWNTIME;test_host_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)
        self.sched.update_downtimes_and_comments()
        self.scheduler_loop(1, [], do_sleep=False)  # push the downtime notification
        #self.worker_loop() # push the downtime notification
        time.sleep(10)
        #----------------------------------------------------------------
        # check if a downtime object exists (scheduler and host)
        # check the start downtime notification
        #----------------------------------------------------------------
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(host.downtimes) == 1)
        self.assert_(host.in_scheduled_downtime)
        self.assert_(host.downtimes[0] in self.sched.downtimes.values())
        self.assert_(host.downtimes[0].fixed)
        self.assert_(host.downtimes[0].is_in_effect)
        self.assert_(not host.downtimes[0].can_be_deleted)
        self.assert_(len(self.sched.comments) == 1)
        self.assert_(len(host.comments) == 1)
        self.assert_(host.comments[0] in self.sched.comments.values())
        self.assert_(host.downtimes[0].comment_id == host.comments[0].id)
        self.scheduler_loop(4, [[host, 2, 'DOWN']], do_sleep=True)
        self.show_logs()
        self.show_actions()
        self.assert_(self.count_logs() == 8)    # start downt, notif downt, soft1, evt1, soft 2, evt2, hard 3, evt3
        self.clear_logs()
        self.clear_actions()
        #----------------------------------------------------------------
        # now the service becomes critical
        # check that the host has a downtime, _not_ the service
        # check logs, (no) notifications and eventhandlers
        #----------------------------------------------------------------
        print "now the service goes critical"
        self.scheduler_loop(4, [[svc, 2, 'CRITICAL']], do_sleep=True)
        self.assert_(len(self.sched.downtimes) == 1)
        self.assert_(len(svc.downtimes) == 0)
        self.assert_(not svc.in_scheduled_downtime)
        self.assert_(svc.host.in_scheduled_downtime)
        self.show_logs()
        self.show_actions()
        # soft 1, evt1, hard 2, evt2
        self.assert_(self.count_logs() == 4)
        self.clear_logs()
        self.clear_actions()
        #----------------------------------------------------------------
        # the host comes UP again
        # check log messages, (no) notifications and eventhandlers
        #----------------------------------------------------------------
        print "now the host comes up"
        self.scheduler_loop(2, [[host, 0, 'UP']], do_sleep=True)
        self.show_logs()
        self.show_actions()
        # hard 3, eventhandler
        self.assert_(self.count_logs() == 2)    # up, evt
        self.clear_logs()
        self.clear_actions()
        #----------------------------------------------------------------
        # the service becomes OK again
        # check log messages, (no) notifications and eventhandlers
        # check if the stop downtime notification is the only one
        #----------------------------------------------------------------
        self.scheduler_loop(2, [[host, 0, 'UP']], do_sleep=True)
        self.assert_(len(self.sched.downtimes) == 0)
        self.assert_(len(host.downtimes) == 0)
        self.assert_(not host.in_scheduled_downtime)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'HOST DOWNTIME ALERT.*STOPPED'))
        self.clear_logs()
        self.clear_actions()
        # todo
        # checks return 1=warn. this means normally up
        # set use_aggressive_host_checking which treats warn as down

        # send host into downtime
        # run service checks with result critical
        # host exits downtime
        # does the service send a notification like when it exts a svc dt?
        # check for notifications

        # host is down and in downtime. what about service eventhandlers?

    def test_notification_after_cancel_flexible_svc_downtime(self):
        # schedule flexible downtime
        # good check
        # bad check -> SOFT;1
        #  eventhandler SOFT;1
        # bad check -> HARD;2
        #  downtime alert
        #  eventhandler HARD;2
        # cancel downtime
        # bad check -> HARD;2
        #  notification critical
        #
        pass

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dummy
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_escalations
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test host- and service-downtimes.
#

from shinken_test import *

time_hacker.set_real_time()

class TestEscalations(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_escalations.cfg')

    def test_simple_escalation(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)

        tolevel2 = self.sched.conf.escalations.find_by_name('ToLevel2')
        self.assert_(tolevel2 is not None)
        self.assert_(tolevel2 in svc.escalations)
        tolevel3 = self.sched.conf.escalations.find_by_name('ToLevel3')
        self.assert_(tolevel3 is not None)
        self.assert_(tolevel3 in svc.escalations)


        for es in svc.escalations:
            print es.__dict__

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)

        # We check if we really notify the level1
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;CRITICAL;'))
        self.show_and_clear_logs()
        #self.show_and_clear_actions()
        self.show_actions()
        print svc.notifications_in_progress
        for n in svc.notifications_in_progress.values():
            print n
        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # notification_number is already sent. the next one has been scheduled
        # and is waiting for notification_interval to pass. so the current
        # number is 2
        self.assert_(svc.current_notification_number == 1)
        print "OK, level1 is notified, notif nb = 1"

        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 1 x BAD repeat -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)

        # Now we raise the notif number of 2, so we can escalade
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()
        print "cnn and cur", cnn, svc.current_notification_number
        self.assert_(svc.current_notification_number > cnn)
        cnn = svc.current_notification_number

        # One more bad, we go 3
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()

        # We go 4, still level2
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        # We go 5! we escalade to level3

        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()

        # Now we send 10 more notif, we must be still level5
        for i in range(10):
            self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
            self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
            self.show_and_clear_logs()

        # Now we recover, it will be fun because all of level{1,2,3} must be send a
        # notif
        self.scheduler_loop(2, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.show_actions()
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;OK;'))
        self.show_and_clear_logs()

    def test_time_based_escalation(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0_time")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)

        # We check if we correclty linked our escalations
        tolevel2_time = self.sched.conf.escalations.find_by_name('ToLevel2-time')
        self.assert_(tolevel2_time is not None)
        self.assert_(tolevel2_time in svc.escalations)
        tolevel3_time = self.sched.conf.escalations.find_by_name('ToLevel3-time')
        self.assert_(tolevel3_time is not None)
        self.assert_(tolevel3_time in svc.escalations)

        # Go for the running part!

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)

        # We check if we really notify the level1
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # notification_number is already sent. the next one has been scheduled
        # and is waiting for notification_interval to pass. so the current
        # number is 2
        self.assert_(svc.current_notification_number == 1)
        print "OK, level1 is notified, notif nb = 1"

        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 1 x BAD repeat -------------------------------------"

        # For the test, we hack the notif value because we do not wan to wait 1 hour!
        for n in svc.notifications_in_progress.values():
            # HOP, we say: it's already 3600 second since the last notif,
            svc.notification_interval = 3600
            # and we say that there is still 1hour since the notification creation
            # so it will say the notification time is huge, and so it will escalade
            n.creation_time = n.creation_time - 3600

        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.001)

        # Now we raise a notification time of 1hour, we escalade to level2
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        print "cnn and cur", cnn, svc.current_notification_number
        # We check that we really raise the notif number too
        self.assert_(svc.current_notification_number > cnn)
        cnn = svc.current_notification_number

        for n in svc.notifications_in_progress.values():
            # HOP, we say: it's already 3600 second since the last notif
            n.t_to_go = time.time()

        # One more bad, we say: he, it's still near 1 hour, so still level2
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()

        # Now we go for level3, so again we say: he, in fact we start one hour earlyer,
        # so the total notification duration is near 2 hour, so we will raise level3
        for n in svc.notifications_in_progress.values():
            # HOP, we say: it's already 3600 second since the last notif,
            n.t_to_go = time.time()
            n.creation_time = n.creation_time - 3600

        # One more, we bypass 7200, so now it's level3
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()


        # Now we send 10 more notif, we must be still level5
        for i in range(10):
            for n in svc.notifications_in_progress.values():
                # HOP, we say: it's already 3600 second since the last notif,
                n.t_to_go = time.time()

            self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
            self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
            self.show_and_clear_logs()

        # Now we recover, it will be fun because all of level{1,2,3} must be send a
        # recovery notif
        self.scheduler_loop(2, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.show_actions()
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;OK;'))
        self.show_and_clear_logs()

    # Here we search to know if a escalation really short the notification
    # interval if the escalation if BEFORE the next notification. For example
    # let say we notify one a day, if the escalation if at 4hour, we need
    # to notify at t=0, and get the next notification at 4h, and not 1day.
    def test_time_based_escalation_with_shorting_interval(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0_time")

        # To make tests quicker we make notifications send very quickly
        # 1 day notification interval
        svc.notification_interval = 1400

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)

        # We check that we really linked our escalations :)
        tolevel2_time = self.sched.conf.escalations.find_by_name('ToLevel2-time')
        self.assert_(tolevel2_time is not None)
        self.assert_(tolevel2_time in svc.escalations)
        tolevel3_time = self.sched.conf.escalations.find_by_name('ToLevel3-time')
        self.assert_(tolevel3_time is not None)
        self.assert_(tolevel3_time in svc.escalations)

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)

        print "  ** LEVEL1 ** " * 20
        # We check if we really notify the level1
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # notification_number is already sent. the next one has been scheduled
        # and is waiting for notification_interval to pass. so the current
        # number is 2
        self.assert_(svc.current_notification_number == 1)
        print "OK, level1 is notified, notif nb = 1"

        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 1 x BAD repeat -------------------------------------"

        # Now we go for the level2 escalation, so we will need to say: he, it's 1 hour since the begining:p
        print "*************Next", svc.notification_interval * svc.__class__.interval_length

        # first, we check if the next notification will really be near 1 hour because the escalation
        # to level2 is asking for it. If it don't, the standard was 1 day!
        for n in svc.notifications_in_progress.values():
            next = svc.get_next_notification_time(n)
            print abs(next - now)
            # Check if we find the next notification for the next hour,
            # and not for the next day like we ask before
            self.assert_(abs(next - now - 3600) < 10)

        # And we hack the notification so we can raise really the level2 escalation
        for n in svc.notifications_in_progress.values():
            n.t_to_go = time.time()
            n.creation_time -= 3600

        print "  ** LEVEL2 ** " * 20

        # We go in trouble too
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.001)

        # Now we raise the time since the begining at 1 hour, so we can escalade
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        print "Level 2 got warn, now we search for level3"
        print "cnn and cur", cnn, svc.current_notification_number
        self.assert_(svc.current_notification_number > cnn)
        cnn = svc.current_notification_number

        # Now the same thing, but for level3, so one more hour
        for n in svc.notifications_in_progress.values():
            # HOP, we say: it's already 3600 second since the last notif,
            n.t_to_go = time.time()
            n.creation_time -= 3600

        # One more bad, we say: he, it's 7200 sc of notif, so must be still level3
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()

        for n in svc.notifications_in_progress.values():
            # we say that the next notif will be right now
            # so we can raise a notif now
            n.t_to_go = time.time()

        # One more, we bypass 7200, so now it's still level3
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()


        # Now we send 10 more notif, we must be still level3
        for i in range(10):
            for n in svc.notifications_in_progress.values():
                # HOP, we say: it's already 3600 second since the last notif,
                n.t_to_go = time.time()

            self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
            self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
            self.show_and_clear_logs()

        # Ok now we get the normal stuff, we do NOT want to raise so soon a
        # notification.
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_actions()
        print svc.notifications_in_progress
        # Should be far away
        for n in svc.notifications_in_progress.values():
            print n, n.t_to_go, time.time(), n.t_to_go - time.time()
            # Should be "near" one day now, so 84000s
            self.assert_(8300 < abs(n.t_to_go - time.time()) < 85000)
        # And so no notification
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))

        # Now we recover, it will be fun because all of level{1,2,3} must be send a
        # recovery notif
        self.scheduler_loop(2, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.show_actions()
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;OK;'))
        self.show_and_clear_logs()

    def test_time_based_escalation_with_short_notif_interval(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = [] # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0_time_long_notif_interval")
        # For this specific test, notif interval will be something like 10s
        #svc.notification_interval = 0.1

        svc.checks_in_progress = []
        svc.act_depend_of = [] # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)

        # We hack the interval_length for short time, like 10s
        svc.__class__.interval_length = 5

        # We check if we correclty linked our escalations
        tolevel2_time = self.sched.conf.escalations.find_by_name('ToLevel2-shortinterval')
        self.assert_(tolevel2_time is not None)
        self.assert_(tolevel2_time in svc.escalations)
        #tolevel3_time = self.sched.conf.escalations.find_by_name('ToLevel3-time')
        #self.assert_(tolevel3_time is not None)
        #self.assert_(tolevel3_time in svc.escalations)

        # Go for the running part!

        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)

        # We check if we really notify the level1
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # notification_number is already sent. the next one has been scheduled
        # and is waiting for notification_interval to pass. so the current
        # number is 2
        self.assert_(svc.current_notification_number == 1)
        print "OK, level1 is notified, notif nb = 1"

        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 1 x BAD repeat -------------------------------------"

        # For the test, we hack the notif value because we do not wan to wait 1 hour!
        #for n in svc.notifications_in_progress.values():
            # HOP, we say: it's already 3600 second since the last notif,
        #    svc.notification_interval = 3600
            # and we say that there is still 1hour since the notification creation
            # so it will say the notification time is huge, and so it will escalade
        #    n.creation_time = n.creation_time - 3600

        # Sleep 1min and look how the notification is going, only 6s because we will go in
        # escalation in 5s (5s = interval_length, 1 for escalation time)
        print "---" * 200
        print "We wait a bit, but not enough to go in escalation level2"
        time.sleep(2)

        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.001)

        # Now we raise a notification time of 1hour, we escalade to level2
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        print "---" * 200
        print "OK NOW we will have an escalation!"
        time.sleep(5)

        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.001)

        # Now we raise a notification time of 1hour, we escalade to level2
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()

        print "cnn and cur", cnn, svc.current_notification_number
        # We check that we really raise the notif number too
        self.assert_(svc.current_notification_number > cnn)
        cnn = svc.current_notification_number
        
        # Ok we should have one notification
        next_notifications = svc.notifications_in_progress.values()
        print "LEN", len(next_notifications)
        for n in next_notifications:
            print n
        self.assert_(len(next_notifications) == 1)
        n = next_notifications.pop()
        print "Current NOTIFICATION", n.__dict__, n.t_to_go, time.time(), n.t_to_go - time.time(), n.already_start_escalations
        # Should be in the escalation ToLevel2-shortinterval
        self.assert_('ToLevel2-shortinterval' in n.already_start_escalations)

        # Ok we want to be sure we are using the current escalation interval, the 1 interval = 5s
        # So here we should have a new notification for level2
        print "*--*--" * 20
        print "Ok now another notification during the escalation 2"
        time.sleep(10)

        # One more bad, we say: he, it's still near 1 hour, so still level2
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.show_and_clear_logs()

        # Ok now go in the Level3 thing
        print "*--*--" * 20
        print "Ok now goes in level3 too"
        time.sleep(10)

        # One more, we bypass 7200, so now it's level3
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;CRITICAL;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()

        # Ok we should have one notification
        next_notifications = svc.notifications_in_progress.values()
        self.assert_(len(next_notifications) == 1)
        n = next_notifications.pop()
        print "Current NOTIFICATION", n.__dict__, n.t_to_go, time.time(), n.t_to_go - time.time(), n.already_start_escalations
        # Should be in the escalation ToLevel2-shortinterval
        self.assert_('ToLevel2-shortinterval' in n.already_start_escalations)
        self.assert_('ToLevel3-shortinterval' in n.already_start_escalations)

        # Make a loop for pass the next notification
        time.sleep(5)
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()

        print "Current NOTIFICATION", n.__dict__, n.t_to_go, time.time(), n.t_to_go - time.time(), n.already_start_escalations

        # Now way a little bit, and with such low value, the escalation3 value must be ok for this test to pass
        time.sleep(5)

        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;CRITICAL;'))
        self.show_and_clear_logs()

        # Now we recover, it will be fun because all of level{1,2,3} must be send a
        # recovery notif
        self.scheduler_loop(2, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.show_actions()
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level1.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level2.*;OK;'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION: level3.*;OK;'))
        self.show_and_clear_logs()




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_eventids
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


#
# This file is used to test current_event_id, last_event_id,
# current_problem_id and last_problem_id which are used for
# $HOSTEVENTID$, $HOSTPROBLEMID$ etc.
#

from shinken_test import *
from shinken.objects.schedulingitem import SchedulingItem


class TestConfig(ShinkenTest):

    def print_ids(self, host, svc, router):
        print "global: cei,lei,cpi,lpi = %d,%d" % (SchedulingItem.current_event_id, SchedulingItem.current_problem_id)
        print "service: cei,lei,cpi,lpi = %d,%d,%d,%d" % (svc.current_event_id, svc.last_event_id, svc.current_problem_id, svc.last_problem_id)
        print "host:    cei,lei,cpi,lpi = %d,%d,%d,%d" % (host.current_event_id, host.last_event_id, host.current_problem_id, host.last_problem_id)
        print "router:  cei,lei,cpi,lpi = %d,%d,%d,%d" % (router.current_event_id, router.last_event_id, router.current_problem_id, router.last_problem_id)

    def test_global_counters(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.print_ids(host, svc, router)
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=False)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 0)
        self.assert_(svc.last_event_id == 0)
        self.assert_(svc.current_problem_id == 0)
        self.assert_(svc.last_problem_id == 0)
        #--------------------------------------------------------------
        # service reaches soft;1
        # svc: 1,0,1,0
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 1)
        self.assert_(svc.last_event_id == 0)
        self.assert_(svc.current_problem_id == 1)
        self.assert_(svc.last_problem_id == 0)
        #--------------------------------------------------------------
        # service reaches hard;2
        # svc: 1,0,1,0
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 1)
        self.assert_(svc.last_event_id == 0)
        self.assert_(svc.current_problem_id == 1)
        self.assert_(svc.last_problem_id == 0)
        print "- 5 x BAD repeat -------------------------------------"
        self.scheduler_loop(5, [[svc, 2, 'BAD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 1)
        self.assert_(svc.last_event_id == 0)
        self.assert_(svc.current_problem_id == 1)
        self.assert_(svc.last_problem_id == 0)
        #--------------------------------------------------------------
        # now recover.
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 2)
        self.assert_(svc.last_event_id == 1)
        self.assert_(svc.current_problem_id == 0)
        self.assert_(svc.last_problem_id == 1)
        #--------------------------------------------------------------
        # service fails again, ok->w->c
        #--------------------------------------------------------------
        print "- 4 x BAD get hard with non-ok statechange -------------"
        self.scheduler_loop(2, [[svc, 1, 'BAD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 3)
        self.assert_(svc.last_event_id == 2)
        self.assert_(svc.current_problem_id == 2)
        self.assert_(svc.last_problem_id == 0)
        # another statechange
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 4)
        self.assert_(svc.last_event_id == 3)
        self.assert_(svc.current_problem_id == 2)
        self.assert_(svc.last_problem_id == 0)
        #--------------------------------------------------------------
        # now recover.
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 5)
        self.assert_(svc.last_event_id == 4)
        self.assert_(svc.current_problem_id == 0)
        self.assert_(svc.last_problem_id == 2)
        #--------------------------------------------------------------
        # mix in  two hosts
        #--------------------------------------------------------------
        print "- 4 x BAD get hard with non-ok statechange -------------"
        self.scheduler_loop(2, [[router, 2, 'DOWN']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(SchedulingItem.current_event_id == 6)
        self.assert_(SchedulingItem.current_problem_id == 3)
        self.assert_(host.current_event_id == 0)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 0)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 5)
        self.assert_(svc.last_event_id == 4)
        self.assert_(svc.current_problem_id == 0)
        self.assert_(svc.last_problem_id == 2)
        self.assert_(router.current_event_id == 6)
        self.assert_(router.last_event_id == 0)
        self.assert_(router.current_problem_id == 3)
        self.assert_(router.last_problem_id == 0)
        # add chaos
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=False)
        self.scheduler_loop(2, [[router, 0, 'UP']], do_sleep=False)
        self.scheduler_loop(5, [[host, 2, 'DOWN']], do_sleep=False)
        self.print_ids(host, svc, router)
        self.assert_(SchedulingItem.current_event_id == 9)
        self.assert_(SchedulingItem.current_problem_id == 5)
        self.assert_(host.current_event_id == 9)
        self.assert_(host.last_event_id == 0)
        self.assert_(host.current_problem_id == 5)
        self.assert_(host.last_problem_id == 0)
        self.assert_(svc.current_event_id == 7)
        self.assert_(svc.last_event_id == 5)
        self.assert_(svc.current_problem_id == 4)
        self.assert_(svc.last_problem_id == 0)
        self.assert_(router.current_event_id == 8)
        self.assert_(router.last_event_id == 6)
        self.assert_(router.current_problem_id == 0)
        self.assert_(router.last_problem_id == 3)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_exclude_services
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test object properties overriding.
#

import re
from shinken_test import unittest, ShinkenTest


class TestPropertyOverride(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_exclude_services.cfg')

    def test_exclude_services(self):
        hst1 = self.sched.hosts.find_by_name("test_host_01")
        hst2 = self.sched.hosts.find_by_name("test_host_02")

        self.assert_(hst1.service_excludes == [])
        self.assert_(hst2.service_excludes == ["srv-svc11", "srv-svc21", "proc proc1"])

        # All services should exist for test_host_01
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc11")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc12")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc21")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc22")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "proc proc1")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "proc proc2")
        self.assert_(svc is not None)

        # Half the services only should exist for test_host_02
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc11")
        self.assert_(svc is None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc12")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc21")
        self.assert_(svc is None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc22")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "proc proc1")
        self.assert_(svc is None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "proc proc2")
        self.assert_(svc is not None)


class TestConfigBroken(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_exclude_services_broken.cfg')

    def test_exclude_services_errors(self):
        self.assert_(not self.conf.conf_is_correct)

        # Get the arbiter's log broks
        [b.prepare() for b in self.broks.values()]
        logs = [b.data['log'] for b in self.broks.values() if b.type == 'log']

        self.assert_(len([log for log in logs if re.search('Error: exclusion contains an undefined service: fake', log)]) == 1)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_external_commands
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
import os


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def send_cmd(self, line):
        s = '[%d] %s\n' % (int(time.time()), line)
        print "Writing %s in %s" % (s, self.conf.command_file)
        fd = open(self.conf.command_file, 'wb')
        fd.write(s)
        fd.close()

    def test_external_comand(self):
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        router = self.sched.hosts.find_by_name("test_router_0")
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_host_0;2;Bob is not happy' % time.time()
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(host.state == 'DOWN')
        self.assert_(host.output == 'Bob is not happy')

        # Now with performance data
        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_host_0;2;Bob is not happy|rtt=9999' % time.time()
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(host.state == 'DOWN')
        self.assert_(host.output == 'Bob is not happy')
        self.assert_(host.perf_data == 'rtt=9999')

        # Now with full-blown performance data. Here we have to watch out:
        # Is a ";" a separator for the external command or is it
        # part of the performance data?
        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_host_0;2;Bob is not happy|rtt=9999;5;10;0;10000' % time.time()
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(host.state == 'DOWN')
        self.assert_(host.output == 'Bob is not happy')
        print "perf (%s)" % host.perf_data
        self.assert_(host.perf_data == 'rtt=9999;5;10;0;10000')

        # The same with a service
        excmd = '[%d] PROCESS_SERVICE_CHECK_RESULT;test_host_0;test_ok_0;1;Bobby is not happy|rtt=9999;5;10;0;10000' % time.time()
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(svc.state == 'WARNING')
        self.assert_(svc.output == 'Bobby is not happy')
        print "perf (%s)" % svc.perf_data
        self.assert_(svc.perf_data == 'rtt=9999;5;10;0;10000')

        # Clean the command_file
        #try:
        #    os.unlink(self.conf.command_file)
        #except:
        #    pass


        # Now with PAST DATA. We take the router because it was not called from now.
        past = int(time.time() - 30)
        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_router_0;2;Bob is not happy|rtt=9999;5;10;0;10000' % past
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(router.state == 'DOWN')
        self.assert_(router.output == 'Bob is not happy')
        print "perf (%s)" % router.perf_data
        self.assert_(router.perf_data == 'rtt=9999;5;10;0;10000')
        print "Is the last check agree?", past, router.last_chk
        self.assert_(past == router.last_chk)

        # Now an even earlier check, should NOT be take
        very_past = int(time.time() - 3600)
        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_router_0;2;Bob is not happy|rtt=9999;5;10;0;10000' % very_past
        self.sched.run_external_command(excmd)
        self.scheduler_loop(1, [])
        self.scheduler_loop(1, [])  # Need 2 run for get then consume)
        self.assert_(router.state == 'DOWN')
        self.assert_(router.output == 'Bob is not happy')
        print "perf (%s)" % router.perf_data
        self.assert_(router.perf_data == 'rtt=9999;5;10;0;10000')
        print "Is the last check agree?", very_past, router.last_chk
        self.assert_(past == router.last_chk)

        # Now with crappy characters, like 
        host = self.sched.hosts.find_by_name("test_router_0")
        excmd = '[%d] PROCESS_HOST_CHECK_RESULT;test_router_0;2;Bob got a crappy character     and so is not not happy|rtt=9999' % int(time.time())
        self.sched.run_external_command(excmd)
        self.scheduler_loop(2, [])
        self.assert_(host.state == 'DOWN')
        self.assert_(host.output == u'Bob got a crappy character     and so is not not happy')
        self.assert_(host.perf_data == 'rtt=9999')




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_external_mapping
#!/usr/bin/env python
#
# Copyright (C) 2012:
#    Hartmut Goebel <h.goebel@crazy-compilers.com>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.
"""
Test libexec/external_mapping.py
"""

import os
import time
import subprocess
import unittest
from tempfile import NamedTemporaryFile
from shinken_test import *

try:
    import json
except ImportError:
    # For old Python version, load
    # simple json (it can be hard json?! It's 2 functions guy!)
    try:
        import simplejson as json
    except ImportError:
        print "Error: you need the json or simplejson module"
        raise
                                                
external_mapping = os.path.join(os.path.dirname(__file__),
                                '..', 'libexec', 'external_mapping.py')

time_hacker.set_real_time()

class TestExternalMapping(ShinkenTest):

    def setUp(self):
        pass

    def __setup(self, inputlines):
        """
        Create a temporary input file and a temporary output-file.
        """
        # create output file fist, so it is older
        outputfile = NamedTemporaryFile("w", suffix='.json', delete=False)
        outputfile.write('--- empty marker ---')
        outputfile.close()
        self.output_filename = outputfile.name

        time.sleep(1) # ensure a time-difference between files

        inputfile = NamedTemporaryFile("w", suffix='.txt', delete=False)
        for line in inputlines:
            inputfile.writelines((line, '\n'))
        inputfile.close()
        self.input_filename = inputfile.name

    def __cleanup(self):
        """
        Cleanup the temporary files.
        """
        os.remove(self.input_filename)
        os.remove(self.output_filename)

    def __run(self, lines):
        self.__setup(lines)
        subprocess.call([external_mapping,
                         '--input', self.input_filename,
                         '--output', self.output_filename])
        result = json.load(open(self.output_filename))
        self.__cleanup()
        return result


    def test_simple(self):
        lines = [
            'myhost:vm1',
            'yourhost:vm1',
            'theirhost:xen3',
            ]
        result = self.__run(lines)
        self.assertEqual(result,
                         [[["host", "myhost"], ["host", "vm1"]],
                          [["host", "yourhost"], ["host", "vm1"]],
                          [["host", "theirhost"], ["host", "xen3"]]])

    def test_empty(self):
        lines = []
        result = self.__run(lines)
        self.assertEqual(result, [])

    def test_spaces_around_names(self):
        lines = [
            '   myhost   :    vm1   ',
            'yourhost :vm1',
            'theirhost:  xen3   ',
            ]
        result = self.__run(lines)
        self.assertEqual(result,
                         [[["host", "myhost"], ["host", "vm1"]],
                          [["host", "yourhost"], ["host", "vm1"]],
                          [["host", "theirhost"], ["host", "xen3"]]])

    def test_comment_line(self):
        lines = [
            'myhost:vm1',
            '# this is a comment',
            'yourhost:vm1',
            ]
        result = self.__run(lines)
        self.assertEqual(result,
                         [[["host", "myhost"], ["host", "vm1"]],
                          [["host", "yourhost"], ["host", "vm1"]]])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_flapping
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestFlapping(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_flapping.cfg')

    def test_flapping(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        router = self.sched.hosts.find_by_name("test_router_0")
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        self.assert_(svc.flap_detection_enabled)

        print 'A' * 41, svc.low_flap_threshold
        self.assert_(svc.low_flap_threshold == -1)

        # Now 1 test with a bad state
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "******* Current flap change lsit", svc.flapping_changes
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "****** Current flap change lsit", svc.flapping_changes
        # Ok, now go in flap!
        for i in xrange(1, 10):
            "**************************************************"
            print "I:", i
            self.scheduler_loop(1, [[svc, 0, 'Ok']])
            print "******* Current flap change lsit", svc.flapping_changes
            self.scheduler_loop(1, [[svc, 2, 'Crit']])
            print "****** Current flap change lsit", svc.flapping_changes
            print "In flapping?", svc.is_flapping

        # Should get in flapping now
        self.assert_(svc.is_flapping)
        # and get a log about it
        self.assert_(self.any_log_match('SERVICE FLAPPING ALERT.*;STARTED'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;FLAPPINGSTART'))

        # Now we put it as back :)
        # 10 is not enouth to get back as normal
        for i in xrange(1, 11):
            self.scheduler_loop(1, [[svc, 0, 'Ok']])
            print "In flapping?", svc.is_flapping
        self.assert_(svc.is_flapping)

        # 10 others can be good (near 4.1 %)
        for i in xrange(1, 11):
            self.scheduler_loop(1, [[svc, 0, 'Ok']])
            print "In flapping?", svc.is_flapping
        self.assert_(not svc.is_flapping)
        self.assert_(self.any_log_match('SERVICE FLAPPING ALERT.*;STOPPED'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;FLAPPINGSTOP'))

        ############ Now get back in flap, and try the exteral commands change

        # Now 1 test with a bad state
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "******* Current flap change lsit", svc.flapping_changes
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "****** Current flap change lsit", svc.flapping_changes
        # Ok, now go in flap!
        for i in xrange(1, 10):
            "**************************************************"
            print "I:", i
            self.scheduler_loop(1, [[svc, 0, 'Ok']])
            print "******* Current flap change lsit", svc.flapping_changes
            self.scheduler_loop(1, [[svc, 2, 'Crit']])
            print "****** Current flap change lsit", svc.flapping_changes
            print "In flapping?", svc.is_flapping

        # Should get in flapping now
        self.assert_(svc.is_flapping)
        # and get a log about it
        self.assert_(self.any_log_match('SERVICE FLAPPING ALERT.*;STARTED'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;FLAPPINGSTART'))

        # We run a globa lflap disable, so we should stop flapping now
        cmd = "[%lu] DISABLE_FLAP_DETECTION" % int(time.time())
        self.sched.run_external_command(cmd)

        self.assert_(not svc.is_flapping)

        ############# NOW a local command for this service
        # First reenable flap:p
        cmd = "[%lu] ENABLE_FLAP_DETECTION" % int(time.time())
        self.sched.run_external_command(cmd)

        # Now 1 test with a bad state
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "******* Current flap change lsit", svc.flapping_changes
        self.scheduler_loop(1, [[svc, 2, 'Crit']])
        print "****** Current flap change lsit", svc.flapping_changes
        # Ok, now go in flap!
        for i in xrange(1, 10):
            "**************************************************"
            print "I:", i
            self.scheduler_loop(1, [[svc, 0, 'Ok']])
            print "******* Current flap change lsit", svc.flapping_changes
            self.scheduler_loop(1, [[svc, 2, 'Crit']])
            print "****** Current flap change lsit", svc.flapping_changes
            print "In flapping?", svc.is_flapping

        # Should get in flapping now
        self.assert_(svc.is_flapping)
        # and get a log about it
        self.assert_(self.any_log_match('SERVICE FLAPPING ALERT.*;STARTED'))
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;FLAPPINGSTART'))

        # We run a globa lflap disable, so we should stop flapping now
        cmd = "[%lu] DISABLE_SVC_FLAP_DETECTION;test_host_0;test_ok_0" % int(time.time())
        self.sched.run_external_command(cmd)

        self.assert_(not svc.is_flapping)




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_freshness
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestFreshness(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_freshness.cfg')

    # Check if the check_freshnes is doing it's job
    def test_check_freshness(self):
        self.print_header()
        # We want an eventhandelr (the perfdata command) to be put in the actions dict
        # after we got a service check
        now = time.time()
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        svc.active_checks_enabled = False
        self.assert_(svc.check_freshness == True)
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        # We do not want to be just a string but a real command
        print "Additonal freshness latency", svc.__class__.additional_freshness_latency
        self.scheduler_loop(1, [[svc, 0, 'OK | bibi=99%']])
        print "Addi:", svc.last_state_update, svc.freshness_threshold, svc.check_freshness
        # By default check fresh ness is set at false, so no new checks
        self.assert_(len(svc.actions) == 0)
        svc.do_check_freshness()
        self.assert_(len(svc.actions) == 0)

        # We make it 10s less than it was
        svc.last_state_update = svc.last_state_update - 10

        #svc.check_freshness = True
        # Now we active it, with a too small value (now - 10s is still higer than now - (1 - 15, the addition time)
        # So still no check
        svc.freshness_threshold = 1
        print "Addi:", svc.last_state_update, svc.freshness_threshold, svc.check_freshness
        svc.do_check_freshness()
        self.assert_(len(svc.actions) == 0)

        # Now active globaly the check freshness
        cmd = "[%lu] ENABLE_SERVICE_FRESHNESS_CHECKS" % now
        self.sched.run_external_command(cmd)

        # Ok, now, we remove again 10s. Here we will saw the new entry
        svc.last_state_update = svc.last_state_update - 10
        svc.do_check_freshness()
        self.assert_(len(svc.actions) == 1)
        # And we check for the message in the log too
        self.assert_(self.any_log_match('The results of service.*'))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_groups_with_no_alias
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestGroupwithNoAlias(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_groups_with_no_alias.cfg')

    def test_look_for_alias(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        hg = self.sched.hostgroups.find_by_name("NOALIAS")
        self.assert_(hg is not None)
        print hg.__dict__
        self.assert_(hg.alias == "NOALIAS")

        sg = self.sched.servicegroups.find_by_name("NOALIAS")
        self.assert_(sg is not None)
        print sg.__dict__
        self.assert_(sg.alias == "NOALIAS")


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_hostdep_withno_depname
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestHostDepWithNodepname(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_hostdep_withno_depname.cfg')

    def test_hostdep_withno_depname(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        h2 = self.sched.hosts.find_by_name("test_host_1")
        self.assert_(h2 is not None)
        # Should got a link between host and h2
        print h2.act_depend_of
        self.assert_(len(h2.act_depend_of) > 0)
        l = h2.act_depend_of[0]
        h = l[0]  # the host that h2 depend on
        self.assert_(h is host)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_hostdep_with_multiple_names
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestHostDepWithMultipleNames(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_hostdep_with_multiple_names.cfg')

    def test_DepWithMultipleNames(self):
        for n in ['svn1', 'svn2', 'svn3', 'svn4', 'nas1', 'nas2', 'nas3']:
            globals()[n] = self.sched.hosts.find_by_name(n)
            self.assert_(globals()[n] is not None)
        # We check that nas3 is a father of svn4, the simple case
        self.assert_(nas3 in [e[0] for e in svn4.act_depend_of])

        # Now the more complex one
        for son in [svn1, svn2, svn3]:
            for father in [nas1, nas2]:
                print 'Checking if', father.get_name(), 'is the father of', son.get_name()
                print son.act_depend_of
                for e in son.act_depend_of:
                    print e[0].get_name()
                self.assert_(father in [e[0] for e in son.act_depend_of])

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_hostgroup_no_host
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestHostGroupNoHost(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_hostgroup_no_host.cfg')

    def test_hostgroup_wit_no_host(self):
        self.assert_(self.sched.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_hostgroup_with_space
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestHostGroupWithSpace(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_hostgroup_with_space.cfg')


    def test_hostgroup_with_space(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_With Spaces")
        self.assert_(svc is not None)

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", 'test_With anoter Spaces')
        self.assert_(svc is not None)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_hosts
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import copy
from shinken_test import *


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def get_hst(self):
        return self.sched.hosts.find_by_name("test_host_0")

    # Look if get_*_name return the good result
    def test_get_name(self):
        hst = self.get_hst()
        print hst.get_dbg_name()
        self.assert_(hst.get_name() == 'test_host_0')
        self.assert_(hst.get_dbg_name() == 'test_host_0')

    # getstate should be with all properties in dict class + id
    # check also the setstate
    def test___getstate__(self):
        hst = self.get_hst()
        cls = hst.__class__
        # We get the state
        state = hst.__getstate__()
        # Check it's the good length
        self.assert_(len(state) == len(cls.properties) + len(cls.running_properties) + 1)
        # we copy the service
        hst_copy = copy.copy(hst)
        # reset the state in the original service
        hst.__setstate__(state)
        # And it should be the same:then before :)
        for p in cls.properties:
            ## print getattr(hst_copy, p)
            ## print getattr(hst, p)
            self.assert_(getattr(hst_copy, p) == getattr(hst, p))

    # Look if it can detect all incorrect cases
    def test_is_correct(self):
        hst = self.get_hst()

        # first it's ok
        self.assert_(hst.is_correct() == True)

        # Now try to delete a required property
        max_check_attempts = hst.max_check_attempts
        del hst.max_check_attempts
        self.assert_(hst.is_correct() == True)
        hst.max_check_attempts = max_check_attempts

        ###
        ### Now special cases
        ###

        # no check command
        check_command = hst.check_command
        del hst.check_command
        self.assert_(hst.is_correct() == False)
        hst.check_command = check_command
        self.assert_(hst.is_correct() == True)

        # no notification_interval
        notification_interval = hst.notification_interval
        del hst.notification_interval
        self.assert_(hst.is_correct() == False)
        hst.notification_interval = notification_interval
        self.assert_(hst.is_correct() == True)

    # Look for set/unset impacted states (unknown)
    def test_impact_state(self):
        hst = self.get_hst()
        ori_state = hst.state
        ori_state_id = hst.state_id
        hst.set_impact_state()
        self.assert_(hst.state == 'UNREACHABLE')
        self.assert_(hst.state_id == 2)
        hst.unset_impact_state()
        self.assert_(hst.state == ori_state)
        self.assert_(hst.state_id == ori_state_id)

    def test_set_state_from_exit_status(self):
        hst = self.get_hst()
        # First OK
        hst.set_state_from_exit_status(0)
        self.assert_(hst.state == 'UP')
        self.assert_(hst.state_id == 0)
        self.assert_(hst.is_state('UP') == True)
        self.assert_(hst.is_state('o') == True)
        # Then warning
        hst.set_state_from_exit_status(1)
        self.assert_(hst.state == 'UP')
        self.assert_(hst.state_id == 0)
        self.assert_(hst.is_state('UP') == True)
        self.assert_(hst.is_state('o') == True)
        # Then Critical
        hst.set_state_from_exit_status(2)
        self.assert_(hst.state == 'DOWN')
        self.assert_(hst.state_id == 1)
        self.assert_(hst.is_state('DOWN') == True)
        self.assert_(hst.is_state('d') == True)
        # And unknown
        hst.set_state_from_exit_status(3)
        self.assert_(hst.state == 'DOWN')
        self.assert_(hst.state_id == 1)
        self.assert_(hst.is_state('DOWN') == True)
        self.assert_(hst.is_state('d') == True)

        # And something else :)
        hst.set_state_from_exit_status(99)
        self.assert_(hst.state == 'DOWN')
        self.assert_(hst.state_id == 1)
        self.assert_(hst.is_state('DOWN') == True)
        self.assert_(hst.is_state('d') == True)

        # And a special case: use_aggressive_host_checking
        hst.__class__.use_aggressive_host_checking = 1
        hst.set_state_from_exit_status(1)
        self.assert_(hst.state == 'DOWN')
        self.assert_(hst.state_id == 1)
        self.assert_(hst.is_state('DOWN') == True)
        self.assert_(hst.is_state('d') == True)

    def test_hostgroup(self):
        hg = self.sched.hostgroups.find_by_name("hostgroup_01")
        self.assert_(hg is not None)
        h = self.sched.hosts.find_by_name('test_host_0')
        self.assert_(h in hg.members)
        self.assert_(hg in h.hostgroups)

    def test_childs(self):
        h = self.sched.hosts.find_by_name('test_host_0')
        r = self.sched.hosts.find_by_name('test_router_0')

        # Search if h is in r.childs
        self.assert_(h in r.childs)
        # and the reverse
        self.assert_(r in h.parents)
        print "r.childs", r.childs
        print "h.childs", h.childs

        # And also in the parent/childs dep list
        self.assert_(h in r.child_dependencies)
        # and the reverse
        self.assert_(r in h.parent_dependencies)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_host_extented_info
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_host_extented_info.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        self.assert_(host.icon_image == 'icon.png')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_host_missing_adress
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_host_missing_adress.cfg')

    def test_host_missing_adress(self):
        # The router got no adress. It should be set with the
        # host_name instead and should nto be an error
        now = time.time()
        router = self.sched.hosts.find_by_name("test_router_0")
        print "router adress:", router.address
        self.assert_(router.address == 'test_router_0')

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_host_without_cmd
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_host_without_cmd.cfg')

    def test_host_is_pending(self):
        self.print_header()
        # first of all, a host without check_command must be valid
        self.assert_(self.conf.conf_is_correct)
        # service always ok, host stays pending
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        for c in host.checks_in_progress:
            # hurry up, we need an immediate result
            c.t_to_go = 0
        # scheduler.schedule() always schedules a check, even for this
        # kind of hosts
        #host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        host.checks_in_progress = []
        host.in_checking = False
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        # this time we need the dependency from service to host
        #svc.act_depend_of = [] # no hostchecks on critical checkresults

        # initially the host is pending
        self.assert_(host.state == 'PENDING')
        self.assert_(svc.state == 'PENDING')
        # now force a dependency check of the host
        self.scheduler_loop(2, [[svc, 2, 'BAD | value1=0 value2=0']])
        self.show_actions()
        # and now the host is magically UP
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        self.assert_(host.output == 'Host assumed to be UP')



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_illegal_names
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def test_illegal_caracter_in_names(self):
        illegal_caracts = self.sched.conf.illegal_object_name_chars
        print "Illegal caracters: %s" % illegal_caracts
        host = self.sched.hosts.find_by_name("test_host_0")
        # should be correct
        self.assert_(host.is_correct())

        # Now change the name with incorrect caract
        for c in illegal_caracts:
            host.host_name = 'test_host_0' + c
            # and Now I want an incorrect here
            self.assert_(host.is_correct() == False)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_inheritance_and_plus
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestInheritanceAndPlus(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_inheritance_and_plus.cfg')

    def test_inheritance_and_plus(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        linux = self.sched.hostgroups.find_by_name('linux')
        self.assert_(linux is not None)
        dmz = self.sched.hostgroups.find_by_name('DMZ')
        self.assert_(dmz is not None)
        mysql = self.sched.hostgroups.find_by_name('mysql')
        self.assert_(mysql is not None)

        host1 = self.sched.hosts.find_by_name("test-server1")
        host2 = self.sched.hosts.find_by_name("test-server2")
        # HOST 1 is lin-servers,dmz, so should be in linux AND DMZ group
        for hg in host1.hostgroups:
            print hg.get_name()
        self.assert_(linux in host1.hostgroups)
        self.assert_(dmz in host1.hostgroups)

        # HOST2 is in lin-servers,dmz and +mysql, so all three of them
        for hg in host2.hostgroups:
            print hg.get_name()
        self.assert_(linux in host2.hostgroups)
        self.assert_(dmz in host2.hostgroups)
        self.assert_(mysql in host2.hostgroups)




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_linkify_template
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestLinkifyTemplate(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_linkify_template.cfg')

    def test_linkify_template(self):
        svc = self.conf.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        b = svc.is_correct()
        self.assert_(not b)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_logging
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2012:
#    Hartmut Goebel <h.goebel@crazy-compilers.com>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
Test shinken.logging
"""

import sys
import os
import time
import cPickle
from cStringIO import StringIO

import unittest
from tempfile import NamedTemporaryFile

import __import_shinken
from shinken.log import logger, Log
import shinken.log as logging
from shinken.brok import Brok
from shinken_test import *

# The logging module requires some object for collecting broks
class Dummy:
    """Dummy class for collecting broks"""
    def add(self, o):
        pass

class Collector:
    """Dummy class for collecting broks"""
    def __init__(self):
        self.list = []

    def add(self, o):
        self.list.append(o)


class NoSetup:
    def setUp(self):
        pass



logger.load_obj(Dummy())



class TestLevels(NoSetup, ShinkenTest):

    def test_get_level_id(self):
        for name , level in (
            ('NOTSET',   logger.NOTSET),
            ('DEBUG',    logger.DEBUG),
            ('INFO',     logger.INFO),
            ('WARNING',  logger.WARNING),
            ('ERROR',    logger.ERROR),
            ('CRITICAL', logger.CRITICAL),
            ):
            self.assertEqual(logger.get_level_id(level), name)

    def test_get_level_id_unknown_level_raises(self):
        self.assertRaises(KeyError, logger.get_level_id, 'MYLEVEL')

    def test_default_level(self):
        logger = Log()
        # :fixme: `_level` is private, needs an official accessor
        self.assertEqual(logger._level, logger.NOTSET)

    def test_set_level(self):
        logger.set_level(logger.WARNING)
        self.assertEqual(logger._level, logger.WARNING)

    def test_set_level_non_integer_raises(self):
        self.assertRaises(TypeError, logger.set_level, 1.0)
        # Why raise if there is an easy way to give the value like this string?
        #self.assertRaises(TypeError, logger.set_level, 'INFO')

    def test_load_obj_must_not_change_level(self):
        # argl, load_obj() unsets the level! save and restore it
        logger.set_level(logger.CRITICAL)
        logger.load_obj(Dummy())
        self.assertEqual(logger._level, logger.CRITICAL)

class TestBasics(NoSetup, ShinkenTest):

    def test_setting_and_unsetting_human_timestamp_format(self):
        # :hack: logging.human_timestamp_log is a global variable
        self.assertEqual(logging.human_timestamp_log, False)
        logger.set_human_format(True)
        self.assertEqual(logging.human_timestamp_log, True)
        logger.set_human_format(False)
        self.assertEqual(logging.human_timestamp_log, False)
        logger.set_human_format(True)
        self.assertEqual(logging.human_timestamp_log, True)
        logger.set_human_format(False)
        self.assertEqual(logging.human_timestamp_log, False)


class LogCollectMixin:
    def _get_brok_log_messages(self, collector):
        """
        Return the log messages stored as Broks into the collector.

        This also tests whether all objects collected by the collector
        are log entries.
        """
        for obj in collector.list:
            self.assertIsInstance(obj, Brok)
            self.assertEqual(obj.type, 'log')
            data = cPickle.loads(obj.data)
            self.assertEqual(data.keys(), ['log'])
            yield data['log']

    def _prepare_logging(self):
        self._collector = Collector()
        logger.load_obj(self._collector)
        self._stdout = sys.stdout
        sys.stdout = StringIO()

    def _get_logging_output(self):
        msgs = list(self._get_brok_log_messages(self._collector))
        lines = sys.stdout.getvalue().splitlines()
        sys.stdout = self._stdout
        return msgs, lines

    def _put_log(self, log_method, *messages):
        self._prepare_logging()
        try:
            for msg in messages:
                log_method(msg)
        finally:
            return self._get_logging_output()
    

class TestDefaultLoggingMethods(NoSetup, ShinkenTest, LogCollectMixin):

    def test_basic_logging_log(self):
        msgs, lines = self._put_log(logger.log, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0], r'^\[\d+\] Some log-message\n$')
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Some log-message$')

    def test_basic_logging_debug_does_not_send_broks(self):
        logger.set_level(logger.DEBUG)
        msgs, lines = self._put_log(logger.debug, 'Some log-message')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Debug :\s+Some log-message$')

    def test_basic_logging_info(self):
        logger.set_level(logger.INFO)
        msgs, lines = self._put_log(logger.info, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0], r'^\[\d+\] Info :\s+Some log-message\n$')
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Info :\s+Some log-message$')

    def test_basic_logging_warning(self):
        logger.set_level(logger.WARNING)
        msgs, lines = self._put_log(logger.warning, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0], r'^\[\d+\] Warning :\s+Some log-message\n$')
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Warning :\s+Some log-message$')


    def test_basic_logging_error(self):
        logger.set_level(logger.ERROR)
        msgs, lines = self._put_log(logger.error, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0], r'^\[\d+\] Error :\s+Some log-message\n$')
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Error :\s+Some log-message$')

    def test_basic_logging_critical(self):
        msgs, lines = self._put_log(logger.critical, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0], r'^\[\d+\] Critical :\s+Some log-message\n$')
        self.assertRegexpMatches(lines[0], r'^\[\d+\] Critical :\s+Some log-message$')

    def test_level_is_higher_then_the_one_set(self):
        # just test two samples
        logger.set_level(logger.CRITICAL)
        msgs, lines = self._put_log(logger.error, 'Some log-message')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 0)

        logger.set_level(logger.INFO)
        msgs, lines = self._put_log(logger.debug, 'Some log-message$')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 0)

    def test_human_timestamp_format(self):
        "test output using the human timestamp format"
        logger.set_level(logger.INFO)
        logger.set_human_format(True)
        msgs, lines = self._put_log(logger.info, 'Some ] log-message')
        self.assertRegexpMatches(msgs[0],
            r'^\[[^\]]+] Info :\s+Some \] log-message\n$')
        time.strptime(msgs[0].split(' Info :    ', 1)[0], '[%a %b %d %H:%M:%S %Y]')
        self.assertRegexpMatches(lines[0],
            r'^\[[^\]]+] Info :\s+Some \] log-message$')
        time.strptime(msgs[0].split(' Info :    ', 1)[0], '[%a %b %d %H:%M:%S %Y]')
        logger.set_human_format(False)

    def test_reset_human_timestamp_format(self):
        "test output after switching of the human timestamp format"
        # ensure the human timestamp format is set, ...
        self.test_human_timestamp_format()
        # ... then turn it off
        logger.set_human_format(False)
        # test whether the normal format is used again
        self.test_basic_logging_info()


class TestWithLocalLogging(NoSetup, ShinkenTest, LogCollectMixin):

    def _prepare_logging(self):
        super(TestWithLocalLogging, self)._prepare_logging()
        # set up a temporary file for logging
        logfile = NamedTemporaryFile("w")
        logfile.close()
        self.logfile_name = logfile.name
        logger.register_local_log(logfile.name)

    def _get_logging_output(self):
        msgs, lines = super(TestWithLocalLogging, self)._get_logging_output()
        f = open(self.logfile_name)
        local_lines = list(f.readlines())
        f.close()
        try:
            os.remove(self.logfile_name)
        except : # On windows, the file is still lock. But should be close!?!
            pass
        return msgs, lines, local_lines
    

    def test_register_local_log_keeps_level(self):
        logger.set_level(logger.ERROR)
        self.assertEqual(logger._level, logger.ERROR)
        logfile = NamedTemporaryFile("w")
        logfile.close()
        logfile_name = logfile.name
        logger.register_local_log(logfile_name)
        self.assertEqual(logger._level, logger.ERROR)

    def test_basic_logging_log(self):
        msgs, lines, local_log = self._put_log(logger.log, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertEqual(len(local_log), 1)
        self.assertRegexpMatches(local_log[0], r' \[\d+\] Some log-message\n$')

    def test_basic_logging_debug_does_not_send_broks(self):
        logger.set_level(logger.DEBUG)
        msgs, lines, local_log = self._put_log(logger.debug, 'Some log-message')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 1)
        self.assertEqual(len(local_log), 1)
        self.assertRegexpMatches(local_log[0],
            r' \[\d+\] Debug :\s+Some log-message$')

    def test_basic_logging_info(self):
        logger.set_level(logger.INFO)
        msgs, lines, local_log = self._put_log(logger.info, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertEqual(len(local_log), 1)
        self.assertRegexpMatches(local_log[0],
            r' \[\d+\] Info :\s+Some log-message\n$')

    def test_basic_logging_error(self):
        logger.set_level(logger.ERROR)
        msgs, lines, local_log = self._put_log(logger.error, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertEqual(len(local_log), 1)
        print >> sys.stderr, local_log[0]
        self.assertRegexpMatches(local_log[0],
            r' \[\d+\] Error :\s+Some log-message\n$')

    def test_basic_logging_critical(self):
        logger.set_level(logger.CRITICAL)
        msgs, lines, local_log = self._put_log(logger.critical, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertEqual(len(local_log), 1)
        self.assertRegexpMatches(local_log[0],
            r' \[\d+\] Critical :\s+Some log-message\n$')

    def test_level_is_higher_then_the_one_set(self):
        # just test two samples
        logger.set_level(logger.CRITICAL)
        msgs, lines, local_log = self._put_log(logger.debug, 'Some log-message')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 0)
        self.assertEqual(len(local_log), 0)

        logger.set_level(logger.INFO)
        msgs, lines, local_log = self._put_log(logger.debug, 'Some log-message')
        self.assertEqual(len(msgs), 0)
        self.assertEqual(len(lines), 0)
        self.assertEqual(len(local_log), 0)


    def test_human_timestamp_format(self):
        logger.set_level(logger.INFO)
        logger.set_human_format(True)
        msgs, lines, local_log = self._put_log(logger.info, 'Some ] log-message')
        self.assertEqual(len(local_log), 1)
        self.assertRegexpMatches(local_log[0],
            r' \[[^\]]+] Info :\s+Some \] log-message\n$')
        # :fixme: Currently, the local log gets prefixed another
        # timestamp. As it is yet unclear, whether this intended or
        # not, we test it, too.
        times = local_log[0].split(' Info :    ', 1)[0]
        time1, time2 = times.rsplit('[', 1)
        time.strptime(time1.rsplit(',')[0], '%Y-%m-%d %H:%M:%S')
        time.strptime(time2, '%a %b %d %H:%M:%S %Y]')
        logger.set_human_format(False)

    def test_reset_human_timestamp_format(self):
        "test output after switching of the human timestamp format"
        # ensure the human timestamp format is set, ...
        self.test_human_timestamp_format()
        # ... then turn it off
        logger.set_human_format(False)
        # test whether the normal format is used again
        self.test_basic_logging_info()


class TestNamedCollector(NoSetup, ShinkenTest, LogCollectMixin):

    # :todo: add a test for the local log file, too

    def _prepare_logging(self):
        self._collector = Collector()
        logger.load_obj(self._collector, 'Tiroler Schinken')
        self._stdout = sys.stdout
        sys.stdout = StringIO()


    def test_basic_logging_info(self):
        logger.set_level(logger.INFO)
        msgs, lines = self._put_log(logger.info, 'Some log-message')
        self.assertEqual(len(msgs), 1)
        self.assertEqual(len(lines), 1)
        self.assertRegexpMatches(msgs[0],
             r'^\[\d+\] Info :\s+\[Tiroler Schinken\] Some log-message\n$')
        self.assertRegexpMatches(lines[0],
             r'^\[\d+\] Info :\s+\[Tiroler Schinken\] Some log-message$')

    def test_human_timestamp_format(self):
        logger.set_level(logger.INFO)
        logger.set_human_format(True)
        msgs, lines = self._put_log(logger.info, 'Some ] log-message')
        self.assertRegexpMatches(msgs[0],
            r'^\[[^\]]+] Info :\s+\[Tiroler Schinken\] Some \] log-message\n$')
        time.strptime(msgs[0].split(' Info :    ', 1)[0], '[%a %b %d %H:%M:%S %Y]')
        self.assertRegexpMatches(lines[0],
            r'^\[[^\]]+] Info :\s+\[Tiroler Schinken\] Some \] log-message$')
        time.strptime(msgs[0].split(' Info :    ', 1)[0], '[%a %b %d %H:%M:%S %Y]')
        logger.set_human_format(False)

    def test_reset_human_timestamp_format(self):
        # ensure human timestamp format is set and working
        self.test_human_timestamp_format()
        # turn of human timestamp format
        logger.set_human_format(False)
        # test for normal format
        self.test_basic_logging_info()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_macromodulations
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestMacroModulations(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_macromodulations.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("host_modulated")
        self.assert_(host is not None)
        print host.macromodulations

        mod = self.sched.macromodulations.find_by_name("MODULATION")
        self.assert_(mod is not None)

        self.assert_(mod in host.macromodulations)

        c = None
        for c in host.checks_in_progress:
            print c.command
            # THE hst got 2 modulations. The first with the value MODULATED
            # and the second with NOT_THE_GOOD. Both are currently active, but we want the firt one
            self.assert_(c.command == 'plugins/nothing MODULATED')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_macroresolver
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
from shinken.macroresolver import MacroResolver
from shinken.commandcall import CommandCall
from shinken.objects import Command


class TestMacroResolver(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def setUp(self):
        self.setup_with_file('etc/shinken_macroresolver.cfg')
                

    def get_mr(self):
        mr = MacroResolver()
        mr.init(self.conf)
        return mr

    def get_hst_svc(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        hst = self.sched.hosts.find_by_name("test_host_0")
        return (svc, hst)

    def test_resolv_simple(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        com = mr.resolve_command(svc.check_command, data)
        print com
        self.assert_(com == "plugins/test_servicecheck.pl --type=ok --failchance=5% --previous-state=PENDING --state-duration=0 --total-critical-on-host=0 --total-warning-on-host=0 --hostname test_host_0 --servicedesc test_ok_0 --custom custvalue")


    # Here call with a special macro TOTALHOSTSUP
    # but call it as arg. So will need 2 pass in macro resolver
    # at last to resolv it.
    def test_special_macros(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        hst.state = 'UP'
        dummy_call = "special_macro!$TOTALHOSTSUP$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing 1')



    # Here call with a special macro HOSTREALM
    def test_special_macros_realm(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        hst.state = 'UP'
        dummy_call = "special_macro!$HOSTREALM$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing Default')


    # For output macro we want to delete all illegal macro caracter
    def test_illegal_macro_output_chars(self):
        "$HOSTOUTPUT$, $HOSTPERFDATA$, $HOSTACKAUTHOR$, $HOSTACKCOMMENT$, $SERVICEOUTPUT$, $SERVICEPERFDATA$, $SERVICEACKAUTHOR$, and $SERVICEACKCOMMENT$ "
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        illegal_macro_output_chars = self.sched.conf.illegal_macro_output_chars
        print "Illegal macros caracters:", illegal_macro_output_chars
        hst.output = 'monculcestdupoulet'
        dummy_call = "special_macro!$HOSTOUTPUT$"

        for c in illegal_macro_output_chars:
            hst.output = 'monculcestdupoulet' + c
            cc = CommandCall(self.conf.commands, dummy_call)
            com = mr.resolve_command(cc, data)
            print com
            self.assert_(com == 'plugins/nothing monculcestdupoulet')

    def test_env_macros(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        data.append(self.conf)

        env = mr.get_env_macros(data)
        print "Env:", env        
        self.assert_(env != {})
        self.assert_(env['NAGIOS_HOSTNAME'] == 'test_host_0')
        self.assert_(env['NAGIOS_SERVICEPERCENTCHANGE'] == '0.0')
        self.assert_(env['NAGIOS__SERVICECUSTNAME'] == 'custvalue')
        self.assert_(env['NAGIOS__HOSTOSTYPE'] == 'gnulinux')
        self.assert_('NAGIOS_USER1' not in env)


    def test_resource_file(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = svc.get_data_for_checks()
        dummy_call = "special_macro!$USER1$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        self.assert_(com == 'plugins/nothing plugins')

        dummy_call = "special_macro!$INTERESTINGVARIABLE$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print "CUCU", com
        self.assert_(com == 'plugins/nothing interestingvalue')

        # Look for multiple = in lines, should split the first
        # and keep others in the macro value
        dummy_call = "special_macro!$ANOTHERVALUE$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print "CUCU", com
        self.assert_(com == 'plugins/nothing blabla=toto')



    # Look at on demand macros
    def test_ondemand_macros(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = hst.get_data_for_checks()
        hst.state = 'UP'
        svc.state = 'UNKNOWN'

        # Ok sample host call
        dummy_call = "special_macro!$HOSTSTATE:test_host_0$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing UP')

        # Call with a void host name, means : myhost
        data = hst.get_data_for_checks()
        dummy_call = "special_macro!$HOSTSTATE:$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing UP')

        # Now with a service, for our implicit host state
        data = svc.get_data_for_checks()
        dummy_call = "special_macro!$HOSTSTATE:test_host_0$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing UP')
                                                        
                                        
        # Now with a service, for our implicit host state
        data = svc.get_data_for_checks()
        dummy_call = "special_macro!$HOSTSTATE:$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing UP')

        # Now prepare another service
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_another_service")
        svc2.output = 'you should not pass'

        # Now call this data from our previous service
        data = svc.get_data_for_checks()
        dummy_call = "special_macro!$SERVICEOUTPUT:test_host_0:test_another_service$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing you should not pass')

        # Ok now with an host implicit way
        data = svc.get_data_for_checks()
        dummy_call = "special_macro!$SERVICEOUTPUT::test_another_service$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing you should not pass')
                                                
                                                

    # Look at on demand macros
    def test_hostadressX_macros(self):
        mr = self.get_mr()
        (svc, hst) = self.get_hst_svc()
        data = hst.get_data_for_checks()

        # Ok sample host call
        dummy_call = "special_macro!$HOSTADDRESS6$"
        cc = CommandCall(self.conf.commands, dummy_call)
        com = mr.resolve_command(cc, data)
        print com
        self.assert_(com == 'plugins/nothing ::1')

        

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_maintenance_period
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
#time.time = original_time_time
#time.sleep = original_time_sleep
from shinken.objects.timeperiod import Timeperiod


class TestMaintPeriod(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_maintenance_period.cfg')

    def test_check_defined_maintenance_period(self):
        a_24_7 = self.sched.timeperiods.find_by_name("24x7")
        print "Get the hosts and services"
        test_router_0 = self.sched.hosts.find_by_name("test_router_0")
        test_host_0 = self.sched.hosts.find_by_name("test_host_0")
        test_nobody = self.sched.hosts.find_by_name("test_nobody")

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "test_ok_0")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_nobody", "test_ok_0")

        # Standard links
        self.assert_(test_router_0.maintenance_period == a_24_7)
        self.assert_(test_host_0.maintenance_period is None)
        self.assert_(test_nobody.maintenance_period is None)

        # Now inplicit inheritance
        # This one is defined in the service conf
        self.assert_(svc1.maintenance_period == a_24_7)
        # And others are implicitly inherited
        self.assert_(svc2.maintenance_period is a_24_7)
        # This one got nothing :)
        self.assert_(svc3.maintenance_period is None)

    def test_check_enter_downtime(self):
        test_router_0 = self.sched.hosts.find_by_name("test_router_0")
        test_host_0 = self.sched.hosts.find_by_name("test_host_0")
        test_nobody = self.sched.hosts.find_by_name("test_nobody")

        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "test_ok_0")
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_nobody", "test_ok_0")
        # we want to focus on only one maintenance
        test_router_0.maintenance_period = None
        test_host_0.maintenance_period = None
        test_nobody.maintenance_period = None
        svc1.maintenance_period = None
        svc2.maintenance_period = None

        # be sure we have some time before a new minute begins.
        # otherwise we get a race condition and a failed test here.
        now = time.time()
        x = time.gmtime(now)
        while x.tm_sec < 50:
            time.sleep(1)
            now = time.time()
            x = time.gmtime(now)

        now = time.time()
        print "now it is", time.asctime(time.localtime(now))
        nowday = time.strftime("%A", time.localtime(now + 60)).lower()
        soonstart = time.strftime("%H:%M", time.localtime(now + 60))
        soonend = time.strftime("%H:%M", time.localtime(now + 180))

        range = "%s %s-%s" % (nowday, soonstart, soonend)
        print "range is ", range
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, range)
        t_next = t.get_next_valid_time_from_t(now)
        print "planned start", time.asctime(time.localtime(t_next))
        t_next = t.get_next_invalid_time_from_t(t_next + 1)
        print "planned stop ", time.asctime(time.localtime(t_next))
        svc3.maintenance_period = t

        self.assert_(not svc3.in_maintenance)
        #
        # now let the scheduler run and wait until the maintenance period begins
        # it is now 10 seconds before the full minute. run for 30 seconds
        # in 1-second-intervals. this should be enough to trigger the downtime
        # in 10 seconds from now the downtime starts
        print "scheduler_loop start", time.asctime()
        self.scheduler_loop(30, [[svc3, 0, 'OK']], do_sleep=True, sleep_time=1)
        print "scheduler_loop end  ", time.asctime()

        self.assert_(hasattr(svc3, 'in_maintenance'))
        self.assert_(len(self.sched.downtimes) == 1)
        try:
            print "........................................."
            print self.sched.downtimes[1]
            print "downtime starts", time.asctime(self.sched.downtimes[1].start_time)
            print "downtime ends  ", time.asctime(self.sched.downtimes[1].end_time)
        except:
            print "looks like there is no downtime"
            pass
        self.assert_(len(svc3.downtimes) == 1)
        self.assert_(svc3.downtimes[0] in self.sched.downtimes.values())
        self.assert_(svc3.in_scheduled_downtime)
        self.assert_(svc3.downtimes[0].fixed)
        self.assert_(svc3.downtimes[0].is_in_effect)
        self.assert_(not svc3.downtimes[0].can_be_deleted)
        self.assert_(svc3.in_maintenance == svc3.downtimes[0].id)

        #
        # now the downtime should expire...
        # we already have 20 seconds (after 10 seconds of startup).
        # the downtime is 120 seconds long.
        # run the remaining 100 seconds plus 5 seconds just to be sure
        self.scheduler_loop(105, [[svc3, 0, 'OK']], do_sleep=True, sleep_time=1)

        self.assert_(len(self.sched.downtimes) == 0)
        self.assert_(len(svc3.downtimes) == 0)
        self.assert_(not svc3.in_scheduled_downtime)
        self.assert_(svc3.in_maintenance is None)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_missing_cariarereturn
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_missing_cariarereturn.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "TEST")
        self.assert_(svc is not None)
        self.assert_(len(svc.checks_in_progress) >= 1)
        print svc.checks_in_progress[0].command
        self.assert_(svc.checks_in_progress[0].command == 'plugins/nothing BLABLA')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_missing_object_value
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestMissingObjectValue(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_missing_object_value.cfg')

    def test_missing_object_value(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #

        print "Get the hosts and services"
        now = time.time()
        host = self.conf.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.conf.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.conf.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        # The service is mising a value for active_check_enabled, it's an error.
        self.assert_(svc.is_correct() == False)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_missing_timeperiod
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestMissingTimeperiod(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_missing_timeperiod.cfg')

    def test_dummy(self):
        self.assert_(not self.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_modulemanager
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *

time_hacker.set_real_time()


class TestModuleManager(ShinkenTest):
    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    # Try to see if the module manager can manage modules
    def test_modulemanager(self):
        mod = Module({'module_name': 'DummyExternal', 'module_type': 'dummy_broker_external'})
        self.modulemanager = ModulesManager('broker', modules_dir, [])
        self.modulemanager.set_modules([mod])
        self.modulemanager.load_and_init()
        # And start external ones, like our LiveStatus
        self.modulemanager.start_external_instances()
        print "I correctly loaded the modules: %s " % ([inst.get_name() for inst in self.modulemanager.instances])

        print "*** First kill ****"
        # Now I will try to kill the livestatus module
        ls = self.modulemanager.instances[0]
        ls._BaseModule__kill()

        time.sleep(1)
        print "Check alive?"
        print "Is alive?", ls.process.is_alive()
        # Should be dead
        self.assert_(not ls.process.is_alive())
        self.modulemanager.check_alive_instances()
        self.modulemanager.try_to_restart_deads()

        # In fact it's too early, so it won't do it

        # Here the inst should still be dead
        print "Is alive?", ls.process.is_alive()
        self.assert_(not ls.process.is_alive())

        # So we lie
        ls.last_init_try = -5
        self.modulemanager.check_alive_instances()
        self.modulemanager.try_to_restart_deads()

        # In fact it's too early, so it won't do it

        # Here the inst should be alive again
        print "Is alive?", ls.process.is_alive()
        self.assert_(ls.process.is_alive())

        # should be nothing more in to_restart of
        # the module manager
        self.assert_(self.modulemanager.to_restart == [])

        # Now we look for time restart so we kill it again
        ls._BaseModule__kill()
        time.sleep(1)
        self.assert_(not ls.process.is_alive())

        # Should be too early
        self.modulemanager.check_alive_instances()
        self.modulemanager.try_to_restart_deads()
        print "Is alive or not", ls.process.is_alive()
        self.assert_(not ls.process.is_alive())
        # We lie for the test again
        ls.last_init_try = -5
        self.modulemanager.check_alive_instances()
        self.modulemanager.try_to_restart_deads()

        # Here the inst should be alive again
        print "Is alive?", ls.process.is_alive()
        self.assert_(ls.process.is_alive())

        # And we clear all now
        print "Ask to die"
        self.modulemanager.stop_all()
        print "Died"



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_module_on_module
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestModuleOnModule(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_module_on_module.cfg')

    def test_module_on_module(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        mod1 = self.sched.conf.modules.find_by_name("Simple-log")
        self.assert_(mod1 is not None)
        print "Got module", mod1.get_name()
        mod_sub = self.sched.conf.modules.find_by_name("ToNdodb_Mysql")
        self.assert_(mod_sub is not None)
        print "Got sub module", mod_sub.get_name()
        self.assert_(mod_sub in mod1.modules)
        self.assert_(mod_sub.modules == [])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_multiple_not_hostgroups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestMultipleNotHG(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_multiple_not_hostgroups.cfg')

    def test_dummy(self):

        for s in self.sched.services:
            print "SERVICES", s.get_full_name()
        
        svc = self.sched.services.find_srv_by_name_and_hostname("hst_in_BIG", "THE_SERVICE")
        self.assert_(svc is not None)

        svc = self.sched.services.find_srv_by_name_and_hostname("hst_in_IncludeLast", "THE_SERVICE")
        self.assert_(svc is not None)

        svc = self.sched.services.find_srv_by_name_and_hostname("hst_in_NotOne", "THE_SERVICE")
        self.assert_(svc is None)

        svc = self.sched.services.find_srv_by_name_and_hostname("hst_in_NotTwo", "THE_SERVICE")
        self.assert_(svc is None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_multi_attribute
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test multi valued attribute feature.
#

import re
from shinken_test import unittest, ShinkenTest


class TestMultiVuledAttributes(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_multi_attribute.cfg')

    def test_multi_valued_attributes(self):
        hst1 = self.sched.hosts.find_by_name("test_host_01")
        srv1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv1")
        self.assert_(hst1 is not None)
        self.assert_(srv1 is not None)

        # inherited parameter
        self.assert_(hst1.active_checks_enabled is True)
        self.assert_(srv1.active_checks_enabled is True)

        # non list parameter (only the last value set should remain)
        self.assert_(hst1.max_check_attempts == 3)
        self.assert_(srv1.max_check_attempts == 3)

        # list parameter (all items should appear in the order they are defined)
        self.assert_(hst1.notification_options == ['+1', 's', 'f', 'r', 'u', 'd'])
        self.assert_(srv1.notification_options == ['+1', 's', 'f', 'r', 'c', 'u', 'w'])


class TestConfigBroken(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_multi_attribute_broken.cfg')

    def test_multi_valued_attribute_errors(self):
        self.assert_(not self.conf.conf_is_correct)

        # Get the arbiter's log broks
        [b.prepare() for b in self.broks.values()]
        logs = [b.data['log'] for b in self.broks.values() if b.type == 'log']

        self.assert_(len([log for log in logs if re.search(r'no support for _ syntax in multiple valued attributes', log)]) == 1)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_multi_hostgroups_def
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_multi_hostgroups_def.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("will crash")
        self.assert_(host is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("will crash", "Crash")
        self.assert_(svc is not None)

        grp = self.sched.servicegroups.find_by_name("Crashed")
        self.assert_(grp is not None)
        self.assert_(svc in grp.members)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nested_hostgroups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNestedHostgroups(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_nested_hostgroups.cfg')

    # We got the service "NestedService" apply in High level
    # group. And this one got a sub group, low one. each got ONE
    # Host, so we must have this servie on both.
    def test_lookup_nested_hostgroups(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        router = self.sched.hosts.find_by_name("test_router_0")
        hg_high = self.sched.conf.hostgroups.find_by_name('high_level')
        self.assert_(hg_high is not None)
        self.assert_(host in hg_high.members)
        self.assert_(router in hg_high.members)
        hg_low = self.sched.conf.hostgroups.find_by_name('low_level')
        self.assert_(hg_low is not None)
        self.assert_(host in hg_low.members)
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "NestedService")
        self.assert_(svc1 is not None)
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "NestedService")
        self.assert_(svc2 is not None)

        # And now look for the service testHostToGroup apply on the group
        # high_level, and the host test_host_2 should be on it, so it must have
        # this service too
        host2 = self.sched.hosts.find_by_name("test_host_2")
        self.assert_(host2 in hg_high.members)
        svc3 = self.sched.services.find_srv_by_name_and_hostname("test_host_2", "testHostToGroup")
        self.assert_(svc3 is not None)

        # And same with an host in the low_group, should have it too
        host3 = self.sched.hosts.find_by_name("test_host_3")
        self.assert_(host3 in hg_high.members)
        svc4 = self.sched.services.find_srv_by_name_and_hostname("test_host_3", "testHostToGroup")
        self.assert_(svc4 is not None)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nocontacts
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNoContact(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_nocontacts.cfg')

    # Seems that Nagios allow non contacts elements, just warning
    # and not error. Should do the same.
    def test_nocontact(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        self.assert_(host.contacts == [])
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        self.assert_(svc.contacts == [])
        self.assert_(self.sched.conf.is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nohostsched
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestHostspecialSched(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_nohostsched.cfg')

    # The hosts can have no check_period nor check_interval.
    # It's valid, and say: 24x7 and 5min interval in fact.
    def test_nohostsched(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("moncul")
        self.assert_(host is not None)
        print "check", host.next_chk
        print "Check in", host.next_chk - now
        self.assert_(host.next_chk - now < 301)
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        print "Loop"
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')
        # Reschedule the host as a normal way
        host.schedule()
        print "Final", host.next_chk, host.in_checking
        print "Next check?", host.next_chk - now
        print "Next check should be still < 300", host.next_chk - now
        self.assert_(host.next_chk - now < 301)
        # but in 5min in fact, so more than 290,
        # something like 299.0
        self.assert_(host.next_chk - now > 290)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_notifications
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


#
# This file is used to test host- and service-downtimes.
#

import time

from shinken_test import unittest, ShinkenTest


class TestNotif(ShinkenTest):

    def test_continuous_notifications(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)
        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        #self.show_and_clear_actions()
        self.show_actions()
        print svc.notifications_in_progress
        for n in svc.notifications_in_progress.values():
            print n
        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # notification_number is already sent. the next one has been scheduled
        # and is waiting for notification_interval to pass. so the current
        # number is 2
        self.assert_(svc.current_notification_number == 1)
        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 5 x BAD repeat -------------------------------------"
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "cnn and cur", cnn, svc.current_notification_number
        self.assert_(svc.current_notification_number > cnn)
        cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn
        self.assert_(svc.current_notification_number > cnn)
        #--------------------------------------------------------------
        # 2 cycles = 2 minutes = 2 new notifications
        #--------------------------------------------------------------
        cnn = svc.current_notification_number
        self.scheduler_loop(2, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn
        self.assert_(svc.current_notification_number > cnn)
        #--------------------------------------------------------------
        # 2 cycles = 2 minutes = 2 new notifications (theoretically)
        # BUT: test_contact filters notifications
        # we do not raise current_notification_number if no mail was sent
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] DISABLE_CONTACT_SVC_NOTIFICATIONS;test_contact" % now
        self.sched.run_external_command(cmd)
        cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == cnn)
        #--------------------------------------------------------------
        # again a normal cycle
        # test_contact receives his mail
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] ENABLE_CONTACT_SVC_NOTIFICATIONS;test_contact" % now
        self.sched.run_external_command(cmd)
        #cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn
        self.assert_(svc.current_notification_number == cnn + 1)
        #--------------------------------------------------------------
        # now recover. there must be no scheduled/inpoller notification
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        self.assert_(svc.current_notification_number == 0)

    def test_continuous_notifications_delayed(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001  # and send imediatly then

        svc.first_notification_delay = 0.1  # set 6s for first notif delay
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=1)
        #-----------------------------------------------------------------
        # initialize with a good check. there must be no pending notification
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=1)
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        self.assert_(svc.current_notification_number == 0)
        #-----------------------------------------------------------------
        # check fails and enters soft state.
        # there must be no notification, only the event handler
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 1, 'BAD']], do_sleep=True, sleep_time=1)
        self.assert_(self.count_actions() == 1)
        print time.time()
        print  svc.last_time_warning, svc.last_time_critical, svc.last_time_unknown, svc.last_time_ok
        last_time_not_ok = svc.last_time_non_ok_or_up()
        deadline = svc.last_time_non_ok_or_up() + svc.first_notification_delay * svc.__class__.interval_length
        #-----------------------------------------------------------------
        # check fails again and enters hard state.
        # now there is a (scheduled for later) notification and an event handler
        # current_notification_number is still 0, until notifications
        # have actually been sent
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == 0)
        #-----------------------------------------------------------------
        # repeat bad checks during the delay time
        # there is 1 action which is the scheduled notification
        #-----------------------------------------------------------------
        loop = 0
        while deadline > time.time():
            loop += 1
            self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
            self.show_and_clear_logs()
            self.show_actions()
            print deadline - time.time()
            ###self.assert_(self.count_actions() == 1)
        #-----------------------------------------------------------------
        # now the delay period is over and the notification can be sent
        # with the next bad check
        # there is 1 action, the notification (
        # 1 notification was sent, so current_notification_number is 1
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=1)
        print "Counted actions", self.count_actions()
        self.assert_(self.count_actions() == 2)
        # 1 master, 1 child
        self.assert_(svc.current_notification_number == 1)
        self.show_actions()
        self.assert_(len(svc.notifications_in_progress) == 1)  # master is zombieand removed_from_in_progress
        self.show_logs()
        self.assert_(self.log_match(1, 'SERVICE NOTIFICATION.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_actions()
        #-----------------------------------------------------------------
        # relax with a successful check
        # there are 2 actions, one notification and one eventhandler
        # current_notification_number was reset to 0
        #-----------------------------------------------------------------
        self.scheduler_loop(2, [[svc, 0, 'GOOD']], do_sleep=True, sleep_time=1)
        self.assert_(self.log_match(1, 'SERVICE ALERT.*;OK;'))
        self.assert_(self.log_match(2, 'SERVICE EVENT HANDLER.*;OK;'))
        self.assert_(self.log_match(3, 'SERVICE NOTIFICATION.*;OK;'))
        # evt reap 2 loops
        self.assert_(svc.current_notification_number == 0)
        self.assert_(len(svc.notifications_in_progress) == 0)
        self.assert_(len(svc.notified_contacts) == 0)
        #self.assert_(self.count_actions() == 2)
        self.show_and_clear_logs()
        self.show_and_clear_actions()

    def test_continuous_notifications_delayed_recovers_fast(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.first_notification_delay = 5
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        #-----------------------------------------------------------------
        # initialize with a good check. there must be no pending notification
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        self.assert_(svc.current_notification_number == 0)
        #-----------------------------------------------------------------
        # check fails and enters soft state.
        # there must be no notification, only the event handler
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 1, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(self.count_actions() == 1)
        #-----------------------------------------------------------------
        # check fails again and enters hard state.
        # now there is a (scheduled for later) notification and an event handler
        # current_notification_number is still 0 (will be raised when
        # a notification is actually sent)
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(self.count_actions() == 2)
        self.assert_(svc.current_notification_number == 0)
        #-----------------------------------------------------------------
        # repeat bad checks during the delay time
        # but only one time. we don't want to reach the deadline
        # there is one action: the pending notification
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(self.count_actions() == 1)
        #-----------------------------------------------------------------
        # relax with a successful check
        # there is 1 action, the eventhandler.
        #-----------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.log_match(1, 'SERVICE ALERT.*;OK;'))
        self.assert_(self.log_match(2, 'SERVICE EVENT HANDLER.*;OK;'))
        self.assert_(not self.log_match(3, 'SERVICE NOTIFICATION.*;OK;'))
        self.show_actions()
        self.assert_(len(svc.notifications_in_progress) == 0)
        self.assert_(len(svc.notified_contacts) == 0)
        self.assert_(self.count_actions() == 1)
        self.show_and_clear_logs()
        self.show_and_clear_actions()


    def test_host_in_downtime_or_down_service_critical(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP'], [svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.assert_(svc.current_notification_number == 0)
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[host, 0, 'UP'], [svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'SERVICE ALERT.*;CRITICAL;SOFT'))
        self.assert_(self.log_match(2, 'SERVICE EVENT HANDLER.*;CRITICAL;SOFT'))
        self.assert_(self.log_match(3, 'SERVICE ALERT.*;CRITICAL;HARD'))
        self.assert_(self.log_match(4, 'SERVICE EVENT HANDLER.*;CRITICAL;HARD'))
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION.*;CRITICAL;'))
        self.assert_(svc.current_notification_number == 1)
        self.clear_logs()
        self.clear_actions()
        #--------------------------------------------------------------
        # reset host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP'], [svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.assert_(svc.current_notification_number == 0)
        duration = 300
        now = time.time()
        # fixed downtime valid for the next 5 minutes
        cmd = "[%lu] SCHEDULE_HOST_DOWNTIME;test_host_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)
        #--------------------------------------------------------------
        # service reaches hard;2
        # no notificatio
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[host, 0, 'UP'], [svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;DOWNTIMESTART'))
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL;'))
        self.show_and_clear_logs()
        self.show_and_clear_actions()

    def test_only_notified_contacts_notifications(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # To make tests quicker we make notifications send very quickly
        svc.notification_interval = 0.001

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # We want the contact to do not have a mail, so we remove tyhe 'u'
        test_contact = self.sched.contacts.find_by_name('test_contact')
        for nw in test_contact.notificationways:
            nw.service_notification_options.remove('u')

        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP']], do_sleep=True, sleep_time=0.1)
        print "- 1 x OK -------------------------------------"
        self.scheduler_loop(1, [[svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)

        self.assert_(svc.current_notification_number == 0)
        #--------------------------------------------------------------
        # service reaches soft;1
        # there must not be any notification
        #--------------------------------------------------------------
        print "- 1 x BAD get soft -------------------------------------"
        self.scheduler_loop(1, [[svc, 3, 'UNKNOWN']], do_sleep=True, sleep_time=0.1)
        # check_notification: not (soft)
        print "---current_notification_number", svc.current_notification_number
        print "Contact we notified", svc.notified_contacts
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        print "- 1 x BAD get hard -------------------------------------"
        self.scheduler_loop(1, [[svc, 3, 'UNKNOWN']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        #self.show_and_clear_actions()
        print "TOTO2"
        self.show_actions()
        print "notif in progress", svc.notifications_in_progress
        for n in svc.notifications_in_progress.values():
            print "TOTO", n.__dict__
        # check_notification: yes (hard)
        print "---current_notification_number", svc.current_notification_number
        # The contact refuse our notification, so we are still at 0
        self.assert_(svc.current_notification_number == 0)
        print "---------------------------------1st round with a hard"
        print "find a way to get the number of the last reaction"
        cnn = svc.current_notification_number
        print "- 5 x BAD repeat -------------------------------------"
        self.scheduler_loop(1, [[svc, 3, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "cnn and cur", cnn, svc.current_notification_number

        cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 3, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn

        #--------------------------------------------------------------
        # 2 cycles = 2 minutes = 2 new notifications
        #--------------------------------------------------------------
        cnn = svc.current_notification_number
        self.scheduler_loop(2, [[svc, 3, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn

        #--------------------------------------------------------------
        # 2 cycles = 2 minutes = 2 new notifications (theoretically)
        # BUT: test_contact filters notifications
        # we do not raise current_notification_number if no mail was sent
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] DISABLE_CONTACT_SVC_NOTIFICATIONS;test_contact" % now
        self.sched.run_external_command(cmd)
        cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 3, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        self.assert_(svc.current_notification_number == cnn)
        #--------------------------------------------------------------
        # again a normal cycle
        # test_contact receives his mail
        #--------------------------------------------------------------
        now = time.time()
        cmd = "[%lu] ENABLE_CONTACT_SVC_NOTIFICATIONS;test_contact" % now
        self.sched.run_external_command(cmd)
        #cnn = svc.current_notification_number
        self.scheduler_loop(1, [[svc, 3, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_and_clear_logs()
        self.show_actions()
        print "svc.current_notification_number, cnn", svc.current_notification_number, cnn
        #self.assert_(svc.current_notification_number == cnn + 1)
        #--------------------------------------------------------------
        # now recover. there must be no scheduled/inpoller notification
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[svc, 0, 'GOOD']], do_sleep=True, sleep_time=0.1)

        # I do not want a notification of a recovery because
        # the user did not have the notif first!
        self.assert_(not self.any_log_match('notify-service'))
        self.show_and_clear_logs()
        self.show_and_clear_actions()
        self.assert_(svc.current_notification_number == 0)

    def test_svc_in_dt_and_crit_and_notif_interval_0(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.notification_interval = 0
        host.notification_options = 'c'
        svc.notification_options = 'c'

        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'UP'], [svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        self.assert_(svc.current_notification_number == 0)
        #--------------------------------------------------------------
        # service reaches hard;2
        # a notification must have been created
        # notification number must be 1
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[host, 0, 'UP'], [svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.show_logs()
        self.show_actions()
        self.assert_(self.log_match(1, 'SERVICE ALERT.*;CRITICAL;SOFT'))
        self.assert_(self.log_match(2, 'SERVICE EVENT HANDLER.*;CRITICAL;SOFT'))
        self.assert_(self.log_match(3, 'SERVICE ALERT.*;CRITICAL;HARD'))
        self.assert_(self.log_match(4, 'SERVICE EVENT HANDLER.*;CRITICAL;HARD'))
        self.assert_(self.log_match(5, 'SERVICE NOTIFICATION.*;CRITICAL;'))
        self.assert_(svc.current_notification_number == 1)
        self.clear_logs()
        self.clear_actions()
        #--------------------------------------------------------------
        # reset host/service state
        #--------------------------------------------------------------
        #self.scheduler_loop(1, [[host, 0, 'UP'], [svc, 0, 'OK']], do_sleep=True, sleep_time=0.1)
        #self.assert_(svc.current_notification_number == 0)
        duration = 2
        now = time.time()
        # fixed downtime valid for the next 5 minutes
        cmd = "[%lu] SCHEDULE_SVC_DOWNTIME;test_host_0;test_ok_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + duration, duration)
        self.sched.run_external_command(cmd)
        #--------------------------------------------------------------
        # service reaches hard;2
        # no notificatio
        #--------------------------------------------------------------
        self.scheduler_loop(2, [[host, 0, 'UP'], [svc, 2, 'BAD']], do_sleep=True, sleep_time=0.1)
        self.assert_(self.any_log_match('SERVICE DOWNTIME ALERT.*;STARTED'))
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL;'))
        # To get out of the DT.
        self.scheduler_loop(2, [[host, 0, 'UP'], [svc, 2, 'BAD']], do_sleep=True, sleep_time=2)
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL;'))
        self.assert_(svc.current_notification_number == 1)
        self.show_and_clear_logs()
        self.show_and_clear_actions()

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_notification_warning
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *

from shinken.notification import Notification


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def test_raise_warning_on_notification_errors(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        cmd = "/error/pl"
        # Create a dummy notif
        n = Notification('PROBLEM', 'scheduled', 'BADCOMMAND', cmd, host, None, 0)
        n.execute()
        time.sleep(0.2)
        if n.status is not 'done':
            n.check_finished(8000)
        print n.__dict__
        self.sched.actions[n.id] = n
        self.sched.put_results(n)
        # Should have raised something like "Warning: the notification command 'BADCOMMAND' raised an error (exit code=2): '[Errno 2] No such file or directory'"
        # Ok, in HUDSON, we got a problem here. so always run with a shell run before release please
        if os.environ.get('HUDSON_URL', None):
            return

        self.assert_(self.any_log_match(u'.*BADCOMMAND.*') or self.any_log_match('.*BADCOMMAND.*'))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_notifway
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import time

from shinken_test import unittest, ShinkenTest


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_notif_way.cfg')

    def test_contact_def(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the contact"
        now = time.time()
        contact = self.sched.contacts.find_by_name("test_contact")
        print "The contact", contact.__dict__

        print "All notification Way:"
        for nw in self.sched.notificationways:
            print "\t", nw.notificationway_name

        email_in_day = self.sched.notificationways.find_by_name('email_in_day')
        self.assert_(email_in_day in contact.notificationways)
        email_s_cmd = email_in_day.service_notification_commands.pop()
        email_h_cmd = email_in_day.host_notification_commands.pop()

        sms_the_night = self.sched.notificationways.find_by_name('sms_the_night')
        self.assert_(sms_the_night in contact.notificationways)
        sms_s_cmd = sms_the_night.service_notification_commands.pop()
        sms_h_cmd = sms_the_night.host_notification_commands.pop()

        # And check the criticity values
        self.assert_(email_in_day.min_business_impact == 0)
        self.assert_(sms_the_night.min_business_impact == 5)

        print "Contact notification way(s):"
        for nw in contact.notificationways:
            print "\t", nw.notificationway_name
            for c in nw.service_notification_commands:
                print "\t\t", c.get_name()

        contact_simple = self.sched.contacts.find_by_name("test_contact_simple")
        # It's the created notifway for this simple contact
        test_contact_simple_inner_notificationway = self.sched.notificationways.find_by_name("test_contact_simple_inner_notificationway")
        print "Simple contact"
        for nw in contact_simple.notificationways:
            print "\t", nw.notificationway_name
            for c in nw.service_notification_commands:
                print "\t\t", c.get_name()
        self.assert_(test_contact_simple_inner_notificationway in contact_simple.notificationways)

        # we take as criticity a huge value from now
        huge_criticity = 5

        # Now all want* functions
        # First is ok with warning alerts
        self.assert_(email_in_day.want_service_notification(now, 'WARNING', 'PROBLEM', huge_criticity) == True)

        # But a SMS is now WAY for warning. When we sleep, we wake up for critical only guy!
        self.assert_(sms_the_night.want_service_notification(now, 'WARNING', 'PROBLEM', huge_criticity) == False)

        # Same with contacts now
        # First is ok for warning in the email_in_day nw
        self.assert_(contact.want_service_notification(now, 'WARNING', 'PROBLEM', huge_criticity) == True)
        # Simple is not ok for it
        self.assert_(contact_simple.want_service_notification(now, 'WARNING', 'PROBLEM', huge_criticity) == False)

        # Then for host notification
        # First is ok for warning in the email_in_day nw
        self.assert_(contact.want_host_notification(now, 'FLAPPING', 'PROBLEM', huge_criticity) == True)
        # Simple is not ok for it
        self.assert_(contact_simple.want_host_notification(now, 'FLAPPING', 'PROBLEM', huge_criticity) == False)

        # And now we check that we refuse SMS for a low level criticity
        # I do not want to be awaken by a dev server! When I sleep, I sleep!
        # (and my wife will kill me if I do...)

        # We take the EMAIL test because SMS got the night ony, so we take a very low value for criticity here
        self.assert_(email_in_day.want_service_notification(now, 'WARNING', 'PROBLEM', -1) == False)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_notif_macros
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNotifMacros(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_notif_macros.cfg')

    def test_notif_macro(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        #now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        # Should got a notif here
        self.assert_(len(svc.notifications_in_progress.values()) > 0)
        #n = svc.notifications_in_progress.values()[0]
        got_notif = False
        r = 'plugins/macros_check.sh "_HOSTADMINEMAIL=" "monemail@masociete.domain" ' \
            '"_HOSTCOMPANYNAME=" "masociete" "_CONTACTTESTC=" "sender@masociete.domain" "toto"'
        for a in self.sched.actions.values():
            print a.command
            if a.command == r:
                got_notif = True
        self.assert_(got_notif)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_notif_too_much
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNotifTooMuch(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_notif_too_much.cfg')

    # The goal of this test is to check if we manage this case:
    # 2 notif ways on one contact. One notif ways should activate, not the other
    # for one timeperiod
    def test_notif_too_much(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        test_contact = self.sched.contacts.find_by_name('test_contact')
        self.assert_(test_contact is not None)
        self.scheduler_loop(1, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        self.scheduler_loop(1, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])

        # We should NOT see a send for the notify-service2 call because it's the good contact
        # but NOT the good period for this notifways. So 24x7 ok, not the never :)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;notify-service'))
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;notify-service2'))




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_not_execute_host_check
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNoHostCheck(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_not_execute_host_check.cfg')

    # We must look taht host checks are disable, and services ones are running
    def test_no_host_check(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        print host.checks_in_progress
        self.assert_(len(host.checks_in_progress) == 0)
        #
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        print svc.checks_in_progress
        self.assert_(len(svc.checks_in_progress) != 0)

        # Now launch passive checks
        cmd = "[%lu] PROCESS_HOST_CHECK_RESULT;test_host_0;1;bobo" % now
        self.sched.run_external_command(cmd)

        self.scheduler_loop(2, [])

        print "Output", host.output
        self.assert_(host.output == 'bobo')

        # Now disable passive host check
        cmd = "[%lu] STOP_ACCEPTING_PASSIVE_HOST_CHECKS" % now
        self.sched.run_external_command(cmd)

        # And now run a new command
        cmd = "[%lu] PROCESS_HOST_CHECK_RESULT;test_host_0;1;bobo2" % now
        self.sched.run_external_command(cmd)

        self.scheduler_loop(2, [])

        # This should NOT change this time
        print "Output", host.output
        self.assert_(host.output == 'bobo')



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_not_hostname
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import time

from shinken_test import unittest, ShinkenTest


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_not_hostname.cfg')

    def test_not_hostname_in_service(self):
        # The service is apply with a host_group on "test_host_0","test_host_1"
        # but have a host_name with !"test_host_1" so there will be just "test_host_0"
        # defined on the end
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        svc_not = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_ok_0")
        # Check if the service for the good host is here
        self.assert_(svc is not None)
        # check if the service for the not one (!) is not here
        self.assert_(svc_not is None)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_no_broker_in_realm_warning
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestWarnAboutNoBrokerInRealm(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_no_broker_in_realm_warning.cfg')

    def test_no_broker_in_realm_warning(self):
        dist = self.conf.realms.find_by_name("Distant")
        self.assert_(dist is not None)
        sched = self.conf.schedulers.find_by_name("Scheduler-distant")
        self.assert_(sched is not None)
        self.assert_(len(sched.realm.potential_brokers) == 0)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_no_event_handler_during_downtime
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNoEventHandlerDuringDowntime(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_no_event_handler_during_downtime.cfg')

    def test_no_event_handler_during_downtime(self):

        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        now = time.time()
        # downtime valid for the next 2 minutes
        cmd = "[%lu] SCHEDULE_SVC_DOWNTIME;test_host_0;test_ok_0;%d;%d;1;0;%d;lausser;blablub" % (now, now, now + 3600, 3600)
        self.sched.run_external_command(cmd)

        # Make a loop to activate the downtime
        self.scheduler_loop(1, [])
        # We check so the downtime is really active
        self.assert_(self.any_log_match('SERVICE DOWNTIME ALERT.*;STARTED'))

        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'OK | value1=0 valu\
e2=0']])

        # There should be NO event handlers during a downtime!
        self.assert_(not self.any_log_match('SERVICE EVENT HANDLER.*;CRITICAL'))



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_no_host_template
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNoHostTemplate(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_no_host_template.cfg')

    def test_host_without_a_template(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("my_host")
        b = host.is_correct()
        self.assert_(b)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_no_notification_period
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNoNotificationPeriod(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_no_notification_period.cfg')

    # no notification period should do a 24x7 like period
    # so a None, but always valid in create_notification
    def test_no_notification_period(self):
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        # Now get bad :)
        self.scheduler_loop(2, [[svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(svc.notification_period is None)
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))

        # Now for the host :)
        self.scheduler_loop(5, [[host, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.notification_period is None)
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;DOWN'))



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_nullinheritance
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestNullInheritance(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_nullinheritance.cfg')

    # We search to see if null as value really delete the inheritance
    # of a property
    def test_null_inheritance(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        self.assert_(svc.icon_image == '')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_objects_and_notifways
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestObjectsAndNotifWays(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_objects_and_notifways.cfg')

    # We got strange "objects" for some contacts property when we are using notif ways
    # and asking for  broks. Search why
    def test_dummy(self):
        c_normal = self.sched.contacts.find_by_name("test_contact")
        self.assert_(c_normal is not None)
        c_nw = self.sched.contacts.find_by_name("test_contact_nw")
        self.assert_(c_nw is not None)

        b = c_normal.get_initial_status_brok()
        b.prepare()
        print "B normal", b
        self.assert_(b.data['host_notification_options'] == u'd,u,r,f,s')
        b2 = c_nw.get_initial_status_brok()
        b2.prepare()
        print "B nw", b2
        self.assert_(b2.data['host_notification_options'] == u'')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_obsess
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test acknowledge of problems
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_obsess.cfg')

    def test_ocsp(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.assert_(svc.obsess_over_service)
        self.assert_(svc.__class__.obsess_over)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 1)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 1)

        now = time.time()
        cmd = "[%lu] STOP_OBSESSING_OVER_SVC;test_host_0;test_ok_0" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.assert_(not svc.obsess_over_service)
        self.assert_(svc.__class__.obsess_over)
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 0)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 0)

        now = time.time()
        cmd = "[%lu] START_OBSESSING_OVER_SVC;test_host_0;test_ok_0" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.assert_(svc.obsess_over_service)
        self.assert_(svc.__class__.obsess_over)
        self.sched.run_external_command(cmd)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 1)
        self.scheduler_loop(1, [[svc, 0, 'OK']])
        self.assert_(self.count_actions() == 1)

        now = time.time()
        cmd = "[%lu] START_OBSESSING_OVER_SVC_CHECKS" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.assert_(svc.obsess_over_service)
        self.assert_(svc.__class__.obsess_over)

        now = time.time()
        cmd = "[%lu] STOP_OBSESSING_OVER_SVC_CHECKS" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.assert_(svc.obsess_over_service)
        self.assert_(not svc.__class__.obsess_over)

    def test_ochp(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.scheduler_loop(1, [[host, 0, 'OK']])
        self.show_actions()
        self.assert_(self.count_actions() == 1)
        self.scheduler_loop(1, [[router, 0, 'OK']])
        self.show_actions()
        print "host", host.obsess_over
        print "rout", router.obsess_over
        print "host", host.obsess_over_host
        print "rout", router.obsess_over_host
        self.assert_(self.count_actions() == 0)
        self.assert_(host.obsess_over_host)
        self.assert_(not router.obsess_over_host)
        # the router does not obsess (host definition)
        # but it's class does (shinken.cfg)
        self.assert_(router.__class__.obsess_over)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_on_demand_event_handlers
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test acknowledge of problems
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_on_demand_event_handlers.cfg')

    def test_on_demand_eh(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        self.assert_(svc.event_handler_enabled == False)

        self.scheduler_loop(5, [[svc, 2, 'CRITICAL']])
        # We should NOT see any event hnalder here :)
        self.assert_(not self.any_log_match('SERVICE EVENT HANDLER'))
        print "MY Actions", self.sched.actions

        # And now we ask for a launch in manual
        now = time.time()
        cmd = "[%lu] LAUNCH_SVC_EVENT_HANDLER;test_host_0;test_ok_0" % now
        self.sched.run_external_command(cmd)
        self.sched.get_new_actions()
        self.worker_loop()
        self.assert_(self.any_log_match('SERVICE EVENT HANDLER'))




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_orphaned
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestOrphaned(ShinkenTest):
    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    def test_orphaned(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        #self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        #self.assert_(host.state == 'UP')
        #self.assert_(host.state_type == 'HARD')

        svc.schedule()
        print svc.actions
        self.sched.get_new_actions()
        for c in self.sched.checks.values():
            print c
            # simulate a orphaned situation
            c.t_to_go = now - 301
            c.status = 'inpoller'

        self.sched.check_orphaned()

        # Should be available to poller now :)
        for c in self.sched.checks.values():
            self.assert_(c.status == 'scheduled')

        # And we correctly raise the log
        self.assert_(self.any_log_match('actions never came back for the satellite'))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_pack_hash_memory
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestPackHashMemory(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_pack_hash_memory.cfg')

    def setUp2(self):
        self.setup_with_file('etc/shinken_pack_hash_memory2.cfg')

    def test_pack_hash_memory(self):
        packs = {0: set(), 1: set()}
        for h in self.sched.hosts:
            print 'First pass: ', h.get_name(), h.pack_id, '\n'
            packs[h.pack_id].add(h)

        nb_same = 0

        # Reset IDs
        SchedulerLink.id = 0

        self.setUp()
        for h in self.sched.hosts:
            same_pack = h.get_name() in [i.get_name() for i in packs[h.pack_id]]
            print 'Is in the same pack??', h.get_name(), h.pack_id, ':', same_pack
            if same_pack:
                nb_same += 1

        # Should have nearly all in the same pack
        self.assert_(nb_same >= 100)
        print 'Total same', nb_same


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_parse_perfdata
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
from shinken.misc.perfdata import Metric, PerfDatas


class TestParsePerfdata(ShinkenTest):
    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_parse_perfdata.cfg')

    def test_parsing_perfdata(self):
        s = 'ramused=1009MB;;;0;1982 swapused=540MB;;;0;3827 memused=1550MB;2973;3964;0;5810'
        s = 'ramused=1009MB;;;0;1982'
        m = Metric(s)
        self.assert_(m.name == 'ramused')
        self.assert_(m.value == 1009)
        self.assert_(m.uom == 'MB')
        self.assert_(m.warning == None)
        self.assert_(m.critical == None)
        self.assert_(m.min == 0)
        self.assert_(m.max == 1982)

        s = 'ramused=90%;85;95;;'
        m = Metric(s)
        self.assert_(m.name == 'ramused')
        self.assert_(m.value == 90)
        self.assert_(m.uom == '%')
        self.assert_(m.warning == 85)
        self.assert_(m.critical == 95)
        self.assert_(m.min == 0)
        self.assert_(m.max == 100)

        s = 'ramused=1009MB;;;0;1982 swapused=540MB;;;; memused=90%'
        p = PerfDatas(s)
        p.metrics
        m = p['swapused']
        self.assert_(m.name == 'swapused')
        self.assert_(m.value == 540)
        self.assert_(m.uom == 'MB')
        self.assert_(m.warning == None)
        self.assert_(m.critical == None)
        self.assert_(m.min == None)
        self.assert_(m.max == None)

        m = p['memused']
        self.assert_(m.name == 'memused')
        self.assert_(m.value == 90)
        self.assert_(m.uom == '%')
        self.assert_(m.warning == None)
        self.assert_(m.critical == None)
        self.assert_(m.min == 0)
        self.assert_(m.max == 100)

        self.assert_(len(p) == 3)

        s = "'Physical Memory Used'=12085620736Bytes; 'Physical Memory Utilisation'=94%;80;90;"
        p = PerfDatas(s)
        p.metrics
        m = p['Physical Memory Used']
        self.assert_(m.name == 'Physical Memory Used')
        self.assert_(m.value == 12085620736)
        self.assert_(m.uom == 'Bytes')
        self.assert_(m.warning is None)
        self.assert_(m.critical is None)
        self.assert_(m.min is None)
        self.assert_(m.max is None)

        m = p['Physical Memory Utilisation']
        self.assert_(m.name == 'Physical Memory Utilisation')
        self.assert_(m.value == 94)
        self.assert_(m.uom == '%')
        self.assert_(m.warning == 80)
        self.assert_(m.critical == 90)
        self.assert_(m.min == 0)
        self.assert_(m.max == 100)

        s = "'C: Space'=35.07GB; 'C: Utilisation'=87.7%;90;95;"
        p = PerfDatas(s)
        p.metrics
        m = p['C: Space']
        self.assert_(m.name == 'C: Space')
        self.assert_(m.value == 35.07)
        self.assert_(m.uom == 'GB')
        self.assert_(m.warning is None)
        self.assert_(m.critical is None)
        self.assert_(m.min is None)
        self.assert_(m.max is None)

        m = p['C: Utilisation']
        self.assert_(m.name == 'C: Utilisation')
        self.assert_(m.value == 87.7)
        self.assert_(m.uom == '%')
        self.assert_(m.warning == 90)
        self.assert_(m.critical == 95)
        self.assert_(m.min == 0)
        self.assert_(m.max == 100)

        s = "time_offset-192.168.0.1=-7.22636468709e-05s;1;2;0;;"
        p = PerfDatas(s)
        m = p['time_offset-192.168.0.1']
        self.assert_(m.name == 'time_offset-192.168.0.1')
        self.assert_(m.value == -7.22636468709e-05)
        self.assert_(m.uom == 's')
        self.assert_(m.warning == 1)
        self.assert_(m.critical == 2)
        self.assert_(m.min == 0)
        self.assert_(m.max is None)


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_passive_pollers
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class GoodArbiter(ArbiterLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def do_not_run(self):
        pass


class GoodScheduler(SchedulerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def put_conf(self, conf):
        return True


class BadScheduler(SchedulerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()

    def have_conf(self, i):
        return False


class GoodPoller(PollerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadPoller(PollerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodReactionner(ReactionnerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadReactionner(ReactionnerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodBroker(BrokerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadBroker(BrokerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class TestPassivePoller(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_passive_pollers.cfg')

    def test_simple_passive_pollers(self):
        print "The dispatcher", self.dispatcher
        # dummy for the arbiter
        for a in self.conf.arbiters:
            a.__class__ = GoodArbiter
        print "Preparing schedulers"
        scheduler1 = self.conf.schedulers.find_by_name('scheduler-all-1')
        self.assert_(scheduler1 is not None)
        scheduler1.__class__ = GoodScheduler
        scheduler2 = self.conf.schedulers.find_by_name('scheduler-all-2')
        self.assert_(scheduler2 is not None)
        scheduler2.__class__ = BadScheduler

        # Poller 1 is normal, 2 and 3 are passives
        print "Preparing pollers"
        poller1 = self.conf.pollers.find_by_name('poller-all-1')
        self.assert_(poller1 is not None)
        poller1.__class__ = GoodPoller
        print poller1.__dict__
        self.assert_(poller1.passive == False)
        poller2 = self.conf.pollers.find_by_name('poller-all-2')
        self.assert_(poller2 is not None)
        poller2.__class__ = GoodPoller
        self.assert_(poller2.passive == True)
        poller3 = self.conf.pollers.find_by_name('poller-all-3')
        self.assert_(poller3 is not None)
        poller3.__class__ = GoodPoller
        self.assert_(poller3.passive == True)

        print "Preparing reactionners"
        reactionner1 = self.conf.reactionners.find_by_name('reactionner-all-1')
        self.assert_(reactionner1 is not None)
        reactionner1.__class__ = GoodReactionner
        reactionner2 = self.conf.reactionners.find_by_name('reactionner-all-2')
        self.assert_(reactionner2 is not None)
        reactionner2.__class__ = BadReactionner

        print "Preparing brokers"
        broker1 = self.conf.brokers.find_by_name('broker-all-1')
        self.assert_(broker1 is not None)
        broker1.__class__ = GoodBroker
        broker2 = self.conf.brokers.find_by_name('broker-all-2')
        self.assert_(broker2 is not None)
        broker2.__class__ = BadBroker

        # Ping all elements. Should have 1 as OK, 2 as
        # one bad attempt (3 max)
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 1)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 0)
        self.assert_(poller2.reachable == True)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 1)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 1)
        self.assert_(broker2.reachable == False)

        time.sleep(60)
        ### Now add another attempt, still alive, but attemp=2/3
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        #import pdb; pdb.set_trace()
        self.assert_(scheduler2.attempt == 2)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 0)
        self.assert_(poller2.reachable == True)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 2)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 2)
        self.assert_(broker2.reachable == False)

        time.sleep(60)
        ### Now we get BAD, We go DEAD for N2!
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == False)
        self.assert_(scheduler2.attempt == 3)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 0)
        self.assert_(poller2.reachable == True)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == False)
        self.assert_(reactionner2.attempt == 3)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == False)
        self.assert_(broker2.attempt == 3)
        self.assert_(broker2.reachable == False)

        # Now we check how we should dispatch confs
        self.dispatcher.check_dispatch()
        # the conf should not be in a good shape
        self.assert_(self.dispatcher.dispatch_ok == False)

        # Now we really dispatch them!
        self.dispatcher.dispatch()
        self.assert_(self.any_log_match('Dispatch OK of conf in scheduler scheduler-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to reactionner reactionner-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to poller poller-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to broker broker-all-1'))
        self.clear_logs()

        # And look if we really dispatch conf as we should
        for r in self.conf.realms:
            for cfg in r.confs.values():
                self.assert_(cfg.is_assigned == True)
                self.assert_(cfg.assigned_to == scheduler1)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_poller_addition
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class GoodArbiter(ArbiterLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def do_not_run(self):
        pass


class GoodScheduler(SchedulerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def have_conf(self, i):
        return True

    def put_conf(self, conf):
        return True


class BadScheduler(SchedulerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()

    def have_conf(self, i):
        return False


class GoodPoller(PollerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadPoller(PollerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodReactionner(ReactionnerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadReactionner(ReactionnerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class GoodBroker(BrokerLink):

    # To lie about satellites
    def ping(self):
        print "Dummy OK for", self.get_name()
        self.set_alive()

    def put_conf(self, conf):
        return True


class BadBroker(BrokerLink):
    def ping(self):
        print "Dummy bad ping", self.get_name()
        self.add_failed_check_attempt()


class TestPollerAddition(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_dispatcher.cfg')

    def test_simple_dispatch_and_addition(self):
        print "The dispatcher", self.dispatcher
        # dummy for the arbiter
        for a in self.conf.arbiters:
            a.__class__ = GoodArbiter
        print "Preparing schedulers"
        scheduler1 = self.conf.schedulers.find_by_name('scheduler-all-1')
        self.assert_(scheduler1 is not None)
        scheduler1.__class__ = GoodScheduler
        scheduler2 = self.conf.schedulers.find_by_name('scheduler-all-2')
        self.assert_(scheduler2 is not None)
        scheduler2.__class__ = BadScheduler

        print "Preparing pollers"
        poller1 = self.conf.pollers.find_by_name('poller-all-1')
        self.assert_(poller1 is not None)
        poller1.__class__ = GoodPoller
        poller2 = self.conf.pollers.find_by_name('poller-all-2')
        self.assert_(poller2 is not None)
        poller2.__class__ = BadPoller

        print "Preparing reactionners"
        reactionner1 = self.conf.reactionners.find_by_name('reactionner-all-1')
        self.assert_(reactionner1 is not None)
        reactionner1.__class__ = GoodReactionner
        reactionner2 = self.conf.reactionners.find_by_name('reactionner-all-2')
        self.assert_(reactionner2 is not None)
        reactionner2.__class__ = BadReactionner

        print "Preparing brokers"
        broker1 = self.conf.brokers.find_by_name('broker-all-1')
        self.assert_(broker1 is not None)
        broker1.__class__ = GoodBroker
        broker2 = self.conf.brokers.find_by_name('broker-all-2')
        self.assert_(broker2 is not None)
        broker2.__class__ = BadBroker

        # Ping all elements. Should have 1 as OK, 2 as
        # one bad attempt (3 max)
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 1)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 1)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 1)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 1)
        self.assert_(broker2.reachable == False)

        time.sleep(60)
        ### Now add another attempt, still alive, but attemp=2/3
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == True)
        self.assert_(scheduler2.attempt == 2)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == True)
        self.assert_(poller2.attempt == 2)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == True)
        self.assert_(reactionner2.attempt == 2)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == True)
        self.assert_(broker2.attempt == 2)
        self.assert_(broker2.reachable == False)

        time.sleep(60)
        ### Now we get BAD, We go DEAD for N2!
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(scheduler1.alive == True)
        self.assert_(scheduler1.attempt == 0)
        self.assert_(scheduler1.reachable == True)
        # still alive, just unreach
        self.assert_(scheduler2.alive == False)
        self.assert_(scheduler2.attempt == 3)
        self.assert_(scheduler2.reachable == False)

        # and others satellites too
        self.assert_(poller1.alive == True)
        self.assert_(poller1.attempt == 0)
        self.assert_(poller1.reachable == True)
        # still alive, just unreach
        self.assert_(poller2.alive == False)
        self.assert_(poller2.attempt == 3)
        self.assert_(poller2.reachable == False)

        # and others satellites too
        self.assert_(reactionner1.alive == True)
        self.assert_(reactionner1.attempt == 0)
        self.assert_(reactionner1.reachable == True)
        # still alive, just unreach
        self.assert_(reactionner2.alive == False)
        self.assert_(reactionner2.attempt == 3)
        self.assert_(reactionner2.reachable == False)

        # and others satellites too
        self.assert_(broker1.alive == True)
        self.assert_(broker1.attempt == 0)
        self.assert_(broker1.reachable == True)
        # still alive, just unreach
        self.assert_(broker2.alive == False)
        self.assert_(broker2.attempt == 3)
        self.assert_(broker2.reachable == False)

        # Now we check how we should dispatch confs
        self.dispatcher.check_dispatch()
        # the conf should not be in a good shape
        self.assert_(self.dispatcher.dispatch_ok == False)

        # Now we really dispatch them!
        self.dispatcher.dispatch()
        self.assert_(self.any_log_match('Dispatch OK of conf in scheduler scheduler-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to reactionner reactionner-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to poller poller-all-1'))
        self.assert_(self.any_log_match('Dispatch OK of configuration 0 to broker broker-all-1'))
        self.clear_logs()

        # And look if we really dispatch conf as we should
        for r in self.conf.realms:
            for cfg in r.confs.values():
                self.assert_(cfg.is_assigned == True)
                self.assert_(cfg.assigned_to == scheduler1)

        cmd = "[%lu] ADD_SIMPLE_POLLER;All;newpoller;localhost;7771" % int(time.time())
        ext_cmd = ExternalCommand(cmd)
        self.external_command_dispatcher.resolve_command(ext_cmd)

        # Look for the poller now
        newpoller = self.conf.pollers.find_by_name('newpoller')
        self.assert_(newpoller is not None)
        newpoller.__class__ = GoodPoller

        ### Wht now with our new poller object?
        self.dispatcher.check_alive()

        # Check good values
        self.assert_(newpoller.alive == True)
        self.assert_(newpoller.attempt == 0)
        self.assert_(newpoller.reachable == True)

        # Now we check how we should dispatch confs
        self.dispatcher.check_bad_dispatch()
        self.dispatcher.dispatch()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_poller_tag_get_checks
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestPollerTagGetchecks(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_poller_tag_get_checks.cfg')

    def test_good_checks_get_only_tags_with_specific_tags(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # schedule the host so it will have a check :)
        # and for ce the execution now
        host.schedule()
        self.assert_(host.check_command.command.poller_tag == 'mytestistrue')
        for a in host.actions:
            print "Tag", a.poller_tag
            a.t_to_go = 0
        svc.schedule()
        for a in svc.actions:
            print "Tag", a.poller_tag
            a.t_to_go = 0
        # the scheduler need to get this new checks in its own queues
        self.sched.get_new_actions()
        # Ask for untag checks only
        untaggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['None'])
        print "Got untaggued_checks", untaggued_checks
        self.assert_(len(untaggued_checks) > 0)
        for c in untaggued_checks:
            # Should be the service one, but not the host one
            self.assert_(c.command.startswith('plugins/test_servicecheck.pl'))

        # Now get only tag ones
        taggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['mytestistrue'])
        self.assert_(len(taggued_checks) > 0)
        for c in taggued_checks:
            # Should be the host one only
            self.assert_(c.command.startswith('plugins/test_hostcheck.pl'))

    def test_good_checks_get_only_tags_with_specific_module_types(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        # schedule the host so it will have a check :)
        # and for ce the execution now
        host.schedule()
        self.assert_(host.check_command.command.poller_tag == 'mytestistrue')
        for a in host.actions:
            print "Tag", a.poller_tag
            a.t_to_go = 0
        svc.schedule()
        for a in svc.actions:
            print "Tag", a.poller_tag
            a.t_to_go = 0
        # the scheduler need to get this new checks in its own queues
        self.sched.get_new_actions()

        # Ask for badly named module type
        untaggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['None'], module_types=['fork'])
        print "Got untaggued_checks for forks", untaggued_checks
        self.assert_(len(untaggued_checks) > 0)
        print "NB CHECKS", len(untaggued_checks)
        for c in untaggued_checks:
            print c.command
            # Should be the service one, but not the host one
            self.assert_(c.command.startswith('plugins/test_servicecheck.pl') or c.command.startswith('plugins/test_hostcheck.pl'))

        # Now get only tag ones and with a bad module type, so get NOTHING
        taggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['mytestistrue'], module_types=['myassischicken'])
        self.assert_(len(taggued_checks) == 0)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_problem_impact
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test host- and service-downtimes.
#

from shinken_test import *


class TestProblemImpact(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_problem_impact.cfg')

    def test_problems_impacts(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification

        # First initialize routers 0 and 1
        now = time.time()

        # The problem_impact_state change should be enabled in the configuration
        self.assert_(self.conf.enable_problem_impacts_states_change == True)

        host_router_0 = self.sched.hosts.find_by_name("test_router_0")
        host_router_0.checks_in_progress = []
        self.assert_(host_router_0.business_impact == 2)
        host_router_1 = self.sched.hosts.find_by_name("test_router_1")
        host_router_1.checks_in_progress = []
        self.assert_(host_router_1.business_impact == 2)

        # Then initialize host under theses routers
        host_0 = self.sched.hosts.find_by_name("test_host_0")
        host_0.checks_in_progress = []
        host_1 = self.sched.hosts.find_by_name("test_host_1")
        host_1.checks_in_progress = []

        all_hosts = [host_router_0, host_router_1, host_0, host_1]
        all_routers = [host_router_0, host_router_1]
        all_servers = [host_0, host_1]

        #--------------------------------------------------------------
        # initialize host states as UP
        #--------------------------------------------------------------
        print "- 4 x UP -------------------------------------"
        self.scheduler_loop(1, [[host_router_0, 0, 'UP'], [host_router_1, 0, 'UP'], [host_0, 0, 'UP'], [host_1, 0, 'UP']], do_sleep=False)

        for h in all_hosts:
            self.assert_(h.state == 'UP')
            self.assert_(h.state_type == 'HARD')

        #--------------------------------------------------------------
        # Now we add some problems to routers
        #--------------------------------------------------------------
        print "- routers get DOWN /SOFT-------------------------------------"
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        # Max attempt is at 5, should be soft now
        for h in all_routers:
            self.assert_(h.state == 'DOWN')
            self.assert_(h.state_type == 'SOFT')

        print "- routers get DOWN /HARD-------------------------------------"
        # Now put 4 more checks so we get DOWN/HARD
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)

        # Max attempt is reach, should be HARD now
        for h in all_routers:
            self.assert_(h.state == 'DOWN')
            self.assert_(h.state_type == 'HARD')

        #--------------------------------------------------------------
        # Routers get HARD/DOWN
        # should be problems now!
        #--------------------------------------------------------------
        # Now check in the brok generation too
        host_router_0_brok = host_router_0.get_update_status_brok()
        host_router_0_brok.prepare()
        host_router_1_brok = host_router_1.get_update_status_brok()
        host_router_1_brok.prepare()

        # Should be problems and have sub servers as impacts
        for h in all_routers:
            self.assert_(h.is_problem == True)
            # Now routers are problems, they should have take the max
            # business_impact value ofthe impacts, so here 5
            self.assert_(h.business_impact == 5)
            for s in all_servers:
                self.assert_(s in h.impacts)
                self.assert_(s.get_dbg_name() in host_router_0_brok.data['impacts']['hosts'])
                self.assert_(s.get_dbg_name() in host_router_1_brok.data['impacts']['hosts'])

        # Should have host notification, but it's not so simple:
        # our contact say: not under 5, and our hosts are 2. But
        # the impacts have huge business_impact, so the hosts gain such business_impact
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;'))
        self.show_and_clear_logs()


        # Now impacts should really be .. impacts :)
        for s in all_servers:
            self.assert_(s.is_impact == True)
            self.assert_(s.state == 'UNREACHABLE')
            # And check the services are impacted too
            for svc in s.services:
                print "Service state", svc.state
                self.assert_(svc.state == 'UNKNOWN')
                self.assert_(svc.get_dbg_name() in host_router_0_brok.data['impacts']['services'])
                self.assert_(svc.get_dbg_name() in host_router_1_brok.data['impacts']['services'])
                brk_svc = svc.get_update_status_brok()
                brk_svc.prepare()
                self.assert_(brk_svc.data['source_problems']['hosts'] == ['test_router_0', 'test_router_1'])
            for h in all_routers:
                self.assert_(h in s.source_problems)
                brk_hst = s.get_update_status_brok()
                brk_hst.prepare()
                self.assert_(h.get_dbg_name() in brk_hst.data['source_problems']['hosts'])

        #--------------------------------------------------------------
        # One router get UP now
        #--------------------------------------------------------------
        print "- 1 X UP for a router ------------------------------"
        # Ok here the problem/impact propagation is Checked. Now what
        # if one router get back? :)
        self.scheduler_loop(1, [[host_router_0, 0, 'UP']], do_sleep=False)

        # should be UP/HARD now
        self.assert_(host_router_0.state == 'UP')
        self.assert_(host_router_0.state_type == 'HARD')

        # And should not be a problem any more!
        self.assert_(host_router_0.is_problem == False)
        self.assert_(host_router_0.impacts == [])

        # And check if it's no more in sources problems of others servers
        for s in all_servers:
            # Still impacted by the other server
            self.assert_(s.is_impact == True)
            self.assert_(s.source_problems == [host_router_1])

        #--------------------------------------------------------------
        # The other router get UP :)
        #--------------------------------------------------------------
        print "- 1 X UP for the last router ------------------------------"
        # What is the last router get back? :)
        self.scheduler_loop(1, [[host_router_1, 0, 'UP']], do_sleep=False)

        # should be UP/HARD now
        self.assert_(host_router_1.state == 'UP')
        self.assert_(host_router_1.state_type == 'HARD')

        # And should not be a problem any more!
        self.assert_(host_router_1.is_problem == False)
        self.assert_(host_router_1.impacts == [])

        # And check if it's no more in sources problems of others servers
        for s in all_servers:
            # Still impacted by the other server
            self.assert_(s.is_impact == False)
            self.assert_(s.state == 'UP')
            self.assert_(s.source_problems == [])

        # And our "business_impact" should have failed back to our
        # conf value, so 2
        self.assert_(host_router_0.business_impact == 2)
        self.assert_(host_router_1.business_impact == 2)
        # It's done :)

    def test_problems_impacts_with_crit_mod(self):
        self.print_header()
        # retry_interval 2
        # critical notification
        # run loop -> another notification

        # First initialize routers 0 and 1
        now = time.time()

        # The problem_impact_state change should be enabled in the configuration
        self.assert_(self.conf.enable_problem_impacts_states_change == True)

        host_router_0 = self.sched.hosts.find_by_name("test_router_0")
        host_router_0.checks_in_progress = []
        self.assert_(host_router_0.business_impact == 2)
        host_router_1 = self.sched.hosts.find_by_name("test_router_1")
        host_router_1.checks_in_progress = []
        self.assert_(host_router_1.business_impact == 2)

        # Then initialize host under theses routers
        host_0 = self.sched.hosts.find_by_name("test_host_0")
        host_0.checks_in_progress = []
        host_1 = self.sched.hosts.find_by_name("test_host_1")
        host_1.checks_in_progress = []

        all_hosts = [host_router_0, host_router_1, host_0, host_1]
        all_routers = [host_router_0, host_router_1]
        all_servers = [host_0, host_1]

        # Our crit mod that will allow us to play with on the fly
        # business_impact modulation
        critmod = self.sched.conf.businessimpactmodulations.find_by_name('Raise')
        self.assert_(critmod is not None)

        # We lie here, from now we do not want criticities
        for h in all_hosts:
            for s in h.services:
                s.business_impact = 2

        #--------------------------------------------------------------
        # initialize host states as UP
        #--------------------------------------------------------------
        print "- 4 x UP -------------------------------------"
        self.scheduler_loop(1, [[host_router_0, 0, 'UP'], [host_router_1, 0, 'UP'], [host_0, 0, 'UP'], [host_1, 0, 'UP']], do_sleep=False)

        for h in all_hosts:
            self.assert_(h.state == 'UP')
            self.assert_(h.state_type == 'HARD')

        #--------------------------------------------------------------
        # Now we add some problems to routers
        #--------------------------------------------------------------
        print "- routers get DOWN /SOFT-------------------------------------"
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        # Max attempt is at 5, should be soft now
        for h in all_routers:
            self.assert_(h.state == 'DOWN')
            self.assert_(h.state_type == 'SOFT')

        print "- routers get DOWN /HARD-------------------------------------"
        # Now put 4 more checks so we get DOWN/HARD
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)
        self.scheduler_loop(1, [[host_router_0, 2, 'DOWN'], [host_router_1, 2, 'DOWN']], do_sleep=False)

        # Max attempt is reach, should be HARD now
        for h in all_routers:
            self.assert_(h.state == 'DOWN')
            self.assert_(h.state_type == 'HARD')

        #--------------------------------------------------------------
        # Routers get HARD/DOWN
        # should be problems now!
        #--------------------------------------------------------------
        # Now check in the brok generation too
        host_router_0_brok = host_router_0.get_update_status_brok()
        host_router_0_brok.prepare()
        host_router_1_brok = host_router_1.get_update_status_brok()
        host_router_1_brok.prepare()

        # Should be problems and have sub servers as impacts
        for h in all_routers:
            self.assert_(h.is_problem == True)
            # Now routers are problems, they should have take the max
            # business_impact value ofthe impacts, so here 2 because we lower all critcity for our test
            self.assert_(h.business_impact == 2)
            for s in all_servers:
                self.assert_(s in h.impacts)
                self.assert_(s.get_dbg_name() in host_router_0_brok.data['impacts']['hosts'])
                self.assert_(s.get_dbg_name() in host_router_1_brok.data['impacts']['hosts'])

        # Should have host notification, but it's not so simple:
        # our contact say: not under 5, and our hosts are 2. And here
        # the business_impact was still low for our test
        self.assert_(not self.any_log_match('HOST NOTIFICATION.*;'))
        self.show_and_clear_logs()


        # Now impacts should really be .. impacts :)
        for s in all_servers:
            self.assert_(s.is_impact == True)
            self.assert_(s.state == 'UNREACHABLE')
            # And check the services are impacted too
            for svc in s.services:
                print "Service state", svc.state
                self.assert_(svc.state == 'UNKNOWN')
                self.assert_(svc.get_dbg_name() in host_router_0_brok.data['impacts']['services'])
                self.assert_(svc.get_dbg_name() in host_router_1_brok.data['impacts']['services'])
                brk_svc = svc.get_update_status_brok()
                brk_svc.prepare()
                self.assert_(brk_svc.data['source_problems']['hosts'] == ['test_router_0', 'test_router_1'])
            for h in all_routers:
                self.assert_(h in s.source_problems)
                brk_hst = s.get_update_status_brok()
                brk_hst.prepare()
                self.assert_(h.get_dbg_name() in brk_hst.data['source_problems']['hosts'])


        for h in all_hosts:
            for s in h.services:
                s.update_business_impact_value()
                self.assert_(s.business_impact == 2)

        # Now we play with modulation!
        # We put modulation period as None so it will be right all time :)
        critmod.modulation_period = None

        crit_srv = self.sched.services.find_srv_by_name_and_hostname("test_host_1", "test_ok_1")
        self.assert_(critmod in crit_srv.business_impact_modulations)

        # Now we set the modulation period as always good, we check that the service
        # really update it's business_impact value
        self.sched.update_business_values()
        # So the service with the modulation should got it's business_impact raised
        self.assert_(crit_srv.business_impact == 5)
        # And the routers too (problems)
        self.assert_(host_router_0.business_impact == 5)
        self.assert_(host_router_1.business_impact == 5)

        #--------------------------------------------------------------
        # One router get UP now
        #--------------------------------------------------------------
        print "- 1 X UP for a router ------------------------------"
        # Ok here the problem/impact propagation is Checked. Now what
        # if one router get back? :)
        self.scheduler_loop(1, [[host_router_0, 0, 'UP']], do_sleep=False)

        # should be UP/HARD now
        self.assert_(host_router_0.state == 'UP')
        self.assert_(host_router_0.state_type == 'HARD')

        # And should not be a problem any more!
        self.assert_(host_router_0.is_problem == False)
        self.assert_(host_router_0.impacts == [])

        # And check if it's no more in sources problems of others servers
        for s in all_servers:
            # Still impacted by the other server
            self.assert_(s.is_impact == True)
            self.assert_(s.source_problems == [host_router_1])

        #--------------------------------------------------------------
        # The other router get UP :)
        #--------------------------------------------------------------
        print "- 1 X UP for the last router ------------------------------"
        # What is the last router get back? :)
        self.scheduler_loop(1, [[host_router_1, 0, 'UP']], do_sleep=False)

        # should be UP/HARD now
        self.assert_(host_router_1.state == 'UP')
        self.assert_(host_router_1.state_type == 'HARD')

        # And should not be a problem any more!
        self.assert_(host_router_1.is_problem == False)
        self.assert_(host_router_1.impacts == [])

        # And check if it's no more in sources problems of others servers
        for s in all_servers:
            # Still impacted by the other server
            self.assert_(s.is_impact == False)
            self.assert_(s.state == 'UP')
            self.assert_(s.source_problems == [])

        # And our "business_impact" should have failed back to our
        # conf value, so 2
        self.assert_(host_router_0.business_impact == 2)
        self.assert_(host_router_1.business_impact == 2)
        # It's done :)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_properties
#!/usr/bin/env python
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
Test shinken.property
"""

import unittest

import __import_shinken
import shinken.property
from shinken.property import none_object

from shinken_test import *


class PropertyTests:
    """Common tests for all property classes"""

    def setUp(self):
        pass

    def test_no_default_value(self):
        p = self.prop_class()
        self.assertIs(p.default, none_object)
        self.assertFalse(p.has_default)
        self.assertTrue(p.required)

    def test_default_value(self):
        default_value = object()
        p = self.prop_class(default=default_value)
        self.assertIs(p.default, default_value)
        self.assertTrue(p.has_default)
        self.assertFalse(p.required)

    def test_fill_brok(self):
        p = self.prop_class()
        self.assertNotIn('full_status', p.fill_brok)
        p = self.prop_class(default='0', fill_brok=['full_status'])
        self.assertIn('full_status', p.fill_brok)

    def test_unused(self):
        p = self.prop_class()
        self.assertFalse(p.unused)


#ShinkenTest

class TestBoolProp(PropertyTests, ShinkenTest):
    """Test the BoolProp class"""

    prop_class = shinken.property.BoolProp

    def test_pythonize(self):
        p = self.prop_class()
        # allowed strings for `True`
        self.assertEqual(p.pythonize("1"), True)
        self.assertEqual(p.pythonize("yes"), True)
        self.assertEqual(p.pythonize("true"), True)
        self.assertEqual(p.pythonize("on"), True)
        self.assertEqual(p.pythonize(["off", "on"]), True)
        # allowed strings for `False`
        self.assertEqual(p.pythonize("0"), False)
        self.assertEqual(p.pythonize("no"), False)
        self.assertEqual(p.pythonize("false"), False)
        self.assertEqual(p.pythonize("off"), False)
        self.assertEqual(p.pythonize(["on", "off"]), False)



class TestIntegerProp(PropertyTests, ShinkenTest):
    """Test the IntegerProp class"""

    prop_class = shinken.property.IntegerProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("1"), 1)
        self.assertEqual(p.pythonize("0"), 0)
        self.assertEqual(p.pythonize("1000.33"), 1000)
        self.assertEqual(p.pythonize(["2000.66", "1000.33"]), 1000)


class TestFloatProp(PropertyTests, ShinkenTest):
    """Test the FloatProp class"""

    prop_class = shinken.property.FloatProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("1"), 1.0)
        self.assertEqual(p.pythonize("0"), 0.0)
        self.assertEqual(p.pythonize("1000.33"), 1000.33)
        self.assertEqual(p.pythonize(["2000.66", "1000.33"]), 1000.33)


class TestStringProp(PropertyTests, ShinkenTest):
    """Test the StringProp class"""

    prop_class = shinken.property.StringProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("1"), "1")
        self.assertEqual(p.pythonize("yes"), "yes")
        self.assertEqual(p.pythonize("0"), "0")
        self.assertEqual(p.pythonize("no"), "no")
        self.assertEqual(p.pythonize(["yes", "no"]), "no")


class TestCharProp(PropertyTests, ShinkenTest):
    """Test the CharProp class"""

    prop_class = shinken.property.CharProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("c"), "c")
        self.assertEqual(p.pythonize("cxxxx"), "c")
        self.assertEqual(p.pythonize(["bxxxx", "cxxxx"]), "c")
        # this raises IndexError. is this intented?
        ## self.assertEqual(p.pythonize(""), "")


class TestPathProp(TestStringProp):
    """Test the PathProp class"""

    prop_class = shinken.property.PathProp

    # As of now, PathProp is a subclass of StringProp without any
    # relevant change. So no further tests are implemented here.


class TestConfigPathProp(TestStringProp):
    """Test the ConfigPathProp class"""

    prop_class = shinken.property.ConfigPathProp

    # As of now, ConfigPathProp is a subclass of StringProp without
    # any relevant change. So no further tests are implemented here.


class TestListProp(PropertyTests, ShinkenTest):
    """Test the ListProp class"""

    prop_class = shinken.property.ListProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize(""), [])
        self.assertEqual(p.pythonize("1,2,3"), ["1", "2", "3"])
        self.assertEquals(p.pythonize(["1,2,3", "4,5,6"]), ["1,2,3", "4,5,6"])


class TestLogLevelProp(PropertyTests, ShinkenTest):
    """Test the LogLevelProp class"""

    prop_class = shinken.property.LogLevelProp

    def test_pythonize(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("NOTSET"), 0)
        self.assertEqual(p.pythonize("DEBUG"), 10)
        self.assertEqual(p.pythonize("INFO"), 20)
        self.assertEqual(p.pythonize("WARN"), 30)
        self.assertEqual(p.pythonize("WARNING"), 30)
        self.assertEqual(p.pythonize("ERROR"), 40)
        ## 'FATAL' is not defined in std-module `logging._levelNames`
        #self.assertEqual(p.pythonize("FATAL"), 50)
        self.assertEqual(p.pythonize("CRITICAL"), 50)
        self.assertEqual(p.pythonize(["NOTSET", "CRITICAL"]), 50)


## :todo: fix DictProp error if no `elts_prop` are passed
## class TestDictProp(PropertyTests, ShinkenTest):
##     """Test the DictProp class"""
##
##     prop_class = shinken.property.DictProp
##
##     def test_pythonize(self):
##         p = self.prop_class()
##         self.assertEqual(p.pythonize(""), "")


class TestAddrProp(PropertyTests, ShinkenTest):
    """Test the AddrProp class"""

    prop_class = shinken.property.AddrProp

    def test_pythonize_with_IPv4_addr(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("192.168.10.11:445"),
                         {'address': "192.168.10.11",
                          'port': 445})
        # no colon, no port
        self.assertEqual(p.pythonize("192.168.10.11"),
                         {'address': "192.168.10.11"})
        # colon but no port number
        self.assertRaises(ValueError, p.pythonize, "192.168.10.11:")
        # only colon, no addr, no port number
        self.assertRaises(ValueError, p.pythonize, ":")
        # no address, only port number
        self.assertEqual(p.pythonize(":445"),
                         {'address': "",
                          'port': 445})

    def test_pythonize_with_hostname(self):
        p = self.prop_class()
        self.assertEqual(p.pythonize("host_123:445"),
                         {'address': "host_123",
                          'port': 445})
        # no colon, no port
        self.assertEqual(p.pythonize("host_123"),
                         {'address': "host_123"})
        # colon but no port number
        self.assertRaises(ValueError, p.pythonize, "host_123:")
        # only colon, no addr, no port number
        self.assertRaises(ValueError, p.pythonize, ":")
        # no address, only port number
        self.assertEqual(p.pythonize(":445"),
                         {'address': "",
                          'port': 445})
        self.assertEqual(p.pythonize([":444", ":445"]),
                         {'address': "",
                          'port': 445})

    # :fixme: IPv6 addresses are no tested since they are not parsed
    # correcly


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_properties_defaults
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2012:
#    Hartmut Goebel <h.goebel@crazy-compilers.com>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""
Test default values for item types.
"""

import unittest

import __import_shinken
from shinken.property import UnusedProp, none_object
import shinken.daemon

from shinken_test import *


class PropertiesTester(object):

    def test_unused_properties(self):
        item = self.item # shortcut
        for name in self.unused_props:
            self.assertIn(name, item.properties,
                          msg='property %r not found in %s' % (name, self.item.my_type))
            self.assertIsInstance(item.properties[name], UnusedProp)

    def test_properties_without_default(self):
        item = self.item # shortcut
        for name in self.without_default:
            self.assertIn(name, item.properties,
                          msg='property %r not found in %s' % (name, self.item.my_type))
            self.assertIs(item.properties[name].default, none_object,
                          msg='property %r is not `none_object` but %r' % (name, item.properties[name]))
            self.assertTrue(item.properties[name].required)

    def test_default_values(self):
        item = self.item # shortcut
        for name, value in self.properties.iteritems():
            self.assertIn(name, item.properties,
                          msg='property %r not found in %s' % (name, self.item.my_type))
            if hasattr(item.properties[name], 'default'):
                if item.properties[name].default != value:
                    print "%s, %s: %s, %s" % (name, value, item.properties[name].default, value)
                self.assertEqual(item.properties[name].default, value)

    def test_all_props_are_tested(self):
        item = self.item # shortcut
        prop_names = set(list(self.properties.keys()) + self.unused_props + self.without_default)

        for name in item.properties:
            if name.startswith('$') and name.endswith('$'):
                continue
            self.assertIn(name, prop_names,
                          msg='unknown property %r found' % name)

class TestConfig(PropertiesTester, ShinkenTest):

    unused_props = [
        'log_file', 'object_cache_file', 'precached_object_file',
        'temp_file', 'status_file', 'status_update_interval',
        'command_check_interval', 'external_command_buffer_slots',
        'check_for_updates', 'bare_update_checks',
        'retain_state_information', 'use_retained_program_state',
        'use_retained_scheduling_info',
        'retained_host_attribute_mask',
        'retained_service_attribute_mask',
        'retained_process_host_attribute_mask',
        'retained_process_service_attribute_mask',
        'retained_contact_host_attribute_mask',
        'retained_contact_service_attribute_mask', 'sleep_time',
        'service_inter_check_delay_method',
        'service_interleave_factor', 'max_concurrent_checks',
        'check_result_reaper_frequency',
        'max_check_result_reaper_time', 'check_result_path',
        'max_check_result_file_age', 'host_inter_check_delay_method',
        'free_child_process_memory', 'child_processes_fork_twice',
        'admin_email', 'admin_pager', 'event_broker_options',
        'debug_file', 'debug_level', 'debug_verbosity',
        'max_debug_file_size']

    without_default = []

    properties = dict([
        ('prefix', '/usr/local/shinken/'),
        ('workdir', '/var/run/shinken/'),
        ('config_base_dir', ''),
        ('modules_dir', '/var/lib/shinken/modules'),
        ('use_local_log', '1'),
        ('log_level', 'WARNING'),
        ('local_log', '/var/log/shinken/arbiterd.log'),
        ('resource_file', '/tmp/resources.txt'),
        ('shinken_user', shinken.daemon.get_cur_user()),
        ('shinken_group', shinken.daemon.get_cur_group()),
        ('enable_notifications', '1'),
        ('execute_service_checks', '1'),
        ('accept_passive_service_checks', '1'),
        ('execute_host_checks', '1'),
        ('accept_passive_host_checks', '1'),
        ('enable_event_handlers', '1'),
        ('log_rotation_method', 'd'),
        ('log_archive_path', '/usr/local/shinken/var/archives'),
        ('check_external_commands', '1'),
        ('command_file', ''),
        ('lock_file', '/var/run/shinken/arbiterd.pid'),
        ('state_retention_file', ''),
        ('retention_update_interval', '60'),
        ('use_syslog', '0'),
        ('log_notifications', '1'),
        ('log_service_retries', '1'),
        ('log_host_retries', '1'),
        ('log_event_handlers', '1'),
        ('log_initial_states', '1'),
        ('log_external_commands', '1'),
        ('log_passive_checks', '1'),
        ('global_host_event_handler', ''),
        ('global_service_event_handler', ''),
        ('max_service_check_spread', '30'),
        ('max_host_check_spread', '30'),
        ('interval_length', '60'),
        ('auto_reschedule_checks', '1'),
        ('auto_rescheduling_interval', '1'),
        ('auto_rescheduling_window', '180'),
        ('use_aggressive_host_checking', '0'),
        ('translate_passive_host_checks', '1'),
        ('passive_host_checks_are_soft', '1'),
        ('enable_predictive_host_dependency_checks', '1'),
        ('enable_predictive_service_dependency_checks', '1'),
        ('cached_host_check_horizon', '0'),
        ('cached_service_check_horizon', '0'),
        ('use_large_installation_tweaks', '0'),
        ('enable_environment_macros', '1'),
        ('enable_flap_detection', '1'),
        ('low_service_flap_threshold', '20'),
        ('high_service_flap_threshold', '30'),
        ('low_host_flap_threshold', '20'),
        ('high_host_flap_threshold', '30'),
        ('soft_state_dependencies', '0'),
        ('service_check_timeout', '60'),
        ('host_check_timeout', '30'),
        ('event_handler_timeout', '30'),
        ('notification_timeout', '30'),
        ('ocsp_timeout', '15'),
        ('ochp_timeout', '15'),
        ('perfdata_timeout', '5'),
        ('obsess_over_services', '0'),
        ('ocsp_command', ''),
        ('obsess_over_hosts', '0'),
        ('ochp_command', ''),
        ('process_performance_data', '1'),
        ('host_perfdata_command', ''),
        ('service_perfdata_command', ''),
        ('host_perfdata_file', ''),
        ('service_perfdata_file', ''),
        ('host_perfdata_file_template', '/tmp/host.perf'),
        ('service_perfdata_file_template', '/tmp/host.perf'),
        ('host_perfdata_file_mode', 'a'),
        ('service_perfdata_file_mode', 'a'),
        ('host_perfdata_file_processing_interval', '15'),
        ('service_perfdata_file_processing_interval', '15'),
        ('host_perfdata_file_processing_command', ''),
        ('service_perfdata_file_processing_command', None),
        ('check_for_orphaned_services', '1'),
        ('check_for_orphaned_hosts', '1'),
        ('check_service_freshness', '1'),
        ('service_freshness_check_interval', '60'),
        ('check_host_freshness', '1'),
        ('host_freshness_check_interval', '60'),
        ('additional_freshness_latency', '15'),
        ('enable_embedded_perl', '1'),
        ('use_embedded_perl_implicitly', '0'),
        ('date_format', None),
        ('use_timezone', ''),
        ('illegal_object_name_chars', '`~!$%^&*"|\'<>?,()='),
        ('illegal_macro_output_chars', ''),
        ('use_regexp_matching', '0'),
        ('use_true_regexp_matching', None),
        ('broker_module', ''),
        ('modified_attributes', 0L),
        ('daemon_enabled', '1'),

        # Shinken specific
        ('idontcareaboutsecurity', '0'),
        ('flap_history', '20'),
        ('max_plugins_output_length', '8192'),
        ('no_event_handlers_during_downtimes', '0'),
        ('cleaning_queues_interval', '900'),
        ('disable_old_nagios_parameters_whining', '0'),
        ('enable_problem_impacts_states_change', '0'),
        ('resource_macros_names', []),

        # SSL part
        ('use_ssl', '0'),
        ('server_key', 'etc/certs/server.key'),
        ('ca_cert', 'etc/certs/ca.pem'),
        ('server_cert', 'etc/certs/server.cert'),
        ('hard_ssl_name_check', '0'),

        ('human_timestamp_log', '0'),

        # Discovery part
        ('strip_idname_fqdn', '1'),
        ('runners_timeout', '3600'),
        ('pack_distribution_file', 'pack_distribution.dat'),

        # WebUI part
        ('webui_lock_file', 'webui.pid'),
        ('webui_port', '8080'),
        ('webui_host', '0.0.0.0'),

        ('use_multiprocesses_serializer', '0'),
        ('daemon_thread_pool_size', '8'),
        ('enable_environment_macros', '1'),
        ('timeout_exit_status', '2'),
        ])

    def setUp(self):
        from shinken.objects.config import Config
        self.item = Config()


class TestCommand(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['command_name', 'command_line']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('poller_tag', 'None'),
        ('reactionner_tag', 'None'),
        ('module_type', None),
        ('timeout', '-1'),
        ('enable_environment_macros', 0),
        ])

    def setUp(self):
        from shinken.objects.command import Command
        self.item = Command()


class TestContactgroup(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['members', 'contactgroup_name', 'alias']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('unknown_members', []),
        ('id', 0),
        ])

    def setUp(self):
        from shinken.objects.contactgroup import Contactgroup
        self.item = Contactgroup()


class TestContact(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'contact_name',
        'host_notification_period', 'service_notification_period',
        'host_notification_options', 'service_notification_options',
        'host_notification_commands', 'service_notification_commands'
        ]

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('alias', 'none'),
        ('contactgroups', ''),
        ('host_notifications_enabled', '1'),
        ('service_notifications_enabled', '1'),
        ('min_business_impact', '0'),
        ('email', 'none'),
        ('pager', 'none'),
        ('address1', 'none'),
        ('address2', 'none'),
        ('address3', 'none'),
        ('address4', 'none'),
        ('address5', 'none'),
        ('address6', 'none'),
        ('can_submit_commands', '0'),
        ('is_admin', '0'),
        ('retain_status_information', '1'),
        ('notificationways', ''),
        ('password', 'NOPASSWORDSET'),
        ])

    def setUp(self):
        from shinken.objects.contact import Contact
        self.item = Contact()


class TestDiscoveryrule(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['discoveryrule_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('creation_type', 'service'),
        ('discoveryrule_order', '0'),
        ])

    def setUp(self):
        from shinken.objects.discoveryrule import Discoveryrule
        self.item = Discoveryrule()


class TestDiscoveryrun(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['discoveryrun_name', 'discoveryrun_command']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ])

    def setUp(self):
        from shinken.objects.discoveryrun import Discoveryrun
        self.item = Discoveryrun()


class TestEscalation(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['escalation_name', 'first_notification', 'last_notification', 'first_notification_time', 'last_notification_time', 'contacts', 'contact_groups']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('notification_interval', '-1'),
        ('escalation_period', ''),
        ('escalation_options', 'd,u,r,w,c'),
        ])

    def setUp(self):
        from shinken.objects.escalation import Escalation
        self.item = Escalation()


class TestHostdependency(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['dependent_host_name', 'host_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('dependent_hostgroup_name', ''),
        ('hostgroup_name', ''),
        ('inherits_parent', '0'),
        ('execution_failure_criteria', 'n'),
        ('notification_failure_criteria', 'n'),
        ('dependency_period', ''),
        ])

    def setUp(self):
        from shinken.objects.hostdependency import Hostdependency
        self.item = Hostdependency()


class TestHostescalation(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'host_name', 'hostgroup_name',
        'first_notification', 'last_notification',
        'contacts', 'contact_groups'
        ]

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('notification_interval', '30'),
        ('escalation_period', ''),
        ('escalation_options', 'd,u,r,w,c'),
        ])

    def setUp(self):
        from shinken.objects.hostescalation import Hostescalation
        self.item = Hostescalation()


class TestHostextinfo(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['host_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('notes', ''),
        ('notes_url', ''),
        ('icon_image', ''),
        ('icon_image_alt', ''),
        ('vrml_image', ''),
        ('statusmap_image', ''),
        ('2d_coords', ''),
        ('3d_coords', ''),
        ])

    def setUp(self):
        from shinken.objects.hostextinfo import HostExtInfo
        self.item = HostExtInfo()


class TestHostgroup(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['members', 'hostgroup_name', 'alias']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('unknown_members', []),
        ('id', 0),
        ('notes', ''),
        ('notes_url', ''),
        ('action_url', ''),
        ('realm', ''),
        ])

    def setUp(self):
        from shinken.objects.hostgroup import Hostgroup
        self.item = Hostgroup()


class TestHost(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'host_name', 'alias', 'address',
        'check_period', 'notification_period']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('display_name', ''),
        ('parents', ''),
        ('hostgroups', ''),
        ('check_command', '_internal_host_up'),
        ('initial_state', 'u'),
        ('check_interval', '0'),
        ('max_check_attempts', '1'),
        ('retry_interval', '0'),
        ('active_checks_enabled', '1'),
        ('passive_checks_enabled', '1'),
        ('obsess_over_host', '0'),
        ('check_freshness', '0'),
        ('freshness_threshold', '0'),
        ('event_handler', ''),
        ('event_handler_enabled', '0'),
        ('low_flap_threshold', '25'),
        ('high_flap_threshold', '50'),
        ('flap_detection_enabled', '1'),
        ('flap_detection_options', 'o,d,u'),
        ('process_perf_data', '1'),
        ('retain_status_information', '1'),
        ('retain_nonstatus_information', '1'),
        ('contacts', ''),
        ('contact_groups', ''),
        ('notification_interval', '60'),
        ('first_notification_delay', '0'),
        ('notification_options', 'd,u,r,f'),
        ('notifications_enabled', '1'),
        ('stalking_options', ''),
        ('notes', ''),
        ('notes_url', ''),
        ('action_url', ''),
        ('icon_image', ''),
        ('icon_image_alt', ''),
        ('icon_set', ''),
        ('vrml_image', ''),
        ('statusmap_image', ''),
        ('2d_coords', ''),
        ('3d_coords', ''),
        ('failure_prediction_enabled', '0'),
        ('realm', None),
        ('poller_tag', 'None'),
        ('reactionner_tag', 'None'),
        ('resultmodulations', ''),
        ('business_impact_modulations', ''),
        ('escalations', ''),
        ('maintenance_period', ''),
        ('business_impact', '2'),
        ('trigger', ''),
        ('trigger_name', ''),
        ('trigger_broker_raise_enabled', '0'),
        ('time_to_orphanage', '300'),
        ('trending_policies', ''),
        ('checkmodulations', ''),
        ('macromodulations', ''),
        ('custom_views', ''),
        ('service_overrides', ''),
        ('service_excludes', ''),
        ('business_rule_output_template', ''),
        ('business_rule_smart_notifications', '0'),
        ('business_rule_downtime_as_ack', '0'),
        ('labels', ''),
        ('business_rule_host_notification_options', ''),
        ('business_rule_service_notification_options', ''),
        ])

    def setUp(self):
        from shinken.objects.host import Host
        self.item = Host()


class TestModule(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['module_name', 'module_type']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('modules', ''),
        ])

    def setUp(self):
        from shinken.objects.module import Module
        self.item = Module()


class TestNotificationway(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'notificationway_name',
        'host_notification_period', 'service_notification_period',
        'host_notification_options', 'service_notification_options',
        'host_notification_commands', 'service_notification_commands']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('host_notifications_enabled', '1'),
        ('service_notifications_enabled', '1'),
        ('min_business_impact', '0'),
        ])

    def setUp(self):
        from shinken.objects.notificationway import NotificationWay
        self.item = NotificationWay()


class TestPack(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['pack_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ])

    def setUp(self):
        from shinken.objects.pack import Pack
        self.item = Pack()


class TestRealm(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['members', 'realm_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('unknown_members', []),
        ('id', 0),
        ('realm_members', ''),
        ('higher_realms', ''),
        ('default', '0'),
        ('broker_complete_links', '0'),
        ])

    def setUp(self):
        from shinken.objects.realm import Realm
        self.item = Realm()


class TestResultmodulation(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['resultmodulation_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('exit_codes_match', ''),
        ('exit_code_modulation', None),
        ('modulation_period', None),
        ])

    def setUp(self):
        from shinken.objects.resultmodulation import Resultmodulation
        self.item = Resultmodulation()


class TestServicedependency(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['dependent_host_name', 'dependent_service_description', 'host_name', 'service_description']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('dependent_hostgroup_name', ''),
        ('hostgroup_name', ''),
        ('inherits_parent', '0'),
        ('execution_failure_criteria', 'n'),
        ('notification_failure_criteria', 'n'),
        ('dependency_period', ''),
        ('explode_hostgroup', '0'),
        ])

    def setUp(self):
        from shinken.objects.servicedependency import Servicedependency
        self.item = Servicedependency()


class TestServiceescalation(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'host_name', 'hostgroup_name',
        'service_description',
        'first_notification', 'last_notification',
        'contacts', 'contact_groups']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('notification_interval', '30'),
        ('escalation_period', ''),
        ('escalation_options', 'd,u,r,w,c'),
        ])

    def setUp(self):
        from shinken.objects.serviceescalation import Serviceescalation
        self.item = Serviceescalation()


class TestServiceextinfo(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['host_name', 'service_description']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('notes', ''),
        ('notes_url', ''),
        ('icon_image', ''),
        ('icon_image_alt', ''),
        ])

    def setUp(self):
        from shinken.objects.serviceextinfo import ServiceExtInfo
        self.item = ServiceExtInfo()


class TestServicegroup(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['members', 'servicegroup_name', 'alias']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('unknown_members', []),
        ('id', 0),
        ('notes', ''),
        ('notes_url', ''),
        ('action_url', ''),
        ])

    def setUp(self):
        from shinken.objects.servicegroup import Servicegroup
        self.item = Servicegroup()


class TestService(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = [
        'host_name', 'service_description',
        'check_command', 'check_interval',
        'retry_interval', 'check_period', 'notification_period']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('max_check_attempts', '1'),
        ('hostgroup_name', ''),
        ('display_name', ''),
        ('servicegroups', ''),
        ('is_volatile', '0'),
        ('initial_state', 'o'),
        ('active_checks_enabled', '1'),
        ('passive_checks_enabled', '1'),
        ('obsess_over_service', '0'),
        ('check_freshness', '0'),
        ('freshness_threshold', '0'),
        ('event_handler', ''),
        ('event_handler_enabled', '0'),
        ('low_flap_threshold', '-1'),
        ('high_flap_threshold', '-1'),
        ('flap_detection_enabled', '1'),
        ('flap_detection_options', 'o,w,c,u'),
        ('process_perf_data', '1'),
        ('retain_status_information', '1'),
        ('retain_nonstatus_information', '1'),
        ('notification_interval', '60'),
        ('first_notification_delay', '0'),
        ('notification_options', 'w,u,c,r,f,s'),
        ('notifications_enabled', '1'),
        ('contacts', ''),
        ('contact_groups', ''),
        ('stalking_options', ''),
        ('notes', ''),
        ('notes_url', ''),
        ('action_url', ''),
        ('icon_image', ''),
        ('icon_image_alt', ''),
        ('icon_set', ''),
        ('failure_prediction_enabled', '0'),
        ('parallelize_check', '1'),
        ('poller_tag', 'None'),
        ('reactionner_tag', 'None'),
        ('resultmodulations', ''),
        ('business_impact_modulations', ''),
        ('escalations', ''),
        ('maintenance_period', ''),
        ('duplicate_foreach', ''),
        ('default_value', ''),
        ('business_impact', '2'),
        ('trigger', ''),
        ('trigger_name', ''),
        ('trigger_broker_raise_enabled', '0'),
        ('time_to_orphanage', '300'),
        ('trending_policies', ''),
        ('checkmodulations', ''),
        ('macromodulations', ''),
        ('aggregation', ''),
        ('service_dependencies', ''),
        ('custom_views', ''),
        ('merge_host_contacts', '0'),
        ('business_rule_output_template', ''),
        ('business_rule_smart_notifications', '0'),
        ('business_rule_downtime_as_ack', '0'),
        ('labels', ''),
        ('business_rule_host_notification_options', ''),
        ('business_rule_service_notification_options', ''),
        ])

    def setUp(self):
        from shinken.objects.service import Service
        self.item = Service()


class TestTimeperiod(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['timeperiod_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('alias', ''),
        ('use', ''),
        ('definition_order', '100'),
        ('register', '1'),
        ('dateranges', []),
        ('exclude', []),
        ('is_active', '0'),
        ])

    def setUp(self):
        from shinken.objects.timeperiod import Timeperiod
        self.item = Timeperiod()


class TestTrigger(PropertiesTester, ShinkenTest):

    unused_props = []

    without_default = ['trigger_name']

    properties = dict([
        ('imported_from', 'unknown'),
        ('use', ''),
        ('definition_order', '100'),
        ('name', ''),
        ('code_src', ''),
        ])

    def setUp(self):
        from shinken.objects.trigger import Trigger
        self.item = Trigger()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_property_override
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test object properties overriding.
#

import re
from shinken_test import unittest, ShinkenTest


class TestPropertyOverride(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_property_override.cfg')

    def test_service_property_override(self):
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc")
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc")
        svc1proc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "proc proc1")
        svc1proc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "proc proc2")
        svc2proc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "proc proc1")
        svc2proc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "proc proc2")
        tp24x7 = self.sched.timeperiods.find_by_name("24x7")
        tptest = self.sched.timeperiods.find_by_name("testperiod")
        cgtest = self.sched.contactgroups.find_by_name("test_contact")
        cgadm = self.sched.contactgroups.find_by_name("admins")
        cmdsvc = self.sched.commands.find_by_name("check_service")
        cmdtest = self.sched.commands.find_by_name("dummy_command")
        svc12 = self.sched.services.find_srv_by_name_and_hostname("test_host_01", "srv-svc2")
        svc22 = self.sched.services.find_srv_by_name_and_hostname("test_host_02", "srv-svc2")

        # Checks we got the objects we need
        self.assert_(svc1 is not None)
        self.assert_(svc2 is not None)
        self.assert_(svc1proc1 is not None)
        self.assert_(svc1proc2 is not None)
        self.assert_(svc2proc1 is not None)
        self.assert_(svc2proc2 is not None)
        self.assert_(tp24x7 is not None)
        self.assert_(tptest is not None)
        self.assert_(cgtest is not None)
        self.assert_(cgadm is not None)
        self.assert_(cmdsvc is not None)
        self.assert_(cmdtest is not None)
        self.assert_(svc12 is not None)
        self.assert_(svc22 is not None)

        # Check non overriden properies value
        for svc in (svc1, svc1proc1, svc1proc2, svc2proc1, svc12):
            self.assert_(svc.contact_groups == "test_contact")
            self.assert_(svc.maintenance_period is tp24x7)
            self.assert_(svc.retry_interval == 1)
            self.assert_(svc.check_command.command is cmdsvc)
            self.assert_(svc.notification_options == ["w","u","c","r","f","s"])
            self.assert_(svc.notifications_enabled is True)

        # Check overriden properies value
        for svc in (svc2, svc2proc2, svc22):
            self.assert_(svc.contact_groups == "admins")
            self.assert_(svc.maintenance_period is tptest)
            self.assert_(svc.retry_interval == 3)
            self.assert_(svc.check_command.command is cmdtest)
            self.assert_(svc.notification_options == ["c","r"])
            self.assert_(svc.notifications_enabled is False)


class TestConfigBroken(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_property_override_broken.cfg')

    def test_service_property_override_errors(self):
        self.assert_(not self.conf.conf_is_correct)

        # Get the arbiter's log broks
        [b.prepare() for b in self.broks.values()]
        logs = [b.data['log'] for b in self.broks.values() if b.type == 'log']

        self.assert_(len([log for log in logs if re.search('Error: invalid service override syntax: fake', log)]) == 1)
        self.assert_(len([log for log in logs if re.search("Error: trying to override property 'retry_interval' on service 'fakesrv' but it's unknown for this host", log)]) == 1)
        self.assert_(len([log for log in logs if re.search("Error: trying to override 'host_name', a forbidden property for service 'proc proc2'", log)]) == 1)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_protect_esclamation_point
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestProtectEscalmationPoint(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_protect_esclamation_point.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        print svc.check_command.args
        self.assert_(u'ti!ti' in svc.check_command.args)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_python_crash_with_recursive_bp_rules
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_python_crash_with_recursive_bp_rules.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host1 = self.sched.hosts.find_by_name("ht34-peret-2-dif0")
        host2 = self.sched.hosts.find_by_name("ht34-peret-2-dif1")
        self.scheduler_loop(5, [[host1, 2, 'DOWN | value1=1 value2=2'], [host2, 2, 'DOWN | rtt=10']])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_reactionner_tag_get_notif
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestReactionnerTagGetNotifs(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_reactionner_tag_get_notif.cfg')

    # For a service, we generate a notification and a event handler.
    # Each one got a specific reactionner_tag that we will look for.
    def test_good_checks_get_only_tags_with_specific_tags(self):
        now = int(time.time())
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'BAD | value1=0 value2=0']])

        print "Go bad now"
        self.scheduler_loop(2, [[svc, 2, 'BAD | value1=0 value2=0']])

        to_del = []
        for a in self.sched.actions.values():
            print "\n\nA?", a, "\nZZZ%sZZZ" % a.command
            # Set them go NOW
            a.t_to_go = now
            # In fact they are already launched, so we-reenabled them :)
            print "AHAH?", a.status, a.__class__.my_type
            if a.__class__.my_type == 'notification' and (a.status == 'zombie' or a.status == ' scheduled'):
                to_del.append(a.id)

            a.status = 'scheduled'
            # And look for good tagging
            if a.command.startswith('plugins/notifier.pl'):
                print 'TAG:%s' % a.reactionner_tag
                self.assert_(a.reactionner_tag == 'runonwindows')
            if a.command.startswith('plugins/sms.pl'):
                print 'TAG:%s' % a.reactionner_tag
                self.assert_(a.reactionner_tag == 'sms')
            if a.command.startswith('plugins/test_eventhandler.pl'):
                print 'TAG: %s' % a.reactionner_tag
                self.assert_(a.reactionner_tag == 'eventtag')

        print "\n\n"
        for _i in to_del:
            print "DELETING", self.sched.actions[_i]
            del self.sched.actions[_i]

        print "NOW ACTION!"*20,'\n\n'

        # Ok the tags are defined as it should, now try to get them as a reactionner :)
        # Now get only tag ones
        taggued_runonwindows_checks = self.sched.get_to_run_checks(False, True, reactionner_tags=['runonwindows'])
        self.assert_(len(taggued_runonwindows_checks) > 0)
        for c in taggued_runonwindows_checks:
            # Should be the host one only
            self.assert_(c.command.startswith('plugins/notifier.pl'))


        # Ok the tags are defined as it should, now try to get them as a reactionner :)
        # Now get only tag ones
        taggued_sms_checks = self.sched.get_to_run_checks(False, True, reactionner_tags=['sms'])
        self.assert_(len(taggued_sms_checks) > 0)
        for c in taggued_sms_checks:
            # Should be the host one only
            self.assert_(c.command.startswith('plugins/sms.pl'))


        taggued_eventtag_checks = self.sched.get_to_run_checks(False, True, reactionner_tags=['eventtag'])
        self.assert_(len(taggued_eventtag_checks) > 0)
        for c in taggued_eventtag_checks:
            # Should be the host one only
            self.assert_(c.command.startswith('plugins/test_eventhandler.pl'))


    # Same that upper, but with modules types
    def test_good_checks_get_only_tags_with_specific_tags_andmodule_types(self):
        now = int(time.time())
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'BAD | value1=0 value2=0']])

        print "Go bad now"
        self.scheduler_loop(2, [[svc, 2, 'BAD | value1=0 value2=0']])

        for a in self.sched.actions.values():
            # Set them go NOW
            a.t_to_go = now
            # In fact they are already launched, so we-reenabled them :)
            a.status = 'scheduled'
            # And look for good tagging
            if a.command.startswith('plugins/notifier.pl'):
                print a.__dict__
                print a.reactionner_tag
                self.assert_(a.reactionner_tag == 'runonwindows')
            if a.command.startswith('plugins/test_eventhandler.pl'):
                print a.__dict__
                print a.reactionner_tag
                self.assert_(a.reactionner_tag == 'eventtag')

        # Ok the tags are defined as it should, now try to get them as a reactionner :)
        # Now get only tag ones
        taggued_runonwindows_checks = self.sched.get_to_run_checks(False, True, reactionner_tags=['runonwindows'], module_types=['fork'])
        self.assert_(len(taggued_runonwindows_checks) > 0)
        for c in taggued_runonwindows_checks:
            # Should be the host one only
            self.assert_(c.command.startswith('plugins/notifier.pl'))

        taggued_eventtag_checks = self.sched.get_to_run_checks(False, True, reactionner_tags=['eventtag'], module_types=['myassischicken'])
        self.assert_(len(taggued_eventtag_checks) == 0)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_realms
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_realms.cfg')

    # We check for each host, if they are in the good realm
    def test_realm_assigntion(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        realm1 = self.conf.realms.find_by_name('realm1')
        self.assert_(realm1 is not None)
        realm2 = self.conf.realms.find_by_name('realm2')
        self.assert_(realm2 is not None)
        test_host_realm1 = self.sched.hosts.find_by_name("test_host_realm1")
        self.assert_(test_host_realm1 is not None)
        self.assert_(test_host_realm1.realm == realm1.get_name())
        test_host_realm2 = self.sched.hosts.find_by_name("test_host_realm2")
        self.assert_(test_host_realm2 is not None)
        self.assert_(test_host_realm2.realm == realm2.get_name())

    # We check for each host, if they are in the good realm
    # but when they are apply in a hostgroup link
    def test_realm_hostgroup_assigntion(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        in_realm2 = self.sched.hostgroups.find_by_name('in_realm2')
        realm1 = self.conf.realms.find_by_name('realm1')
        self.assert_(realm1 is not None)
        realm2 = self.conf.realms.find_by_name('realm2')
        self.assert_(realm2 is not None)
        # 1 and 2 are link to realm2 because they are in the hostgroup in_realm2
        test_host1_hg_realm2 = self.sched.hosts.find_by_name("test_host1_hg_realm2")
        self.assert_(test_host1_hg_realm2 is not None)
        self.assert_(test_host1_hg_realm2.realm == realm2.get_name())
        self.assert_(in_realm2 in test_host1_hg_realm2.hostgroups)

        test_host2_hg_realm2 = self.sched.hosts.find_by_name("test_host2_hg_realm2")
        self.assert_(test_host2_hg_realm2 is not None)
        self.assert_(test_host2_hg_realm2.realm == realm2.get_name())
        self.assert_(in_realm2 in test_host2_hg_realm2.hostgroups)

        test_host3_hg_realm2 = self.sched.hosts.find_by_name("test_host3_hg_realm2")
        self.assert_(test_host3_hg_realm2 is not None)
        self.assert_(test_host3_hg_realm2.realm == realm1.get_name())
        self.assert_(in_realm2 in test_host3_hg_realm2.hostgroups)


    # Realms should be stripped when linking to hosts and hostgroups
    # so we don't pickle the whole object, but just a name
    def test_realm_stripping_before_sending(self):
        test_host_realm1 = self.sched.hosts.find_by_name("test_host_realm1")
        self.assert_(test_host_realm1 is not None)
        print type(test_host_realm1.realm)
        self.assert_(isinstance(test_host_realm1.realm, basestring))

        in_realm2 = self.sched.hostgroups.find_by_name('in_realm2')
        self.assert_(in_realm2 is not None)
        print type(in_realm2.realm)
        self.assert_(isinstance(in_realm2.realm, basestring))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_regenerator
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import time

from shinken_test import ShinkenTest, unittest

from shinken.objects import Service
from shinken.misc.regenerator import Regenerator


class TestRegenerator(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_regenerator.cfg')

    def look_for_same_values(self):
        # Look at Regenerator values
        print "Hosts:", self.rg.hosts.__dict__
        for h in self.rg.hosts:
            orig_h = self.sched.hosts.find_by_name(h.host_name)
            print h.state, orig_h.state
            # Look for same states
            self.assert_(h.state == orig_h.state)
            self.assert_(h.state_type == orig_h.state_type)
            # Look for same impacts
            for i in h.impacts:
                print "Got impact", i.get_name()
                same_impacts = i.get_name() in [j.get_name() for j in orig_h.impacts]
                self.assert_(same_impacts)
            # And look for same source problems
            for i in h.source_problems:
                print "Got source pb", i.get_name()
                same_pbs = i.get_name() in [j.get_name() for j in orig_h.source_problems]
                self.assert_(same_pbs)

        print "Services:", self.rg.services.__dict__
        for s in self.rg.services:
            orig_s = self.sched.services.find_srv_by_name_and_hostname(s.host.host_name, s.service_description)
            print s.state, orig_s.state
            self.assert_(s.state == orig_s.state)
            self.assert_(s.state_type == orig_s.state_type)
            # Look for same impacts too
            for i in s.impacts:
                print "Got impact", i.get_name()
                same_impacts = i.get_name() in [j.get_name() for j in orig_s.impacts]
                self.assert_(same_impacts)
            # And look for same source problems
            for i in s.source_problems:
                print "Got source pb", i.get_name()
                same_pbs = i.get_name() in [j.get_name() for j in orig_s.source_problems]
                self.assert_(same_pbs)
            # Look for same host
            self.assert_(s.host.get_name() == orig_s.host.get_name())

    def test_regenerator(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        # for h in self.sched.hosts:
        #    h.realm = h.realm.get_name()
        self.sched.conf.skip_initial_broks = False
        self.sched.brokers['Default-Broker'] = {'broks' : {}, 'has_full_broks' : False}
        self.sched.fill_initial_broks('Default-Broker')
        self.rg = Regenerator()

        # Got the initial creation ones
        ids = self.sched.broks.keys()
        ids.sort()
        t0 = time.time()
        for i in ids:
            b = self.sched.broks[i]
            print "Manage b", b.type
            b.prepare()
            self.rg.manage_brok(b)
        t1 = time.time()
        print 'First inc', t1 - t0, len(self.sched.broks)
        self.sched.broks.clear()

        self.look_for_same_values()

        print "Get the hosts and services"
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        self.scheduler_loop(3, [[host, 2, 'DOWN | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(host.state == 'DOWN')
        self.assert_(host.state_type == 'HARD')

        ids = self.sched.broks.keys()
        ids.sort()
        t0 = time.time()
        for i in ids:
            b = self.sched.broks[i]
            print "Manage b", b.type
            b.prepare()
            self.rg.manage_brok(b)
        t1 = time.time()
        print 'Time', t1 - t0
        self.sched.broks.clear()

        self.look_for_same_values()

        print 'Time', t1 - t0

        b = svc.get_initial_status_brok()
        b.prepare()
        print "GO BENCH!"
        t0 = time.time()
        for i in xrange(1, 1000):
            b = svc.get_initial_status_brok()
            b.prepare()
            s = Service({})
            for (prop, value) in b.data.iteritems():
                setattr(s, prop, value)
        t1 = time.time()
        print "Bench end:", t1 - t0

        times = {}
        sizes = {}
        import cPickle
        data = {}
        cls = svc.__class__
        start = time.time()
        for i in xrange(1, 10000):
            for prop, entry in svc.__class__.properties.items():
                # Is this property intended for brokking?
                if 'full_status' in entry.fill_brok:
                    data[prop] = svc.get_property_value_for_brok(prop, cls.properties)
                    if not prop in times:
                        times[prop] = 0
                        sizes[prop] = 0
                    t0 = time.time()
                    tmp = cPickle.dumps(data[prop], 0)
                    sizes[prop] += len(tmp)
                    times[prop] += time.time() - t0

        print "Times"
        for (k, v) in times.iteritems():
            print "\t%s: %s" % (k, v)
        print "\n\n"
        print "Sizes"
        for (k, v) in sizes.iteritems():
            print "\t%s: %s" % (k, v)
        print "\n"
        print "total time", time.time() - start

    def test_regenerator_load_from_scheduler(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        # for h in self.sched.hosts:
        #    h.realm = h.realm.get_name()

        self.rg = Regenerator()
        self.rg.load_from_scheduler(self.sched)

        self.sched.conf.skip_initial_broks = False
        self.sched.brokers['Default-Broker'] = {'broks' : {}, 'has_full_broks' : False}
        self.sched.fill_initial_broks('Default-Broker')
        # Got the initial creation ones
        ids = self.sched.broks.keys()
        ids.sort()
        t0 = time.time()
        for i in ids:
            b = self.sched.broks[i]
            print "Manage b", b.type
            b.prepare()
            self.rg.manage_brok(b)
        t1 = time.time()
        print 'First inc', t1 - t0, len(self.sched.broks)
        self.sched.broks.clear()

        self.look_for_same_values()




if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_registered_funs
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#


import subprocess
from time import sleep

from shinken_test import *

import shinken.log as shinken_log

from shinken.daemons.schedulerdaemon import Shinken
from shinken.daemons.arbiterdaemon import Arbiter

daemons_config = {
    Shinken:      "etc/test_registered_funs/schedulerd.ini",
    Arbiter:    ["etc/test_registered_funs/shinken.cfg"]
}


class testRegisteredFunctions(ShinkenTest):
    def create_daemon(self):
        cls = Shinken
        return cls(daemons_config[cls], False, True, False, None, '')

    def test_registered(self):
        logger.info("Testing register funs")

        shinken_log.local_log = None  # otherwise get some "trashs" logs..
        d = self.create_daemon()

        d.load_config_file()

        d.http_backend = 'wsgiref'
        d.do_daemon_init_and_start()
        d.load_modules_manager()
        d.http_daemon.register(d.interface)
        reg_list = d.http_daemon.registered_fun
        expected_list = ['get_external_commands', 'get_running_id', 'got_conf', 'have_conf',
                         'ping', 'push_broks', 'push_host_names', 'put_conf', 'remove_from_conf',
                         'run_external_commands', 'set_log_level', 'wait_new_conf', 'what_i_managed']
        for fun in expected_list:
            assert(fun in reg_list)
        subprocess.Popen(["../bin/shinken-arbiter", "-c", daemons_config[Arbiter][0], "-d"])
        # Ok, now the conf
        d.wait_for_initial_conf(timeout=20)
        if not d.new_conf:
            return
        logger.info("New configuration received")
        d.setup_new_conf()
        reg_list = d.http_daemon.registered_fun
        expected_list = ['get_external_commands', 'get_running_id', 'got_conf', 'have_conf',
                         'ping', 'push_broks', 'push_host_names', 'put_conf', 'remove_from_conf',
                         'run_external_commands', 'set_log_level', 'wait_new_conf', 'what_i_managed',
                         'get_checks', 'put_results', 'fill_initial_broks', 'get_broks']
        for fun in expected_list:
            assert(fun in reg_list)

        sleep(2)
        pid = int(file("tmp/arbiterd.pid").read())
        print ("KILLING %d" % pid)*50
        os.kill(int(file("tmp/arbiterd.pid").read()), 2)
        d.do_stop()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_resultmodulation
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_resultmodulation.cfg')

    def get_svc(self):
        return self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

    def get_host(self):
        return self.sched.hosts.find_by_name("test_host_0")

    def get_router(self):
        return self.sched.hosts.find_by_name("test_router_0")

    def test_service_resultmodulation(self):
        svc = self.get_svc()
        host = self.get_host()
        router = self.get_router()

        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [svc, 2, 'BAD | value1=0 value2=0'],])
        self.assert_(host.state == 'UP')
        self.assert_(host.state_type == 'HARD')

        # This service got a result modulation. So Criticals are in fact
        # Warnings. So even with some CRITICAL (2), it must be warning
        self.assert_(svc.state == 'WARNING')

        # If we remove the resultmodulations, we should have theclassic behavior
        svc.resultmodulations = []
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [svc, 2, 'BAD | value1=0 value2=0']])
        self.assert_(svc.state == 'CRITICAL')

        # Now look for the inheritaed thing
        # resultmodulation is a inplicit inherited parameter
        # and router define it, but not test_router_0/test_ok_0. So this service should also be impacted
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "test_ok_0")
        self.assert_(svc2.resultmodulations == router.resultmodulations)

        self.scheduler_loop(2, [[svc2, 2, 'BAD | value1=0 value2=0']])
        self.assert_(svc2.state == 'WARNING')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_satellites
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def test_satellite_failed_check(self):
        print "Create a Scheduler dummy"
        r = self.conf.realms.find_by_name('Default')

        creation_tab = {'scheduler_name': 'scheduler-1', 'address': '0.0.0.0', 'spare': '0',
                        'port': '9999', 'check_interval': '1', 'realm': 'Default', 'use_ssl': '0', 'hard_ssl_name_check': '0'}
        s = SchedulerLink(creation_tab)
        s.pythonize()
        s.last_check = time.time() - 100
        s.timeout = 3
        s.check_interval = 1
        s.data_timeout = 120
        s.port = 9999
        s.max_check_attempts = 4
        s.realm = r
        # Lie: we start at true here
        s.alive = True
        print s.__dict__

        # Should be attempt = 0
        self.assert_(s.attempt == 0)
        # Now make bad ping, sould be unreach and dead (but not dead
        s.ping()
        self.assert_(s.attempt == 1)
        self.assert_(s.alive == True)
        self.assert_(s.reachable == False)

        # Now make bad ping, sould be unreach and dead (but not dead
        s.last_check = time.time() - 100
        s.ping()
        self.assert_(s.attempt == 2)
        self.assert_(s.alive == True)
        self.assert_(s.reachable == False)

        # Now make bad ping, sould be unreach and dead (but not dead
        s.last_check = time.time() - 100
        s.ping()
        self.assert_(s.attempt == 3)
        self.assert_(s.alive == True)
        self.assert_(s.reachable == False)

        # Ok, this time we go DEAD!
        s.last_check = time.time() - 100
        s.ping()
        self.assert_(s.attempt == 4)
        self.assert_(s.alive == False)
        self.assert_(s.reachable == False)

        # Now set a OK ping (false because we won't open the port here...)
        s.last_check = time.time() - 100
        s.set_alive()
        self.assert_(s.attempt == 0)
        self.assert_(s.alive == True)
        self.assert_(s.reachable == True)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_servicedependency_complexes
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_servicedependency_complexes.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        for s in self.sched.services:
            print s.get_full_name()
        NRPE = self.sched.services.find_srv_by_name_and_hostname("myspecifichost", "NRPE")
        self.assert_(NRPE is not None)
        Load = self.sched.services.find_srv_by_name_and_hostname("myspecifichost", "Load")
        self.assert_(Load is not None)
        print Load.act_depend_of
        self.assert_(NRPE in [e[0] for e in Load.act_depend_of])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_servicedependency_explode_hostgroup
#!/usr/bin/env python2.6
#Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
#This file is part of Shinken.
#
#Shinken is free software: you can redistribute it and/or modify
#it under the terms of the GNU Affero General Public License as published by
#the Free Software Foundation, either version 3 of the License, or
#(at your option) any later version.
#
#Shinken is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU Affero General Public License for more details.
#
#You should have received a copy of the GNU Affero General Public License
#along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

#It's ugly I know....
from shinken_test import *


class TestServiceDepAndGroups(ShinkenTest):
    #Uncomment this is you want to use a specific configuration
    #for your test
    def setUp(self):
        self.setup_with_file('etc/shinken_servicedependency_explode_hostgroup.cfg')


    #Change ME :)
    def test_explodehostgroup(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        svc = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "SNMP")
        self.assert_(len(svc.act_depend_of_me),2)

        service_dependencies = []
        service_dependency_postfix = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "POSTFIX")
        service_dependency_cpu = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "CPU")
        service_dependencies.append(service_dependency_postfix)
        service_dependencies.append(service_dependency_cpu)

        # Is service correctly depend of first one
        self.assert_(service_dependency_postfix in s for s in svc.act_depend_of_me)
        self.assert_(service_dependency_cpu in s for s in svc.act_depend_of_me)

if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_servicedependency_implicit_hostgroup
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestServiceDepAndGroups(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_servicedependency_implicit_hostgroup.cfg')

    def test_implicithostgroups(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc_postfix = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "POSTFIX")
        self.assert_(svc_postfix is not None)

        svc_snmp = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "SNMP")
        self.assert_(svc_snmp is not None)

        svc_cpu = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "CPU")
        self.assert_(svc_cpu is not None)

        svc_snmp2 = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "SNMP")
        self.assert_(svc_snmp2 is not None)

        svc_postfix_fathers = [c[0].get_full_name() for c in svc_postfix.act_depend_of]
        print svc_postfix_fathers
        # Should be [u'test_router_0/SNMP', u'test_host_0/SNMP', u'test_host_0']
        self.assert_('test_router_0/SNMP' in svc_postfix_fathers)
        self.assert_('test_host_0/SNMP' in svc_postfix_fathers)

        # Now look for the routers services
        svc_cpu_fathers = [c[0].get_full_name() for c in svc_cpu.act_depend_of]
        print svc_cpu_fathers
        # Should be [u'test_router_0/SNMP', u'test_host_0/SNMP', u'test_host_0']
        self.assert_('test_router_0/SNMP' in svc_cpu_fathers)
        self.assert_('test_host_0/SNMP' in svc_cpu_fathers)

        svc.act_depend_of = []  # no hostchecks on critical checkresults

    def test_implicithostnames(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        svc_postfix = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "POSTFIX_BYSSH")
        self.assert_(svc_postfix is not None)

        svc_ssh = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "SSH")
        self.assert_(svc_ssh is not None)

        svc_cpu = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "CPU_BYSSH")
        self.assert_(svc_cpu is not None)

        svc_postfix_fathers = [c[0].get_full_name() for c in svc_postfix.act_depend_of]
        print svc_postfix_fathers
        # Should be [u'test_router_0/SNMP', u'test_host_0/SNMP', u'test_host_0']
        self.assert_('test_host_0/SSH' in svc_postfix_fathers)

        # Now look for the routers services
        svc_cpu_fathers = [c[0].get_full_name() for c in svc_cpu.act_depend_of]
        print svc_cpu_fathers
        # Should be [u'test_router_0/SNMP', u'test_host_0/SNMP', u'test_host_0']
        self.assert_('test_host_0/SSH' in svc_cpu_fathers)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_services
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

import copy
from shinken_test import *


class TestService(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def get_svc(self):
        return self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

    # Look if get_*_name return the good result
    def test_get_name(self):
        svc = self.get_svc()
        print svc.get_dbg_name()
        self.assert_(svc.get_name() == 'test_ok_0')
        self.assert_(svc.get_dbg_name() == 'test_host_0/test_ok_0')

    # getstate should be with all properties in dict class + id
    # check also the setstate
    def test___getstate__(self):
        svc = self.get_svc()
        cls = svc.__class__
        # We get the state
        state = svc.__getstate__()
        # Check it's the good length
        self.assert_(len(state) == len(cls.properties) + len(cls.running_properties) + 1)
        # we copy the service
        svc_copy = copy.copy(svc)
        # reset the state in the original service
        svc.__setstate__(state)
        # And it should be the same:then before :)
        for p in cls.properties:
            ## print getattr(svc_copy, p)
            ## print getattr(svc, p)
            self.assert_(getattr(svc_copy, p) == getattr(svc, p))

    # Look if it can detect all incorrect cases
    def test_is_correct(self):
        svc = self.get_svc()

        # first it's ok
        self.assert_(svc.is_correct() == True)

        # Now try to delete a required property
        max_check_attempts = svc.max_check_attempts
        del svc.max_check_attempts
        self.assert_(svc.is_correct() == True)
        svc.max_check_attempts = max_check_attempts

        ###
        ### Now special cases
        ###

        # no check command
        check_command = svc.check_command
        del svc.check_command
        self.assert_(svc.is_correct() == False)
        svc.check_command = check_command
        self.assert_(svc.is_correct() == True)

        # no notification_interval
        notification_interval = svc.notification_interval
        del svc.notification_interval
        self.assert_(svc.is_correct() == False)
        svc.notification_interval = notification_interval
        self.assert_(svc.is_correct() == True)

    # Look for set/unset impacted states (unknown)
    def test_impact_state(self):
        svc = self.get_svc()
        ori_state = svc.state
        ori_state_id = svc.state_id
        svc.set_impact_state()
        self.assert_(svc.state == 'UNKNOWN')
        self.assert_(svc.state_id == 3)
        svc.unset_impact_state()
        self.assert_(svc.state == ori_state)
        self.assert_(svc.state_id == ori_state_id)

    # Look for display name setting
    def test_display_name(self):
        svc = self.get_svc()
        print 'Display name', svc.display_name, 'toto'
        print 'Full name', svc.get_full_name()
        self.assert_(svc.display_name == 'test_ok_0')

    def test_set_state_from_exit_status(self):
        svc = self.get_svc()
        # First OK
        svc.set_state_from_exit_status(0)
        self.assert_(svc.state == 'OK')
        self.assert_(svc.state_id == 0)
        self.assert_(svc.is_state('OK') == True)
        self.assert_(svc.is_state('o') == True)
        # Then warning
        svc.set_state_from_exit_status(1)
        self.assert_(svc.state == 'WARNING')
        self.assert_(svc.state_id == 1)
        self.assert_(svc.is_state('WARNING') == True)
        self.assert_(svc.is_state('w') == True)
        # Then Critical
        svc.set_state_from_exit_status(2)
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_id == 2)
        self.assert_(svc.is_state('CRITICAL') == True)
        self.assert_(svc.is_state('c') == True)
        # And unknown
        svc.set_state_from_exit_status(3)
        self.assert_(svc.state == 'UNKNOWN')
        self.assert_(svc.state_id == 3)
        self.assert_(svc.is_state('UNKNOWN') == True)
        self.assert_(svc.is_state('u') == True)

        # And something else :)
        svc.set_state_from_exit_status(99)
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_id == 2)
        self.assert_(svc.is_state('CRITICAL') == True)
        self.assert_(svc.is_state('c') == True)

    def test_business_impact_value(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        # This service inherit the improtance value from his father, 5
        print "FUCK", svc.business_impact
        self.assert_(svc.business_impact == 5)

    # Look if the service is in the servicegroup
    def test_servicegroup(self):
        sg = self.sched.servicegroups.find_by_name("servicegroup_01")
        self.assert_(sg is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        self.assert_(svc in sg.members)
        self.assert_(sg in svc.servicegroups)

    # Look at the good of the last_hard_state_change
    def test_service_last_hard_state(self):
        self.print_header()
        # We want an eventhandelr (the perfdata command) to be put in the actions dict
        # after we got a service check
        now = time.time()
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #--------------------------------------------------------------
        # initialize host/service state
        #--------------------------------------------------------------
        # We do not want to be just a string but a real command
        self.scheduler_loop(1, [[svc, 0, 'OK | bibi=99%']])
        print "FUCK", svc.last_hard_state_change
        orig = svc.last_hard_state_change
        self.assert_(svc.last_hard_state == 'OK')

        # now still ok
        self.scheduler_loop(1, [[svc, 0, 'OK | bibi=99%']])
        self.assert_(svc.last_hard_state_change == orig)
        self.assert_(svc.last_hard_state == 'OK')

        # now error but still SOFT
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | bibi=99%']])
        print "FUCK", svc.state_type
        self.assert_(svc.last_hard_state_change == orig)
        self.assert_(svc.last_hard_state == 'OK')

        # now go hard!
        now = int(time.time())
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | bibi=99%']])
        print "FUCK", svc.state_type
        self.assert_(svc.last_hard_state_change == now)
        self.assert_(svc.last_hard_state == 'CRITICAL')
        print "Last hard state id", svc.last_hard_state_id
        self.assert_(svc.last_hard_state_id == 2)

    # Check if the autoslots are fill like it should
    def test_autoslots(self):
        svc = self.get_svc()
        self.assert_("check_period" not in svc.__dict__)

    # Check if the parent/childs dependencies are fill like it should
    def test_parent_child_dep_list(self):
        svc = self.get_svc()
        # Look if our host is a parent
        self.assert_(svc.host in svc.parent_dependencies)
        # and if we are a child of it
        self.assert_(svc in svc.host.child_dependencies)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_generators
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_generators.cfg')

    def test_service_generators(self):

        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        print "All service of", "test_host_0"
        for s in host.services:
            print s.get_name()
        # We ask for 4 services with our disks :)
        svc_c = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service C")
        svc_d = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service D")
        svc_e = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service E")
        svc_f = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service F")
        svc_g = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service G")

        self.assert_(svc_c is not None)
        self.assert_(svc_d is not None)
        self.assert_(svc_e is not None)
        self.assert_(svc_f is not None)
        self.assert_(svc_g is not None)

        # two classics
        self.assert_(svc_c.check_command.args == ['C', '80%', '90%'])
        self.assert_(svc_d.check_command.args == ['D', '95%', '70%'])
        # a default parameters
        self.assert_(svc_e.check_command.args == ['E', '38%', '24%'])
        # and another one
        self.assert_(svc_f.check_command.args == ['F', '95%', '70%'])
        # and the tricky last one (with no value :) )
        self.assert_(svc_g.check_command.args == ['G', '38%', '24%'])


        # Now check that the dependencies are also created as Generated Service C Dependant -> Generated Service C
        svc_c_dep = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service C Dependant")
        self.assert_(svc_c_dep is not None)
        # Dep version should a child of svc
        self.assert_(svc_c_dep in svc_c.child_dependencies)
        # But not on other of course
        self.assert_(svc_c_dep not in svc_d.child_dependencies)

        

    def test_service_generators_not(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        print "All service of", "test_host_0"
        for s in host.services:
            print s.get_name()
        # We ask for 4 services with our disks :)
        svc_c = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service NOT C")
        svc_d = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service NOT D")
        svc_e = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service NOT E")
        svc_f = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service NOT F")
        svc_g = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Generated Service NOT G")

        self.assert_(svc_c is not None)
        self.assert_(svc_d is not None)
        self.assert_(svc_e is None)
        self.assert_(svc_f is None)
        self.assert_(svc_g is not None)

    def test_service_generators_key_generator(self):

        host = self.sched.hosts.find_by_name("sw_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router

        print "All service of", "sw_0"
        for s in host.services:
            print s.get_name()

        # We ask for our 6*46 + 6 services with our ports :)
        # _ports  Unit [1-6] Port [0-46]$(80%!90%)$,Unit [1-6] Port 47$(80%!90%)$
        for unit_id in xrange(1, 7):
            for port_id in xrange(0, 47):
                n = "Unit %d Port %d" % (unit_id, port_id)
                print "Look for port", 'Generated Service ' + n
                svc = self.sched.services.find_srv_by_name_and_hostname("sw_0", 'Generated Service ' + n)
                self.assert_(svc is not None)
        for unit_id in xrange(1, 7):
            port_id = 47
            n = "Unit %d Port %d" % (unit_id, port_id)
            print "Look for port", 'Generated Service ' + n
            svc = self.sched.services.find_srv_by_name_and_hostname("sw_0", 'Generated Service ' + n)
            self.assert_(svc is not None)

    def test_service_generators_array(self):

        host = self.sched.hosts.find_by_name("sw_1")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router

        print "All service of", "sw_1"
        for s in host.services:
            print s.get_name()

        svc = self.sched.services.find_srv_by_name_and_hostname("sw_1", 'Generated Service Gigabit0/1')
        self.assert_(svc is not None)
        self.assert_(svc.check_command.call == 'check_service!1!80%!90%')

        svc = self.sched.services.find_srv_by_name_and_hostname("sw_1", 'Generated Service Gigabit0/2')
        self.assert_(svc is not None)
        self.assert_(svc.check_command.call == 'check_service!2!80%!90%')

        svc = self.sched.services.find_srv_by_name_and_hostname("sw_1", 'Generated Service Ethernet0/1')
        self.assert_(svc is not None)
        self.assert_(svc.check_command.call == 'check_service!3!80%!95%')

        svc = self.sched.services.find_srv_by_name_and_hostname("sw_1", 'Generated Service ISDN1')
        self.assert_(svc is not None)
        self.assert_(svc.check_command.call == 'check_service!4!80%!95%')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_nohost
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestServiceNoHost(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_nohost.cfg')

    def test_service_with_no_host(self):
        # Be sure than the conf is valid (a service is void, but it's not a crime)
        # and it will not be defined
        self.assert_(self.sched.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_on_missing_template
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestSrvOnMissingTemplate(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_service_on_missing_template.cfg')

    def test_missing_template(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_2", "ZE-SERVICE")
        self.assert_(svc is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_template_inheritance
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test attribute inheritance and the right order
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_template_inheritance.cfg')

    def test_action_url(self):
        # base-service-prod,no-graph
        svc1 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        # no-graph,base-service-prod
        svc2 = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_1")
        self.assert_(svc1.action_url.startswith("/"))
        self.assert_(svc1.process_perf_data == True)
        self.assert_(not svc2.action_url)
        self.assert_(svc2.process_perf_data == False)

        print svc1.tags
        self.assert_('no-graph' in svc1.tags)
        self.assert_('base-service-prod' in svc1.tags)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_tpl_on_host_tpl
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestSrvTplOnHostTpl(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_tpl_on_host_tpl.cfg')

    # Look is a service template apply on a host one will
    # make hosts that inherit from it got such service
    def test_service_tpl_on_host_tpl(self):
        # In fact the whole thing will be to have the service defined :)
        host = self.sched.hosts.find_by_name("test_host_0")
        print "All the test_host_0 services"
        for s in host.services:
            print s.get_dbg_name()

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "Service_Template_Description")
        self.assert_(svc is not None)

    # And look for multy layer template too. Like a service is apply on
    # layer1, that use layer2. And srv is apply on layer2
    def test_service_tpl_on_host_tpl_n_layers(self):

        host = self.sched.hosts.find_by_name("host_multi_layers")
        print "All the test_host_0 services"
        for s in host.services:
            print s.get_dbg_name()

        svc = self.sched.services.find_srv_by_name_and_hostname("host_multi_layers", "srv_multi_layer")
        self.assert_(svc is not None)

    # And look for multy layer template too. Like a service is apply on
    # layer1, that use layer2. And srv is apply on layer2
    def test_complex_expr(self):
        h_linux = self.sched.hosts.find_by_name("host_linux_http")
        print "All the host_linux_http services"
        for s in h_linux.services:
            print s.get_dbg_name()

        # The linux and http service should exist on the linux host
        svc = self.sched.services.find_srv_by_name_and_hostname("host_linux_http", "http_AND_linux")
        self.assert_(svc is not None)

        # But not on the windows one
        h_windows = self.sched.hosts.find_by_name("host_windows_http")
        print "All the host_windows_http services"
        for s in h_windows.services:
            print s.get_dbg_name()

        # The linux and http service should exist on the linux host
        svc = self.sched.services.find_srv_by_name_and_hostname("host_windows_http", "http_AND_linux")
        self.assert_(svc is None)

        # The http_OR_linux should be every where
        svc = self.sched.services.find_srv_by_name_and_hostname("host_linux_http", "http_OR_linux")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("host_windows_http", "http_OR_linux")
        self.assert_(svc is not None)

        # The http_BUT_NOT_linux should be in the windows host only
        svc = self.sched.services.find_srv_by_name_and_hostname("host_linux_http", "http_BUT_NOT_linux")
        self.assert_(svc is None)
        svc = self.sched.services.find_srv_by_name_and_hostname("host_windows_http", "http_BUT_NOT_linux")
        self.assert_(svc is not None)

        # The http_ALL_BUT_NOT_linux should be in the windows host only
        svc = self.sched.services.find_srv_by_name_and_hostname("host_linux_http", "http_ALL_BUT_NOT_linux")
        self.assert_(svc is None)
        svc = self.sched.services.find_srv_by_name_and_hostname("host_windows_http", "http_ALL_BUT_NOT_linux")
        self.assert_(svc is not None)

        # The http_ALL_BUT_NOT_linux_AND_EVEN_LINUX should be every where :)
        # yes, it's a stupid example, but at least it help to test :)
        svc = self.sched.services.find_srv_by_name_and_hostname("host_linux_http", "http_ALL_BUT_NOT_linux_AND_EVEN_LINUX")
        self.assert_(svc is not None)
        svc = self.sched.services.find_srv_by_name_and_hostname("host_windows_http", "http_ALL_BUT_NOT_linux_AND_EVEN_LINUX")
        self.assert_(svc is not None)





if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_withhost_exclude
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class Testservice_withhost_exclude(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_withhost_exclude.cfg')

    def test_service_withhost_exclude(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        svc_exist = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "NotEverywhere")
        self.assert_(svc_exist is not None)
        svc_not_exist = self.sched.services.find_srv_by_name_and_hostname("test_router_0", "NotEverywhere")
        self.assert_(svc_not_exist is None)
        self.assert_(self.sched.conf.is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_without_host
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class Testservice_without_host(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_service_without_host.cfg')

    def test_service_without_host_do_not_break(self):
        self.assert_(self.conf.conf_is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_service_with_print_as_name
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestServiceWithPrintName(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_service_with_print_as_name.cfg')

    def test_service_with_print_as_name(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "print")
        self.assert_(svc is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_spaces_in_commands
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_spaces_in_commands.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_port_2")
        ## for a in host.actions:
        ##     a.t_to_go = 0
        svc.schedule()
        for a in svc.actions:
            a.t_to_go = 0
        # the scheduler need to get this new checks in its own queues
        self.sched.get_new_actions()
        untaggued_checks = self.sched.get_to_run_checks(True, False, poller_tags=['None'])
        cc = untaggued_checks[0]
        # There must still be a sequence of 10 blanks
        self.assert_(cc.command.find("Port 2          ") != -1)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_srv_badhost
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestServiceWhithBadHost(ShinkenTest):
    def setUp(self):
        try:
            self.setup_with_file('etc/shinken_srv_badhost.cfg')
        except AttributeError:
            pass

    # Nagios allow service with no host to exist, it will just drop them
    def test_ServiceWhitNoHost(self):
        self.assert_(self.conf.conf_is_correct == False)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_srv_nohost
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestServiceWhitNoHost(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_srv_nohost.cfg')

    # Nagios allow service with no host to exist, it will just drop them
    def test_ServiceWhitNoHost(self):
        self.assert_(self.sched.conf.is_correct)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_startmember_group
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestStarMemberGroup(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_startmember_group.cfg')

    # Check if service apply on a hostgroup * is good or not
    def test_starmembergroupdef(self):
        hg = self.sched.conf.hostgroups.find_by_name('ping-servers')
        self.assert_(hg is not None)
        print hg.members
        h = self.sched.conf.hosts.find_by_name('test_host_0')
        r = self.sched.conf.hosts.find_by_name('test_router_0')
        self.assert_(h in hg.members and r in hg.members)

        s = self.sched.conf.services.find_srv_by_name_and_hostname('test_host_0', 'PING')
        self.assert_(s is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_star_in_hostgroups
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestStarInGroups(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_star_in_hostgroups.cfg')

    # If we reach a good start, we are ok :)
    # the bug was that an * hostgroup expand get all host_name != ''
    # without looking at register 0 or not
    def test_star_in_groups(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "TEST")
        self.assert_(svc is not None)

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "TEST_HNAME_STAR")
        self.assert_(svc is not None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_strange_characters_commands
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
time_hacker.set_real_time()



class TestStrangeCaracterInCommands(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_strange_characters_commands.cfg')

    # Try to call check dummy with very strange caracters and co, see if it run or
    # failed badly
    def test_strange_characters_commands(self):
        if os.name == 'nt':
            return
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults
        #self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']])
        #self.assert_(host.state == 'UP')
        #self.assert_(host.state_type == 'HARD')
        print svc.check_command
        self.assert_(len(svc.checks_in_progress) == 0)
        svc.launch_check(time.time())
        print svc.checks_in_progress
        self.assert_(len(svc.checks_in_progress) == 1)
        c = svc.checks_in_progress.pop()
        #print c
        c.execute()
        time.sleep(0.5)
        c.check_finished(8000)
        print c.status
        self.assert_(c.status == 'done')
        self.assert_(c.output == '')
        print "Done with good output, that's great"
        svc.consume_result(c)
        self.assert_(svc.output == unicode(''.decode('utf8')))



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_system_time_change
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
import commands


class TestSystemTimeChange(ShinkenTest):
    # setUp is inherited from ShinkenTest

    def set_time(self, d):
        cmd = 'sudo date -s "%s"' % d
        print "CMD,", cmd
        # NB: disabled for now because we test in a totally direct way
        #a = commands.getstatusoutput(cmd)
        # Check the time is set correctly!
        #self.assert_(a[0] == 0)

    def test_system_time_change(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        host = self.sched.hosts.find_by_name("test_host_0")
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        now = time.time()
        now_str = time.asctime(time.localtime(now))
        print "Now:", now
        print "Now:", time.asctime(time.localtime(now))
        tomorow = time.asctime(time.localtime(now + 86400))
        yesterday = time.asctime(time.localtime(now - 86400))

        # Simulate a change now, because by default the value is 1970
        host.last_state_change = now

        host.schedule()
        host_check = host.actions[0]

        svc.schedule()
        srv_check = svc.actions[0]
        print "Service check", srv_check, time.asctime(time.localtime(srv_check.t_to_go))

        print "Current Host last_state_change", time.asctime(time.localtime(host.last_state_change))

        # Ok, start to check for bad time
        self.set_time(tomorow)
        last_state_change = host.last_state_change
        host.compensate_system_time_change(86400)
        self.assert_(host.last_state_change - last_state_change == 86400)
        svc.compensate_system_time_change(86400)
        print "Tomorow Host last_state_change", time.asctime(time.localtime(host.last_state_change))

        # And now a huge change: yesterday (so a 2 day move)
        self.set_time(yesterday)
        last_state_change = host.last_state_change
        host.compensate_system_time_change(-86400 * 2)
        self.assert_(host.last_state_change - last_state_change == -86400*2)
        svc.compensate_system_time_change(-86400*2)
        print "Yesterday Host last_state_change", time.asctime(time.localtime(host.last_state_change))

        self.set_time(now_str)

        # Ok, now the scheduler and check things
        # Put checks in the scheduler
        self.sched.get_new_actions()

        host_to_go = host_check.t_to_go
        srv_to_go = srv_check.t_to_go
        print "current Host check", time.asctime(time.localtime(host_check.t_to_go))
        print "current Service check", time.asctime(time.localtime(srv_check.t_to_go))
        self.set_time(tomorow)
        self.sched.sched_daemon.compensate_system_time_change(86400)
        print "Tomorow Host check", time.asctime(time.localtime(host_check.t_to_go))
        print "Tomorow Service check", time.asctime(time.localtime(srv_check.t_to_go))
        self.assert_(host_check.t_to_go - host_to_go == 86400)
        self.assert_(srv_check.t_to_go - srv_to_go == 86400)

        # and yesterday
        host_to_go = host_check.t_to_go
        srv_to_go = srv_check.t_to_go
        self.set_time(yesterday)
        self.sched.sched_daemon.compensate_system_time_change(-86400*2)
        print "Yesterday Host check", time.asctime(time.localtime(host_check.t_to_go))
        print "Yesterday Service check", time.asctime(time.localtime(srv_check.t_to_go))
        print "New host check", time.asctime(time.localtime(host.next_chk))
        self.assert_(host.next_chk == host_check.t_to_go)
        self.assert_(svc.next_chk == srv_check.t_to_go)
        self.assert_(host_check.t_to_go - host_to_go == -86400*2)
        self.assert_(srv_check.t_to_go - srv_to_go == -86400*2)

        self.set_time(now_str)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_timeout
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
# we have an external process, so we must un-fake time functions
time_hacker.set_real_time()

from shinken.worker import Worker
from multiprocessing import Queue, Manager
from shinken.objects.service import Service
from shinken.objects.host import Host
from shinken.objects.contact import Contact
modconf = Module()


class TestTimeout(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_check_timeout.cfg')

    def test_notification_timeout(self):
        if os.name == 'nt':
            return

        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")

        # These queues connect a poller/reactionner with a worker
        to_queue = Queue()
        #manager = Manager()
        from_queue = Queue() #manager.list()
        control_queue = Queue()

        # This testscript plays the role of the reactionner
        # Now "fork" a worker
        w = Worker(1, to_queue, from_queue, 1)
        w.id = 1
        w.i_am_dying = False

        # We prepare a notification in the to_queue
        c = Contact()
        c.contact_name = "mr.schinken"
        n = Notification('PROBLEM', 'scheduled', 'libexec/sleep_command.sh 7', '', svc, '', '', id=1)
        n.status = "queue"
        #n.command = "libexec/sleep_command.sh 7"
        n.t_to_go = time.time()
        n.contact = c
        n.timeout = 2
        n.env = {}
        n.exit_status = 0
        n.module_type = "fork"
        nn = n.copy_shell()

        # Send the job to the worker
        msg = Message(id=0, type='Do', data=nn)
        to_queue.put(msg)

        w.checks = []
        w.returns_queue = from_queue
        w.s = to_queue
        w.c = control_queue
        # Now we simulate the Worker's work() routine. We can't call it
        # as w.work() because it is an endless loop
        for i in xrange(1, 10):
            w.get_new_checks()
            # During the first loop the sleeping command is launched
            w.launch_new_checks()
            w.manage_finished_checks()
            time.sleep(1)

        # The worker should have finished it's job now, either correctly or
        # with a timeout
        o = from_queue.get()

        self.assert_(o.status == 'timeout')
        self.assert_(o.exit_status == 3)
        self.assert_(o.execution_time < n.timeout+1)

        # Be a good poller and clean up.
        to_queue.close()
        control_queue.close()

        # Now look what the scheduler says to all this
        self.sched.actions[n.id] = n
        self.sched.put_results(o)
        self.show_logs()
        self.assert_(self.any_log_match("Contact mr.schinken service notification command 'libexec/sleep_command.sh 7 ' timed out after 2 seconds"))



    def test_notification_timeout_on_command(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        print svc.checks_in_progress
        cs = svc.checks_in_progress
        self.assert_(len(cs) == 1)
        c = cs.pop()
        print c
        print c.timeout
        self.assert_(c.timeout == 5)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_timeperiods
#!/usr/bin/env python
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.


#
# This file is used to test timeperiods
#

from shinken_test import *
from shinken.objects.timeperiod import Timeperiod


class TestTimeperiods(ShinkenTest):

    def test_simple_timeperiod(self):
        self.print_header()
        t = Timeperiod()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_12

        # First a false test, no results
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, '1999-01-28  00:00-24:00')
        t_next = t.get_next_valid_time_from_t(now)
        self.assert_(t_next is None)

        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, 'tuesday 16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print t_next
        self.assert_(t_next == "Tue Jul 13 16:30:00 2010")

    def test_simple_with_multiple_time(self):
        self.print_header()
        t = Timeperiod()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_12

        # First a false test, no results
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, '1999-01-28  00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(now)
        self.assert_(t_next is None)

        # Then a simple same day
        print "Cheking validity for", time.asctime(time.localtime(july_the_12))
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "RES:", t_next
        self.assert_(t_next == "Tue Jul 13 00:00:00 2010")

        # Now ask about at 00:00 time?
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 00:00:00", "%d %b %Y %H:%M:%S"))
        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "Next?", t_next
        self.assert_(t_next == "Tue Jul 13 00:00:00 2010")

    def test_simple_with_multiple_time_mutltiple_days(self):
        self.print_header()
        t = Timeperiod()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_12

        # First a false test, no results
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, '1999-01-28  00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(now)
        self.assert_(t_next is None)

        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = ''
        # monday          00:00-07:00,21:30-24:00
        # tuesday         00:00-07:00,21:30-24:00
        print "Cheking validity for", time.asctime(time.localtime(july_the_12))
        t.resolve_daterange(t.dateranges, 'monday 00:00-07:00,21:30-24:00')
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "RES:", t_next
        self.assert_(t_next == "Mon Jul 12 21:30:00 2010")
        # what about the next invalid?
        t_next_inv = t.get_next_invalid_time_from_t(july_the_12)
        t_next_inv = time.asctime(time.localtime(t_next_inv))
        print "RES:", t_next_inv
        self.assert_(t_next_inv == "Mon Jul 12 15:00:00 2010")
        # what about a valid time and ask next invalid? Like at 22:00h?
        print "GO" * 10
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 22:00:00", "%d %b %Y %H:%M:%S"))
        t_next_inv = t.get_next_invalid_time_from_t(july_the_12)
        t_next_inv = time.asctime(time.localtime(t_next_inv))
        print "RES:", t_next_inv #, t.is_time_valid(july_the_12)
        self.assert_(t_next_inv == "Tue Jul 13 07:01:00 2010")

        # Now ask about at 00:00 time?
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 00:00:00", "%d %b %Y %H:%M:%S"))
        print "Cheking validity for", time.asctime(time.localtime(july_the_12))
        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, 'monday 00:00-07:00,21:30-24:00')
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-07:00,21:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "Next?", t_next
        self.assert_(t_next == "Mon Jul 12 00:00:00 2010")

        # Now look for the never case
        print "24x7" * 10
        t = self.conf.timeperiods.find_by_name('24x7')
        self.assert_(t is not None)
        t_next_inv = t.get_next_invalid_time_from_t(july_the_12)
        t_next_inv = time.asctime(time.localtime(t_next_inv))
        print "RES:", t_next_inv #, t.is_time_valid(july_the_12)
        self.assert_(t_next_inv == 'Wed Jul 13 00:01:00 2011')

    def test_simple_timeperiod_with_exclude(self):
        self.print_header()
        t = Timeperiod()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_12

        # First a false test, no results
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, '1999-01-28  00:00-24:00')
        t_next = t.get_next_valid_time_from_t(now)
        self.assert_(t_next is None)

        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = ''
        t.resolve_daterange(t.dateranges, 'tuesday 16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print t_next
        self.assert_(t_next == "Tue Jul 13 16:30:00 2010")

        # Now we add this timeperiod an exception
        t2 = Timeperiod()
        t2.timeperiod_name = ''
        t2.resolve_daterange(t2.dateranges, 'tuesday 08:30-21:00')
        t.exclude = [t2]
        # So the next will be after 16:30 and not before 21:00. So
        # It will be 21:00:01 (first second after invalid is valid)

        # we clean the cache of previous calc of t ;)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "T nxt with exclude:", t_next
        self.assert_(t_next == "Tue Jul 13 21:00:01 2010")

    def test_dayweek_timeperiod_with_exclude(self):
        self.print_header()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))

        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = 'T1'
        t.resolve_daterange(t.dateranges, 'tuesday 2 16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "T next", t_next
        self.assert_(t_next == "Tue Jul 13 16:30:00 2010")

        # Now we add this timeperiod an exception
        t2 = Timeperiod()
        t2.timeperiod_name = 'T2'
        t2.resolve_daterange(t2.dateranges, 'tuesday 00:00-23:58')
        t.exclude = [t2]
        # We are a bad boy: first time period want a tuesday
        # but exclude do not want it until 23:58. So next is 58 + 1second :)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_exclude = t2.get_next_valid_time_from_t(july_the_12)
        t_exclude_inv = t2.get_next_invalid_time_from_t(july_the_12)

        print "T next raw", t_next
        t_next = time.asctime(time.localtime(t_next))
        print "TOTO T next", t_next

        self.assert_(t_next == 'Tue Jul 13 23:58:01 2010')

    def test_mondayweek_timeperiod_with_exclude(self):
        self.print_header()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))

        # Then a simple same day
        t = Timeperiod()
        t.timeperiod_name = 'T1'
        t.resolve_daterange(t.dateranges, 'tuesday 2 16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        self.assert_(t_next == "Tue Jul 13 16:30:00 2010")

        # Now we add this timeperiod an exception
        # And a good one: from april (so before so agust (after), and full time.
        # But the 17 is a tuesday, but the 3 of august, so the next 2 tuesday is
        # ..... the Tue Sep 14 :) Yes, we should wait quite a lot :)
        t2 = Timeperiod()
        t2.timeperiod_name = 'T2'
        t2.resolve_daterange(t2.dateranges, 'april 1 - august 16 00:00-24:00')
        #print t2.__dict__
        t.exclude = [t2]
        # We are a bad boy: first time period want a tuesday
        # but exclude do not want it until 23:58. So next is 59 :)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        #print "Check from", time.asctime(time.localtime(july_the_12))
        #t_exclude = t2.get_next_valid_time_from_t(july_the_12)
        t_exclude_inv = t2.get_next_invalid_time_from_t(july_the_12)
        #print "T2 next valid", time.asctime(time.localtime(t_exclude))
        print "Next invalid T2", time.asctime(time.localtime(t_exclude_inv))

        print "T next raw JEAN", t_next
        print "T next?", time.asctime(time.localtime(t_next))
        t_next = time.asctime(time.localtime(t_next))

        self.assert_(t_next == 'Tue Sep 14 16:30:00 2010')

    def test_mondayweek_timeperiod_with_exclude_bis(self):
        self.print_header()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))

        # Then a funny daterange
        print "Testing daterange", 'tuesday -1 - monday 1  16:30-24:00'
        t = Timeperiod()
        t.timeperiod_name = 'T1'
        t.resolve_daterange(t.dateranges, 'tuesday -1 - monday 1  16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "Next without exclude", t_next
        self.assert_(t_next == "Tue Jul 27 16:30:00 2010")

        # Now we add this timeperiod an exception
        # And a good one: from april (so before so agust (after), and full time.
        # But the 27 is nw not possible? So what next? Add a month!
        # last tuesday of august, the 31 :)
        t2 = Timeperiod()
        t2.timeperiod_name = 'T2'
        t2.resolve_daterange(t2.dateranges, 'april 1 - august 16 00:00-24:00')
        #print t2.__dict__
        t.exclude = [t2]
        # We are a bad boy: first time period want a tuesday
        # but exclude do not want it until 23:58. So next is 59 :)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        #print "Check from", time.asctime(time.localtime(july_the_12))
        #t_exclude = t2.get_next_valid_time_from_t(july_the_12)
        t_exclude_inv = t2.get_next_invalid_time_from_t(july_the_12)
        #print "T2 next valid", time.asctime(time.localtime(t_exclude))
        print "Next invalid T2", time.asctime(time.localtime(t_exclude_inv))

        print "T next raw JEAN2", t_next
        print "T next?", time.asctime(time.localtime(t_next))
        t_next = time.asctime(time.localtime(t_next))

        self.assert_(t_next == 'Tue Aug 31 16:30:00 2010')

    def test_funky_mondayweek_timeperiod_with_exclude_and_multiple_daterange(self):
        self.print_header()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))

        # Then a funny daterange
        print "Testing daterange", 'tuesday -1 - monday 1  16:30-24:00'
        t = Timeperiod()
        t.timeperiod_name = 'T1'
        t.resolve_daterange(t.dateranges, 'tuesday -1 - monday 1  16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "Next without exclude", t_next
        self.assert_(t_next == "Tue Jul 27 16:30:00 2010")

        # Now we add this timeperiod an exception
        # And a good one: from april (so before so agust (after), and full time.
        # But the 27 is nw not possible? So what next? Add a month!
        # But maybe it's not enoutgth? :)
        # The withoutthe 2nd exclude, it's the Tues Aug 31, btu it's inside
        # saturday -1 - monday 1 because saturday -1 is the 28 august, so no.
        # in september saturday -1 is the 25, and tuesday -1 is 28, so still no
        # A month again! So now tuesday -1 is 26 and saturday -1 is 30. So ok
        # for this one! that was quite long isn't it? And funky! :)
        t2 = Timeperiod()
        t2.timeperiod_name = 'T2'
        t2.resolve_daterange(t2.dateranges, 'april 1 - august 16 00:00-24:00')
        # Oups, I add a inner daterange ;)
        t2.resolve_daterange(t2.dateranges, 'saturday -1 - monday 1  16:00-24:00')
        t.exclude = [t2]
        # We are a bad boy: first time period want a tuesday
        # but exclude do not want it until 23:58. So next is 59 :)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        #print "Check from", time.asctime(time.localtime(july_the_12))
        #t_exclude = t2.get_next_valid_time_from_t(july_the_12)
        t_exclude_inv = t2.get_next_invalid_time_from_t(july_the_12)
        #print "T2 next valid", time.asctime(time.localtime(t_exclude))
        print "Next invalid T2", time.asctime(time.localtime(t_exclude_inv))

        print "T next raw", t_next
        print "T next?", time.asctime(time.localtime(t_next))
        t_next = time.asctime(time.localtime(t_next))

        self.assert_(t_next == 'Tue Oct 26 16:30:00 2010')
        print "Finish this Funky test :)"

    def test_monweekday_timeperiod_with_exclude(self):
        self.print_header()
        now = time.time()
        # Get the 12 of july 2010 at 15:00, monday
        july_the_12 = time.mktime(time.strptime("12 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))

        # Then a funny daterange
        print "Testing daterange", 'tuesday -1 july - monday 1 august  16:30-24:00'
        t = Timeperiod()
        t.timeperiod_name = 'T1'
        t.resolve_daterange(t.dateranges, 'tuesday -1 july - monday 1 september  16:30-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_12)
        t_next = time.asctime(time.localtime(t_next))
        print "Next without exclude", t_next
        self.assert_(t_next == "Tue Jul 27 16:30:00 2010")

        # Now we add this timeperiod an exception
        # and from april (before) to august monday 3 (monday 16 august),
        # so Jul 17 is no more possible. So just after it, Tue 17
        t2 = Timeperiod()
        t2.timeperiod_name = 'T2'
        t2.resolve_daterange(t2.dateranges, 'thursday 1 april - monday 3 august 00:00-24:00')
        print t2.dateranges[0].__dict__
        t.exclude = [t2]
        # We are a bad boy: first time period want a tuesday
        # but exclude do not want it until 23:58. So next is 59 :)
        t.cache = {}
        t_next = t.get_next_valid_time_from_t(july_the_12)
        #print "Check from", time.asctime(time.localtime(july_the_12))
        #t_exclude = t2.get_next_valid_time_from_t(july_the_12)
        t_exclude_inv = t2.get_next_invalid_time_from_t(july_the_12)
        #print "T2 next valid", time.asctime(time.localtime(t_exclude))
        print "Next invalid T2", time.asctime(time.localtime(t_exclude_inv))

        print "T next raw", t_next
        print "T next?", time.asctime(time.localtime(t_next))
        t_next = time.asctime(time.localtime(t_next))

        self.assert_(t_next == 'Tue Aug 17 16:30:00 2010')

    def test_dayweek_exclusion_timeperiod(self):
        self.print_header()
        t = Timeperiod()
        now = time.time()
        # Get the 13 of july 2010 at 15:00, tuesday
        july_the_13 = time.mktime(time.strptime("13 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_13

        # Now we add this timeperiod an exception
        t2 = Timeperiod()
        t2.timeperiod_name = ''
        t2.resolve_daterange(t2.dateranges, 'tuesday 00:00-24:00')
        t.exclude = [t2]
	
        t.resolve_daterange(t.dateranges, 'monday 00:00-24:00')
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-24:00')
        t.resolve_daterange(t.dateranges, 'wednesday 00:00-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_13)
        t_next = time.asctime(time.localtime(t_next))
        print "T next", t_next
        self.assert_(t_next == "Wed Jul 14 00:00:00 2010")

    def test_dayweek_exclusion_timeperiod_with_day_range(self):
        self.print_header()
        t = Timeperiod()
        # Get the 13 of july 2010 at 15:00, tuesday
        july_the_13 = time.mktime(time.strptime("13 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_13

        # Now we add this timeperiod an exception
        t2 = Timeperiod()
        t2.timeperiod_name = ''
        t2.resolve_daterange(t2.dateranges, 'tuesday 00:00-24:00')
        t.exclude = [t2]

        t.resolve_daterange(t.dateranges, '2010-03-01 - 2020-03-01 00:00-24:00')
        t_next = t.get_next_valid_time_from_t(july_the_13)
        t_next = time.asctime(time.localtime(t_next))

        now = time.time()
        now = time.asctime(time.localtime(now))

        print "T next", t_next
    #    print "T now", now
    #    self.assert_(t_next == now)
        self.assert_(t_next == "Wed Jul 14 00:00:01 2010")

    # short test to check the invalid function of timeranges
    def test_next_invalid_day(self):
        self.print_header()

        # Get the 13 of july 2010 at 15:00, tuesday
        july_the_13 = time.mktime(time.strptime("13 Jul 2010 15:00:00", "%d %b %Y %H:%M:%S"))
        print july_the_13

        t = Timeperiod()
        t.timeperiod_name = 'test_next_invalid_day'
        t.resolve_daterange(t.dateranges, 'tuesday 00:00-24:00')
        t.exclude = []

        t_next_invalid = t.get_next_invalid_time_from_t(july_the_13)
        t_next_invalid = time.asctime(time.localtime(t_next_invalid))
        print "T next invalid", t_next_invalid
        self.assert_(t_next_invalid == "Wed Jul 14 00:00:01 2010")

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_timeperiods_state_logs
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestTPStateLog(ShinkenTest):

    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_timeperiods_state_logs.cfg')


    # A timeperiod state change should raise a log, and only when change.
    def test_tp_state_log(self):
        now = time.time()
        tp = self.sched.timeperiods.find_by_name('24x7')

        self.assert_(tp is not None)
        tp.check_and_log_activation_change()
        self.assert_(self.any_log_match("TIMEPERIOD TRANSITION: 24x7;-1;1"))
        self.show_and_clear_logs()

        # Now make this tp unable to be active again by removing al it's daterange:p
        dr = tp.dateranges
        tp.dateranges = []
        tp.check_and_log_activation_change()
        self.assert_(self.any_log_match("TIMEPERIOD TRANSITION: 24x7;1;0"))
        self.show_and_clear_logs()

        # Ok, let get back to work :)
        tp.dateranges = dr
        tp.check_and_log_activation_change()
        self.assert_(self.any_log_match("TIMEPERIOD TRANSITION: 24x7;0;1"))
        self.show_and_clear_logs()



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_timeperiod_inheritance
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_timeperiod_inheritance.cfg')

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the Timeperiods"
        now = time.time()
        tp = self.sched.timeperiods.find_by_name("24x7")
        print "TP", tp.__dict__

        # sunday should be inherited from templates
        print "Check for sunday in the timeperiod"
        got_sunday = False
        for dr in tp.dateranges:
            print dr.__dict__
            if hasattr(dr, 'day') and dr.day == 'sunday':
                got_sunday = True
        self.assert_(got_sunday == True)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_triggers
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
from shinken.objects.trigger import Trigger


class TestTriggers(ShinkenTest):
    def setUp(self):
        self.setup_with_file('etc/shinken_triggers.cfg')

    # Try to catch the perf_datas of self
    def test_function_perf(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "sample_perf_function")
        svc.output = 'I am OK'
        svc.perf_data = 'cpu=95%'
        # Go launch it!
        svc.eval_triggers()
        self.scheduler_loop(2, [])
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "not good!")
        self.assert_(svc.perf_data == "cpu=95%")

    # Try to catch the perf_datas of self
    def test_function_perfs(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "AVG-HTTP")

        srvs = []
        for i in xrange(1, 4):
            s = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "HTTP-" + str(i))
            s.output = 'Http ok'
            s.perf_data = 'time=%dms' % i

        # Go launch it!
        svc.eval_triggers()
        self.scheduler_loop(4, [])
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "OK all is green")
        self.assert_(svc.perf_data == "avgtime=2ms")

    # Try to catch the perf_datas of self
    def test_function_custom(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "sample_custom_function")
        svc.output = 'Nb users?'
        svc.perf_data = 'users=6'
        # Go launch it!
        svc.eval_triggers()
        self.scheduler_loop(4, [])
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "OK all is green")
        self.assert_(svc.perf_data == "users=12")

    def test_in_conf_trigger(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "i_got_trigger")
        print 'will run', svc.trigger
        # Go!
        svc.eval_triggers()
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "New output")
        self.assert_(svc.perf_data == "New perf_data")

    # Try to catch the perf_datas of self
    def test_simple_cpu_too_high(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "cpu_too_high")
        svc.output = 'I am OK'
        svc.perf_data = 'cpu=95%'
        # Go launch it!
        svc.eval_triggers()
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "not good!")
        self.assert_(svc.perf_data == "cpu=95%")

        # Same with an host
        host = self.sched.hosts.find_by_name("test_host_trigger")
        host.output = 'I am OK'
        host.perf_data = 'cpu=95%'
        # Go launch it!
        host.eval_triggers()
        self.scheduler_loop(2, [])
        print "Output", host.output
        print "Perf_Data", host.perf_data
        self.assert_(host.output == "not good!")
        self.assert_(host.perf_data == "cpu=95")

    # Try to catch the perf_datas of self
    def test_morecomplex_cpu_too_high(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "cpu_too_high_bis")

        firstlen = len([b for b in self.sched.broks.values() if b.type == 'service_check_result'])
        self.scheduler_loop(1, [(svc, 0, 'I am OK | cpu=95%')])
        seclen = len([b for b in self.sched.broks.values() if b.type == 'service_check_result'])
        self.scheduler_loop(1, [])
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        print firstlen, seclen

        self.assert_(svc.output == "not good!")
        self.assert_(svc.perf_data == "cpu=95")
        self.assert_(firstlen == seclen)

    # Try to load .trig files
    def test_trig_file_loading(self):
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "cpu_too_high_ter")
        t = self.conf.triggers.find_by_name('simple_cpu')
        self.assert_(t in svc.triggers)
        svc.output = 'I am OK'
        svc.perf_data = 'cpu=95%'
        svc.eval_triggers()
        self.scheduler_loop(2, [])
        print "Output", svc.output
        print "Perf_Data", svc.perf_data
        self.assert_(svc.output == "not good!")
        self.assert_(svc.perf_data == "cpu=95")

        # same for host
        host = self.sched.hosts.find_by_name('test_host_trigger2')
        t = self.conf.triggers.find_by_name('simple_cpu')
        self.assert_(t in host.triggers)
        host.output = 'I am OK'
        host.perf_data = 'cpu=95%'
        host.eval_triggers()
        self.scheduler_loop(2, [])
        print "Output", host.output
        print "Perf_Data", host.perf_data
        self.assert_(host.output == "not good!")
        self.assert_(host.perf_data == "cpu=95")

    def test_simple_triggers(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        code = '''r = self.get_name()'''.replace(r'\n', '\n').replace(r'\t', '\t')
        t = Trigger({'trigger_name': 'none', 'code_src': code})
        t.compile()
        r = t.eval(svc)
        print r

        code = '''self.output = "Moncul c'est du poulet" '''.replace(r'\n', '\n').replace(r'\t', '\t')
        t = Trigger({'trigger_name': 'none', 'code_src': code})
        t.compile()
        r = t.eval(svc)
        print "Service output", svc.output
        self.assert_(svc.output == "Moncul c'est du poulet")

        code = '''self.output = "Moncul c'est du poulet2"
self.perf_data = "Moncul c'est du poulet3"
'''.replace(r'\n', '\n').replace(r'\t', '\t')
        t = Trigger({'trigger_name': 'none', 'code_src': code})
        t.compile()
        r = t.eval(svc)
        print "Service output", svc.output
        print "Service perf_data", svc.perf_data
        self.assert_(svc.output == "Moncul c'est du poulet2")
        self.assert_(svc.perf_data == "Moncul c'est du poulet3")



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_uknown_event_handler
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestUnknownEventHandler(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_uknown_event_handler.cfg')

    def test_dummy(self):
        self.assert_(not self.conf.conf_is_correct)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_unknown_do_not_change
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestUnknownNotChangeState(ShinkenTest):

    # Uncomment this is you want to use a specific configuration
    # for your test
    #def setUp(self):
    #    self.setup_with_file('etc/shinken_1r_1h_1s.cfg')


    # We got problem with unknown results on bad connections
    # for critical services and host: if it was in a notification pass
    # then the notification is restarted, but it's just a missing data,
    # not a reason to warn about it
    def test_unknown_do_not_change_state(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        print "GO OK" * 10
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(svc.state == 'OK')
        self.assert_(svc.state_type == 'HARD')

        print "GO CRITICAL SOFT" * 10
        # Ok we are UP, now we seach to go in trouble
        self.scheduler_loop(1, [[svc, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/SOFT
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_type == 'SOFT')
        # And again and again :)
        print "GO CRITICAL HARD" * 10
        self.scheduler_loop(2, [[svc, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/HARD
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_type == 'HARD')

        # Should have a notification about it
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        print "GO UNKNOWN HARD" * 10
        # Then we make it as a unknown state
        self.scheduler_loop(1, [[svc, 3, 'Unknown | value1=1 value2=2']])
        # And we DO NOT WANT A NOTIF HERE
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;UNKNOWN'))
        self.show_and_clear_logs()

        print "Return CRITICAL HARD" * 10
        # Then we came back as CRITICAL
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        print "Still CRITICAL HARD" * 10
        # Then we came back as CRITICAL
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        # We check if we can still have new notifications of course
        # And we speedup the notification
        for n in svc.notifications_in_progress.values():
            n.t_to_go = time.time()
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

    # We got problem with unknown results on bad connections
    # for critical services and host: if it was in a notification pass
    # then the notification is restarted, but it's just a missing data,
    # not a reason to warn about it
    def test_unknown_do_not_change_state_with_different_exit_status_phase(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        print "GO OK" * 10
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(svc.state == 'OK')
        self.assert_(svc.state_type == 'HARD')

        print "GO CRITICAL SOFT" * 10
        # Ok we are UP, now we seach to go in trouble
        self.scheduler_loop(1, [[svc, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/SOFT
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_type == 'SOFT')
        # And again and again :)
        print "GO CRITICAL HARD" * 10
        self.scheduler_loop(2, [[svc, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/HARD
        self.assert_(svc.state == 'CRITICAL')
        self.assert_(svc.state_type == 'HARD')

        # Should have a notification about it
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

        print "GO UNKNOWN HARD" * 10
        # Then we make it as a unknown state
        self.scheduler_loop(1, [[svc, 3, 'Unknown | value1=1 value2=2']])
        # And we DO NOT WANT A NOTIF HERE
        self.assert_(not self.any_log_match('SERVICE NOTIFICATION.*;UNKNOWN'))
        self.show_and_clear_logs()

        print "Return CRITICAL HARD" * 10
        # Then we came back as WARNING here, so a different than we came in the phase!
        self.scheduler_loop(1, [[svc, 1, 'WARNING | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;WARNING'))
        self.show_and_clear_logs()

        # We check if we can still have new notifications of course
        # And we speedup the notification
        for n in svc.notifications_in_progress.values():
            n.t_to_go = time.time()
        self.scheduler_loop(1, [[svc, 1, 'WARNING | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;WARNING'))
        self.show_and_clear_logs()

        # And what if we came back as critical so? :)
        self.scheduler_loop(1, [[svc, 2, 'CRITICAL | value1=1 value2=2']])
        print svc.state, svc.state_type
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

    # But we want to still raise notif as unknown if we first met this state
    def test_unknown_still_raise_notif(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        router.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(svc.state == 'OK')
        self.assert_(svc.state_type == 'HARD')

        # Ok we are UP, now we seach to go in trouble
        self.scheduler_loop(1, [[svc, 3, 'PROBLEM | value1=1 value2=2']])
        # UNKOWN/SOFT
        self.assert_(svc.state == 'UNKNOWN')
        self.assert_(svc.state_type == 'SOFT')
        # And again and again :)
        self.scheduler_loop(2, [[svc, 3, 'PROBLEM | value1=1 value2=2']])
        # UNKNOWN/HARD
        self.assert_(svc.state == 'UNKNOWN')
        self.assert_(svc.state_type == 'HARD')

        # Should have a notification about it!
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;UNKNOWN'))
        self.show_and_clear_logs()

        # Then we make it as a critical state
        # and we want a notif too
        self.scheduler_loop(1, [[svc, 2, 'critical | value1=1 value2=2']])
        self.assert_(self.any_log_match('SERVICE NOTIFICATION.*;CRITICAL'))
        self.show_and_clear_logs()

    # We got problem with unknown results on bad connections
    # for critical services and host: if it was in a notification pass
    # then the notification is restarted, but it's just a missing data,
    # not a reason to warn about it
    def test_unreach_do_not_change_state(self):
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        router = self.sched.hosts.find_by_name("test_router_0")
        router.checks_in_progress = []
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        print "GO OK" * 10
        self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 0, 'OK | value1=0 value2=0']])
        self.assert_(svc.state == 'OK')
        self.assert_(svc.state_type == 'HARD')

        print "GO DOWN SOFT" * 10
        # Ok we are UP, now we seach to go in trouble
        self.scheduler_loop(1, [[host, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/SOFT
        self.assert_(host.state == 'DOWN')
        self.assert_(host.state_type == 'SOFT')
        # And again and again :)
        print "GO CRITICAL HARD" * 10
        self.scheduler_loop(2, [[host, 2, 'PROBLEM | value1=1 value2=2']])
        # CRITICAL/HARD
        self.assert_(host.state == 'DOWN')
        self.assert_(host.state_type == 'HARD')

        # Should have a notification about it
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;DOWN'))
        self.show_and_clear_logs()

        print "GO UNREACH HARD" * 10
        # Then we make it as a unknown state
        self.scheduler_loop(3, [[router, 2, 'Bad router | value1=1 value2=2']])
        # so we warn about the router, not the host
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;DOWN'))
        self.show_and_clear_logs()

        print "BIBI" * 100
        for n in host.notifications_in_progress.values():
            print n.__dict__

        # the we go in UNREACH
        self.scheduler_loop(1, [[host, 2, 'CRITICAL | value1=1 value2=2']])
        print host.state, host.state_type
        self.show_and_clear_logs()
        self.assert_(host.state == 'UNREACHABLE')
        self.assert_(host.state_type == 'HARD')

        # The the router came back :)
        print "Router is back from Hell" * 10
        self.scheduler_loop(1, [[router, 0, 'Ok, I am back guys | value1=1 value2=2']])
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;UP'))
        self.show_and_clear_logs()

        # But how the host will say now?
        self.scheduler_loop(1, [[host, 2, 'CRITICAL | value1=1 value2=2']])
        print host.state, host.state_type
        # And here we DO NOT WANT new notification
        # If you follow, it THE important point of this test!
        self.assert_(not self.any_log_match('HOST NOTIFICATION.*;DOWN'))
        self.show_and_clear_logs()

        print "Now go in the future, I want a notification"
        # Check if we still got the next notification for this of course

        # Hack so the notification will raise now if it can
        for n in host.notifications_in_progress.values():
            n.t_to_go = time.time()
        self.scheduler_loop(1, [[host, 2, 'CRITICAL | value1=1 value2=2']])
        print host.state, host.state_type
        # And here we DO NOT WANT new notification
        self.assert_(self.any_log_match('HOST NOTIFICATION.*;DOWN'))
        self.show_and_clear_logs()





if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_update_output_ext_command
#!/usr/bin/env python
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *


class TestUpdateOutputExtCommand(ShinkenTest):

    def test_dummy(self):
        #
        # Config is not correct because of a wrong relative path
        # in the main config file
        #
        print "Get the hosts and services"
        now = time.time()
        host = self.sched.hosts.find_by_name("test_host_0")
        host.checks_in_progress = []
        host.act_depend_of = []  # ignore the router
        svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0")
        svc.checks_in_progress = []
        svc.act_depend_of = []  # no hostchecks on critical checkresults

        cmd = "[%lu] PROCESS_SERVICE_OUTPUT;test_host_0;test_ok_0;My ass is cool | toto=30%%" % now
        self.sched.run_external_command(cmd)
        self.scheduler_loop(2, [])
        print svc.perf_data
        self.assert_(svc.perf_data == 'toto=30%')

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_utf8_log
#!/usr/bin/env python
#  -*- coding: utf-8 -*-
#
# Copyright (C) 2009-2010:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

#
# This file is used to test reading and processing of config files
#

from shinken_test import *
from shinken.log import logger


class TestConfig(ShinkenTest):

    def setUp(self):
        self.setup_with_file('etc/shinken_1r_1h_1s.cfg')

    # Try to raise an utf8 log message
    def test_utf8log(self):
        sutf = 'h\351h\351'  # Latin Small Letter E with acute in Latin-1
        logger.log(sutf)
        sutf8 = u'I love myself $'  # dollar, pound, currency
        logger.log(sutf8)
        s = unichr(40960) + u'abcd' + unichr(1972)
        logger.log(s)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = __import_shinken
# -*- coding: utf-8 ; mode: python -*-
#
# Copyright (C) 2012:
#    Hartmut Goebel <h.goebel@crazy-compilers.com>
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

"""

Helper module for importing the shinken library from the (uninstalled)
test-suite.

If importing shinken fails, try to load from parent directory to
support running the test-suite without installation.

This does not manipulate sys.path, but uses lower-level Python modules
for looking up and loading the module `shinken` from the directory one
level above this module.
"""

try:
    import shinken
except ImportError:
    import imp, os
    # For security reasons, try not to load `shinken` from parent
    # directory when running as root.
    if True or not hasattr(os, 'getuid') or os.getuid() != 0:
        imp.load_module('shinken', *imp.find_module('shinken',
            [os.path.dirname(os.path.dirname(os.path.abspath(__file__)))]))
    else:
        # running as root: re-raise the exception
        raise


########NEW FILE########
__FILENAME__ = checkmodule
#!/usr/bin/env python
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    David GUENAULT, dguenault@monitoring-fr.org
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import sys
import getopt


def main(argv):
    try:
        opts, args = getopt.getopt(argv, "m:")
        ret = 0
        for o, a in opts:
            if o == "-m":
                try:
                    exec("import " + a)
                    print "OK"
                except:
                    print "KO"
                    ret = 2
    except:
        ret = 1
    sys.exit(ret)

if __name__ == "__main__":
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = skonf
#!/usr/bin/env python
#
# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    David GUENAULT, dguenault@monitoring-fr.org
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

import os
import cmd
import sys
import time
import datetime
import copy
import socket

#try:
#    from shinken.bin import VERSION
#    import shinken
#except ImportError:
#    # If importing shinken fails, try to load from current directory
#    # or parent directory to support running without installation.
#    # Submodules will then be loaded from there, too.
#    import imp
#    imp.load_module('shinken', *imp.find_module('shinken', [os.path.realpath("."), os.path.realpath(".."), os.path.join(os.path.abspath(os.path.dirname(sys.argv[0])), "..")]))


from shinken.bin import VERSION
from shinken.objects.config import Config

import getopt, sys


def usage():
    print "skonf.py -a action -f configfile -o objecttype -d directive -v value -r directive=value,directive=value"
    print ""
    print " * actions:"
    print "   - control (control action is specified with -d [stop|start|restart]). Apply action on all satellites"
    print "   - sync: deploy shinken-specific on all satellites"
    print "   - deploy deploy shinken on hosts defined in authfile (-f path/to/auth)"
    print "   - macros: execute macros file"
    print "   - delobject: remove a shinken object from the shinken configuration file"
    print "   - cloneobject: clone an object (currently only pollers are suported"
    print "   - showconfig: display configuration of object"
    print "   - setparam: set directive value for an object"
    print "   - delparam: remove directive for an object"
    print "   - getdirective: get a directive value from an object"
    print "   - getobjectnames: get a list of objects names (required parameters: configfile, objectype)"
    print " * configfile: full path to the shinken-specific.cfg file"
    print " * objectype: configuration object type on which the action apply"
    print " * directive: the directive name of a configuration object"
    print " * value: the directive value of a configuration object"
    print " * r: this parameter restric the application to objects matching the directive/value pair list"


def main():
    config = ()
    action = ""
    configfile = ""
    objectype = ""
    directive = ""
    value = ""
    filters = ""
    commit = True
    try:
        opts, args = getopt.getopt(sys.argv[1:], "qa:f:o:d:v:r:", [])
    except getopt.GetoptError, err:
        print str(err)
        usage()
        sys.exit(2)
    for o, a in opts:
        if o == "-a":
            actions = ["setparam", "delparam", "showconfig", "addobject", "getobjectnames", "getdirective", "getaddresses", "delobject", "cloneobject", "macros", "sync", "control", "deploy", "removemodule"]
            if a in actions:
                action = a
            else:
                print "Invalid action"
                usage()
                sys.exit(2)
        elif o == "-f":
            configfile = a
        elif o == "-q":
            quiet = 1
        elif o == "-o":
            objectype = a
        elif o == "-d":
            directive = a
        elif o == "-v":
            value = a
        elif o == "-r":
            filters = a
        else:
            assert False, "unhandled option"
            sys.exit(2)


    if action == "":
        print "action is mandatory"
        usage()
        sys.exit(2)

    if configfile == "" and action != "control":
        print "config file is mandatory"
        usage()
        sys.exit(2)

    if objectype == "" and action != "getaddresses" and action != "showconfig" and action != "macros" and action != "sync" and action != "control" and action != "deploy":
        print "object type is mandatory"
        usage()
        sys.exit(2)
    if directive == "" and (action == "setparam" or action == "addobject"):
        print "directive is mandatory"
        usage()
        sys.exit(2)

    if filters == "" and action == "delobject":
        print "filters is mandatory"
        usage()
        sys.exit(2)

    if value == "" and action == "setparam":
        print "value is mandatory"
        usage()
        sys.exit(2)

    if action != "macros" and action != "control" and action != "deploy":
        result, config = loadconfig([configfile])
        if not result:
            print config
            sys.exit(2)
        commit = False
    else:
        config = None

    allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner']

    if action == "setparam":
        result, content = setparam(config, objectype, directive, value, filters)
        print content
        if not result:
            print content
            sys.exit(2)
        else:
            result, content = writeconfig(config, configfile)
            if not result:
                sys.exit(2)
            else:
                sys.exit(0)
    if action == "delparam":
        result, content = delparam(config, objectype, directive, filters)
        print content
        if not result:
            print content
            sys.exit(2)
        else:
            result, content = writeconfig(config, configfile)
            if not result:
                sys.exit(2)
            else:
                sys.exit(0)
    elif action == "macros":
        if directive != "":
            result, content = domacros(configfile, directive.split(','))
        else:
            result, content = domacros(configfile)
        if not result:
            print content
            sys.exit(2)
        else:
            sys.exit(0)
    elif action == "sync":
        if directive == "":
            print "You must specify the authentication file with -d option"
            sys.exit(2)
        result, content = sync(config, configfile, directive)
        if not result:
            print content
            sys.exit(2)
        else:
            sys.exit(0)
    elif action == "control":
        if directive == "":
            print "You must specify the authentication file with -d option"
            sys.exit(2)
        result, content = control(config, directive)
        if not result:
            print content
            sys.exit(2)
        else:
            sys.exit(0)
    elif action == "showconfig":
        allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner', 'module']
        dumpconfig(objectype, config, allowed)
    elif action == "getobjectnames":
        allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner', 'module']
        getobjectnames(objectype, config, allowed)
    elif action == "cloneobject":
        allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner', 'module']
        if objectype not in allowed:
            print "Clone of %s is not supported" % (objectype)
            sys.exit(2)
        else:
            result, confignew = cloneobject(config, objectype, directive, filters)
            if not result:
                print confignew
                sys.exit(2)
            else:
                result, message = writeconfig(confignew, configfile)
                if not result:
                    print message
                    sys.exit(2)
                print "The objectype %s has been cloned with the new attributes: %s" % (objectype, filter)
    elif action == "addobject":
        print "Not implemented"
        sys.exit(2)
    elif action == "delobject":
        result, confignew = delobject(config, objectype, filters)
        if not result:
            print confignew
            sys.exit(2)
        else:
            result, message = writeconfig(confignew, configfile)
            print message
            if not result:
                sys.exit(2)
            else:
                sys.exit(0)
    elif action == "deploy":
        """ deploy shinken on remote hosts """
        result, content = deploy(configfile)
        if not result:
            print content
            sys.exit(2)
        else:
            print "Deploy ok"
    elif action == "getdirective":
        result, content = getdirective(config, objectype, directive, filters)
        if not result:
            print content
            sys.exit(2)
        else:
            print content
            sys.exit(0)
    elif action == "getaddresses":
        getaddresses(config)
    else:
        print "Unknown action %s" % (action)
        sys.exit(2)


def domacros(configfile, args=[]):
    import string
    import re
    """ load macro """
    try:
        fd = open(configfile, 'r')
        data = map(string.strip, fd.readlines())
        fd.close()
    except:
        return (False, "Error while reading macros file")

    authfile = ""

    """ remove comments lines """
    index_line = 0
    cleandata = []
    for line in data:
        if re.match(r"^#", line) == None:
            cleandata.append(line)
        index_line += 1
    index_line = 0
    data = cleandata

    """ merge arguments with macro file content """
    if len(args) > 0:
        index_line = 0
        while index_line < len(data):
            index_args = 0
            tmp = data[index_line]
            while index_args < len(args):
                tmp = tmp.replace("ARG%d" % (index_args+1), args[index_args])
                data[index_line] = tmp
                index_args += 1
            index_line += 1

    allowed = ["arbiter", "scheduler", "poller", "broker", "reactionner", "receiver"]

    commands = {
            "onerror": r"(?P<action>\w+)",
            "setconfigfile": r"(?P<configfile>.*)",
            "setauthfile": r"(?P<authfile>.*)",
            "clone": r"(?P<object>\w+) set (?P<directives>.*) where (?P<clauses>.*)",
            "delete": r"(?P<object>\w+) where (?P<clauses>.*)",
            "showconfig": r"(?P<object>\w+)",
            "setparam": r"(?P<directive>\w+)=(?P<value>.*) from (?P<object>\w+) where (?P<clauses>.*)",
            "delparam": r"(?P<directive>\w+)=(?P<value>.*) from (?P<object>\w+) where (?P<clauses>.*)",
            "getdirective": r"(?P<directives>\w+) from (?P<object>\w+) where (?P<clauses>.*)",
            "removemodule": r"(?P<module>\w+) from (?P<object>\w+) where (?P<clauses>.*)",
            "control": r"(?P<action>\w+)",
            "writeconfig": r"",
            "sync": r""
            }

    """ Compile regexp """
    ccommands = {}
    for cmd, reg in commands.items():
        if reg != "":
            creg = re.compile(r"^(" + cmd + ") " + reg)
            ccommands[cmd] = creg
        else:
            ccommands[cmd] = False
    last = False
    indexline = 1

    """ macros execution """
    for line in data:
        maction = "stop"
        matched = False
        if last != False:
            line = line.replace("LAST", last)
        else:
            line = line.replace("LAST,", "")
        for command, regexp in ccommands.items():
            if re.match("^" + command, line):
                if type(regexp).__name__ == "SRE_Pattern":
                    result = regexp.match(line)
                    if result == None:
                        return (False, "There was an error with %s" % (command))
                    if command == "setconfigfile":
                        code, config = loadconfig([result.group('configfile')])
                        if not code:
                            return (code, config)
                        configfile = result.group('configfile')
                    if command == "setauthfile":
                        authfile = result.group('authfile')
                    elif command == "delete":
                        code, message = delobject(config, result.group('object'), result.group('clauses'))
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "control":
                        code, message = control(authfile, result.group('action'))
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "onerror":
                        if result.group('action') in ('continue', 'stop'):
                            maction = result.group('action')
                        else:
                            return  (False, "Unknown action on error %s" % (result.group('action')))
                    elif command == "clone":
                        code, message = cloneobject(config, result.group('object'), result.group('directives'), result.group('clauses'))
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "showconfig":
                        dumpconfig(result.group('object'), config, allowed)
                    elif command == "getdirective":
                        code, last = getdirective(config, result.group('object'), result.group('directives'), result.group('clauses'))
                        if not code:
                            last = False
                            #return (code,last)
                    elif command == "setparam":
                        code, message = setparam(config, result.group('object'), result.group('directive'), result.group('value'), result.group('clauses'))
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "delparam":
                        code, message = delparam(config, result.group('object'), result.group('directive'), result.group('clauses'))
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "removemodule":
                        code, message = removemodule(config, result.group('module'), result.group('object'), result.group('clauses'))
                        if not code:
                            if maction == "stop": return (code, message)
                else:
                    if command == "writeconfig":
                        code, message = writeconfig(config, configfile)
                        if not code:
                            if maction == "stop": return (code, message)
                    elif command == "sync":
                        code, message = sync(config, configfile, authfile)
                        if not code:
                            if maction == "stop": return (code, message)
                matched = True
        if not matched:
            if not line == "":
                return (False, "Error Unknown command %s" % (line))
        indexline += 1
    return (True, "Macro execution success")


def delobject(config, objectype, filters):
    dfilters = {}
    max = 0
    if len(filters) > 0:
        t = filters.split(',')
        for i in range(len(t)):
            (k, v) = t[i].split('=')
            dfilters[k] = v
    else:
        return (False, "Filter is mandatory")

    if config.has_key(objectype):
        filterok = 0
        max = len(config[objectype])
        removed = 0
        for i in range(max):
            filterok = 0
            for (d, v) in dfilters.items():
                filterok = filterok + 1
                if config[objectype][i].has_key(d):
                    if config[objectype][i][d] != v:
                        filterok = filterok - 1
                else:
                    filterok = filterok - 1
            if filterok == len(dfilters):
                config[objectype].pop(i)
                removed = removed + 1
        if removed == 0:
            return (False, "Filter did not return any result")
        else:
            return (True, "%d objects removed" % (removed))
    else:
        return (False, "No %s objects found" % (objectype))


def cloneobject(config, objectype, directive, filter):
    directives = {}
    filters = {}
    newobj = {}
    # extract directives to be modified
    for pair in directive.split(','):
        (d, v) = pair.split('=')
        directives[d] = v
    # extract filters
    for pair in filter.split(','):
        (d, v) = pair.split('=')
        filters[d] = v
    filterok = 0
    # find the matching object
    for o in config[objectype]:
        for (d, v) in filters.items():
            if o.has_key(d) and o[d] == v:
                filterok = filterok + 1
        if filterok == len(filters):
            newobj = copy.deepcopy(o)
            filterok = 0
    if len(newobj) == 0:
        return (False, "I was unable to find the object to be cloned")
    # create the new object
    for (d, v) in directives.items():
        newobj[d] = v
    # verify the unicity of the object
    for o in config[objectype]:
        if o[objectype + "_name"] == newobj[objectype + "_name"]:
            return (False, "An object of type %s with the name %s allready exist" % (objectype, newobj[objectype + "_name"]))

    config[objectype].append(newobj)
    return (True, config)


def getaddresses(config):
    allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner']
    addresses = []
    for (ot, oc) in config.items():
        if ot in allowed:
            for o in oc:
                for (d, v) in o.items():
                    if d == "address" and v != "localhost" and v != "127.0.01":
                        if not v in addresses:
                            addresses.append(v)
                            print v


def showconfig(config, objectype, filters=""):
    dfilters = {}
    if len(filters) > 0:
        t = filters.split(',')
        for i in range(len(t)):
            (k, v) = t[i].split('=')
            dfilters[k] = v

    if config.has_key(objectype):
        max = len(config[objectype])
        filterok = 0
        for i in range(max):
            filterok = 0
            #if config[objectype][i].has_key(directive):
            for (d, v) in dfilters.items():
                filterok = filterok + 1
                if config[objectype][i].has_key(d):
                    if config[objectype][i][d] != v:
                        filterok = filterok - 1
                else:
                    filterok = filterok - 1
            if filterok == len(dfilters):
                print "%s[%d]" % (objectype, i)
                for (d, v) in config[objectype][i].items():
                    print "  %s = %s" % (d, v)
    else:
        print "Unknown object type %s" % (o)
    return config


def getsatellitesaddresses(config):
    import netifaces
    import re
    allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner']
    addresses = []
    local = []

    """ detect local adresses """
    for ifname in netifaces.interfaces():
        for t in netifaces.ifaddresses(ifname).items():
            for e in t[1]:
                if e.has_key('addr'):
                    if re.match(r"[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}", e['addr']):
                        if e['addr'] != "127.0.0.1":
                            local.append(e['addr'])

    """ get all adresses defined in configuration """
    for (ot, oc) in config.items():
        if ot in allowed:
            for o in oc:
                for (d, v) in o.items():
                    if d == "address" and v != "localhost" and v != "127.0.01":
                        if not v in local and not v in addresses:
                            addresses.append(v)

    return (True, addresses)


def getauthdata(authfile):
    import re
    import string
    """ load authentication data """
    auth = {}
    creg = re.compile(r"^(?P<address>.*):(?P<login>.*):(?P<password>.*)")
    try:
        fd = open(authfile, 'r')
        data = map(string.strip, fd.readlines())
        fd.close()

        for line in data:
            if line != "":
                result = creg.match(line)
                if result == None:
                    return "There was an error in the authentication file at line: %s" % (line)
                auth[result.group("address")] = {"login": result.group("login"), "password": result.group("password")}
        return (True, auth)
    except:
        return (False, "Error while loading authentication data")


def sync(config, configfile, authfile):
    import re
    import paramiko
    import string

    code, addresses = getsatellitesaddresses(config)

    code, auth = getauthdata(authfile)
    if not code:
        return (False, auth)

    """ now push configuration to each satellite """
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        for address in addresses:
            print "Synch with: %s" % (address)
            if not auth.has_key(address):
                return (False, "Auth informations for %s does not exist in authfile" % (address))
            else:
                ssh.connect(address, username=auth[address]["login"], password=auth[address]["password"])
                ftp = ssh.open_sftp()
                ftp.put(configfile, configfile)
                ftp.close()
                ssh.close()
    except:
        return (False, "There was an error trying to push configuration to %s" % (address))

    return (True, addresses)


def deploy(authfile):
    import paramiko
    import tarfile

    code, auths = getauthdata(authfile)

    if not code:
        return (False, auths)

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    """ current user """
    user = os.getlogin()

    """ define home """
    if user == "root":
        home = "/root"
    else:
        home = "/home/%s" % (user)

    """ compress shinken in tar gz format """
    print "Make archive"
    source = os.path.abspath(os.getcwd() + '/../../../../')
    tar = tarfile.open('/tmp/shinken.tar.gz', 'w:gz')
    tar.add(source)
    tar.close()

    """ upload shinken archive to remote server """
    for address, auth in auths.items():
        print "Upload archive on %s"
        ssh.connect(address, username=auth["login"], password=auth["password"])
        ftp = ssh.open_sftp()
        ftp.put('/tmp/shinken.tar.gz', os.path.abspath('/tmp/shinken.tar.gz'))
        ftp.close()
        print "Extract archive"
        stdin, stdout, stderr = ssh.exec_command('cd /tmp && tar zxvf shinken.tar.gz && rm -Rf %s/shinken && mv %s/shinken %s/' % (home, user, home))
        out = stdout.read()
        err = stderr.read()
        print "Launch installation"
        stdin, stdout, stderr = ssh.exec_command('cd %s/shinken/contrib/alternative-installation/shinken-install/ && ./shinken.sh -d && ./shinken.sh -i' % (home))
        out = stdout.read()
        err = stderr.read()
        print out
        print err
        ssh.close()

    return (True, "OK")


def control(authfile, action):
    import re
    import paramiko
    import string

    code, auth = getauthdata(authfile)
    if not code:
        return (False, auth)

    """ which command for an action """
    commands = {"stop": "service shinken stop", "start": "service shinken start", "restart": "service shinken restart"}
    if not commands.has_key(action):
        return (False, "Unknown action command")

    """ now apply control action to all elements """

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    for address, authdata in auth.items():
        try:
            ssh.connect(address, username=authdata["login"], password=authdata["password"])
            ssh.exec_command("service shinken %s" % (action))
            ssh.close()
        except socket.error:
            return (False, "socket error (%s)" % (address))
        except paramiko.BadAuthenticationType:
            return (False, "BadAuthenticationType (%s)" % (address))
        except paramiko.BadHostKeyException:
            return (False, "BadHostKeyException (%s)" % (address))
        except paramiko.ChannelException:
            return (False, "ChannelException (%s)" % (address))
        except paramiko.ChannelException:
            return (False, "ChannelException (%s)" % (address))
        except paramiko.PasswordRequiredException:
            return (False, "PasswordRequiredException (%s)" % (address))
        except paramiko.SSHException:
            return (False, "SSHException (%s)" % (address))
        except paramiko.AuthenticationException:
            return (False, "AuthenticationException (%s)" % (address))

    return (True, "Action completed")


def writeconfig(config, configfile):
    bck = "%s.%d" % (configfile, time.time())
    os.rename(configfile, bck)
    fd = open(configfile, 'w')
    objects = ["arbiter", "poller", "scheduler", "broker", "reactionner", "receiver", "module", "realm"]
    for (t, s) in config.items():
        if t in objects:
            for o in range(len(config[t])):
                buff = "define %s {\n" % (t)
                fd.write(buff)
                for (d, v) in config[t][o].items():
                    if d != "imported_from":
                        buff = "  %s %s\n" % (d, v)
                        fd.write(buff)
                buff = "}\n\n"
                fd.write(buff)
    fd.close()
    return (True, "Config saved")


def addobject(config, objectype, directive):
    # allowed objects types to be added
    allowed = ['poller', 'arbiter', 'scheduler', 'broker', 'receiver', 'reactionner']

    # veritfy if object type is allowed
    if not objectype in allowed:
        print "Invalid objectype"
        sys.exit(2)

    # get a dict of directives
    try:
        directives = {}
        for pair in directive.split(','):
            (d, v) = pair.split('=')
            directives[d] = v
    except:
        print "An unrecoverable error occured while checking directives"
        sys.exit(2)

    # at least the directive objectype_name should exist
    if not directives.has_key(objectype + "_name"):
        print "The object definition should have at least an object name directive"
        sys.exit(2)

    # check if an object with the same name and type allready exist
    if config.has_key(objectype):
        good = 1
        # an object with the same type allready exist so we check it have different name
        name = directives[objectype + "_name"]
        for o in config[objectype]:
            if o[objectype + "_name"] == name:
                # ouch same object allready defined
                print "%s %s allready exist" % (objectype, name)
                sys.exit(2)

    # so we can create the new object
    newobject = {}
    for (d, v) in directives.items():
        if d != "imported_from":
            newobject[d] = v
    config[objectype].append(newobject)
    return config


def splitCount(s, count):
    return [s[i:i + count] for i in range(0, len(s), count)]


def dumpconfig(type, config, allowed):
    for (k, oc) in config.items():
        if k in allowed:
            if type != "" and type == k:
                display = 1
            else:
                display = 0

            if display == 1:
                print "".center(100, "=")
                print "| " + k.center(97, " ") + "|"
                print "".center(100, "=")
                for o in oc:
                    print "+".ljust(99, "-") + "+"
                    for (d, v) in o.items():
                        if d != "imported_from":
                            if len(v) > 48:
                                vp = splitCount(v, 47)
                                col1 = "| " + d.ljust(47, " ") + "| "
                                col2 = vp[0].ljust(48, " ") + "|"
                                print col1 + col2
                                vp.pop(0)
                                for vpe in vp:
                                    col1 = "| " + " ".ljust(47, " ") + "| "
                                    col2 = vpe.ljust(48, " ") + "|"
                                    print col1 + col2
                            else:
                                col1 = "| " + d.ljust(47, " ") + "| "
                                col2 = v.ljust(48, " ") + "|"
                                print col1 + col2
                    print "+".ljust(99, "-") + "+"


def getobjectnames(objectype, config, allowed):
    names = []
    for (k, oc) in config.items():
        if k in allowed and k == objectype:
            for o in oc:
                for (d, v) in o.items():
                    if objectype + "_name" == d:
                        names.append(v)
    print ','.join(names)
    return (True, ','.join(names))


def getdirective(config, objectype, directive, filters):
    try:
        dfilters = {}
        if len(filters) > 0:
            t = filters.split(',')
            for i in range(len(t)):
                (k, v) = t[i].split('=')
                dfilters[k] = v

        if config.has_key(objectype):
            ## max=len(config[objectype])
            ## filterok=0
            ## if max > 1 or max == 0:
            ##     return (False,"Two many values. Refine your filter")
            filterok = 0
            for (d, v) in dfilters.items():
                filterok = filterok + 1
                if config[objectype][0].has_key(d):
                    if config[objectype][0][d] != v:
                        filterok = filterok - 1
                else:
                    filterok = filterok - 1
            if filterok == len(dfilters):
                if not config[objectype][0].has_key(directive):
                    code = False
                    content = "Directive not found %s for object %s" % (directive, objectype)
                    return code, content
                else:
                    code = True
                    content = config[objectype][0][directive]
                    return code, content
            else:
                return (False, "Filters not matched")
        else:
            return (False, "%s not found" % (objectype))
    except:
        return (False, "Unknown error in getdirective")


def setparam(config, objectype, directive, value, filters):
    import re
    dfilters = {}
    if len(filters) > 0:
        t = filters.split(',')
        for i in range(len(t)):
            (k, v) = t[i].split('=')
            dfilters[k] = v

    if config.has_key(objectype):
        max = len(config[objectype])
        filterok = 0
        for i in range(max):
            filterok = 0
            for (d, v) in dfilters.items():
                filterok = filterok + 1
                if config[objectype][i].has_key(d):
                    if config[objectype][i][d] != v:
                        filterok = filterok - 1
                else:
                    filterok = filterok - 1
            if filterok == len(dfilters):
                """ if directive does not exist create it! """
                if not config[objectype][i].has_key(directive):
                    config[objectype][i][directive] = value
                    message = "Added configuration %s[%d] %s=%s" % (objectype, i, directive, value)
                else:
                    """ check if directive value allready exist """
                    if re.search(value, config[objectype][i][directive]) != None:
                        message = "Directive value allready exist"
                    else:
                        config[objectype][i][directive] = value
                        message = "updated configuration of %s[%d] %s=%s" % (objectype, i, directive, value)
                print message
                return (True, message)
    else:
        return (False, "Unknown object type %s" % (o))


def removemodule(config, module, objectype, filters):
    import re
    dfilters = {}
    if len(filters) > 0:
        t = filters.split(',')
        for i in range(len(t)):
            (k, v) = t[i].split('=')
            dfilters[k.strip()] = v.strip()

    code = False
    message = "Nothing was done"


    # check wether objectype is defined or not
    if config.has_key(objectype):
        # verify each filter (directive,value)
        for (directive, value) in dfilters.items():
            for o in config[objectype]:
                if o.has_key(directive) and o[directive] == value:
                    modules = []
                    for m in o["modules"].split(','):
                        modules.append(m.strip())
                    if module in modules:
                        while module in modules:
                            modules.remove(module)
                        o["modules"] = ",".join(modules)
                    message = "removed module %s from objects of type %s" % (module, objectype)
                    code = True
                    print message
                    return (code, message)
        message = "No module %s found in object of type %s" % (module, objectype)
        code = True
        print message
        return (code, message)
    else:
        message = "no objectype %s was found in configuration" % (objectype)
        code = True
        print message
        return (code, message)


def delparam(config, objectype, directive, filters):
    import re
    dfilters = {}
    if len(filters) > 0:
        t = filters.split(',')
        for i in range(len(t)):
            (k, v) = t[i].split('=')
            dfilters[k] = v

    if config.has_key(objectype):
        max = len(config[objectype])
        filterok = 0
        for i in range(max):
            filterok = 0
            for (d, v) in dfilters.items():
                filterok = filterok + 1
                if config[objectype][i].has_key(d):
                    if config[objectype][i][d] != v:
                        filterok = filterok - 1
                else:
                    filterok = filterok - 1
            if filterok == len(dfilters):
                """ if directive exist remove it! """
                if config[objectype][i].has_key(directive):
                    """ config[objectype][i][directive]=value"""
                    config[objectype][i].pop(directive)
                    print config[objectype][i]
                    message = "Removed directive %s from %s" % (directive, objectype)
                else:
                    message = "Nothing to remove"
                return (True, message)
    else:
        return (False, "Unknown object type %s" % (o))


def loadconfig(configfile):
    try:
        c = Config()
        c.read_config_silent = 1
        r = c.read_config(configfile)
        b = c.read_config_buf(r)
        return (True, b)
    except:
        return (False, "There was an error reading the configuration file")

if __name__ == "__main__":
    main()

########NEW FILE########
